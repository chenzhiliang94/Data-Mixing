2026-01-01 18:45:57.076740: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-01 18:45:57.103411: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-01 18:45:57.103462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-01 18:45:57.104294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-01 18:45:57.108850: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-01 18:45:57.962634: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'truthfulqa_gen', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_truthfulqa_gen_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['truthfulqa_gen']
evaluation tasks and weights:  {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
truthfulqa_gen
evaluation dataset:
data domain:  truthfulqa_gen  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/truthfulqa_gen/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 20, 1, 1, 1, 1, 1, 18, 0.05137445104835325, 2, 1]
Checking history sample input_X_between_0_1:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.5137445104835324, 0.041666666666666664, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2949618101119995
Checking history sample input_X:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 22, 1, 1, 0, 0, 1, 9, 0.09530863992118319, 22, 1]
Checking history sample input_X_between_0_1:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 0.6875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0703125, 0.9530863992118318, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.064080834388733
Checking history sample input_X:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 7, 0, 0, 0, 0, 1, 45, 0.012049704078718804, 22, 1]
Checking history sample input_X_between_0_1:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 0.21875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3515625, 0.12049704078718804, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.052124261856079
Checking history sample input_X:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 2, 1, 1, 1, 0, 0, 32, 0.03789572912213354, 26, 1]
Checking history sample input_X_between_0_1:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.3789572912213354, 0.5416666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4835323095321655
Checking history sample input_X:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 8, 0, 0, 1, 1, 1, 57, 0.05639372568359048, 47, 0]
Checking history sample input_X_between_0_1:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4453125, 0.5639372568359048, 0.9791666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2198251485824585
Checking history sample input_X:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 18, 1, 1, 0, 0, 1, 112, 0.0011300351648876107, 2, 1]
Checking history sample input_X_between_0_1:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 0.5625, 1.0, 1.0, 0.0, 0.0, 1.0, 0.875, 0.011300351648876106, 0.041666666666666664, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1017459630966187
Checking history sample input_X:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 11, 0, 1, 1, 0, 1, 121, 0.04409228366491266, 38, 0]
Checking history sample input_X_between_0_1:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 0.34375, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9453125, 0.4409228366491266, 0.7916666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0767676830291748
Checking history sample input_X:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 19, 1, 0, 0, 1, 0, 20, 0.057419890903339765, 17, 1]
Checking history sample input_X_between_0_1:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 0.59375, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15625, 0.5741989090333977, 0.3541666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0715935230255127
Checking history sample input_X:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 13, 0, 1, 0, 0, 1, 54, 0.07044921211215552, 48, 0]
Checking history sample input_X_between_0_1:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 0.40625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.421875, 0.7044921211215551, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3453316688537598
Checking history sample input_X:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 9, 1, 0, 1, 1, 0, 17, 0.07776680881547844, 40, 0]
Checking history sample input_X_between_0_1:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 0.28125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1328125, 0.7776680881547844, 0.8333333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.148055076599121
Checking history sample input_X:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 1, 0, 0, 0, 1, 0, 60, 0.001514616808966751, 14, 1]
Checking history sample input_X_between_0_1:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.46875, 0.015146168089667511, 0.2916666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4578598737716675
Checking history sample input_X:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 18, 1, 1, 1, 0, 1, 45, 0.001287895623877422, 34, 1]
Checking history sample input_X_between_0_1:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 0.5625, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3515625, 0.01287895623877422, 0.7083333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9687060117721558
Checking history sample input_X:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 8, 0, 0, 0, 1, 1, 51, 0.08835721159033366, 35, 0]
Checking history sample input_X_between_0_1:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3984375, 0.8835721159033366, 0.7291666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2380133867263794
Checking history sample input_X:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 2, 0, 0, 0, 1, 1, 28, 0.09450434861769766, 16, 1]
Checking history sample input_X_between_0_1:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 0.0625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.21875, 0.9450434861769765, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.283164620399475
Checking history sample input_X:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 2, 0, 0, 0, 1, 0, 27, 0.08742966606550949, 14, 0]
Checking history sample input_X_between_0_1:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2109375, 0.8742966606550948, 0.2916666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.5741345882415771
Checking history sample input_X:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 30, 0, 0, 0, 1, 0, 18, 0.07914275308569024, 23, 0]
Checking history sample input_X_between_0_1:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.140625, 0.7914275308569024, 0.4791666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0483520030975342
Checking history sample input_X:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 4, 1, 1, 1, 1, 1, 126, 0.005789639303569194, 25, 1]
Checking history sample input_X_between_0_1:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.05789639303569194, 0.5208333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9796491861343384
Checking history sample input_X:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 9, 1, 1, 1, 1, 1, 31, 0.03322680456132531, 22, 1]
Checking history sample input_X_between_0_1:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2421875, 0.33226804561325307, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1166555881500244
Checking history sample input_X:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 15, 0, 1, 0, 1, 1, 49, 0.0008783405064032635, 29, 1]
Checking history sample input_X_between_0_1:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 0.46875, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3828125, 0.008783405064032634, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8782313466072083
Checking history sample input_X:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 1, 0, 1, 1, 0, 1, 106, 0.08696702158391928, 5, 0]
Checking history sample input_X_between_0_1:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 0.03125, 0.0, 1.0, 1.0, 0.0, 1.0, 0.828125, 0.8696702158391928, 0.10416666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.4728790521621704
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1405 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11568570137023926, 0.5593270063400269, 0.8775433301925659, 0.39152204990386963, 0.850050151348114, 0.17808640003204346, 0.6121442914009094, 0.8796148300170898, 0.7908386588096619, 0.2741488516330719, 0.29945099353790283, 0.4006419777870178, 0.42160695791244507, 0.36082738637924194, 0.7397079467773438, 0.9440648555755615, 0.6489205360412598, 0.31095486879348755, 0.05506134033203125]  ‚Üí  acq = -0.6791512658911019
X = [0.715932309627533, 0.6539514064788818, 0.8098462224006653, 0.7744975090026855, 0.972852885723114, 0.16816496849060059, 0.5569791197776794, 0.02419304847717285, 0.704524576663971, 0.5567772388458252, 0.14607775211334229, 0.704124927520752, 0.49959105253219604, 0.112773597240448, 0.07655537128448486, 0.2058016061782837, 0.84648597240448, 0.6948097944259644, 0.19780606031417847]  ‚Üí  acq = -0.6791523302859666
X = [0.6376544833183289, 0.6774193048477173, 0.5981900095939636, 0.6874723434448242, 0.6874771118164062, 0.7081990242004395, 0.09618484973907471, 0.2818906307220459, 0.9322887659072876, 0.48387861251831055, 0.051174819469451904, 0.7482204437255859, 0.9775378108024597, 0.39128345251083374, 0.5683888792991638, 0.6686438322067261, 0.4476739764213562, 0.6335521936416626, 0.057910382747650146]  ‚Üí  acq = -0.6797207368604937
X = [0.34760361909866333, 0.6865503787994385, 0.5060580372810364, 0.4740711450576782, 0.20379775762557983, 0.9897298216819763, 0.05699533224105835, 0.7927238345146179, 0.9353301525115967, 0.18663886189460754, 0.8364107012748718, 0.8052005171775818, 0.015818774700164795, 0.9462813138961792, 0.07396399974822998, 0.4127258062362671, 0.2794000506401062, 0.517184853553772, 0.7394168972969055]  ‚Üí  acq = -0.6835171723127182
X = [0.8009253740310669, 0.9470440745353699, 0.9393591284751892, 0.6748310923576355, 0.13480311632156372, 0.43249398469924927, 0.6687753796577454, 0.25928670167922974, 0.663540244102478, 0.9486629962921143, 0.9985557794570923, 0.6633133888244629, 0.7510613203048706, 0.976007878780365, 0.25301873683929443, 0.07873918861150742, 0.7045878767967224, 0.12423109263181686, 0.8364186882972717]  ‚Üí  acq = -0.6791507568042675
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.0216, dtype=torch.float64), 0, tensor(0.2055, dtype=torch.float64), 0, 0, 0, tensor(0.1956, dtype=torch.float64), tensor(0.5773, dtype=torch.float64), 20, 1, 1, 1, 1, 1, 2, 1.0093173416231136e-17, 48.0, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(4.0619e-17, dtype=torch.float64), tensor(0.0216, dtype=torch.float64), tensor(2.0423e-16, dtype=torch.float64), tensor(0.2055, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.0343e-17, dtype=torch.float64), tensor(1.3546e-17, dtype=torch.float64), tensor(0.1956, dtype=torch.float64), tensor(0.5773, dtype=torch.float64), tensor(0.6165, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.0093e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.022
  rowan_hellaswag: 0
  sciq: 0.206
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.196
  arc_challenge: 0.577

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.0093173416231136e-17,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  2
lora dropout:  1.0093173416231136e-17
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,744,320 || all params: 8,033,005,568 || trainable%: 0.0342
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260101_185454-62d2o85c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/62d2o85c
{'loss': 2.4722, 'grad_norm': 10.209877014160156, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.492647647857666, 'eval_runtime': 3.8292, 'eval_samples_per_second': 261.15, 'eval_steps_per_second': 16.452, 'epoch': 0.04}
{'loss': 1.1304, 'grad_norm': 3.6489627361297607, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2980024814605713, 'eval_runtime': 3.7837, 'eval_samples_per_second': 264.291, 'eval_steps_per_second': 16.65, 'epoch': 0.08}
{'loss': 0.9722, 'grad_norm': 3.0721752643585205, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1923044919967651, 'eval_runtime': 3.7682, 'eval_samples_per_second': 265.376, 'eval_steps_per_second': 16.719, 'epoch': 0.12}
{'loss': 0.9237, 'grad_norm': 2.107025146484375, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.184078574180603, 'eval_runtime': 3.7713, 'eval_samples_per_second': 265.159, 'eval_steps_per_second': 16.705, 'epoch': 0.16}
{'loss': 0.9089, 'grad_norm': 1.9532514810562134, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.195697546005249, 'eval_runtime': 3.7779, 'eval_samples_per_second': 264.699, 'eval_steps_per_second': 16.676, 'epoch': 0.2}
{'loss': 0.9161, 'grad_norm': 2.2613470554351807, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2375190258026123, 'eval_runtime': 3.775, 'eval_samples_per_second': 264.899, 'eval_steps_per_second': 16.689, 'epoch': 0.24}
{'loss': 0.8226, 'grad_norm': 2.0564136505126953, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.15426504611969, 'eval_runtime': 3.7795, 'eval_samples_per_second': 264.586, 'eval_steps_per_second': 16.669, 'epoch': 0.28}
{'loss': 0.9007, 'grad_norm': 1.969499111175537, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.185806155204773, 'eval_runtime': 3.7855, 'eval_samples_per_second': 264.163, 'eval_steps_per_second': 16.642, 'epoch': 0.32}
{'loss': 0.819, 'grad_norm': 1.9055476188659668, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2084084749221802, 'eval_runtime': 3.7763, 'eval_samples_per_second': 264.812, 'eval_steps_per_second': 16.683, 'epoch': 0.36}
{'loss': 0.7379, 'grad_norm': 2.3208107948303223, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1515229940414429, 'eval_runtime': 3.7775, 'eval_samples_per_second': 264.724, 'eval_steps_per_second': 16.678, 'epoch': 0.4}
{'loss': 0.7485, 'grad_norm': 2.6158154010772705, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2430671453475952, 'eval_runtime': 3.7767, 'eval_samples_per_second': 264.785, 'eval_steps_per_second': 16.681, 'epoch': 0.44}
{'loss': 0.7683, 'grad_norm': 2.5140931606292725, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2202078104019165, 'eval_runtime': 3.7693, 'eval_samples_per_second': 265.3, 'eval_steps_per_second': 16.714, 'epoch': 0.48}
{'loss': 0.7045, 'grad_norm': 2.557875394821167, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2282177209854126, 'eval_runtime': 3.7853, 'eval_samples_per_second': 264.179, 'eval_steps_per_second': 16.643, 'epoch': 0.52}
{'loss': 0.7079, 'grad_norm': 2.625819206237793, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.192426323890686, 'eval_runtime': 3.7601, 'eval_samples_per_second': 265.947, 'eval_steps_per_second': 16.755, 'epoch': 0.56}
{'loss': 0.6384, 'grad_norm': 2.4914703369140625, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2049254179000854, 'eval_runtime': 3.7652, 'eval_samples_per_second': 265.593, 'eval_steps_per_second': 16.732, 'epoch': 0.6}
{'loss': 0.6737, 'grad_norm': 2.2955188751220703, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1888686418533325, 'eval_runtime': 3.7736, 'eval_samples_per_second': 264.996, 'eval_steps_per_second': 16.695, 'epoch': 0.64}
{'loss': 0.6055, 'grad_norm': 2.951443910598755, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1930112838745117, 'eval_runtime': 3.7698, 'eval_samples_per_second': 265.269, 'eval_steps_per_second': 16.712, 'epoch': 0.68}
{'loss': 0.6984, 'grad_norm': 2.9118247032165527, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.193463921546936, 'eval_runtime': 3.7753, 'eval_samples_per_second': 264.881, 'eval_steps_per_second': 16.688, 'epoch': 0.72}
{'loss': 0.6051, 'grad_norm': 3.035615921020508, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.227920413017273, 'eval_runtime': 3.7667, 'eval_samples_per_second': 265.486, 'eval_steps_per_second': 16.726, 'epoch': 0.76}
{'loss': 0.6481, 'grad_norm': 3.364672899246216, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2016520500183105, 'eval_runtime': 3.7739, 'eval_samples_per_second': 264.981, 'eval_steps_per_second': 16.694, 'epoch': 0.8}
{'loss': 0.5327, 'grad_norm': 2.3924827575683594, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.225429654121399, 'eval_runtime': 3.7741, 'eval_samples_per_second': 264.961, 'eval_steps_per_second': 16.693, 'epoch': 0.84}
{'loss': 0.5945, 'grad_norm': 2.7965731620788574, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.233730673789978, 'eval_runtime': 3.7617, 'eval_samples_per_second': 265.835, 'eval_steps_per_second': 16.748, 'epoch': 0.88}
{'loss': 0.5136, 'grad_norm': 3.1345396041870117, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2128407955169678, 'eval_runtime': 3.7814, 'eval_samples_per_second': 264.452, 'eval_steps_per_second': 16.66, 'epoch': 0.92}
{'loss': 0.57, 'grad_norm': 2.538982391357422, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.221909761428833, 'eval_runtime': 3.761, 'eval_samples_per_second': 265.889, 'eval_steps_per_second': 16.751, 'epoch': 0.96}
{'loss': 0.5936, 'grad_norm': 3.34372878074646, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2251425981521606, 'eval_runtime': 3.7685, 'eval_samples_per_second': 265.355, 'eval_steps_per_second': 16.717, 'epoch': 1.0}
{'train_runtime': 303.5874, 'train_samples_per_second': 32.933, 'train_steps_per_second': 2.059, 'train_loss': 0.8082577102661133, 'epoch': 1.0}
train_results:  {'eval_loss': [1.492647647857666, 1.2980024814605713, 1.1923044919967651, 1.184078574180603, 1.195697546005249, 1.2375190258026123, 1.15426504611969, 1.185806155204773, 1.2084084749221802, 1.1515229940414429, 1.2430671453475952, 1.2202078104019165, 1.2282177209854126, 1.192426323890686, 1.2049254179000854, 1.1888686418533325, 1.1930112838745117, 1.193463921546936, 1.227920413017273, 1.2016520500183105, 1.225429654121399, 1.233730673789978, 1.2128407955169678, 1.221909761428833, 1.2251425981521606], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.492647647857666, 1.2980024814605713, 1.1923044919967651, 1.184078574180603, 1.195697546005249, 1.2375190258026123, 1.15426504611969, 1.185806155204773, 1.2084084749221802, 1.1515229940414429, 1.2430671453475952, 1.2202078104019165, 1.2282177209854126, 1.192426323890686, 1.2049254179000854, 1.1888686418533325, 1.1930112838745117, 1.193463921546936, 1.227920413017273, 1.2016520500183105, 1.225429654121399, 1.233730673789978, 1.2128407955169678, 1.221909761428833, 1.2251425981521606]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -1.2251425981521606
max eval_loss so far:  -1.2251425981521606
BO observations:  [-1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6100 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.8402390604506028
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.8413208989784624
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.8412964438649706
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.8415105458066969
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.8721085601774998
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4797, dtype=torch.float64), 0, 0, 0, tensor(0.1230, dtype=torch.float64), 0, tensor(0.1487, dtype=torch.float64), tensor(0.2487, dtype=torch.float64), 21, 0, 0, 0, 1, 1, 128, 0.0, 29.275239470650614, 1]
normalized proposed parameters for next round by BO: [tensor(6.7729e-17, dtype=torch.float64), tensor(0.4797, dtype=torch.float64), tensor(5.3487e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1230, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1487, dtype=torch.float64), tensor(0.2487, dtype=torch.float64), tensor(0.6540, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6099, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.48
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.123
  wikitext: 0
  mmlu: 0.149
  arc_challenge: 0.249

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (29.275239470650614,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  29.275239470650614
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 99,090,432 || all params: 8,129,351,680 || trainable%: 1.2189
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4322, 'grad_norm': 0.553510308265686, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9195829629898071, 'eval_runtime': 3.4796, 'eval_samples_per_second': 287.388, 'eval_steps_per_second': 18.105, 'epoch': 0.04}
{'loss': 1.2309, 'grad_norm': 0.4252026379108429, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1720836162567139, 'eval_runtime': 3.446, 'eval_samples_per_second': 290.192, 'eval_steps_per_second': 18.282, 'epoch': 0.08}
{'loss': 1.0015, 'grad_norm': 0.15095770359039307, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.999029278755188, 'eval_runtime': 3.4273, 'eval_samples_per_second': 291.778, 'eval_steps_per_second': 18.382, 'epoch': 0.12}
{'loss': 1.0158, 'grad_norm': 0.16848592460155487, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9158771634101868, 'eval_runtime': 3.447, 'eval_samples_per_second': 290.106, 'eval_steps_per_second': 18.277, 'epoch': 0.16}
{'loss': 0.9588, 'grad_norm': 0.20250831544399261, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8532776832580566, 'eval_runtime': 3.4326, 'eval_samples_per_second': 291.324, 'eval_steps_per_second': 18.353, 'epoch': 0.2}
{'loss': 0.9393, 'grad_norm': 0.16050970554351807, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8016288876533508, 'eval_runtime': 3.4386, 'eval_samples_per_second': 290.813, 'eval_steps_per_second': 18.321, 'epoch': 0.24}
{'loss': 0.9019, 'grad_norm': 0.166523277759552, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.791935920715332, 'eval_runtime': 3.4406, 'eval_samples_per_second': 290.647, 'eval_steps_per_second': 18.311, 'epoch': 0.28}
{'loss': 0.9283, 'grad_norm': 0.17998462915420532, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7786737084388733, 'eval_runtime': 3.4412, 'eval_samples_per_second': 290.593, 'eval_steps_per_second': 18.307, 'epoch': 0.32}
{'loss': 0.907, 'grad_norm': 0.15550194680690765, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7558565735816956, 'eval_runtime': 3.4285, 'eval_samples_per_second': 291.669, 'eval_steps_per_second': 18.375, 'epoch': 0.36}
{'loss': 0.8994, 'grad_norm': 0.14092904329299927, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7551870942115784, 'eval_runtime': 3.4339, 'eval_samples_per_second': 291.214, 'eval_steps_per_second': 18.346, 'epoch': 0.4}
{'loss': 0.8921, 'grad_norm': 0.16130952537059784, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7375192642211914, 'eval_runtime': 3.4485, 'eval_samples_per_second': 289.984, 'eval_steps_per_second': 18.269, 'epoch': 0.44}
{'loss': 0.8788, 'grad_norm': 0.1532999873161316, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7289547324180603, 'eval_runtime': 3.4501, 'eval_samples_per_second': 289.849, 'eval_steps_per_second': 18.261, 'epoch': 0.48}
{'loss': 0.884, 'grad_norm': 0.1672086864709854, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7105705738067627, 'eval_runtime': 3.4491, 'eval_samples_per_second': 289.929, 'eval_steps_per_second': 18.266, 'epoch': 0.52}
{'loss': 0.8762, 'grad_norm': 0.1589229851961136, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7025511264801025, 'eval_runtime': 3.4503, 'eval_samples_per_second': 289.829, 'eval_steps_per_second': 18.259, 'epoch': 0.56}
{'loss': 0.8663, 'grad_norm': 0.1941339075565338, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6904189586639404, 'eval_runtime': 3.4931, 'eval_samples_per_second': 286.28, 'eval_steps_per_second': 18.036, 'epoch': 0.6}
{'loss': 0.838, 'grad_norm': 0.18851974606513977, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6932401657104492, 'eval_runtime': 3.4624, 'eval_samples_per_second': 288.814, 'eval_steps_per_second': 18.195, 'epoch': 0.64}
{'loss': 0.8767, 'grad_norm': 0.18963217735290527, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6788022518157959, 'eval_runtime': 3.4841, 'eval_samples_per_second': 287.014, 'eval_steps_per_second': 18.082, 'epoch': 0.68}
{'loss': 0.8451, 'grad_norm': 0.16814105212688446, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6680673956871033, 'eval_runtime': 3.4791, 'eval_samples_per_second': 287.427, 'eval_steps_per_second': 18.108, 'epoch': 0.72}
{'loss': 0.837, 'grad_norm': 0.1601540595293045, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6481189727783203, 'eval_runtime': 3.4891, 'eval_samples_per_second': 286.607, 'eval_steps_per_second': 18.056, 'epoch': 0.76}
{'loss': 0.8876, 'grad_norm': 0.17319267988204956, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6516443490982056, 'eval_runtime': 3.4846, 'eval_samples_per_second': 286.979, 'eval_steps_per_second': 18.08, 'epoch': 0.8}
{'loss': 0.8569, 'grad_norm': 0.1816883534193039, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6459276080131531, 'eval_runtime': 3.4907, 'eval_samples_per_second': 286.48, 'eval_steps_per_second': 18.048, 'epoch': 0.84}
{'loss': 0.8715, 'grad_norm': 0.21092969179153442, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6343182921409607, 'eval_runtime': 3.4912, 'eval_samples_per_second': 286.436, 'eval_steps_per_second': 18.045, 'epoch': 0.88}
{'loss': 0.8546, 'grad_norm': 0.18467172980308533, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6274380683898926, 'eval_runtime': 3.4532, 'eval_samples_per_second': 289.588, 'eval_steps_per_second': 18.244, 'epoch': 0.92}
{'loss': 0.8271, 'grad_norm': 0.18109361827373505, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6211353540420532, 'eval_runtime': 3.4539, 'eval_samples_per_second': 289.531, 'eval_steps_per_second': 18.24, 'epoch': 0.96}
{'loss': 0.8261, 'grad_norm': 0.19243039190769196, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6216726303100586, 'eval_runtime': 3.4584, 'eval_samples_per_second': 289.149, 'eval_steps_per_second': 18.216, 'epoch': 1.0}
{'train_runtime': 294.8639, 'train_samples_per_second': 33.907, 'train_steps_per_second': 2.12, 'train_loss': 0.9653182495117187, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9195829629898071, 1.1720836162567139, 0.999029278755188, 0.9158771634101868, 0.8532776832580566, 0.8016288876533508, 0.791935920715332, 0.7786737084388733, 0.7558565735816956, 0.7551870942115784, 0.7375192642211914, 0.7289547324180603, 0.7105705738067627, 0.7025511264801025, 0.6904189586639404, 0.6932401657104492, 0.6788022518157959, 0.6680673956871033, 0.6481189727783203, 0.6516443490982056, 0.6459276080131531, 0.6343182921409607, 0.6274380683898926, 0.6211353540420532, 0.6216726303100586], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9195829629898071, 1.1720836162567139, 0.999029278755188, 0.9158771634101868, 0.8532776832580566, 0.8016288876533508, 0.791935920715332, 0.7786737084388733, 0.7558565735816956, 0.7551870942115784, 0.7375192642211914, 0.7289547324180603, 0.7105705738067627, 0.7025511264801025, 0.6904189586639404, 0.6932401657104492, 0.6788022518157959, 0.6680673956871033, 0.6481189727783203, 0.6516443490982056, 0.6459276080131531, 0.6343182921409607, 0.6274380683898926, 0.6211353540420532, 0.6216726303100586]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.6216726303100586
max eval_loss so far:  -0.6216726303100586
BO observations:  [-1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.7707 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.8950122082302946
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.895001698421102
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.8949979740987815
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.9353229900145842
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.8942143892563292
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0280, dtype=torch.float64), 0, tensor(0.2328, dtype=torch.float64), 0, tensor(0.3577, dtype=torch.float64), 0, tensor(0.1128, dtype=torch.float64), tensor(0.2687, dtype=torch.float64), 21, 1, 1, 1, 1, 1, 128, 0.09731501319020856, 26.801795577226084, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0280, dtype=torch.float64), tensor(1.0041e-19, dtype=torch.float64), tensor(0.2328, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3577, dtype=torch.float64), tensor(6.6460e-17, dtype=torch.float64), tensor(0.1128, dtype=torch.float64), tensor(0.2687, dtype=torch.float64), tensor(0.6543, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9732, dtype=torch.float64), tensor(0.5584, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.028
  rowan_hellaswag: 0
  sciq: 0.233
  triviaqa: 0
  truthfulqa_gen: 0.358
  wikitext: 0
  mmlu: 0.113
  arc_challenge: 0.269

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09731501319020856,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (26.801795577226084,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.09731501319020856
lora alpha:  26.801795577226084
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,418,304 || all params: 8,214,679,552 || trainable%: 2.2450
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9729, 'grad_norm': 0.7176038026809692, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4630861282348633, 'eval_runtime': 3.749, 'eval_samples_per_second': 266.74, 'eval_steps_per_second': 16.805, 'epoch': 0.04}
{'loss': 1.2296, 'grad_norm': 0.2620275020599365, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9859775304794312, 'eval_runtime': 3.7479, 'eval_samples_per_second': 266.817, 'eval_steps_per_second': 16.809, 'epoch': 0.08}
{'loss': 1.0635, 'grad_norm': 0.2630784809589386, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8852393627166748, 'eval_runtime': 3.7425, 'eval_samples_per_second': 267.199, 'eval_steps_per_second': 16.834, 'epoch': 0.12}
{'loss': 1.0007, 'grad_norm': 0.3337964415550232, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7680578827857971, 'eval_runtime': 3.745, 'eval_samples_per_second': 267.026, 'eval_steps_per_second': 16.823, 'epoch': 0.16}
{'loss': 0.8869, 'grad_norm': 0.3199019134044647, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6921885013580322, 'eval_runtime': 3.7426, 'eval_samples_per_second': 267.197, 'eval_steps_per_second': 16.833, 'epoch': 0.2}
{'loss': 0.8494, 'grad_norm': 0.23636247217655182, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6489406228065491, 'eval_runtime': 3.7436, 'eval_samples_per_second': 267.119, 'eval_steps_per_second': 16.829, 'epoch': 0.24}
{'loss': 0.8793, 'grad_norm': 0.27555590867996216, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6141010522842407, 'eval_runtime': 3.7545, 'eval_samples_per_second': 266.346, 'eval_steps_per_second': 16.78, 'epoch': 0.28}
{'loss': 0.8325, 'grad_norm': 0.23821261525154114, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5698902606964111, 'eval_runtime': 3.7601, 'eval_samples_per_second': 265.95, 'eval_steps_per_second': 16.755, 'epoch': 0.32}
{'loss': 0.816, 'grad_norm': 0.2430061548948288, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5538055896759033, 'eval_runtime': 3.7669, 'eval_samples_per_second': 265.471, 'eval_steps_per_second': 16.725, 'epoch': 0.36}
{'loss': 0.8119, 'grad_norm': 0.20489858090877533, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5071613788604736, 'eval_runtime': 3.7598, 'eval_samples_per_second': 265.972, 'eval_steps_per_second': 16.756, 'epoch': 0.4}
{'loss': 0.7875, 'grad_norm': 0.22206933796405792, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.47702035307884216, 'eval_runtime': 3.7516, 'eval_samples_per_second': 266.552, 'eval_steps_per_second': 16.793, 'epoch': 0.44}
{'loss': 0.7885, 'grad_norm': 0.2476872205734253, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.450797438621521, 'eval_runtime': 3.7509, 'eval_samples_per_second': 266.599, 'eval_steps_per_second': 16.796, 'epoch': 0.48}
{'loss': 0.7578, 'grad_norm': 0.22090288996696472, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.43804657459259033, 'eval_runtime': 3.7659, 'eval_samples_per_second': 265.541, 'eval_steps_per_second': 16.729, 'epoch': 0.52}
{'loss': 0.7652, 'grad_norm': 0.2220773696899414, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.40481433272361755, 'eval_runtime': 3.7519, 'eval_samples_per_second': 266.529, 'eval_steps_per_second': 16.791, 'epoch': 0.56}
{'loss': 0.7202, 'grad_norm': 0.2867722511291504, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.36386650800704956, 'eval_runtime': 3.7505, 'eval_samples_per_second': 266.629, 'eval_steps_per_second': 16.798, 'epoch': 0.6}
{'loss': 0.7685, 'grad_norm': 0.31359437108039856, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.3525546193122864, 'eval_runtime': 3.7505, 'eval_samples_per_second': 266.634, 'eval_steps_per_second': 16.798, 'epoch': 0.64}
{'loss': 0.7603, 'grad_norm': 0.2658728361129761, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3224634528160095, 'eval_runtime': 3.7641, 'eval_samples_per_second': 265.665, 'eval_steps_per_second': 16.737, 'epoch': 0.68}
{'loss': 0.6773, 'grad_norm': 0.3198759853839874, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.30719757080078125, 'eval_runtime': 3.7673, 'eval_samples_per_second': 265.445, 'eval_steps_per_second': 16.723, 'epoch': 0.72}
{'loss': 0.6598, 'grad_norm': 0.31604671478271484, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.27976658940315247, 'eval_runtime': 3.7469, 'eval_samples_per_second': 266.884, 'eval_steps_per_second': 16.814, 'epoch': 0.76}
{'loss': 0.7057, 'grad_norm': 0.3140617907047272, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.26925191283226013, 'eval_runtime': 3.7535, 'eval_samples_per_second': 266.418, 'eval_steps_per_second': 16.784, 'epoch': 0.8}
{'loss': 0.6468, 'grad_norm': 0.27143508195877075, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.25421223044395447, 'eval_runtime': 3.7443, 'eval_samples_per_second': 267.07, 'eval_steps_per_second': 16.825, 'epoch': 0.84}
{'loss': 0.6887, 'grad_norm': 0.3377537429332733, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.24810586869716644, 'eval_runtime': 3.7482, 'eval_samples_per_second': 266.791, 'eval_steps_per_second': 16.808, 'epoch': 0.88}
{'loss': 0.6528, 'grad_norm': 0.2794829308986664, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.23654800653457642, 'eval_runtime': 3.7534, 'eval_samples_per_second': 266.429, 'eval_steps_per_second': 16.785, 'epoch': 0.92}
{'loss': 0.6608, 'grad_norm': 0.37056243419647217, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.22809970378875732, 'eval_runtime': 3.7561, 'eval_samples_per_second': 266.236, 'eval_steps_per_second': 16.773, 'epoch': 0.96}
{'loss': 0.6265, 'grad_norm': 0.28502917289733887, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.22662609815597534, 'eval_runtime': 3.7606, 'eval_samples_per_second': 265.917, 'eval_steps_per_second': 16.753, 'epoch': 1.0}
{'train_runtime': 282.2822, 'train_samples_per_second': 35.415, 'train_steps_per_second': 2.214, 'train_loss': 0.8803528106689453, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4630861282348633, 0.9859775304794312, 0.8852393627166748, 0.7680578827857971, 0.6921885013580322, 0.6489406228065491, 0.6141010522842407, 0.5698902606964111, 0.5538055896759033, 0.5071613788604736, 0.47702035307884216, 0.450797438621521, 0.43804657459259033, 0.40481433272361755, 0.36386650800704956, 0.3525546193122864, 0.3224634528160095, 0.30719757080078125, 0.27976658940315247, 0.26925191283226013, 0.25421223044395447, 0.24810586869716644, 0.23654800653457642, 0.22809970378875732, 0.22662609815597534], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4630861282348633, 0.9859775304794312, 0.8852393627166748, 0.7680578827857971, 0.6921885013580322, 0.6489406228065491, 0.6141010522842407, 0.5698902606964111, 0.5538055896759033, 0.5071613788604736, 0.47702035307884216, 0.450797438621521, 0.43804657459259033, 0.40481433272361755, 0.36386650800704956, 0.3525546193122864, 0.3224634528160095, 0.30719757080078125, 0.27976658940315247, 0.26925191283226013, 0.25421223044395447, 0.24810586869716644, 0.23654800653457642, 0.22809970378875732, 0.22662609815597534]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.22662609815597534
max eval_loss so far:  -0.22662609815597534
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.3036 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.8698107892730245
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.8816650862248967
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.8799799546003042
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.8816651443892226
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.8816650862273963
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3536, dtype=torch.float64), tensor(0.0461, dtype=torch.float64), tensor(0.4335, dtype=torch.float64), 0, tensor(0.0344, dtype=torch.float64), 0, 0, tensor(0.1324, dtype=torch.float64), 13, 0, 0, 0, 1, 1, 111, 0.0, 31.737160320822, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.3536, dtype=torch.float64), tensor(0.0461, dtype=torch.float64), tensor(0.4335, dtype=torch.float64), tensor(3.6826e-18, dtype=torch.float64), tensor(0.0344, dtype=torch.float64), tensor(5.0920e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1324, dtype=torch.float64), tensor(0.3949, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8643, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6612, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.354
  rowan_hellaswag: 0.046
  sciq: 0.433
  triviaqa: 0
  truthfulqa_gen: 0.034
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.132

LoRA Parameters:
  lora_r: (111,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (31.737160320822,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  111
lora dropout:  0.0
lora alpha:  31.737160320822
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,194,752 || all params: 8,083,456,000 || trainable%: 0.6581
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8292, 'grad_norm': 0.9099093675613403, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.027895927429199, 'eval_runtime': 3.405, 'eval_samples_per_second': 293.688, 'eval_steps_per_second': 18.502, 'epoch': 0.04}
{'loss': 1.38, 'grad_norm': 0.7023954391479492, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.219827651977539, 'eval_runtime': 3.3894, 'eval_samples_per_second': 295.037, 'eval_steps_per_second': 18.587, 'epoch': 0.08}
{'loss': 1.0339, 'grad_norm': 0.3168255388736725, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1462942361831665, 'eval_runtime': 3.3912, 'eval_samples_per_second': 294.882, 'eval_steps_per_second': 18.578, 'epoch': 0.12}
{'loss': 1.0093, 'grad_norm': 0.2274985909461975, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0807921886444092, 'eval_runtime': 3.392, 'eval_samples_per_second': 294.813, 'eval_steps_per_second': 18.573, 'epoch': 0.16}
{'loss': 0.9953, 'grad_norm': 0.3019355535507202, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9976770281791687, 'eval_runtime': 3.4038, 'eval_samples_per_second': 293.787, 'eval_steps_per_second': 18.509, 'epoch': 0.2}
{'loss': 0.9799, 'grad_norm': 0.24130560457706451, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9570052623748779, 'eval_runtime': 3.413, 'eval_samples_per_second': 292.997, 'eval_steps_per_second': 18.459, 'epoch': 0.24}
{'loss': 0.9301, 'grad_norm': 0.25475019216537476, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9493043422698975, 'eval_runtime': 3.4181, 'eval_samples_per_second': 292.558, 'eval_steps_per_second': 18.431, 'epoch': 0.28}
{'loss': 0.9761, 'grad_norm': 0.20542114973068237, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9282054305076599, 'eval_runtime': 3.4108, 'eval_samples_per_second': 293.19, 'eval_steps_per_second': 18.471, 'epoch': 0.32}
{'loss': 0.9182, 'grad_norm': 0.23175710439682007, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9751737117767334, 'eval_runtime': 3.4148, 'eval_samples_per_second': 292.84, 'eval_steps_per_second': 18.449, 'epoch': 0.36}
{'loss': 0.9465, 'grad_norm': 0.21106889843940735, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9405490756034851, 'eval_runtime': 3.4298, 'eval_samples_per_second': 291.563, 'eval_steps_per_second': 18.368, 'epoch': 0.4}
{'loss': 0.9105, 'grad_norm': 0.19375768303871155, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9385719895362854, 'eval_runtime': 3.4261, 'eval_samples_per_second': 291.878, 'eval_steps_per_second': 18.388, 'epoch': 0.44}
{'loss': 0.9479, 'grad_norm': 0.19240771234035492, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9180154800415039, 'eval_runtime': 3.4296, 'eval_samples_per_second': 291.579, 'eval_steps_per_second': 18.369, 'epoch': 0.48}
{'loss': 0.9093, 'grad_norm': 0.21329107880592346, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9207925796508789, 'eval_runtime': 3.4334, 'eval_samples_per_second': 291.253, 'eval_steps_per_second': 18.349, 'epoch': 0.52}
{'loss': 0.8891, 'grad_norm': 0.2156817615032196, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9046497344970703, 'eval_runtime': 3.4269, 'eval_samples_per_second': 291.81, 'eval_steps_per_second': 18.384, 'epoch': 0.56}
{'loss': 0.9044, 'grad_norm': 0.20730158686637878, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.924270510673523, 'eval_runtime': 3.4406, 'eval_samples_per_second': 290.644, 'eval_steps_per_second': 18.311, 'epoch': 0.6}
{'loss': 0.9422, 'grad_norm': 0.19498737156391144, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8950262665748596, 'eval_runtime': 3.4261, 'eval_samples_per_second': 291.875, 'eval_steps_per_second': 18.388, 'epoch': 0.64}
{'loss': 0.9194, 'grad_norm': 0.19695860147476196, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8854354023933411, 'eval_runtime': 3.4525, 'eval_samples_per_second': 289.643, 'eval_steps_per_second': 18.247, 'epoch': 0.68}
{'loss': 0.9233, 'grad_norm': 0.2093234807252884, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8866645693778992, 'eval_runtime': 3.4452, 'eval_samples_per_second': 290.258, 'eval_steps_per_second': 18.286, 'epoch': 0.72}
{'loss': 0.9087, 'grad_norm': 0.20720350742340088, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8762829303741455, 'eval_runtime': 3.4356, 'eval_samples_per_second': 291.068, 'eval_steps_per_second': 18.337, 'epoch': 0.76}
{'loss': 0.9211, 'grad_norm': 0.2128620147705078, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8822435140609741, 'eval_runtime': 3.4336, 'eval_samples_per_second': 291.236, 'eval_steps_per_second': 18.348, 'epoch': 0.8}
{'loss': 0.8713, 'grad_norm': 0.1998821198940277, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8738338351249695, 'eval_runtime': 3.439, 'eval_samples_per_second': 290.785, 'eval_steps_per_second': 18.319, 'epoch': 0.84}
{'loss': 0.8912, 'grad_norm': 0.20113015174865723, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8755700588226318, 'eval_runtime': 3.4427, 'eval_samples_per_second': 290.473, 'eval_steps_per_second': 18.3, 'epoch': 0.88}
{'loss': 0.8728, 'grad_norm': 0.2077399641275406, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.874694287776947, 'eval_runtime': 3.4365, 'eval_samples_per_second': 290.994, 'eval_steps_per_second': 18.333, 'epoch': 0.92}
{'loss': 0.8553, 'grad_norm': 0.21519985795021057, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8634272217750549, 'eval_runtime': 3.4336, 'eval_samples_per_second': 291.241, 'eval_steps_per_second': 18.348, 'epoch': 0.96}
{'loss': 0.8612, 'grad_norm': 0.23431535065174103, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.865646481513977, 'eval_runtime': 3.4321, 'eval_samples_per_second': 291.369, 'eval_steps_per_second': 18.356, 'epoch': 1.0}
{'train_runtime': 290.0469, 'train_samples_per_second': 34.47, 'train_steps_per_second': 2.155, 'train_loss': 1.021044497680664, 'epoch': 1.0}
train_results:  {'eval_loss': [2.027895927429199, 1.219827651977539, 1.1462942361831665, 1.0807921886444092, 0.9976770281791687, 0.9570052623748779, 0.9493043422698975, 0.9282054305076599, 0.9751737117767334, 0.9405490756034851, 0.9385719895362854, 0.9180154800415039, 0.9207925796508789, 0.9046497344970703, 0.924270510673523, 0.8950262665748596, 0.8854354023933411, 0.8866645693778992, 0.8762829303741455, 0.8822435140609741, 0.8738338351249695, 0.8755700588226318, 0.874694287776947, 0.8634272217750549, 0.865646481513977], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.027895927429199, 1.219827651977539, 1.1462942361831665, 1.0807921886444092, 0.9976770281791687, 0.9570052623748779, 0.9493043422698975, 0.9282054305076599, 0.9751737117767334, 0.9405490756034851, 0.9385719895362854, 0.9180154800415039, 0.9207925796508789, 0.9046497344970703, 0.924270510673523, 0.8950262665748596, 0.8854354023933411, 0.8866645693778992, 0.8762829303741455, 0.8822435140609741, 0.8738338351249695, 0.8755700588226318, 0.874694287776947, 0.8634272217750549, 0.865646481513977]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.865646481513977
max eval_loss so far:  -0.22662609815597534
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9847 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.9428407737067688
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.9430635080109927
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.9427599982736032
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.9427599145076078
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.9427599241608432
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3674, dtype=torch.float64), 0, tensor(0.0516, dtype=torch.float64), tensor(0.1234, dtype=torch.float64), 0, tensor(0.0965, dtype=torch.float64), 0, tensor(0.0680, dtype=torch.float64), tensor(0.2930, dtype=torch.float64), 20, 0, 1, 0, 0, 1, 47, 0.0, 32.106209822146326, 1]
normalized proposed parameters for next round by BO: [tensor(0.3674, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0516, dtype=torch.float64), tensor(0.1234, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0965, dtype=torch.float64), tensor(2.0292e-17, dtype=torch.float64), tensor(0.0680, dtype=torch.float64), tensor(0.2930, dtype=torch.float64), tensor(0.6161, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3643, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6689, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.367
  gsm8k: 0
  rowan_hellaswag: 0.052
  sciq: 0.123
  triviaqa: 0
  truthfulqa_gen: 0.097
  wikitext: 0
  mmlu: 0.068
  arc_challenge: 0.293

LoRA Parameters:
  lora_r: (47,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (32.106209822146326,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  47
lora dropout:  0.0
lora alpha:  32.106209822146326
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 22,138,880 || all params: 8,052,400,128 || trainable%: 0.2749
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4706, 'grad_norm': 1.108782410621643, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1099555492401123, 'eval_runtime': 3.3643, 'eval_samples_per_second': 297.241, 'eval_steps_per_second': 18.726, 'epoch': 0.04}
{'loss': 1.5474, 'grad_norm': 0.5361732244491577, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3246049880981445, 'eval_runtime': 3.3564, 'eval_samples_per_second': 297.94, 'eval_steps_per_second': 18.77, 'epoch': 0.08}
{'loss': 1.288, 'grad_norm': 0.39169782400131226, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1980630159378052, 'eval_runtime': 3.3561, 'eval_samples_per_second': 297.967, 'eval_steps_per_second': 18.772, 'epoch': 0.12}
{'loss': 1.2131, 'grad_norm': 0.37652966380119324, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0559312105178833, 'eval_runtime': 3.3609, 'eval_samples_per_second': 297.54, 'eval_steps_per_second': 18.745, 'epoch': 0.16}
{'loss': 1.143, 'grad_norm': 0.34128427505493164, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0064752101898193, 'eval_runtime': 3.3754, 'eval_samples_per_second': 296.257, 'eval_steps_per_second': 18.664, 'epoch': 0.2}
{'loss': 1.0622, 'grad_norm': 0.3191048502922058, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0008183717727661, 'eval_runtime': 3.3872, 'eval_samples_per_second': 295.231, 'eval_steps_per_second': 18.6, 'epoch': 0.24}
{'loss': 1.068, 'grad_norm': 0.27369800209999084, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9456778764724731, 'eval_runtime': 3.3782, 'eval_samples_per_second': 296.011, 'eval_steps_per_second': 18.649, 'epoch': 0.28}
{'loss': 1.0566, 'grad_norm': 0.3069440722465515, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9499415159225464, 'eval_runtime': 3.3784, 'eval_samples_per_second': 295.996, 'eval_steps_per_second': 18.648, 'epoch': 0.32}
{'loss': 1.0415, 'grad_norm': 0.33304017782211304, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9337949752807617, 'eval_runtime': 3.3685, 'eval_samples_per_second': 296.871, 'eval_steps_per_second': 18.703, 'epoch': 0.36}
{'loss': 1.0685, 'grad_norm': 0.32913336157798767, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.918051540851593, 'eval_runtime': 3.3752, 'eval_samples_per_second': 296.28, 'eval_steps_per_second': 18.666, 'epoch': 0.4}
{'loss': 1.0226, 'grad_norm': 0.34329378604888916, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8779000043869019, 'eval_runtime': 3.3731, 'eval_samples_per_second': 296.465, 'eval_steps_per_second': 18.677, 'epoch': 0.44}
{'loss': 1.0129, 'grad_norm': 0.34334009885787964, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8538323640823364, 'eval_runtime': 3.3771, 'eval_samples_per_second': 296.116, 'eval_steps_per_second': 18.655, 'epoch': 0.48}
{'loss': 1.003, 'grad_norm': 0.39084675908088684, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.838701069355011, 'eval_runtime': 3.3801, 'eval_samples_per_second': 295.853, 'eval_steps_per_second': 18.639, 'epoch': 0.52}
{'loss': 1.024, 'grad_norm': 0.3655533790588379, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7920871376991272, 'eval_runtime': 3.3989, 'eval_samples_per_second': 294.216, 'eval_steps_per_second': 18.536, 'epoch': 0.56}
{'loss': 0.9281, 'grad_norm': 0.3147426247596741, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7475939393043518, 'eval_runtime': 3.4074, 'eval_samples_per_second': 293.476, 'eval_steps_per_second': 18.489, 'epoch': 0.6}
{'loss': 0.8954, 'grad_norm': 0.3476794958114624, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7528176307678223, 'eval_runtime': 3.4089, 'eval_samples_per_second': 293.347, 'eval_steps_per_second': 18.481, 'epoch': 0.64}
{'loss': 0.9639, 'grad_norm': 0.43087244033813477, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7268808484077454, 'eval_runtime': 3.3784, 'eval_samples_per_second': 295.998, 'eval_steps_per_second': 18.648, 'epoch': 0.68}
{'loss': 0.9022, 'grad_norm': 0.38414275646209717, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7113866209983826, 'eval_runtime': 3.3722, 'eval_samples_per_second': 296.546, 'eval_steps_per_second': 18.682, 'epoch': 0.72}
{'loss': 0.9365, 'grad_norm': 0.4828992784023285, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7185875773429871, 'eval_runtime': 3.3793, 'eval_samples_per_second': 295.924, 'eval_steps_per_second': 18.643, 'epoch': 0.76}
{'loss': 0.9664, 'grad_norm': 0.4289400577545166, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7173421382904053, 'eval_runtime': 3.3763, 'eval_samples_per_second': 296.178, 'eval_steps_per_second': 18.659, 'epoch': 0.8}
{'loss': 0.9201, 'grad_norm': 0.40353158116340637, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6936126947402954, 'eval_runtime': 3.379, 'eval_samples_per_second': 295.95, 'eval_steps_per_second': 18.645, 'epoch': 0.84}
{'loss': 0.9136, 'grad_norm': 0.49635690450668335, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.69771409034729, 'eval_runtime': 3.3779, 'eval_samples_per_second': 296.042, 'eval_steps_per_second': 18.651, 'epoch': 0.88}
{'loss': 0.9549, 'grad_norm': 0.37984171509742737, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6997287273406982, 'eval_runtime': 3.3824, 'eval_samples_per_second': 295.652, 'eval_steps_per_second': 18.626, 'epoch': 0.92}
{'loss': 0.8948, 'grad_norm': 0.47650688886642456, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6908026337623596, 'eval_runtime': 3.3796, 'eval_samples_per_second': 295.897, 'eval_steps_per_second': 18.641, 'epoch': 0.96}
{'loss': 0.9187, 'grad_norm': 0.5428705811500549, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6910058856010437, 'eval_runtime': 3.3752, 'eval_samples_per_second': 296.276, 'eval_steps_per_second': 18.665, 'epoch': 1.0}
{'train_runtime': 258.341, 'train_samples_per_second': 38.697, 'train_steps_per_second': 2.419, 'train_loss': 1.12864482421875, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1099555492401123, 1.3246049880981445, 1.1980630159378052, 1.0559312105178833, 1.0064752101898193, 1.0008183717727661, 0.9456778764724731, 0.9499415159225464, 0.9337949752807617, 0.918051540851593, 0.8779000043869019, 0.8538323640823364, 0.838701069355011, 0.7920871376991272, 0.7475939393043518, 0.7528176307678223, 0.7268808484077454, 0.7113866209983826, 0.7185875773429871, 0.7173421382904053, 0.6936126947402954, 0.69771409034729, 0.6997287273406982, 0.6908026337623596, 0.6910058856010437], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1099555492401123, 1.3246049880981445, 1.1980630159378052, 1.0559312105178833, 1.0064752101898193, 1.0008183717727661, 0.9456778764724731, 0.9499415159225464, 0.9337949752807617, 0.918051540851593, 0.8779000043869019, 0.8538323640823364, 0.838701069355011, 0.7920871376991272, 0.7475939393043518, 0.7528176307678223, 0.7268808484077454, 0.7113866209983826, 0.7185875773429871, 0.7173421382904053, 0.6936126947402954, 0.69771409034729, 0.6997287273406982, 0.6908026337623596, 0.6910058856010437]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1264557838439941
current iteration best possible eval_loss (full train run):  -0.6910058856010437
max eval_loss so far:  -0.22662609815597534
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1488 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.9350444970141253
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.9351322494041852
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.9351322490854594
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.9351023632924277
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.9351322491478666
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0258, dtype=torch.float64), tensor(0.1662, dtype=torch.float64), tensor(0.1606, dtype=torch.float64), tensor(0.1299, dtype=torch.float64), 0, tensor(0.0619, dtype=torch.float64), tensor(0.4556, dtype=torch.float64), 14, 0, 1, 0, 1, 1, 63, 5.177956050417777e-19, 26.611116691575894, 1]
normalized proposed parameters for next round by BO: [tensor(3.4676e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0258, dtype=torch.float64), tensor(0.1662, dtype=torch.float64), tensor(0.1606, dtype=torch.float64), tensor(0.1299, dtype=torch.float64), tensor(3.3727e-17, dtype=torch.float64), tensor(0.0619, dtype=torch.float64), tensor(0.4556, dtype=torch.float64), tensor(0.4326, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4935, dtype=torch.float64), tensor(5.1780e-18, dtype=torch.float64), tensor(0.5544, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.026
  sciq: 0.166
  triviaqa: 0.161
  truthfulqa_gen: 0.13
  wikitext: 0
  mmlu: 0.062
  arc_challenge: 0.456

LoRA Parameters:
  lora_r: (63,)
  lora_dropout: (5.177956050417777e-19,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (26.611116691575894,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  63
lora dropout:  5.177956050417777e-19
lora alpha:  26.611116691575894
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 37,029,888 || all params: 8,067,291,136 || trainable%: 0.4590
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2781, 'grad_norm': 1.88717782497406, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8780912160873413, 'eval_runtime': 3.4591, 'eval_samples_per_second': 289.094, 'eval_steps_per_second': 18.213, 'epoch': 0.04}
{'loss': 1.4182, 'grad_norm': 0.9697611331939697, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1420425176620483, 'eval_runtime': 3.4625, 'eval_samples_per_second': 288.811, 'eval_steps_per_second': 18.195, 'epoch': 0.08}
{'loss': 1.1012, 'grad_norm': 0.46996450424194336, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.006505012512207, 'eval_runtime': 3.4547, 'eval_samples_per_second': 289.457, 'eval_steps_per_second': 18.236, 'epoch': 0.12}
{'loss': 1.0872, 'grad_norm': 0.3364351987838745, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9077023863792419, 'eval_runtime': 3.4652, 'eval_samples_per_second': 288.581, 'eval_steps_per_second': 18.181, 'epoch': 0.16}
{'loss': 1.0024, 'grad_norm': 0.48024606704711914, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8770560026168823, 'eval_runtime': 3.4583, 'eval_samples_per_second': 289.161, 'eval_steps_per_second': 18.217, 'epoch': 0.2}
{'loss': 0.944, 'grad_norm': 0.322852224111557, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8251107335090637, 'eval_runtime': 3.4623, 'eval_samples_per_second': 288.822, 'eval_steps_per_second': 18.196, 'epoch': 0.24}
{'loss': 0.9802, 'grad_norm': 0.4504922330379486, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8067312240600586, 'eval_runtime': 3.4705, 'eval_samples_per_second': 288.147, 'eval_steps_per_second': 18.153, 'epoch': 0.28}
{'loss': 0.9346, 'grad_norm': 0.3300868570804596, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7810920476913452, 'eval_runtime': 3.4702, 'eval_samples_per_second': 288.168, 'eval_steps_per_second': 18.155, 'epoch': 0.32}
{'loss': 0.8892, 'grad_norm': 0.313254714012146, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7814004421234131, 'eval_runtime': 3.4626, 'eval_samples_per_second': 288.8, 'eval_steps_per_second': 18.194, 'epoch': 0.36}
{'loss': 0.8864, 'grad_norm': 0.37960508465766907, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7524638175964355, 'eval_runtime': 3.4619, 'eval_samples_per_second': 288.862, 'eval_steps_per_second': 18.198, 'epoch': 0.4}
{'loss': 0.8882, 'grad_norm': 0.33913472294807434, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7439793348312378, 'eval_runtime': 3.4637, 'eval_samples_per_second': 288.707, 'eval_steps_per_second': 18.189, 'epoch': 0.44}
{'loss': 0.8648, 'grad_norm': 0.3471643626689911, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.730326235294342, 'eval_runtime': 3.4622, 'eval_samples_per_second': 288.831, 'eval_steps_per_second': 18.196, 'epoch': 0.48}
{'loss': 0.8564, 'grad_norm': 0.3684060275554657, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7443183064460754, 'eval_runtime': 3.4632, 'eval_samples_per_second': 288.751, 'eval_steps_per_second': 18.191, 'epoch': 0.52}
{'loss': 0.8819, 'grad_norm': 0.7005398273468018, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7374339699745178, 'eval_runtime': 3.4626, 'eval_samples_per_second': 288.804, 'eval_steps_per_second': 18.195, 'epoch': 0.56}
{'loss': 0.8629, 'grad_norm': 0.4113231897354126, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7188113927841187, 'eval_runtime': 3.4634, 'eval_samples_per_second': 288.733, 'eval_steps_per_second': 18.19, 'epoch': 0.6}
{'loss': 0.8441, 'grad_norm': 0.45946523547172546, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7209463715553284, 'eval_runtime': 3.4606, 'eval_samples_per_second': 288.964, 'eval_steps_per_second': 18.205, 'epoch': 0.64}
{'loss': 0.8322, 'grad_norm': 0.541481614112854, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6990727782249451, 'eval_runtime': 3.4671, 'eval_samples_per_second': 288.424, 'eval_steps_per_second': 18.171, 'epoch': 0.68}
{'loss': 0.8088, 'grad_norm': 0.4754267930984497, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7075947523117065, 'eval_runtime': 3.4836, 'eval_samples_per_second': 287.06, 'eval_steps_per_second': 18.085, 'epoch': 0.72}
{'loss': 0.8174, 'grad_norm': 0.5227854251861572, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6907093524932861, 'eval_runtime': 3.481, 'eval_samples_per_second': 287.278, 'eval_steps_per_second': 18.098, 'epoch': 0.76}
{'loss': 0.8177, 'grad_norm': 0.5271680355072021, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.69974285364151, 'eval_runtime': 3.486, 'eval_samples_per_second': 286.861, 'eval_steps_per_second': 18.072, 'epoch': 0.8}
{'loss': 0.771, 'grad_norm': 0.5272728800773621, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6830554008483887, 'eval_runtime': 3.4857, 'eval_samples_per_second': 286.89, 'eval_steps_per_second': 18.074, 'epoch': 0.84}
{'loss': 0.782, 'grad_norm': 0.597776472568512, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6800461411476135, 'eval_runtime': 3.4831, 'eval_samples_per_second': 287.104, 'eval_steps_per_second': 18.088, 'epoch': 0.88}
{'loss': 0.7319, 'grad_norm': 0.6847007274627686, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6747515797615051, 'eval_runtime': 3.4811, 'eval_samples_per_second': 287.262, 'eval_steps_per_second': 18.098, 'epoch': 0.92}
{'loss': 0.7853, 'grad_norm': 0.5436149835586548, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6768603324890137, 'eval_runtime': 3.4992, 'eval_samples_per_second': 285.778, 'eval_steps_per_second': 18.004, 'epoch': 0.96}
{'loss': 0.7848, 'grad_norm': 0.8580452799797058, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6772779822349548, 'eval_runtime': 3.5107, 'eval_samples_per_second': 284.846, 'eval_steps_per_second': 17.945, 'epoch': 1.0}
{'train_runtime': 262.6475, 'train_samples_per_second': 38.059, 'train_steps_per_second': 2.38, 'train_loss': 0.9940394927978515, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8780912160873413, 1.1420425176620483, 1.006505012512207, 0.9077023863792419, 0.8770560026168823, 0.8251107335090637, 0.8067312240600586, 0.7810920476913452, 0.7814004421234131, 0.7524638175964355, 0.7439793348312378, 0.730326235294342, 0.7443183064460754, 0.7374339699745178, 0.7188113927841187, 0.7209463715553284, 0.6990727782249451, 0.7075947523117065, 0.6907093524932861, 0.69974285364151, 0.6830554008483887, 0.6800461411476135, 0.6747515797615051, 0.6768603324890137, 0.6772779822349548], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8780912160873413, 1.1420425176620483, 1.006505012512207, 0.9077023863792419, 0.8770560026168823, 0.8251107335090637, 0.8067312240600586, 0.7810920476913452, 0.7814004421234131, 0.7524638175964355, 0.7439793348312378, 0.730326235294342, 0.7443183064460754, 0.7374339699745178, 0.7188113927841187, 0.7209463715553284, 0.6990727782249451, 0.7075947523117065, 0.6907093524932861, 0.69974285364151, 0.6830554008483887, 0.6800461411476135, 0.6747515797615051, 0.6768603324890137, 0.6772779822349548]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1269922256469727
current iteration best possible eval_loss (full train run):  -0.6772779822349548
max eval_loss so far:  -0.22662609815597534
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.0841 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.922027467354949
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.9220274666285855
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.9221766979182855
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.929149282020407
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.9220274666286972
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0667, dtype=torch.float64), tensor(0.0601, dtype=torch.float64), tensor(0.1745, dtype=torch.float64), tensor(0.0103, dtype=torch.float64), tensor(0.6832, dtype=torch.float64), 0, 0, 0, 18, 1, 0, 1, 1, 1, 2, 4.689311273921698e-19, 33.35421995035911, 1]
normalized proposed parameters for next round by BO: [tensor(2.1757e-18, dtype=torch.float64), tensor(0.0667, dtype=torch.float64), tensor(0.0601, dtype=torch.float64), tensor(0.1745, dtype=torch.float64), tensor(0.0103, dtype=torch.float64), tensor(0.6832, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.3243e-18, dtype=torch.float64), tensor(0.0052, dtype=torch.float64), tensor(0.5603, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(4.6893e-18, dtype=torch.float64), tensor(0.6949, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.067
  rowan_hellaswag: 0.06
  sciq: 0.175
  triviaqa: 0.01
  truthfulqa_gen: 0.683
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (4.689311273921698e-19,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (33.35421995035911,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  2
lora dropout:  4.689311273921698e-19
lora alpha:  33.35421995035911
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,285,568 || all params: 8,032,546,816 || trainable%: 0.0285
length of training data:  9946
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.162, 'grad_norm': 6.310606002807617, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4217195510864258, 'eval_runtime': 3.6787, 'eval_samples_per_second': 271.834, 'eval_steps_per_second': 17.126, 'epoch': 0.04}
{'loss': 1.2694, 'grad_norm': 3.4586949348449707, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.8894383311271667, 'eval_runtime': 3.6437, 'eval_samples_per_second': 274.443, 'eval_steps_per_second': 17.29, 'epoch': 0.08}
{'loss': 1.0314, 'grad_norm': 2.4824609756469727, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 0.7001793384552002, 'eval_runtime': 3.6501, 'eval_samples_per_second': 273.969, 'eval_steps_per_second': 17.26, 'epoch': 0.12}
{'loss': 0.9469, 'grad_norm': 2.649139404296875, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 0.6101783514022827, 'eval_runtime': 3.6255, 'eval_samples_per_second': 275.82, 'eval_steps_per_second': 17.377, 'epoch': 0.16}
{'loss': 0.9233, 'grad_norm': 2.261201858520508, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 0.557164192199707, 'eval_runtime': 3.6596, 'eval_samples_per_second': 273.254, 'eval_steps_per_second': 17.215, 'epoch': 0.2}
{'loss': 0.8464, 'grad_norm': 2.0797109603881836, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 0.48811981081962585, 'eval_runtime': 3.6478, 'eval_samples_per_second': 274.136, 'eval_steps_per_second': 17.271, 'epoch': 0.24}
{'loss': 0.8757, 'grad_norm': 2.2192487716674805, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 0.4491543471813202, 'eval_runtime': 3.6347, 'eval_samples_per_second': 275.123, 'eval_steps_per_second': 17.333, 'epoch': 0.28}
{'loss': 0.7773, 'grad_norm': 2.471609354019165, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 0.4039672613143921, 'eval_runtime': 3.6499, 'eval_samples_per_second': 273.979, 'eval_steps_per_second': 17.261, 'epoch': 0.32}
{'loss': 0.7831, 'grad_norm': 2.301809787750244, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 0.3805874288082123, 'eval_runtime': 3.6697, 'eval_samples_per_second': 272.503, 'eval_steps_per_second': 17.168, 'epoch': 0.36}
{'loss': 0.7313, 'grad_norm': 2.031337022781372, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 0.3519504964351654, 'eval_runtime': 3.6442, 'eval_samples_per_second': 274.407, 'eval_steps_per_second': 17.288, 'epoch': 0.4}
{'loss': 0.8495, 'grad_norm': 2.870948314666748, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 0.3135688304901123, 'eval_runtime': 3.6424, 'eval_samples_per_second': 274.546, 'eval_steps_per_second': 17.296, 'epoch': 0.44}
{'loss': 0.719, 'grad_norm': 1.6167163848876953, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 0.29175010323524475, 'eval_runtime': 3.6382, 'eval_samples_per_second': 274.859, 'eval_steps_per_second': 17.316, 'epoch': 0.48}
{'loss': 0.6758, 'grad_norm': 1.9129769802093506, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 0.27447518706321716, 'eval_runtime': 3.6408, 'eval_samples_per_second': 274.662, 'eval_steps_per_second': 17.304, 'epoch': 0.52}
{'loss': 0.6566, 'grad_norm': 2.6045846939086914, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 0.25726887583732605, 'eval_runtime': 3.644, 'eval_samples_per_second': 274.423, 'eval_steps_per_second': 17.289, 'epoch': 0.56}
{'loss': 0.606, 'grad_norm': 1.8424774408340454, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 0.2354188710451126, 'eval_runtime': 3.6434, 'eval_samples_per_second': 274.469, 'eval_steps_per_second': 17.292, 'epoch': 0.6}
{'loss': 0.683, 'grad_norm': 1.9818607568740845, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 0.22688256204128265, 'eval_runtime': 3.646, 'eval_samples_per_second': 274.277, 'eval_steps_per_second': 17.279, 'epoch': 0.64}
{'loss': 0.6629, 'grad_norm': 2.0538485050201416, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 0.21217156946659088, 'eval_runtime': 3.6417, 'eval_samples_per_second': 274.596, 'eval_steps_per_second': 17.3, 'epoch': 0.68}
{'loss': 0.5533, 'grad_norm': 2.0175364017486572, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 0.19834494590759277, 'eval_runtime': 3.6452, 'eval_samples_per_second': 274.332, 'eval_steps_per_second': 17.283, 'epoch': 0.72}
{'loss': 0.5699, 'grad_norm': 2.503938674926758, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 0.18349124491214752, 'eval_runtime': 3.6458, 'eval_samples_per_second': 274.29, 'eval_steps_per_second': 17.28, 'epoch': 0.76}
{'loss': 0.576, 'grad_norm': 1.6556451320648193, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.17937569320201874, 'eval_runtime': 3.6429, 'eval_samples_per_second': 274.505, 'eval_steps_per_second': 17.294, 'epoch': 0.8}
{'loss': 0.5294, 'grad_norm': 1.5984927415847778, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 0.17402954399585724, 'eval_runtime': 3.6421, 'eval_samples_per_second': 274.564, 'eval_steps_per_second': 17.298, 'epoch': 0.84}
{'loss': 0.6729, 'grad_norm': 1.7040115594863892, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.17062585055828094, 'eval_runtime': 3.6436, 'eval_samples_per_second': 274.454, 'eval_steps_per_second': 17.291, 'epoch': 0.88}
{'loss': 0.6566, 'grad_norm': 1.4340401887893677, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.16537420451641083, 'eval_runtime': 3.6379, 'eval_samples_per_second': 274.886, 'eval_steps_per_second': 17.318, 'epoch': 0.92}
{'loss': 0.6293, 'grad_norm': 1.5424082279205322, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.16279813647270203, 'eval_runtime': 3.6403, 'eval_samples_per_second': 274.705, 'eval_steps_per_second': 17.306, 'epoch': 0.96}
{'train_runtime': 281.4184, 'train_samples_per_second': 35.342, 'train_steps_per_second': 2.21, 'train_loss': 0.8410752486569322, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4217195510864258, 0.8894383311271667, 0.7001793384552002, 0.6101783514022827, 0.557164192199707, 0.48811981081962585, 0.4491543471813202, 0.4039672613143921, 0.3805874288082123, 0.3519504964351654, 0.3135688304901123, 0.29175010323524475, 0.27447518706321716, 0.25726887583732605, 0.2354188710451126, 0.22688256204128265, 0.21217156946659088, 0.19834494590759277, 0.18349124491214752, 0.17937569320201874, 0.17402954399585724, 0.17062585055828094, 0.16537420451641083, 0.16279813647270203], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.4217195510864258, 0.8894383311271667, 0.7001793384552002, 0.6101783514022827, 0.557164192199707, 0.48811981081962585, 0.4491543471813202, 0.4039672613143921, 0.3805874288082123, 0.3519504964351654, 0.3135688304901123, 0.29175010323524475, 0.27447518706321716, 0.25726887583732605, 0.2354188710451126, 0.22688256204128265, 0.21217156946659088, 0.19834494590759277, 0.18349124491214752, 0.17937569320201874, 0.17402954399585724, 0.17062585055828094, 0.16537420451641083, 0.16279813647270203]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.16279813647270203
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.9320 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.052142441272735596, 0.27593737840652466, 0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.5177018046379089, 0.24106496572494507, 0.0689818263053894, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8632487058639526, 0.4703384041786194, 0.46800002455711365, 0.3034648299217224]  ‚Üí  acq = -0.9210199153122067
X = [0.880847156047821, 0.5330603718757629, 0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.06467144191265106, 0.14988404512405396, 0.9737263321876526, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.48149335384368896, 0.03980189561843872, 0.8312453031539917, 0.0066245198249816895]  ‚Üí  acq = -0.921024796886726
X = [0.2649468183517456, 0.9490264058113098, 0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7666076421737671, 0.9232513308525085, 0.11054229736328125, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.7048742771148682, 0.8974609375, 0.819237232208252, 0.9298104643821716]  ‚Üí  acq = -0.9210200921483049
X = [0.34928226470947266, 0.9340886473655701, 0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.7950482368469238, 0.9106844663619995, 0.8419823050498962, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7994585037231445, 0.8544618487358093, 0.520532488822937, 0.9426186680793762]  ‚Üí  acq = -0.9210201355183387
X = [0.8286707401275635, 0.7412276268005371, 0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8989940881729126, 0.1521429419517517, 0.3317450284957886, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3382951021194458, 0.8489412665367126, 0.4699510335922241, 0.4256795048713684]  ‚Üí  acq = -0.9201252427438806
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3001, dtype=torch.float64), tensor(0.1267, dtype=torch.float64), 0, tensor(0.1072, dtype=torch.float64), tensor(0.3997, dtype=torch.float64), 0, 0, tensor(0.0639, dtype=torch.float64), 16, 1, 1, 1, 0, 1, 128, 0.005118820700197063, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(2.0838e-17, dtype=torch.float64), tensor(0.3001, dtype=torch.float64), tensor(0.1267, dtype=torch.float64), tensor(0.0023, dtype=torch.float64), tensor(0.1072, dtype=torch.float64), tensor(0.3997, dtype=torch.float64), tensor(9.9174e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0639, dtype=torch.float64), tensor(0.4895, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0512, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.3
  rowan_hellaswag: 0.127
  sciq: 0
  triviaqa: 0.107
  truthfulqa_gen: 0.4
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.064

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.005118820700197063,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.005118820700197063
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 102,760,448 || all params: 8,133,021,696 || trainable%: 1.2635
length of training data:  9975
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6003, 'grad_norm': 0.22047212719917297, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.8409061431884766, 'eval_runtime': 3.4408, 'eval_samples_per_second': 290.627, 'eval_steps_per_second': 18.309, 'epoch': 0.04}
{'loss': 2.3528, 'grad_norm': 0.07450038939714432, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.8642305135726929, 'eval_runtime': 3.4362, 'eval_samples_per_second': 291.017, 'eval_steps_per_second': 18.334, 'epoch': 0.08}
{'loss': 1.8007, 'grad_norm': 0.07762603461742401, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.562232494354248, 'eval_runtime': 3.4253, 'eval_samples_per_second': 291.942, 'eval_steps_per_second': 18.392, 'epoch': 0.12}
{'loss': 1.5407, 'grad_norm': 0.06890483945608139, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.4122675657272339, 'eval_runtime': 3.4311, 'eval_samples_per_second': 291.45, 'eval_steps_per_second': 18.361, 'epoch': 0.16}
{'loss': 1.4211, 'grad_norm': 0.05907130241394043, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.3230276107788086, 'eval_runtime': 3.4345, 'eval_samples_per_second': 291.16, 'eval_steps_per_second': 18.343, 'epoch': 0.2}
{'loss': 1.3656, 'grad_norm': 0.07729487121105194, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 1.1951627731323242, 'eval_runtime': 3.4438, 'eval_samples_per_second': 290.374, 'eval_steps_per_second': 18.294, 'epoch': 0.24}
{'loss': 1.3363, 'grad_norm': 0.06836887449026108, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 1.1535584926605225, 'eval_runtime': 3.4514, 'eval_samples_per_second': 289.736, 'eval_steps_per_second': 18.253, 'epoch': 0.28}
{'loss': 1.3171, 'grad_norm': 0.07411830127239227, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 1.135998010635376, 'eval_runtime': 3.4509, 'eval_samples_per_second': 289.779, 'eval_steps_per_second': 18.256, 'epoch': 0.32}
{'loss': 1.3036, 'grad_norm': 0.055274877697229385, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 1.1121165752410889, 'eval_runtime': 3.4718, 'eval_samples_per_second': 288.031, 'eval_steps_per_second': 18.146, 'epoch': 0.36}
{'loss': 1.2584, 'grad_norm': 0.05543117597699165, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 1.0943974256515503, 'eval_runtime': 3.4774, 'eval_samples_per_second': 287.574, 'eval_steps_per_second': 18.117, 'epoch': 0.4}
{'loss': 1.2678, 'grad_norm': 0.06118232384324074, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 1.082680344581604, 'eval_runtime': 3.475, 'eval_samples_per_second': 287.768, 'eval_steps_per_second': 18.129, 'epoch': 0.44}
{'loss': 1.2783, 'grad_norm': 0.059984494000673294, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 1.067144513130188, 'eval_runtime': 3.5004, 'eval_samples_per_second': 285.683, 'eval_steps_per_second': 17.998, 'epoch': 0.48}
{'loss': 1.2549, 'grad_norm': 0.07551828026771545, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 1.0457123517990112, 'eval_runtime': 3.5263, 'eval_samples_per_second': 283.583, 'eval_steps_per_second': 17.866, 'epoch': 0.52}
{'loss': 1.1862, 'grad_norm': 0.05395359918475151, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 1.0210763216018677, 'eval_runtime': 3.5045, 'eval_samples_per_second': 285.344, 'eval_steps_per_second': 17.977, 'epoch': 0.56}
{'loss': 1.1816, 'grad_norm': 0.058609411120414734, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.9714738726615906, 'eval_runtime': 3.514, 'eval_samples_per_second': 284.576, 'eval_steps_per_second': 17.928, 'epoch': 0.6}
{'loss': 1.184, 'grad_norm': 0.07602764666080475, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.9284480810165405, 'eval_runtime': 3.5112, 'eval_samples_per_second': 284.803, 'eval_steps_per_second': 17.943, 'epoch': 0.64}
{'loss': 1.1463, 'grad_norm': 0.06128265708684921, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.8967893123626709, 'eval_runtime': 3.4987, 'eval_samples_per_second': 285.817, 'eval_steps_per_second': 18.006, 'epoch': 0.68}
{'loss': 1.1534, 'grad_norm': 0.06731157749891281, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.875272810459137, 'eval_runtime': 3.4845, 'eval_samples_per_second': 286.989, 'eval_steps_per_second': 18.08, 'epoch': 0.72}
{'loss': 1.0845, 'grad_norm': 0.059239715337753296, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.8709600567817688, 'eval_runtime': 3.4802, 'eval_samples_per_second': 287.341, 'eval_steps_per_second': 18.103, 'epoch': 0.76}
{'loss': 1.1237, 'grad_norm': 0.06121581792831421, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.8616703152656555, 'eval_runtime': 3.5088, 'eval_samples_per_second': 285.002, 'eval_steps_per_second': 17.955, 'epoch': 0.8}
{'loss': 1.1698, 'grad_norm': 0.06754080206155777, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.8564522862434387, 'eval_runtime': 3.4941, 'eval_samples_per_second': 286.199, 'eval_steps_per_second': 18.031, 'epoch': 0.84}
{'loss': 1.1002, 'grad_norm': 0.061007190495729446, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.8518373966217041, 'eval_runtime': 3.4983, 'eval_samples_per_second': 285.849, 'eval_steps_per_second': 18.008, 'epoch': 0.88}
{'loss': 1.1262, 'grad_norm': 0.06942884624004364, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.852425754070282, 'eval_runtime': 3.4995, 'eval_samples_per_second': 285.755, 'eval_steps_per_second': 18.003, 'epoch': 0.92}
{'loss': 1.1778, 'grad_norm': 0.08117993921041489, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.8489246964454651, 'eval_runtime': 3.4742, 'eval_samples_per_second': 287.839, 'eval_steps_per_second': 18.134, 'epoch': 0.96}
{'train_runtime': 297.5285, 'train_samples_per_second': 33.526, 'train_steps_per_second': 2.097, 'train_loss': 1.3960567743350298, 'epoch': 1.0}
train_results:  {'eval_loss': [3.8409061431884766, 1.8642305135726929, 1.562232494354248, 1.4122675657272339, 1.3230276107788086, 1.1951627731323242, 1.1535584926605225, 1.135998010635376, 1.1121165752410889, 1.0943974256515503, 1.082680344581604, 1.067144513130188, 1.0457123517990112, 1.0210763216018677, 0.9714738726615906, 0.9284480810165405, 0.8967893123626709, 0.875272810459137, 0.8709600567817688, 0.8616703152656555, 0.8564522862434387, 0.8518373966217041, 0.852425754070282, 0.8489246964454651], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [3.8409061431884766, 1.8642305135726929, 1.562232494354248, 1.4122675657272339, 1.3230276107788086, 1.1951627731323242, 1.1535584926605225, 1.135998010635376, 1.1121165752410889, 1.0943974256515503, 1.082680344581604, 1.067144513130188, 1.0457123517990112, 1.0210763216018677, 0.9714738726615906, 0.9284480810165405, 0.8967893123626709, 0.875272810459137, 0.8709600567817688, 0.8616703152656555, 0.8564522862434387, 0.8518373966217041, 0.852425754070282, 0.8489246964454651]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3735257387161255
current iteration best possible eval_loss (full train run):  -0.8489246964454651
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4904 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1560913324356079, 0.29771536588668823, 0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.4007338583469391, 0.8963236808776855, 0.5591384768486023, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.42323532700538635, 0.003319978713989258, 0.666995644569397, 0.6627236008644104]  ‚Üí  acq = -1.0245121964940758
X = [0.6140679717063904, 0.7597888708114624, 0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.5511543154716492, 0.10114860534667969, 0.46051692962646484, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6117270588874817, 0.06090492010116577, 0.29019129276275635, 0.544792890548706]  ‚Üí  acq = -1.0245121964935175
X = [0.4161165952682495, 0.6534545421600342, 0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9124553799629211, 0.5554662346839905, 0.09747803211212158, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9407435655593872, 0.9420399069786072, 0.717787504196167, 0.5669505000114441]  ‚Üí  acq = -1.0245121964934236
X = [0.11413401365280151, 0.7658460736274719, 0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.20504699647426605, 0.40315020084381104, 0.9066953659057617, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7133153080940247, 0.3376033306121826, 0.9497083425521851, 0.04203289747238159]  ‚Üí  acq = -1.0245122242929137
X = [0.7649766802787781, 0.17325276136398315, 0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7676264643669128, 0.04699522256851196, 0.9299392104148865, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.02683671936392784, 0.7503892779350281, 0.20525000989437103, 0.7006362080574036]  ‚Üí  acq = -1.024512196562796
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2044, dtype=torch.float64), tensor(0.0423, dtype=torch.float64), tensor(0.3118, dtype=torch.float64), 0, tensor(0.1421, dtype=torch.float64), tensor(0.0875, dtype=torch.float64), tensor(0.1226, dtype=torch.float64), tensor(0.0812, dtype=torch.float64), 18, 0, 1, 0, 0, 1, 50, 7.121334955216545e-20, 34.600233411904355, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2044, dtype=torch.float64), tensor(0.0423, dtype=torch.float64), tensor(0.3118, dtype=torch.float64), tensor(0.0082, dtype=torch.float64), tensor(0.1421, dtype=torch.float64), tensor(0.0875, dtype=torch.float64), tensor(0.1226, dtype=torch.float64), tensor(0.0812, dtype=torch.float64), tensor(0.5691, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3918, dtype=torch.float64), tensor(7.1213e-19, dtype=torch.float64), tensor(0.7208, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.204
  rowan_hellaswag: 0.042
  sciq: 0.312
  triviaqa: 0
  truthfulqa_gen: 0.142
  wikitext: 0.087
  mmlu: 0.123
  arc_challenge: 0.081

LoRA Parameters:
  lora_r: (50,)
  lora_dropout: (7.121334955216545e-20,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (34.600233411904355,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  50
lora dropout:  7.121334955216545e-20
lora alpha:  34.600233411904355
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 21,196,800 || all params: 8,051,458,048 || trainable%: 0.2633
length of training data:  9914
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9352, 'grad_norm': 0.9048014283180237, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9190095663070679, 'eval_runtime': 3.2935, 'eval_samples_per_second': 303.628, 'eval_steps_per_second': 19.129, 'epoch': 0.04}
{'loss': 1.7137, 'grad_norm': 0.4160677492618561, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.264009714126587, 'eval_runtime': 3.3036, 'eval_samples_per_second': 302.696, 'eval_steps_per_second': 19.07, 'epoch': 0.08}
{'loss': 1.3876, 'grad_norm': 0.3834365904331207, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.2024565935134888, 'eval_runtime': 3.2825, 'eval_samples_per_second': 304.644, 'eval_steps_per_second': 19.193, 'epoch': 0.12}
{'loss': 1.2945, 'grad_norm': 0.46173909306526184, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.0626535415649414, 'eval_runtime': 3.3013, 'eval_samples_per_second': 302.907, 'eval_steps_per_second': 19.083, 'epoch': 0.16}
{'loss': 1.2263, 'grad_norm': 0.33770906925201416, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.993565559387207, 'eval_runtime': 3.3056, 'eval_samples_per_second': 302.513, 'eval_steps_per_second': 19.058, 'epoch': 0.2}
{'loss': 1.1762, 'grad_norm': 0.4312214255332947, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.94659423828125, 'eval_runtime': 3.3051, 'eval_samples_per_second': 302.565, 'eval_steps_per_second': 19.062, 'epoch': 0.24}
{'loss': 1.1076, 'grad_norm': 0.25441503524780273, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.9309156537055969, 'eval_runtime': 3.3058, 'eval_samples_per_second': 302.499, 'eval_steps_per_second': 19.057, 'epoch': 0.28}
{'loss': 1.1326, 'grad_norm': 0.3103223741054535, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.9063979387283325, 'eval_runtime': 3.3064, 'eval_samples_per_second': 302.441, 'eval_steps_per_second': 19.054, 'epoch': 0.32}
{'loss': 1.1815, 'grad_norm': 0.336875319480896, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.8932520747184753, 'eval_runtime': 3.3104, 'eval_samples_per_second': 302.082, 'eval_steps_per_second': 19.031, 'epoch': 0.36}
{'loss': 1.1082, 'grad_norm': 0.2761092483997345, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.867215096950531, 'eval_runtime': 3.3239, 'eval_samples_per_second': 300.85, 'eval_steps_per_second': 18.954, 'epoch': 0.4}
{'loss': 1.1483, 'grad_norm': 0.358216792345047, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.8526642322540283, 'eval_runtime': 3.3133, 'eval_samples_per_second': 301.812, 'eval_steps_per_second': 19.014, 'epoch': 0.44}
{'loss': 1.1352, 'grad_norm': 0.3232855498790741, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.8305293321609497, 'eval_runtime': 3.3171, 'eval_samples_per_second': 301.469, 'eval_steps_per_second': 18.993, 'epoch': 0.48}
{'loss': 1.1057, 'grad_norm': 0.3773062527179718, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.8107329607009888, 'eval_runtime': 3.3246, 'eval_samples_per_second': 300.787, 'eval_steps_per_second': 18.95, 'epoch': 0.52}
{'loss': 1.1001, 'grad_norm': 0.3562977612018585, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.7801423072814941, 'eval_runtime': 3.3187, 'eval_samples_per_second': 301.32, 'eval_steps_per_second': 18.983, 'epoch': 0.56}
{'loss': 1.0899, 'grad_norm': 0.282605916261673, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.7427963614463806, 'eval_runtime': 3.3184, 'eval_samples_per_second': 301.346, 'eval_steps_per_second': 18.985, 'epoch': 0.6}
{'loss': 1.052, 'grad_norm': 0.2804993987083435, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.6942152380943298, 'eval_runtime': 3.3137, 'eval_samples_per_second': 301.779, 'eval_steps_per_second': 19.012, 'epoch': 0.65}
{'loss': 1.044, 'grad_norm': 0.4150451719760895, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.6780874729156494, 'eval_runtime': 3.3067, 'eval_samples_per_second': 302.419, 'eval_steps_per_second': 19.052, 'epoch': 0.69}
{'loss': 1.0595, 'grad_norm': 0.3221918046474457, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.6636010408401489, 'eval_runtime': 3.3, 'eval_samples_per_second': 303.027, 'eval_steps_per_second': 19.091, 'epoch': 0.73}
{'loss': 1.079, 'grad_norm': 0.35543495416641235, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.6529123783111572, 'eval_runtime': 3.3024, 'eval_samples_per_second': 302.814, 'eval_steps_per_second': 19.077, 'epoch': 0.77}
{'loss': 1.0877, 'grad_norm': 0.3735496699810028, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.6513170599937439, 'eval_runtime': 3.2997, 'eval_samples_per_second': 303.062, 'eval_steps_per_second': 19.093, 'epoch': 0.81}
{'loss': 1.0092, 'grad_norm': 0.2854868471622467, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.6409735679626465, 'eval_runtime': 3.3096, 'eval_samples_per_second': 302.153, 'eval_steps_per_second': 19.036, 'epoch': 0.85}
{'loss': 1.0861, 'grad_norm': 0.3002784252166748, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.6361519694328308, 'eval_runtime': 3.327, 'eval_samples_per_second': 300.574, 'eval_steps_per_second': 18.936, 'epoch': 0.89}
{'loss': 1.0487, 'grad_norm': 0.38594338297843933, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.63554847240448, 'eval_runtime': 3.3317, 'eval_samples_per_second': 300.149, 'eval_steps_per_second': 18.909, 'epoch': 0.93}
{'loss': 1.0459, 'grad_norm': 0.3005278408527374, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.6306856870651245, 'eval_runtime': 3.3153, 'eval_samples_per_second': 301.636, 'eval_steps_per_second': 19.003, 'epoch': 0.97}
{'train_runtime': 276.0396, 'train_samples_per_second': 35.915, 'train_steps_per_second': 2.246, 'train_loss': 1.2159160860123173, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9190095663070679, 1.264009714126587, 1.2024565935134888, 1.0626535415649414, 0.993565559387207, 0.94659423828125, 0.9309156537055969, 0.9063979387283325, 0.8932520747184753, 0.867215096950531, 0.8526642322540283, 0.8305293321609497, 0.8107329607009888, 0.7801423072814941, 0.7427963614463806, 0.6942152380943298, 0.6780874729156494, 0.6636010408401489, 0.6529123783111572, 0.6513170599937439, 0.6409735679626465, 0.6361519694328308, 0.63554847240448, 0.6306856870651245], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9190095663070679, 1.264009714126587, 1.2024565935134888, 1.0626535415649414, 0.993565559387207, 0.94659423828125, 0.9309156537055969, 0.9063979387283325, 0.8932520747184753, 0.867215096950531, 0.8526642322540283, 0.8305293321609497, 0.8107329607009888, 0.7801423072814941, 0.7427963614463806, 0.6942152380943298, 0.6780874729156494, 0.6636010408401489, 0.6529123783111572, 0.6513170599937439, 0.6409735679626465, 0.6361519694328308, 0.63554847240448, 0.6306856870651245]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1262141466140747
current iteration best possible eval_loss (full train run):  -0.6306856870651245
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 23.4246 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5173616409301758, 0.9290173649787903, 0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8086898326873779, 0.40152454376220703, 0.7644284963607788, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.11481685936450958, 0.6888900399208069, 0.67311692237854, 0.032737672328948975]  ‚Üí  acq = -0.9833901017248341
X = [0.24963438510894775, 0.8990642428398132, 0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8877483606338501, 0.9734378457069397, 0.9886022210121155, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.3387552499771118, 0.028142869472503662, 0.04686132073402405, 0.12386679649353027]  ‚Üí  acq = -0.9833901017112162
X = [0.8100742101669312, 0.591465413570404, 0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.45014575123786926, 0.044465720653533936, 0.6919609904289246, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.3413050174713135, 0.471097469329834, 0.6840248107910156, 0.8633646368980408]  ‚Üí  acq = -0.9833901017248341
X = [0.565035343170166, 0.14758515357971191, 0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8177289366722107, 0.8848571181297302, 0.35672861337661743, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.5902637243270874, 0.3791559934616089, 0.7795722484588623, 0.572867214679718]  ‚Üí  acq = -0.9833901017248341
X = [0.9166635870933533, 0.9858017563819885, 0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9887644648551941, 0.9247068166732788, 0.6096784472465515, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.931157112121582, 0.6287887096405029, 0.3411646783351898, 0.1318853497505188]  ‚Üí  acq = -0.9833778586573385
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0251, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), tensor(0.1235, dtype=torch.float64), tensor(0.2046, dtype=torch.float64), tensor(0.2882, dtype=torch.float64), tensor(0.0490, dtype=torch.float64), tensor(0.0238, dtype=torch.float64), tensor(0.2459, dtype=torch.float64), 21, 1, 1, 0, 1, 1, 128, 5.1573515439894996e-20, 42.12371485967695, 1]
normalized proposed parameters for next round by BO: [tensor(1.4876e-18, dtype=torch.float64), tensor(0.0251, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), tensor(0.1235, dtype=torch.float64), tensor(0.2046, dtype=torch.float64), tensor(0.2882, dtype=torch.float64), tensor(0.0490, dtype=torch.float64), tensor(0.0238, dtype=torch.float64), tensor(0.2459, dtype=torch.float64), tensor(0.6470, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(5.1574e-19, dtype=torch.float64), tensor(0.8776, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.025
  rowan_hellaswag: 0.04
  sciq: 0.123
  triviaqa: 0.205
  truthfulqa_gen: 0.288
  wikitext: 0.049
  mmlu: 0.024
  arc_challenge: 0.246

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.1573515439894996e-20,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (42.12371485967695,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  128
lora dropout:  5.1573515439894996e-20
lora alpha:  42.12371485967695
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 134,873,088 || all params: 8,165,134,336 || trainable%: 1.6518
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0818, 'grad_norm': 0.6663162708282471, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4597175121307373, 'eval_runtime': 3.5887, 'eval_samples_per_second': 278.649, 'eval_steps_per_second': 17.555, 'epoch': 0.04}
{'loss': 1.3436, 'grad_norm': 0.571431040763855, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9676318168640137, 'eval_runtime': 3.5885, 'eval_samples_per_second': 278.665, 'eval_steps_per_second': 17.556, 'epoch': 0.08}
{'loss': 1.2009, 'grad_norm': 0.40673699975013733, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8490203619003296, 'eval_runtime': 3.5797, 'eval_samples_per_second': 279.356, 'eval_steps_per_second': 17.599, 'epoch': 0.12}
{'loss': 1.0663, 'grad_norm': 0.38936492800712585, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7266284227371216, 'eval_runtime': 3.582, 'eval_samples_per_second': 279.171, 'eval_steps_per_second': 17.588, 'epoch': 0.16}
{'loss': 1.0294, 'grad_norm': 0.34431588649749756, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7053185701370239, 'eval_runtime': 3.5827, 'eval_samples_per_second': 279.122, 'eval_steps_per_second': 17.585, 'epoch': 0.2}
{'loss': 0.983, 'grad_norm': 0.23940549790859222, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6662489175796509, 'eval_runtime': 3.5828, 'eval_samples_per_second': 279.112, 'eval_steps_per_second': 17.584, 'epoch': 0.24}
{'loss': 1.0123, 'grad_norm': 0.373567670583725, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6261947154998779, 'eval_runtime': 3.5878, 'eval_samples_per_second': 278.721, 'eval_steps_per_second': 17.559, 'epoch': 0.28}
{'loss': 1.0556, 'grad_norm': 0.34276461601257324, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.611085832118988, 'eval_runtime': 3.589, 'eval_samples_per_second': 278.633, 'eval_steps_per_second': 17.554, 'epoch': 0.32}
{'loss': 1.0166, 'grad_norm': 0.3578305244445801, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5725828409194946, 'eval_runtime': 3.5996, 'eval_samples_per_second': 277.807, 'eval_steps_per_second': 17.502, 'epoch': 0.36}
{'loss': 0.99, 'grad_norm': 0.3794232904911041, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5455679893493652, 'eval_runtime': 3.5945, 'eval_samples_per_second': 278.2, 'eval_steps_per_second': 17.527, 'epoch': 0.4}
{'loss': 0.9541, 'grad_norm': 0.29273948073387146, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5134512782096863, 'eval_runtime': 3.6157, 'eval_samples_per_second': 276.568, 'eval_steps_per_second': 17.424, 'epoch': 0.44}
{'loss': 0.9541, 'grad_norm': 0.28784531354904175, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.49775025248527527, 'eval_runtime': 3.5973, 'eval_samples_per_second': 277.984, 'eval_steps_per_second': 17.513, 'epoch': 0.48}
{'loss': 0.9149, 'grad_norm': 0.31313762068748474, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4784446954727173, 'eval_runtime': 3.6027, 'eval_samples_per_second': 277.572, 'eval_steps_per_second': 17.487, 'epoch': 0.52}
{'loss': 0.9131, 'grad_norm': 0.4406545162200928, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.43960267305374146, 'eval_runtime': 3.5965, 'eval_samples_per_second': 278.05, 'eval_steps_per_second': 17.517, 'epoch': 0.56}
{'loss': 0.8817, 'grad_norm': 0.3724825382232666, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4195697605609894, 'eval_runtime': 3.6211, 'eval_samples_per_second': 276.156, 'eval_steps_per_second': 17.398, 'epoch': 0.6}
{'loss': 0.8738, 'grad_norm': 0.33264297246932983, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.39885807037353516, 'eval_runtime': 3.6204, 'eval_samples_per_second': 276.212, 'eval_steps_per_second': 17.401, 'epoch': 0.64}
{'loss': 0.8372, 'grad_norm': 0.3535601496696472, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3793988525867462, 'eval_runtime': 3.5989, 'eval_samples_per_second': 277.86, 'eval_steps_per_second': 17.505, 'epoch': 0.68}
{'loss': 0.8915, 'grad_norm': 0.4143008291721344, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3692290782928467, 'eval_runtime': 3.5981, 'eval_samples_per_second': 277.928, 'eval_steps_per_second': 17.509, 'epoch': 0.72}
{'loss': 0.81, 'grad_norm': 0.4030388593673706, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3488934338092804, 'eval_runtime': 3.5941, 'eval_samples_per_second': 278.231, 'eval_steps_per_second': 17.529, 'epoch': 0.76}
{'loss': 0.8584, 'grad_norm': 0.3801324963569641, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3309083878993988, 'eval_runtime': 3.6005, 'eval_samples_per_second': 277.737, 'eval_steps_per_second': 17.497, 'epoch': 0.8}
{'loss': 0.8003, 'grad_norm': 0.3914008140563965, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.31209295988082886, 'eval_runtime': 3.5897, 'eval_samples_per_second': 278.573, 'eval_steps_per_second': 17.55, 'epoch': 0.84}
{'loss': 0.8086, 'grad_norm': 0.4516628384590149, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.29759520292282104, 'eval_runtime': 3.5921, 'eval_samples_per_second': 278.392, 'eval_steps_per_second': 17.539, 'epoch': 0.88}
{'loss': 0.8322, 'grad_norm': 0.5735175013542175, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2851225733757019, 'eval_runtime': 3.5949, 'eval_samples_per_second': 278.17, 'eval_steps_per_second': 17.525, 'epoch': 0.92}
{'loss': 0.8263, 'grad_norm': 0.37994319200515747, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.27698421478271484, 'eval_runtime': 3.5974, 'eval_samples_per_second': 277.982, 'eval_steps_per_second': 17.513, 'epoch': 0.96}
{'loss': 0.8096, 'grad_norm': 0.3155917227268219, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2746446132659912, 'eval_runtime': 3.5847, 'eval_samples_per_second': 278.964, 'eval_steps_per_second': 17.575, 'epoch': 1.0}
{'train_runtime': 279.1841, 'train_samples_per_second': 35.804, 'train_steps_per_second': 2.239, 'train_loss': 1.0297991149902344, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4597175121307373, 0.9676318168640137, 0.8490203619003296, 0.7266284227371216, 0.7053185701370239, 0.6662489175796509, 0.6261947154998779, 0.611085832118988, 0.5725828409194946, 0.5455679893493652, 0.5134512782096863, 0.49775025248527527, 0.4784446954727173, 0.43960267305374146, 0.4195697605609894, 0.39885807037353516, 0.3793988525867462, 0.3692290782928467, 0.3488934338092804, 0.3309083878993988, 0.31209295988082886, 0.29759520292282104, 0.2851225733757019, 0.27698421478271484, 0.2746446132659912], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4597175121307373, 0.9676318168640137, 0.8490203619003296, 0.7266284227371216, 0.7053185701370239, 0.6662489175796509, 0.6261947154998779, 0.611085832118988, 0.5725828409194946, 0.5455679893493652, 0.5134512782096863, 0.49775025248527527, 0.4784446954727173, 0.43960267305374146, 0.4195697605609894, 0.39885807037353516, 0.3793988525867462, 0.3692290782928467, 0.3488934338092804, 0.3309083878993988, 0.31209295988082886, 0.29759520292282104, 0.2851225733757019, 0.27698421478271484, 0.2746446132659912]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1228439807891846
current iteration best possible eval_loss (full train run):  -0.2746446132659912
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.0182 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -1.0028450426211473
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -1.0028457220177605
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -1.0028460899294267
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -1.002845042622087
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -1.0028450426221582
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0814, dtype=torch.float64), tensor(0.4878, dtype=torch.float64), 0, tensor(0.2052, dtype=torch.float64), tensor(0.0326, dtype=torch.float64), tensor(0.1863, dtype=torch.float64), 0, 15, 1, 1, 1, 1, 1, 9, 2.168404344971009e-19, 31.298170553890856, 1]
normalized proposed parameters for next round by BO: [tensor(7.5700e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0814, dtype=torch.float64), tensor(0.4878, dtype=torch.float64), tensor(0.0067, dtype=torch.float64), tensor(0.2052, dtype=torch.float64), tensor(0.0326, dtype=torch.float64), tensor(0.1863, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4760, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0678, dtype=torch.float64), tensor(2.1684e-18, dtype=torch.float64), tensor(0.6520, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.081
  sciq: 0.488
  triviaqa: 0
  truthfulqa_gen: 0.205
  wikitext: 0.033
  mmlu: 0.186
  arc_challenge: 0

LoRA Parameters:
  lora_r: (9,)
  lora_dropout: (2.168404344971009e-19,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (31.298170553890856,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  9
lora dropout:  2.168404344971009e-19
lora alpha:  31.298170553890856
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 9,262,080 || all params: 8,039,523,328 || trainable%: 0.1152
length of training data:  9931
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3239, 'grad_norm': 4.290018558502197, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5960549116134644, 'eval_runtime': 3.6687, 'eval_samples_per_second': 272.578, 'eval_steps_per_second': 17.172, 'epoch': 0.04}
{'loss': 1.6237, 'grad_norm': 1.2708059549331665, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.036867618560791, 'eval_runtime': 3.6728, 'eval_samples_per_second': 272.269, 'eval_steps_per_second': 17.153, 'epoch': 0.08}
{'loss': 1.3082, 'grad_norm': 1.2929749488830566, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.887340784072876, 'eval_runtime': 3.6748, 'eval_samples_per_second': 272.12, 'eval_steps_per_second': 17.144, 'epoch': 0.12}
{'loss': 1.1917, 'grad_norm': 1.2683826684951782, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.7806270718574524, 'eval_runtime': 3.6663, 'eval_samples_per_second': 272.755, 'eval_steps_per_second': 17.184, 'epoch': 0.16}
{'loss': 1.214, 'grad_norm': 1.1393303871154785, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.7551666498184204, 'eval_runtime': 3.6744, 'eval_samples_per_second': 272.15, 'eval_steps_per_second': 17.145, 'epoch': 0.2}
{'loss': 1.1578, 'grad_norm': 1.1749048233032227, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.7351634502410889, 'eval_runtime': 3.6774, 'eval_samples_per_second': 271.933, 'eval_steps_per_second': 17.132, 'epoch': 0.24}
{'loss': 1.0753, 'grad_norm': 0.8608911633491516, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.7121097445487976, 'eval_runtime': 3.7115, 'eval_samples_per_second': 269.429, 'eval_steps_per_second': 16.974, 'epoch': 0.28}
{'loss': 1.1368, 'grad_norm': 0.8923883438110352, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.6924776434898376, 'eval_runtime': 3.6805, 'eval_samples_per_second': 271.704, 'eval_steps_per_second': 17.117, 'epoch': 0.32}
{'loss': 1.1541, 'grad_norm': 0.8372086882591248, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.6803283095359802, 'eval_runtime': 3.6857, 'eval_samples_per_second': 271.316, 'eval_steps_per_second': 17.093, 'epoch': 0.36}
{'loss': 1.2186, 'grad_norm': 0.8164785504341125, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.649827241897583, 'eval_runtime': 3.6791, 'eval_samples_per_second': 271.806, 'eval_steps_per_second': 17.124, 'epoch': 0.4}
{'loss': 1.1609, 'grad_norm': 0.6651254892349243, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.6322031021118164, 'eval_runtime': 3.6775, 'eval_samples_per_second': 271.924, 'eval_steps_per_second': 17.131, 'epoch': 0.44}
{'loss': 1.1117, 'grad_norm': 1.19064199924469, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.6224967837333679, 'eval_runtime': 3.6799, 'eval_samples_per_second': 271.749, 'eval_steps_per_second': 17.12, 'epoch': 0.48}
{'loss': 1.14, 'grad_norm': 0.9014072418212891, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.5931983590126038, 'eval_runtime': 3.6783, 'eval_samples_per_second': 271.867, 'eval_steps_per_second': 17.128, 'epoch': 0.52}
{'loss': 1.1385, 'grad_norm': 0.9957392811775208, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.580915629863739, 'eval_runtime': 3.724, 'eval_samples_per_second': 268.53, 'eval_steps_per_second': 16.917, 'epoch': 0.56}
{'loss': 1.0866, 'grad_norm': 1.072029709815979, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.5582584142684937, 'eval_runtime': 3.7166, 'eval_samples_per_second': 269.066, 'eval_steps_per_second': 16.951, 'epoch': 0.6}
{'loss': 1.0833, 'grad_norm': 1.6164544820785522, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.5348507165908813, 'eval_runtime': 3.7123, 'eval_samples_per_second': 269.374, 'eval_steps_per_second': 16.971, 'epoch': 0.64}
{'loss': 1.0732, 'grad_norm': 1.2051094770431519, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.5181118845939636, 'eval_runtime': 3.7214, 'eval_samples_per_second': 268.715, 'eval_steps_per_second': 16.929, 'epoch': 0.68}
{'loss': 1.0877, 'grad_norm': 1.053321123123169, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.4996239244937897, 'eval_runtime': 3.7214, 'eval_samples_per_second': 268.718, 'eval_steps_per_second': 16.929, 'epoch': 0.72}
{'loss': 1.104, 'grad_norm': 4.093182563781738, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.4823245406150818, 'eval_runtime': 3.7271, 'eval_samples_per_second': 268.303, 'eval_steps_per_second': 16.903, 'epoch': 0.76}
{'loss': 1.0979, 'grad_norm': 1.0386407375335693, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.46536457538604736, 'eval_runtime': 3.7177, 'eval_samples_per_second': 268.987, 'eval_steps_per_second': 16.946, 'epoch': 0.81}
{'loss': 1.0969, 'grad_norm': 1.094786286354065, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.45013439655303955, 'eval_runtime': 3.7094, 'eval_samples_per_second': 269.588, 'eval_steps_per_second': 16.984, 'epoch': 0.85}
{'loss': 1.0754, 'grad_norm': 0.7779442667961121, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.44486889243125916, 'eval_runtime': 3.684, 'eval_samples_per_second': 271.44, 'eval_steps_per_second': 17.101, 'epoch': 0.89}
{'loss': 1.0175, 'grad_norm': 1.4329142570495605, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.427113801240921, 'eval_runtime': 3.6792, 'eval_samples_per_second': 271.8, 'eval_steps_per_second': 17.123, 'epoch': 0.93}
{'loss': 1.0822, 'grad_norm': 0.5962782502174377, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.4222436547279358, 'eval_runtime': 3.68, 'eval_samples_per_second': 271.742, 'eval_steps_per_second': 17.12, 'epoch': 0.97}
{'train_runtime': 307.3425, 'train_samples_per_second': 32.312, 'train_steps_per_second': 2.021, 'train_loss': 1.2352752992688363, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5960549116134644, 1.036867618560791, 0.887340784072876, 0.7806270718574524, 0.7551666498184204, 0.7351634502410889, 0.7121097445487976, 0.6924776434898376, 0.6803283095359802, 0.649827241897583, 0.6322031021118164, 0.6224967837333679, 0.5931983590126038, 0.580915629863739, 0.5582584142684937, 0.5348507165908813, 0.5181118845939636, 0.4996239244937897, 0.4823245406150818, 0.46536457538604736, 0.45013439655303955, 0.44486889243125916, 0.427113801240921, 0.4222436547279358], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.5960549116134644, 1.036867618560791, 0.887340784072876, 0.7806270718574524, 0.7551666498184204, 0.7351634502410889, 0.7121097445487976, 0.6924776434898376, 0.6803283095359802, 0.649827241897583, 0.6322031021118164, 0.6224967837333679, 0.5931983590126038, 0.580915629863739, 0.5582584142684937, 0.5348507165908813, 0.5181118845939636, 0.4996239244937897, 0.4823245406150818, 0.46536457538604736, 0.45013439655303955, 0.44486889243125916, 0.427113801240921, 0.4222436547279358]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.4222436547279358
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.2413 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3405855894088745, 0.2697674036026001, 0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2527220845222473, 0.2958891987800598, 0.798973798751831, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.42427369952201843, 0.12062281370162964, 0.7909995317459106, 0.5773974061012268]  ‚Üí  acq = -1.025234475655695
X = [0.33804601430892944, 0.07152366638183594, 0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.505533754825592, 0.6193363070487976, 0.5416111350059509, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9287145733833313, 0.6309018731117249, 0.6364489793777466, 0.2499348521232605]  ‚Üí  acq = -1.0252427256087409
X = [0.6847147345542908, 0.7650787830352783, 0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5654566884040833, 0.984917938709259, 0.907849907875061, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4765317142009735, 0.10925418138504028, 0.4907895624637604, 0.022005975246429443]  ‚Üí  acq = -1.025234475655691
X = [0.15384602546691895, 0.49969446659088135, 0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7945950031280518, 0.5358787775039673, 0.48621666431427, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9266265034675598, 0.8326297998428345, 0.6380935907363892, 0.630294680595398]  ‚Üí  acq = -1.0252344756812306
X = [0.4769304394721985, 0.06750988960266113, 0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6422049403190613, 0.6458637714385986, 0.9798121452331543, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.285779744386673, 0.47408175468444824, 0.7827727794647217, 0.2598608732223511]  ‚Üí  acq = -1.0252345760537374
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1572, dtype=torch.float64), tensor(0.2997, dtype=torch.float64), tensor(0.0220, dtype=torch.float64), tensor(0.1369, dtype=torch.float64), 0, tensor(0.1993, dtype=torch.float64), tensor(0.0462, dtype=torch.float64), tensor(0.1386, dtype=torch.float64), 0, 13, 1, 1, 1, 1, 1, 36, 9.144437483899277e-19, 26.482701633633493, 1]
normalized proposed parameters for next round by BO: [tensor(0.1572, dtype=torch.float64), tensor(0.2997, dtype=torch.float64), tensor(0.0220, dtype=torch.float64), tensor(0.1369, dtype=torch.float64), tensor(1.4046e-18, dtype=torch.float64), tensor(0.1993, dtype=torch.float64), tensor(0.0462, dtype=torch.float64), tensor(0.1386, dtype=torch.float64), tensor(1.3244e-18, dtype=torch.float64), tensor(0.4145, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2826, dtype=torch.float64), tensor(9.1444e-18, dtype=torch.float64), tensor(0.5517, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.157
  gsm8k: 0.3
  rowan_hellaswag: 0.022
  sciq: 0.137
  triviaqa: 0
  truthfulqa_gen: 0.199
  wikitext: 0.046
  mmlu: 0.139
  arc_challenge: 0

LoRA Parameters:
  lora_r: (36,)
  lora_dropout: (9.144437483899277e-19,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (26.482701633633493,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  36
lora dropout:  9.144437483899277e-19
lora alpha:  26.482701633633493
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 32,108,544 || all params: 8,062,369,792 || trainable%: 0.3983
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7227, 'grad_norm': 1.3674947023391724, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.692993402481079, 'eval_runtime': 3.5309, 'eval_samples_per_second': 283.218, 'eval_steps_per_second': 17.843, 'epoch': 0.04}
{'loss': 1.4005, 'grad_norm': 0.855877697467804, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0144413709640503, 'eval_runtime': 3.5383, 'eval_samples_per_second': 282.624, 'eval_steps_per_second': 17.805, 'epoch': 0.08}
{'loss': 1.1708, 'grad_norm': 0.40542060136795044, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9300314784049988, 'eval_runtime': 3.5325, 'eval_samples_per_second': 283.086, 'eval_steps_per_second': 17.834, 'epoch': 0.12}
{'loss': 1.1359, 'grad_norm': 0.3284046947956085, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8552929759025574, 'eval_runtime': 3.532, 'eval_samples_per_second': 283.124, 'eval_steps_per_second': 17.837, 'epoch': 0.16}
{'loss': 1.0318, 'grad_norm': 0.42254310846328735, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8087247610092163, 'eval_runtime': 3.5385, 'eval_samples_per_second': 282.607, 'eval_steps_per_second': 17.804, 'epoch': 0.2}
{'loss': 1.0179, 'grad_norm': 0.35905197262763977, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7786832451820374, 'eval_runtime': 3.5446, 'eval_samples_per_second': 282.116, 'eval_steps_per_second': 17.773, 'epoch': 0.24}
{'loss': 1.0172, 'grad_norm': 0.3740028738975525, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7653111815452576, 'eval_runtime': 3.5417, 'eval_samples_per_second': 282.348, 'eval_steps_per_second': 17.788, 'epoch': 0.28}
{'loss': 0.9674, 'grad_norm': 0.3650203347206116, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7577140927314758, 'eval_runtime': 3.5472, 'eval_samples_per_second': 281.916, 'eval_steps_per_second': 17.761, 'epoch': 0.32}
{'loss': 1.0603, 'grad_norm': 0.2849712073802948, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7421925067901611, 'eval_runtime': 3.543, 'eval_samples_per_second': 282.246, 'eval_steps_per_second': 17.781, 'epoch': 0.36}
{'loss': 1.0049, 'grad_norm': 0.3456737995147705, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7320982813835144, 'eval_runtime': 3.546, 'eval_samples_per_second': 282.007, 'eval_steps_per_second': 17.766, 'epoch': 0.4}
{'loss': 1.0046, 'grad_norm': 0.29540541768074036, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7026180624961853, 'eval_runtime': 3.5471, 'eval_samples_per_second': 281.917, 'eval_steps_per_second': 17.761, 'epoch': 0.44}
{'loss': 0.9541, 'grad_norm': 0.3516034185886383, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6941072940826416, 'eval_runtime': 3.5496, 'eval_samples_per_second': 281.724, 'eval_steps_per_second': 17.749, 'epoch': 0.48}
{'loss': 0.9913, 'grad_norm': 0.32288044691085815, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6919301748275757, 'eval_runtime': 3.5506, 'eval_samples_per_second': 281.642, 'eval_steps_per_second': 17.743, 'epoch': 0.52}
{'loss': 0.9541, 'grad_norm': 0.36495307087898254, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6651231646537781, 'eval_runtime': 3.5432, 'eval_samples_per_second': 282.233, 'eval_steps_per_second': 17.781, 'epoch': 0.56}
{'loss': 1.0087, 'grad_norm': 0.5103626847267151, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6539164185523987, 'eval_runtime': 3.536, 'eval_samples_per_second': 282.806, 'eval_steps_per_second': 17.817, 'epoch': 0.6}
{'loss': 0.9791, 'grad_norm': 0.37098002433776855, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6514418721199036, 'eval_runtime': 3.5347, 'eval_samples_per_second': 282.907, 'eval_steps_per_second': 17.823, 'epoch': 0.64}
{'loss': 0.9806, 'grad_norm': 0.29579731822013855, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6408949494361877, 'eval_runtime': 3.5392, 'eval_samples_per_second': 282.546, 'eval_steps_per_second': 17.8, 'epoch': 0.68}
{'loss': 0.941, 'grad_norm': 0.31759753823280334, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6209018230438232, 'eval_runtime': 3.5371, 'eval_samples_per_second': 282.716, 'eval_steps_per_second': 17.811, 'epoch': 0.72}
{'loss': 1.0115, 'grad_norm': 0.37820595502853394, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6110697984695435, 'eval_runtime': 3.5364, 'eval_samples_per_second': 282.774, 'eval_steps_per_second': 17.815, 'epoch': 0.76}
{'loss': 0.9728, 'grad_norm': 0.3553420603275299, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6071695685386658, 'eval_runtime': 3.532, 'eval_samples_per_second': 283.128, 'eval_steps_per_second': 17.837, 'epoch': 0.8}
{'loss': 0.9782, 'grad_norm': 0.321233868598938, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5913813710212708, 'eval_runtime': 3.5338, 'eval_samples_per_second': 282.983, 'eval_steps_per_second': 17.828, 'epoch': 0.84}
{'loss': 0.9804, 'grad_norm': 0.39371898770332336, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5771619081497192, 'eval_runtime': 3.5319, 'eval_samples_per_second': 283.134, 'eval_steps_per_second': 17.837, 'epoch': 0.88}
{'loss': 1.0084, 'grad_norm': 0.33627358078956604, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5761525630950928, 'eval_runtime': 3.5332, 'eval_samples_per_second': 283.029, 'eval_steps_per_second': 17.831, 'epoch': 0.92}
{'loss': 0.9743, 'grad_norm': 0.32315734028816223, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5694634914398193, 'eval_runtime': 3.5369, 'eval_samples_per_second': 282.736, 'eval_steps_per_second': 17.812, 'epoch': 0.96}
{'loss': 0.9728, 'grad_norm': 0.3899132013320923, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5678355693817139, 'eval_runtime': 3.5394, 'eval_samples_per_second': 282.535, 'eval_steps_per_second': 17.8, 'epoch': 1.0}
{'train_runtime': 306.0588, 'train_samples_per_second': 32.667, 'train_steps_per_second': 2.042, 'train_loss': 1.0896474670410157, 'epoch': 1.0}
train_results:  {'eval_loss': [1.692993402481079, 1.0144413709640503, 0.9300314784049988, 0.8552929759025574, 0.8087247610092163, 0.7786832451820374, 0.7653111815452576, 0.7577140927314758, 0.7421925067901611, 0.7320982813835144, 0.7026180624961853, 0.6941072940826416, 0.6919301748275757, 0.6651231646537781, 0.6539164185523987, 0.6514418721199036, 0.6408949494361877, 0.6209018230438232, 0.6110697984695435, 0.6071695685386658, 0.5913813710212708, 0.5771619081497192, 0.5761525630950928, 0.5694634914398193, 0.5678355693817139], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.692993402481079, 1.0144413709640503, 0.9300314784049988, 0.8552929759025574, 0.8087247610092163, 0.7786832451820374, 0.7653111815452576, 0.7577140927314758, 0.7421925067901611, 0.7320982813835144, 0.7026180624961853, 0.6941072940826416, 0.6919301748275757, 0.6651231646537781, 0.6539164185523987, 0.6514418721199036, 0.6408949494361877, 0.6209018230438232, 0.6110697984695435, 0.6071695685386658, 0.5913813710212708, 0.5771619081497192, 0.5761525630950928, 0.5694634914398193, 0.5678355693817139]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.5678355693817139
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.6417 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -1.0029987056531486
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -1.003281237055157
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -1.0029987103419078
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -1.0030001312868642
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -1.0029987069224682
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0726, dtype=torch.float64), tensor(0.0299, dtype=torch.float64), tensor(0.1291, dtype=torch.float64), tensor(0.1453, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.4137, dtype=torch.float64), 0, tensor(0.0489, dtype=torch.float64), tensor(0.1257, dtype=torch.float64), 19, 0, 1, 0, 1, 1, 21, 0.0006026085895487142, 32.47128059077284, 1]
normalized proposed parameters for next round by BO: [tensor(0.0726, dtype=torch.float64), tensor(0.0299, dtype=torch.float64), tensor(0.1291, dtype=torch.float64), tensor(0.1453, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.4137, dtype=torch.float64), tensor(0.0066, dtype=torch.float64), tensor(0.0489, dtype=torch.float64), tensor(0.1257, dtype=torch.float64), tensor(0.5807, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1673, dtype=torch.float64), tensor(0.0060, dtype=torch.float64), tensor(0.6765, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.073
  gsm8k: 0.03
  rowan_hellaswag: 0.129
  sciq: 0.145
  triviaqa: 0.028
  truthfulqa_gen: 0.414
  wikitext: 0
  mmlu: 0.049
  arc_challenge: 0.126

LoRA Parameters:
  lora_r: (21,)
  lora_dropout: (0.0006026085895487142,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (32.47128059077284,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  21
lora dropout:  0.0006026085895487142
lora alpha:  32.47128059077284
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,751,616 || all params: 8,047,012,864 || trainable%: 0.2082
length of training data:  9930
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2639, 'grad_norm': 2.30181884765625, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.536767840385437, 'eval_runtime': 3.5393, 'eval_samples_per_second': 282.544, 'eval_steps_per_second': 17.8, 'epoch': 0.04}
{'loss': 1.5291, 'grad_norm': 0.8835223317146301, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9556013345718384, 'eval_runtime': 3.5881, 'eval_samples_per_second': 278.699, 'eval_steps_per_second': 17.558, 'epoch': 0.08}
{'loss': 1.2972, 'grad_norm': 0.6179599761962891, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.8133668899536133, 'eval_runtime': 3.5965, 'eval_samples_per_second': 278.049, 'eval_steps_per_second': 17.517, 'epoch': 0.12}
{'loss': 1.1573, 'grad_norm': 1.2720578908920288, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.7103638648986816, 'eval_runtime': 3.5833, 'eval_samples_per_second': 279.076, 'eval_steps_per_second': 17.582, 'epoch': 0.16}
{'loss': 1.202, 'grad_norm': 0.5635780692100525, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.6742223501205444, 'eval_runtime': 3.5829, 'eval_samples_per_second': 279.1, 'eval_steps_per_second': 17.583, 'epoch': 0.2}
{'loss': 1.0749, 'grad_norm': 0.5826407670974731, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.6360078454017639, 'eval_runtime': 3.5949, 'eval_samples_per_second': 278.168, 'eval_steps_per_second': 17.525, 'epoch': 0.24}
{'loss': 1.1313, 'grad_norm': 0.7258763313293457, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.5927327871322632, 'eval_runtime': 3.5904, 'eval_samples_per_second': 278.521, 'eval_steps_per_second': 17.547, 'epoch': 0.28}
{'loss': 1.0454, 'grad_norm': 0.6498977541923523, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.5622310638427734, 'eval_runtime': 3.6116, 'eval_samples_per_second': 276.889, 'eval_steps_per_second': 17.444, 'epoch': 0.32}
{'loss': 1.0502, 'grad_norm': 0.6004720330238342, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.5363707542419434, 'eval_runtime': 3.5781, 'eval_samples_per_second': 279.476, 'eval_steps_per_second': 17.607, 'epoch': 0.36}
{'loss': 1.0879, 'grad_norm': 0.49291154742240906, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.505321741104126, 'eval_runtime': 3.6005, 'eval_samples_per_second': 277.736, 'eval_steps_per_second': 17.497, 'epoch': 0.4}
{'loss': 1.0152, 'grad_norm': 0.5882284641265869, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.4733150005340576, 'eval_runtime': 3.5923, 'eval_samples_per_second': 278.37, 'eval_steps_per_second': 17.537, 'epoch': 0.44}
{'loss': 1.0544, 'grad_norm': 0.5994255542755127, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.43514910340309143, 'eval_runtime': 3.565, 'eval_samples_per_second': 280.502, 'eval_steps_per_second': 17.672, 'epoch': 0.48}
{'loss': 1.0114, 'grad_norm': 0.56108158826828, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.4021049737930298, 'eval_runtime': 3.5658, 'eval_samples_per_second': 280.445, 'eval_steps_per_second': 17.668, 'epoch': 0.52}
{'loss': 1.022, 'grad_norm': 0.9550042748451233, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.3778548836708069, 'eval_runtime': 3.5624, 'eval_samples_per_second': 280.708, 'eval_steps_per_second': 17.685, 'epoch': 0.56}
{'loss': 1.0359, 'grad_norm': 0.7154801487922668, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.35620489716529846, 'eval_runtime': 3.5576, 'eval_samples_per_second': 281.086, 'eval_steps_per_second': 17.708, 'epoch': 0.6}
{'loss': 0.9996, 'grad_norm': 0.5630719065666199, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.3261868357658386, 'eval_runtime': 3.5553, 'eval_samples_per_second': 281.268, 'eval_steps_per_second': 17.72, 'epoch': 0.64}
{'loss': 0.9659, 'grad_norm': 0.6763347387313843, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.29787617921829224, 'eval_runtime': 3.5522, 'eval_samples_per_second': 281.517, 'eval_steps_per_second': 17.736, 'epoch': 0.68}
{'loss': 1.0464, 'grad_norm': 0.6880545616149902, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.2957613468170166, 'eval_runtime': 3.5435, 'eval_samples_per_second': 282.21, 'eval_steps_per_second': 17.779, 'epoch': 0.72}
{'loss': 0.945, 'grad_norm': 0.8802387118339539, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.2799527943134308, 'eval_runtime': 3.5527, 'eval_samples_per_second': 281.477, 'eval_steps_per_second': 17.733, 'epoch': 0.76}
{'loss': 0.9969, 'grad_norm': 0.5466886758804321, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.260140061378479, 'eval_runtime': 3.5545, 'eval_samples_per_second': 281.337, 'eval_steps_per_second': 17.724, 'epoch': 0.81}
{'loss': 0.9842, 'grad_norm': 0.7099833488464355, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.2529284954071045, 'eval_runtime': 3.5588, 'eval_samples_per_second': 280.992, 'eval_steps_per_second': 17.703, 'epoch': 0.85}
{'loss': 0.9479, 'grad_norm': 0.5442611575126648, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.24237951636314392, 'eval_runtime': 3.5549, 'eval_samples_per_second': 281.301, 'eval_steps_per_second': 17.722, 'epoch': 0.89}
{'loss': 0.9396, 'grad_norm': 0.7273861765861511, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.23222067952156067, 'eval_runtime': 3.5522, 'eval_samples_per_second': 281.519, 'eval_steps_per_second': 17.736, 'epoch': 0.93}
{'loss': 1.0103, 'grad_norm': 0.6116291284561157, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.2271054983139038, 'eval_runtime': 3.549, 'eval_samples_per_second': 281.77, 'eval_steps_per_second': 17.751, 'epoch': 0.97}
{'train_runtime': 298.9505, 'train_samples_per_second': 33.216, 'train_steps_per_second': 2.077, 'train_loss': 1.1514307657877605, 'epoch': 1.0}
train_results:  {'eval_loss': [1.536767840385437, 0.9556013345718384, 0.8133668899536133, 0.7103638648986816, 0.6742223501205444, 0.6360078454017639, 0.5927327871322632, 0.5622310638427734, 0.5363707542419434, 0.505321741104126, 0.4733150005340576, 0.43514910340309143, 0.4021049737930298, 0.3778548836708069, 0.35620489716529846, 0.3261868357658386, 0.29787617921829224, 0.2957613468170166, 0.2799527943134308, 0.260140061378479, 0.2529284954071045, 0.24237951636314392, 0.23222067952156067, 0.2271054983139038], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.536767840385437, 0.9556013345718384, 0.8133668899536133, 0.7103638648986816, 0.6742223501205444, 0.6360078454017639, 0.5927327871322632, 0.5622310638427734, 0.5363707542419434, 0.505321741104126, 0.4733150005340576, 0.43514910340309143, 0.4021049737930298, 0.3778548836708069, 0.35620489716529846, 0.3261868357658386, 0.29787617921829224, 0.2957613468170166, 0.2799527943134308, 0.260140061378479, 0.2529284954071045, 0.24237951636314392, 0.23222067952156067, 0.2271054983139038]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.2271054983139038
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.5320 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9384865760803223, 0.07229626178741455, 0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6294164657592773, 0.5043690800666809, 0.12173128128051758, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.45333993434906006, 0.9395368695259094, 0.38172125816345215, 0.5809779763221741]  ‚Üí  acq = -1.0440500536516353
X = [0.25227582454681396, 0.03358107805252075, 0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.7713519334793091, 0.4265725612640381, 0.10601025819778442, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.3584800362586975, 0.16592669486999512, 0.6665055751800537, 0.5913098454475403]  ‚Üí  acq = -1.0441561892868596
X = [0.019490361213684082, 0.33564668893814087, 0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.24613288044929504, 0.783804178237915, 0.05754297971725464, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6837789416313171, 0.5803513526916504, 0.10683952271938324, 0.5482228994369507]  ‚Üí  acq = -1.0440500523933365
X = [0.08164948225021362, 0.510372519493103, 0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3423933982849121, 0.566781759262085, 0.5619614124298096, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9544339776039124, 0.7185770869255066, 0.4849817454814911, 0.03939622640609741]  ‚Üí  acq = -1.0440500523899952
X = [0.9194858074188232, 0.7918861508369446, 0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5424157381057739, 0.4093313217163086, 0.6975671648979187, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.975213885307312, 0.7470415830612183, 0.2785099744796753, 0.9093899726867676]  ‚Üí  acq = -1.0440503991801775
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1127, dtype=torch.float64), tensor(0.0703, dtype=torch.float64), tensor(0.1414, dtype=torch.float64), 0, tensor(0.4240, dtype=torch.float64), tensor(0.0558, dtype=torch.float64), tensor(0.0752, dtype=torch.float64), tensor(0.1183, dtype=torch.float64), 25, 0, 1, 0, 1, 1, 36, 0.030656145413005098, 27.51657897465981, 1]
normalized proposed parameters for next round by BO: [tensor(0.0024, dtype=torch.float64), tensor(0.1127, dtype=torch.float64), tensor(0.0703, dtype=torch.float64), tensor(0.1414, dtype=torch.float64), tensor(2.2564e-19, dtype=torch.float64), tensor(0.4240, dtype=torch.float64), tensor(0.0558, dtype=torch.float64), tensor(0.0752, dtype=torch.float64), tensor(0.1183, dtype=torch.float64), tensor(0.7737, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2797, dtype=torch.float64), tensor(0.3066, dtype=torch.float64), tensor(0.5733, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.113
  rowan_hellaswag: 0.07
  sciq: 0.141
  triviaqa: 0
  truthfulqa_gen: 0.424
  wikitext: 0.056
  mmlu: 0.075
  arc_challenge: 0.118

LoRA Parameters:
  lora_r: (36,)
  lora_dropout: (0.030656145413005098,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (27.51657897465981,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  36
lora dropout:  0.030656145413005098
lora alpha:  27.51657897465981
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 37,785,600 || all params: 8,068,046,848 || trainable%: 0.4683
length of training data:  9973
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0947, 'grad_norm': 1.9076570272445679, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4824504852294922, 'eval_runtime': 3.6084, 'eval_samples_per_second': 277.132, 'eval_steps_per_second': 17.459, 'epoch': 0.04}
{'loss': 1.5052, 'grad_norm': 0.6497036218643188, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9476208686828613, 'eval_runtime': 3.5963, 'eval_samples_per_second': 278.061, 'eval_steps_per_second': 17.518, 'epoch': 0.08}
{'loss': 1.2195, 'grad_norm': 0.5594637393951416, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.8370352983474731, 'eval_runtime': 3.6016, 'eval_samples_per_second': 277.652, 'eval_steps_per_second': 17.492, 'epoch': 0.12}
{'loss': 1.0573, 'grad_norm': 0.4739800989627838, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.7144885063171387, 'eval_runtime': 3.601, 'eval_samples_per_second': 277.7, 'eval_steps_per_second': 17.495, 'epoch': 0.16}
{'loss': 1.1057, 'grad_norm': 0.3957061171531677, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.6478857398033142, 'eval_runtime': 3.6036, 'eval_samples_per_second': 277.498, 'eval_steps_per_second': 17.482, 'epoch': 0.2}
{'loss': 1.086, 'grad_norm': 0.5583316087722778, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.6002098321914673, 'eval_runtime': 3.5988, 'eval_samples_per_second': 277.87, 'eval_steps_per_second': 17.506, 'epoch': 0.24}
{'loss': 1.023, 'grad_norm': 0.37288981676101685, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.5554279685020447, 'eval_runtime': 3.6034, 'eval_samples_per_second': 277.515, 'eval_steps_per_second': 17.483, 'epoch': 0.28}
{'loss': 1.0279, 'grad_norm': 0.41781195998191833, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.5299089550971985, 'eval_runtime': 3.6069, 'eval_samples_per_second': 277.248, 'eval_steps_per_second': 17.467, 'epoch': 0.32}
{'loss': 1.021, 'grad_norm': 0.350894570350647, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.5060062408447266, 'eval_runtime': 3.6053, 'eval_samples_per_second': 277.373, 'eval_steps_per_second': 17.474, 'epoch': 0.36}
{'loss': 1.059, 'grad_norm': 0.3951052725315094, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.45467260479927063, 'eval_runtime': 3.6041, 'eval_samples_per_second': 277.465, 'eval_steps_per_second': 17.48, 'epoch': 0.4}
{'loss': 1.0004, 'grad_norm': 0.41441231966018677, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.40710657835006714, 'eval_runtime': 3.6087, 'eval_samples_per_second': 277.112, 'eval_steps_per_second': 17.458, 'epoch': 0.44}
{'loss': 0.9707, 'grad_norm': 0.5159340500831604, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.40066081285476685, 'eval_runtime': 3.6032, 'eval_samples_per_second': 277.529, 'eval_steps_per_second': 17.484, 'epoch': 0.48}
{'loss': 1.0113, 'grad_norm': 0.5588722229003906, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.3599444031715393, 'eval_runtime': 3.6111, 'eval_samples_per_second': 276.925, 'eval_steps_per_second': 17.446, 'epoch': 0.52}
{'loss': 0.9342, 'grad_norm': 0.4483190178871155, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.32837536931037903, 'eval_runtime': 3.6097, 'eval_samples_per_second': 277.028, 'eval_steps_per_second': 17.453, 'epoch': 0.56}
{'loss': 0.8695, 'grad_norm': 0.37688809633255005, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.3063725233078003, 'eval_runtime': 3.5986, 'eval_samples_per_second': 277.885, 'eval_steps_per_second': 17.507, 'epoch': 0.6}
{'loss': 0.8964, 'grad_norm': 0.43573519587516785, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.27494317293167114, 'eval_runtime': 3.6102, 'eval_samples_per_second': 276.991, 'eval_steps_per_second': 17.45, 'epoch': 0.64}
{'loss': 0.8973, 'grad_norm': 0.36619579792022705, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.256288081407547, 'eval_runtime': 3.6009, 'eval_samples_per_second': 277.705, 'eval_steps_per_second': 17.495, 'epoch': 0.68}
{'loss': 0.9264, 'grad_norm': 0.4521871507167816, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.2528468072414398, 'eval_runtime': 3.6082, 'eval_samples_per_second': 277.148, 'eval_steps_per_second': 17.46, 'epoch': 0.72}
{'loss': 0.8571, 'grad_norm': 0.3872593641281128, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.22948016226291656, 'eval_runtime': 3.6019, 'eval_samples_per_second': 277.63, 'eval_steps_per_second': 17.491, 'epoch': 0.76}
{'loss': 0.9144, 'grad_norm': 0.342722088098526, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.2200821191072464, 'eval_runtime': 3.6073, 'eval_samples_per_second': 277.219, 'eval_steps_per_second': 17.465, 'epoch': 0.8}
{'loss': 0.9363, 'grad_norm': 0.4878678321838379, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.20557335019111633, 'eval_runtime': 3.6064, 'eval_samples_per_second': 277.282, 'eval_steps_per_second': 17.469, 'epoch': 0.84}
{'loss': 0.8665, 'grad_norm': 0.37171462178230286, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.19866780936717987, 'eval_runtime': 3.6191, 'eval_samples_per_second': 276.311, 'eval_steps_per_second': 17.408, 'epoch': 0.88}
{'loss': 0.8948, 'grad_norm': 0.6099380850791931, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.19521917402744293, 'eval_runtime': 3.6193, 'eval_samples_per_second': 276.297, 'eval_steps_per_second': 17.407, 'epoch': 0.92}
{'loss': 0.9381, 'grad_norm': 0.4487726092338562, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.19127246737480164, 'eval_runtime': 3.6789, 'eval_samples_per_second': 271.822, 'eval_steps_per_second': 17.125, 'epoch': 0.96}
{'train_runtime': 306.7673, 'train_samples_per_second': 32.51, 'train_steps_per_second': 2.034, 'train_loss': 1.0798893219385393, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4824504852294922, 0.9476208686828613, 0.8370352983474731, 0.7144885063171387, 0.6478857398033142, 0.6002098321914673, 0.5554279685020447, 0.5299089550971985, 0.5060062408447266, 0.45467260479927063, 0.40710657835006714, 0.40066081285476685, 0.3599444031715393, 0.32837536931037903, 0.3063725233078003, 0.27494317293167114, 0.256288081407547, 0.2528468072414398, 0.22948016226291656, 0.2200821191072464, 0.20557335019111633, 0.19866780936717987, 0.19521917402744293, 0.19127246737480164], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.4824504852294922, 0.9476208686828613, 0.8370352983474731, 0.7144885063171387, 0.6478857398033142, 0.6002098321914673, 0.5554279685020447, 0.5299089550971985, 0.5060062408447266, 0.45467260479927063, 0.40710657835006714, 0.40066081285476685, 0.3599444031715393, 0.32837536931037903, 0.3063725233078003, 0.27494317293167114, 0.256288081407547, 0.2528468072414398, 0.22948016226291656, 0.2200821191072464, 0.20557335019111633, 0.19866780936717987, 0.19521917402744293, 0.19127246737480164]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.19127246737480164
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.2463 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5840210318565369, 0.5403404831886292, 0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.49047714471817017, 0.8636659979820251, 0.10396885871887207, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.09389074146747589, 0.5328817963600159, 0.1704922765493393, 0.8951139450073242]  ‚Üí  acq = -1.041945314146599
X = [0.20226186513900757, 0.6831576228141785, 0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.24446256458759308, 0.7839422821998596, 0.6810406446456909, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.06929440051317215, 0.9363643527030945, 0.4914133846759796, 0.07482516765594482]  ‚Üí  acq = -1.0827200913497645
X = [0.7401218414306641, 0.8363668322563171, 0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.5837264060974121, 0.37286609411239624, 0.8461750149726868, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.32410305738449097, 0.8762340545654297, 0.1423635184764862, 0.6519256234169006]  ‚Üí  acq = -1.0419453225045399
X = [0.5809492468833923, 0.9983730316162109, 0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.5152989625930786, 0.0025587081909179688, 0.5287423133850098, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.04051867127418518, 0.9936825633049011, 0.13591282069683075, 0.053788065910339355]  ‚Üí  acq = -1.0419453141492498
X = [0.3528999090194702, 0.5949426293373108, 0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.40121349692344666, 0.7189667820930481, 0.7950335144996643, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4900456964969635, 0.8887658715248108, 0.9805734157562256, 0.013015925884246826]  ‚Üí  acq = -1.0426105595194426
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0752, dtype=torch.float64), tensor(0.0673, dtype=torch.float64), tensor(0.2229, dtype=torch.float64), 0, tensor(0.4330, dtype=torch.float64), 0, tensor(0.0861, dtype=torch.float64), tensor(0.1155, dtype=torch.float64), 13, 0, 1, 0, 1, 1, 128, 0.006926009508305727, 35.5637175061527, 1]
normalized proposed parameters for next round by BO: [tensor(1.8479e-18, dtype=torch.float64), tensor(0.0752, dtype=torch.float64), tensor(0.0673, dtype=torch.float64), tensor(0.2229, dtype=torch.float64), tensor(4.4644e-18, dtype=torch.float64), tensor(0.4330, dtype=torch.float64), tensor(8.0456e-19, dtype=torch.float64), tensor(0.0861, dtype=torch.float64), tensor(0.1155, dtype=torch.float64), tensor(0.4217, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0693, dtype=torch.float64), tensor(0.7409, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.075
  rowan_hellaswag: 0.067
  sciq: 0.223
  triviaqa: 0
  truthfulqa_gen: 0.433
  wikitext: 0
  mmlu: 0.086
  arc_challenge: 0.116

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.006926009508305727,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (35.5637175061527,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.006926009508305727
lora alpha:  35.5637175061527
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 69,861,376 || all params: 8,100,122,624 || trainable%: 0.8625
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2112, 'grad_norm': 0.8878729343414307, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5475759506225586, 'eval_runtime': 3.3372, 'eval_samples_per_second': 299.653, 'eval_steps_per_second': 18.878, 'epoch': 0.04}
{'loss': 1.4909, 'grad_norm': 0.379724383354187, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0021246671676636, 'eval_runtime': 3.3537, 'eval_samples_per_second': 298.181, 'eval_steps_per_second': 18.785, 'epoch': 0.08}
{'loss': 1.1866, 'grad_norm': 0.5146311521530151, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8791380524635315, 'eval_runtime': 3.3427, 'eval_samples_per_second': 299.163, 'eval_steps_per_second': 18.847, 'epoch': 0.12}
{'loss': 1.1177, 'grad_norm': 0.3889271020889282, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7766386866569519, 'eval_runtime': 3.347, 'eval_samples_per_second': 298.778, 'eval_steps_per_second': 18.823, 'epoch': 0.16}
{'loss': 1.0398, 'grad_norm': 0.3173181414604187, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.744270384311676, 'eval_runtime': 3.3807, 'eval_samples_per_second': 295.795, 'eval_steps_per_second': 18.635, 'epoch': 0.2}
{'loss': 1.0439, 'grad_norm': 0.28758013248443604, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.700179398059845, 'eval_runtime': 3.3818, 'eval_samples_per_second': 295.703, 'eval_steps_per_second': 18.629, 'epoch': 0.24}
{'loss': 1.055, 'grad_norm': 0.30788514018058777, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6746259927749634, 'eval_runtime': 3.3689, 'eval_samples_per_second': 296.837, 'eval_steps_per_second': 18.701, 'epoch': 0.28}
{'loss': 0.9957, 'grad_norm': 0.2661275565624237, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6541305184364319, 'eval_runtime': 3.3631, 'eval_samples_per_second': 297.347, 'eval_steps_per_second': 18.733, 'epoch': 0.32}
{'loss': 0.9803, 'grad_norm': 0.2574932277202606, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6234290599822998, 'eval_runtime': 3.3742, 'eval_samples_per_second': 296.364, 'eval_steps_per_second': 18.671, 'epoch': 0.36}
{'loss': 0.9607, 'grad_norm': 0.4009751081466675, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.588486909866333, 'eval_runtime': 3.3681, 'eval_samples_per_second': 296.906, 'eval_steps_per_second': 18.705, 'epoch': 0.4}
{'loss': 1.0639, 'grad_norm': 0.39982372522354126, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5649704337120056, 'eval_runtime': 3.3715, 'eval_samples_per_second': 296.6, 'eval_steps_per_second': 18.686, 'epoch': 0.44}
{'loss': 0.9679, 'grad_norm': 0.27851536870002747, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5549954175949097, 'eval_runtime': 3.3805, 'eval_samples_per_second': 295.818, 'eval_steps_per_second': 18.637, 'epoch': 0.48}
{'loss': 1.0146, 'grad_norm': 0.24942229688167572, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5344854593276978, 'eval_runtime': 3.3848, 'eval_samples_per_second': 295.439, 'eval_steps_per_second': 18.613, 'epoch': 0.52}
{'loss': 0.9003, 'grad_norm': 0.4398459792137146, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5185295343399048, 'eval_runtime': 3.3816, 'eval_samples_per_second': 295.714, 'eval_steps_per_second': 18.63, 'epoch': 0.56}
{'loss': 0.9335, 'grad_norm': 0.3633084297180176, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4937460422515869, 'eval_runtime': 3.3814, 'eval_samples_per_second': 295.738, 'eval_steps_per_second': 18.632, 'epoch': 0.6}
{'loss': 0.9022, 'grad_norm': 0.33284854888916016, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4818207621574402, 'eval_runtime': 3.4342, 'eval_samples_per_second': 291.189, 'eval_steps_per_second': 18.345, 'epoch': 0.64}
{'loss': 0.9461, 'grad_norm': 0.24840191006660461, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4472160041332245, 'eval_runtime': 3.3728, 'eval_samples_per_second': 296.493, 'eval_steps_per_second': 18.679, 'epoch': 0.68}
{'loss': 0.9928, 'grad_norm': 0.34263601899147034, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.42149102687835693, 'eval_runtime': 3.3599, 'eval_samples_per_second': 297.629, 'eval_steps_per_second': 18.751, 'epoch': 0.72}
{'loss': 0.9431, 'grad_norm': 0.3775166869163513, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.40306079387664795, 'eval_runtime': 3.3628, 'eval_samples_per_second': 297.369, 'eval_steps_per_second': 18.734, 'epoch': 0.76}
{'loss': 0.8915, 'grad_norm': 0.34825655817985535, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.38877320289611816, 'eval_runtime': 3.3607, 'eval_samples_per_second': 297.559, 'eval_steps_per_second': 18.746, 'epoch': 0.8}
{'loss': 0.8935, 'grad_norm': 0.4248204827308655, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3755652606487274, 'eval_runtime': 3.3581, 'eval_samples_per_second': 297.788, 'eval_steps_per_second': 18.761, 'epoch': 0.84}
{'loss': 0.8383, 'grad_norm': 0.41297557950019836, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3632989823818207, 'eval_runtime': 3.3636, 'eval_samples_per_second': 297.302, 'eval_steps_per_second': 18.73, 'epoch': 0.88}
{'loss': 0.882, 'grad_norm': 0.4067680835723877, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3419312834739685, 'eval_runtime': 3.3622, 'eval_samples_per_second': 297.427, 'eval_steps_per_second': 18.738, 'epoch': 0.92}
{'loss': 0.9195, 'grad_norm': 0.31598684191703796, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3344172239303589, 'eval_runtime': 3.3598, 'eval_samples_per_second': 297.638, 'eval_steps_per_second': 18.751, 'epoch': 0.96}
{'loss': 0.9525, 'grad_norm': 0.31180644035339355, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.32884007692337036, 'eval_runtime': 3.3626, 'eval_samples_per_second': 297.388, 'eval_steps_per_second': 18.735, 'epoch': 1.0}
{'train_runtime': 276.422, 'train_samples_per_second': 36.162, 'train_steps_per_second': 2.261, 'train_loss': 1.0849405029296875, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5475759506225586, 1.0021246671676636, 0.8791380524635315, 0.7766386866569519, 0.744270384311676, 0.700179398059845, 0.6746259927749634, 0.6541305184364319, 0.6234290599822998, 0.588486909866333, 0.5649704337120056, 0.5549954175949097, 0.5344854593276978, 0.5185295343399048, 0.4937460422515869, 0.4818207621574402, 0.4472160041332245, 0.42149102687835693, 0.40306079387664795, 0.38877320289611816, 0.3755652606487274, 0.3632989823818207, 0.3419312834739685, 0.3344172239303589, 0.32884007692337036], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5475759506225586, 1.0021246671676636, 0.8791380524635315, 0.7766386866569519, 0.744270384311676, 0.700179398059845, 0.6746259927749634, 0.6541305184364319, 0.6234290599822998, 0.588486909866333, 0.5649704337120056, 0.5549954175949097, 0.5344854593276978, 0.5185295343399048, 0.4937460422515869, 0.4818207621574402, 0.4472160041332245, 0.42149102687835693, 0.40306079387664795, 0.38877320289611816, 0.3755652606487274, 0.3632989823818207, 0.3419312834739685, 0.3344172239303589, 0.32884007692337036]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.32884007692337036
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.0362 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -1.0456946879157218
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -1.0475697607407932
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -1.0456934915811182
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -1.045695881142078
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -1.0458288744020128
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0983, dtype=torch.float64), tensor(0.0577, dtype=torch.float64), tensor(0.0174, dtype=torch.float64), tensor(0.1962, dtype=torch.float64), tensor(0.1319, dtype=torch.float64), tensor(0.3090, dtype=torch.float64), tensor(0.0147, dtype=torch.float64), tensor(0.1599, dtype=torch.float64), tensor(0.0149, dtype=torch.float64), 21, 0, 1, 0, 1, 1, 50, 0.010746782261566902, 24.25231909001765, 1]
normalized proposed parameters for next round by BO: [tensor(0.0983, dtype=torch.float64), tensor(0.0577, dtype=torch.float64), tensor(0.0174, dtype=torch.float64), tensor(0.1962, dtype=torch.float64), tensor(0.1319, dtype=torch.float64), tensor(0.3090, dtype=torch.float64), tensor(0.0147, dtype=torch.float64), tensor(0.1599, dtype=torch.float64), tensor(0.0149, dtype=torch.float64), tensor(0.6621, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3881, dtype=torch.float64), tensor(0.1075, dtype=torch.float64), tensor(0.5053, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.098
  gsm8k: 0.058
  rowan_hellaswag: 0.017
  sciq: 0.196
  triviaqa: 0.132
  truthfulqa_gen: 0.309
  wikitext: 0.015
  mmlu: 0.16
  arc_challenge: 0.015

LoRA Parameters:
  lora_r: (50,)
  lora_dropout: (0.010746782261566902,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (24.25231909001765,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  50
lora dropout:  0.010746782261566902
lora alpha:  24.25231909001765
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 44,083,200 || all params: 8,074,344,448 || trainable%: 0.5460
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3125, 'grad_norm': 1.560570240020752, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5761611461639404, 'eval_runtime': 3.5339, 'eval_samples_per_second': 282.973, 'eval_steps_per_second': 17.827, 'epoch': 0.04}
{'loss': 1.5206, 'grad_norm': 1.066885232925415, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0073732137680054, 'eval_runtime': 3.5337, 'eval_samples_per_second': 282.992, 'eval_steps_per_second': 17.829, 'epoch': 0.08}
{'loss': 1.2149, 'grad_norm': 0.8051674365997314, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8968625664710999, 'eval_runtime': 3.5294, 'eval_samples_per_second': 283.333, 'eval_steps_per_second': 17.85, 'epoch': 0.12}
{'loss': 1.1331, 'grad_norm': 0.4593985378742218, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.78902268409729, 'eval_runtime': 3.5326, 'eval_samples_per_second': 283.081, 'eval_steps_per_second': 17.834, 'epoch': 0.16}
{'loss': 1.1293, 'grad_norm': 0.41879335045814514, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7312216758728027, 'eval_runtime': 3.5303, 'eval_samples_per_second': 283.266, 'eval_steps_per_second': 17.846, 'epoch': 0.2}
{'loss': 1.0141, 'grad_norm': 0.4595205783843994, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7079020738601685, 'eval_runtime': 3.538, 'eval_samples_per_second': 282.643, 'eval_steps_per_second': 17.806, 'epoch': 0.24}
{'loss': 1.0892, 'grad_norm': 0.4454136788845062, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6773809790611267, 'eval_runtime': 3.5403, 'eval_samples_per_second': 282.459, 'eval_steps_per_second': 17.795, 'epoch': 0.28}
{'loss': 1.0254, 'grad_norm': 0.39365169405937195, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6654927730560303, 'eval_runtime': 3.5455, 'eval_samples_per_second': 282.049, 'eval_steps_per_second': 17.769, 'epoch': 0.32}
{'loss': 1.0062, 'grad_norm': 0.40809330344200134, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6228918433189392, 'eval_runtime': 3.5776, 'eval_samples_per_second': 279.519, 'eval_steps_per_second': 17.61, 'epoch': 0.36}
{'loss': 0.9898, 'grad_norm': 0.3860834836959839, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5931858420372009, 'eval_runtime': 3.5502, 'eval_samples_per_second': 281.672, 'eval_steps_per_second': 17.745, 'epoch': 0.4}
{'loss': 1.0101, 'grad_norm': 0.3687858581542969, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5556969046592712, 'eval_runtime': 3.5499, 'eval_samples_per_second': 281.696, 'eval_steps_per_second': 17.747, 'epoch': 0.44}
{'loss': 0.9639, 'grad_norm': 0.40083205699920654, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5297458171844482, 'eval_runtime': 3.5491, 'eval_samples_per_second': 281.758, 'eval_steps_per_second': 17.751, 'epoch': 0.48}
{'loss': 0.9418, 'grad_norm': 0.4446602463722229, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.48512011766433716, 'eval_runtime': 3.5504, 'eval_samples_per_second': 281.658, 'eval_steps_per_second': 17.744, 'epoch': 0.52}
{'loss': 1.0296, 'grad_norm': 0.34432253241539, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.46220695972442627, 'eval_runtime': 3.5921, 'eval_samples_per_second': 278.387, 'eval_steps_per_second': 17.538, 'epoch': 0.56}
{'loss': 0.9321, 'grad_norm': 0.3462364673614502, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4432019889354706, 'eval_runtime': 3.5982, 'eval_samples_per_second': 277.917, 'eval_steps_per_second': 17.509, 'epoch': 0.6}
{'loss': 0.9798, 'grad_norm': 0.3747868835926056, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.427554190158844, 'eval_runtime': 3.6117, 'eval_samples_per_second': 276.882, 'eval_steps_per_second': 17.444, 'epoch': 0.64}
{'loss': 0.9569, 'grad_norm': 0.3154255449771881, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.40904009342193604, 'eval_runtime': 3.5952, 'eval_samples_per_second': 278.147, 'eval_steps_per_second': 17.523, 'epoch': 0.68}
{'loss': 0.8849, 'grad_norm': 0.44793906807899475, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.38399702310562134, 'eval_runtime': 3.5929, 'eval_samples_per_second': 278.325, 'eval_steps_per_second': 17.534, 'epoch': 0.72}
{'loss': 0.9122, 'grad_norm': 0.40006163716316223, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.37410783767700195, 'eval_runtime': 3.6127, 'eval_samples_per_second': 276.803, 'eval_steps_per_second': 17.439, 'epoch': 0.76}
{'loss': 0.9264, 'grad_norm': 0.3672374486923218, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.35667040944099426, 'eval_runtime': 3.5935, 'eval_samples_per_second': 278.283, 'eval_steps_per_second': 17.532, 'epoch': 0.8}
{'loss': 0.914, 'grad_norm': 0.40603554248809814, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3358793556690216, 'eval_runtime': 3.6031, 'eval_samples_per_second': 277.541, 'eval_steps_per_second': 17.485, 'epoch': 0.84}
{'loss': 0.8816, 'grad_norm': 0.2979138195514679, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.32319432497024536, 'eval_runtime': 3.5973, 'eval_samples_per_second': 277.988, 'eval_steps_per_second': 17.513, 'epoch': 0.88}
{'loss': 0.8903, 'grad_norm': 0.4155602753162384, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.31017276644706726, 'eval_runtime': 3.6086, 'eval_samples_per_second': 277.115, 'eval_steps_per_second': 17.458, 'epoch': 0.92}
{'loss': 0.9245, 'grad_norm': 0.3814099431037903, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.30883297324180603, 'eval_runtime': 3.6082, 'eval_samples_per_second': 277.147, 'eval_steps_per_second': 17.46, 'epoch': 0.96}
{'loss': 0.8888, 'grad_norm': 0.49319133162498474, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.29901114106178284, 'eval_runtime': 3.5895, 'eval_samples_per_second': 278.59, 'eval_steps_per_second': 17.551, 'epoch': 1.0}
{'train_runtime': 283.6252, 'train_samples_per_second': 35.24, 'train_steps_per_second': 2.204, 'train_loss': 1.0988747039794922, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5761611461639404, 1.0073732137680054, 0.8968625664710999, 0.78902268409729, 0.7312216758728027, 0.7079020738601685, 0.6773809790611267, 0.6654927730560303, 0.6228918433189392, 0.5931858420372009, 0.5556969046592712, 0.5297458171844482, 0.48512011766433716, 0.46220695972442627, 0.4432019889354706, 0.427554190158844, 0.40904009342193604, 0.38399702310562134, 0.37410783767700195, 0.35667040944099426, 0.3358793556690216, 0.32319432497024536, 0.31017276644706726, 0.30883297324180603, 0.29901114106178284], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5761611461639404, 1.0073732137680054, 0.8968625664710999, 0.78902268409729, 0.7312216758728027, 0.7079020738601685, 0.6773809790611267, 0.6654927730560303, 0.6228918433189392, 0.5931858420372009, 0.5556969046592712, 0.5297458171844482, 0.48512011766433716, 0.46220695972442627, 0.4432019889354706, 0.427554190158844, 0.40904009342193604, 0.38399702310562134, 0.37410783767700195, 0.35667040944099426, 0.3358793556690216, 0.32319432497024536, 0.31017276644706726, 0.30883297324180603, 0.29901114106178284]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1272388696670532
current iteration best possible eval_loss (full train run):  -0.29901114106178284
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.9844 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -1.0444046733873076
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -1.0456497458214256
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -1.0444046759677417
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -1.0529545905323663
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -1.0444047302887636
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0374, dtype=torch.float64), tensor(0.1406, dtype=torch.float64), tensor(0.0882, dtype=torch.float64), tensor(0.1597, dtype=torch.float64), tensor(0.2032, dtype=torch.float64), tensor(0.2513, dtype=torch.float64), tensor(0.0236, dtype=torch.float64), 0, tensor(0.0891, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 57, 0.05747561757857964, 32.360253262882196, 1]
normalized proposed parameters for next round by BO: [tensor(0.0374, dtype=torch.float64), tensor(0.1406, dtype=torch.float64), tensor(0.0882, dtype=torch.float64), tensor(0.1597, dtype=torch.float64), tensor(0.2032, dtype=torch.float64), tensor(0.2513, dtype=torch.float64), tensor(0.0236, dtype=torch.float64), tensor(0.0068, dtype=torch.float64), tensor(0.0891, dtype=torch.float64), tensor(0.4675, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4452, dtype=torch.float64), tensor(0.5748, dtype=torch.float64), tensor(0.6742, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.037
  gsm8k: 0.141
  rowan_hellaswag: 0.088
  sciq: 0.16
  triviaqa: 0.203
  truthfulqa_gen: 0.251
  wikitext: 0.024
  mmlu: 0
  arc_challenge: 0.089

LoRA Parameters:
  lora_r: (57,)
  lora_dropout: (0.05747561757857964,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (32.360253262882196,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  57
lora dropout:  0.05747561757857964
lora alpha:  32.360253262882196
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 35,896,320 || all params: 8,066,157,568 || trainable%: 0.4450
length of training data:  9929
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2536, 'grad_norm': 1.3944422006607056, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6431306600570679, 'eval_runtime': 3.4808, 'eval_samples_per_second': 287.293, 'eval_steps_per_second': 18.099, 'epoch': 0.04}
{'loss': 1.5051, 'grad_norm': 0.5404054522514343, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.013619065284729, 'eval_runtime': 3.4908, 'eval_samples_per_second': 286.471, 'eval_steps_per_second': 18.048, 'epoch': 0.08}
{'loss': 1.229, 'grad_norm': 0.5869840979576111, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.9118744134902954, 'eval_runtime': 3.4947, 'eval_samples_per_second': 286.144, 'eval_steps_per_second': 18.027, 'epoch': 0.12}
{'loss': 1.106, 'grad_norm': 0.5275817513465881, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.8032163381576538, 'eval_runtime': 3.5003, 'eval_samples_per_second': 285.688, 'eval_steps_per_second': 17.998, 'epoch': 0.16}
{'loss': 1.0993, 'grad_norm': 0.48180311918258667, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.7658047080039978, 'eval_runtime': 3.4892, 'eval_samples_per_second': 286.6, 'eval_steps_per_second': 18.056, 'epoch': 0.2}
{'loss': 1.1163, 'grad_norm': 0.43068256974220276, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.7437373399734497, 'eval_runtime': 3.4932, 'eval_samples_per_second': 286.274, 'eval_steps_per_second': 18.035, 'epoch': 0.24}
{'loss': 1.0949, 'grad_norm': 0.39614343643188477, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.7401884198188782, 'eval_runtime': 3.5037, 'eval_samples_per_second': 285.41, 'eval_steps_per_second': 17.981, 'epoch': 0.28}
{'loss': 1.1116, 'grad_norm': 0.7356569170951843, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.7240521907806396, 'eval_runtime': 3.4947, 'eval_samples_per_second': 286.146, 'eval_steps_per_second': 18.027, 'epoch': 0.32}
{'loss': 1.0972, 'grad_norm': 0.3207758963108063, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.6907236576080322, 'eval_runtime': 3.4951, 'eval_samples_per_second': 286.118, 'eval_steps_per_second': 18.025, 'epoch': 0.36}
{'loss': 1.0399, 'grad_norm': 0.38876309990882874, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.6745280623435974, 'eval_runtime': 3.4919, 'eval_samples_per_second': 286.38, 'eval_steps_per_second': 18.042, 'epoch': 0.4}
{'loss': 1.0335, 'grad_norm': 0.3015470802783966, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.6538859605789185, 'eval_runtime': 3.5051, 'eval_samples_per_second': 285.299, 'eval_steps_per_second': 17.974, 'epoch': 0.44}
{'loss': 1.0384, 'grad_norm': 0.3231932520866394, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.6277919411659241, 'eval_runtime': 3.5033, 'eval_samples_per_second': 285.442, 'eval_steps_per_second': 17.983, 'epoch': 0.48}
{'loss': 1.0266, 'grad_norm': 0.28714868426322937, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.6068834066390991, 'eval_runtime': 3.5055, 'eval_samples_per_second': 285.269, 'eval_steps_per_second': 17.972, 'epoch': 0.52}
{'loss': 1.0531, 'grad_norm': 0.31900790333747864, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.5901523232460022, 'eval_runtime': 3.4991, 'eval_samples_per_second': 285.786, 'eval_steps_per_second': 18.005, 'epoch': 0.56}
{'loss': 1.0667, 'grad_norm': 0.3493090569972992, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.5803738832473755, 'eval_runtime': 3.4981, 'eval_samples_per_second': 285.871, 'eval_steps_per_second': 18.01, 'epoch': 0.6}
{'loss': 1.024, 'grad_norm': 0.2970157265663147, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.5857968926429749, 'eval_runtime': 3.4986, 'eval_samples_per_second': 285.825, 'eval_steps_per_second': 18.007, 'epoch': 0.64}
{'loss': 1.0453, 'grad_norm': 0.26471617817878723, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.5583066940307617, 'eval_runtime': 3.4996, 'eval_samples_per_second': 285.746, 'eval_steps_per_second': 18.002, 'epoch': 0.68}
{'loss': 1.0299, 'grad_norm': 0.29783931374549866, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.5339308977127075, 'eval_runtime': 3.4978, 'eval_samples_per_second': 285.896, 'eval_steps_per_second': 18.011, 'epoch': 0.72}
{'loss': 1.0053, 'grad_norm': 0.33671438694000244, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.5298357009887695, 'eval_runtime': 3.5223, 'eval_samples_per_second': 283.908, 'eval_steps_per_second': 17.886, 'epoch': 0.76}
{'loss': 1.0109, 'grad_norm': 0.26948022842407227, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.5052037835121155, 'eval_runtime': 3.4968, 'eval_samples_per_second': 285.972, 'eval_steps_per_second': 18.016, 'epoch': 0.81}
{'loss': 1.0707, 'grad_norm': 0.33220329880714417, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.5047647356987, 'eval_runtime': 3.4964, 'eval_samples_per_second': 286.01, 'eval_steps_per_second': 18.019, 'epoch': 0.85}
{'loss': 0.9715, 'grad_norm': 0.28751233220100403, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.47974133491516113, 'eval_runtime': 3.5052, 'eval_samples_per_second': 285.293, 'eval_steps_per_second': 17.973, 'epoch': 0.89}
{'loss': 1.1161, 'grad_norm': 0.3612014055252075, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.47789129614830017, 'eval_runtime': 3.4982, 'eval_samples_per_second': 285.864, 'eval_steps_per_second': 18.009, 'epoch': 0.93}
{'loss': 1.0165, 'grad_norm': 0.40349218249320984, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.4729959964752197, 'eval_runtime': 3.5041, 'eval_samples_per_second': 285.376, 'eval_steps_per_second': 17.979, 'epoch': 0.97}
{'train_runtime': 293.6578, 'train_samples_per_second': 33.811, 'train_steps_per_second': 2.115, 'train_loss': 1.1647303269489184, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6431306600570679, 1.013619065284729, 0.9118744134902954, 0.8032163381576538, 0.7658047080039978, 0.7437373399734497, 0.7401884198188782, 0.7240521907806396, 0.6907236576080322, 0.6745280623435974, 0.6538859605789185, 0.6277919411659241, 0.6068834066390991, 0.5901523232460022, 0.5803738832473755, 0.5857968926429749, 0.5583066940307617, 0.5339308977127075, 0.5298357009887695, 0.5052037835121155, 0.5047647356987, 0.47974133491516113, 0.47789129614830017, 0.4729959964752197], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.6431306600570679, 1.013619065284729, 0.9118744134902954, 0.8032163381576538, 0.7658047080039978, 0.7437373399734497, 0.7401884198188782, 0.7240521907806396, 0.6907236576080322, 0.6745280623435974, 0.6538859605789185, 0.6277919411659241, 0.6068834066390991, 0.5901523232460022, 0.5803738832473755, 0.5857968926429749, 0.5583066940307617, 0.5339308977127075, 0.5298357009887695, 0.5052037835121155, 0.5047647356987, 0.47974133491516113, 0.47789129614830017, 0.4729959964752197]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1259040832519531
current iteration best possible eval_loss (full train run):  -0.4729959964752197
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.1190 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.35491812229156494, 0.0927686095237732, 0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7262229919433594, 0.8165992498397827, 0.8520699143409729, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9559857845306396, 0.3273009657859802, 0.9828505516052246, 0.5352497100830078]  ‚Üí  acq = -1.0312823142554413
X = [0.8189860582351685, 0.7869956493377686, 0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8973821401596069, 0.7314273715019226, 0.9184417128562927, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9420246481895447, 0.9331071972846985, 0.31236356496810913, 0.36077266931533813]  ‚Üí  acq = -1.0312819546709795
X = [0.7738390564918518, 0.5021045207977295, 0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5741828680038452, 0.5517953038215637, 0.7892404794692993, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9668935537338257, 0.5031551122665405, 0.7251023054122925, 0.2269490361213684]  ‚Üí  acq = -1.0312831027511242
X = [0.20103693008422852, 0.27278316020965576, 0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.14547844231128693, 0.0291365385055542, 0.3460528254508972, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.32679685950279236, 0.4884251356124878, 0.9665257930755615, 0.2274898886680603]  ‚Üí  acq = -1.0312819330644707
X = [0.6374526023864746, 0.36883002519607544, 0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3681538701057434, 0.6981962323188782, 0.7622081637382507, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.5123441219329834, 0.3907546401023865, 0.21269100904464722, 0.09688013792037964]  ‚Üí  acq = -1.0312819637137347
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1128, dtype=torch.float64), tensor(0.0459, dtype=torch.float64), tensor(0.0609, dtype=torch.float64), tensor(0.2184, dtype=torch.float64), 0, tensor(0.3375, dtype=torch.float64), 0, tensor(0.0768, dtype=torch.float64), tensor(0.1477, dtype=torch.float64), 15, 1, 1, 0, 1, 1, 54, 9.145307880490847e-19, 32.7729818677275, 1]
normalized proposed parameters for next round by BO: [tensor(0.1128, dtype=torch.float64), tensor(0.0459, dtype=torch.float64), tensor(0.0609, dtype=torch.float64), tensor(0.2184, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3375, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0768, dtype=torch.float64), tensor(0.1477, dtype=torch.float64), tensor(0.4828, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4205, dtype=torch.float64), tensor(9.1453e-18, dtype=torch.float64), tensor(0.6828, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.113
  gsm8k: 0.046
  rowan_hellaswag: 0.061
  sciq: 0.218
  triviaqa: 0
  truthfulqa_gen: 0.337
  wikitext: 0
  mmlu: 0.077
  arc_challenge: 0.148

LoRA Parameters:
  lora_r: (54,)
  lora_dropout: (9.145307880490847e-19,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (32.7729818677275,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  54
lora dropout:  9.145307880490847e-19
lora alpha:  32.7729818677275
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 40,642,560 || all params: 8,070,903,808 || trainable%: 0.5036
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3116, 'grad_norm': 1.4952139854431152, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.580201268196106, 'eval_runtime': 3.5052, 'eval_samples_per_second': 285.288, 'eval_steps_per_second': 17.973, 'epoch': 0.04}
{'loss': 1.4176, 'grad_norm': 0.6459037065505981, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9825214743614197, 'eval_runtime': 3.5071, 'eval_samples_per_second': 285.137, 'eval_steps_per_second': 17.964, 'epoch': 0.08}
{'loss': 1.1958, 'grad_norm': 0.47800877690315247, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8790444135665894, 'eval_runtime': 3.4791, 'eval_samples_per_second': 287.433, 'eval_steps_per_second': 18.108, 'epoch': 0.12}
{'loss': 1.1647, 'grad_norm': 0.41533035039901733, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.777477502822876, 'eval_runtime': 3.4865, 'eval_samples_per_second': 286.824, 'eval_steps_per_second': 18.07, 'epoch': 0.16}
{'loss': 1.0793, 'grad_norm': 0.38790076971054077, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7332732677459717, 'eval_runtime': 3.4846, 'eval_samples_per_second': 286.975, 'eval_steps_per_second': 18.079, 'epoch': 0.2}
{'loss': 1.0742, 'grad_norm': 0.3954198360443115, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7238610982894897, 'eval_runtime': 3.4944, 'eval_samples_per_second': 286.17, 'eval_steps_per_second': 18.029, 'epoch': 0.24}
{'loss': 1.0235, 'grad_norm': 0.39699843525886536, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7052112817764282, 'eval_runtime': 3.4945, 'eval_samples_per_second': 286.165, 'eval_steps_per_second': 18.028, 'epoch': 0.28}
{'loss': 0.9932, 'grad_norm': 0.367707222700119, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6715489625930786, 'eval_runtime': 3.4979, 'eval_samples_per_second': 285.882, 'eval_steps_per_second': 18.011, 'epoch': 0.32}
{'loss': 1.0268, 'grad_norm': 0.3502291738986969, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6375231742858887, 'eval_runtime': 3.4989, 'eval_samples_per_second': 285.808, 'eval_steps_per_second': 18.006, 'epoch': 0.36}
{'loss': 0.9414, 'grad_norm': 0.38342204689979553, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6209909319877625, 'eval_runtime': 3.4965, 'eval_samples_per_second': 286.004, 'eval_steps_per_second': 18.018, 'epoch': 0.4}
{'loss': 0.9861, 'grad_norm': 0.49411118030548096, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.602159321308136, 'eval_runtime': 3.4992, 'eval_samples_per_second': 285.781, 'eval_steps_per_second': 18.004, 'epoch': 0.44}
{'loss': 0.9962, 'grad_norm': 0.3449569642543793, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5872634649276733, 'eval_runtime': 3.5048, 'eval_samples_per_second': 285.322, 'eval_steps_per_second': 17.975, 'epoch': 0.48}
{'loss': 1.0151, 'grad_norm': 0.31048423051834106, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5868272185325623, 'eval_runtime': 3.5021, 'eval_samples_per_second': 285.544, 'eval_steps_per_second': 17.989, 'epoch': 0.52}
{'loss': 0.9747, 'grad_norm': 0.40535876154899597, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5545793175697327, 'eval_runtime': 3.4958, 'eval_samples_per_second': 286.056, 'eval_steps_per_second': 18.021, 'epoch': 0.56}
{'loss': 0.9804, 'grad_norm': 0.37007394433021545, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5291855931282043, 'eval_runtime': 3.4996, 'eval_samples_per_second': 285.744, 'eval_steps_per_second': 18.002, 'epoch': 0.6}
{'loss': 0.9822, 'grad_norm': 0.41739577054977417, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5024234056472778, 'eval_runtime': 3.4967, 'eval_samples_per_second': 285.987, 'eval_steps_per_second': 18.017, 'epoch': 0.64}
{'loss': 0.931, 'grad_norm': 0.41783514618873596, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.477508544921875, 'eval_runtime': 3.4977, 'eval_samples_per_second': 285.901, 'eval_steps_per_second': 18.012, 'epoch': 0.68}
{'loss': 0.9503, 'grad_norm': 0.42304664850234985, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.4583898186683655, 'eval_runtime': 3.4987, 'eval_samples_per_second': 285.817, 'eval_steps_per_second': 18.006, 'epoch': 0.72}
{'loss': 0.9372, 'grad_norm': 0.4392659366130829, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4421140253543854, 'eval_runtime': 3.4993, 'eval_samples_per_second': 285.772, 'eval_steps_per_second': 18.004, 'epoch': 0.76}
{'loss': 0.8697, 'grad_norm': 0.4106689989566803, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.4276908040046692, 'eval_runtime': 3.4973, 'eval_samples_per_second': 285.934, 'eval_steps_per_second': 18.014, 'epoch': 0.8}
{'loss': 0.868, 'grad_norm': 0.4521838128566742, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3959965407848358, 'eval_runtime': 3.5033, 'eval_samples_per_second': 285.446, 'eval_steps_per_second': 17.983, 'epoch': 0.84}
{'loss': 0.8578, 'grad_norm': 0.3754466772079468, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3807017207145691, 'eval_runtime': 3.5177, 'eval_samples_per_second': 284.276, 'eval_steps_per_second': 17.909, 'epoch': 0.88}
{'loss': 0.9222, 'grad_norm': 0.34043586254119873, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.36636364459991455, 'eval_runtime': 3.5384, 'eval_samples_per_second': 282.613, 'eval_steps_per_second': 17.805, 'epoch': 0.92}
{'loss': 0.8573, 'grad_norm': 0.4141182005405426, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.36019134521484375, 'eval_runtime': 3.5175, 'eval_samples_per_second': 284.292, 'eval_steps_per_second': 17.91, 'epoch': 0.96}
{'loss': 0.8517, 'grad_norm': 0.36326363682746887, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3560742735862732, 'eval_runtime': 3.5155, 'eval_samples_per_second': 284.457, 'eval_steps_per_second': 17.921, 'epoch': 1.0}
{'train_runtime': 289.1075, 'train_samples_per_second': 34.579, 'train_steps_per_second': 2.162, 'train_loss': 1.088319790649414, 'epoch': 1.0}
train_results:  {'eval_loss': [1.580201268196106, 0.9825214743614197, 0.8790444135665894, 0.777477502822876, 0.7332732677459717, 0.7238610982894897, 0.7052112817764282, 0.6715489625930786, 0.6375231742858887, 0.6209909319877625, 0.602159321308136, 0.5872634649276733, 0.5868272185325623, 0.5545793175697327, 0.5291855931282043, 0.5024234056472778, 0.477508544921875, 0.4583898186683655, 0.4421140253543854, 0.4276908040046692, 0.3959965407848358, 0.3807017207145691, 0.36636364459991455, 0.36019134521484375, 0.3560742735862732], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.580201268196106, 0.9825214743614197, 0.8790444135665894, 0.777477502822876, 0.7332732677459717, 0.7238610982894897, 0.7052112817764282, 0.6715489625930786, 0.6375231742858887, 0.6209909319877625, 0.602159321308136, 0.5872634649276733, 0.5868272185325623, 0.5545793175697327, 0.5291855931282043, 0.5024234056472778, 0.477508544921875, 0.4583898186683655, 0.4421140253543854, 0.4276908040046692, 0.3959965407848358, 0.3807017207145691, 0.36636364459991455, 0.36019134521484375, 0.3560742735862732]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1256964206695557
current iteration best possible eval_loss (full train run):  -0.3560742735862732
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.0978 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -1.0624121748487902
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -1.038947973459572
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -1.062411641177306
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -1.0624116176135134
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -1.062411360018956
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1226, dtype=torch.float64), tensor(0.0335, dtype=torch.float64), tensor(0.0382, dtype=torch.float64), tensor(0.2185, dtype=torch.float64), tensor(0.0349, dtype=torch.float64), tensor(0.1456, dtype=torch.float64), tensor(0.0670, dtype=torch.float64), tensor(0.0675, dtype=torch.float64), tensor(0.2721, dtype=torch.float64), 14, 0, 1, 0, 1, 1, 49, 0.0, 29.296249977228467, 1]
normalized proposed parameters for next round by BO: [tensor(0.1226, dtype=torch.float64), tensor(0.0335, dtype=torch.float64), tensor(0.0382, dtype=torch.float64), tensor(0.2185, dtype=torch.float64), tensor(0.0349, dtype=torch.float64), tensor(0.1456, dtype=torch.float64), tensor(0.0670, dtype=torch.float64), tensor(0.0675, dtype=torch.float64), tensor(0.2721, dtype=torch.float64), tensor(0.4220, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3841, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6103, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.123
  gsm8k: 0.033
  rowan_hellaswag: 0.038
  sciq: 0.218
  triviaqa: 0.035
  truthfulqa_gen: 0.146
  wikitext: 0.067
  mmlu: 0.068
  arc_challenge: 0.272

LoRA Parameters:
  lora_r: (49,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (29.296249977228467,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  49
lora dropout:  0.0
lora alpha:  29.296249977228467
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 28,801,024 || all params: 8,059,062,272 || trainable%: 0.3574
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3265, 'grad_norm': 1.8322421312332153, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7548818588256836, 'eval_runtime': 3.451, 'eval_samples_per_second': 289.769, 'eval_steps_per_second': 18.255, 'epoch': 0.04}
{'loss': 1.5215, 'grad_norm': 0.9344944953918457, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1016082763671875, 'eval_runtime': 3.4583, 'eval_samples_per_second': 289.161, 'eval_steps_per_second': 18.217, 'epoch': 0.08}
{'loss': 1.2042, 'grad_norm': 0.7036601305007935, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9741300940513611, 'eval_runtime': 3.4714, 'eval_samples_per_second': 288.068, 'eval_steps_per_second': 18.148, 'epoch': 0.12}
{'loss': 1.1365, 'grad_norm': 0.5438641905784607, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8639163374900818, 'eval_runtime': 3.4567, 'eval_samples_per_second': 289.296, 'eval_steps_per_second': 18.226, 'epoch': 0.16}
{'loss': 1.0825, 'grad_norm': 0.3772444725036621, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.853606641292572, 'eval_runtime': 3.4797, 'eval_samples_per_second': 287.384, 'eval_steps_per_second': 18.105, 'epoch': 0.2}
{'loss': 1.1331, 'grad_norm': 0.5989171862602234, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8227007985115051, 'eval_runtime': 3.4746, 'eval_samples_per_second': 287.805, 'eval_steps_per_second': 18.132, 'epoch': 0.24}
{'loss': 1.0696, 'grad_norm': 0.46120786666870117, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.79462730884552, 'eval_runtime': 3.4674, 'eval_samples_per_second': 288.398, 'eval_steps_per_second': 18.169, 'epoch': 0.28}
{'loss': 1.0461, 'grad_norm': 0.4622482657432556, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7752634286880493, 'eval_runtime': 3.4659, 'eval_samples_per_second': 288.527, 'eval_steps_per_second': 18.177, 'epoch': 0.32}
{'loss': 1.0847, 'grad_norm': 0.3873001039028168, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7904750108718872, 'eval_runtime': 3.4654, 'eval_samples_per_second': 288.567, 'eval_steps_per_second': 18.18, 'epoch': 0.36}
{'loss': 1.0266, 'grad_norm': 0.356320858001709, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7708735466003418, 'eval_runtime': 3.4685, 'eval_samples_per_second': 288.311, 'eval_steps_per_second': 18.164, 'epoch': 0.4}
{'loss': 1.0272, 'grad_norm': 0.47659119963645935, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7544880509376526, 'eval_runtime': 3.4578, 'eval_samples_per_second': 289.197, 'eval_steps_per_second': 18.219, 'epoch': 0.44}
{'loss': 1.0328, 'grad_norm': 0.33244919776916504, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7562966346740723, 'eval_runtime': 3.4631, 'eval_samples_per_second': 288.762, 'eval_steps_per_second': 18.192, 'epoch': 0.48}
{'loss': 1.0374, 'grad_norm': 0.5435537695884705, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7373006343841553, 'eval_runtime': 3.5014, 'eval_samples_per_second': 285.6, 'eval_steps_per_second': 17.993, 'epoch': 0.52}
{'loss': 0.9871, 'grad_norm': 0.5187178254127502, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.72266685962677, 'eval_runtime': 3.4725, 'eval_samples_per_second': 287.978, 'eval_steps_per_second': 18.143, 'epoch': 0.56}
{'loss': 1.0138, 'grad_norm': 0.40611740946769714, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7092146277427673, 'eval_runtime': 3.4655, 'eval_samples_per_second': 288.562, 'eval_steps_per_second': 18.179, 'epoch': 0.6}
{'loss': 1.0268, 'grad_norm': 0.37166455388069153, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7124136686325073, 'eval_runtime': 3.4892, 'eval_samples_per_second': 286.597, 'eval_steps_per_second': 18.056, 'epoch': 0.64}
{'loss': 1.0013, 'grad_norm': 0.3346672058105469, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7011234164237976, 'eval_runtime': 3.4573, 'eval_samples_per_second': 289.24, 'eval_steps_per_second': 18.222, 'epoch': 0.68}
{'loss': 1.0142, 'grad_norm': 0.343487411737442, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6971742510795593, 'eval_runtime': 3.4602, 'eval_samples_per_second': 289.004, 'eval_steps_per_second': 18.207, 'epoch': 0.72}
{'loss': 1.0008, 'grad_norm': 0.4466049373149872, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6803989410400391, 'eval_runtime': 3.4574, 'eval_samples_per_second': 289.232, 'eval_steps_per_second': 18.222, 'epoch': 0.76}
{'loss': 1.0266, 'grad_norm': 0.4200039505958557, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6758586168289185, 'eval_runtime': 3.4558, 'eval_samples_per_second': 289.373, 'eval_steps_per_second': 18.23, 'epoch': 0.8}
{'loss': 1.0058, 'grad_norm': 0.41302716732025146, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.679463803768158, 'eval_runtime': 3.4521, 'eval_samples_per_second': 289.676, 'eval_steps_per_second': 18.25, 'epoch': 0.84}
{'loss': 0.956, 'grad_norm': 0.46118783950805664, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6605460047721863, 'eval_runtime': 3.4506, 'eval_samples_per_second': 289.806, 'eval_steps_per_second': 18.258, 'epoch': 0.88}
{'loss': 0.992, 'grad_norm': 0.5029659867286682, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.652025043964386, 'eval_runtime': 3.447, 'eval_samples_per_second': 290.107, 'eval_steps_per_second': 18.277, 'epoch': 0.92}
{'loss': 0.9218, 'grad_norm': 0.5244719982147217, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6541045904159546, 'eval_runtime': 3.4504, 'eval_samples_per_second': 289.818, 'eval_steps_per_second': 18.259, 'epoch': 0.96}
{'loss': 0.9666, 'grad_norm': 0.5982000231742859, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6536168456077576, 'eval_runtime': 3.4596, 'eval_samples_per_second': 289.054, 'eval_steps_per_second': 18.21, 'epoch': 1.0}
{'train_runtime': 271.7069, 'train_samples_per_second': 36.786, 'train_steps_per_second': 2.3, 'train_loss': 1.1456729888916015, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7548818588256836, 1.1016082763671875, 0.9741300940513611, 0.8639163374900818, 0.853606641292572, 0.8227007985115051, 0.79462730884552, 0.7752634286880493, 0.7904750108718872, 0.7708735466003418, 0.7544880509376526, 0.7562966346740723, 0.7373006343841553, 0.72266685962677, 0.7092146277427673, 0.7124136686325073, 0.7011234164237976, 0.6971742510795593, 0.6803989410400391, 0.6758586168289185, 0.679463803768158, 0.6605460047721863, 0.652025043964386, 0.6541045904159546, 0.6536168456077576], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7548818588256836, 1.1016082763671875, 0.9741300940513611, 0.8639163374900818, 0.853606641292572, 0.8227007985115051, 0.79462730884552, 0.7752634286880493, 0.7904750108718872, 0.7708735466003418, 0.7544880509376526, 0.7562966346740723, 0.7373006343841553, 0.72266685962677, 0.7092146277427673, 0.7124136686325073, 0.7011234164237976, 0.6971742510795593, 0.6803989410400391, 0.6758586168289185, 0.679463803768158, 0.6605460047721863, 0.652025043964386, 0.6541045904159546, 0.6536168456077576]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1267603635787964
current iteration best possible eval_loss (full train run):  -0.6536168456077576
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.8841 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -1.0655780285871452
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -1.0645375532030372
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -1.066052808265688
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -1.0629981023161235
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -1.0627484697705198
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1344, dtype=torch.float64), tensor(0.0127, dtype=torch.float64), tensor(0.1544, dtype=torch.float64), tensor(0.1342, dtype=torch.float64), tensor(0.1578, dtype=torch.float64), tensor(0.0822, dtype=torch.float64), tensor(0.0520, dtype=torch.float64), tensor(0.2723, dtype=torch.float64), 19, 1, 1, 1, 0, 1, 48, 0.005691973419476557, 26.395946670281205, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.1344, dtype=torch.float64), tensor(0.0127, dtype=torch.float64), tensor(0.1544, dtype=torch.float64), tensor(0.1342, dtype=torch.float64), tensor(0.1578, dtype=torch.float64), tensor(0.0822, dtype=torch.float64), tensor(0.0520, dtype=torch.float64), tensor(0.2723, dtype=torch.float64), tensor(0.5817, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3737, dtype=torch.float64), tensor(0.0569, dtype=torch.float64), tensor(0.5499, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.134
  rowan_hellaswag: 0.013
  sciq: 0.154
  triviaqa: 0.134
  truthfulqa_gen: 0.158
  wikitext: 0.082
  mmlu: 0.052
  arc_challenge: 0.272

LoRA Parameters:
  lora_r: (48,)
  lora_dropout: (0.005691973419476557,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (26.395946670281205,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  48
lora dropout:  0.005691973419476557
lora alpha:  26.395946670281205
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 45,760,512 || all params: 8,076,021,760 || trainable%: 0.5666
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0002, 'grad_norm': 0.9765662550926208, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8039968013763428, 'eval_runtime': 3.5171, 'eval_samples_per_second': 284.325, 'eval_steps_per_second': 17.912, 'epoch': 0.04}
{'loss': 1.447, 'grad_norm': 0.5113328099250793, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2227352857589722, 'eval_runtime': 3.5015, 'eval_samples_per_second': 285.588, 'eval_steps_per_second': 17.992, 'epoch': 0.08}
{'loss': 1.2496, 'grad_norm': 0.4737648367881775, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1101973056793213, 'eval_runtime': 3.5019, 'eval_samples_per_second': 285.561, 'eval_steps_per_second': 17.99, 'epoch': 0.12}
{'loss': 1.1963, 'grad_norm': 0.27265968918800354, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.978207528591156, 'eval_runtime': 3.4883, 'eval_samples_per_second': 286.673, 'eval_steps_per_second': 18.06, 'epoch': 0.16}
{'loss': 1.1237, 'grad_norm': 0.25489678978919983, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.950232982635498, 'eval_runtime': 3.5075, 'eval_samples_per_second': 285.104, 'eval_steps_per_second': 17.962, 'epoch': 0.2}
{'loss': 1.1226, 'grad_norm': 0.33683261275291443, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9171174168586731, 'eval_runtime': 3.507, 'eval_samples_per_second': 285.147, 'eval_steps_per_second': 17.964, 'epoch': 0.24}
{'loss': 1.0665, 'grad_norm': 0.3956642150878906, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8918676376342773, 'eval_runtime': 3.5101, 'eval_samples_per_second': 284.892, 'eval_steps_per_second': 17.948, 'epoch': 0.28}
{'loss': 1.056, 'grad_norm': 0.287800133228302, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8678814172744751, 'eval_runtime': 3.5082, 'eval_samples_per_second': 285.049, 'eval_steps_per_second': 17.958, 'epoch': 0.32}
{'loss': 1.1044, 'grad_norm': 0.33455878496170044, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8282552361488342, 'eval_runtime': 3.5131, 'eval_samples_per_second': 284.651, 'eval_steps_per_second': 17.933, 'epoch': 0.36}
{'loss': 1.0371, 'grad_norm': 0.30330410599708557, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7717801332473755, 'eval_runtime': 3.5163, 'eval_samples_per_second': 284.387, 'eval_steps_per_second': 17.916, 'epoch': 0.4}
{'loss': 1.0162, 'grad_norm': 0.43424347043037415, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.706895112991333, 'eval_runtime': 3.5105, 'eval_samples_per_second': 284.857, 'eval_steps_per_second': 17.946, 'epoch': 0.44}
{'loss': 1.001, 'grad_norm': 0.3554256856441498, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6847370862960815, 'eval_runtime': 3.5054, 'eval_samples_per_second': 285.278, 'eval_steps_per_second': 17.973, 'epoch': 0.48}
{'loss': 1.0063, 'grad_norm': 0.4021061956882477, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6742814779281616, 'eval_runtime': 3.5065, 'eval_samples_per_second': 285.182, 'eval_steps_per_second': 17.966, 'epoch': 0.52}
{'loss': 0.9752, 'grad_norm': 0.2931535542011261, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6602451205253601, 'eval_runtime': 3.5034, 'eval_samples_per_second': 285.437, 'eval_steps_per_second': 17.983, 'epoch': 0.56}
{'loss': 0.9815, 'grad_norm': 0.29737573862075806, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6385934948921204, 'eval_runtime': 3.5053, 'eval_samples_per_second': 285.286, 'eval_steps_per_second': 17.973, 'epoch': 0.6}
{'loss': 0.9303, 'grad_norm': 0.2689991593360901, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6316344141960144, 'eval_runtime': 3.5101, 'eval_samples_per_second': 284.894, 'eval_steps_per_second': 17.948, 'epoch': 0.64}
{'loss': 0.9506, 'grad_norm': 0.2847844958305359, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.622452437877655, 'eval_runtime': 3.5055, 'eval_samples_per_second': 285.264, 'eval_steps_per_second': 17.972, 'epoch': 0.68}
{'loss': 0.9261, 'grad_norm': 0.35004520416259766, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6063151359558105, 'eval_runtime': 3.4943, 'eval_samples_per_second': 286.183, 'eval_steps_per_second': 18.03, 'epoch': 0.72}
{'loss': 0.902, 'grad_norm': 0.4591401219367981, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5909782648086548, 'eval_runtime': 3.495, 'eval_samples_per_second': 286.126, 'eval_steps_per_second': 18.026, 'epoch': 0.76}
{'loss': 0.8989, 'grad_norm': 0.3574519455432892, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5767666101455688, 'eval_runtime': 3.5134, 'eval_samples_per_second': 284.628, 'eval_steps_per_second': 17.932, 'epoch': 0.8}
{'loss': 0.8898, 'grad_norm': 0.4112713038921356, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5699262619018555, 'eval_runtime': 3.5025, 'eval_samples_per_second': 285.514, 'eval_steps_per_second': 17.987, 'epoch': 0.84}
{'loss': 0.9195, 'grad_norm': 0.41448622941970825, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5577784180641174, 'eval_runtime': 3.4948, 'eval_samples_per_second': 286.138, 'eval_steps_per_second': 18.027, 'epoch': 0.88}
{'loss': 0.8892, 'grad_norm': 0.4416581094264984, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5467764735221863, 'eval_runtime': 3.4985, 'eval_samples_per_second': 285.836, 'eval_steps_per_second': 18.008, 'epoch': 0.92}
{'loss': 0.8695, 'grad_norm': 0.5086541175842285, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5459768176078796, 'eval_runtime': 3.5084, 'eval_samples_per_second': 285.028, 'eval_steps_per_second': 17.957, 'epoch': 0.96}
{'loss': 0.8751, 'grad_norm': 0.6987000107765198, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5458507537841797, 'eval_runtime': 3.5071, 'eval_samples_per_second': 285.135, 'eval_steps_per_second': 17.963, 'epoch': 1.0}
{'train_runtime': 287.0492, 'train_samples_per_second': 34.82, 'train_steps_per_second': 2.177, 'train_loss': 1.097384259033203, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8039968013763428, 1.2227352857589722, 1.1101973056793213, 0.978207528591156, 0.950232982635498, 0.9171174168586731, 0.8918676376342773, 0.8678814172744751, 0.8282552361488342, 0.7717801332473755, 0.706895112991333, 0.6847370862960815, 0.6742814779281616, 0.6602451205253601, 0.6385934948921204, 0.6316344141960144, 0.622452437877655, 0.6063151359558105, 0.5909782648086548, 0.5767666101455688, 0.5699262619018555, 0.5577784180641174, 0.5467764735221863, 0.5459768176078796, 0.5458507537841797], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8039968013763428, 1.2227352857589722, 1.1101973056793213, 0.978207528591156, 0.950232982635498, 0.9171174168586731, 0.8918676376342773, 0.8678814172744751, 0.8282552361488342, 0.7717801332473755, 0.706895112991333, 0.6847370862960815, 0.6742814779281616, 0.6602451205253601, 0.6385934948921204, 0.6316344141960144, 0.622452437877655, 0.6063151359558105, 0.5909782648086548, 0.5767666101455688, 0.5699262619018555, 0.5577784180641174, 0.5467764735221863, 0.5459768176078796, 0.5458507537841797]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1266839504241943
current iteration best possible eval_loss (full train run):  -0.5458507537841797
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9070 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -1.0373051901962986
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -1.0577642369684865
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -1.058947586817548
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -1.0577642368308837
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -1.0593189638174234
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1021, dtype=torch.float64), tensor(0.1832, dtype=torch.float64), tensor(0.0103, dtype=torch.float64), tensor(0.0249, dtype=torch.float64), tensor(0.0705, dtype=torch.float64), 0, 0, tensor(0.1918, dtype=torch.float64), tensor(0.4172, dtype=torch.float64), 12, 1, 1, 1, 1, 1, 125, 0.005753349568666645, 28.989827631196647, 1]
normalized proposed parameters for next round by BO: [tensor(0.1021, dtype=torch.float64), tensor(0.1832, dtype=torch.float64), tensor(0.0103, dtype=torch.float64), tensor(0.0249, dtype=torch.float64), tensor(0.0705, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1918, dtype=torch.float64), tensor(0.4172, dtype=torch.float64), tensor(0.3806, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9768, dtype=torch.float64), tensor(0.0575, dtype=torch.float64), tensor(0.6040, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.102
  gsm8k: 0.183
  rowan_hellaswag: 0.01
  sciq: 0.025
  triviaqa: 0.071
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.192
  arc_challenge: 0.417

LoRA Parameters:
  lora_r: (125,)
  lora_dropout: (0.005753349568666645,)
  num_layers_to_apply: (12,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (28.989827631196647,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  12
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  125
lora dropout:  0.005753349568666645
lora alpha:  28.989827631196647
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 102,912,000 || all params: 8,133,173,248 || trainable%: 1.2653
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7892, 'grad_norm': 0.7154229283332825, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2389888763427734, 'eval_runtime': 3.6804, 'eval_samples_per_second': 271.706, 'eval_steps_per_second': 17.117, 'epoch': 0.04}
{'loss': 1.3542, 'grad_norm': 0.5022342801094055, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3775451183319092, 'eval_runtime': 3.682, 'eval_samples_per_second': 271.591, 'eval_steps_per_second': 17.11, 'epoch': 0.08}
{'loss': 1.1339, 'grad_norm': 0.3058604896068573, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2200570106506348, 'eval_runtime': 3.6755, 'eval_samples_per_second': 272.069, 'eval_steps_per_second': 17.14, 'epoch': 0.12}
{'loss': 1.0631, 'grad_norm': 0.2353370189666748, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.150055170059204, 'eval_runtime': 3.7059, 'eval_samples_per_second': 269.838, 'eval_steps_per_second': 17.0, 'epoch': 0.16}
{'loss': 1.0186, 'grad_norm': 0.2570081353187561, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1226589679718018, 'eval_runtime': 3.6835, 'eval_samples_per_second': 271.482, 'eval_steps_per_second': 17.103, 'epoch': 0.2}
{'loss': 1.0204, 'grad_norm': 0.21926110982894897, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1370707750320435, 'eval_runtime': 3.6878, 'eval_samples_per_second': 271.166, 'eval_steps_per_second': 17.083, 'epoch': 0.24}
{'loss': 1.0396, 'grad_norm': 0.21749870479106903, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.104892373085022, 'eval_runtime': 3.6987, 'eval_samples_per_second': 270.365, 'eval_steps_per_second': 17.033, 'epoch': 0.28}
{'loss': 0.9607, 'grad_norm': 0.15150614082813263, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1052993535995483, 'eval_runtime': 3.6882, 'eval_samples_per_second': 271.135, 'eval_steps_per_second': 17.082, 'epoch': 0.32}
{'loss': 0.9423, 'grad_norm': 0.23878039419651031, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2006444931030273, 'eval_runtime': 3.6908, 'eval_samples_per_second': 270.943, 'eval_steps_per_second': 17.069, 'epoch': 0.36}
{'loss': 0.9852, 'grad_norm': 0.2049742043018341, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1094502210617065, 'eval_runtime': 3.6848, 'eval_samples_per_second': 271.383, 'eval_steps_per_second': 17.097, 'epoch': 0.4}
{'loss': 0.9861, 'grad_norm': 0.28290995955467224, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1669963598251343, 'eval_runtime': 3.7023, 'eval_samples_per_second': 270.101, 'eval_steps_per_second': 17.016, 'epoch': 0.44}
{'loss': 0.9491, 'grad_norm': 0.1987602412700653, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1720818281173706, 'eval_runtime': 3.6912, 'eval_samples_per_second': 270.913, 'eval_steps_per_second': 17.068, 'epoch': 0.48}
{'loss': 0.924, 'grad_norm': 0.21754100918769836, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.176605463027954, 'eval_runtime': 3.6837, 'eval_samples_per_second': 271.467, 'eval_steps_per_second': 17.102, 'epoch': 0.52}
{'loss': 0.9707, 'grad_norm': 0.21948426961898804, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1662341356277466, 'eval_runtime': 3.6784, 'eval_samples_per_second': 271.854, 'eval_steps_per_second': 17.127, 'epoch': 0.56}
{'loss': 0.8962, 'grad_norm': 0.2555828094482422, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1835185289382935, 'eval_runtime': 3.6954, 'eval_samples_per_second': 270.61, 'eval_steps_per_second': 17.048, 'epoch': 0.6}
{'loss': 0.8928, 'grad_norm': 0.2095208615064621, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.179986596107483, 'eval_runtime': 3.677, 'eval_samples_per_second': 271.961, 'eval_steps_per_second': 17.134, 'epoch': 0.64}
{'loss': 0.9249, 'grad_norm': 0.2617220878601074, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1497316360473633, 'eval_runtime': 3.6984, 'eval_samples_per_second': 270.386, 'eval_steps_per_second': 17.034, 'epoch': 0.68}
{'loss': 0.8655, 'grad_norm': 0.24911582469940186, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1501355171203613, 'eval_runtime': 3.6875, 'eval_samples_per_second': 271.184, 'eval_steps_per_second': 17.085, 'epoch': 0.72}
{'loss': 0.9384, 'grad_norm': 0.28814399242401123, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1520975828170776, 'eval_runtime': 3.6901, 'eval_samples_per_second': 270.993, 'eval_steps_per_second': 17.073, 'epoch': 0.76}
{'loss': 0.9029, 'grad_norm': 0.2877036929130554, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1622076034545898, 'eval_runtime': 3.7007, 'eval_samples_per_second': 270.218, 'eval_steps_per_second': 17.024, 'epoch': 0.8}
{'loss': 0.8735, 'grad_norm': 0.287555992603302, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1867756843566895, 'eval_runtime': 3.6896, 'eval_samples_per_second': 271.031, 'eval_steps_per_second': 17.075, 'epoch': 0.84}
{'loss': 0.8586, 'grad_norm': 0.28143420815467834, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2084766626358032, 'eval_runtime': 3.7013, 'eval_samples_per_second': 270.175, 'eval_steps_per_second': 17.021, 'epoch': 0.88}
{'loss': 0.9059, 'grad_norm': 0.30137526988983154, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.187784194946289, 'eval_runtime': 3.7129, 'eval_samples_per_second': 269.328, 'eval_steps_per_second': 16.968, 'epoch': 0.92}
{'loss': 0.8519, 'grad_norm': 0.31659170985221863, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1993298530578613, 'eval_runtime': 3.7612, 'eval_samples_per_second': 265.875, 'eval_steps_per_second': 16.75, 'epoch': 0.96}
{'loss': 0.8541, 'grad_norm': 0.28330421447753906, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1997750997543335, 'eval_runtime': 3.73, 'eval_samples_per_second': 268.1, 'eval_steps_per_second': 16.89, 'epoch': 1.0}
{'train_runtime': 312.3295, 'train_samples_per_second': 32.008, 'train_steps_per_second': 2.001, 'train_loss': 1.0360748962402344, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2389888763427734, 1.3775451183319092, 1.2200570106506348, 1.150055170059204, 1.1226589679718018, 1.1370707750320435, 1.104892373085022, 1.1052993535995483, 1.2006444931030273, 1.1094502210617065, 1.1669963598251343, 1.1720818281173706, 1.176605463027954, 1.1662341356277466, 1.1835185289382935, 1.179986596107483, 1.1497316360473633, 1.1501355171203613, 1.1520975828170776, 1.1622076034545898, 1.1867756843566895, 1.2084766626358032, 1.187784194946289, 1.1993298530578613, 1.1997750997543335], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2389888763427734, 1.3775451183319092, 1.2200570106506348, 1.150055170059204, 1.1226589679718018, 1.1370707750320435, 1.104892373085022, 1.1052993535995483, 1.2006444931030273, 1.1094502210617065, 1.1669963598251343, 1.1720818281173706, 1.176605463027954, 1.1662341356277466, 1.1835185289382935, 1.179986596107483, 1.1497316360473633, 1.1501355171203613, 1.1520975828170776, 1.1622076034545898, 1.1867756843566895, 1.2084766626358032, 1.187784194946289, 1.1993298530578613, 1.1997750997543335]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -1.1997750997543335
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.7576 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -1.0597311376209881
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -1.059713587631031
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -1.0597137842880375
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -1.059713597042013
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -1.0600860145661013
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2093, dtype=torch.float64), tensor(0.1860, dtype=torch.float64), tensor(0.0275, dtype=torch.float64), 0, tensor(0.1097, dtype=torch.float64), tensor(0.0346, dtype=torch.float64), tensor(0.0662, dtype=torch.float64), tensor(0.1449, dtype=torch.float64), tensor(0.2125, dtype=torch.float64), 18, 1, 1, 0, 0, 1, 99, 0.03894786867881531, 27.583586557138233, 1]
normalized proposed parameters for next round by BO: [tensor(0.2093, dtype=torch.float64), tensor(0.1860, dtype=torch.float64), tensor(0.0275, dtype=torch.float64), tensor(0.0094, dtype=torch.float64), tensor(0.1097, dtype=torch.float64), tensor(0.0346, dtype=torch.float64), tensor(0.0662, dtype=torch.float64), tensor(0.1449, dtype=torch.float64), tensor(0.2125, dtype=torch.float64), tensor(0.5691, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7758, dtype=torch.float64), tensor(0.3895, dtype=torch.float64), tensor(0.5747, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.209
  gsm8k: 0.186
  rowan_hellaswag: 0.028
  sciq: 0
  triviaqa: 0.11
  truthfulqa_gen: 0.035
  wikitext: 0.066
  mmlu: 0.145
  arc_challenge: 0.213

LoRA Parameters:
  lora_r: (99,)
  lora_dropout: (0.03894786867881531,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (27.583586557138233,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  99
lora dropout:  0.03894786867881531
lora alpha:  27.583586557138233
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 56,567,808 || all params: 8,086,829,056 || trainable%: 0.6995
length of training data:  9903
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0481, 'grad_norm': 0.47087812423706055, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.3003458976745605, 'eval_runtime': 3.5255, 'eval_samples_per_second': 283.645, 'eval_steps_per_second': 17.87, 'epoch': 0.04}
{'loss': 1.6547, 'grad_norm': 0.267211377620697, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4374135732650757, 'eval_runtime': 3.4957, 'eval_samples_per_second': 286.069, 'eval_steps_per_second': 18.022, 'epoch': 0.08}
{'loss': 1.3691, 'grad_norm': 0.22410811483860016, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 1.3024343252182007, 'eval_runtime': 3.5024, 'eval_samples_per_second': 285.515, 'eval_steps_per_second': 17.987, 'epoch': 0.12}
{'loss': 1.3408, 'grad_norm': 0.2905656695365906, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 1.2315117120742798, 'eval_runtime': 3.4949, 'eval_samples_per_second': 286.127, 'eval_steps_per_second': 18.026, 'epoch': 0.16}
{'loss': 1.199, 'grad_norm': 0.2321702241897583, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 1.1182403564453125, 'eval_runtime': 3.4978, 'eval_samples_per_second': 285.894, 'eval_steps_per_second': 18.011, 'epoch': 0.2}
{'loss': 1.2007, 'grad_norm': 0.27709853649139404, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 1.0759623050689697, 'eval_runtime': 3.5008, 'eval_samples_per_second': 285.651, 'eval_steps_per_second': 17.996, 'epoch': 0.24}
{'loss': 1.1476, 'grad_norm': 0.2526431977748871, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 1.0672341585159302, 'eval_runtime': 3.4997, 'eval_samples_per_second': 285.738, 'eval_steps_per_second': 18.001, 'epoch': 0.28}
{'loss': 1.1635, 'grad_norm': 0.19629505276679993, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 1.0725319385528564, 'eval_runtime': 3.5018, 'eval_samples_per_second': 285.571, 'eval_steps_per_second': 17.991, 'epoch': 0.32}
{'loss': 1.1381, 'grad_norm': 0.1858973652124405, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 1.0250400304794312, 'eval_runtime': 3.5041, 'eval_samples_per_second': 285.38, 'eval_steps_per_second': 17.979, 'epoch': 0.36}
{'loss': 1.1275, 'grad_norm': 0.20599089562892914, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 0.9991092681884766, 'eval_runtime': 3.5081, 'eval_samples_per_second': 285.058, 'eval_steps_per_second': 17.959, 'epoch': 0.4}
{'loss': 1.1273, 'grad_norm': 0.2141924500465393, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 0.9894431829452515, 'eval_runtime': 3.508, 'eval_samples_per_second': 285.061, 'eval_steps_per_second': 17.959, 'epoch': 0.44}
{'loss': 1.1159, 'grad_norm': 0.1957445740699768, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 0.9713445901870728, 'eval_runtime': 3.5037, 'eval_samples_per_second': 285.417, 'eval_steps_per_second': 17.981, 'epoch': 0.48}
{'loss': 1.125, 'grad_norm': 0.19641147553920746, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 0.9879376292228699, 'eval_runtime': 3.4992, 'eval_samples_per_second': 285.781, 'eval_steps_per_second': 18.004, 'epoch': 0.53}
{'loss': 1.1055, 'grad_norm': 0.18729130923748016, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 0.9656816720962524, 'eval_runtime': 3.4906, 'eval_samples_per_second': 286.48, 'eval_steps_per_second': 18.048, 'epoch': 0.57}
{'loss': 1.0151, 'grad_norm': 0.17733831703662872, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 0.9358769059181213, 'eval_runtime': 3.4878, 'eval_samples_per_second': 286.711, 'eval_steps_per_second': 18.063, 'epoch': 0.61}
{'loss': 1.0614, 'grad_norm': 0.18443945050239563, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 0.9171297550201416, 'eval_runtime': 3.4907, 'eval_samples_per_second': 286.477, 'eval_steps_per_second': 18.048, 'epoch': 0.65}
{'loss': 1.0874, 'grad_norm': 0.2149059921503067, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 0.9231433868408203, 'eval_runtime': 3.4892, 'eval_samples_per_second': 286.595, 'eval_steps_per_second': 18.055, 'epoch': 0.69}
{'loss': 1.0287, 'grad_norm': 0.18947070837020874, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 0.8990272283554077, 'eval_runtime': 3.4887, 'eval_samples_per_second': 286.636, 'eval_steps_per_second': 18.058, 'epoch': 0.73}
{'loss': 1.0219, 'grad_norm': 0.2142402082681656, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 0.885779619216919, 'eval_runtime': 3.4886, 'eval_samples_per_second': 286.65, 'eval_steps_per_second': 18.059, 'epoch': 0.77}
{'loss': 1.0694, 'grad_norm': 0.1740957349538803, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 0.8771185278892517, 'eval_runtime': 3.4879, 'eval_samples_per_second': 286.702, 'eval_steps_per_second': 18.062, 'epoch': 0.81}
{'loss': 1.0534, 'grad_norm': 0.21829640865325928, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 0.8667976260185242, 'eval_runtime': 3.4927, 'eval_samples_per_second': 286.314, 'eval_steps_per_second': 18.038, 'epoch': 0.85}
{'loss': 1.0342, 'grad_norm': 0.23604093492031097, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 0.8493886590003967, 'eval_runtime': 3.4931, 'eval_samples_per_second': 286.282, 'eval_steps_per_second': 18.036, 'epoch': 0.89}
{'loss': 1.0235, 'grad_norm': 0.2165723294019699, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 0.8386656641960144, 'eval_runtime': 3.5004, 'eval_samples_per_second': 285.681, 'eval_steps_per_second': 17.998, 'epoch': 0.93}
{'loss': 1.0362, 'grad_norm': 0.2237984836101532, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 0.8316977620124817, 'eval_runtime': 3.5065, 'eval_samples_per_second': 285.188, 'eval_steps_per_second': 17.967, 'epoch': 0.97}
{'train_runtime': 291.7517, 'train_samples_per_second': 33.943, 'train_steps_per_second': 2.122, 'train_loss': 1.215771195961701, 'epoch': 1.0}
train_results:  {'eval_loss': [2.3003458976745605, 1.4374135732650757, 1.3024343252182007, 1.2315117120742798, 1.1182403564453125, 1.0759623050689697, 1.0672341585159302, 1.0725319385528564, 1.0250400304794312, 0.9991092681884766, 0.9894431829452515, 0.9713445901870728, 0.9879376292228699, 0.9656816720962524, 0.9358769059181213, 0.9171297550201416, 0.9231433868408203, 0.8990272283554077, 0.885779619216919, 0.8771185278892517, 0.8667976260185242, 0.8493886590003967, 0.8386656641960144, 0.8316977620124817], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.3003458976745605, 1.4374135732650757, 1.3024343252182007, 1.2315117120742798, 1.1182403564453125, 1.0759623050689697, 1.0672341585159302, 1.0725319385528564, 1.0250400304794312, 0.9991092681884766, 0.9894431829452515, 0.9713445901870728, 0.9879376292228699, 0.9656816720962524, 0.9358769059181213, 0.9171297550201416, 0.9231433868408203, 0.8990272283554077, 0.885779619216919, 0.8771185278892517, 0.8667976260185242, 0.8493886590003967, 0.8386656641960144, 0.8316977620124817]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.8316977620124817
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.3330 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1793595552444458, 0.3586342930793762, 0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.34960928559303284, 0.057854533195495605, 0.7710180282592773, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6546881198883057, 0.3577408194541931, 0.8840495347976685, 0.5400984883308411]  ‚Üí  acq = -1.0475813997048462
X = [0.7988576889038086, 0.1653779149055481, 0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9513463973999023, 0.8898367881774902, 0.2988312840461731, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.24033895134925842, 0.2050917148590088, 0.051226988434791565, 0.8115118741989136]  ‚Üí  acq = -1.0458151731259049
X = [0.22445803880691528, 0.07812052965164185, 0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9895481467247009, 0.6373879313468933, 0.9490465521812439, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6339337825775146, 0.44919973611831665, 0.9787859916687012, 0.45290279388427734]  ‚Üí  acq = -1.0458824042867065
X = [0.3654218316078186, 0.6524207592010498, 0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.8971459269523621, 0.9170647859573364, 0.6597838997840881, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.8601893186569214, 0.164650559425354, 0.14350587129592896, 0.7966952323913574]  ‚Üí  acq = -1.045815178779289
X = [0.4475772976875305, 0.6951091885566711, 0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.06307335197925568, 0.7526708841323853, 0.20946532487869263, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.10499551892280579, 0.07692551612854004, 0.719855546951294, 0.04362046718597412]  ‚Üí  acq = -1.045870444300579
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0341, dtype=torch.float64), tensor(0.1148, dtype=torch.float64), tensor(0.0960, dtype=torch.float64), tensor(0.2081, dtype=torch.float64), 0, tensor(0.2997, dtype=torch.float64), 0, tensor(0.0555, dtype=torch.float64), tensor(0.1903, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 40, 4.792406786123038e-19, 36.93298264054999, 1]
normalized proposed parameters for next round by BO: [tensor(0.0341, dtype=torch.float64), tensor(0.1148, dtype=torch.float64), tensor(0.0960, dtype=torch.float64), tensor(0.2081, dtype=torch.float64), tensor(0.0016, dtype=torch.float64), tensor(0.2997, dtype=torch.float64), tensor(8.2755e-19, dtype=torch.float64), tensor(0.0555, dtype=torch.float64), tensor(0.1903, dtype=torch.float64), tensor(0.4743, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3093, dtype=torch.float64), tensor(4.7924e-18, dtype=torch.float64), tensor(0.7694, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.034
  gsm8k: 0.115
  rowan_hellaswag: 0.096
  sciq: 0.208
  triviaqa: 0
  truthfulqa_gen: 0.3
  wikitext: 0
  mmlu: 0.056
  arc_challenge: 0.19

LoRA Parameters:
  lora_r: (40,)
  lora_dropout: (4.792406786123038e-19,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (36.93298264054999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  40
lora dropout:  4.792406786123038e-19
lora alpha:  36.93298264054999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 25,190,400 || all params: 8,055,451,648 || trainable%: 0.3127
length of training data:  9980
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1041, 'grad_norm': 1.4098005294799805, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.56777024269104, 'eval_runtime': 3.3904, 'eval_samples_per_second': 294.953, 'eval_steps_per_second': 18.582, 'epoch': 0.04}
{'loss': 1.4354, 'grad_norm': 0.7052438259124756, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9782488942146301, 'eval_runtime': 3.3776, 'eval_samples_per_second': 296.067, 'eval_steps_per_second': 18.652, 'epoch': 0.08}
{'loss': 1.1626, 'grad_norm': 0.7061660885810852, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.8759730458259583, 'eval_runtime': 3.3614, 'eval_samples_per_second': 297.494, 'eval_steps_per_second': 18.742, 'epoch': 0.12}
{'loss': 1.1059, 'grad_norm': 0.4898841381072998, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.7813819646835327, 'eval_runtime': 3.3768, 'eval_samples_per_second': 296.141, 'eval_steps_per_second': 18.657, 'epoch': 0.16}
{'loss': 1.1018, 'grad_norm': 0.4154132902622223, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.7390064597129822, 'eval_runtime': 3.3762, 'eval_samples_per_second': 296.194, 'eval_steps_per_second': 18.66, 'epoch': 0.2}
{'loss': 1.0834, 'grad_norm': 0.4687703251838684, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.7257068157196045, 'eval_runtime': 3.3883, 'eval_samples_per_second': 295.133, 'eval_steps_per_second': 18.593, 'epoch': 0.24}
{'loss': 1.0557, 'grad_norm': 0.4212366044521332, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.6896322965621948, 'eval_runtime': 3.3992, 'eval_samples_per_second': 294.184, 'eval_steps_per_second': 18.534, 'epoch': 0.28}
{'loss': 1.033, 'grad_norm': 0.40727755427360535, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.6676496267318726, 'eval_runtime': 3.3933, 'eval_samples_per_second': 294.7, 'eval_steps_per_second': 18.566, 'epoch': 0.32}
{'loss': 0.9911, 'grad_norm': 0.3563210368156433, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.6436454653739929, 'eval_runtime': 3.3923, 'eval_samples_per_second': 294.783, 'eval_steps_per_second': 18.571, 'epoch': 0.36}
{'loss': 1.0205, 'grad_norm': 0.441204309463501, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.6294832825660706, 'eval_runtime': 3.3977, 'eval_samples_per_second': 294.319, 'eval_steps_per_second': 18.542, 'epoch': 0.4}
{'loss': 1.013, 'grad_norm': 0.3601166307926178, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.6161797046661377, 'eval_runtime': 3.3931, 'eval_samples_per_second': 294.714, 'eval_steps_per_second': 18.567, 'epoch': 0.44}
{'loss': 1.0333, 'grad_norm': 0.502074658870697, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.5966534614562988, 'eval_runtime': 3.3933, 'eval_samples_per_second': 294.694, 'eval_steps_per_second': 18.566, 'epoch': 0.48}
{'loss': 1.0022, 'grad_norm': 0.4855441749095917, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.5794108510017395, 'eval_runtime': 3.399, 'eval_samples_per_second': 294.2, 'eval_steps_per_second': 18.535, 'epoch': 0.52}
{'loss': 1.0126, 'grad_norm': 0.44566255807876587, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.5521694421768188, 'eval_runtime': 3.3973, 'eval_samples_per_second': 294.348, 'eval_steps_per_second': 18.544, 'epoch': 0.56}
{'loss': 0.9614, 'grad_norm': 0.37694981694221497, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.5306470394134521, 'eval_runtime': 3.3945, 'eval_samples_per_second': 294.595, 'eval_steps_per_second': 18.559, 'epoch': 0.6}
{'loss': 0.9617, 'grad_norm': 0.5747835636138916, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.5076747536659241, 'eval_runtime': 3.3938, 'eval_samples_per_second': 294.656, 'eval_steps_per_second': 18.563, 'epoch': 0.64}
{'loss': 0.9656, 'grad_norm': 0.4423521161079407, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.492631196975708, 'eval_runtime': 3.3899, 'eval_samples_per_second': 294.991, 'eval_steps_per_second': 18.584, 'epoch': 0.68}
{'loss': 0.992, 'grad_norm': 0.45058637857437134, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.48329129815101624, 'eval_runtime': 3.3903, 'eval_samples_per_second': 294.957, 'eval_steps_per_second': 18.582, 'epoch': 0.72}
{'loss': 0.9855, 'grad_norm': 0.4097370505332947, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.4675447940826416, 'eval_runtime': 3.3947, 'eval_samples_per_second': 294.575, 'eval_steps_per_second': 18.558, 'epoch': 0.76}
{'loss': 1.0132, 'grad_norm': 0.4174502491950989, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.44284722208976746, 'eval_runtime': 3.4022, 'eval_samples_per_second': 293.926, 'eval_steps_per_second': 18.517, 'epoch': 0.8}
{'loss': 0.9687, 'grad_norm': 0.5000145435333252, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.4223769009113312, 'eval_runtime': 3.3926, 'eval_samples_per_second': 294.762, 'eval_steps_per_second': 18.57, 'epoch': 0.84}
{'loss': 0.9877, 'grad_norm': 0.6227294206619263, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.41194799542427063, 'eval_runtime': 3.3894, 'eval_samples_per_second': 295.041, 'eval_steps_per_second': 18.588, 'epoch': 0.88}
{'loss': 1.0031, 'grad_norm': 0.4404549300670624, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.39951568841934204, 'eval_runtime': 3.3868, 'eval_samples_per_second': 295.262, 'eval_steps_per_second': 18.602, 'epoch': 0.92}
{'loss': 0.9377, 'grad_norm': 0.4249715507030487, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.39640137553215027, 'eval_runtime': 3.3787, 'eval_samples_per_second': 295.969, 'eval_steps_per_second': 18.646, 'epoch': 0.96}
{'train_runtime': 285.5613, 'train_samples_per_second': 34.949, 'train_steps_per_second': 2.185, 'train_loss': 1.1150158552023082, 'epoch': 1.0}
train_results:  {'eval_loss': [1.56777024269104, 0.9782488942146301, 0.8759730458259583, 0.7813819646835327, 0.7390064597129822, 0.7257068157196045, 0.6896322965621948, 0.6676496267318726, 0.6436454653739929, 0.6294832825660706, 0.6161797046661377, 0.5966534614562988, 0.5794108510017395, 0.5521694421768188, 0.5306470394134521, 0.5076747536659241, 0.492631196975708, 0.48329129815101624, 0.4675447940826416, 0.44284722208976746, 0.4223769009113312, 0.41194799542427063, 0.39951568841934204, 0.39640137553215027], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.56777024269104, 0.9782488942146301, 0.8759730458259583, 0.7813819646835327, 0.7390064597129822, 0.7257068157196045, 0.6896322965621948, 0.6676496267318726, 0.6436454653739929, 0.6294832825660706, 0.6161797046661377, 0.5966534614562988, 0.5794108510017395, 0.5521694421768188, 0.5306470394134521, 0.5076747536659241, 0.492631196975708, 0.48329129815101624, 0.4675447940826416, 0.44284722208976746, 0.4223769009113312, 0.41194799542427063, 0.39951568841934204, 0.39640137553215027]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.39640137553215027
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.0989 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7418680787086487, 0.34305238723754883, 0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.24965649843215942, 0.4866626262664795, 0.6463397145271301, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.6175775527954102, 0.9655588865280151, 0.30760660767555237, 0.14850598573684692]  ‚Üí  acq = -1.0478781703293791
X = [0.5951085090637207, 0.6337894201278687, 0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.14596298336982727, 0.1414751410484314, 0.14421969652175903, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2858731746673584, 0.8522514700889587, 0.9279651641845703, 0.5002951622009277]  ‚Üí  acq = -1.0478521157519822
X = [0.5818041563034058, 0.9915117621421814, 0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.12225708365440369, 0.8182916045188904, 0.2420574426651001, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6762534976005554, 0.9445821046829224, 0.28039222955703735, 0.026326775550842285]  ‚Üí  acq = -1.047852115751983
X = [0.375633180141449, 0.817596435546875, 0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9535425305366516, 0.6659542918205261, 0.4328734874725342, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6761766076087952, 0.7598890066146851, 0.8752217292785645, 0.10180890560150146]  ‚Üí  acq = -1.034208113049854
X = [0.13599658012390137, 0.6423792243003845, 0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9463992118835449, 0.9866945743560791, 0.903969407081604, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.46579673886299133, 0.231636643409729, 0.8483097553253174, 0.5441073775291443]  ‚Üí  acq = -1.0478521157519822
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0555, dtype=torch.float64), tensor(0.0868, dtype=torch.float64), tensor(0.1813, dtype=torch.float64), 0, tensor(0.4789, dtype=torch.float64), 0, tensor(0.0507, dtype=torch.float64), tensor(0.1468, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 128, 6.130700940266754e-21, 28.170077246134205, 1]
normalized proposed parameters for next round by BO: [tensor(3.6193e-18, dtype=torch.float64), tensor(0.0555, dtype=torch.float64), tensor(0.0868, dtype=torch.float64), tensor(0.1813, dtype=torch.float64), tensor(3.2740e-18, dtype=torch.float64), tensor(0.4789, dtype=torch.float64), tensor(1.8147e-18, dtype=torch.float64), tensor(0.0507, dtype=torch.float64), tensor(0.1468, dtype=torch.float64), tensor(0.4724, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(6.1307e-20, dtype=torch.float64), tensor(0.5869, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.056
  rowan_hellaswag: 0.087
  sciq: 0.181
  triviaqa: 0
  truthfulqa_gen: 0.479
  wikitext: 0
  mmlu: 0.051
  arc_challenge: 0.147

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (6.130700940266754e-21,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (28.170077246134205,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  6.130700940266754e-21
lora alpha:  28.170077246134205
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 80,609,280 || all params: 8,110,870,528 || trainable%: 0.9938
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4348, 'grad_norm': 0.7300954461097717, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5963586568832397, 'eval_runtime': 3.4003, 'eval_samples_per_second': 294.096, 'eval_steps_per_second': 18.528, 'epoch': 0.04}
{'loss': 1.5188, 'grad_norm': 0.9898618459701538, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9833471775054932, 'eval_runtime': 3.4109, 'eval_samples_per_second': 293.174, 'eval_steps_per_second': 18.47, 'epoch': 0.08}
{'loss': 1.2317, 'grad_norm': 0.2897897958755493, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.880252480506897, 'eval_runtime': 3.423, 'eval_samples_per_second': 292.138, 'eval_steps_per_second': 18.405, 'epoch': 0.12}
{'loss': 1.0685, 'grad_norm': 0.34040406346321106, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7761901617050171, 'eval_runtime': 3.4032, 'eval_samples_per_second': 293.839, 'eval_steps_per_second': 18.512, 'epoch': 0.16}
{'loss': 1.0864, 'grad_norm': 0.3073302209377289, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7109829187393188, 'eval_runtime': 3.407, 'eval_samples_per_second': 293.515, 'eval_steps_per_second': 18.491, 'epoch': 0.2}
{'loss': 1.0158, 'grad_norm': 0.24011875689029694, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6842076182365417, 'eval_runtime': 3.4053, 'eval_samples_per_second': 293.662, 'eval_steps_per_second': 18.501, 'epoch': 0.24}
{'loss': 1.035, 'grad_norm': 0.26364457607269287, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6527870893478394, 'eval_runtime': 3.4099, 'eval_samples_per_second': 293.266, 'eval_steps_per_second': 18.476, 'epoch': 0.28}
{'loss': 1.0229, 'grad_norm': 0.2729688882827759, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6322459578514099, 'eval_runtime': 3.4098, 'eval_samples_per_second': 293.272, 'eval_steps_per_second': 18.476, 'epoch': 0.32}
{'loss': 0.9911, 'grad_norm': 0.20906230807304382, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5943118333816528, 'eval_runtime': 3.4096, 'eval_samples_per_second': 293.285, 'eval_steps_per_second': 18.477, 'epoch': 0.36}
{'loss': 1.0271, 'grad_norm': 0.2123885601758957, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5699942708015442, 'eval_runtime': 3.418, 'eval_samples_per_second': 292.569, 'eval_steps_per_second': 18.432, 'epoch': 0.4}
{'loss': 0.9345, 'grad_norm': 0.3018132448196411, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.539503812789917, 'eval_runtime': 3.4071, 'eval_samples_per_second': 293.504, 'eval_steps_per_second': 18.491, 'epoch': 0.44}
{'loss': 1.0297, 'grad_norm': 0.24510693550109863, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.51810222864151, 'eval_runtime': 3.394, 'eval_samples_per_second': 294.635, 'eval_steps_per_second': 18.562, 'epoch': 0.48}
{'loss': 0.9549, 'grad_norm': 0.2659316658973694, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5051372051239014, 'eval_runtime': 3.4003, 'eval_samples_per_second': 294.091, 'eval_steps_per_second': 18.528, 'epoch': 0.52}
{'loss': 0.9437, 'grad_norm': 0.2793065309524536, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.47781094908714294, 'eval_runtime': 3.4029, 'eval_samples_per_second': 293.869, 'eval_steps_per_second': 18.514, 'epoch': 0.56}
{'loss': 0.9419, 'grad_norm': 0.2645702660083771, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4427575469017029, 'eval_runtime': 3.4205, 'eval_samples_per_second': 292.354, 'eval_steps_per_second': 18.418, 'epoch': 0.6}
{'loss': 0.8791, 'grad_norm': 0.29513639211654663, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.425118625164032, 'eval_runtime': 3.4062, 'eval_samples_per_second': 293.582, 'eval_steps_per_second': 18.496, 'epoch': 0.64}
{'loss': 0.8952, 'grad_norm': 0.2891770005226135, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3957515060901642, 'eval_runtime': 3.3989, 'eval_samples_per_second': 294.216, 'eval_steps_per_second': 18.536, 'epoch': 0.68}
{'loss': 0.8883, 'grad_norm': 0.38552752137184143, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3725025951862335, 'eval_runtime': 3.415, 'eval_samples_per_second': 292.823, 'eval_steps_per_second': 18.448, 'epoch': 0.72}
{'loss': 0.9214, 'grad_norm': 0.37738195061683655, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3533115088939667, 'eval_runtime': 3.4, 'eval_samples_per_second': 294.121, 'eval_steps_per_second': 18.53, 'epoch': 0.76}
{'loss': 0.9363, 'grad_norm': 0.39244380593299866, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3403249979019165, 'eval_runtime': 3.4003, 'eval_samples_per_second': 294.088, 'eval_steps_per_second': 18.528, 'epoch': 0.8}
{'loss': 0.9284, 'grad_norm': 0.30103209614753723, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.32028016448020935, 'eval_runtime': 3.3937, 'eval_samples_per_second': 294.661, 'eval_steps_per_second': 18.564, 'epoch': 0.84}
{'loss': 0.9794, 'grad_norm': 0.3435564339160919, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3026866316795349, 'eval_runtime': 3.3976, 'eval_samples_per_second': 294.33, 'eval_steps_per_second': 18.543, 'epoch': 0.88}
{'loss': 0.853, 'grad_norm': 0.24198417365550995, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2972392439842224, 'eval_runtime': 3.4009, 'eval_samples_per_second': 294.041, 'eval_steps_per_second': 18.525, 'epoch': 0.92}
{'loss': 0.8489, 'grad_norm': 0.3483681380748749, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.28067705035209656, 'eval_runtime': 3.4018, 'eval_samples_per_second': 293.965, 'eval_steps_per_second': 18.52, 'epoch': 0.96}
{'loss': 0.81, 'grad_norm': 0.27640262246131897, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2762918174266815, 'eval_runtime': 3.3989, 'eval_samples_per_second': 294.213, 'eval_steps_per_second': 18.535, 'epoch': 1.0}
{'train_runtime': 281.2054, 'train_samples_per_second': 35.551, 'train_steps_per_second': 2.223, 'train_loss': 1.0870797912597656, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5963586568832397, 0.9833471775054932, 0.880252480506897, 0.7761901617050171, 0.7109829187393188, 0.6842076182365417, 0.6527870893478394, 0.6322459578514099, 0.5943118333816528, 0.5699942708015442, 0.539503812789917, 0.51810222864151, 0.5051372051239014, 0.47781094908714294, 0.4427575469017029, 0.425118625164032, 0.3957515060901642, 0.3725025951862335, 0.3533115088939667, 0.3403249979019165, 0.32028016448020935, 0.3026866316795349, 0.2972392439842224, 0.28067705035209656, 0.2762918174266815], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5963586568832397, 0.9833471775054932, 0.880252480506897, 0.7761901617050171, 0.7109829187393188, 0.6842076182365417, 0.6527870893478394, 0.6322459578514099, 0.5943118333816528, 0.5699942708015442, 0.539503812789917, 0.51810222864151, 0.5051372051239014, 0.47781094908714294, 0.4427575469017029, 0.425118625164032, 0.3957515060901642, 0.3725025951862335, 0.3533115088939667, 0.3403249979019165, 0.32028016448020935, 0.3026866316795349, 0.2972392439842224, 0.28067705035209656, 0.2762918174266815]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.2762918174266815
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.7469 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -1.0633339447362236
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -1.0633259973408065
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -1.0633259972979583
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -1.0651390749294676
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -1.0636933608392172
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1415, dtype=torch.float64), tensor(0.0659, dtype=torch.float64), tensor(0.1837, dtype=torch.float64), tensor(0.0273, dtype=torch.float64), tensor(0.4494, dtype=torch.float64), 0, tensor(0.0294, dtype=torch.float64), tensor(0.1027, dtype=torch.float64), 14, 0, 1, 0, 1, 1, 50, 0.004471980819431203, 38.30824810975315, 1]
normalized proposed parameters for next round by BO: [tensor(3.6304e-19, dtype=torch.float64), tensor(0.1415, dtype=torch.float64), tensor(0.0659, dtype=torch.float64), tensor(0.1837, dtype=torch.float64), tensor(0.0273, dtype=torch.float64), tensor(0.4494, dtype=torch.float64), tensor(9.1600e-19, dtype=torch.float64), tensor(0.0294, dtype=torch.float64), tensor(0.1027, dtype=torch.float64), tensor(0.4513, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3943, dtype=torch.float64), tensor(0.0447, dtype=torch.float64), tensor(0.7981, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.141
  rowan_hellaswag: 0.066
  sciq: 0.184
  triviaqa: 0.027
  truthfulqa_gen: 0.449
  wikitext: 0
  mmlu: 0.029
  arc_challenge: 0.103

LoRA Parameters:
  lora_r: (50,)
  lora_dropout: (0.004471980819431203,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (38.30824810975315,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  50
lora dropout:  0.004471980819431203
lora alpha:  38.30824810975315
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 29,388,800 || all params: 8,059,650,048 || trainable%: 0.3646
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0698, 'grad_norm': 0.9570411443710327, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5302685499191284, 'eval_runtime': 3.3828, 'eval_samples_per_second': 295.613, 'eval_steps_per_second': 18.624, 'epoch': 0.04}
{'loss': 1.5162, 'grad_norm': 0.9263088703155518, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9665940999984741, 'eval_runtime': 3.3866, 'eval_samples_per_second': 295.281, 'eval_steps_per_second': 18.603, 'epoch': 0.08}
{'loss': 1.1162, 'grad_norm': 0.4540342092514038, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8440249562263489, 'eval_runtime': 3.4028, 'eval_samples_per_second': 293.879, 'eval_steps_per_second': 18.514, 'epoch': 0.12}
{'loss': 1.0066, 'grad_norm': 0.47415676712989807, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.740509033203125, 'eval_runtime': 3.415, 'eval_samples_per_second': 292.827, 'eval_steps_per_second': 18.448, 'epoch': 0.16}
{'loss': 1.0222, 'grad_norm': 0.3657289147377014, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.705288290977478, 'eval_runtime': 3.4081, 'eval_samples_per_second': 293.416, 'eval_steps_per_second': 18.485, 'epoch': 0.2}
{'loss': 1.0133, 'grad_norm': 0.43217191100120544, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.66865074634552, 'eval_runtime': 3.374, 'eval_samples_per_second': 296.388, 'eval_steps_per_second': 18.672, 'epoch': 0.24}
{'loss': 0.9973, 'grad_norm': 0.3927762508392334, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6338895559310913, 'eval_runtime': 3.3953, 'eval_samples_per_second': 294.521, 'eval_steps_per_second': 18.555, 'epoch': 0.28}
{'loss': 0.9701, 'grad_norm': 0.42261865735054016, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6133502125740051, 'eval_runtime': 3.4001, 'eval_samples_per_second': 294.108, 'eval_steps_per_second': 18.529, 'epoch': 0.32}
{'loss': 0.9455, 'grad_norm': 0.4739740788936615, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5852967500686646, 'eval_runtime': 3.408, 'eval_samples_per_second': 293.432, 'eval_steps_per_second': 18.486, 'epoch': 0.36}
{'loss': 0.9094, 'grad_norm': 0.49159157276153564, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5514965057373047, 'eval_runtime': 3.4035, 'eval_samples_per_second': 293.816, 'eval_steps_per_second': 18.51, 'epoch': 0.4}
{'loss': 0.9212, 'grad_norm': 0.5886021852493286, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5158352255821228, 'eval_runtime': 3.3931, 'eval_samples_per_second': 294.715, 'eval_steps_per_second': 18.567, 'epoch': 0.44}
{'loss': 0.9206, 'grad_norm': 0.4558666944503784, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4965575635433197, 'eval_runtime': 3.3933, 'eval_samples_per_second': 294.702, 'eval_steps_per_second': 18.566, 'epoch': 0.48}
{'loss': 0.9587, 'grad_norm': 0.32748883962631226, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4798435568809509, 'eval_runtime': 3.3873, 'eval_samples_per_second': 295.222, 'eval_steps_per_second': 18.599, 'epoch': 0.52}
{'loss': 0.9365, 'grad_norm': 0.554597795009613, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.45165279507637024, 'eval_runtime': 3.4052, 'eval_samples_per_second': 293.668, 'eval_steps_per_second': 18.501, 'epoch': 0.56}
{'loss': 0.8887, 'grad_norm': 0.5304741859436035, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4250449538230896, 'eval_runtime': 3.4244, 'eval_samples_per_second': 292.019, 'eval_steps_per_second': 18.397, 'epoch': 0.6}
{'loss': 0.9074, 'grad_norm': 0.45852842926979065, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.403695672750473, 'eval_runtime': 3.4182, 'eval_samples_per_second': 292.554, 'eval_steps_per_second': 18.431, 'epoch': 0.64}
{'loss': 0.8275, 'grad_norm': 0.6046775579452515, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.38070353865623474, 'eval_runtime': 3.4275, 'eval_samples_per_second': 291.755, 'eval_steps_per_second': 18.381, 'epoch': 0.68}
{'loss': 0.8208, 'grad_norm': 0.4377348721027374, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3654293119907379, 'eval_runtime': 3.4264, 'eval_samples_per_second': 291.849, 'eval_steps_per_second': 18.386, 'epoch': 0.72}
{'loss': 0.8263, 'grad_norm': 0.8148157596588135, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3440850079059601, 'eval_runtime': 3.4241, 'eval_samples_per_second': 292.047, 'eval_steps_per_second': 18.399, 'epoch': 0.76}
{'loss': 0.8239, 'grad_norm': 0.3590856194496155, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3328917920589447, 'eval_runtime': 3.43, 'eval_samples_per_second': 291.547, 'eval_steps_per_second': 18.367, 'epoch': 0.8}
{'loss': 0.8337, 'grad_norm': 0.7050122618675232, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3159988820552826, 'eval_runtime': 3.4358, 'eval_samples_per_second': 291.057, 'eval_steps_per_second': 18.337, 'epoch': 0.84}
{'loss': 0.8396, 'grad_norm': 0.5128244161605835, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3026903569698334, 'eval_runtime': 3.4285, 'eval_samples_per_second': 291.67, 'eval_steps_per_second': 18.375, 'epoch': 0.88}
{'loss': 0.8372, 'grad_norm': 0.5440403819084167, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2879444360733032, 'eval_runtime': 3.4374, 'eval_samples_per_second': 290.915, 'eval_steps_per_second': 18.328, 'epoch': 0.92}
{'loss': 0.8333, 'grad_norm': 0.42395514249801636, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2780737280845642, 'eval_runtime': 3.4484, 'eval_samples_per_second': 289.986, 'eval_steps_per_second': 18.269, 'epoch': 0.96}
{'loss': 0.8527, 'grad_norm': 0.6173385977745056, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2743092477321625, 'eval_runtime': 3.4411, 'eval_samples_per_second': 290.607, 'eval_steps_per_second': 18.308, 'epoch': 1.0}
{'train_runtime': 284.2205, 'train_samples_per_second': 35.17, 'train_steps_per_second': 2.199, 'train_loss': 1.0237830291748047, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5302685499191284, 0.9665940999984741, 0.8440249562263489, 0.740509033203125, 0.705288290977478, 0.66865074634552, 0.6338895559310913, 0.6133502125740051, 0.5852967500686646, 0.5514965057373047, 0.5158352255821228, 0.4965575635433197, 0.4798435568809509, 0.45165279507637024, 0.4250449538230896, 0.403695672750473, 0.38070353865623474, 0.3654293119907379, 0.3440850079059601, 0.3328917920589447, 0.3159988820552826, 0.3026903569698334, 0.2879444360733032, 0.2780737280845642, 0.2743092477321625], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5302685499191284, 0.9665940999984741, 0.8440249562263489, 0.740509033203125, 0.705288290977478, 0.66865074634552, 0.6338895559310913, 0.6133502125740051, 0.5852967500686646, 0.5514965057373047, 0.5158352255821228, 0.4965575635433197, 0.4798435568809509, 0.45165279507637024, 0.4250449538230896, 0.403695672750473, 0.38070353865623474, 0.3654293119907379, 0.3440850079059601, 0.3328917920589447, 0.3159988820552826, 0.3026903569698334, 0.2879444360733032, 0.2780737280845642, 0.2743092477321625]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.126366376876831
current iteration best possible eval_loss (full train run):  -0.2743092477321625
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.126366376876831]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.6630 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -1.0761374620567217
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -1.0766922994684345
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -1.0680292722172169
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -1.0761374620367108
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -1.076137462032015
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1178, dtype=torch.float64), tensor(0.0807, dtype=torch.float64), tensor(0.0875, dtype=torch.float64), tensor(0.1765, dtype=torch.float64), tensor(0.0356, dtype=torch.float64), tensor(0.0847, dtype=torch.float64), tensor(0.0510, dtype=torch.float64), tensor(0.0917, dtype=torch.float64), tensor(0.2745, dtype=torch.float64), 19, 0, 1, 0, 1, 1, 42, 0.0, 27.859450368071677, 1]
normalized proposed parameters for next round by BO: [tensor(0.1178, dtype=torch.float64), tensor(0.0807, dtype=torch.float64), tensor(0.0875, dtype=torch.float64), tensor(0.1765, dtype=torch.float64), tensor(0.0356, dtype=torch.float64), tensor(0.0847, dtype=torch.float64), tensor(0.0510, dtype=torch.float64), tensor(0.0917, dtype=torch.float64), tensor(0.2745, dtype=torch.float64), tensor(0.6093, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3297, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5804, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.118
  gsm8k: 0.081
  rowan_hellaswag: 0.088
  sciq: 0.176
  triviaqa: 0.036
  truthfulqa_gen: 0.085
  wikitext: 0.051
  mmlu: 0.092
  arc_challenge: 0.274

LoRA Parameters:
  lora_r: (42,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (27.859450368071677,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  42
lora dropout:  0.0
lora alpha:  27.859450368071677
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 33,503,232 || all params: 8,063,764,480 || trainable%: 0.4155
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1255, 'grad_norm': 1.4306674003601074, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6350449323654175, 'eval_runtime': 3.4819, 'eval_samples_per_second': 287.203, 'eval_steps_per_second': 18.094, 'epoch': 0.04}
{'loss': 1.4721, 'grad_norm': 0.5582910776138306, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0695574283599854, 'eval_runtime': 3.4675, 'eval_samples_per_second': 288.389, 'eval_steps_per_second': 18.169, 'epoch': 0.08}
{'loss': 1.3209, 'grad_norm': 0.46873801946640015, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.00723135471344, 'eval_runtime': 3.4762, 'eval_samples_per_second': 287.669, 'eval_steps_per_second': 18.123, 'epoch': 0.12}
{'loss': 1.186, 'grad_norm': 0.5643136501312256, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9282103180885315, 'eval_runtime': 3.4891, 'eval_samples_per_second': 286.609, 'eval_steps_per_second': 18.056, 'epoch': 0.16}
{'loss': 1.1716, 'grad_norm': 0.4507647156715393, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8736888766288757, 'eval_runtime': 3.4839, 'eval_samples_per_second': 287.034, 'eval_steps_per_second': 18.083, 'epoch': 0.2}
{'loss': 1.2056, 'grad_norm': 0.523685097694397, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8586440086364746, 'eval_runtime': 3.4772, 'eval_samples_per_second': 287.584, 'eval_steps_per_second': 18.118, 'epoch': 0.24}
{'loss': 1.1153, 'grad_norm': 0.34646183252334595, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8083658218383789, 'eval_runtime': 3.4851, 'eval_samples_per_second': 286.939, 'eval_steps_per_second': 18.077, 'epoch': 0.28}
{'loss': 1.1354, 'grad_norm': 0.3317139744758606, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8182288408279419, 'eval_runtime': 3.4882, 'eval_samples_per_second': 286.681, 'eval_steps_per_second': 18.061, 'epoch': 0.32}
{'loss': 1.0893, 'grad_norm': 0.394015371799469, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8187107443809509, 'eval_runtime': 3.4974, 'eval_samples_per_second': 285.931, 'eval_steps_per_second': 18.014, 'epoch': 0.36}
{'loss': 1.1499, 'grad_norm': 0.3649672865867615, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7923069000244141, 'eval_runtime': 3.5, 'eval_samples_per_second': 285.711, 'eval_steps_per_second': 18.0, 'epoch': 0.4}
{'loss': 1.1117, 'grad_norm': 0.6032897233963013, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7821474671363831, 'eval_runtime': 3.5277, 'eval_samples_per_second': 283.474, 'eval_steps_per_second': 17.859, 'epoch': 0.44}
{'loss': 1.0881, 'grad_norm': 0.3144773542881012, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7827932834625244, 'eval_runtime': 3.5019, 'eval_samples_per_second': 285.561, 'eval_steps_per_second': 17.99, 'epoch': 0.48}
{'loss': 1.114, 'grad_norm': 0.412107914686203, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7590909004211426, 'eval_runtime': 3.507, 'eval_samples_per_second': 285.14, 'eval_steps_per_second': 17.964, 'epoch': 0.52}
{'loss': 1.1021, 'grad_norm': 0.35536226630210876, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7542392611503601, 'eval_runtime': 3.5007, 'eval_samples_per_second': 285.657, 'eval_steps_per_second': 17.996, 'epoch': 0.56}
{'loss': 1.0886, 'grad_norm': 0.508554220199585, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7494917511940002, 'eval_runtime': 3.5028, 'eval_samples_per_second': 285.488, 'eval_steps_per_second': 17.986, 'epoch': 0.6}
{'loss': 1.0731, 'grad_norm': 0.3499254882335663, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7325068116188049, 'eval_runtime': 3.5007, 'eval_samples_per_second': 285.66, 'eval_steps_per_second': 17.997, 'epoch': 0.64}
{'loss': 1.0442, 'grad_norm': 0.5768706798553467, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7365983724594116, 'eval_runtime': 3.5116, 'eval_samples_per_second': 284.774, 'eval_steps_per_second': 17.941, 'epoch': 0.68}
{'loss': 1.0322, 'grad_norm': 0.36320069432258606, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7244968414306641, 'eval_runtime': 3.5075, 'eval_samples_per_second': 285.105, 'eval_steps_per_second': 17.962, 'epoch': 0.72}
{'loss': 1.0504, 'grad_norm': 0.42810603976249695, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.714225709438324, 'eval_runtime': 3.5009, 'eval_samples_per_second': 285.641, 'eval_steps_per_second': 17.995, 'epoch': 0.76}
{'loss': 1.0068, 'grad_norm': 0.4574088156223297, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7092023491859436, 'eval_runtime': 3.4981, 'eval_samples_per_second': 285.868, 'eval_steps_per_second': 18.01, 'epoch': 0.8}
{'loss': 1.0015, 'grad_norm': 0.4851360023021698, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.71103835105896, 'eval_runtime': 3.5054, 'eval_samples_per_second': 285.271, 'eval_steps_per_second': 17.972, 'epoch': 0.84}
{'loss': 1.0209, 'grad_norm': 0.5206583142280579, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.699457049369812, 'eval_runtime': 3.5108, 'eval_samples_per_second': 284.831, 'eval_steps_per_second': 17.944, 'epoch': 0.88}
{'loss': 0.9514, 'grad_norm': 0.6111356019973755, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6952844262123108, 'eval_runtime': 3.5331, 'eval_samples_per_second': 283.04, 'eval_steps_per_second': 17.832, 'epoch': 0.92}
{'loss': 1.0315, 'grad_norm': 0.4480317234992981, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6947395205497742, 'eval_runtime': 3.5281, 'eval_samples_per_second': 283.441, 'eval_steps_per_second': 17.857, 'epoch': 0.96}
{'loss': 0.9794, 'grad_norm': 0.5124630928039551, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6929773092269897, 'eval_runtime': 3.5188, 'eval_samples_per_second': 284.188, 'eval_steps_per_second': 17.904, 'epoch': 1.0}
{'train_runtime': 295.2008, 'train_samples_per_second': 33.858, 'train_steps_per_second': 2.117, 'train_loss': 1.18669453125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6350449323654175, 1.0695574283599854, 1.00723135471344, 0.9282103180885315, 0.8736888766288757, 0.8586440086364746, 0.8083658218383789, 0.8182288408279419, 0.8187107443809509, 0.7923069000244141, 0.7821474671363831, 0.7827932834625244, 0.7590909004211426, 0.7542392611503601, 0.7494917511940002, 0.7325068116188049, 0.7365983724594116, 0.7244968414306641, 0.714225709438324, 0.7092023491859436, 0.71103835105896, 0.699457049369812, 0.6952844262123108, 0.6947395205497742, 0.6929773092269897], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6350449323654175, 1.0695574283599854, 1.00723135471344, 0.9282103180885315, 0.8736888766288757, 0.8586440086364746, 0.8083658218383789, 0.8182288408279419, 0.8187107443809509, 0.7923069000244141, 0.7821474671363831, 0.7827932834625244, 0.7590909004211426, 0.7542392611503601, 0.7494917511940002, 0.7325068116188049, 0.7365983724594116, 0.7244968414306641, 0.714225709438324, 0.7092023491859436, 0.71103835105896, 0.699457049369812, 0.6952844262123108, 0.6947395205497742, 0.6929773092269897]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1271183490753174
current iteration best possible eval_loss (full train run):  -0.6929773092269897
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.126366376876831, -1.1271183490753174]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.5934 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -1.063149532243912
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -1.0631503691734867
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -1.063169807757315
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -1.0631493662225442
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -1.0584453498625819
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1361, dtype=torch.float64), tensor(0.2064, dtype=torch.float64), 0, tensor(0.0925, dtype=torch.float64), tensor(0.0878, dtype=torch.float64), tensor(0.1015, dtype=torch.float64), 0, tensor(0.1311, dtype=torch.float64), tensor(0.2446, dtype=torch.float64), 13, 1, 0, 0, 1, 0, 22, 0.07692924628218017, 16.720352388457265, 1]
normalized proposed parameters for next round by BO: [tensor(0.1361, dtype=torch.float64), tensor(0.2064, dtype=torch.float64), tensor(1.8959e-17, dtype=torch.float64), tensor(0.0925, dtype=torch.float64), tensor(0.0878, dtype=torch.float64), tensor(0.1015, dtype=torch.float64), tensor(7.2394e-18, dtype=torch.float64), tensor(0.1311, dtype=torch.float64), tensor(0.2446, dtype=torch.float64), tensor(0.4207, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1696, dtype=torch.float64), tensor(0.7693, dtype=torch.float64), tensor(0.3483, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.136
  gsm8k: 0.206
  rowan_hellaswag: 0
  sciq: 0.092
  triviaqa: 0.088
  truthfulqa_gen: 0.102
  wikitext: 0
  mmlu: 0.131
  arc_challenge: 0.245

LoRA Parameters:
  lora_r: (22,)
  lora_dropout: (0.07692924628218017,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([1, 0, 0, 1, 0],)
  lora_alpha: (16.720352388457265,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 0]
lora rank:  22
lora dropout:  0.07692924628218017
lora alpha:  16.720352388457265
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 7,614,464 || all params: 8,037,875,712 || trainable%: 0.0947
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2614, 'grad_norm': 2.2356441020965576, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.774556875228882, 'eval_runtime': 3.2977, 'eval_samples_per_second': 303.242, 'eval_steps_per_second': 19.104, 'epoch': 0.04}
{'loss': 1.6909, 'grad_norm': 0.8502119183540344, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.395309567451477, 'eval_runtime': 3.3075, 'eval_samples_per_second': 302.34, 'eval_steps_per_second': 19.047, 'epoch': 0.08}
{'loss': 1.2126, 'grad_norm': 0.608973503112793, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.083833932876587, 'eval_runtime': 3.3211, 'eval_samples_per_second': 301.101, 'eval_steps_per_second': 18.969, 'epoch': 0.12}
{'loss': 1.1212, 'grad_norm': 0.544796347618103, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0044969320297241, 'eval_runtime': 3.3095, 'eval_samples_per_second': 302.162, 'eval_steps_per_second': 19.036, 'epoch': 0.16}
{'loss': 1.0374, 'grad_norm': 0.4734809994697571, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9591038823127747, 'eval_runtime': 3.3064, 'eval_samples_per_second': 302.446, 'eval_steps_per_second': 19.054, 'epoch': 0.2}
{'loss': 1.024, 'grad_norm': 0.4047228991985321, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9172282218933105, 'eval_runtime': 3.3118, 'eval_samples_per_second': 301.952, 'eval_steps_per_second': 19.023, 'epoch': 0.24}
{'loss': 1.0337, 'grad_norm': 0.4827761650085449, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8886893391609192, 'eval_runtime': 3.32, 'eval_samples_per_second': 301.209, 'eval_steps_per_second': 18.976, 'epoch': 0.28}
{'loss': 0.9632, 'grad_norm': 0.3804079294204712, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8947635293006897, 'eval_runtime': 3.3158, 'eval_samples_per_second': 301.588, 'eval_steps_per_second': 19.0, 'epoch': 0.32}
{'loss': 0.9995, 'grad_norm': 0.5128566026687622, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8464671969413757, 'eval_runtime': 3.3129, 'eval_samples_per_second': 301.852, 'eval_steps_per_second': 19.017, 'epoch': 0.36}
{'loss': 0.961, 'grad_norm': 0.4897520840167999, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8482832908630371, 'eval_runtime': 3.3189, 'eval_samples_per_second': 301.303, 'eval_steps_per_second': 18.982, 'epoch': 0.4}
{'loss': 0.9265, 'grad_norm': 0.5305283665657043, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8447362184524536, 'eval_runtime': 3.3109, 'eval_samples_per_second': 302.03, 'eval_steps_per_second': 19.028, 'epoch': 0.44}
{'loss': 0.9416, 'grad_norm': 0.6317170262336731, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8360781073570251, 'eval_runtime': 3.2907, 'eval_samples_per_second': 303.887, 'eval_steps_per_second': 19.145, 'epoch': 0.48}
{'loss': 0.9404, 'grad_norm': 0.3301226794719696, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8465083241462708, 'eval_runtime': 3.2917, 'eval_samples_per_second': 303.791, 'eval_steps_per_second': 19.139, 'epoch': 0.52}
{'loss': 0.9574, 'grad_norm': 0.49931618571281433, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.823235034942627, 'eval_runtime': 3.2947, 'eval_samples_per_second': 303.522, 'eval_steps_per_second': 19.122, 'epoch': 0.56}
{'loss': 0.9517, 'grad_norm': 0.40897756814956665, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8103123903274536, 'eval_runtime': 3.2937, 'eval_samples_per_second': 303.611, 'eval_steps_per_second': 19.127, 'epoch': 0.6}
{'loss': 0.991, 'grad_norm': 0.4283689260482788, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8167150616645813, 'eval_runtime': 3.3022, 'eval_samples_per_second': 302.832, 'eval_steps_per_second': 19.078, 'epoch': 0.64}
{'loss': 0.9351, 'grad_norm': 0.3486480712890625, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8215868473052979, 'eval_runtime': 3.2979, 'eval_samples_per_second': 303.22, 'eval_steps_per_second': 19.103, 'epoch': 0.68}
{'loss': 0.9391, 'grad_norm': 0.36383986473083496, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8114014863967896, 'eval_runtime': 3.2931, 'eval_samples_per_second': 303.662, 'eval_steps_per_second': 19.131, 'epoch': 0.72}
{'loss': 0.9392, 'grad_norm': 0.3377935588359833, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8004103899002075, 'eval_runtime': 3.2964, 'eval_samples_per_second': 303.365, 'eval_steps_per_second': 19.112, 'epoch': 0.76}
{'loss': 0.9266, 'grad_norm': 0.39354604482650757, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8099954724311829, 'eval_runtime': 3.2923, 'eval_samples_per_second': 303.74, 'eval_steps_per_second': 19.136, 'epoch': 0.8}
{'loss': 0.9, 'grad_norm': 0.42661944031715393, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7982940077781677, 'eval_runtime': 3.2957, 'eval_samples_per_second': 303.426, 'eval_steps_per_second': 19.116, 'epoch': 0.84}
{'loss': 0.9465, 'grad_norm': 0.34790536761283875, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8039583563804626, 'eval_runtime': 3.2934, 'eval_samples_per_second': 303.64, 'eval_steps_per_second': 19.129, 'epoch': 0.88}
{'loss': 0.8988, 'grad_norm': 0.3697543442249298, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7993737459182739, 'eval_runtime': 3.2892, 'eval_samples_per_second': 304.025, 'eval_steps_per_second': 19.154, 'epoch': 0.92}
{'loss': 0.965, 'grad_norm': 0.3327045142650604, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8010103106498718, 'eval_runtime': 3.2883, 'eval_samples_per_second': 304.109, 'eval_steps_per_second': 19.159, 'epoch': 0.96}
{'loss': 0.8955, 'grad_norm': 0.4404446482658386, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7989123463630676, 'eval_runtime': 3.2932, 'eval_samples_per_second': 303.658, 'eval_steps_per_second': 19.13, 'epoch': 1.0}
{'train_runtime': 275.8748, 'train_samples_per_second': 36.234, 'train_steps_per_second': 2.266, 'train_loss': 1.0943721405029296, 'epoch': 1.0}
train_results:  {'eval_loss': [2.774556875228882, 1.395309567451477, 1.083833932876587, 1.0044969320297241, 0.9591038823127747, 0.9172282218933105, 0.8886893391609192, 0.8947635293006897, 0.8464671969413757, 0.8482832908630371, 0.8447362184524536, 0.8360781073570251, 0.8465083241462708, 0.823235034942627, 0.8103123903274536, 0.8167150616645813, 0.8215868473052979, 0.8114014863967896, 0.8004103899002075, 0.8099954724311829, 0.7982940077781677, 0.8039583563804626, 0.7993737459182739, 0.8010103106498718, 0.7989123463630676], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.774556875228882, 1.395309567451477, 1.083833932876587, 1.0044969320297241, 0.9591038823127747, 0.9172282218933105, 0.8886893391609192, 0.8947635293006897, 0.8464671969413757, 0.8482832908630371, 0.8447362184524536, 0.8360781073570251, 0.8465083241462708, 0.823235034942627, 0.8103123903274536, 0.8167150616645813, 0.8215868473052979, 0.8114014863967896, 0.8004103899002075, 0.8099954724311829, 0.7982940077781677, 0.8039583563804626, 0.7993737459182739, 0.8010103106498718, 0.7989123463630676]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1240540742874146
current iteration best possible eval_loss (full train run):  -0.7989123463630676
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.126366376876831, -1.1271183490753174, -1.1240540742874146]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.3262 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -1.08725230630243
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -1.0897254094602027
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -1.0897254657099573
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -1.0897256532670414
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -1.0897254167841364
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0617, dtype=torch.float64), tensor(0.0410, dtype=torch.float64), 0, tensor(0.1106, dtype=torch.float64), tensor(0.0221, dtype=torch.float64), tensor(0.2598, dtype=torch.float64), tensor(0.0551, dtype=torch.float64), tensor(0.1186, dtype=torch.float64), tensor(0.3312, dtype=torch.float64), 13, 1, 1, 1, 1, 1, 128, 0.0, 25.783373216892613, 1]
normalized proposed parameters for next round by BO: [tensor(0.0617, dtype=torch.float64), tensor(0.0410, dtype=torch.float64), tensor(1.6984e-17, dtype=torch.float64), tensor(0.1106, dtype=torch.float64), tensor(0.0221, dtype=torch.float64), tensor(0.2598, dtype=torch.float64), tensor(0.0551, dtype=torch.float64), tensor(0.1186, dtype=torch.float64), tensor(0.3312, dtype=torch.float64), tensor(0.3969, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5372, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.062
  gsm8k: 0.041
  rowan_hellaswag: 0
  sciq: 0.111
  triviaqa: 0.022
  truthfulqa_gen: 0.26
  wikitext: 0.055
  mmlu: 0.119
  arc_challenge: 0.331

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (25.783373216892613,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  25.783373216892613
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 114,163,712 || all params: 8,144,424,960 || trainable%: 1.4017
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1147, 'grad_norm': 0.861793577671051, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7121392488479614, 'eval_runtime': 3.4899, 'eval_samples_per_second': 286.542, 'eval_steps_per_second': 18.052, 'epoch': 0.04}
{'loss': 1.4065, 'grad_norm': 0.865053653717041, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0661556720733643, 'eval_runtime': 3.5379, 'eval_samples_per_second': 282.65, 'eval_steps_per_second': 17.807, 'epoch': 0.08}
{'loss': 1.156, 'grad_norm': 0.29455599188804626, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.937267005443573, 'eval_runtime': 3.4952, 'eval_samples_per_second': 286.11, 'eval_steps_per_second': 18.025, 'epoch': 0.12}
{'loss': 1.1463, 'grad_norm': 0.25275129079818726, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8731801509857178, 'eval_runtime': 3.4939, 'eval_samples_per_second': 286.209, 'eval_steps_per_second': 18.031, 'epoch': 0.16}
{'loss': 1.1202, 'grad_norm': 0.21667203307151794, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7980695962905884, 'eval_runtime': 3.5004, 'eval_samples_per_second': 285.683, 'eval_steps_per_second': 17.998, 'epoch': 0.2}
{'loss': 0.9972, 'grad_norm': 0.27263355255126953, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7824515700340271, 'eval_runtime': 3.5061, 'eval_samples_per_second': 285.215, 'eval_steps_per_second': 17.969, 'epoch': 0.24}
{'loss': 0.9996, 'grad_norm': 0.39812856912612915, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.757211446762085, 'eval_runtime': 3.5012, 'eval_samples_per_second': 285.617, 'eval_steps_per_second': 17.994, 'epoch': 0.28}
{'loss': 0.9803, 'grad_norm': 0.2738003730773926, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7194951772689819, 'eval_runtime': 3.5044, 'eval_samples_per_second': 285.359, 'eval_steps_per_second': 17.978, 'epoch': 0.32}
{'loss': 0.9639, 'grad_norm': 0.20246697962284088, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7172719836235046, 'eval_runtime': 3.52, 'eval_samples_per_second': 284.088, 'eval_steps_per_second': 17.898, 'epoch': 0.36}
{'loss': 0.9645, 'grad_norm': 0.39378538727760315, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6842473149299622, 'eval_runtime': 3.5062, 'eval_samples_per_second': 285.206, 'eval_steps_per_second': 17.968, 'epoch': 0.4}
{'loss': 0.9694, 'grad_norm': 0.24044768512248993, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.68503737449646, 'eval_runtime': 3.5109, 'eval_samples_per_second': 284.824, 'eval_steps_per_second': 17.944, 'epoch': 0.44}
{'loss': 0.9178, 'grad_norm': 0.23040160536766052, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6674858927726746, 'eval_runtime': 3.5202, 'eval_samples_per_second': 284.075, 'eval_steps_per_second': 17.897, 'epoch': 0.48}
{'loss': 0.8959, 'grad_norm': 0.27105557918548584, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6479617357254028, 'eval_runtime': 3.5086, 'eval_samples_per_second': 285.012, 'eval_steps_per_second': 17.956, 'epoch': 0.52}
{'loss': 0.8614, 'grad_norm': 0.24107682704925537, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6454870700836182, 'eval_runtime': 3.5105, 'eval_samples_per_second': 284.86, 'eval_steps_per_second': 17.946, 'epoch': 0.56}
{'loss': 0.9101, 'grad_norm': 0.22897639870643616, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6259149312973022, 'eval_runtime': 3.5076, 'eval_samples_per_second': 285.092, 'eval_steps_per_second': 17.961, 'epoch': 0.6}
{'loss': 0.9186, 'grad_norm': 0.3391905725002289, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6093783378601074, 'eval_runtime': 3.5125, 'eval_samples_per_second': 284.699, 'eval_steps_per_second': 17.936, 'epoch': 0.64}
{'loss': 0.9165, 'grad_norm': 0.3375486135482788, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5962132811546326, 'eval_runtime': 3.5373, 'eval_samples_per_second': 282.698, 'eval_steps_per_second': 17.81, 'epoch': 0.68}
{'loss': 0.8534, 'grad_norm': 0.22192414104938507, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5849617719650269, 'eval_runtime': 3.5082, 'eval_samples_per_second': 285.046, 'eval_steps_per_second': 17.958, 'epoch': 0.72}
{'loss': 0.9316, 'grad_norm': 0.3839401304721832, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5614395141601562, 'eval_runtime': 3.51, 'eval_samples_per_second': 284.898, 'eval_steps_per_second': 17.949, 'epoch': 0.76}
{'loss': 0.9293, 'grad_norm': 0.4291946291923523, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5538996458053589, 'eval_runtime': 3.5113, 'eval_samples_per_second': 284.796, 'eval_steps_per_second': 17.942, 'epoch': 0.8}
{'loss': 0.915, 'grad_norm': 0.26514923572540283, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.543994128704071, 'eval_runtime': 3.5142, 'eval_samples_per_second': 284.561, 'eval_steps_per_second': 17.927, 'epoch': 0.84}
{'loss': 0.8852, 'grad_norm': 0.3131001889705658, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5342535972595215, 'eval_runtime': 3.5121, 'eval_samples_per_second': 284.729, 'eval_steps_per_second': 17.938, 'epoch': 0.88}
{'loss': 0.8555, 'grad_norm': 0.30334150791168213, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5213093161582947, 'eval_runtime': 3.516, 'eval_samples_per_second': 284.417, 'eval_steps_per_second': 17.918, 'epoch': 0.92}
{'loss': 0.8632, 'grad_norm': 0.30583781003952026, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5150397419929504, 'eval_runtime': 3.5462, 'eval_samples_per_second': 281.99, 'eval_steps_per_second': 17.765, 'epoch': 0.96}
{'loss': 0.8414, 'grad_norm': 0.321150541305542, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5101006627082825, 'eval_runtime': 3.5564, 'eval_samples_per_second': 281.186, 'eval_steps_per_second': 17.715, 'epoch': 1.0}
{'train_runtime': 268.8647, 'train_samples_per_second': 37.182, 'train_steps_per_second': 2.325, 'train_loss': 1.0525386993408203, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7121392488479614, 1.0661556720733643, 0.937267005443573, 0.8731801509857178, 0.7980695962905884, 0.7824515700340271, 0.757211446762085, 0.7194951772689819, 0.7172719836235046, 0.6842473149299622, 0.68503737449646, 0.6674858927726746, 0.6479617357254028, 0.6454870700836182, 0.6259149312973022, 0.6093783378601074, 0.5962132811546326, 0.5849617719650269, 0.5614395141601562, 0.5538996458053589, 0.543994128704071, 0.5342535972595215, 0.5213093161582947, 0.5150397419929504, 0.5101006627082825], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7121392488479614, 1.0661556720733643, 0.937267005443573, 0.8731801509857178, 0.7980695962905884, 0.7824515700340271, 0.757211446762085, 0.7194951772689819, 0.7172719836235046, 0.6842473149299622, 0.68503737449646, 0.6674858927726746, 0.6479617357254028, 0.6454870700836182, 0.6259149312973022, 0.6093783378601074, 0.5962132811546326, 0.5849617719650269, 0.5614395141601562, 0.5538996458053589, 0.543994128704071, 0.5342535972595215, 0.5213093161582947, 0.5150397419929504, 0.5101006627082825]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1293683052062988
current iteration best possible eval_loss (full train run):  -0.5101006627082825
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.126366376876831, -1.1271183490753174, -1.1240540742874146, -1.1293683052062988]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 32.1772 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.0878067660252748
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.0877738156877874
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.08778303904285
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.087773461825587
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.0877737031559862
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1429, dtype=torch.float64), tensor(0.0594, dtype=torch.float64), tensor(0.1053, dtype=torch.float64), tensor(0.1280, dtype=torch.float64), tensor(0.0537, dtype=torch.float64), tensor(0.3499, dtype=torch.float64), 0, tensor(0.0922, dtype=torch.float64), tensor(0.0686, dtype=torch.float64), 24, 0, 1, 0, 1, 1, 64, 0.0, 29.971032931310212, 1]
normalized proposed parameters for next round by BO: [tensor(0.1429, dtype=torch.float64), tensor(0.0594, dtype=torch.float64), tensor(0.1053, dtype=torch.float64), tensor(0.1280, dtype=torch.float64), tensor(0.0537, dtype=torch.float64), tensor(0.3499, dtype=torch.float64), tensor(1.4879e-18, dtype=torch.float64), tensor(0.0922, dtype=torch.float64), tensor(0.0686, dtype=torch.float64), tensor(0.7444, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5023, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6244, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.143
  gsm8k: 0.059
  rowan_hellaswag: 0.105
  sciq: 0.128
  triviaqa: 0.054
  truthfulqa_gen: 0.35
  wikitext: 0
  mmlu: 0.092
  arc_challenge: 0.069

LoRA Parameters:
  lora_r: (64,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (29.971032931310212,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  64
lora dropout:  0.0
lora alpha:  29.971032931310212
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 64,487,424 || all params: 8,094,748,672 || trainable%: 0.7967
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2584, 'grad_norm': 1.2605105638504028, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.535822868347168, 'eval_runtime': 3.5772, 'eval_samples_per_second': 279.548, 'eval_steps_per_second': 17.612, 'epoch': 0.04}
{'loss': 1.5097, 'grad_norm': 0.416301965713501, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9556754231452942, 'eval_runtime': 3.5635, 'eval_samples_per_second': 280.627, 'eval_steps_per_second': 17.679, 'epoch': 0.08}
{'loss': 1.2602, 'grad_norm': 0.388388991355896, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8413989543914795, 'eval_runtime': 3.5711, 'eval_samples_per_second': 280.027, 'eval_steps_per_second': 17.642, 'epoch': 0.12}
{'loss': 1.2135, 'grad_norm': 0.38984009623527527, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7383453249931335, 'eval_runtime': 3.5736, 'eval_samples_per_second': 279.833, 'eval_steps_per_second': 17.629, 'epoch': 0.16}
{'loss': 1.1079, 'grad_norm': 0.31554898619651794, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6836127042770386, 'eval_runtime': 3.5663, 'eval_samples_per_second': 280.406, 'eval_steps_per_second': 17.666, 'epoch': 0.2}
{'loss': 1.1659, 'grad_norm': 0.3341636061668396, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6498823165893555, 'eval_runtime': 3.5712, 'eval_samples_per_second': 280.018, 'eval_steps_per_second': 17.641, 'epoch': 0.24}
{'loss': 1.1356, 'grad_norm': 0.25885212421417236, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6097596883773804, 'eval_runtime': 3.5624, 'eval_samples_per_second': 280.71, 'eval_steps_per_second': 17.685, 'epoch': 0.28}
{'loss': 1.0958, 'grad_norm': 0.3098387122154236, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5700335502624512, 'eval_runtime': 3.5577, 'eval_samples_per_second': 281.078, 'eval_steps_per_second': 17.708, 'epoch': 0.32}
{'loss': 1.0388, 'grad_norm': 0.3677731454372406, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5471879839897156, 'eval_runtime': 3.5629, 'eval_samples_per_second': 280.674, 'eval_steps_per_second': 17.682, 'epoch': 0.36}
{'loss': 1.0893, 'grad_norm': 0.3684389293193817, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5171676278114319, 'eval_runtime': 3.5635, 'eval_samples_per_second': 280.621, 'eval_steps_per_second': 17.679, 'epoch': 0.4}
{'loss': 1.039, 'grad_norm': 0.3453860282897949, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.4750635325908661, 'eval_runtime': 3.5688, 'eval_samples_per_second': 280.208, 'eval_steps_per_second': 17.653, 'epoch': 0.44}
{'loss': 1.0495, 'grad_norm': 0.26344776153564453, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.44988471269607544, 'eval_runtime': 3.5847, 'eval_samples_per_second': 278.964, 'eval_steps_per_second': 17.575, 'epoch': 0.48}
{'loss': 1.0296, 'grad_norm': 0.3607821464538574, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4371126890182495, 'eval_runtime': 3.5741, 'eval_samples_per_second': 279.788, 'eval_steps_per_second': 17.627, 'epoch': 0.52}
{'loss': 0.9834, 'grad_norm': 0.3047238886356354, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.4002363979816437, 'eval_runtime': 3.5764, 'eval_samples_per_second': 279.612, 'eval_steps_per_second': 17.616, 'epoch': 0.56}
{'loss': 1.0341, 'grad_norm': 0.29513993859291077, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.3808040916919708, 'eval_runtime': 3.5581, 'eval_samples_per_second': 281.049, 'eval_steps_per_second': 17.706, 'epoch': 0.6}
{'loss': 1.0224, 'grad_norm': 0.3111764192581177, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.3577285408973694, 'eval_runtime': 3.5633, 'eval_samples_per_second': 280.641, 'eval_steps_per_second': 17.68, 'epoch': 0.64}
{'loss': 1.0277, 'grad_norm': 0.3313106596469879, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3362918198108673, 'eval_runtime': 3.5762, 'eval_samples_per_second': 279.626, 'eval_steps_per_second': 17.616, 'epoch': 0.68}
{'loss': 0.9989, 'grad_norm': 0.4004410207271576, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3245909512042999, 'eval_runtime': 3.537, 'eval_samples_per_second': 282.725, 'eval_steps_per_second': 17.812, 'epoch': 0.72}
{'loss': 1.015, 'grad_norm': 0.39344462752342224, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.30618488788604736, 'eval_runtime': 3.541, 'eval_samples_per_second': 282.404, 'eval_steps_per_second': 17.791, 'epoch': 0.76}
{'loss': 0.9147, 'grad_norm': 0.32509005069732666, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.2827781140804291, 'eval_runtime': 3.5428, 'eval_samples_per_second': 282.265, 'eval_steps_per_second': 17.783, 'epoch': 0.8}
{'loss': 1.037, 'grad_norm': 0.32272425293922424, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2607479989528656, 'eval_runtime': 3.5399, 'eval_samples_per_second': 282.492, 'eval_steps_per_second': 17.797, 'epoch': 0.84}
{'loss': 0.9061, 'grad_norm': 0.33490216732025146, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2514025866985321, 'eval_runtime': 3.5418, 'eval_samples_per_second': 282.343, 'eval_steps_per_second': 17.788, 'epoch': 0.88}
{'loss': 1.0593, 'grad_norm': 0.26622870564460754, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2392321079969406, 'eval_runtime': 3.5329, 'eval_samples_per_second': 283.051, 'eval_steps_per_second': 17.832, 'epoch': 0.92}
{'loss': 0.9547, 'grad_norm': 0.45012032985687256, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.231266051530838, 'eval_runtime': 3.5234, 'eval_samples_per_second': 283.817, 'eval_steps_per_second': 17.88, 'epoch': 0.96}
{'loss': 0.9494, 'grad_norm': 0.8797093033790588, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2321995347738266, 'eval_runtime': 3.5284, 'eval_samples_per_second': 283.411, 'eval_steps_per_second': 17.855, 'epoch': 1.0}
{'train_runtime': 296.8763, 'train_samples_per_second': 33.674, 'train_steps_per_second': 2.105, 'train_loss': 1.1558365539550781, 'epoch': 1.0}
train_results:  {'eval_loss': [1.535822868347168, 0.9556754231452942, 0.8413989543914795, 0.7383453249931335, 0.6836127042770386, 0.6498823165893555, 0.6097596883773804, 0.5700335502624512, 0.5471879839897156, 0.5171676278114319, 0.4750635325908661, 0.44988471269607544, 0.4371126890182495, 0.4002363979816437, 0.3808040916919708, 0.3577285408973694, 0.3362918198108673, 0.3245909512042999, 0.30618488788604736, 0.2827781140804291, 0.2607479989528656, 0.2514025866985321, 0.2392321079969406, 0.231266051530838, 0.2321995347738266], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.535822868347168, 0.9556754231452942, 0.8413989543914795, 0.7383453249931335, 0.6836127042770386, 0.6498823165893555, 0.6097596883773804, 0.5700335502624512, 0.5471879839897156, 0.5171676278114319, 0.4750635325908661, 0.44988471269607544, 0.4371126890182495, 0.4002363979816437, 0.3808040916919708, 0.3577285408973694, 0.3362918198108673, 0.3245909512042999, 0.30618488788604736, 0.2827781140804291, 0.2607479989528656, 0.2514025866985321, 0.2392321079969406, 0.231266051530838, 0.2321995347738266]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1254125833511353
current iteration best possible eval_loss (full train run):  -0.2321995347738266
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.126366376876831, -1.1271183490753174, -1.1240540742874146, -1.1293683052062988, -1.1254125833511353]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.5815 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -1.0830752158883143
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -1.0828601996875384
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -1.0850647160223184
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -1.0828606480394962
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -1.0828608053154283
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2226, dtype=torch.float64), tensor(0.0313, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(0.1344, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), tensor(0.3287, dtype=torch.float64), 0, tensor(0.1143, dtype=torch.float64), tensor(0.0703, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 2, 0.006758202125242452, 25.19007385455864, 1]
normalized proposed parameters for next round by BO: [tensor(0.2226, dtype=torch.float64), tensor(0.0313, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(0.1344, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), tensor(0.3287, dtype=torch.float64), tensor(1.7802e-17, dtype=torch.float64), tensor(0.1143, dtype=torch.float64), tensor(0.0703, dtype=torch.float64), tensor(0.4671, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.0676, dtype=torch.float64), tensor(0.5248, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.223
  gsm8k: 0.031
  rowan_hellaswag: 0.042
  sciq: 0.134
  triviaqa: 0.056
  truthfulqa_gen: 0.329
  wikitext: 0
  mmlu: 0.114
  arc_challenge: 0.07

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.006758202125242452,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (25.19007385455864,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  2
lora dropout:  0.006758202125242452
lora alpha:  25.19007385455864
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,259,520 || all params: 8,031,520,768 || trainable%: 0.0157
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4536, 'grad_norm': 13.511553764343262, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8307697772979736, 'eval_runtime': 3.3909, 'eval_samples_per_second': 294.905, 'eval_steps_per_second': 18.579, 'epoch': 0.04}
{'loss': 1.5471, 'grad_norm': 6.935497760772705, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0170469284057617, 'eval_runtime': 3.3992, 'eval_samples_per_second': 294.189, 'eval_steps_per_second': 18.534, 'epoch': 0.08}
{'loss': 1.2453, 'grad_norm': 2.527874231338501, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8967756628990173, 'eval_runtime': 3.3955, 'eval_samples_per_second': 294.51, 'eval_steps_per_second': 18.554, 'epoch': 0.12}
{'loss': 1.1238, 'grad_norm': 1.894762396812439, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7901495099067688, 'eval_runtime': 3.3953, 'eval_samples_per_second': 294.523, 'eval_steps_per_second': 18.555, 'epoch': 0.16}
{'loss': 1.1278, 'grad_norm': 1.9478503465652466, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7664420008659363, 'eval_runtime': 3.4047, 'eval_samples_per_second': 293.707, 'eval_steps_per_second': 18.504, 'epoch': 0.2}
{'loss': 1.0438, 'grad_norm': 1.3639154434204102, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.726709246635437, 'eval_runtime': 3.4047, 'eval_samples_per_second': 293.71, 'eval_steps_per_second': 18.504, 'epoch': 0.24}
{'loss': 1.0412, 'grad_norm': 1.4010169506072998, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6960001587867737, 'eval_runtime': 3.4214, 'eval_samples_per_second': 292.281, 'eval_steps_per_second': 18.414, 'epoch': 0.28}
{'loss': 1.0793, 'grad_norm': 1.673516869544983, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6867220997810364, 'eval_runtime': 3.422, 'eval_samples_per_second': 292.225, 'eval_steps_per_second': 18.41, 'epoch': 0.32}
{'loss': 0.9882, 'grad_norm': 1.5891985893249512, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6610437035560608, 'eval_runtime': 3.4277, 'eval_samples_per_second': 291.737, 'eval_steps_per_second': 18.379, 'epoch': 0.36}
{'loss': 1.005, 'grad_norm': 1.5730230808258057, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6368984580039978, 'eval_runtime': 3.4189, 'eval_samples_per_second': 292.495, 'eval_steps_per_second': 18.427, 'epoch': 0.4}
{'loss': 1.0425, 'grad_norm': 1.4727017879486084, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6154174208641052, 'eval_runtime': 3.4222, 'eval_samples_per_second': 292.209, 'eval_steps_per_second': 18.409, 'epoch': 0.44}
{'loss': 1.0113, 'grad_norm': 1.3915761709213257, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5977968573570251, 'eval_runtime': 3.42, 'eval_samples_per_second': 292.399, 'eval_steps_per_second': 18.421, 'epoch': 0.48}
{'loss': 0.993, 'grad_norm': 5.250500679016113, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5685611367225647, 'eval_runtime': 3.4086, 'eval_samples_per_second': 293.374, 'eval_steps_per_second': 18.483, 'epoch': 0.52}
{'loss': 0.9631, 'grad_norm': 1.6316827535629272, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5538308620452881, 'eval_runtime': 3.4225, 'eval_samples_per_second': 292.188, 'eval_steps_per_second': 18.408, 'epoch': 0.56}
{'loss': 0.9865, 'grad_norm': 2.019007921218872, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5249485373497009, 'eval_runtime': 3.4159, 'eval_samples_per_second': 292.753, 'eval_steps_per_second': 18.443, 'epoch': 0.6}
{'loss': 0.9456, 'grad_norm': 1.730187177658081, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4994485378265381, 'eval_runtime': 3.4181, 'eval_samples_per_second': 292.564, 'eval_steps_per_second': 18.432, 'epoch': 0.64}
{'loss': 0.9291, 'grad_norm': 1.286504864692688, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4944705665111542, 'eval_runtime': 3.4184, 'eval_samples_per_second': 292.533, 'eval_steps_per_second': 18.43, 'epoch': 0.68}
{'loss': 0.9295, 'grad_norm': 1.4924960136413574, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.47072264552116394, 'eval_runtime': 3.4118, 'eval_samples_per_second': 293.103, 'eval_steps_per_second': 18.465, 'epoch': 0.72}
{'loss': 0.9129, 'grad_norm': 1.6568810939788818, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4551944434642792, 'eval_runtime': 3.4169, 'eval_samples_per_second': 292.666, 'eval_steps_per_second': 18.438, 'epoch': 0.76}
{'loss': 0.909, 'grad_norm': 1.6149415969848633, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.43967095017433167, 'eval_runtime': 3.4242, 'eval_samples_per_second': 292.04, 'eval_steps_per_second': 18.399, 'epoch': 0.8}
{'loss': 0.9704, 'grad_norm': 1.77364981174469, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4271926283836365, 'eval_runtime': 3.4243, 'eval_samples_per_second': 292.028, 'eval_steps_per_second': 18.398, 'epoch': 0.84}
{'loss': 0.9392, 'grad_norm': 1.3508542776107788, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.4153006374835968, 'eval_runtime': 3.4241, 'eval_samples_per_second': 292.049, 'eval_steps_per_second': 18.399, 'epoch': 0.88}
{'loss': 0.9085, 'grad_norm': 1.250810146331787, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.4050447940826416, 'eval_runtime': 3.422, 'eval_samples_per_second': 292.227, 'eval_steps_per_second': 18.41, 'epoch': 0.92}
{'loss': 0.9533, 'grad_norm': 1.5084881782531738, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.39002376794815063, 'eval_runtime': 3.4134, 'eval_samples_per_second': 292.959, 'eval_steps_per_second': 18.456, 'epoch': 0.96}
{'loss': 0.9149, 'grad_norm': 2.082672595977783, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.387566477060318, 'eval_runtime': 3.4115, 'eval_samples_per_second': 293.13, 'eval_steps_per_second': 18.467, 'epoch': 1.0}
{'train_runtime': 266.3789, 'train_samples_per_second': 37.525, 'train_steps_per_second': 2.346, 'train_loss': 1.1185543884277345, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8307697772979736, 1.0170469284057617, 0.8967756628990173, 0.7901495099067688, 0.7664420008659363, 0.726709246635437, 0.6960001587867737, 0.6867220997810364, 0.6610437035560608, 0.6368984580039978, 0.6154174208641052, 0.5977968573570251, 0.5685611367225647, 0.5538308620452881, 0.5249485373497009, 0.4994485378265381, 0.4944705665111542, 0.47072264552116394, 0.4551944434642792, 0.43967095017433167, 0.4271926283836365, 0.4153006374835968, 0.4050447940826416, 0.39002376794815063, 0.387566477060318], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8307697772979736, 1.0170469284057617, 0.8967756628990173, 0.7901495099067688, 0.7664420008659363, 0.726709246635437, 0.6960001587867737, 0.6867220997810364, 0.6610437035560608, 0.6368984580039978, 0.6154174208641052, 0.5977968573570251, 0.5685611367225647, 0.5538308620452881, 0.5249485373497009, 0.4994485378265381, 0.4944705665111542, 0.47072264552116394, 0.4551944434642792, 0.43967095017433167, 0.4271926283836365, 0.4153006374835968, 0.4050447940826416, 0.39002376794815063, 0.387566477060318]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.387566477060318
max eval_loss so far:  -0.16279813647270203
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1264557838439941, -1.1269922256469727, -1.1277821063995361, -1.3735257387161255, -1.1262141466140747, -1.1228439807891846, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1272388696670532, -1.1259040832519531, -1.1256964206695557, -1.1267603635787964, -1.1266839504241943, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.126366376876831, -1.1271183490753174, -1.1240540742874146, -1.1293683052062988, -1.1254125833511353, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.0957 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -1.0809863402041644
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -1.0808832157306894
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -1.0824548506096237
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -1.1090184763553972
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -1.080885631915424
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0528, dtype=torch.float64), tensor(0.2811, dtype=torch.float64), tensor(0.0709, dtype=torch.float64), tensor(0.1212, dtype=torch.float64), tensor(0.1003, dtype=torch.float64), tensor(0.2597, dtype=torch.float64), 0, 0, tensor(0.1081, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 21, 0.006069123257965951, 28.061501492879856, 1]
normalized proposed parameters for next round by BO: [tensor(0.0528, dtype=torch.float64), tensor(0.2811, dtype=torch.float64), tensor(0.0709, dtype=torch.float64), tensor(0.1212, dtype=torch.float64), tensor(0.1003, dtype=torch.float64), tensor(0.2597, dtype=torch.float64), tensor(2.0154e-18, dtype=torch.float64), tensor(0.0059, dtype=torch.float64), tensor(0.1081, dtype=torch.float64), tensor(0.4728, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1611, dtype=torch.float64), tensor(0.0607, dtype=torch.float64), tensor(0.5846, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-1.2251425981521606, -0.6216726303100586, -0.22662609815597534, -0.22662609815597534, -0.22662609815597534, -0.22662609815597534, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
truthfulqa_gen
evaluation dataset:
data domain:  truthfulqa_gen  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/truthfulqa_gen/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 20, 1, 1, 1, 1, 1, 18, 0.05137445104835325, 2, 1]
Checking history sample input_X_between_0_1:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.5137445104835324, 0.041666666666666664, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2949618101119995
Checking history sample input_X:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 22, 1, 1, 0, 0, 1, 9, 0.09530863992118319, 22, 1]
Checking history sample input_X_between_0_1:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 0.6875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0703125, 0.9530863992118318, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.064080834388733
Checking history sample input_X:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 7, 0, 0, 0, 0, 1, 45, 0.012049704078718804, 22, 1]
Checking history sample input_X_between_0_1:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 0.21875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3515625, 0.12049704078718804, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.052124261856079
Checking history sample input_X:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 2, 1, 1, 1, 0, 0, 32, 0.03789572912213354, 26, 1]
Checking history sample input_X_between_0_1:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.3789572912213354, 0.5416666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4835323095321655
Checking history sample input_X:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 8, 0, 0, 1, 1, 1, 57, 0.05639372568359048, 47, 0]
Checking history sample input_X_between_0_1:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4453125, 0.5639372568359048, 0.9791666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2198251485824585
Checking history sample input_X:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 18, 1, 1, 0, 0, 1, 112, 0.0011300351648876107, 2, 1]
Checking history sample input_X_between_0_1:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 0.5625, 1.0, 1.0, 0.0, 0.0, 1.0, 0.875, 0.011300351648876106, 0.041666666666666664, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1017459630966187
Checking history sample input_X:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 11, 0, 1, 1, 0, 1, 121, 0.04409228366491266, 38, 0]
Checking history sample input_X_between_0_1:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 0.34375, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9453125, 0.4409228366491266, 0.7916666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0767676830291748
Checking history sample input_X:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 19, 1, 0, 0, 1, 0, 20, 0.057419890903339765, 17, 1]
Checking history sample input_X_between_0_1:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 0.59375, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15625, 0.5741989090333977, 0.3541666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0715935230255127
Checking history sample input_X:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 13, 0, 1, 0, 0, 1, 54, 0.07044921211215552, 48, 0]
Checking history sample input_X_between_0_1:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 0.40625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.421875, 0.7044921211215551, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3453316688537598
Checking history sample input_X:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 9, 1, 0, 1, 1, 0, 17, 0.07776680881547844, 40, 0]
Checking history sample input_X_between_0_1:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 0.28125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1328125, 0.7776680881547844, 0.8333333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.148055076599121
Checking history sample input_X:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 1, 0, 0, 0, 1, 0, 60, 0.001514616808966751, 14, 1]
Checking history sample input_X_between_0_1:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.46875, 0.015146168089667511, 0.2916666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4578598737716675
Checking history sample input_X:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 18, 1, 1, 1, 0, 1, 45, 0.001287895623877422, 34, 1]
Checking history sample input_X_between_0_1:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 0.5625, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3515625, 0.01287895623877422, 0.7083333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9687060117721558
Checking history sample input_X:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 8, 0, 0, 0, 1, 1, 51, 0.08835721159033366, 35, 0]
Checking history sample input_X_between_0_1:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3984375, 0.8835721159033366, 0.7291666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2380133867263794
Checking history sample input_X:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 2, 0, 0, 0, 1, 1, 28, 0.09450434861769766, 16, 1]
Checking history sample input_X_between_0_1:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 0.0625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.21875, 0.9450434861769765, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.283164620399475
Checking history sample input_X:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 2, 0, 0, 0, 1, 0, 27, 0.08742966606550949, 14, 0]
Checking history sample input_X_between_0_1:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2109375, 0.8742966606550948, 0.2916666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.5741345882415771
Checking history sample input_X:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 30, 0, 0, 0, 1, 0, 18, 0.07914275308569024, 23, 0]
Checking history sample input_X_between_0_1:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.140625, 0.7914275308569024, 0.4791666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0483520030975342
Checking history sample input_X:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 4, 1, 1, 1, 1, 1, 126, 0.005789639303569194, 25, 1]
Checking history sample input_X_between_0_1:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.05789639303569194, 0.5208333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9796491861343384
Checking history sample input_X:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 9, 1, 1, 1, 1, 1, 31, 0.03322680456132531, 22, 1]
Checking history sample input_X_between_0_1:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2421875, 0.33226804561325307, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1166555881500244
Checking history sample input_X:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 15, 0, 1, 0, 1, 1, 49, 0.0008783405064032635, 29, 1]
Checking history sample input_X_between_0_1:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 0.46875, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3828125, 0.008783405064032634, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8782313466072083
Checking history sample input_X:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 1, 0, 1, 1, 0, 1, 106, 0.08696702158391928, 5, 0]
Checking history sample input_X_between_0_1:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 0.03125, 0.0, 1.0, 1.0, 0.0, 1.0, 0.828125, 0.8696702158391928, 0.10416666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.4728790521621704
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1013 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6126638054847717, 0.09954571723937988, 0.1368652582168579, 0.4565383791923523, 0.4922604560852051, 0.6089981198310852, 0.23302853107452393, 0.9253460764884949, 0.884619951248169, 0.7820370197296143, 0.3011109232902527, 0.5512814521789551, 0.9904283285140991, 0.4000236392021179, 0.8351688385009766, 0.35869503021240234, 0.45190978050231934, 0.04121241718530655, 0.9966626763343811]  ‚Üí  acq = -0.6758830386257274
X = [0.6167836785316467, 0.33959585428237915, 0.9341786503791809, 0.10022354125976562, 0.7589733600616455, 0.5655980110168457, 0.04421800374984741, 0.2677828073501587, 0.3078760504722595, 0.7530345916748047, 0.45523566007614136, 0.45317262411117554, 0.5583834648132324, 0.45635324716567993, 0.9826313257217407, 0.48387446999549866, 0.799554705619812, 0.9689667224884033, 0.3179545998573303]  ‚Üí  acq = -0.6791510433016986
X = [0.5929087996482849, 0.07524234056472778, 0.6124346256256104, 0.5693556666374207, 0.4701502323150635, 0.9075466990470886, 0.14400643110275269, 0.3523794412612915, 0.27236032485961914, 0.5004331469535828, 0.6883965134620667, 0.38738733530044556, 0.4613471031188965, 0.3490599989891052, 0.41269028186798096, 0.6240151524543762, 0.9791287183761597, 0.5598897933959961, 0.30587267875671387]  ‚Üí  acq = -0.6807755958128349
X = [0.589245617389679, 0.3696509003639221, 0.8081424236297607, 0.4568765163421631, 0.13140225410461426, 0.3637266755104065, 0.47812211513519287, 0.49085569381713867, 0.9059937000274658, 0.2638280689716339, 0.9102771878242493, 0.7774316668510437, 0.9270699620246887, 0.26271486282348633, 0.21156245470046997, 0.0759243294596672, 0.2527945637702942, 0.6304056644439697, 0.8085795044898987]  ‚Üí  acq = -0.6791589591263001
X = [0.5728014707565308, 0.7247879505157471, 0.3603056073188782, 0.8137975335121155, 0.08376961946487427, 0.4096831679344177, 0.11057054996490479, 0.13615882396697998, 0.28146523237228394, 0.8536537289619446, 0.34398889541625977, 0.7153622508049011, 0.09053599834442139, 0.5184350609779358, 0.9014852046966553, 0.9587704539299011, 0.6455259919166565, 0.18336792290210724, 0.0965796709060669]  ‚Üí  acq = -0.6926882483735937
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.0213, dtype=torch.float64), 0, tensor(0.2053, dtype=torch.float64), 0, 0, 0, tensor(0.1962, dtype=torch.float64), tensor(0.5771, dtype=torch.float64), 20, 1, 1, 1, 1, 1, 2, 3.469446951953617e-18, 48.0, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(0., dtype=torch.float64), tensor(0.0213, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2053, dtype=torch.float64), tensor(1.0152e-16, dtype=torch.float64), tensor(6.4716e-17, dtype=torch.float64), tensor(1.3000e-16, dtype=torch.float64), tensor(0.1962, dtype=torch.float64), tensor(0.5771, dtype=torch.float64), tensor(0.6168, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.4694e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.021
  rowan_hellaswag: 0
  sciq: 0.205
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.196
  arc_challenge: 0.577

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.469446951953617e-18,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  2
lora dropout:  3.469446951953617e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,744,320 || all params: 8,033,005,568 || trainable%: 0.0342
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 2.4318, 'grad_norm': 6.203065872192383, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4250986576080322, 'eval_runtime': 3.7539, 'eval_samples_per_second': 266.392, 'eval_steps_per_second': 16.783, 'epoch': 0.04}
{'loss': 1.0966, 'grad_norm': 3.654470920562744, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2353169918060303, 'eval_runtime': 3.7458, 'eval_samples_per_second': 266.965, 'eval_steps_per_second': 16.819, 'epoch': 0.08}
{'loss': 0.9788, 'grad_norm': 3.685567617416382, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1371816396713257, 'eval_runtime': 3.7536, 'eval_samples_per_second': 266.411, 'eval_steps_per_second': 16.784, 'epoch': 0.12}
{'loss': 0.9936, 'grad_norm': 1.914806604385376, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1436915397644043, 'eval_runtime': 3.7631, 'eval_samples_per_second': 265.739, 'eval_steps_per_second': 16.742, 'epoch': 0.16}
{'loss': 0.853, 'grad_norm': 2.1604127883911133, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1239622831344604, 'eval_runtime': 3.759, 'eval_samples_per_second': 266.027, 'eval_steps_per_second': 16.76, 'epoch': 0.2}
{'loss': 0.9239, 'grad_norm': 1.987630844116211, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.158738374710083, 'eval_runtime': 3.7731, 'eval_samples_per_second': 265.034, 'eval_steps_per_second': 16.697, 'epoch': 0.24}
{'loss': 0.8433, 'grad_norm': 1.9140887260437012, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1002871990203857, 'eval_runtime': 3.745, 'eval_samples_per_second': 267.025, 'eval_steps_per_second': 16.823, 'epoch': 0.28}
{'loss': 0.8255, 'grad_norm': 2.3719546794891357, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1949892044067383, 'eval_runtime': 3.7456, 'eval_samples_per_second': 266.98, 'eval_steps_per_second': 16.82, 'epoch': 0.32}
{'loss': 0.8346, 'grad_norm': 2.4346392154693604, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.173494815826416, 'eval_runtime': 3.7447, 'eval_samples_per_second': 267.042, 'eval_steps_per_second': 16.824, 'epoch': 0.36}
{'loss': 0.7611, 'grad_norm': 3.157870292663574, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.244296908378601, 'eval_runtime': 3.7478, 'eval_samples_per_second': 266.826, 'eval_steps_per_second': 16.81, 'epoch': 0.4}
{'loss': 0.7794, 'grad_norm': 2.3579981327056885, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.115433931350708, 'eval_runtime': 3.7487, 'eval_samples_per_second': 266.76, 'eval_steps_per_second': 16.806, 'epoch': 0.44}
{'loss': 0.7261, 'grad_norm': 2.1601593494415283, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.162280797958374, 'eval_runtime': 3.7396, 'eval_samples_per_second': 267.406, 'eval_steps_per_second': 16.847, 'epoch': 0.48}
{'loss': 0.7206, 'grad_norm': 3.2736778259277344, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1352449655532837, 'eval_runtime': 3.7411, 'eval_samples_per_second': 267.304, 'eval_steps_per_second': 16.84, 'epoch': 0.52}
{'loss': 0.6914, 'grad_norm': 2.3410472869873047, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.142873764038086, 'eval_runtime': 3.7399, 'eval_samples_per_second': 267.387, 'eval_steps_per_second': 16.845, 'epoch': 0.56}
{'loss': 0.6644, 'grad_norm': 2.82901930809021, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.191068172454834, 'eval_runtime': 3.7341, 'eval_samples_per_second': 267.8, 'eval_steps_per_second': 16.871, 'epoch': 0.6}
{'loss': 0.6502, 'grad_norm': 2.702343702316284, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1830235719680786, 'eval_runtime': 3.7338, 'eval_samples_per_second': 267.822, 'eval_steps_per_second': 16.873, 'epoch': 0.64}
{'loss': 0.6621, 'grad_norm': 3.3467342853546143, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1895248889923096, 'eval_runtime': 3.7395, 'eval_samples_per_second': 267.416, 'eval_steps_per_second': 16.847, 'epoch': 0.68}
{'loss': 0.6692, 'grad_norm': 3.7564079761505127, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1732304096221924, 'eval_runtime': 3.7395, 'eval_samples_per_second': 267.414, 'eval_steps_per_second': 16.847, 'epoch': 0.72}
{'loss': 0.6365, 'grad_norm': 3.291440725326538, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1938551664352417, 'eval_runtime': 3.7393, 'eval_samples_per_second': 267.428, 'eval_steps_per_second': 16.848, 'epoch': 0.76}
{'loss': 0.6417, 'grad_norm': 2.5805234909057617, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1882489919662476, 'eval_runtime': 3.7414, 'eval_samples_per_second': 267.276, 'eval_steps_per_second': 16.838, 'epoch': 0.8}
{'loss': 0.5218, 'grad_norm': 2.839688539505005, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1763001680374146, 'eval_runtime': 3.7419, 'eval_samples_per_second': 267.246, 'eval_steps_per_second': 16.836, 'epoch': 0.84}
{'loss': 0.584, 'grad_norm': 2.165334463119507, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.182843804359436, 'eval_runtime': 3.7512, 'eval_samples_per_second': 266.58, 'eval_steps_per_second': 16.795, 'epoch': 0.88}
{'loss': 0.5894, 'grad_norm': 2.0999557971954346, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1779792308807373, 'eval_runtime': 3.7477, 'eval_samples_per_second': 266.828, 'eval_steps_per_second': 16.81, 'epoch': 0.92}
{'loss': 0.6006, 'grad_norm': 3.3708484172821045, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1745600700378418, 'eval_runtime': 3.7541, 'eval_samples_per_second': 266.379, 'eval_steps_per_second': 16.782, 'epoch': 0.96}
{'loss': 0.6042, 'grad_norm': 2.6669840812683105, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1763049364089966, 'eval_runtime': 3.7689, 'eval_samples_per_second': 265.327, 'eval_steps_per_second': 16.716, 'epoch': 1.0}
{'train_runtime': 299.0185, 'train_samples_per_second': 33.439, 'train_steps_per_second': 2.09, 'train_loss': 0.8113496505737304, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4250986576080322, 1.2353169918060303, 1.1371816396713257, 1.1436915397644043, 1.1239622831344604, 1.158738374710083, 1.1002871990203857, 1.1949892044067383, 1.173494815826416, 1.244296908378601, 1.115433931350708, 1.162280797958374, 1.1352449655532837, 1.142873764038086, 1.191068172454834, 1.1830235719680786, 1.1895248889923096, 1.1732304096221924, 1.1938551664352417, 1.1882489919662476, 1.1763001680374146, 1.182843804359436, 1.1779792308807373, 1.1745600700378418, 1.1763049364089966], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4250986576080322, 1.2353169918060303, 1.1371816396713257, 1.1436915397644043, 1.1239622831344604, 1.158738374710083, 1.1002871990203857, 1.1949892044067383, 1.173494815826416, 1.244296908378601, 1.115433931350708, 1.162280797958374, 1.1352449655532837, 1.142873764038086, 1.191068172454834, 1.1830235719680786, 1.1895248889923096, 1.1732304096221924, 1.1938551664352417, 1.1882489919662476, 1.1763001680374146, 1.182843804359436, 1.1779792308807373, 1.1745600700378418, 1.1763049364089966]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -1.1763049364089966
max eval_loss so far:  -1.1763049364089966
BO observations:  [-1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0298 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.8308795511109398
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.8307846435764707
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.8307822797673771
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.8307828884507346
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.8492564766829449
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2725, dtype=torch.float64), 0, tensor(0.2057, dtype=torch.float64), 0, tensor(0.3318, dtype=torch.float64), 0, 0, tensor(0.1900, dtype=torch.float64), 21, 1, 0, 1, 1, 1, 128, 0.1, 32.79679622748782, 1]
normalized proposed parameters for next round by BO: [tensor(2.3553e-17, dtype=torch.float64), tensor(0.2725, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2057, dtype=torch.float64), tensor(7.5112e-17, dtype=torch.float64), tensor(0.3318, dtype=torch.float64), tensor(1.0243e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1900, dtype=torch.float64), tensor(0.6443, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6833, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.272
  rowan_hellaswag: 0
  sciq: 0.206
  triviaqa: 0
  truthfulqa_gen: 0.332
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.19

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (32.79679622748782,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  32.79679622748782
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 170,655,744 || all params: 8,200,916,992 || trainable%: 2.0809
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6627, 'grad_norm': 0.6077156662940979, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4982272386550903, 'eval_runtime': 3.6692, 'eval_samples_per_second': 272.541, 'eval_steps_per_second': 17.17, 'epoch': 0.04}
{'loss': 1.1097, 'grad_norm': 0.27164655923843384, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9477360844612122, 'eval_runtime': 3.6687, 'eval_samples_per_second': 272.58, 'eval_steps_per_second': 17.173, 'epoch': 0.08}
{'loss': 0.9501, 'grad_norm': 0.32695305347442627, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8856715559959412, 'eval_runtime': 3.6451, 'eval_samples_per_second': 274.337, 'eval_steps_per_second': 17.283, 'epoch': 0.12}
{'loss': 0.8996, 'grad_norm': 0.21586863696575165, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7698549628257751, 'eval_runtime': 3.6493, 'eval_samples_per_second': 274.026, 'eval_steps_per_second': 17.264, 'epoch': 0.16}
{'loss': 0.8446, 'grad_norm': 0.2992185354232788, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6892598867416382, 'eval_runtime': 3.6437, 'eval_samples_per_second': 274.443, 'eval_steps_per_second': 17.29, 'epoch': 0.2}
{'loss': 0.8102, 'grad_norm': 0.2270142138004303, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6411257982254028, 'eval_runtime': 3.6508, 'eval_samples_per_second': 273.912, 'eval_steps_per_second': 17.256, 'epoch': 0.24}
{'loss': 0.7835, 'grad_norm': 0.22815942764282227, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6284199357032776, 'eval_runtime': 3.6537, 'eval_samples_per_second': 273.692, 'eval_steps_per_second': 17.243, 'epoch': 0.28}
{'loss': 0.7842, 'grad_norm': 0.1772189438343048, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5815684199333191, 'eval_runtime': 3.6491, 'eval_samples_per_second': 274.04, 'eval_steps_per_second': 17.265, 'epoch': 0.32}
{'loss': 0.773, 'grad_norm': 0.1639968603849411, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5549712181091309, 'eval_runtime': 3.6529, 'eval_samples_per_second': 273.758, 'eval_steps_per_second': 17.247, 'epoch': 0.36}
{'loss': 0.7882, 'grad_norm': 0.20286059379577637, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5235533714294434, 'eval_runtime': 3.6514, 'eval_samples_per_second': 273.87, 'eval_steps_per_second': 17.254, 'epoch': 0.4}
{'loss': 0.758, 'grad_norm': 0.18717126548290253, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.48374804854393005, 'eval_runtime': 3.6523, 'eval_samples_per_second': 273.798, 'eval_steps_per_second': 17.249, 'epoch': 0.44}
{'loss': 0.7663, 'grad_norm': 0.177212193608284, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.46985897421836853, 'eval_runtime': 3.6508, 'eval_samples_per_second': 273.911, 'eval_steps_per_second': 17.256, 'epoch': 0.48}
{'loss': 0.7284, 'grad_norm': 0.20766933262348175, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4339340925216675, 'eval_runtime': 3.6349, 'eval_samples_per_second': 275.107, 'eval_steps_per_second': 17.332, 'epoch': 0.52}
{'loss': 0.7164, 'grad_norm': 0.20487651228904724, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.4018685221672058, 'eval_runtime': 3.6362, 'eval_samples_per_second': 275.016, 'eval_steps_per_second': 17.326, 'epoch': 0.56}
{'loss': 0.7205, 'grad_norm': 0.17340469360351562, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.3926721215248108, 'eval_runtime': 3.6459, 'eval_samples_per_second': 274.281, 'eval_steps_per_second': 17.28, 'epoch': 0.6}
{'loss': 0.7207, 'grad_norm': 0.21528808772563934, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.38283735513687134, 'eval_runtime': 3.6561, 'eval_samples_per_second': 273.518, 'eval_steps_per_second': 17.232, 'epoch': 0.64}
{'loss': 0.7253, 'grad_norm': 0.21743252873420715, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3439071476459503, 'eval_runtime': 3.6558, 'eval_samples_per_second': 273.537, 'eval_steps_per_second': 17.233, 'epoch': 0.68}
{'loss': 0.7083, 'grad_norm': 0.21465693414211273, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3221474289894104, 'eval_runtime': 3.665, 'eval_samples_per_second': 272.85, 'eval_steps_per_second': 17.19, 'epoch': 0.72}
{'loss': 0.6917, 'grad_norm': 0.20173762738704681, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3006781339645386, 'eval_runtime': 3.6704, 'eval_samples_per_second': 272.453, 'eval_steps_per_second': 17.165, 'epoch': 0.76}
{'loss': 0.7001, 'grad_norm': 0.18453757464885712, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.28853610157966614, 'eval_runtime': 3.6681, 'eval_samples_per_second': 272.62, 'eval_steps_per_second': 17.175, 'epoch': 0.8}
{'loss': 0.6635, 'grad_norm': 0.19815528392791748, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2728012204170227, 'eval_runtime': 3.6475, 'eval_samples_per_second': 274.159, 'eval_steps_per_second': 17.272, 'epoch': 0.84}
{'loss': 0.6821, 'grad_norm': 0.25078117847442627, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2610398530960083, 'eval_runtime': 3.6488, 'eval_samples_per_second': 274.065, 'eval_steps_per_second': 17.266, 'epoch': 0.88}
{'loss': 0.6688, 'grad_norm': 0.22860018908977509, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2542014420032501, 'eval_runtime': 3.6565, 'eval_samples_per_second': 273.482, 'eval_steps_per_second': 17.229, 'epoch': 0.92}
{'loss': 0.6725, 'grad_norm': 0.22079646587371826, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.24701040983200073, 'eval_runtime': 3.656, 'eval_samples_per_second': 273.522, 'eval_steps_per_second': 17.232, 'epoch': 0.96}
{'loss': 0.7015, 'grad_norm': 0.2953726649284363, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.24569274485111237, 'eval_runtime': 3.6473, 'eval_samples_per_second': 274.172, 'eval_steps_per_second': 17.273, 'epoch': 1.0}
{'train_runtime': 310.1567, 'train_samples_per_second': 32.235, 'train_steps_per_second': 2.015, 'train_loss': 0.8411897277832031, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4982272386550903, 0.9477360844612122, 0.8856715559959412, 0.7698549628257751, 0.6892598867416382, 0.6411257982254028, 0.6284199357032776, 0.5815684199333191, 0.5549712181091309, 0.5235533714294434, 0.48374804854393005, 0.46985897421836853, 0.4339340925216675, 0.4018685221672058, 0.3926721215248108, 0.38283735513687134, 0.3439071476459503, 0.3221474289894104, 0.3006781339645386, 0.28853610157966614, 0.2728012204170227, 0.2610398530960083, 0.2542014420032501, 0.24701040983200073, 0.24569274485111237], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4982272386550903, 0.9477360844612122, 0.8856715559959412, 0.7698549628257751, 0.6892598867416382, 0.6411257982254028, 0.6284199357032776, 0.5815684199333191, 0.5549712181091309, 0.5235533714294434, 0.48374804854393005, 0.46985897421836853, 0.4339340925216675, 0.4018685221672058, 0.3926721215248108, 0.38283735513687134, 0.3439071476459503, 0.3221474289894104, 0.3006781339645386, 0.28853610157966614, 0.2728012204170227, 0.2610398530960083, 0.2542014420032501, 0.24701040983200073, 0.24569274485111237]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.24569274485111237
max eval_loss so far:  -0.24569274485111237
BO observations:  [-1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.1318 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.9030585533693472
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.8996364809332353
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.8996357393797612
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.9356936551691651
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.8996285094405012
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2222, dtype=torch.float64), 0, tensor(0.3551, dtype=torch.float64), 0, 0, 0, tensor(0.1406, dtype=torch.float64), tensor(0.2821, dtype=torch.float64), 20, 0, 1, 0, 0, 1, 118, 0.0, 24.338731766076165, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2222, dtype=torch.float64), tensor(2.4249e-18, dtype=torch.float64), tensor(0.3551, dtype=torch.float64), tensor(4.3116e-18, dtype=torch.float64), tensor(4.1504e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1406, dtype=torch.float64), tensor(0.2821, dtype=torch.float64), tensor(0.6351, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9250, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5071, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.222
  rowan_hellaswag: 0
  sciq: 0.355
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.141
  arc_challenge: 0.282

LoRA Parameters:
  lora_r: (118,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (24.338731766076165,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  118
lora dropout:  0.0
lora alpha:  24.338731766076165
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 55,582,720 || all params: 8,085,843,968 || trainable%: 0.6874
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0381, 'grad_norm': 0.6630572080612183, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1197054386138916, 'eval_runtime': 3.3656, 'eval_samples_per_second': 297.121, 'eval_steps_per_second': 18.719, 'epoch': 0.04}
{'loss': 1.4519, 'grad_norm': 0.26215067505836487, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.464421272277832, 'eval_runtime': 3.3683, 'eval_samples_per_second': 296.887, 'eval_steps_per_second': 18.704, 'epoch': 0.08}
{'loss': 1.1962, 'grad_norm': 0.19966135919094086, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4124903678894043, 'eval_runtime': 3.3658, 'eval_samples_per_second': 297.105, 'eval_steps_per_second': 18.718, 'epoch': 0.12}
{'loss': 1.1274, 'grad_norm': 0.17762669920921326, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4557912349700928, 'eval_runtime': 3.3421, 'eval_samples_per_second': 299.213, 'eval_steps_per_second': 18.85, 'epoch': 0.16}
{'loss': 1.0744, 'grad_norm': 0.2402404397726059, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3596857786178589, 'eval_runtime': 3.357, 'eval_samples_per_second': 297.885, 'eval_steps_per_second': 18.767, 'epoch': 0.2}
{'loss': 1.0392, 'grad_norm': 0.197998046875, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3428994417190552, 'eval_runtime': 3.354, 'eval_samples_per_second': 298.152, 'eval_steps_per_second': 18.784, 'epoch': 0.24}
{'loss': 1.0307, 'grad_norm': 0.16486434638500214, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.319520115852356, 'eval_runtime': 3.3572, 'eval_samples_per_second': 297.863, 'eval_steps_per_second': 18.765, 'epoch': 0.28}
{'loss': 1.0122, 'grad_norm': 0.20884105563163757, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3326693773269653, 'eval_runtime': 3.3683, 'eval_samples_per_second': 296.888, 'eval_steps_per_second': 18.704, 'epoch': 0.32}
{'loss': 0.9873, 'grad_norm': 0.175685852766037, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2935314178466797, 'eval_runtime': 3.3977, 'eval_samples_per_second': 294.313, 'eval_steps_per_second': 18.542, 'epoch': 0.36}
{'loss': 0.9911, 'grad_norm': 0.17082154750823975, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2979116439819336, 'eval_runtime': 3.3641, 'eval_samples_per_second': 297.252, 'eval_steps_per_second': 18.727, 'epoch': 0.4}
{'loss': 0.9668, 'grad_norm': 0.21259525418281555, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2801473140716553, 'eval_runtime': 3.3583, 'eval_samples_per_second': 297.77, 'eval_steps_per_second': 18.76, 'epoch': 0.44}
{'loss': 0.9705, 'grad_norm': 0.167861208319664, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.301265835762024, 'eval_runtime': 3.3673, 'eval_samples_per_second': 296.973, 'eval_steps_per_second': 18.709, 'epoch': 0.48}
{'loss': 0.9736, 'grad_norm': 0.17214682698249817, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3099792003631592, 'eval_runtime': 3.3915, 'eval_samples_per_second': 294.854, 'eval_steps_per_second': 18.576, 'epoch': 0.52}
{'loss': 0.913, 'grad_norm': 0.17916777729988098, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2895176410675049, 'eval_runtime': 3.384, 'eval_samples_per_second': 295.51, 'eval_steps_per_second': 18.617, 'epoch': 0.56}
{'loss': 0.9203, 'grad_norm': 0.18669545650482178, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2771848440170288, 'eval_runtime': 3.3805, 'eval_samples_per_second': 295.812, 'eval_steps_per_second': 18.636, 'epoch': 0.6}
{'loss': 0.9409, 'grad_norm': 0.18265007436275482, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3008012771606445, 'eval_runtime': 3.4057, 'eval_samples_per_second': 293.629, 'eval_steps_per_second': 18.499, 'epoch': 0.64}
{'loss': 0.923, 'grad_norm': 0.18510973453521729, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2497581243515015, 'eval_runtime': 3.4051, 'eval_samples_per_second': 293.675, 'eval_steps_per_second': 18.502, 'epoch': 0.68}
{'loss': 0.9455, 'grad_norm': 0.18267878890037537, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2355246543884277, 'eval_runtime': 3.412, 'eval_samples_per_second': 293.08, 'eval_steps_per_second': 18.464, 'epoch': 0.72}
{'loss': 0.9154, 'grad_norm': 0.17825108766555786, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2296816110610962, 'eval_runtime': 3.3841, 'eval_samples_per_second': 295.5, 'eval_steps_per_second': 18.617, 'epoch': 0.76}
{'loss': 0.8972, 'grad_norm': 0.1900235414505005, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1823015213012695, 'eval_runtime': 3.3894, 'eval_samples_per_second': 295.038, 'eval_steps_per_second': 18.587, 'epoch': 0.8}
{'loss': 0.9, 'grad_norm': 0.2158244401216507, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1767170429229736, 'eval_runtime': 3.3721, 'eval_samples_per_second': 296.548, 'eval_steps_per_second': 18.683, 'epoch': 0.84}
{'loss': 0.8708, 'grad_norm': 0.2282177358865738, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1688233613967896, 'eval_runtime': 3.3758, 'eval_samples_per_second': 296.222, 'eval_steps_per_second': 18.662, 'epoch': 0.88}
{'loss': 0.861, 'grad_norm': 0.19804708659648895, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1594867706298828, 'eval_runtime': 3.377, 'eval_samples_per_second': 296.122, 'eval_steps_per_second': 18.656, 'epoch': 0.92}
{'loss': 0.8437, 'grad_norm': 0.18667352199554443, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.167523980140686, 'eval_runtime': 3.3776, 'eval_samples_per_second': 296.066, 'eval_steps_per_second': 18.652, 'epoch': 0.96}
{'loss': 0.8631, 'grad_norm': 0.20014047622680664, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1756494045257568, 'eval_runtime': 3.3747, 'eval_samples_per_second': 296.32, 'eval_steps_per_second': 18.668, 'epoch': 1.0}
{'train_runtime': 280.2822, 'train_samples_per_second': 35.668, 'train_steps_per_second': 2.23, 'train_loss': 1.0661351287841796, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1197054386138916, 1.464421272277832, 1.4124903678894043, 1.4557912349700928, 1.3596857786178589, 1.3428994417190552, 1.319520115852356, 1.3326693773269653, 1.2935314178466797, 1.2979116439819336, 1.2801473140716553, 1.301265835762024, 1.3099792003631592, 1.2895176410675049, 1.2771848440170288, 1.3008012771606445, 1.2497581243515015, 1.2355246543884277, 1.2296816110610962, 1.1823015213012695, 1.1767170429229736, 1.1688233613967896, 1.1594867706298828, 1.167523980140686, 1.1756494045257568], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1197054386138916, 1.464421272277832, 1.4124903678894043, 1.4557912349700928, 1.3596857786178589, 1.3428994417190552, 1.319520115852356, 1.3326693773269653, 1.2935314178466797, 1.2979116439819336, 1.2801473140716553, 1.301265835762024, 1.3099792003631592, 1.2895176410675049, 1.2771848440170288, 1.3008012771606445, 1.2497581243515015, 1.2355246543884277, 1.2296816110610962, 1.1823015213012695, 1.1767170429229736, 1.1688233613967896, 1.1594867706298828, 1.167523980140686, 1.1756494045257568]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -1.1756494045257568
max eval_loss so far:  -0.24569274485111237
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6209 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.8834978656173209
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.9055334530961408
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.9017409630400299
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.905533534950475
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.9055334530997705
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4289, dtype=torch.float64), tensor(0.0554, dtype=torch.float64), 0, 0, tensor(0.2113, dtype=torch.float64), 0, tensor(0.0314, dtype=torch.float64), tensor(0.2730, dtype=torch.float64), 13, 0, 0, 0, 1, 1, 61, 3.4480645183212413e-19, 39.41010416081197, 1]
normalized proposed parameters for next round by BO: [tensor(8.8447e-19, dtype=torch.float64), tensor(0.4289, dtype=torch.float64), tensor(0.0554, dtype=torch.float64), tensor(6.1248e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2113, dtype=torch.float64), tensor(1.2828e-17, dtype=torch.float64), tensor(0.0314, dtype=torch.float64), tensor(0.2730, dtype=torch.float64), tensor(0.4092, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4803, dtype=torch.float64), tensor(3.4481e-18, dtype=torch.float64), tensor(0.8210, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.429
  rowan_hellaswag: 0.055
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.211
  wikitext: 0
  mmlu: 0.031
  arc_challenge: 0.273

LoRA Parameters:
  lora_r: (61,)
  lora_dropout: (3.4480645183212413e-19,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (39.41010416081197,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  61
lora dropout:  3.4480645183212413e-19
lora alpha:  39.41010416081197
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 29,233,152 || all params: 8,059,494,400 || trainable%: 0.3627
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6654, 'grad_norm': 1.3000061511993408, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9760881662368774, 'eval_runtime': 3.3491, 'eval_samples_per_second': 298.589, 'eval_steps_per_second': 18.811, 'epoch': 0.04}
{'loss': 1.3005, 'grad_norm': 0.5208348631858826, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0356769561767578, 'eval_runtime': 3.3678, 'eval_samples_per_second': 296.933, 'eval_steps_per_second': 18.707, 'epoch': 0.08}
{'loss': 1.0578, 'grad_norm': 0.40547600388526917, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9192180633544922, 'eval_runtime': 3.3553, 'eval_samples_per_second': 298.037, 'eval_steps_per_second': 18.776, 'epoch': 0.12}
{'loss': 1.0286, 'grad_norm': 0.3550037443637848, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8346904516220093, 'eval_runtime': 3.3585, 'eval_samples_per_second': 297.756, 'eval_steps_per_second': 18.759, 'epoch': 0.16}
{'loss': 0.978, 'grad_norm': 0.3260396420955658, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.787551760673523, 'eval_runtime': 3.3677, 'eval_samples_per_second': 296.943, 'eval_steps_per_second': 18.707, 'epoch': 0.2}
{'loss': 0.9375, 'grad_norm': 0.2700957953929901, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7561460137367249, 'eval_runtime': 3.3678, 'eval_samples_per_second': 296.926, 'eval_steps_per_second': 18.706, 'epoch': 0.24}
{'loss': 0.9484, 'grad_norm': 0.29924601316452026, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7506546378135681, 'eval_runtime': 3.3649, 'eval_samples_per_second': 297.185, 'eval_steps_per_second': 18.723, 'epoch': 0.28}
{'loss': 0.9173, 'grad_norm': 0.3121818006038666, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7448444962501526, 'eval_runtime': 3.3699, 'eval_samples_per_second': 296.747, 'eval_steps_per_second': 18.695, 'epoch': 0.32}
{'loss': 0.9209, 'grad_norm': 0.24958115816116333, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7152100801467896, 'eval_runtime': 3.369, 'eval_samples_per_second': 296.822, 'eval_steps_per_second': 18.7, 'epoch': 0.36}
{'loss': 0.9454, 'grad_norm': 0.2893919348716736, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7101452350616455, 'eval_runtime': 3.37, 'eval_samples_per_second': 296.739, 'eval_steps_per_second': 18.695, 'epoch': 0.4}
{'loss': 0.9435, 'grad_norm': 0.2820127010345459, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6946871280670166, 'eval_runtime': 3.363, 'eval_samples_per_second': 297.35, 'eval_steps_per_second': 18.733, 'epoch': 0.44}
{'loss': 0.8891, 'grad_norm': 0.24153314530849457, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6869217753410339, 'eval_runtime': 3.3685, 'eval_samples_per_second': 296.872, 'eval_steps_per_second': 18.703, 'epoch': 0.48}
{'loss': 0.919, 'grad_norm': 0.2689877152442932, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6743246912956238, 'eval_runtime': 3.3794, 'eval_samples_per_second': 295.912, 'eval_steps_per_second': 18.642, 'epoch': 0.52}
{'loss': 0.8751, 'grad_norm': 0.26934191584587097, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6547132134437561, 'eval_runtime': 3.3808, 'eval_samples_per_second': 295.791, 'eval_steps_per_second': 18.635, 'epoch': 0.56}
{'loss': 0.9139, 'grad_norm': 0.30974894762039185, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6570255160331726, 'eval_runtime': 3.3852, 'eval_samples_per_second': 295.403, 'eval_steps_per_second': 18.61, 'epoch': 0.6}
{'loss': 0.8919, 'grad_norm': 0.3353089392185211, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6457239389419556, 'eval_runtime': 3.3738, 'eval_samples_per_second': 296.4, 'eval_steps_per_second': 18.673, 'epoch': 0.64}
{'loss': 0.9104, 'grad_norm': 0.28739285469055176, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6413236260414124, 'eval_runtime': 3.3836, 'eval_samples_per_second': 295.543, 'eval_steps_per_second': 18.619, 'epoch': 0.68}
{'loss': 0.89, 'grad_norm': 0.36987701058387756, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6221380829811096, 'eval_runtime': 3.3767, 'eval_samples_per_second': 296.151, 'eval_steps_per_second': 18.657, 'epoch': 0.72}
{'loss': 0.9391, 'grad_norm': 0.2892816960811615, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6170080900192261, 'eval_runtime': 3.3705, 'eval_samples_per_second': 296.695, 'eval_steps_per_second': 18.692, 'epoch': 0.76}
{'loss': 0.897, 'grad_norm': 0.27886223793029785, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6056927442550659, 'eval_runtime': 3.3699, 'eval_samples_per_second': 296.745, 'eval_steps_per_second': 18.695, 'epoch': 0.8}
{'loss': 0.9132, 'grad_norm': 0.28662335872650146, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5958941578865051, 'eval_runtime': 3.3789, 'eval_samples_per_second': 295.955, 'eval_steps_per_second': 18.645, 'epoch': 0.84}
{'loss': 0.8896, 'grad_norm': 0.3537854850292206, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5848451852798462, 'eval_runtime': 3.3718, 'eval_samples_per_second': 296.581, 'eval_steps_per_second': 18.685, 'epoch': 0.88}
{'loss': 0.8711, 'grad_norm': 0.34430360794067383, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5792521834373474, 'eval_runtime': 3.3725, 'eval_samples_per_second': 296.52, 'eval_steps_per_second': 18.681, 'epoch': 0.92}
{'loss': 0.8757, 'grad_norm': 0.29244327545166016, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5743986964225769, 'eval_runtime': 3.3696, 'eval_samples_per_second': 296.77, 'eval_steps_per_second': 18.697, 'epoch': 0.96}
{'loss': 0.8714, 'grad_norm': 0.33813557028770447, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5740166306495667, 'eval_runtime': 3.3734, 'eval_samples_per_second': 296.435, 'eval_steps_per_second': 18.675, 'epoch': 1.0}
{'train_runtime': 294.194, 'train_samples_per_second': 33.984, 'train_steps_per_second': 2.124, 'train_loss': 1.0075982818603515, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9760881662368774, 1.0356769561767578, 0.9192180633544922, 0.8346904516220093, 0.787551760673523, 0.7561460137367249, 0.7506546378135681, 0.7448444962501526, 0.7152100801467896, 0.7101452350616455, 0.6946871280670166, 0.6869217753410339, 0.6743246912956238, 0.6547132134437561, 0.6570255160331726, 0.6457239389419556, 0.6413236260414124, 0.6221380829811096, 0.6170080900192261, 0.6056927442550659, 0.5958941578865051, 0.5848451852798462, 0.5792521834373474, 0.5743986964225769, 0.5740166306495667], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9760881662368774, 1.0356769561767578, 0.9192180633544922, 0.8346904516220093, 0.787551760673523, 0.7561460137367249, 0.7506546378135681, 0.7448444962501526, 0.7152100801467896, 0.7101452350616455, 0.6946871280670166, 0.6869217753410339, 0.6743246912956238, 0.6547132134437561, 0.6570255160331726, 0.6457239389419556, 0.6413236260414124, 0.6221380829811096, 0.6170080900192261, 0.6056927442550659, 0.5958941578865051, 0.5848451852798462, 0.5792521834373474, 0.5743986964225769, 0.5740166306495667]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.124546766281128
current iteration best possible eval_loss (full train run):  -0.5740166306495667
max eval_loss so far:  -0.24569274485111237
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3287 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.9373635569553054
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.9373043188346282
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.9373535471836432
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.9373499919801687
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.9373499923677708
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3104, dtype=torch.float64), 0, tensor(0.0877, dtype=torch.float64), tensor(0.1426, dtype=torch.float64), tensor(0.0150, dtype=torch.float64), tensor(0.1675, dtype=torch.float64), 0, tensor(0.1122, dtype=torch.float64), tensor(0.1646, dtype=torch.float64), 23, 1, 1, 0, 0, 1, 58, 0.0, 30.49685604180935, 1]
normalized proposed parameters for next round by BO: [tensor(0.3104, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0877, dtype=torch.float64), tensor(0.1426, dtype=torch.float64), tensor(0.0150, dtype=torch.float64), tensor(0.1675, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1122, dtype=torch.float64), tensor(0.1646, dtype=torch.float64), tensor(0.7186, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4544, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6354, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.31
  gsm8k: 0
  rowan_hellaswag: 0.088
  sciq: 0.143
  triviaqa: 0.015
  truthfulqa_gen: 0.168
  wikitext: 0
  mmlu: 0.112
  arc_challenge: 0.165

LoRA Parameters:
  lora_r: (58,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (30.49685604180935,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  58
lora dropout:  0.0
lora alpha:  30.49685604180935
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 42,346,496 || all params: 8,072,607,744 || trainable%: 0.5246
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4206, 'grad_norm': 0.9494883418083191, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.892975091934204, 'eval_runtime': 3.4418, 'eval_samples_per_second': 290.545, 'eval_steps_per_second': 18.304, 'epoch': 0.04}
{'loss': 1.5924, 'grad_norm': 0.440563440322876, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.219799280166626, 'eval_runtime': 3.4332, 'eval_samples_per_second': 291.27, 'eval_steps_per_second': 18.35, 'epoch': 0.08}
{'loss': 1.3348, 'grad_norm': 0.31407782435417175, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0937610864639282, 'eval_runtime': 3.4293, 'eval_samples_per_second': 291.601, 'eval_steps_per_second': 18.371, 'epoch': 0.12}
{'loss': 1.2746, 'grad_norm': 0.30868396162986755, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9978688955307007, 'eval_runtime': 3.4522, 'eval_samples_per_second': 289.669, 'eval_steps_per_second': 18.249, 'epoch': 0.16}
{'loss': 1.2363, 'grad_norm': 0.3320068418979645, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.927608072757721, 'eval_runtime': 3.467, 'eval_samples_per_second': 288.434, 'eval_steps_per_second': 18.171, 'epoch': 0.2}
{'loss': 1.2038, 'grad_norm': 0.3296256959438324, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9037236571311951, 'eval_runtime': 3.4558, 'eval_samples_per_second': 289.369, 'eval_steps_per_second': 18.23, 'epoch': 0.24}
{'loss': 1.1928, 'grad_norm': 0.36312583088874817, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8809641003608704, 'eval_runtime': 3.4659, 'eval_samples_per_second': 288.524, 'eval_steps_per_second': 18.177, 'epoch': 0.28}
{'loss': 1.1595, 'grad_norm': 0.2733730673789978, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8659051656723022, 'eval_runtime': 3.4757, 'eval_samples_per_second': 287.71, 'eval_steps_per_second': 18.126, 'epoch': 0.32}
{'loss': 1.1688, 'grad_norm': 0.28282374143600464, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8333227038383484, 'eval_runtime': 3.4789, 'eval_samples_per_second': 287.449, 'eval_steps_per_second': 18.109, 'epoch': 0.36}
{'loss': 1.1678, 'grad_norm': 0.24977809190750122, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8089225888252258, 'eval_runtime': 3.4515, 'eval_samples_per_second': 289.727, 'eval_steps_per_second': 18.253, 'epoch': 0.4}
{'loss': 1.1136, 'grad_norm': 0.2620833218097687, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7758203148841858, 'eval_runtime': 3.4499, 'eval_samples_per_second': 289.859, 'eval_steps_per_second': 18.261, 'epoch': 0.44}
{'loss': 1.1367, 'grad_norm': 0.2633447051048279, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7534527778625488, 'eval_runtime': 3.4505, 'eval_samples_per_second': 289.81, 'eval_steps_per_second': 18.258, 'epoch': 0.48}
{'loss': 1.0794, 'grad_norm': 0.30960917472839355, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7035189270973206, 'eval_runtime': 3.4599, 'eval_samples_per_second': 289.027, 'eval_steps_per_second': 18.209, 'epoch': 0.52}
{'loss': 1.0849, 'grad_norm': 0.27516162395477295, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.636509358882904, 'eval_runtime': 3.4738, 'eval_samples_per_second': 287.873, 'eval_steps_per_second': 18.136, 'epoch': 0.56}
{'loss': 1.0718, 'grad_norm': 0.2756580412387848, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6159279942512512, 'eval_runtime': 3.4691, 'eval_samples_per_second': 288.263, 'eval_steps_per_second': 18.161, 'epoch': 0.6}
{'loss': 1.0799, 'grad_norm': 0.336630254983902, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.625745952129364, 'eval_runtime': 3.4779, 'eval_samples_per_second': 287.528, 'eval_steps_per_second': 18.114, 'epoch': 0.64}
{'loss': 1.0353, 'grad_norm': 0.30529630184173584, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5996159911155701, 'eval_runtime': 3.4833, 'eval_samples_per_second': 287.087, 'eval_steps_per_second': 18.086, 'epoch': 0.68}
{'loss': 1.0743, 'grad_norm': 0.28024357557296753, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5947730541229248, 'eval_runtime': 3.4665, 'eval_samples_per_second': 288.478, 'eval_steps_per_second': 18.174, 'epoch': 0.72}
{'loss': 1.093, 'grad_norm': 0.2848184108734131, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.586457371711731, 'eval_runtime': 3.4634, 'eval_samples_per_second': 288.734, 'eval_steps_per_second': 18.19, 'epoch': 0.76}
{'loss': 1.0661, 'grad_norm': 0.27210989594459534, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5790873765945435, 'eval_runtime': 3.4583, 'eval_samples_per_second': 289.162, 'eval_steps_per_second': 18.217, 'epoch': 0.8}
{'loss': 0.9927, 'grad_norm': 0.26262345910072327, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5649468302726746, 'eval_runtime': 3.4746, 'eval_samples_per_second': 287.804, 'eval_steps_per_second': 18.132, 'epoch': 0.84}
{'loss': 1.0215, 'grad_norm': 0.2969973683357239, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.562730610370636, 'eval_runtime': 3.4901, 'eval_samples_per_second': 286.527, 'eval_steps_per_second': 18.051, 'epoch': 0.88}
{'loss': 1.0305, 'grad_norm': 0.29606130719184875, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5582603812217712, 'eval_runtime': 3.4635, 'eval_samples_per_second': 288.724, 'eval_steps_per_second': 18.19, 'epoch': 0.92}
{'loss': 0.9879, 'grad_norm': 0.32054734230041504, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5528871417045593, 'eval_runtime': 3.4653, 'eval_samples_per_second': 288.574, 'eval_steps_per_second': 18.18, 'epoch': 0.96}
{'loss': 1.0063, 'grad_norm': 0.33756789565086365, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5506955981254578, 'eval_runtime': 3.4646, 'eval_samples_per_second': 288.634, 'eval_steps_per_second': 18.184, 'epoch': 1.0}
{'train_runtime': 281.5698, 'train_samples_per_second': 35.508, 'train_steps_per_second': 2.22, 'train_loss': 1.2250146392822265, 'epoch': 1.0}
train_results:  {'eval_loss': [1.892975091934204, 1.219799280166626, 1.0937610864639282, 0.9978688955307007, 0.927608072757721, 0.9037236571311951, 0.8809641003608704, 0.8659051656723022, 0.8333227038383484, 0.8089225888252258, 0.7758203148841858, 0.7534527778625488, 0.7035189270973206, 0.636509358882904, 0.6159279942512512, 0.625745952129364, 0.5996159911155701, 0.5947730541229248, 0.586457371711731, 0.5790873765945435, 0.5649468302726746, 0.562730610370636, 0.5582603812217712, 0.5528871417045593, 0.5506955981254578], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.892975091934204, 1.219799280166626, 1.0937610864639282, 0.9978688955307007, 0.927608072757721, 0.9037236571311951, 0.8809641003608704, 0.8659051656723022, 0.8333227038383484, 0.8089225888252258, 0.7758203148841858, 0.7534527778625488, 0.7035189270973206, 0.636509358882904, 0.6159279942512512, 0.625745952129364, 0.5996159911155701, 0.5947730541229248, 0.586457371711731, 0.5790873765945435, 0.5649468302726746, 0.562730610370636, 0.5582603812217712, 0.5528871417045593, 0.5506955981254578]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.125186800956726
current iteration best possible eval_loss (full train run):  -0.5506955981254578
max eval_loss so far:  -0.24569274485111237
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.5043 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.9295344921370371
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.9297022368121652
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.9297022352580111
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.9296557545067998
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.9297022358237215
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0243, dtype=torch.float64), tensor(0.1821, dtype=torch.float64), tensor(0.0990, dtype=torch.float64), tensor(0.1079, dtype=torch.float64), 0, tensor(0.1105, dtype=torch.float64), tensor(0.4762, dtype=torch.float64), 13, 0, 0, 0, 1, 1, 83, 1.6216989780348246e-19, 24.502031543125447, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.2683e-18, dtype=torch.float64), tensor(0.0243, dtype=torch.float64), tensor(0.1821, dtype=torch.float64), tensor(0.0990, dtype=torch.float64), tensor(0.1079, dtype=torch.float64), tensor(8.7044e-18, dtype=torch.float64), tensor(0.1105, dtype=torch.float64), tensor(0.4762, dtype=torch.float64), tensor(0.4024, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6481, dtype=torch.float64), tensor(1.6217e-18, dtype=torch.float64), tensor(0.5105, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.024
  sciq: 0.182
  triviaqa: 0.099
  truthfulqa_gen: 0.108
  wikitext: 0
  mmlu: 0.111
  arc_challenge: 0.476

LoRA Parameters:
  lora_r: (83,)
  lora_dropout: (1.6216989780348246e-19,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (24.502031543125447,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  83
lora dropout:  1.6216989780348246e-19
lora alpha:  24.502031543125447
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 39,776,256 || all params: 8,070,037,504 || trainable%: 0.4929
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3723, 'grad_norm': 1.6114557981491089, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1939804553985596, 'eval_runtime': 3.3818, 'eval_samples_per_second': 295.705, 'eval_steps_per_second': 18.629, 'epoch': 0.04}
{'loss': 1.4445, 'grad_norm': 0.738680362701416, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1624433994293213, 'eval_runtime': 3.3838, 'eval_samples_per_second': 295.527, 'eval_steps_per_second': 18.618, 'epoch': 0.08}
{'loss': 1.1647, 'grad_norm': 0.34632110595703125, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0320932865142822, 'eval_runtime': 3.3894, 'eval_samples_per_second': 295.039, 'eval_steps_per_second': 18.587, 'epoch': 0.12}
{'loss': 1.0569, 'grad_norm': 0.28043273091316223, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9471338987350464, 'eval_runtime': 3.3765, 'eval_samples_per_second': 296.165, 'eval_steps_per_second': 18.658, 'epoch': 0.16}
{'loss': 1.0272, 'grad_norm': 0.3608676493167877, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8793339729309082, 'eval_runtime': 3.3721, 'eval_samples_per_second': 296.547, 'eval_steps_per_second': 18.682, 'epoch': 0.2}
{'loss': 0.9787, 'grad_norm': 0.3037818372249603, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8324529528617859, 'eval_runtime': 3.372, 'eval_samples_per_second': 296.563, 'eval_steps_per_second': 18.683, 'epoch': 0.24}
{'loss': 0.9425, 'grad_norm': 0.2728452682495117, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8097026348114014, 'eval_runtime': 3.3709, 'eval_samples_per_second': 296.657, 'eval_steps_per_second': 18.689, 'epoch': 0.28}
{'loss': 0.9701, 'grad_norm': 0.25797751545906067, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8091511130332947, 'eval_runtime': 3.3764, 'eval_samples_per_second': 296.174, 'eval_steps_per_second': 18.659, 'epoch': 0.32}
{'loss': 0.9907, 'grad_norm': 0.28582340478897095, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8085517287254333, 'eval_runtime': 3.3805, 'eval_samples_per_second': 295.811, 'eval_steps_per_second': 18.636, 'epoch': 0.36}
{'loss': 0.9226, 'grad_norm': 0.29105502367019653, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7908530235290527, 'eval_runtime': 3.386, 'eval_samples_per_second': 295.333, 'eval_steps_per_second': 18.606, 'epoch': 0.4}
{'loss': 0.9238, 'grad_norm': 0.27747076749801636, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7749268412590027, 'eval_runtime': 3.383, 'eval_samples_per_second': 295.599, 'eval_steps_per_second': 18.623, 'epoch': 0.44}
{'loss': 0.9491, 'grad_norm': 0.2412004917860031, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7773693799972534, 'eval_runtime': 3.3854, 'eval_samples_per_second': 295.384, 'eval_steps_per_second': 18.609, 'epoch': 0.48}
{'loss': 0.9294, 'grad_norm': 0.3131421208381653, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7808565497398376, 'eval_runtime': 3.3834, 'eval_samples_per_second': 295.558, 'eval_steps_per_second': 18.62, 'epoch': 0.52}
{'loss': 0.8798, 'grad_norm': 0.39548224210739136, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7642338871955872, 'eval_runtime': 3.3996, 'eval_samples_per_second': 294.15, 'eval_steps_per_second': 18.531, 'epoch': 0.56}
{'loss': 0.8881, 'grad_norm': 0.27911868691444397, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7621307969093323, 'eval_runtime': 3.4329, 'eval_samples_per_second': 291.3, 'eval_steps_per_second': 18.352, 'epoch': 0.6}
{'loss': 0.8663, 'grad_norm': 0.2999909222126007, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7417006492614746, 'eval_runtime': 3.4247, 'eval_samples_per_second': 291.995, 'eval_steps_per_second': 18.396, 'epoch': 0.64}
{'loss': 0.9288, 'grad_norm': 0.35670721530914307, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7471544146537781, 'eval_runtime': 3.4387, 'eval_samples_per_second': 290.804, 'eval_steps_per_second': 18.321, 'epoch': 0.68}
{'loss': 0.8473, 'grad_norm': 0.3314306437969208, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7288736701011658, 'eval_runtime': 3.4374, 'eval_samples_per_second': 290.918, 'eval_steps_per_second': 18.328, 'epoch': 0.72}
{'loss': 0.8751, 'grad_norm': 0.39908650517463684, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7247885465621948, 'eval_runtime': 3.4344, 'eval_samples_per_second': 291.171, 'eval_steps_per_second': 18.344, 'epoch': 0.76}
{'loss': 0.8813, 'grad_norm': 0.37282970547676086, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7263790965080261, 'eval_runtime': 3.394, 'eval_samples_per_second': 294.634, 'eval_steps_per_second': 18.562, 'epoch': 0.8}
{'loss': 0.8624, 'grad_norm': 0.41921812295913696, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7346377372741699, 'eval_runtime': 3.4125, 'eval_samples_per_second': 293.036, 'eval_steps_per_second': 18.461, 'epoch': 0.84}
{'loss': 0.8457, 'grad_norm': 0.3784947991371155, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7229226231575012, 'eval_runtime': 3.3946, 'eval_samples_per_second': 294.59, 'eval_steps_per_second': 18.559, 'epoch': 0.88}
{'loss': 0.8415, 'grad_norm': 0.44396355748176575, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7265045046806335, 'eval_runtime': 3.3911, 'eval_samples_per_second': 294.886, 'eval_steps_per_second': 18.578, 'epoch': 0.92}
{'loss': 0.8684, 'grad_norm': 0.3766159117221832, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7228031754493713, 'eval_runtime': 3.3951, 'eval_samples_per_second': 294.541, 'eval_steps_per_second': 18.556, 'epoch': 0.96}
{'loss': 0.8621, 'grad_norm': 0.4365071952342987, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.723082423210144, 'eval_runtime': 3.3966, 'eval_samples_per_second': 294.413, 'eval_steps_per_second': 18.548, 'epoch': 1.0}
{'train_runtime': 261.3859, 'train_samples_per_second': 38.25, 'train_steps_per_second': 2.391, 'train_loss': 1.044766162109375, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1939804553985596, 1.1624433994293213, 1.0320932865142822, 0.9471338987350464, 0.8793339729309082, 0.8324529528617859, 0.8097026348114014, 0.8091511130332947, 0.8085517287254333, 0.7908530235290527, 0.7749268412590027, 0.7773693799972534, 0.7808565497398376, 0.7642338871955872, 0.7621307969093323, 0.7417006492614746, 0.7471544146537781, 0.7288736701011658, 0.7247885465621948, 0.7263790965080261, 0.7346377372741699, 0.7229226231575012, 0.7265045046806335, 0.7228031754493713, 0.723082423210144], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1939804553985596, 1.1624433994293213, 1.0320932865142822, 0.9471338987350464, 0.8793339729309082, 0.8324529528617859, 0.8097026348114014, 0.8091511130332947, 0.8085517287254333, 0.7908530235290527, 0.7749268412590027, 0.7773693799972534, 0.7808565497398376, 0.7642338871955872, 0.7621307969093323, 0.7417006492614746, 0.7471544146537781, 0.7288736701011658, 0.7247885465621948, 0.7263790965080261, 0.7346377372741699, 0.7229226231575012, 0.7265045046806335, 0.7228031754493713, 0.723082423210144]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.723082423210144
max eval_loss so far:  -0.24569274485111237
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.3346 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.9290629809091863
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.9290629807275597
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.9291523431613649
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.9336092595318667
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.9290629807275683
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0253, dtype=torch.float64), tensor(0.4029, dtype=torch.float64), tensor(0.1363, dtype=torch.float64), tensor(0.3572, dtype=torch.float64), 0, tensor(0.0249, dtype=torch.float64), tensor(0.0534, dtype=torch.float64), 15, 1, 1, 1, 1, 1, 2, 8.673617379884038e-20, 25.987460987107283, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.5191e-17, dtype=torch.float64), tensor(0.0253, dtype=torch.float64), tensor(0.4029, dtype=torch.float64), tensor(0.1363, dtype=torch.float64), tensor(0.3572, dtype=torch.float64), tensor(1.9801e-17, dtype=torch.float64), tensor(0.0249, dtype=torch.float64), tensor(0.0534, dtype=torch.float64), tensor(0.4774, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(8.6736e-19, dtype=torch.float64), tensor(0.5414, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.025
  sciq: 0.403
  triviaqa: 0.136
  truthfulqa_gen: 0.357
  wikitext: 0
  mmlu: 0.025
  arc_challenge: 0.053

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (8.673617379884038e-20,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (25.987460987107283,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  2
lora dropout:  8.673617379884038e-20
lora alpha:  25.987460987107283
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,058,240 || all params: 8,032,319,488 || trainable%: 0.0256
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4826, 'grad_norm': 11.750298500061035, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5575039386749268, 'eval_runtime': 3.5807, 'eval_samples_per_second': 279.272, 'eval_steps_per_second': 17.594, 'epoch': 0.04}
{'loss': 1.4572, 'grad_norm': 5.641186237335205, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9787365794181824, 'eval_runtime': 3.5983, 'eval_samples_per_second': 277.907, 'eval_steps_per_second': 17.508, 'epoch': 0.08}
{'loss': 1.1002, 'grad_norm': 2.105607509613037, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8416869044303894, 'eval_runtime': 3.5791, 'eval_samples_per_second': 279.402, 'eval_steps_per_second': 17.602, 'epoch': 0.12}
{'loss': 0.96, 'grad_norm': 2.58552885055542, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7333568334579468, 'eval_runtime': 3.5839, 'eval_samples_per_second': 279.024, 'eval_steps_per_second': 17.578, 'epoch': 0.16}
{'loss': 0.9371, 'grad_norm': 2.6699917316436768, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6951777338981628, 'eval_runtime': 3.5861, 'eval_samples_per_second': 278.851, 'eval_steps_per_second': 17.568, 'epoch': 0.2}
{'loss': 0.9303, 'grad_norm': 2.40256404876709, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6717043519020081, 'eval_runtime': 3.5902, 'eval_samples_per_second': 278.533, 'eval_steps_per_second': 17.548, 'epoch': 0.24}
{'loss': 0.8675, 'grad_norm': 2.3048548698425293, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6496613621711731, 'eval_runtime': 3.5962, 'eval_samples_per_second': 278.068, 'eval_steps_per_second': 17.518, 'epoch': 0.28}
{'loss': 0.8752, 'grad_norm': 2.2595410346984863, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6232591271400452, 'eval_runtime': 3.5937, 'eval_samples_per_second': 278.266, 'eval_steps_per_second': 17.531, 'epoch': 0.32}
{'loss': 0.8309, 'grad_norm': 2.2410056591033936, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5829318761825562, 'eval_runtime': 3.5969, 'eval_samples_per_second': 278.019, 'eval_steps_per_second': 17.515, 'epoch': 0.36}
{'loss': 0.87, 'grad_norm': 2.0448665618896484, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5705241560935974, 'eval_runtime': 3.6052, 'eval_samples_per_second': 277.374, 'eval_steps_per_second': 17.475, 'epoch': 0.4}
{'loss': 0.8515, 'grad_norm': 1.7275261878967285, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.54622483253479, 'eval_runtime': 3.5922, 'eval_samples_per_second': 278.382, 'eval_steps_per_second': 17.538, 'epoch': 0.44}
{'loss': 0.8169, 'grad_norm': 1.9442719221115112, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5103548169136047, 'eval_runtime': 3.5931, 'eval_samples_per_second': 278.313, 'eval_steps_per_second': 17.534, 'epoch': 0.48}
{'loss': 0.8094, 'grad_norm': 2.60207200050354, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4885123372077942, 'eval_runtime': 3.5976, 'eval_samples_per_second': 277.961, 'eval_steps_per_second': 17.512, 'epoch': 0.52}
{'loss': 0.8694, 'grad_norm': 1.3721128702163696, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.4691944718360901, 'eval_runtime': 3.5885, 'eval_samples_per_second': 278.67, 'eval_steps_per_second': 17.556, 'epoch': 0.56}
{'loss': 0.775, 'grad_norm': 1.7995810508728027, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4415012300014496, 'eval_runtime': 3.5917, 'eval_samples_per_second': 278.417, 'eval_steps_per_second': 17.54, 'epoch': 0.6}
{'loss': 0.8315, 'grad_norm': 2.336846113204956, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.42808666825294495, 'eval_runtime': 3.5873, 'eval_samples_per_second': 278.761, 'eval_steps_per_second': 17.562, 'epoch': 0.64}
{'loss': 0.8044, 'grad_norm': 1.683579921722412, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.40712201595306396, 'eval_runtime': 3.5866, 'eval_samples_per_second': 278.813, 'eval_steps_per_second': 17.565, 'epoch': 0.68}
{'loss': 0.7681, 'grad_norm': 2.2720930576324463, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3835711181163788, 'eval_runtime': 3.5944, 'eval_samples_per_second': 278.212, 'eval_steps_per_second': 17.527, 'epoch': 0.72}
{'loss': 0.7383, 'grad_norm': 2.1823863983154297, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.37196725606918335, 'eval_runtime': 3.5766, 'eval_samples_per_second': 279.597, 'eval_steps_per_second': 17.615, 'epoch': 0.76}
{'loss': 0.7711, 'grad_norm': 1.8598188161849976, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3527776896953583, 'eval_runtime': 3.5884, 'eval_samples_per_second': 278.672, 'eval_steps_per_second': 17.556, 'epoch': 0.8}
{'loss': 0.7907, 'grad_norm': 2.4632673263549805, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.33566826581954956, 'eval_runtime': 3.5933, 'eval_samples_per_second': 278.293, 'eval_steps_per_second': 17.532, 'epoch': 0.84}
{'loss': 0.7757, 'grad_norm': 3.1805524826049805, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.31819456815719604, 'eval_runtime': 3.5893, 'eval_samples_per_second': 278.608, 'eval_steps_per_second': 17.552, 'epoch': 0.88}
{'loss': 0.8096, 'grad_norm': 1.8687098026275635, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3142279088497162, 'eval_runtime': 3.6003, 'eval_samples_per_second': 277.752, 'eval_steps_per_second': 17.498, 'epoch': 0.92}
{'loss': 0.7688, 'grad_norm': 1.6647194623947144, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.30542150139808655, 'eval_runtime': 3.6033, 'eval_samples_per_second': 277.525, 'eval_steps_per_second': 17.484, 'epoch': 0.96}
{'loss': 0.7995, 'grad_norm': 2.1930949687957764, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.30426260828971863, 'eval_runtime': 3.6167, 'eval_samples_per_second': 276.495, 'eval_steps_per_second': 17.419, 'epoch': 1.0}
{'train_runtime': 239.1171, 'train_samples_per_second': 41.808, 'train_steps_per_second': 2.614, 'train_loss': 0.9716309020996093, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5575039386749268, 0.9787365794181824, 0.8416869044303894, 0.7333568334579468, 0.6951777338981628, 0.6717043519020081, 0.6496613621711731, 0.6232591271400452, 0.5829318761825562, 0.5705241560935974, 0.54622483253479, 0.5103548169136047, 0.4885123372077942, 0.4691944718360901, 0.4415012300014496, 0.42808666825294495, 0.40712201595306396, 0.3835711181163788, 0.37196725606918335, 0.3527776896953583, 0.33566826581954956, 0.31819456815719604, 0.3142279088497162, 0.30542150139808655, 0.30426260828971863], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5575039386749268, 0.9787365794181824, 0.8416869044303894, 0.7333568334579468, 0.6951777338981628, 0.6717043519020081, 0.6496613621711731, 0.6232591271400452, 0.5829318761825562, 0.5705241560935974, 0.54622483253479, 0.5103548169136047, 0.4885123372077942, 0.4691944718360901, 0.4415012300014496, 0.42808666825294495, 0.40712201595306396, 0.3835711181163788, 0.37196725606918335, 0.3527776896953583, 0.33566826581954956, 0.31819456815719604, 0.3142279088497162, 0.30542150139808655, 0.30426260828971863]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.30426260828971863
max eval_loss so far:  -0.24569274485111237
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5372 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.9811532301823123
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.9811577305671311
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.9811577306614181
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.9811577304840766
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.9811608664831508
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0776, dtype=torch.float64), tensor(0.3650, dtype=torch.float64), tensor(0.0432, dtype=torch.float64), tensor(0.1900, dtype=torch.float64), 0, tensor(0.1162, dtype=torch.float64), 0, tensor(0.0556, dtype=torch.float64), tensor(0.1525, dtype=torch.float64), 13, 0, 1, 1, 1, 1, 69, 4.3172370583502765e-20, 34.04989285200343, 1]
normalized proposed parameters for next round by BO: [tensor(0.0776, dtype=torch.float64), tensor(0.3650, dtype=torch.float64), tensor(0.0432, dtype=torch.float64), tensor(0.1900, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1162, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0556, dtype=torch.float64), tensor(0.1525, dtype=torch.float64), tensor(0.4109, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5408, dtype=torch.float64), tensor(4.3172e-19, dtype=torch.float64), tensor(0.7094, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.078
  gsm8k: 0.365
  rowan_hellaswag: 0.043
  sciq: 0.19
  triviaqa: 0
  truthfulqa_gen: 0.116
  wikitext: 0
  mmlu: 0.056
  arc_challenge: 0.153

LoRA Parameters:
  lora_r: (69,)
  lora_dropout: (4.3172370583502765e-20,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (34.04989285200343,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  69
lora dropout:  4.3172370583502765e-20
lora alpha:  34.04989285200343
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 54,193,152 || all params: 8,084,454,400 || trainable%: 0.6703
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6312, 'grad_norm': 0.8166831135749817, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7060319185256958, 'eval_runtime': 3.5452, 'eval_samples_per_second': 282.07, 'eval_steps_per_second': 17.77, 'epoch': 0.04}
{'loss': 1.3178, 'grad_norm': 0.365089476108551, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0563474893569946, 'eval_runtime': 3.5614, 'eval_samples_per_second': 280.788, 'eval_steps_per_second': 17.69, 'epoch': 0.08}
{'loss': 1.0821, 'grad_norm': 0.3517027795314789, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9667258262634277, 'eval_runtime': 3.5821, 'eval_samples_per_second': 279.165, 'eval_steps_per_second': 17.587, 'epoch': 0.12}
{'loss': 1.053, 'grad_norm': 0.3736557364463806, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8636379241943359, 'eval_runtime': 3.6021, 'eval_samples_per_second': 277.619, 'eval_steps_per_second': 17.49, 'epoch': 0.16}
{'loss': 0.974, 'grad_norm': 0.33205991983413696, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8447223901748657, 'eval_runtime': 3.5731, 'eval_samples_per_second': 279.87, 'eval_steps_per_second': 17.632, 'epoch': 0.2}
{'loss': 1.0173, 'grad_norm': 0.3076663017272949, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8214605450630188, 'eval_runtime': 3.5617, 'eval_samples_per_second': 280.762, 'eval_steps_per_second': 17.688, 'epoch': 0.24}
{'loss': 0.9395, 'grad_norm': 0.29720941185951233, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8200700283050537, 'eval_runtime': 3.5636, 'eval_samples_per_second': 280.619, 'eval_steps_per_second': 17.679, 'epoch': 0.28}
{'loss': 0.9465, 'grad_norm': 0.2730400562286377, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8074702024459839, 'eval_runtime': 3.578, 'eval_samples_per_second': 279.483, 'eval_steps_per_second': 17.607, 'epoch': 0.32}
{'loss': 0.9529, 'grad_norm': 0.3227200210094452, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7839997410774231, 'eval_runtime': 3.5664, 'eval_samples_per_second': 280.395, 'eval_steps_per_second': 17.665, 'epoch': 0.36}
{'loss': 0.9264, 'grad_norm': 0.30819621682167053, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7829679250717163, 'eval_runtime': 3.5656, 'eval_samples_per_second': 280.458, 'eval_steps_per_second': 17.669, 'epoch': 0.4}
{'loss': 0.9403, 'grad_norm': 0.24060660600662231, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.751014232635498, 'eval_runtime': 3.5735, 'eval_samples_per_second': 279.837, 'eval_steps_per_second': 17.63, 'epoch': 0.44}
{'loss': 0.9139, 'grad_norm': 0.24860745668411255, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7373836636543274, 'eval_runtime': 3.5791, 'eval_samples_per_second': 279.401, 'eval_steps_per_second': 17.602, 'epoch': 0.48}
{'loss': 0.9156, 'grad_norm': 0.2316305935382843, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7341979742050171, 'eval_runtime': 3.569, 'eval_samples_per_second': 280.187, 'eval_steps_per_second': 17.652, 'epoch': 0.52}
{'loss': 0.9475, 'grad_norm': 0.23237963020801544, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7327026128768921, 'eval_runtime': 3.5706, 'eval_samples_per_second': 280.066, 'eval_steps_per_second': 17.644, 'epoch': 0.56}
{'loss': 0.9395, 'grad_norm': 0.24026601016521454, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7273722290992737, 'eval_runtime': 3.5812, 'eval_samples_per_second': 279.235, 'eval_steps_per_second': 17.592, 'epoch': 0.6}
{'loss': 0.9006, 'grad_norm': 0.26015159487724304, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7107554078102112, 'eval_runtime': 3.6048, 'eval_samples_per_second': 277.408, 'eval_steps_per_second': 17.477, 'epoch': 0.64}
{'loss': 0.9183, 'grad_norm': 0.22496262192726135, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6958520412445068, 'eval_runtime': 3.6163, 'eval_samples_per_second': 276.525, 'eval_steps_per_second': 17.421, 'epoch': 0.68}
{'loss': 0.8883, 'grad_norm': 0.2272018939256668, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6981455087661743, 'eval_runtime': 3.6177, 'eval_samples_per_second': 276.42, 'eval_steps_per_second': 17.414, 'epoch': 0.72}
{'loss': 0.9212, 'grad_norm': 0.2480492889881134, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6875061392784119, 'eval_runtime': 3.6047, 'eval_samples_per_second': 277.415, 'eval_steps_per_second': 17.477, 'epoch': 0.76}
{'loss': 0.9033, 'grad_norm': 0.27266839146614075, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6855940222740173, 'eval_runtime': 3.615, 'eval_samples_per_second': 276.625, 'eval_steps_per_second': 17.427, 'epoch': 0.8}
{'loss': 0.8783, 'grad_norm': 0.2650759518146515, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.675517737865448, 'eval_runtime': 3.6021, 'eval_samples_per_second': 277.616, 'eval_steps_per_second': 17.49, 'epoch': 0.84}
{'loss': 0.9253, 'grad_norm': 0.2327466607093811, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6673281788825989, 'eval_runtime': 3.6017, 'eval_samples_per_second': 277.65, 'eval_steps_per_second': 17.492, 'epoch': 0.88}
{'loss': 0.9035, 'grad_norm': 0.2214474081993103, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6744762063026428, 'eval_runtime': 3.5735, 'eval_samples_per_second': 279.839, 'eval_steps_per_second': 17.63, 'epoch': 0.92}
{'loss': 0.9132, 'grad_norm': 0.2556506097316742, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6651938557624817, 'eval_runtime': 3.5585, 'eval_samples_per_second': 281.017, 'eval_steps_per_second': 17.704, 'epoch': 0.96}
{'loss': 0.8598, 'grad_norm': 0.24432535469532013, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6650970578193665, 'eval_runtime': 3.568, 'eval_samples_per_second': 280.267, 'eval_steps_per_second': 17.657, 'epoch': 1.0}
{'train_runtime': 313.3449, 'train_samples_per_second': 31.904, 'train_steps_per_second': 1.995, 'train_loss': 1.0203699737548828, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7060319185256958, 1.0563474893569946, 0.9667258262634277, 0.8636379241943359, 0.8447223901748657, 0.8214605450630188, 0.8200700283050537, 0.8074702024459839, 0.7839997410774231, 0.7829679250717163, 0.751014232635498, 0.7373836636543274, 0.7341979742050171, 0.7327026128768921, 0.7273722290992737, 0.7107554078102112, 0.6958520412445068, 0.6981455087661743, 0.6875061392784119, 0.6855940222740173, 0.675517737865448, 0.6673281788825989, 0.6744762063026428, 0.6651938557624817, 0.6650970578193665], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7060319185256958, 1.0563474893569946, 0.9667258262634277, 0.8636379241943359, 0.8447223901748657, 0.8214605450630188, 0.8200700283050537, 0.8074702024459839, 0.7839997410774231, 0.7829679250717163, 0.751014232635498, 0.7373836636543274, 0.7341979742050171, 0.7327026128768921, 0.7273722290992737, 0.7107554078102112, 0.6958520412445068, 0.6981455087661743, 0.6875061392784119, 0.6855940222740173, 0.675517737865448, 0.6673281788825989, 0.6744762063026428, 0.6651938557624817, 0.6650970578193665]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1255121231079102
current iteration best possible eval_loss (full train run):  -0.6650970578193665
max eval_loss so far:  -0.24569274485111237
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.7657 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.9594950220861034
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.9594950077358275
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.9594950077358173
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.9594949860101617
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.9594950077358175
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1232, dtype=torch.float64), tensor(0.0344, dtype=torch.float64), tensor(0.3416, dtype=torch.float64), 0, tensor(0.2816, dtype=torch.float64), tensor(0.0555, dtype=torch.float64), 0, tensor(0.1637, dtype=torch.float64), 17, 1, 0, 1, 1, 1, 128, 2.5930463851202934e-18, 47.8954707142289, 1]
normalized proposed parameters for next round by BO: [tensor(6.8071e-18, dtype=torch.float64), tensor(0.1232, dtype=torch.float64), tensor(0.0344, dtype=torch.float64), tensor(0.3416, dtype=torch.float64), tensor(3.8551e-18, dtype=torch.float64), tensor(0.2816, dtype=torch.float64), tensor(0.0555, dtype=torch.float64), tensor(2.2414e-18, dtype=torch.float64), tensor(0.1637, dtype=torch.float64), tensor(0.5248, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.5930e-17, dtype=torch.float64), tensor(0.9978, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.123
  rowan_hellaswag: 0.034
  sciq: 0.342
  triviaqa: 0
  truthfulqa_gen: 0.282
  wikitext: 0.056
  mmlu: 0
  arc_challenge: 0.164

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.5930463851202934e-18,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.8954707142289,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.5930463851202934e-18
lora alpha:  47.8954707142289
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 138,149,888 || all params: 8,168,411,136 || trainable%: 1.6913
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9202, 'grad_norm': 0.6472440958023071, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4604219198226929, 'eval_runtime': 3.5393, 'eval_samples_per_second': 282.541, 'eval_steps_per_second': 17.8, 'epoch': 0.04}
{'loss': 1.2784, 'grad_norm': 0.7089095711708069, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.971817135810852, 'eval_runtime': 3.5217, 'eval_samples_per_second': 283.956, 'eval_steps_per_second': 17.889, 'epoch': 0.08}
{'loss': 1.1356, 'grad_norm': 0.3009563684463501, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.845942497253418, 'eval_runtime': 3.5313, 'eval_samples_per_second': 283.178, 'eval_steps_per_second': 17.84, 'epoch': 0.12}
{'loss': 0.9874, 'grad_norm': 0.26628702878952026, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7364230155944824, 'eval_runtime': 3.5296, 'eval_samples_per_second': 283.321, 'eval_steps_per_second': 17.849, 'epoch': 0.16}
{'loss': 1.0015, 'grad_norm': 0.4375637173652649, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7070112824440002, 'eval_runtime': 3.5441, 'eval_samples_per_second': 282.159, 'eval_steps_per_second': 17.776, 'epoch': 0.2}
{'loss': 0.9967, 'grad_norm': 0.2988058030605316, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6829906702041626, 'eval_runtime': 3.5492, 'eval_samples_per_second': 281.752, 'eval_steps_per_second': 17.75, 'epoch': 0.24}
{'loss': 0.9701, 'grad_norm': 0.33941373229026794, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6442710161209106, 'eval_runtime': 3.55, 'eval_samples_per_second': 281.692, 'eval_steps_per_second': 17.747, 'epoch': 0.28}
{'loss': 0.958, 'grad_norm': 0.2350364476442337, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6197354793548584, 'eval_runtime': 3.5485, 'eval_samples_per_second': 281.806, 'eval_steps_per_second': 17.754, 'epoch': 0.32}
{'loss': 0.9738, 'grad_norm': 0.2805819809436798, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5857612490653992, 'eval_runtime': 3.5537, 'eval_samples_per_second': 281.396, 'eval_steps_per_second': 17.728, 'epoch': 0.36}
{'loss': 0.9155, 'grad_norm': 0.2822277843952179, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.587401270866394, 'eval_runtime': 3.5323, 'eval_samples_per_second': 283.1, 'eval_steps_per_second': 17.835, 'epoch': 0.4}
{'loss': 0.9058, 'grad_norm': 0.31182214617729187, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5308340787887573, 'eval_runtime': 3.5321, 'eval_samples_per_second': 283.12, 'eval_steps_per_second': 17.837, 'epoch': 0.44}
{'loss': 0.8626, 'grad_norm': 0.2757168710231781, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5061281323432922, 'eval_runtime': 3.5353, 'eval_samples_per_second': 282.865, 'eval_steps_per_second': 17.82, 'epoch': 0.48}
{'loss': 0.8761, 'grad_norm': 0.29794615507125854, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5013540387153625, 'eval_runtime': 3.5441, 'eval_samples_per_second': 282.16, 'eval_steps_per_second': 17.776, 'epoch': 0.52}
{'loss': 0.8765, 'grad_norm': 0.26877135038375854, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.484100878238678, 'eval_runtime': 3.5362, 'eval_samples_per_second': 282.792, 'eval_steps_per_second': 17.816, 'epoch': 0.56}
{'loss': 0.9058, 'grad_norm': 0.3470692038536072, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.47400951385498047, 'eval_runtime': 3.5366, 'eval_samples_per_second': 282.754, 'eval_steps_per_second': 17.813, 'epoch': 0.6}
{'loss': 0.8317, 'grad_norm': 0.29272451996803284, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.44483324885368347, 'eval_runtime': 3.5346, 'eval_samples_per_second': 282.92, 'eval_steps_per_second': 17.824, 'epoch': 0.64}
{'loss': 0.8309, 'grad_norm': 0.3118166923522949, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.42009472846984863, 'eval_runtime': 3.5374, 'eval_samples_per_second': 282.696, 'eval_steps_per_second': 17.81, 'epoch': 0.68}
{'loss': 0.7815, 'grad_norm': 0.4388240873813629, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.4002871811389923, 'eval_runtime': 3.5414, 'eval_samples_per_second': 282.377, 'eval_steps_per_second': 17.79, 'epoch': 0.72}
{'loss': 0.8694, 'grad_norm': 0.30603957176208496, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3814849853515625, 'eval_runtime': 3.5525, 'eval_samples_per_second': 281.489, 'eval_steps_per_second': 17.734, 'epoch': 0.76}
{'loss': 0.8778, 'grad_norm': 0.3758451044559479, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.35969671607017517, 'eval_runtime': 3.5632, 'eval_samples_per_second': 280.648, 'eval_steps_per_second': 17.681, 'epoch': 0.8}
{'loss': 0.8484, 'grad_norm': 0.3416779339313507, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3402251601219177, 'eval_runtime': 3.5574, 'eval_samples_per_second': 281.107, 'eval_steps_per_second': 17.71, 'epoch': 0.84}
{'loss': 0.8877, 'grad_norm': 0.2843186557292938, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3317917287349701, 'eval_runtime': 3.555, 'eval_samples_per_second': 281.297, 'eval_steps_per_second': 17.722, 'epoch': 0.88}
{'loss': 0.7902, 'grad_norm': 0.24738048017024994, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3221055269241333, 'eval_runtime': 3.5588, 'eval_samples_per_second': 280.99, 'eval_steps_per_second': 17.702, 'epoch': 0.92}
{'loss': 0.792, 'grad_norm': 0.3585575520992279, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.31446170806884766, 'eval_runtime': 3.5569, 'eval_samples_per_second': 281.141, 'eval_steps_per_second': 17.712, 'epoch': 0.96}
{'loss': 0.7875, 'grad_norm': 0.2976147532463074, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.31098079681396484, 'eval_runtime': 3.5676, 'eval_samples_per_second': 280.3, 'eval_steps_per_second': 17.659, 'epoch': 1.0}
{'train_runtime': 293.4451, 'train_samples_per_second': 34.068, 'train_steps_per_second': 2.13, 'train_loss': 0.9944396118164063, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4604219198226929, 0.971817135810852, 0.845942497253418, 0.7364230155944824, 0.7070112824440002, 0.6829906702041626, 0.6442710161209106, 0.6197354793548584, 0.5857612490653992, 0.587401270866394, 0.5308340787887573, 0.5061281323432922, 0.5013540387153625, 0.484100878238678, 0.47400951385498047, 0.44483324885368347, 0.42009472846984863, 0.4002871811389923, 0.3814849853515625, 0.35969671607017517, 0.3402251601219177, 0.3317917287349701, 0.3221055269241333, 0.31446170806884766, 0.31098079681396484], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4604219198226929, 0.971817135810852, 0.845942497253418, 0.7364230155944824, 0.7070112824440002, 0.6829906702041626, 0.6442710161209106, 0.6197354793548584, 0.5857612490653992, 0.587401270866394, 0.5308340787887573, 0.5061281323432922, 0.5013540387153625, 0.484100878238678, 0.47400951385498047, 0.44483324885368347, 0.42009472846984863, 0.4002871811389923, 0.3814849853515625, 0.35969671607017517, 0.3402251601219177, 0.3317917287349701, 0.3221055269241333, 0.31446170806884766, 0.31098079681396484]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.121098518371582
current iteration best possible eval_loss (full train run):  -0.31098079681396484
max eval_loss so far:  -0.24569274485111237
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.7604 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.9806327260184543
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.9839592796210473
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.9806327260184547
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.9806328253255463
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.9830137800345624
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1413, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), 0, tensor(0.7253, dtype=torch.float64), tensor(0.0260, dtype=torch.float64), tensor(0.0372, dtype=torch.float64), 0, 15, 1, 1, 1, 0, 1, 128, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.0343e-19, dtype=torch.float64), tensor(9.6863e-18, dtype=torch.float64), tensor(0.1413, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), tensor(6.2942e-18, dtype=torch.float64), tensor(0.7253, dtype=torch.float64), tensor(0.0260, dtype=torch.float64), tensor(0.0372, dtype=torch.float64), tensor(3.0728e-18, dtype=torch.float64), tensor(0.4666, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.141
  sciq: 0.07
  triviaqa: 0
  truthfulqa_gen: 0.725
  wikitext: 0.026
  mmlu: 0.037
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,337,920 || all params: 8,126,599,168 || trainable%: 1.1855
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4135, 'grad_norm': 0.526736319065094, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.673366665840149, 'eval_runtime': 3.4017, 'eval_samples_per_second': 293.972, 'eval_steps_per_second': 18.52, 'epoch': 0.04}
{'loss': 1.6839, 'grad_norm': 0.35149598121643066, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0939699411392212, 'eval_runtime': 3.4177, 'eval_samples_per_second': 292.593, 'eval_steps_per_second': 18.433, 'epoch': 0.08}
{'loss': 1.4171, 'grad_norm': 0.35042187571525574, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8836808800697327, 'eval_runtime': 3.4098, 'eval_samples_per_second': 293.276, 'eval_steps_per_second': 18.476, 'epoch': 0.12}
{'loss': 1.2372, 'grad_norm': 0.315135657787323, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7905645966529846, 'eval_runtime': 3.4012, 'eval_samples_per_second': 294.01, 'eval_steps_per_second': 18.523, 'epoch': 0.16}
{'loss': 1.1666, 'grad_norm': 0.33524876832962036, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7323802709579468, 'eval_runtime': 3.4053, 'eval_samples_per_second': 293.658, 'eval_steps_per_second': 18.5, 'epoch': 0.2}
{'loss': 1.1513, 'grad_norm': 0.32511594891548157, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.646122932434082, 'eval_runtime': 3.4137, 'eval_samples_per_second': 292.937, 'eval_steps_per_second': 18.455, 'epoch': 0.24}
{'loss': 1.1962, 'grad_norm': 0.4083022177219391, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.571088969707489, 'eval_runtime': 3.4213, 'eval_samples_per_second': 292.284, 'eval_steps_per_second': 18.414, 'epoch': 0.28}
{'loss': 1.1156, 'grad_norm': 0.37118715047836304, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.4949139356613159, 'eval_runtime': 3.416, 'eval_samples_per_second': 292.737, 'eval_steps_per_second': 18.442, 'epoch': 0.32}
{'loss': 1.074, 'grad_norm': 0.4402608275413513, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.4283502697944641, 'eval_runtime': 3.4178, 'eval_samples_per_second': 292.587, 'eval_steps_per_second': 18.433, 'epoch': 0.36}
{'loss': 0.9835, 'grad_norm': 0.6285656690597534, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.386067271232605, 'eval_runtime': 3.426, 'eval_samples_per_second': 291.889, 'eval_steps_per_second': 18.389, 'epoch': 0.4}
{'loss': 1.029, 'grad_norm': 0.3277777433395386, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.34809330105781555, 'eval_runtime': 3.4308, 'eval_samples_per_second': 291.475, 'eval_steps_per_second': 18.363, 'epoch': 0.44}
{'loss': 1.0247, 'grad_norm': 0.3661573529243469, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3325328528881073, 'eval_runtime': 3.4231, 'eval_samples_per_second': 292.133, 'eval_steps_per_second': 18.404, 'epoch': 0.48}
{'loss': 0.8819, 'grad_norm': 0.4305666983127594, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3021934926509857, 'eval_runtime': 3.4227, 'eval_samples_per_second': 292.166, 'eval_steps_per_second': 18.406, 'epoch': 0.52}
{'loss': 0.8426, 'grad_norm': 0.30674922466278076, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.2773236632347107, 'eval_runtime': 3.442, 'eval_samples_per_second': 290.528, 'eval_steps_per_second': 18.303, 'epoch': 0.56}
{'loss': 0.9346, 'grad_norm': 0.3308703303337097, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.25790879130363464, 'eval_runtime': 3.4405, 'eval_samples_per_second': 290.656, 'eval_steps_per_second': 18.311, 'epoch': 0.6}
{'loss': 0.9622, 'grad_norm': 0.4353812634944916, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.24091342091560364, 'eval_runtime': 3.4492, 'eval_samples_per_second': 289.92, 'eval_steps_per_second': 18.265, 'epoch': 0.64}
{'loss': 0.9508, 'grad_norm': 0.27366095781326294, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2272706776857376, 'eval_runtime': 3.4233, 'eval_samples_per_second': 292.119, 'eval_steps_per_second': 18.403, 'epoch': 0.68}
{'loss': 0.8277, 'grad_norm': 0.44453591108322144, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.2096683233976364, 'eval_runtime': 3.4259, 'eval_samples_per_second': 291.892, 'eval_steps_per_second': 18.389, 'epoch': 0.72}
{'loss': 0.8879, 'grad_norm': 0.24873879551887512, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.19995944201946259, 'eval_runtime': 3.4225, 'eval_samples_per_second': 292.184, 'eval_steps_per_second': 18.408, 'epoch': 0.76}
{'loss': 0.9584, 'grad_norm': 0.2602149248123169, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.1896408200263977, 'eval_runtime': 3.4228, 'eval_samples_per_second': 292.155, 'eval_steps_per_second': 18.406, 'epoch': 0.8}
{'loss': 0.932, 'grad_norm': 0.2840523421764374, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.18313004076480865, 'eval_runtime': 3.4261, 'eval_samples_per_second': 291.876, 'eval_steps_per_second': 18.388, 'epoch': 0.84}
{'loss': 0.8579, 'grad_norm': 0.23052942752838135, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.17833010852336884, 'eval_runtime': 3.4244, 'eval_samples_per_second': 292.018, 'eval_steps_per_second': 18.397, 'epoch': 0.88}
{'loss': 0.8437, 'grad_norm': 0.27540093660354614, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.17419785261154175, 'eval_runtime': 3.4181, 'eval_samples_per_second': 292.561, 'eval_steps_per_second': 18.431, 'epoch': 0.92}
{'loss': 0.8283, 'grad_norm': 0.2470545917749405, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.1682841181755066, 'eval_runtime': 3.4119, 'eval_samples_per_second': 293.09, 'eval_steps_per_second': 18.465, 'epoch': 0.96}
{'loss': 0.8108, 'grad_norm': 0.24960742890834808, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.16776354610919952, 'eval_runtime': 3.4129, 'eval_samples_per_second': 293.002, 'eval_steps_per_second': 18.459, 'epoch': 1.0}
{'train_runtime': 280.7179, 'train_samples_per_second': 35.616, 'train_steps_per_second': 2.226, 'train_loss': 1.1204656005859375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.673366665840149, 1.0939699411392212, 0.8836808800697327, 0.7905645966529846, 0.7323802709579468, 0.646122932434082, 0.571088969707489, 0.4949139356613159, 0.4283502697944641, 0.386067271232605, 0.34809330105781555, 0.3325328528881073, 0.3021934926509857, 0.2773236632347107, 0.25790879130363464, 0.24091342091560364, 0.2272706776857376, 0.2096683233976364, 0.19995944201946259, 0.1896408200263977, 0.18313004076480865, 0.17833010852336884, 0.17419785261154175, 0.1682841181755066, 0.16776354610919952], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.673366665840149, 1.0939699411392212, 0.8836808800697327, 0.7905645966529846, 0.7323802709579468, 0.646122932434082, 0.571088969707489, 0.4949139356613159, 0.4283502697944641, 0.386067271232605, 0.34809330105781555, 0.3325328528881073, 0.3021934926509857, 0.2773236632347107, 0.25790879130363464, 0.24091342091560364, 0.2272706776857376, 0.2096683233976364, 0.19995944201946259, 0.1896408200263977, 0.18313004076480865, 0.17833010852336884, 0.17419785261154175, 0.1682841181755066, 0.16776354610919952]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1213496923446655
current iteration best possible eval_loss (full train run):  -0.16776354610919952
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.7164 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -1.0026273735223126
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -1.0026333503034068
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -1.0026347786498269
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -1.0026273736130082
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -1.0026273736130094
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0553, dtype=torch.float64), tensor(0.0387, dtype=torch.float64), tensor(0.2078, dtype=torch.float64), 0, tensor(0.4001, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.2135, dtype=torch.float64), 18, 1, 1, 0, 0, 1, 59, 2.781952198396969e-19, 26.55594176355511, 1]
normalized proposed parameters for next round by BO: [tensor(2.1785e-18, dtype=torch.float64), tensor(0.0553, dtype=torch.float64), tensor(0.0387, dtype=torch.float64), tensor(0.2078, dtype=torch.float64), tensor(1.0939e-17, dtype=torch.float64), tensor(0.4001, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.2135, dtype=torch.float64), tensor(0.5558, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4640, dtype=torch.float64), tensor(2.7820e-18, dtype=torch.float64), tensor(0.5532, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.055
  rowan_hellaswag: 0.039
  sciq: 0.208
  triviaqa: 0
  truthfulqa_gen: 0.4
  wikitext: 0.056
  mmlu: 0.028
  arc_challenge: 0.213

LoRA Parameters:
  lora_r: (59,)
  lora_dropout: (2.781952198396969e-19,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (26.55594176355511,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  59
lora dropout:  2.781952198396969e-19
lora alpha:  26.55594176355511
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 33,712,128 || all params: 8,063,973,376 || trainable%: 0.4181
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4079, 'grad_norm': 1.896368384361267, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8511343002319336, 'eval_runtime': 3.4192, 'eval_samples_per_second': 292.468, 'eval_steps_per_second': 18.426, 'epoch': 0.04}
{'loss': 1.6284, 'grad_norm': 0.36487364768981934, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2258491516113281, 'eval_runtime': 3.4249, 'eval_samples_per_second': 291.979, 'eval_steps_per_second': 18.395, 'epoch': 0.08}
{'loss': 1.3429, 'grad_norm': 0.5345982909202576, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0890823602676392, 'eval_runtime': 3.4162, 'eval_samples_per_second': 292.725, 'eval_steps_per_second': 18.442, 'epoch': 0.12}
{'loss': 1.2473, 'grad_norm': 0.3507458567619324, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9823856949806213, 'eval_runtime': 3.4078, 'eval_samples_per_second': 293.448, 'eval_steps_per_second': 18.487, 'epoch': 0.16}
{'loss': 1.1578, 'grad_norm': 0.36050841212272644, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8776130080223083, 'eval_runtime': 3.4068, 'eval_samples_per_second': 293.534, 'eval_steps_per_second': 18.493, 'epoch': 0.2}
{'loss': 1.1274, 'grad_norm': 0.35336434841156006, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8363874554634094, 'eval_runtime': 3.4101, 'eval_samples_per_second': 293.243, 'eval_steps_per_second': 18.474, 'epoch': 0.24}
{'loss': 1.059, 'grad_norm': 0.29222461581230164, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8057915568351746, 'eval_runtime': 3.4154, 'eval_samples_per_second': 292.788, 'eval_steps_per_second': 18.446, 'epoch': 0.28}
{'loss': 1.0202, 'grad_norm': 0.32049721479415894, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7777054309844971, 'eval_runtime': 3.4151, 'eval_samples_per_second': 292.82, 'eval_steps_per_second': 18.448, 'epoch': 0.32}
{'loss': 1.0723, 'grad_norm': 0.3863260746002197, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.742051899433136, 'eval_runtime': 3.4251, 'eval_samples_per_second': 291.96, 'eval_steps_per_second': 18.393, 'epoch': 0.36}
{'loss': 1.0261, 'grad_norm': 0.34831270575523376, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7167471647262573, 'eval_runtime': 3.4248, 'eval_samples_per_second': 291.99, 'eval_steps_per_second': 18.395, 'epoch': 0.4}
{'loss': 1.0944, 'grad_norm': 0.37877708673477173, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6850120425224304, 'eval_runtime': 3.4215, 'eval_samples_per_second': 292.266, 'eval_steps_per_second': 18.413, 'epoch': 0.44}
{'loss': 1.0059, 'grad_norm': 0.3109759986400604, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6523363590240479, 'eval_runtime': 3.4356, 'eval_samples_per_second': 291.069, 'eval_steps_per_second': 18.337, 'epoch': 0.48}
{'loss': 1.0048, 'grad_norm': 0.4209979176521301, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6057518124580383, 'eval_runtime': 3.4268, 'eval_samples_per_second': 291.814, 'eval_steps_per_second': 18.384, 'epoch': 0.52}
{'loss': 1.0037, 'grad_norm': 0.3691578209400177, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5770169496536255, 'eval_runtime': 3.4192, 'eval_samples_per_second': 292.466, 'eval_steps_per_second': 18.425, 'epoch': 0.56}
{'loss': 0.9863, 'grad_norm': 0.4510918855667114, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5400501489639282, 'eval_runtime': 3.4285, 'eval_samples_per_second': 291.671, 'eval_steps_per_second': 18.375, 'epoch': 0.6}
{'loss': 0.9615, 'grad_norm': 0.3393408954143524, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5067609548568726, 'eval_runtime': 3.4199, 'eval_samples_per_second': 292.407, 'eval_steps_per_second': 18.422, 'epoch': 0.64}
{'loss': 0.9614, 'grad_norm': 0.4690631628036499, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4674742519855499, 'eval_runtime': 3.4173, 'eval_samples_per_second': 292.626, 'eval_steps_per_second': 18.435, 'epoch': 0.68}
{'loss': 0.9188, 'grad_norm': 0.38190534710884094, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.42439472675323486, 'eval_runtime': 3.4217, 'eval_samples_per_second': 292.256, 'eval_steps_per_second': 18.412, 'epoch': 0.72}
{'loss': 0.899, 'grad_norm': 0.48622748255729675, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.387237548828125, 'eval_runtime': 3.4251, 'eval_samples_per_second': 291.961, 'eval_steps_per_second': 18.394, 'epoch': 0.76}
{'loss': 0.8511, 'grad_norm': 0.4311193823814392, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.36551806330680847, 'eval_runtime': 3.4343, 'eval_samples_per_second': 291.178, 'eval_steps_per_second': 18.344, 'epoch': 0.8}
{'loss': 0.8211, 'grad_norm': 0.46030622720718384, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3442031741142273, 'eval_runtime': 3.4189, 'eval_samples_per_second': 292.493, 'eval_steps_per_second': 18.427, 'epoch': 0.84}
{'loss': 0.854, 'grad_norm': 0.3727763593196869, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.33183708786964417, 'eval_runtime': 3.4245, 'eval_samples_per_second': 292.017, 'eval_steps_per_second': 18.397, 'epoch': 0.88}
{'loss': 0.825, 'grad_norm': 0.4079021215438843, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3186795115470886, 'eval_runtime': 3.4298, 'eval_samples_per_second': 291.564, 'eval_steps_per_second': 18.369, 'epoch': 0.92}
{'loss': 0.8661, 'grad_norm': 0.5202409625053406, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3151642382144928, 'eval_runtime': 3.4295, 'eval_samples_per_second': 291.585, 'eval_steps_per_second': 18.37, 'epoch': 0.96}
{'loss': 0.8276, 'grad_norm': 0.47922539710998535, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.309855580329895, 'eval_runtime': 3.4431, 'eval_samples_per_second': 290.438, 'eval_steps_per_second': 18.298, 'epoch': 1.0}
{'train_runtime': 273.5518, 'train_samples_per_second': 36.538, 'train_steps_per_second': 2.285, 'train_loss': 1.1187891723632812, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8511343002319336, 1.2258491516113281, 1.0890823602676392, 0.9823856949806213, 0.8776130080223083, 0.8363874554634094, 0.8057915568351746, 0.7777054309844971, 0.742051899433136, 0.7167471647262573, 0.6850120425224304, 0.6523363590240479, 0.6057518124580383, 0.5770169496536255, 0.5400501489639282, 0.5067609548568726, 0.4674742519855499, 0.42439472675323486, 0.387237548828125, 0.36551806330680847, 0.3442031741142273, 0.33183708786964417, 0.3186795115470886, 0.3151642382144928, 0.309855580329895], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8511343002319336, 1.2258491516113281, 1.0890823602676392, 0.9823856949806213, 0.8776130080223083, 0.8363874554634094, 0.8057915568351746, 0.7777054309844971, 0.742051899433136, 0.7167471647262573, 0.6850120425224304, 0.6523363590240479, 0.6057518124580383, 0.5770169496536255, 0.5400501489639282, 0.5067609548568726, 0.4674742519855499, 0.42439472675323486, 0.387237548828125, 0.36551806330680847, 0.3442031741142273, 0.33183708786964417, 0.3186795115470886, 0.3151642382144928, 0.309855580329895]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1264408826828003
current iteration best possible eval_loss (full train run):  -0.309855580329895
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.4452 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.9614309563104335
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.9614309583014431
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.9621406495334501
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.9615795338141142
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.9614309563104357
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0217, dtype=torch.float64), tensor(0.0898, dtype=torch.float64), tensor(0.3525, dtype=torch.float64), 0, tensor(0.2669, dtype=torch.float64), 0, tensor(0.2691, dtype=torch.float64), 0, 16, 1, 1, 1, 1, 1, 2, 4.444464148472774e-21, 40.69577891992306, 1]
normalized proposed parameters for next round by BO: [tensor(1.0380e-18, dtype=torch.float64), tensor(0.0217, dtype=torch.float64), tensor(0.0898, dtype=torch.float64), tensor(0.3525, dtype=torch.float64), tensor(6.7373e-19, dtype=torch.float64), tensor(0.2669, dtype=torch.float64), tensor(5.0866e-19, dtype=torch.float64), tensor(0.2691, dtype=torch.float64), tensor(1.5717e-17, dtype=torch.float64), tensor(0.4890, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(4.4445e-20, dtype=torch.float64), tensor(0.8478, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.022
  rowan_hellaswag: 0.09
  sciq: 0.353
  triviaqa: 0
  truthfulqa_gen: 0.267
  wikitext: 0
  mmlu: 0.269
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (4.444464148472774e-21,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (40.69577891992306,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  2
lora dropout:  4.444464148472774e-21
lora alpha:  40.69577891992306
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,195,456 || all params: 8,032,456,704 || trainable%: 0.0273
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0247, 'grad_norm': 10.478073120117188, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4757434129714966, 'eval_runtime': 3.6216, 'eval_samples_per_second': 276.118, 'eval_steps_per_second': 17.395, 'epoch': 0.04}
{'loss': 1.4407, 'grad_norm': 4.3494439125061035, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9567046165466309, 'eval_runtime': 3.6437, 'eval_samples_per_second': 274.443, 'eval_steps_per_second': 17.29, 'epoch': 0.08}
{'loss': 1.3197, 'grad_norm': 2.3053929805755615, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8299456834793091, 'eval_runtime': 3.6367, 'eval_samples_per_second': 274.972, 'eval_steps_per_second': 17.323, 'epoch': 0.12}
{'loss': 1.2137, 'grad_norm': 2.7769572734832764, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7418012619018555, 'eval_runtime': 3.6144, 'eval_samples_per_second': 276.674, 'eval_steps_per_second': 17.43, 'epoch': 0.16}
{'loss': 1.1923, 'grad_norm': 1.7733811140060425, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7130311131477356, 'eval_runtime': 3.6235, 'eval_samples_per_second': 275.978, 'eval_steps_per_second': 17.387, 'epoch': 0.2}
{'loss': 1.1671, 'grad_norm': 2.2038638591766357, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6764642596244812, 'eval_runtime': 3.6192, 'eval_samples_per_second': 276.307, 'eval_steps_per_second': 17.407, 'epoch': 0.24}
{'loss': 1.1856, 'grad_norm': 1.7701265811920166, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.657899796962738, 'eval_runtime': 3.6222, 'eval_samples_per_second': 276.075, 'eval_steps_per_second': 17.393, 'epoch': 0.28}
{'loss': 1.1233, 'grad_norm': 1.8171988725662231, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6479269862174988, 'eval_runtime': 3.6233, 'eval_samples_per_second': 275.993, 'eval_steps_per_second': 17.388, 'epoch': 0.32}
{'loss': 1.1609, 'grad_norm': 2.134805202484131, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6153648495674133, 'eval_runtime': 3.6218, 'eval_samples_per_second': 276.109, 'eval_steps_per_second': 17.395, 'epoch': 0.36}
{'loss': 1.1178, 'grad_norm': 2.629809617996216, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6033747792243958, 'eval_runtime': 3.6241, 'eval_samples_per_second': 275.929, 'eval_steps_per_second': 17.384, 'epoch': 0.4}
{'loss': 1.0347, 'grad_norm': 5.378170490264893, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5685921907424927, 'eval_runtime': 3.6336, 'eval_samples_per_second': 275.213, 'eval_steps_per_second': 17.338, 'epoch': 0.44}
{'loss': 1.127, 'grad_norm': 2.1255874633789062, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5610749125480652, 'eval_runtime': 3.6322, 'eval_samples_per_second': 275.317, 'eval_steps_per_second': 17.345, 'epoch': 0.48}
{'loss': 1.0841, 'grad_norm': 2.074930191040039, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5293682813644409, 'eval_runtime': 3.6244, 'eval_samples_per_second': 275.909, 'eval_steps_per_second': 17.382, 'epoch': 0.52}
{'loss': 1.0918, 'grad_norm': 1.6701637506484985, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5071195363998413, 'eval_runtime': 3.6289, 'eval_samples_per_second': 275.564, 'eval_steps_per_second': 17.361, 'epoch': 0.56}
{'loss': 1.1894, 'grad_norm': 1.6976829767227173, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.489984929561615, 'eval_runtime': 3.6364, 'eval_samples_per_second': 274.999, 'eval_steps_per_second': 17.325, 'epoch': 0.6}
{'loss': 1.1756, 'grad_norm': 4.271993637084961, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.48896118998527527, 'eval_runtime': 3.6329, 'eval_samples_per_second': 275.264, 'eval_steps_per_second': 17.342, 'epoch': 0.64}
{'loss': 1.1839, 'grad_norm': 1.946529507637024, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.46623173356056213, 'eval_runtime': 3.6297, 'eval_samples_per_second': 275.508, 'eval_steps_per_second': 17.357, 'epoch': 0.68}
{'loss': 1.0682, 'grad_norm': 2.3726558685302734, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.44387945532798767, 'eval_runtime': 3.6333, 'eval_samples_per_second': 275.234, 'eval_steps_per_second': 17.34, 'epoch': 0.72}
{'loss': 1.1458, 'grad_norm': 1.8283162117004395, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4300808310508728, 'eval_runtime': 3.6359, 'eval_samples_per_second': 275.035, 'eval_steps_per_second': 17.327, 'epoch': 0.76}
{'loss': 1.1228, 'grad_norm': 1.670809268951416, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.4109950661659241, 'eval_runtime': 3.6505, 'eval_samples_per_second': 273.936, 'eval_steps_per_second': 17.258, 'epoch': 0.8}
{'loss': 1.0671, 'grad_norm': 2.149019479751587, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.39171162247657776, 'eval_runtime': 3.6344, 'eval_samples_per_second': 275.145, 'eval_steps_per_second': 17.334, 'epoch': 0.84}
{'loss': 1.1215, 'grad_norm': 1.7386571168899536, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3825797736644745, 'eval_runtime': 3.6296, 'eval_samples_per_second': 275.509, 'eval_steps_per_second': 17.357, 'epoch': 0.88}
{'loss': 1.1173, 'grad_norm': 1.7997571229934692, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3700394332408905, 'eval_runtime': 3.6277, 'eval_samples_per_second': 275.657, 'eval_steps_per_second': 17.366, 'epoch': 0.92}
{'loss': 1.0361, 'grad_norm': 1.5348315238952637, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3625493049621582, 'eval_runtime': 3.6319, 'eval_samples_per_second': 275.334, 'eval_steps_per_second': 17.346, 'epoch': 0.96}
{'loss': 1.0342, 'grad_norm': 1.4977198839187622, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.36154478788375854, 'eval_runtime': 3.6368, 'eval_samples_per_second': 274.969, 'eval_steps_per_second': 17.323, 'epoch': 1.0}
{'train_runtime': 310.9247, 'train_samples_per_second': 32.156, 'train_steps_per_second': 2.01, 'train_loss': 1.2218157592773438, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4757434129714966, 0.9567046165466309, 0.8299456834793091, 0.7418012619018555, 0.7130311131477356, 0.6764642596244812, 0.657899796962738, 0.6479269862174988, 0.6153648495674133, 0.6033747792243958, 0.5685921907424927, 0.5610749125480652, 0.5293682813644409, 0.5071195363998413, 0.489984929561615, 0.48896118998527527, 0.46623173356056213, 0.44387945532798767, 0.4300808310508728, 0.4109950661659241, 0.39171162247657776, 0.3825797736644745, 0.3700394332408905, 0.3625493049621582, 0.36154478788375854], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4757434129714966, 0.9567046165466309, 0.8299456834793091, 0.7418012619018555, 0.7130311131477356, 0.6764642596244812, 0.657899796962738, 0.6479269862174988, 0.6153648495674133, 0.6033747792243958, 0.5685921907424927, 0.5610749125480652, 0.5293682813644409, 0.5071195363998413, 0.489984929561615, 0.48896118998527527, 0.46623173356056213, 0.44387945532798767, 0.4300808310508728, 0.4109950661659241, 0.39171162247657776, 0.3825797736644745, 0.3700394332408905, 0.3625493049621582, 0.36154478788375854]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.36154478788375854
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.1612 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -0.9916832881936746
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -0.9918199346726158
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -0.9916832881938216
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -0.9916832881938877
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -0.9916832882053066
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0204, dtype=torch.float64), tensor(0.1343, dtype=torch.float64), tensor(0.0902, dtype=torch.float64), 0, tensor(0.2725, dtype=torch.float64), tensor(0.4265, dtype=torch.float64), 0, tensor(0.0561, dtype=torch.float64), 0, 15, 0, 0, 0, 1, 1, 31, 0.05397133499040581, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0204, dtype=torch.float64), tensor(0.1343, dtype=torch.float64), tensor(0.0902, dtype=torch.float64), tensor(1.4757e-18, dtype=torch.float64), tensor(0.2725, dtype=torch.float64), tensor(0.4265, dtype=torch.float64), tensor(5.3252e-17, dtype=torch.float64), tensor(0.0561, dtype=torch.float64), tensor(1.6772e-18, dtype=torch.float64), tensor(0.4634, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2422, dtype=torch.float64), tensor(0.5397, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.02
  gsm8k: 0.134
  rowan_hellaswag: 0.09
  sciq: 0
  triviaqa: 0.273
  truthfulqa_gen: 0.426
  wikitext: 0
  mmlu: 0.056
  arc_challenge: 0

LoRA Parameters:
  lora_r: (31,)
  lora_dropout: (0.05397133499040581,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  31
lora dropout:  0.05397133499040581
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 17,141,760 || all params: 8,047,403,008 || trainable%: 0.2130
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1568, 'grad_norm': 1.4239822626113892, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5590747594833374, 'eval_runtime': 3.3653, 'eval_samples_per_second': 297.152, 'eval_steps_per_second': 18.721, 'epoch': 0.04}
{'loss': 1.5979, 'grad_norm': 1.1971420049667358, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9337209463119507, 'eval_runtime': 3.354, 'eval_samples_per_second': 298.149, 'eval_steps_per_second': 18.783, 'epoch': 0.08}
{'loss': 1.2332, 'grad_norm': 0.5762172341346741, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8001185059547424, 'eval_runtime': 3.3629, 'eval_samples_per_second': 297.363, 'eval_steps_per_second': 18.734, 'epoch': 0.12}
{'loss': 1.1206, 'grad_norm': 0.6455888152122498, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7111180424690247, 'eval_runtime': 3.3783, 'eval_samples_per_second': 296.004, 'eval_steps_per_second': 18.648, 'epoch': 0.16}
{'loss': 1.0886, 'grad_norm': 0.6263251304626465, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6662819981575012, 'eval_runtime': 3.3802, 'eval_samples_per_second': 295.84, 'eval_steps_per_second': 18.638, 'epoch': 0.2}
{'loss': 1.0771, 'grad_norm': 0.4668380916118622, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6392077803611755, 'eval_runtime': 3.3879, 'eval_samples_per_second': 295.171, 'eval_steps_per_second': 18.596, 'epoch': 0.24}
{'loss': 1.0609, 'grad_norm': 0.43168914318084717, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.611383318901062, 'eval_runtime': 3.3889, 'eval_samples_per_second': 295.077, 'eval_steps_per_second': 18.59, 'epoch': 0.28}
{'loss': 1.0612, 'grad_norm': 0.6886287927627563, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5908958911895752, 'eval_runtime': 3.3944, 'eval_samples_per_second': 294.604, 'eval_steps_per_second': 18.56, 'epoch': 0.32}
{'loss': 1.0854, 'grad_norm': 0.6473642587661743, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.548603892326355, 'eval_runtime': 3.4062, 'eval_samples_per_second': 293.582, 'eval_steps_per_second': 18.496, 'epoch': 0.36}
{'loss': 1.0005, 'grad_norm': 0.49413836002349854, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5185888409614563, 'eval_runtime': 3.4088, 'eval_samples_per_second': 293.361, 'eval_steps_per_second': 18.482, 'epoch': 0.4}
{'loss': 0.9471, 'grad_norm': 0.676773726940155, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.4996320307254791, 'eval_runtime': 3.3928, 'eval_samples_per_second': 294.741, 'eval_steps_per_second': 18.569, 'epoch': 0.44}
{'loss': 0.9843, 'grad_norm': 0.5973916053771973, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.46532467007637024, 'eval_runtime': 3.3957, 'eval_samples_per_second': 294.487, 'eval_steps_per_second': 18.553, 'epoch': 0.48}
{'loss': 1.0624, 'grad_norm': 0.5628852844238281, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4422949552536011, 'eval_runtime': 3.3951, 'eval_samples_per_second': 294.539, 'eval_steps_per_second': 18.556, 'epoch': 0.52}
{'loss': 1.0071, 'grad_norm': 0.5200280547142029, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.42094287276268005, 'eval_runtime': 3.3947, 'eval_samples_per_second': 294.575, 'eval_steps_per_second': 18.558, 'epoch': 0.56}
{'loss': 0.9921, 'grad_norm': 0.7271794676780701, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.3891296088695526, 'eval_runtime': 3.3981, 'eval_samples_per_second': 294.279, 'eval_steps_per_second': 18.54, 'epoch': 0.6}
{'loss': 1.0017, 'grad_norm': 0.7201117277145386, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.37087371945381165, 'eval_runtime': 3.3934, 'eval_samples_per_second': 294.686, 'eval_steps_per_second': 18.565, 'epoch': 0.64}
{'loss': 1.0003, 'grad_norm': 0.5115170478820801, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.35165736079216003, 'eval_runtime': 3.3991, 'eval_samples_per_second': 294.194, 'eval_steps_per_second': 18.534, 'epoch': 0.68}
{'loss': 0.9539, 'grad_norm': 0.5342322587966919, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3267114758491516, 'eval_runtime': 3.3959, 'eval_samples_per_second': 294.469, 'eval_steps_per_second': 18.552, 'epoch': 0.72}
{'loss': 0.8767, 'grad_norm': 0.743990421295166, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3081425726413727, 'eval_runtime': 3.3985, 'eval_samples_per_second': 294.249, 'eval_steps_per_second': 18.538, 'epoch': 0.76}
{'loss': 0.9406, 'grad_norm': 0.5731543898582458, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.2949417233467102, 'eval_runtime': 3.3991, 'eval_samples_per_second': 294.193, 'eval_steps_per_second': 18.534, 'epoch': 0.8}
{'loss': 0.9238, 'grad_norm': 0.8520405888557434, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.276885986328125, 'eval_runtime': 3.3947, 'eval_samples_per_second': 294.578, 'eval_steps_per_second': 18.558, 'epoch': 0.84}
{'loss': 0.938, 'grad_norm': 0.39893630146980286, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2694740295410156, 'eval_runtime': 3.3937, 'eval_samples_per_second': 294.661, 'eval_steps_per_second': 18.564, 'epoch': 0.88}
{'loss': 0.8673, 'grad_norm': 0.8633533716201782, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2599502503871918, 'eval_runtime': 3.3987, 'eval_samples_per_second': 294.234, 'eval_steps_per_second': 18.537, 'epoch': 0.92}
{'loss': 0.8946, 'grad_norm': 0.4983152151107788, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2476852834224701, 'eval_runtime': 3.393, 'eval_samples_per_second': 294.725, 'eval_steps_per_second': 18.568, 'epoch': 0.96}
{'loss': 0.9113, 'grad_norm': 0.742218554019928, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2435191571712494, 'eval_runtime': 3.3972, 'eval_samples_per_second': 294.362, 'eval_steps_per_second': 18.545, 'epoch': 1.0}
{'train_runtime': 293.4919, 'train_samples_per_second': 34.059, 'train_steps_per_second': 2.13, 'train_loss': 1.1113352142333985, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5590747594833374, 0.9337209463119507, 0.8001185059547424, 0.7111180424690247, 0.6662819981575012, 0.6392077803611755, 0.611383318901062, 0.5908958911895752, 0.548603892326355, 0.5185888409614563, 0.4996320307254791, 0.46532467007637024, 0.4422949552536011, 0.42094287276268005, 0.3891296088695526, 0.37087371945381165, 0.35165736079216003, 0.3267114758491516, 0.3081425726413727, 0.2949417233467102, 0.276885986328125, 0.2694740295410156, 0.2599502503871918, 0.2476852834224701, 0.2435191571712494], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5590747594833374, 0.9337209463119507, 0.8001185059547424, 0.7111180424690247, 0.6662819981575012, 0.6392077803611755, 0.611383318901062, 0.5908958911895752, 0.548603892326355, 0.5185888409614563, 0.4996320307254791, 0.46532467007637024, 0.4422949552536011, 0.42094287276268005, 0.3891296088695526, 0.37087371945381165, 0.35165736079216003, 0.3267114758491516, 0.3081425726413727, 0.2949417233467102, 0.276885986328125, 0.2694740295410156, 0.2599502503871918, 0.2476852834224701, 0.2435191571712494]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.2435191571712494
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.6337 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -1.0214986610575894
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -1.0215865416910854
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -1.0214986610573185
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -1.0203758884177967
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -1.021498661072501
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0864, dtype=torch.float64), tensor(0.5337, dtype=torch.float64), 0, tensor(0.2378, dtype=torch.float64), tensor(0.0296, dtype=torch.float64), tensor(0.0345, dtype=torch.float64), tensor(0.0780, dtype=torch.float64), 14, 0, 1, 0, 1, 1, 94, 2.3534919234240353e-19, 1.4800000190734866, 1]
normalized proposed parameters for next round by BO: [tensor(1.0149e-18, dtype=torch.float64), tensor(2.6463e-19, dtype=torch.float64), tensor(0.0864, dtype=torch.float64), tensor(0.5337, dtype=torch.float64), tensor(3.5160e-19, dtype=torch.float64), tensor(0.2378, dtype=torch.float64), tensor(0.0296, dtype=torch.float64), tensor(0.0345, dtype=torch.float64), tensor(0.0780, dtype=torch.float64), tensor(0.4386, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7380, dtype=torch.float64), tensor(2.3535e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.086
  sciq: 0.534
  triviaqa: 0
  truthfulqa_gen: 0.238
  wikitext: 0.03
  mmlu: 0.034
  arc_challenge: 0.078

LoRA Parameters:
  lora_r: (94,)
  lora_dropout: (2.3534919234240353e-19,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  94
lora dropout:  2.3534919234240353e-19
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 55,250,944 || all params: 8,085,512,192 || trainable%: 0.6833
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.7869, 'grad_norm': 0.4925149977207184, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.872277021408081, 'eval_runtime': 3.3941, 'eval_samples_per_second': 294.625, 'eval_steps_per_second': 18.561, 'epoch': 0.04}
{'loss': 2.8419, 'grad_norm': 0.9742668867111206, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6790415048599243, 'eval_runtime': 3.4478, 'eval_samples_per_second': 290.041, 'eval_steps_per_second': 18.273, 'epoch': 0.08}
{'loss': 1.8201, 'grad_norm': 0.19652080535888672, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.262528419494629, 'eval_runtime': 3.447, 'eval_samples_per_second': 290.11, 'eval_steps_per_second': 18.277, 'epoch': 0.12}
{'loss': 1.5337, 'grad_norm': 0.19847220182418823, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1596726179122925, 'eval_runtime': 3.4467, 'eval_samples_per_second': 290.134, 'eval_steps_per_second': 18.278, 'epoch': 0.16}
{'loss': 1.3884, 'grad_norm': 0.2306319773197174, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0907396078109741, 'eval_runtime': 3.448, 'eval_samples_per_second': 290.021, 'eval_steps_per_second': 18.271, 'epoch': 0.2}
{'loss': 1.3568, 'grad_norm': 0.11989843100309372, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.00022554397583, 'eval_runtime': 3.4648, 'eval_samples_per_second': 288.619, 'eval_steps_per_second': 18.183, 'epoch': 0.24}
{'loss': 1.219, 'grad_norm': 0.11167307943105698, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9513183832168579, 'eval_runtime': 3.4281, 'eval_samples_per_second': 291.704, 'eval_steps_per_second': 18.377, 'epoch': 0.28}
{'loss': 1.152, 'grad_norm': 0.14683732390403748, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8668349981307983, 'eval_runtime': 3.4294, 'eval_samples_per_second': 291.597, 'eval_steps_per_second': 18.371, 'epoch': 0.32}
{'loss': 1.1095, 'grad_norm': 0.08933724462985992, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8225613236427307, 'eval_runtime': 3.4589, 'eval_samples_per_second': 289.105, 'eval_steps_per_second': 18.214, 'epoch': 0.36}
{'loss': 1.1696, 'grad_norm': 0.13153484463691711, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8295167088508606, 'eval_runtime': 3.443, 'eval_samples_per_second': 290.446, 'eval_steps_per_second': 18.298, 'epoch': 0.4}
{'loss': 1.202, 'grad_norm': 0.09308120608329773, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7977402210235596, 'eval_runtime': 3.4345, 'eval_samples_per_second': 291.161, 'eval_steps_per_second': 18.343, 'epoch': 0.44}
{'loss': 1.1961, 'grad_norm': 0.11208852380514145, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8135233521461487, 'eval_runtime': 3.4271, 'eval_samples_per_second': 291.791, 'eval_steps_per_second': 18.383, 'epoch': 0.48}
{'loss': 1.1577, 'grad_norm': 0.08367270231246948, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8016838431358337, 'eval_runtime': 3.4253, 'eval_samples_per_second': 291.947, 'eval_steps_per_second': 18.393, 'epoch': 0.52}
{'loss': 1.0735, 'grad_norm': 0.09959821403026581, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7882961630821228, 'eval_runtime': 3.4224, 'eval_samples_per_second': 292.189, 'eval_steps_per_second': 18.408, 'epoch': 0.56}
{'loss': 1.1481, 'grad_norm': 0.12493468075990677, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7959346771240234, 'eval_runtime': 3.4319, 'eval_samples_per_second': 291.384, 'eval_steps_per_second': 18.357, 'epoch': 0.6}
{'loss': 1.0545, 'grad_norm': 0.1087680384516716, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7844992280006409, 'eval_runtime': 3.4276, 'eval_samples_per_second': 291.753, 'eval_steps_per_second': 18.38, 'epoch': 0.64}
{'loss': 1.081, 'grad_norm': 0.11096776276826859, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.775495171546936, 'eval_runtime': 3.427, 'eval_samples_per_second': 291.804, 'eval_steps_per_second': 18.384, 'epoch': 0.68}
{'loss': 1.1386, 'grad_norm': 0.11029601842164993, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7781906723976135, 'eval_runtime': 3.4239, 'eval_samples_per_second': 292.066, 'eval_steps_per_second': 18.4, 'epoch': 0.72}
{'loss': 1.0887, 'grad_norm': 0.09721401333808899, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.778858482837677, 'eval_runtime': 3.4287, 'eval_samples_per_second': 291.656, 'eval_steps_per_second': 18.374, 'epoch': 0.76}
{'loss': 1.0673, 'grad_norm': 0.09239904582500458, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7742080092430115, 'eval_runtime': 3.4205, 'eval_samples_per_second': 292.356, 'eval_steps_per_second': 18.418, 'epoch': 0.8}
{'loss': 1.0775, 'grad_norm': 0.08819116652011871, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7815022468566895, 'eval_runtime': 3.4293, 'eval_samples_per_second': 291.605, 'eval_steps_per_second': 18.371, 'epoch': 0.84}
{'loss': 1.1646, 'grad_norm': 0.10635436326265335, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7565511465072632, 'eval_runtime': 3.4255, 'eval_samples_per_second': 291.932, 'eval_steps_per_second': 18.392, 'epoch': 0.88}
{'loss': 1.1099, 'grad_norm': 0.09360192716121674, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7613704800605774, 'eval_runtime': 3.4275, 'eval_samples_per_second': 291.758, 'eval_steps_per_second': 18.381, 'epoch': 0.92}
{'loss': 1.0345, 'grad_norm': 0.11231716722249985, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7631461024284363, 'eval_runtime': 3.4246, 'eval_samples_per_second': 292.001, 'eval_steps_per_second': 18.396, 'epoch': 0.96}
{'loss': 1.1072, 'grad_norm': 0.15013238787651062, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7637869715690613, 'eval_runtime': 3.4264, 'eval_samples_per_second': 291.852, 'eval_steps_per_second': 18.387, 'epoch': 1.0}
{'train_runtime': 274.0941, 'train_samples_per_second': 36.473, 'train_steps_per_second': 2.28, 'train_loss': 1.4031588623046876, 'epoch': 1.0}
train_results:  {'eval_loss': [3.872277021408081, 1.6790415048599243, 1.262528419494629, 1.1596726179122925, 1.0907396078109741, 1.00022554397583, 0.9513183832168579, 0.8668349981307983, 0.8225613236427307, 0.8295167088508606, 0.7977402210235596, 0.8135233521461487, 0.8016838431358337, 0.7882961630821228, 0.7959346771240234, 0.7844992280006409, 0.775495171546936, 0.7781906723976135, 0.778858482837677, 0.7742080092430115, 0.7815022468566895, 0.7565511465072632, 0.7613704800605774, 0.7631461024284363, 0.7637869715690613], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.872277021408081, 1.6790415048599243, 1.262528419494629, 1.1596726179122925, 1.0907396078109741, 1.00022554397583, 0.9513183832168579, 0.8668349981307983, 0.8225613236427307, 0.8295167088508606, 0.7977402210235596, 0.8135233521461487, 0.8016838431358337, 0.7882961630821228, 0.7959346771240234, 0.7844992280006409, 0.775495171546936, 0.7781906723976135, 0.778858482837677, 0.7742080092430115, 0.7815022468566895, 0.7565511465072632, 0.7613704800605774, 0.7631461024284363, 0.7637869715690613]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3225500583648682
current iteration best possible eval_loss (full train run):  -0.7637869715690613
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.4487 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -1.0187237782667786
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -1.018724494416592
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -1.019145703634818
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -1.018728831872629
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -1.0187240881219204
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0680, dtype=torch.float64), tensor(0.1086, dtype=torch.float64), tensor(0.0921, dtype=torch.float64), 0, tensor(0.5978, dtype=torch.float64), 0, tensor(0.0320, dtype=torch.float64), tensor(0.1015, dtype=torch.float64), 19, 0, 1, 0, 1, 1, 3, 0.025368363008023573, 31.485834545520206, 1]
normalized proposed parameters for next round by BO: [tensor(6.0844e-19, dtype=torch.float64), tensor(0.0680, dtype=torch.float64), tensor(0.1086, dtype=torch.float64), tensor(0.0921, dtype=torch.float64), tensor(7.6917e-18, dtype=torch.float64), tensor(0.5978, dtype=torch.float64), tensor(6.4809e-19, dtype=torch.float64), tensor(0.0320, dtype=torch.float64), tensor(0.1015, dtype=torch.float64), tensor(0.5788, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0203, dtype=torch.float64), tensor(0.2537, dtype=torch.float64), tensor(0.6560, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.068
  rowan_hellaswag: 0.109
  sciq: 0.092
  triviaqa: 0
  truthfulqa_gen: 0.598
  wikitext: 0
  mmlu: 0.032
  arc_challenge: 0.102

LoRA Parameters:
  lora_r: (3,)
  lora_dropout: (0.025368363008023573,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (31.485834545520206,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  3
lora dropout:  0.025368363008023573
lora alpha:  31.485834545520206
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,393,088 || all params: 8,032,654,336 || trainable%: 0.0298
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1374, 'grad_norm': 5.747897148132324, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4544955492019653, 'eval_runtime': 3.5281, 'eval_samples_per_second': 283.439, 'eval_steps_per_second': 17.857, 'epoch': 0.04}
{'loss': 1.4932, 'grad_norm': 2.330718517303467, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9161989688873291, 'eval_runtime': 3.5176, 'eval_samples_per_second': 284.286, 'eval_steps_per_second': 17.91, 'epoch': 0.08}
{'loss': 1.1599, 'grad_norm': 2.0802032947540283, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7546587586402893, 'eval_runtime': 3.5284, 'eval_samples_per_second': 283.413, 'eval_steps_per_second': 17.855, 'epoch': 0.12}
{'loss': 1.2066, 'grad_norm': 1.5231072902679443, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6516900062561035, 'eval_runtime': 3.5143, 'eval_samples_per_second': 284.551, 'eval_steps_per_second': 17.927, 'epoch': 0.16}
{'loss': 1.0717, 'grad_norm': 1.4189345836639404, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6132400035858154, 'eval_runtime': 3.5282, 'eval_samples_per_second': 283.431, 'eval_steps_per_second': 17.856, 'epoch': 0.2}
{'loss': 1.0273, 'grad_norm': 2.161975622177124, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5679349303245544, 'eval_runtime': 3.5225, 'eval_samples_per_second': 283.888, 'eval_steps_per_second': 17.885, 'epoch': 0.24}
{'loss': 0.9659, 'grad_norm': 2.2762610912323, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5004661083221436, 'eval_runtime': 3.5275, 'eval_samples_per_second': 283.49, 'eval_steps_per_second': 17.86, 'epoch': 0.28}
{'loss': 0.9921, 'grad_norm': 2.3256001472473145, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.47148171067237854, 'eval_runtime': 3.5305, 'eval_samples_per_second': 283.244, 'eval_steps_per_second': 17.844, 'epoch': 0.32}
{'loss': 0.9243, 'grad_norm': 1.58528470993042, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.41791531443595886, 'eval_runtime': 3.5223, 'eval_samples_per_second': 283.905, 'eval_steps_per_second': 17.886, 'epoch': 0.36}
{'loss': 0.9123, 'grad_norm': 1.8223259449005127, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.3944493532180786, 'eval_runtime': 3.5218, 'eval_samples_per_second': 283.943, 'eval_steps_per_second': 17.888, 'epoch': 0.4}
{'loss': 0.979, 'grad_norm': 2.4234046936035156, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.3604510724544525, 'eval_runtime': 3.5279, 'eval_samples_per_second': 283.458, 'eval_steps_per_second': 17.858, 'epoch': 0.44}
{'loss': 0.8101, 'grad_norm': 1.7274727821350098, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3355334997177124, 'eval_runtime': 3.5211, 'eval_samples_per_second': 283.998, 'eval_steps_per_second': 17.892, 'epoch': 0.48}
{'loss': 0.8552, 'grad_norm': 2.304831027984619, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3075820505619049, 'eval_runtime': 3.5249, 'eval_samples_per_second': 283.699, 'eval_steps_per_second': 17.873, 'epoch': 0.52}
{'loss': 0.8685, 'grad_norm': 1.8210511207580566, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.2857487201690674, 'eval_runtime': 3.526, 'eval_samples_per_second': 283.606, 'eval_steps_per_second': 17.867, 'epoch': 0.56}
{'loss': 0.8896, 'grad_norm': 1.671398639678955, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2655864953994751, 'eval_runtime': 3.5488, 'eval_samples_per_second': 281.788, 'eval_steps_per_second': 17.753, 'epoch': 0.6}
{'loss': 0.8794, 'grad_norm': 1.1615389585494995, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.24889937043190002, 'eval_runtime': 3.5561, 'eval_samples_per_second': 281.205, 'eval_steps_per_second': 17.716, 'epoch': 0.64}
{'loss': 0.8399, 'grad_norm': 1.2070374488830566, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.23007994890213013, 'eval_runtime': 3.5492, 'eval_samples_per_second': 281.751, 'eval_steps_per_second': 17.75, 'epoch': 0.68}
{'loss': 0.837, 'grad_norm': 1.5155874490737915, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.22026535868644714, 'eval_runtime': 3.5333, 'eval_samples_per_second': 283.018, 'eval_steps_per_second': 17.83, 'epoch': 0.72}
{'loss': 0.8113, 'grad_norm': 2.907310962677002, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2066541165113449, 'eval_runtime': 3.5408, 'eval_samples_per_second': 282.421, 'eval_steps_per_second': 17.793, 'epoch': 0.76}
{'loss': 0.8589, 'grad_norm': 1.2456955909729004, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.19290319085121155, 'eval_runtime': 3.5338, 'eval_samples_per_second': 282.98, 'eval_steps_per_second': 17.828, 'epoch': 0.8}
{'loss': 0.785, 'grad_norm': 2.463624954223633, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.18748269975185394, 'eval_runtime': 3.5666, 'eval_samples_per_second': 280.381, 'eval_steps_per_second': 17.664, 'epoch': 0.84}
{'loss': 0.8246, 'grad_norm': 1.0705169439315796, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.1870192289352417, 'eval_runtime': 3.5524, 'eval_samples_per_second': 281.5, 'eval_steps_per_second': 17.734, 'epoch': 0.88}
{'loss': 0.7943, 'grad_norm': 1.3284189701080322, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.17944438755512238, 'eval_runtime': 3.5361, 'eval_samples_per_second': 282.799, 'eval_steps_per_second': 17.816, 'epoch': 0.92}
{'loss': 0.8067, 'grad_norm': 1.2339870929718018, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.17682214081287384, 'eval_runtime': 3.5311, 'eval_samples_per_second': 283.197, 'eval_steps_per_second': 17.841, 'epoch': 0.96}
{'loss': 0.8051, 'grad_norm': 1.5660072565078735, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.17481054365634918, 'eval_runtime': 3.5287, 'eval_samples_per_second': 283.389, 'eval_steps_per_second': 17.854, 'epoch': 1.0}
{'train_runtime': 300.2501, 'train_samples_per_second': 33.289, 'train_steps_per_second': 2.082, 'train_loss': 1.0214141845703124, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4544955492019653, 0.9161989688873291, 0.7546587586402893, 0.6516900062561035, 0.6132400035858154, 0.5679349303245544, 0.5004661083221436, 0.47148171067237854, 0.41791531443595886, 0.3944493532180786, 0.3604510724544525, 0.3355334997177124, 0.3075820505619049, 0.2857487201690674, 0.2655864953994751, 0.24889937043190002, 0.23007994890213013, 0.22026535868644714, 0.2066541165113449, 0.19290319085121155, 0.18748269975185394, 0.1870192289352417, 0.17944438755512238, 0.17682214081287384, 0.17481054365634918], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4544955492019653, 0.9161989688873291, 0.7546587586402893, 0.6516900062561035, 0.6132400035858154, 0.5679349303245544, 0.5004661083221436, 0.47148171067237854, 0.41791531443595886, 0.3944493532180786, 0.3604510724544525, 0.3355334997177124, 0.3075820505619049, 0.2857487201690674, 0.2655864953994751, 0.24889937043190002, 0.23007994890213013, 0.22026535868644714, 0.2066541165113449, 0.19290319085121155, 0.18748269975185394, 0.1870192289352417, 0.17944438755512238, 0.17682214081287384, 0.17481054365634918]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.17481054365634918
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1737 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -1.044327011856596
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -1.044407919216943
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -1.044326998916643
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -1.0443270045164423
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -1.0441859771413986
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0194, dtype=torch.float64), tensor(0.1812, dtype=torch.float64), tensor(0.0479, dtype=torch.float64), tensor(0.2153, dtype=torch.float64), tensor(0.2417, dtype=torch.float64), tensor(0.1416, dtype=torch.float64), 0, tensor(0.1511, dtype=torch.float64), 0, 17, 0, 0, 0, 1, 0, 44, 0.017951878864132364, 29.794565958083233, 0]
normalized proposed parameters for next round by BO: [tensor(0.0194, dtype=torch.float64), tensor(0.1812, dtype=torch.float64), tensor(0.0479, dtype=torch.float64), tensor(0.2153, dtype=torch.float64), tensor(0.2417, dtype=torch.float64), tensor(0.1416, dtype=torch.float64), tensor(0.0017, dtype=torch.float64), tensor(0.1511, dtype=torch.float64), tensor(1.6974e-18, dtype=torch.float64), tensor(0.5421, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3441, dtype=torch.float64), tensor(0.1795, dtype=torch.float64), tensor(0.6207, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.019
  gsm8k: 0.181
  rowan_hellaswag: 0.048
  sciq: 0.215
  triviaqa: 0.242
  truthfulqa_gen: 0.142
  wikitext: 0
  mmlu: 0.151
  arc_challenge: 0

LoRA Parameters:
  lora_r: (44,)
  lora_dropout: (0.017951878864132364,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (29.794565958083233,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  44
lora dropout:  0.017951878864132364
lora alpha:  29.794565958083233
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 13,787,136 || all params: 8,044,048,384 || trainable%: 0.1714
length of training data:  9979
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3781, 'grad_norm': 1.8999898433685303, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.277384042739868, 'eval_runtime': 3.2193, 'eval_samples_per_second': 310.622, 'eval_steps_per_second': 19.569, 'epoch': 0.04}
{'loss': 1.6396, 'grad_norm': 0.574394702911377, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.018937110900879, 'eval_runtime': 3.2314, 'eval_samples_per_second': 309.463, 'eval_steps_per_second': 19.496, 'epoch': 0.08}
{'loss': 1.2611, 'grad_norm': 0.3713085949420929, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.9058797955513, 'eval_runtime': 3.2445, 'eval_samples_per_second': 308.213, 'eval_steps_per_second': 19.417, 'epoch': 0.12}
{'loss': 1.1957, 'grad_norm': 0.301250159740448, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.8666986227035522, 'eval_runtime': 3.2376, 'eval_samples_per_second': 308.871, 'eval_steps_per_second': 19.459, 'epoch': 0.16}
{'loss': 1.1781, 'grad_norm': 0.33100900053977966, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.85569167137146, 'eval_runtime': 3.2462, 'eval_samples_per_second': 308.051, 'eval_steps_per_second': 19.407, 'epoch': 0.2}
{'loss': 1.1395, 'grad_norm': 0.3152172565460205, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.8328810334205627, 'eval_runtime': 3.2442, 'eval_samples_per_second': 308.247, 'eval_steps_per_second': 19.42, 'epoch': 0.24}
{'loss': 1.2037, 'grad_norm': 0.2837997078895569, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.8264118432998657, 'eval_runtime': 3.2497, 'eval_samples_per_second': 307.72, 'eval_steps_per_second': 19.386, 'epoch': 0.28}
{'loss': 1.1109, 'grad_norm': 0.2643148601055145, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.821172297000885, 'eval_runtime': 3.251, 'eval_samples_per_second': 307.599, 'eval_steps_per_second': 19.379, 'epoch': 0.32}
{'loss': 1.1453, 'grad_norm': 0.2411995828151703, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.8174946308135986, 'eval_runtime': 3.2512, 'eval_samples_per_second': 307.579, 'eval_steps_per_second': 19.377, 'epoch': 0.36}
{'loss': 1.1508, 'grad_norm': 0.27363601326942444, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.8062101006507874, 'eval_runtime': 3.2568, 'eval_samples_per_second': 307.046, 'eval_steps_per_second': 19.344, 'epoch': 0.4}
{'loss': 1.045, 'grad_norm': 0.2966337502002716, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.8019089102745056, 'eval_runtime': 3.2585, 'eval_samples_per_second': 306.887, 'eval_steps_per_second': 19.334, 'epoch': 0.44}
{'loss': 1.1367, 'grad_norm': 0.2765748202800751, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.7919291853904724, 'eval_runtime': 3.2603, 'eval_samples_per_second': 306.72, 'eval_steps_per_second': 19.323, 'epoch': 0.48}
{'loss': 1.1085, 'grad_norm': 0.2751982808113098, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.7867394685745239, 'eval_runtime': 3.2576, 'eval_samples_per_second': 306.978, 'eval_steps_per_second': 19.34, 'epoch': 0.52}
{'loss': 1.1073, 'grad_norm': 0.30740588903427124, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.7790465354919434, 'eval_runtime': 3.2608, 'eval_samples_per_second': 306.673, 'eval_steps_per_second': 19.32, 'epoch': 0.56}
{'loss': 1.0781, 'grad_norm': 0.3218172490596771, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.7717011570930481, 'eval_runtime': 3.2544, 'eval_samples_per_second': 307.273, 'eval_steps_per_second': 19.358, 'epoch': 0.6}
{'loss': 1.103, 'grad_norm': 0.31282731890678406, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.779281497001648, 'eval_runtime': 3.256, 'eval_samples_per_second': 307.13, 'eval_steps_per_second': 19.349, 'epoch': 0.64}
{'loss': 1.119, 'grad_norm': 0.31480056047439575, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.7696710228919983, 'eval_runtime': 3.2565, 'eval_samples_per_second': 307.077, 'eval_steps_per_second': 19.346, 'epoch': 0.68}
{'loss': 1.0968, 'grad_norm': 0.2615137994289398, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.7614625096321106, 'eval_runtime': 3.2553, 'eval_samples_per_second': 307.19, 'eval_steps_per_second': 19.353, 'epoch': 0.72}
{'loss': 1.0734, 'grad_norm': 0.29043132066726685, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.7571626901626587, 'eval_runtime': 3.2576, 'eval_samples_per_second': 306.977, 'eval_steps_per_second': 19.34, 'epoch': 0.76}
{'loss': 1.1413, 'grad_norm': 0.3074702024459839, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.7553467154502869, 'eval_runtime': 3.2574, 'eval_samples_per_second': 306.99, 'eval_steps_per_second': 19.34, 'epoch': 0.8}
{'loss': 1.0466, 'grad_norm': 0.36686933040618896, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.7556812167167664, 'eval_runtime': 3.2731, 'eval_samples_per_second': 305.52, 'eval_steps_per_second': 19.248, 'epoch': 0.84}
{'loss': 1.0679, 'grad_norm': 0.28083786368370056, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.7489442825317383, 'eval_runtime': 3.2762, 'eval_samples_per_second': 305.232, 'eval_steps_per_second': 19.23, 'epoch': 0.88}
{'loss': 1.1067, 'grad_norm': 0.3229922354221344, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.7512247562408447, 'eval_runtime': 3.2743, 'eval_samples_per_second': 305.406, 'eval_steps_per_second': 19.241, 'epoch': 0.92}
{'loss': 1.0196, 'grad_norm': 0.29058051109313965, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.7505104541778564, 'eval_runtime': 3.2743, 'eval_samples_per_second': 305.41, 'eval_steps_per_second': 19.241, 'epoch': 0.96}
{'train_runtime': 228.6383, 'train_samples_per_second': 43.645, 'train_steps_per_second': 2.729, 'train_loss': 1.2291943660149207, 'epoch': 1.0}
train_results:  {'eval_loss': [2.277384042739868, 1.018937110900879, 0.9058797955513, 0.8666986227035522, 0.85569167137146, 0.8328810334205627, 0.8264118432998657, 0.821172297000885, 0.8174946308135986, 0.8062101006507874, 0.8019089102745056, 0.7919291853904724, 0.7867394685745239, 0.7790465354919434, 0.7717011570930481, 0.779281497001648, 0.7696710228919983, 0.7614625096321106, 0.7571626901626587, 0.7553467154502869, 0.7556812167167664, 0.7489442825317383, 0.7512247562408447, 0.7505104541778564], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.277384042739868, 1.018937110900879, 0.9058797955513, 0.8666986227035522, 0.85569167137146, 0.8328810334205627, 0.8264118432998657, 0.821172297000885, 0.8174946308135986, 0.8062101006507874, 0.8019089102745056, 0.7919291853904724, 0.7867394685745239, 0.7790465354919434, 0.7717011570930481, 0.779281497001648, 0.7696710228919983, 0.7614625096321106, 0.7571626901626587, 0.7553467154502869, 0.7556812167167664, 0.7489442825317383, 0.7512247562408447, 0.7505104541778564]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1273829936981201
current iteration best possible eval_loss (full train run):  -0.7505104541778564
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.2958 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.007365286350250244, 0.05544036626815796, 0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9870830178260803, 0.754863977432251, 0.9104870557785034, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8787160515785217, 0.16884422302246094, 0.48048388957977295, 0.9369502067565918]  ‚Üí  acq = -1.0518824622505234
X = [0.06936377286911011, 0.632781982421875, 0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7510114908218384, 0.5261915326118469, 0.9568371772766113, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.13144083321094513, 0.18786925077438354, 0.9205564260482788, 0.21238505840301514]  ‚Üí  acq = -1.0519101855868584
X = [0.1816890835762024, 0.5671297311782837, 0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.3449631333351135, 0.8766421675682068, 0.5072073936462402, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.30366525053977966, 0.8914388418197632, 0.9394388198852539, 0.7902622818946838]  ‚Üí  acq = -1.0519114430461438
X = [0.4756810665130615, 0.8949254751205444, 0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.4715011417865753, 0.27662307024002075, 0.1057593822479248, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.19461123645305634, 0.8357580900192261, 0.8589680194854736, 0.6040682792663574]  ‚Üí  acq = -1.0519418528572124
X = [0.14415353536605835, 0.1752747893333435, 0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.6041354537010193, 0.18114352226257324, 0.5152944922447205, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.49458131194114685, 0.7254683971405029, 0.4729866683483124, 0.9238991737365723]  ‚Üí  acq = -1.0519114179041344
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0803, dtype=torch.float64), tensor(0.2767, dtype=torch.float64), tensor(0.0456, dtype=torch.float64), tensor(0.0845, dtype=torch.float64), 0, tensor(0.2604, dtype=torch.float64), tensor(0.0769, dtype=torch.float64), tensor(0.0930, dtype=torch.float64), tensor(0.0826, dtype=torch.float64), 17, 0, 1, 0, 1, 1, 65, 0.006917288113729968, 28.993457433915104, 1]
normalized proposed parameters for next round by BO: [tensor(0.0803, dtype=torch.float64), tensor(0.2767, dtype=torch.float64), tensor(0.0456, dtype=torch.float64), tensor(0.0845, dtype=torch.float64), tensor(3.4322e-19, dtype=torch.float64), tensor(0.2604, dtype=torch.float64), tensor(0.0769, dtype=torch.float64), tensor(0.0930, dtype=torch.float64), tensor(0.0826, dtype=torch.float64), tensor(0.5360, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5046, dtype=torch.float64), tensor(0.0692, dtype=torch.float64), tensor(0.6040, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.08
  gsm8k: 0.277
  rowan_hellaswag: 0.046
  sciq: 0.084
  triviaqa: 0
  truthfulqa_gen: 0.26
  wikitext: 0.077
  mmlu: 0.093
  arc_challenge: 0.083

LoRA Parameters:
  lora_r: (65,)
  lora_dropout: (0.006917288113729968,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (28.993457433915104,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  65
lora dropout:  0.006917288113729968
lora alpha:  28.993457433915104
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 46,392,320 || all params: 8,076,653,568 || trainable%: 0.5744
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8565, 'grad_norm': 1.1931242942810059, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.617314338684082, 'eval_runtime': 3.5182, 'eval_samples_per_second': 284.236, 'eval_steps_per_second': 17.907, 'epoch': 0.04}
{'loss': 1.4333, 'grad_norm': 0.6659114360809326, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9856204390525818, 'eval_runtime': 3.5195, 'eval_samples_per_second': 284.127, 'eval_steps_per_second': 17.9, 'epoch': 0.08}
{'loss': 1.1791, 'grad_norm': 0.46533510088920593, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9137101173400879, 'eval_runtime': 3.5153, 'eval_samples_per_second': 284.469, 'eval_steps_per_second': 17.922, 'epoch': 0.12}
{'loss': 1.0964, 'grad_norm': 0.3585031032562256, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8193525671958923, 'eval_runtime': 3.5156, 'eval_samples_per_second': 284.443, 'eval_steps_per_second': 17.92, 'epoch': 0.16}
{'loss': 1.0918, 'grad_norm': 0.3942670524120331, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7643600106239319, 'eval_runtime': 3.5145, 'eval_samples_per_second': 284.535, 'eval_steps_per_second': 17.926, 'epoch': 0.2}
{'loss': 1.0602, 'grad_norm': 0.37719619274139404, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7316284775733948, 'eval_runtime': 3.5209, 'eval_samples_per_second': 284.017, 'eval_steps_per_second': 17.893, 'epoch': 0.24}
{'loss': 1.0905, 'grad_norm': 0.26119348406791687, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.716616690158844, 'eval_runtime': 3.5213, 'eval_samples_per_second': 283.989, 'eval_steps_per_second': 17.891, 'epoch': 0.28}
{'loss': 1.0222, 'grad_norm': 0.3309987783432007, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6953503489494324, 'eval_runtime': 3.5243, 'eval_samples_per_second': 283.747, 'eval_steps_per_second': 17.876, 'epoch': 0.32}
{'loss': 1.0537, 'grad_norm': 0.31284549832344055, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6768032312393188, 'eval_runtime': 3.5306, 'eval_samples_per_second': 283.241, 'eval_steps_per_second': 17.844, 'epoch': 0.36}
{'loss': 0.9967, 'grad_norm': 0.26969027519226074, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.654941201210022, 'eval_runtime': 3.5564, 'eval_samples_per_second': 281.183, 'eval_steps_per_second': 17.715, 'epoch': 0.4}
{'loss': 1.0529, 'grad_norm': 0.2776411771774292, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6296930909156799, 'eval_runtime': 3.5511, 'eval_samples_per_second': 281.605, 'eval_steps_per_second': 17.741, 'epoch': 0.44}
{'loss': 1.0609, 'grad_norm': 0.30013492703437805, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6128221750259399, 'eval_runtime': 3.5406, 'eval_samples_per_second': 282.44, 'eval_steps_per_second': 17.794, 'epoch': 0.48}
{'loss': 0.9966, 'grad_norm': 0.2723179757595062, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5843240022659302, 'eval_runtime': 3.5379, 'eval_samples_per_second': 282.656, 'eval_steps_per_second': 17.807, 'epoch': 0.52}
{'loss': 1.0123, 'grad_norm': 0.2880306839942932, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.582785427570343, 'eval_runtime': 3.5382, 'eval_samples_per_second': 282.632, 'eval_steps_per_second': 17.806, 'epoch': 0.56}
{'loss': 0.9797, 'grad_norm': 0.38393858075141907, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5570968389511108, 'eval_runtime': 3.5421, 'eval_samples_per_second': 282.315, 'eval_steps_per_second': 17.786, 'epoch': 0.6}
{'loss': 0.968, 'grad_norm': 0.5973297953605652, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5348706245422363, 'eval_runtime': 3.5453, 'eval_samples_per_second': 282.063, 'eval_steps_per_second': 17.77, 'epoch': 0.64}
{'loss': 1.0014, 'grad_norm': 0.29359185695648193, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5262289643287659, 'eval_runtime': 3.5463, 'eval_samples_per_second': 281.983, 'eval_steps_per_second': 17.765, 'epoch': 0.68}
{'loss': 0.9344, 'grad_norm': 0.2772531509399414, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5053589940071106, 'eval_runtime': 3.5501, 'eval_samples_per_second': 281.686, 'eval_steps_per_second': 17.746, 'epoch': 0.72}
{'loss': 1.0247, 'grad_norm': 0.37929588556289673, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4939497113227844, 'eval_runtime': 3.5369, 'eval_samples_per_second': 282.734, 'eval_steps_per_second': 17.812, 'epoch': 0.76}
{'loss': 1.0453, 'grad_norm': 0.3855593204498291, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.478924959897995, 'eval_runtime': 3.5357, 'eval_samples_per_second': 282.83, 'eval_steps_per_second': 17.818, 'epoch': 0.8}
{'loss': 1.0177, 'grad_norm': 0.26344409584999084, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4686206877231598, 'eval_runtime': 3.5327, 'eval_samples_per_second': 283.073, 'eval_steps_per_second': 17.834, 'epoch': 0.84}
{'loss': 0.9883, 'grad_norm': 0.23960086703300476, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.45682182908058167, 'eval_runtime': 3.5353, 'eval_samples_per_second': 282.858, 'eval_steps_per_second': 17.82, 'epoch': 0.88}
{'loss': 0.9014, 'grad_norm': 0.3381950855255127, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.44370678067207336, 'eval_runtime': 3.5333, 'eval_samples_per_second': 283.024, 'eval_steps_per_second': 17.83, 'epoch': 0.92}
{'loss': 1.0057, 'grad_norm': 0.3144912123680115, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.4373320937156677, 'eval_runtime': 3.5342, 'eval_samples_per_second': 282.951, 'eval_steps_per_second': 17.826, 'epoch': 0.96}
{'loss': 0.9397, 'grad_norm': 0.4674762487411499, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.4345969259738922, 'eval_runtime': 3.53, 'eval_samples_per_second': 283.288, 'eval_steps_per_second': 17.847, 'epoch': 1.0}
{'train_runtime': 308.527, 'train_samples_per_second': 32.396, 'train_steps_per_second': 2.026, 'train_loss': 1.1123782348632814, 'epoch': 1.0}
train_results:  {'eval_loss': [1.617314338684082, 0.9856204390525818, 0.9137101173400879, 0.8193525671958923, 0.7643600106239319, 0.7316284775733948, 0.716616690158844, 0.6953503489494324, 0.6768032312393188, 0.654941201210022, 0.6296930909156799, 0.6128221750259399, 0.5843240022659302, 0.582785427570343, 0.5570968389511108, 0.5348706245422363, 0.5262289643287659, 0.5053589940071106, 0.4939497113227844, 0.478924959897995, 0.4686206877231598, 0.45682182908058167, 0.44370678067207336, 0.4373320937156677, 0.4345969259738922], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.617314338684082, 0.9856204390525818, 0.9137101173400879, 0.8193525671958923, 0.7643600106239319, 0.7316284775733948, 0.716616690158844, 0.6953503489494324, 0.6768032312393188, 0.654941201210022, 0.6296930909156799, 0.6128221750259399, 0.5843240022659302, 0.582785427570343, 0.5570968389511108, 0.5348706245422363, 0.5262289643287659, 0.5053589940071106, 0.4939497113227844, 0.478924959897995, 0.4686206877231598, 0.45682182908058167, 0.44370678067207336, 0.4373320937156677, 0.4345969259738922]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1263079643249512
current iteration best possible eval_loss (full train run):  -0.4345969259738922
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.4660 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -1.0481376063967915
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -1.047810264955083
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -1.0478231389506558
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -1.0478231352958116
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -1.0478194463772206
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1540, dtype=torch.float64), tensor(0.1679, dtype=torch.float64), tensor(0.2895, dtype=torch.float64), 0, tensor(0.0261, dtype=torch.float64), 0, tensor(0.3625, dtype=torch.float64), 17, 1, 0, 1, 0, 1, 8, 0.02997305159457278, 32.84718857312018, 1]
normalized proposed parameters for next round by BO: [tensor(1.3440e-18, dtype=torch.float64), tensor(2.6140e-20, dtype=torch.float64), tensor(0.1540, dtype=torch.float64), tensor(0.1679, dtype=torch.float64), tensor(0.2895, dtype=torch.float64), tensor(2.1536e-17, dtype=torch.float64), tensor(0.0261, dtype=torch.float64), tensor(4.4464e-21, dtype=torch.float64), tensor(0.3625, dtype=torch.float64), tensor(0.5171, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0642, dtype=torch.float64), tensor(0.2997, dtype=torch.float64), tensor(0.6843, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.154
  sciq: 0.168
  triviaqa: 0.289
  truthfulqa_gen: 0
  wikitext: 0.026
  mmlu: 0
  arc_challenge: 0.362

LoRA Parameters:
  lora_r: (8,)
  lora_dropout: (0.02997305159457278,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (32.84718857312018,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  8
lora dropout:  0.02997305159457278
lora alpha:  32.84718857312018
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 6,127,616 || all params: 8,036,388,864 || trainable%: 0.0762
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4195, 'grad_norm': 1.6575170755386353, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.112504482269287, 'eval_runtime': 3.3802, 'eval_samples_per_second': 295.84, 'eval_steps_per_second': 18.638, 'epoch': 0.04}
{'loss': 1.7737, 'grad_norm': 1.0609567165374756, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.454775094985962, 'eval_runtime': 3.3836, 'eval_samples_per_second': 295.54, 'eval_steps_per_second': 18.619, 'epoch': 0.08}
{'loss': 1.4328, 'grad_norm': 0.6910524964332581, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4152461290359497, 'eval_runtime': 3.3784, 'eval_samples_per_second': 295.998, 'eval_steps_per_second': 18.648, 'epoch': 0.12}
{'loss': 1.2663, 'grad_norm': 0.5979419350624084, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3576401472091675, 'eval_runtime': 3.3847, 'eval_samples_per_second': 295.448, 'eval_steps_per_second': 18.613, 'epoch': 0.16}
{'loss': 1.3192, 'grad_norm': 0.590544581413269, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4831606149673462, 'eval_runtime': 3.3918, 'eval_samples_per_second': 294.828, 'eval_steps_per_second': 18.574, 'epoch': 0.2}
{'loss': 1.2757, 'grad_norm': 0.6053304076194763, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3660448789596558, 'eval_runtime': 3.3982, 'eval_samples_per_second': 294.276, 'eval_steps_per_second': 18.539, 'epoch': 0.24}
{'loss': 1.2532, 'grad_norm': 0.596447229385376, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2779136896133423, 'eval_runtime': 3.4023, 'eval_samples_per_second': 293.918, 'eval_steps_per_second': 18.517, 'epoch': 0.28}
{'loss': 1.2138, 'grad_norm': 0.6882462501525879, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3455109596252441, 'eval_runtime': 3.4104, 'eval_samples_per_second': 293.219, 'eval_steps_per_second': 18.473, 'epoch': 0.32}
{'loss': 1.1619, 'grad_norm': 0.8188254833221436, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2326154708862305, 'eval_runtime': 3.4009, 'eval_samples_per_second': 294.036, 'eval_steps_per_second': 18.524, 'epoch': 0.36}
{'loss': 1.1485, 'grad_norm': 0.5857834815979004, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2510918378829956, 'eval_runtime': 3.405, 'eval_samples_per_second': 293.686, 'eval_steps_per_second': 18.502, 'epoch': 0.4}
{'loss': 1.1335, 'grad_norm': 0.7731661796569824, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.28523850440979, 'eval_runtime': 3.3974, 'eval_samples_per_second': 294.339, 'eval_steps_per_second': 18.543, 'epoch': 0.44}
{'loss': 1.1391, 'grad_norm': 0.6707339882850647, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.278842568397522, 'eval_runtime': 3.4011, 'eval_samples_per_second': 294.019, 'eval_steps_per_second': 18.523, 'epoch': 0.48}
{'loss': 1.1318, 'grad_norm': 0.7671568989753723, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2432688474655151, 'eval_runtime': 3.4118, 'eval_samples_per_second': 293.104, 'eval_steps_per_second': 18.466, 'epoch': 0.52}
{'loss': 1.103, 'grad_norm': 0.8471682667732239, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.267582654953003, 'eval_runtime': 3.432, 'eval_samples_per_second': 291.375, 'eval_steps_per_second': 18.357, 'epoch': 0.56}
{'loss': 1.1551, 'grad_norm': 0.8217447996139526, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.259303092956543, 'eval_runtime': 3.4366, 'eval_samples_per_second': 290.988, 'eval_steps_per_second': 18.332, 'epoch': 0.6}
{'loss': 1.0722, 'grad_norm': 0.7285378575325012, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3009533882141113, 'eval_runtime': 3.4553, 'eval_samples_per_second': 289.407, 'eval_steps_per_second': 18.233, 'epoch': 0.64}
{'loss': 1.051, 'grad_norm': 1.0371092557907104, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2014551162719727, 'eval_runtime': 3.4534, 'eval_samples_per_second': 289.572, 'eval_steps_per_second': 18.243, 'epoch': 0.68}
{'loss': 1.1151, 'grad_norm': 1.0212451219558716, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.243133306503296, 'eval_runtime': 3.4477, 'eval_samples_per_second': 290.048, 'eval_steps_per_second': 18.273, 'epoch': 0.72}
{'loss': 1.0684, 'grad_norm': 1.0830214023590088, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2841367721557617, 'eval_runtime': 3.4643, 'eval_samples_per_second': 288.662, 'eval_steps_per_second': 18.186, 'epoch': 0.76}
{'loss': 1.0498, 'grad_norm': 1.0557894706726074, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3030742406845093, 'eval_runtime': 3.4448, 'eval_samples_per_second': 290.294, 'eval_steps_per_second': 18.289, 'epoch': 0.8}
{'loss': 1.0588, 'grad_norm': 0.8209934234619141, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3076260089874268, 'eval_runtime': 3.449, 'eval_samples_per_second': 289.937, 'eval_steps_per_second': 18.266, 'epoch': 0.84}
{'loss': 1.1181, 'grad_norm': 1.1970936059951782, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3064206838607788, 'eval_runtime': 3.4379, 'eval_samples_per_second': 290.871, 'eval_steps_per_second': 18.325, 'epoch': 0.88}
{'loss': 1.0438, 'grad_norm': 0.986332893371582, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2732412815093994, 'eval_runtime': 3.4413, 'eval_samples_per_second': 290.584, 'eval_steps_per_second': 18.307, 'epoch': 0.92}
{'loss': 1.0065, 'grad_norm': 1.066362977027893, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.26737642288208, 'eval_runtime': 3.4524, 'eval_samples_per_second': 289.655, 'eval_steps_per_second': 18.248, 'epoch': 0.96}
{'loss': 1.0049, 'grad_norm': 0.9644259214401245, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.268872857093811, 'eval_runtime': 3.4482, 'eval_samples_per_second': 290.004, 'eval_steps_per_second': 18.27, 'epoch': 1.0}
{'train_runtime': 287.0464, 'train_samples_per_second': 34.827, 'train_steps_per_second': 2.177, 'train_loss': 1.2606301971435547, 'epoch': 1.0}
train_results:  {'eval_loss': [2.112504482269287, 1.454775094985962, 1.4152461290359497, 1.3576401472091675, 1.4831606149673462, 1.3660448789596558, 1.2779136896133423, 1.3455109596252441, 1.2326154708862305, 1.2510918378829956, 1.28523850440979, 1.278842568397522, 1.2432688474655151, 1.267582654953003, 1.259303092956543, 1.3009533882141113, 1.2014551162719727, 1.243133306503296, 1.2841367721557617, 1.3030742406845093, 1.3076260089874268, 1.3064206838607788, 1.2732412815093994, 1.26737642288208, 1.268872857093811], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.112504482269287, 1.454775094985962, 1.4152461290359497, 1.3576401472091675, 1.4831606149673462, 1.3660448789596558, 1.2779136896133423, 1.3455109596252441, 1.2326154708862305, 1.2510918378829956, 1.28523850440979, 1.278842568397522, 1.2432688474655151, 1.267582654953003, 1.259303092956543, 1.3009533882141113, 1.2014551162719727, 1.243133306503296, 1.2841367721557617, 1.3030742406845093, 1.3076260089874268, 1.3064206838607788, 1.2732412815093994, 1.26737642288208, 1.268872857093811]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -1.268872857093811
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.2598 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -1.0437080970845916
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.9660823434698114
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -1.043724385100042
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -1.0437240118075968
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -1.0437243849078515
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0667, dtype=torch.float64), tensor(0.0341, dtype=torch.float64), tensor(0.4711, dtype=torch.float64), tensor(0.0657, dtype=torch.float64), 0, tensor(0.0259, dtype=torch.float64), 0, tensor(0.3365, dtype=torch.float64), 16, 1, 1, 0, 1, 1, 75, 3.4694469519536153e-19, 34.297930073478895, 1]
normalized proposed parameters for next round by BO: [tensor(1.9955e-19, dtype=torch.float64), tensor(0.0667, dtype=torch.float64), tensor(0.0341, dtype=torch.float64), tensor(0.4711, dtype=torch.float64), tensor(0.0657, dtype=torch.float64), tensor(3.9804e-18, dtype=torch.float64), tensor(0.0259, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3365, dtype=torch.float64), tensor(0.5010, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5895, dtype=torch.float64), tensor(3.4694e-18, dtype=torch.float64), tensor(0.7145, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.067
  rowan_hellaswag: 0.034
  sciq: 0.471
  triviaqa: 0.066
  truthfulqa_gen: 0
  wikitext: 0.026
  mmlu: 0
  arc_challenge: 0.336

LoRA Parameters:
  lora_r: (75,)
  lora_dropout: (3.4694469519536153e-19,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (34.297930073478895,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  75
lora dropout:  3.4694469519536153e-19
lora alpha:  34.297930073478895
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 60,211,200 || all params: 8,090,472,448 || trainable%: 0.7442
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2028, 'grad_norm': 1.3572407960891724, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6070736646652222, 'eval_runtime': 3.6212, 'eval_samples_per_second': 276.148, 'eval_steps_per_second': 17.397, 'epoch': 0.04}
{'loss': 1.3032, 'grad_norm': 0.5624992251396179, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2406755685806274, 'eval_runtime': 3.6244, 'eval_samples_per_second': 275.909, 'eval_steps_per_second': 17.382, 'epoch': 0.08}
{'loss': 1.0941, 'grad_norm': 0.43167027831077576, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.170344591140747, 'eval_runtime': 3.63, 'eval_samples_per_second': 275.483, 'eval_steps_per_second': 17.355, 'epoch': 0.12}
{'loss': 0.9696, 'grad_norm': 0.3812221586704254, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1477506160736084, 'eval_runtime': 3.6378, 'eval_samples_per_second': 274.895, 'eval_steps_per_second': 17.318, 'epoch': 0.16}
{'loss': 0.9608, 'grad_norm': 0.4155401587486267, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1100749969482422, 'eval_runtime': 3.6334, 'eval_samples_per_second': 275.223, 'eval_steps_per_second': 17.339, 'epoch': 0.2}
{'loss': 0.9237, 'grad_norm': 0.4645039439201355, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.13631272315979, 'eval_runtime': 3.6386, 'eval_samples_per_second': 274.83, 'eval_steps_per_second': 17.314, 'epoch': 0.24}
{'loss': 0.9398, 'grad_norm': 0.3134848475456238, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1017858982086182, 'eval_runtime': 3.6153, 'eval_samples_per_second': 276.604, 'eval_steps_per_second': 17.426, 'epoch': 0.28}
{'loss': 0.8683, 'grad_norm': 0.30432265996932983, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1693179607391357, 'eval_runtime': 3.6136, 'eval_samples_per_second': 276.735, 'eval_steps_per_second': 17.434, 'epoch': 0.32}
{'loss': 0.951, 'grad_norm': 0.2950199842453003, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1599140167236328, 'eval_runtime': 3.7406, 'eval_samples_per_second': 267.339, 'eval_steps_per_second': 16.842, 'epoch': 0.36}
{'loss': 0.8815, 'grad_norm': 0.4462120831012726, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.18452787399292, 'eval_runtime': 4.1182, 'eval_samples_per_second': 242.824, 'eval_steps_per_second': 15.298, 'epoch': 0.4}
{'loss': 0.8597, 'grad_norm': 0.28585752844810486, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1595423221588135, 'eval_runtime': 3.609, 'eval_samples_per_second': 277.088, 'eval_steps_per_second': 17.457, 'epoch': 0.44}
{'loss': 0.8972, 'grad_norm': 0.3498545289039612, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2348434925079346, 'eval_runtime': 3.6178, 'eval_samples_per_second': 276.412, 'eval_steps_per_second': 17.414, 'epoch': 0.48}
{'loss': 0.8793, 'grad_norm': 0.3872101902961731, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1387207508087158, 'eval_runtime': 3.6181, 'eval_samples_per_second': 276.385, 'eval_steps_per_second': 17.412, 'epoch': 0.52}
{'loss': 0.8535, 'grad_norm': 0.3948342800140381, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1511335372924805, 'eval_runtime': 3.6327, 'eval_samples_per_second': 275.274, 'eval_steps_per_second': 17.342, 'epoch': 0.56}
{'loss': 0.8493, 'grad_norm': 0.3222198486328125, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1853153705596924, 'eval_runtime': 3.6321, 'eval_samples_per_second': 275.326, 'eval_steps_per_second': 17.346, 'epoch': 0.6}
{'loss': 0.8701, 'grad_norm': 0.34379613399505615, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1993476152420044, 'eval_runtime': 3.6323, 'eval_samples_per_second': 275.306, 'eval_steps_per_second': 17.344, 'epoch': 0.64}
{'loss': 0.8204, 'grad_norm': 0.35727575421333313, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1590783596038818, 'eval_runtime': 3.6354, 'eval_samples_per_second': 275.073, 'eval_steps_per_second': 17.33, 'epoch': 0.68}
{'loss': 0.8379, 'grad_norm': 0.369109183549881, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.169785976409912, 'eval_runtime': 3.6389, 'eval_samples_per_second': 274.808, 'eval_steps_per_second': 17.313, 'epoch': 0.72}
{'loss': 0.8802, 'grad_norm': 0.3617148697376251, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2114208936691284, 'eval_runtime': 3.6192, 'eval_samples_per_second': 276.301, 'eval_steps_per_second': 17.407, 'epoch': 0.76}
{'loss': 0.8536, 'grad_norm': 0.4312433898448944, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1890734434127808, 'eval_runtime': 3.6162, 'eval_samples_per_second': 276.537, 'eval_steps_per_second': 17.422, 'epoch': 0.8}
{'loss': 0.8293, 'grad_norm': 0.4851556122303009, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.172375202178955, 'eval_runtime': 3.6149, 'eval_samples_per_second': 276.633, 'eval_steps_per_second': 17.428, 'epoch': 0.84}
{'loss': 0.8153, 'grad_norm': 0.41404783725738525, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1885075569152832, 'eval_runtime': 3.6161, 'eval_samples_per_second': 276.541, 'eval_steps_per_second': 17.422, 'epoch': 0.88}
{'loss': 0.8156, 'grad_norm': 0.6090584397315979, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.200466513633728, 'eval_runtime': 3.6117, 'eval_samples_per_second': 276.877, 'eval_steps_per_second': 17.443, 'epoch': 0.92}
{'loss': 0.787, 'grad_norm': 0.4496476352214813, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.197075605392456, 'eval_runtime': 3.6108, 'eval_samples_per_second': 276.945, 'eval_steps_per_second': 17.448, 'epoch': 0.96}
{'loss': 0.8102, 'grad_norm': 0.3836449980735779, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1987717151641846, 'eval_runtime': 3.614, 'eval_samples_per_second': 276.7, 'eval_steps_per_second': 17.432, 'epoch': 1.0}
{'train_runtime': 297.1783, 'train_samples_per_second': 33.643, 'train_steps_per_second': 2.103, 'train_loss': 0.9901351593017578, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6070736646652222, 1.2406755685806274, 1.170344591140747, 1.1477506160736084, 1.1100749969482422, 1.13631272315979, 1.1017858982086182, 1.1693179607391357, 1.1599140167236328, 1.18452787399292, 1.1595423221588135, 1.2348434925079346, 1.1387207508087158, 1.1511335372924805, 1.1853153705596924, 1.1993476152420044, 1.1590783596038818, 1.169785976409912, 1.2114208936691284, 1.1890734434127808, 1.172375202178955, 1.1885075569152832, 1.200466513633728, 1.197075605392456, 1.1987717151641846], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6070736646652222, 1.2406755685806274, 1.170344591140747, 1.1477506160736084, 1.1100749969482422, 1.13631272315979, 1.1017858982086182, 1.1693179607391357, 1.1599140167236328, 1.18452787399292, 1.1595423221588135, 1.2348434925079346, 1.1387207508087158, 1.1511335372924805, 1.1853153705596924, 1.1993476152420044, 1.1590783596038818, 1.169785976409912, 1.2114208936691284, 1.1890734434127808, 1.172375202178955, 1.1885075569152832, 1.200466513633728, 1.197075605392456, 1.1987717151641846]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.124749779701233
current iteration best possible eval_loss (full train run):  -1.1987717151641846
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.8016 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -1.0545476000819478
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -1.0548579764467
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -1.060392184950833
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -1.0547995519682394
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -1.054798063559497
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2208, dtype=torch.float64), tensor(0.0882, dtype=torch.float64), 0, tensor(0.2437, dtype=torch.float64), 0, tensor(0.2693, dtype=torch.float64), tensor(0.0480, dtype=torch.float64), tensor(0.0962, dtype=torch.float64), tensor(0.0264, dtype=torch.float64), 14, 0, 1, 0, 1, 1, 63, 1.823477323089251e-19, 28.891492099938546, 1]
normalized proposed parameters for next round by BO: [tensor(0.2208, dtype=torch.float64), tensor(0.0882, dtype=torch.float64), tensor(0.0060, dtype=torch.float64), tensor(0.2437, dtype=torch.float64), tensor(0.0014, dtype=torch.float64), tensor(0.2693, dtype=torch.float64), tensor(0.0480, dtype=torch.float64), tensor(0.0962, dtype=torch.float64), tensor(0.0264, dtype=torch.float64), tensor(0.4332, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4958, dtype=torch.float64), tensor(1.8235e-18, dtype=torch.float64), tensor(0.6019, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.221
  gsm8k: 0.088
  rowan_hellaswag: 0
  sciq: 0.244
  triviaqa: 0
  truthfulqa_gen: 0.269
  wikitext: 0.048
  mmlu: 0.096
  arc_challenge: 0.026

LoRA Parameters:
  lora_r: (63,)
  lora_dropout: (1.823477323089251e-19,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (28.891492099938546,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  63
lora dropout:  1.823477323089251e-19
lora alpha:  28.891492099938546
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 37,029,888 || all params: 8,067,291,136 || trainable%: 0.4590
length of training data:  9922
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1751, 'grad_norm': 2.329313039779663, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.646691918373108, 'eval_runtime': 3.4338, 'eval_samples_per_second': 291.221, 'eval_steps_per_second': 18.347, 'epoch': 0.04}
{'loss': 1.4467, 'grad_norm': 0.7932276725769043, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0348567962646484, 'eval_runtime': 3.418, 'eval_samples_per_second': 292.566, 'eval_steps_per_second': 18.432, 'epoch': 0.08}
{'loss': 1.127, 'grad_norm': 0.5789691209793091, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.9157950282096863, 'eval_runtime': 3.4281, 'eval_samples_per_second': 291.704, 'eval_steps_per_second': 18.377, 'epoch': 0.12}
{'loss': 1.0041, 'grad_norm': 0.3860028088092804, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.8134722709655762, 'eval_runtime': 3.4348, 'eval_samples_per_second': 291.135, 'eval_steps_per_second': 18.342, 'epoch': 0.16}
{'loss': 1.0144, 'grad_norm': 0.39812061190605164, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.780657172203064, 'eval_runtime': 3.4414, 'eval_samples_per_second': 290.578, 'eval_steps_per_second': 18.306, 'epoch': 0.2}
{'loss': 1.0311, 'grad_norm': 0.3595093786716461, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.7465594410896301, 'eval_runtime': 3.4476, 'eval_samples_per_second': 290.059, 'eval_steps_per_second': 18.274, 'epoch': 0.24}
{'loss': 1.0044, 'grad_norm': 0.4201607406139374, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.7398841381072998, 'eval_runtime': 3.4485, 'eval_samples_per_second': 289.978, 'eval_steps_per_second': 18.269, 'epoch': 0.28}
{'loss': 0.9357, 'grad_norm': 0.4182456433773041, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.7159907817840576, 'eval_runtime': 3.4578, 'eval_samples_per_second': 289.202, 'eval_steps_per_second': 18.22, 'epoch': 0.32}
{'loss': 0.9168, 'grad_norm': 0.3480851352214813, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.6893404722213745, 'eval_runtime': 3.4435, 'eval_samples_per_second': 290.406, 'eval_steps_per_second': 18.296, 'epoch': 0.36}
{'loss': 0.9879, 'grad_norm': 0.2349368780851364, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.6776877641677856, 'eval_runtime': 3.4505, 'eval_samples_per_second': 289.812, 'eval_steps_per_second': 18.258, 'epoch': 0.4}
{'loss': 0.9495, 'grad_norm': 0.3143007755279541, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.654186487197876, 'eval_runtime': 3.4479, 'eval_samples_per_second': 290.031, 'eval_steps_per_second': 18.272, 'epoch': 0.44}
{'loss': 0.9861, 'grad_norm': 0.34530115127563477, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.6448708772659302, 'eval_runtime': 3.449, 'eval_samples_per_second': 289.942, 'eval_steps_per_second': 18.266, 'epoch': 0.48}
{'loss': 0.9647, 'grad_norm': 0.3003653585910797, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.639575183391571, 'eval_runtime': 3.4557, 'eval_samples_per_second': 289.38, 'eval_steps_per_second': 18.231, 'epoch': 0.52}
{'loss': 0.9393, 'grad_norm': 0.2886236310005188, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.613188624382019, 'eval_runtime': 3.4567, 'eval_samples_per_second': 289.296, 'eval_steps_per_second': 18.226, 'epoch': 0.56}
{'loss': 0.9023, 'grad_norm': 0.3321060240268707, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.5935229659080505, 'eval_runtime': 3.4551, 'eval_samples_per_second': 289.429, 'eval_steps_per_second': 18.234, 'epoch': 0.6}
{'loss': 0.95, 'grad_norm': 0.3575087785720825, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.5872505903244019, 'eval_runtime': 3.4848, 'eval_samples_per_second': 286.964, 'eval_steps_per_second': 18.079, 'epoch': 0.64}
{'loss': 0.9522, 'grad_norm': 0.34266188740730286, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.568098783493042, 'eval_runtime': 3.46, 'eval_samples_per_second': 289.014, 'eval_steps_per_second': 18.208, 'epoch': 0.68}
{'loss': 0.9393, 'grad_norm': 0.4327373504638672, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.553849458694458, 'eval_runtime': 3.4481, 'eval_samples_per_second': 290.018, 'eval_steps_per_second': 18.271, 'epoch': 0.72}
{'loss': 0.9407, 'grad_norm': 0.3777599036693573, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.5433286428451538, 'eval_runtime': 3.4557, 'eval_samples_per_second': 289.374, 'eval_steps_per_second': 18.231, 'epoch': 0.76}
{'loss': 0.8982, 'grad_norm': 0.4585527777671814, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.5340721011161804, 'eval_runtime': 3.4573, 'eval_samples_per_second': 289.241, 'eval_steps_per_second': 18.222, 'epoch': 0.81}
{'loss': 0.9009, 'grad_norm': 0.40790870785713196, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.5133082866668701, 'eval_runtime': 3.4576, 'eval_samples_per_second': 289.221, 'eval_steps_per_second': 18.221, 'epoch': 0.85}
{'loss': 0.9243, 'grad_norm': 0.3476976752281189, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.5060730576515198, 'eval_runtime': 3.46, 'eval_samples_per_second': 289.014, 'eval_steps_per_second': 18.208, 'epoch': 0.89}
{'loss': 0.8708, 'grad_norm': 0.5826290249824524, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.4906245768070221, 'eval_runtime': 3.4527, 'eval_samples_per_second': 289.624, 'eval_steps_per_second': 18.246, 'epoch': 0.93}
{'loss': 0.9103, 'grad_norm': 0.30709055066108704, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.4849514961242676, 'eval_runtime': 3.4603, 'eval_samples_per_second': 288.996, 'eval_steps_per_second': 18.207, 'epoch': 0.97}
{'train_runtime': 270.4025, 'train_samples_per_second': 36.693, 'train_steps_per_second': 2.297, 'train_loss': 1.0635605349824817, 'epoch': 1.0}
train_results:  {'eval_loss': [1.646691918373108, 1.0348567962646484, 0.9157950282096863, 0.8134722709655762, 0.780657172203064, 0.7465594410896301, 0.7398841381072998, 0.7159907817840576, 0.6893404722213745, 0.6776877641677856, 0.654186487197876, 0.6448708772659302, 0.639575183391571, 0.613188624382019, 0.5935229659080505, 0.5872505903244019, 0.568098783493042, 0.553849458694458, 0.5433286428451538, 0.5340721011161804, 0.5133082866668701, 0.5060730576515198, 0.4906245768070221, 0.4849514961242676], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.646691918373108, 1.0348567962646484, 0.9157950282096863, 0.8134722709655762, 0.780657172203064, 0.7465594410896301, 0.7398841381072998, 0.7159907817840576, 0.6893404722213745, 0.6776877641677856, 0.654186487197876, 0.6448708772659302, 0.639575183391571, 0.613188624382019, 0.5935229659080505, 0.5872505903244019, 0.568098783493042, 0.553849458694458, 0.5433286428451538, 0.5340721011161804, 0.5133082866668701, 0.5060730576515198, 0.4906245768070221, 0.4849514961242676]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1266225576400757
current iteration best possible eval_loss (full train run):  -0.4849514961242676
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.9131 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7657198309898376, 0.9410857558250427, 0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.32434797286987305, 0.1894705891609192, 0.7398009896278381, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.25163692235946655, 0.6103376746177673, 0.06646472215652466, 0.1532604694366455]  ‚Üí  acq = -1.0255503578836271
X = [0.9276310801506042, 0.01835566759109497, 0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.4864007830619812, 0.852687656879425, 0.4021565914154053, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7941768169403076, 0.03410828113555908, 0.9558417797088623, 0.6570152640342712]  ‚Üí  acq = -1.024449691033483
X = [0.3898862600326538, 0.1672157645225525, 0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8220118284225464, 0.09181463718414307, 0.7922564744949341, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8501660823822021, 0.8251820206642151, 0.8375023603439331, 0.5925764441490173]  ‚Üí  acq = -1.0255146165628102
X = [0.7604454159736633, 0.9365105628967285, 0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.14734964072704315, 0.7609574198722839, 0.3722277879714966, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9713605642318726, 0.19831812381744385, 0.38370370864868164, 0.6161256432533264]  ‚Üí  acq = -1.0327894523863672
X = [0.0232393741607666, 0.5331325531005859, 0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8101420402526855, 0.707656979560852, 0.6279451251029968, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.42141979932785034, 0.38733386993408203, 0.770684003829956, 0.287418007850647]  ‚Üí  acq = -1.024923646771018
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0557, dtype=torch.float64), tensor(0.1363, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.0495, dtype=torch.float64), tensor(0.1226, dtype=torch.float64), tensor(0.1030, dtype=torch.float64), 0, tensor(0.1201, dtype=torch.float64), tensor(0.3836, dtype=torch.float64), 20, 1, 1, 0, 1, 1, 101, 8.656664920849258e-20, 32.281455366633956, 1]
normalized proposed parameters for next round by BO: [tensor(0.0557, dtype=torch.float64), tensor(0.1363, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.0495, dtype=torch.float64), tensor(0.1226, dtype=torch.float64), tensor(0.1030, dtype=torch.float64), tensor(1.0309e-18, dtype=torch.float64), tensor(0.1201, dtype=torch.float64), tensor(0.3836, dtype=torch.float64), tensor(0.6283, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7852, dtype=torch.float64), tensor(8.6567e-19, dtype=torch.float64), tensor(0.6725, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.056
  gsm8k: 0.136
  rowan_hellaswag: 0.029
  sciq: 0.05
  triviaqa: 0.123
  truthfulqa_gen: 0.103
  wikitext: 0
  mmlu: 0.12
  arc_challenge: 0.384

LoRA Parameters:
  lora_r: (101,)
  lora_dropout: (8.656664920849258e-20,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (32.281455366633956,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  101
lora dropout:  8.656664920849258e-20
lora alpha:  32.281455366633956
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 101,355,520 || all params: 8,131,616,768 || trainable%: 1.2464
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8764, 'grad_norm': 0.684783399105072, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6229969263076782, 'eval_runtime': 3.7836, 'eval_samples_per_second': 264.296, 'eval_steps_per_second': 16.651, 'epoch': 0.04}
{'loss': 1.2988, 'grad_norm': 0.46777597069740295, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0162092447280884, 'eval_runtime': 3.7866, 'eval_samples_per_second': 264.09, 'eval_steps_per_second': 16.638, 'epoch': 0.08}
{'loss': 1.0728, 'grad_norm': 0.40705472230911255, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9562476873397827, 'eval_runtime': 3.7813, 'eval_samples_per_second': 264.457, 'eval_steps_per_second': 16.661, 'epoch': 0.12}
{'loss': 1.0562, 'grad_norm': 0.37887004017829895, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8427256941795349, 'eval_runtime': 3.7834, 'eval_samples_per_second': 264.311, 'eval_steps_per_second': 16.652, 'epoch': 0.16}
{'loss': 1.0288, 'grad_norm': 0.30170655250549316, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8172823190689087, 'eval_runtime': 3.7902, 'eval_samples_per_second': 263.838, 'eval_steps_per_second': 16.622, 'epoch': 0.2}
{'loss': 0.9945, 'grad_norm': 0.19106808304786682, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8066696524620056, 'eval_runtime': 3.7957, 'eval_samples_per_second': 263.454, 'eval_steps_per_second': 16.598, 'epoch': 0.24}
{'loss': 0.9416, 'grad_norm': 0.25583168864250183, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7608280181884766, 'eval_runtime': 3.8055, 'eval_samples_per_second': 262.778, 'eval_steps_per_second': 16.555, 'epoch': 0.28}
{'loss': 0.9422, 'grad_norm': 0.2866601049900055, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7535160183906555, 'eval_runtime': 3.8071, 'eval_samples_per_second': 262.665, 'eval_steps_per_second': 16.548, 'epoch': 0.32}
{'loss': 0.969, 'grad_norm': 0.6933819055557251, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7607108354568481, 'eval_runtime': 3.8029, 'eval_samples_per_second': 262.957, 'eval_steps_per_second': 16.566, 'epoch': 0.36}
{'loss': 0.938, 'grad_norm': 0.2518483102321625, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7275157570838928, 'eval_runtime': 3.8137, 'eval_samples_per_second': 262.214, 'eval_steps_per_second': 16.52, 'epoch': 0.4}
{'loss': 0.909, 'grad_norm': 0.28590333461761475, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7164282202720642, 'eval_runtime': 3.8137, 'eval_samples_per_second': 262.211, 'eval_steps_per_second': 16.519, 'epoch': 0.44}
{'loss': 0.9268, 'grad_norm': 0.2619863748550415, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7179378271102905, 'eval_runtime': 3.8175, 'eval_samples_per_second': 261.954, 'eval_steps_per_second': 16.503, 'epoch': 0.48}
{'loss': 0.9104, 'grad_norm': 0.2468208223581314, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7025836110115051, 'eval_runtime': 3.7995, 'eval_samples_per_second': 263.194, 'eval_steps_per_second': 16.581, 'epoch': 0.52}
{'loss': 0.9312, 'grad_norm': 0.35029372572898865, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6941378116607666, 'eval_runtime': 3.804, 'eval_samples_per_second': 262.884, 'eval_steps_per_second': 16.562, 'epoch': 0.56}
{'loss': 0.9158, 'grad_norm': 0.3129001259803772, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6905288100242615, 'eval_runtime': 3.8006, 'eval_samples_per_second': 263.116, 'eval_steps_per_second': 16.576, 'epoch': 0.6}
{'loss': 0.8712, 'grad_norm': 0.29227808117866516, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6855655312538147, 'eval_runtime': 3.7968, 'eval_samples_per_second': 263.382, 'eval_steps_per_second': 16.593, 'epoch': 0.64}
{'loss': 0.8302, 'grad_norm': 0.3302222788333893, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6727280020713806, 'eval_runtime': 3.8015, 'eval_samples_per_second': 263.051, 'eval_steps_per_second': 16.572, 'epoch': 0.68}
{'loss': 0.8506, 'grad_norm': 0.3922244906425476, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6734527349472046, 'eval_runtime': 3.8033, 'eval_samples_per_second': 262.932, 'eval_steps_per_second': 16.565, 'epoch': 0.72}
{'loss': 0.8332, 'grad_norm': 0.4341643750667572, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6578768491744995, 'eval_runtime': 3.7954, 'eval_samples_per_second': 263.474, 'eval_steps_per_second': 16.599, 'epoch': 0.76}
{'loss': 0.8577, 'grad_norm': 0.2361135184764862, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6689153909683228, 'eval_runtime': 3.8104, 'eval_samples_per_second': 262.442, 'eval_steps_per_second': 16.534, 'epoch': 0.8}
{'loss': 0.8574, 'grad_norm': 0.35555052757263184, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6507982015609741, 'eval_runtime': 3.8088, 'eval_samples_per_second': 262.548, 'eval_steps_per_second': 16.541, 'epoch': 0.84}
{'loss': 0.8294, 'grad_norm': 0.29566678404808044, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6398690342903137, 'eval_runtime': 3.8018, 'eval_samples_per_second': 263.036, 'eval_steps_per_second': 16.571, 'epoch': 0.88}
{'loss': 0.7865, 'grad_norm': 0.4902401268482208, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6426651477813721, 'eval_runtime': 3.798, 'eval_samples_per_second': 263.298, 'eval_steps_per_second': 16.588, 'epoch': 0.92}
{'loss': 0.8097, 'grad_norm': 0.4983108341693878, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6384060382843018, 'eval_runtime': 3.8045, 'eval_samples_per_second': 262.846, 'eval_steps_per_second': 16.559, 'epoch': 0.96}
{'loss': 0.8099, 'grad_norm': 0.5587469935417175, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6377667188644409, 'eval_runtime': 3.8022, 'eval_samples_per_second': 263.008, 'eval_steps_per_second': 16.569, 'epoch': 1.0}
{'train_runtime': 322.0724, 'train_samples_per_second': 31.036, 'train_steps_per_second': 1.941, 'train_loss': 1.0018901153564452, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6229969263076782, 1.0162092447280884, 0.9562476873397827, 0.8427256941795349, 0.8172823190689087, 0.8066696524620056, 0.7608280181884766, 0.7535160183906555, 0.7607108354568481, 0.7275157570838928, 0.7164282202720642, 0.7179378271102905, 0.7025836110115051, 0.6941378116607666, 0.6905288100242615, 0.6855655312538147, 0.6727280020713806, 0.6734527349472046, 0.6578768491744995, 0.6689153909683228, 0.6507982015609741, 0.6398690342903137, 0.6426651477813721, 0.6384060382843018, 0.6377667188644409], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6229969263076782, 1.0162092447280884, 0.9562476873397827, 0.8427256941795349, 0.8172823190689087, 0.8066696524620056, 0.7608280181884766, 0.7535160183906555, 0.7607108354568481, 0.7275157570838928, 0.7164282202720642, 0.7179378271102905, 0.7025836110115051, 0.6941378116607666, 0.6905288100242615, 0.6855655312538147, 0.6727280020713806, 0.6734527349472046, 0.6578768491744995, 0.6689153909683228, 0.6507982015609741, 0.6398690342903137, 0.6426651477813721, 0.6384060382843018, 0.6377667188644409]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1249819993972778
current iteration best possible eval_loss (full train run):  -0.6377667188644409
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.7200 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -1.0559577180968
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -1.0636106646435588
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -1.06191042444341
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -1.0619104244433053
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -1.0619118704235526
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2612, dtype=torch.float64), tensor(0.0717, dtype=torch.float64), tensor(0.0337, dtype=torch.float64), tensor(0.0760, dtype=torch.float64), tensor(0.5574, dtype=torch.float64), 0, 0, 0, 17, 0, 1, 0, 1, 1, 128, 0.029528209240282818, 30.52929371085512, 1]
normalized proposed parameters for next round by BO: [tensor(2.4655e-18, dtype=torch.float64), tensor(0.2612, dtype=torch.float64), tensor(0.0717, dtype=torch.float64), tensor(0.0337, dtype=torch.float64), tensor(0.0760, dtype=torch.float64), tensor(0.5574, dtype=torch.float64), tensor(9.6455e-19, dtype=torch.float64), tensor(2.3609e-18, dtype=torch.float64), tensor(3.1841e-18, dtype=torch.float64), tensor(0.5376, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2953, dtype=torch.float64), tensor(0.6360, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.261
  rowan_hellaswag: 0.072
  sciq: 0.034
  triviaqa: 0.076
  truthfulqa_gen: 0.557
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.029528209240282818,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (30.52929371085512,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.029528209240282818
lora alpha:  30.52929371085512
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 91,357,184 || all params: 8,121,618,432 || trainable%: 1.1249
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0295, 'grad_norm': 0.7070333957672119, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4899771213531494, 'eval_runtime': 3.3956, 'eval_samples_per_second': 294.495, 'eval_steps_per_second': 18.553, 'epoch': 0.04}
{'loss': 1.3665, 'grad_norm': 0.366864413022995, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9284393787384033, 'eval_runtime': 3.406, 'eval_samples_per_second': 293.6, 'eval_steps_per_second': 18.497, 'epoch': 0.08}
{'loss': 1.0869, 'grad_norm': 0.2774263620376587, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8320671319961548, 'eval_runtime': 3.4244, 'eval_samples_per_second': 292.022, 'eval_steps_per_second': 18.397, 'epoch': 0.12}
{'loss': 0.9949, 'grad_norm': 0.3298017978668213, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7197682857513428, 'eval_runtime': 3.4058, 'eval_samples_per_second': 293.614, 'eval_steps_per_second': 18.498, 'epoch': 0.16}
{'loss': 0.9757, 'grad_norm': 0.32435521483421326, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6437605023384094, 'eval_runtime': 3.4056, 'eval_samples_per_second': 293.63, 'eval_steps_per_second': 18.499, 'epoch': 0.2}
{'loss': 1.0075, 'grad_norm': 0.28777599334716797, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6044655442237854, 'eval_runtime': 3.4174, 'eval_samples_per_second': 292.623, 'eval_steps_per_second': 18.435, 'epoch': 0.24}
{'loss': 0.8993, 'grad_norm': 0.2311922013759613, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5656479597091675, 'eval_runtime': 3.4188, 'eval_samples_per_second': 292.505, 'eval_steps_per_second': 18.428, 'epoch': 0.28}
{'loss': 0.9456, 'grad_norm': 0.20935258269309998, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5426539182662964, 'eval_runtime': 3.4183, 'eval_samples_per_second': 292.543, 'eval_steps_per_second': 18.43, 'epoch': 0.32}
{'loss': 0.9009, 'grad_norm': 0.2384568750858307, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5057578086853027, 'eval_runtime': 3.4248, 'eval_samples_per_second': 291.984, 'eval_steps_per_second': 18.395, 'epoch': 0.36}
{'loss': 0.8734, 'grad_norm': 0.2065902054309845, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.47674688696861267, 'eval_runtime': 3.4181, 'eval_samples_per_second': 292.564, 'eval_steps_per_second': 18.432, 'epoch': 0.4}
{'loss': 0.8724, 'grad_norm': 0.2863560616970062, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.4270426630973816, 'eval_runtime': 3.4234, 'eval_samples_per_second': 292.109, 'eval_steps_per_second': 18.403, 'epoch': 0.44}
{'loss': 0.8823, 'grad_norm': 0.19009196758270264, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4053479731082916, 'eval_runtime': 3.4216, 'eval_samples_per_second': 292.263, 'eval_steps_per_second': 18.413, 'epoch': 0.48}
{'loss': 0.8237, 'grad_norm': 0.27848777174949646, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3903208076953888, 'eval_runtime': 3.4319, 'eval_samples_per_second': 291.382, 'eval_steps_per_second': 18.357, 'epoch': 0.52}
{'loss': 0.8457, 'grad_norm': 0.317781001329422, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.35863593220710754, 'eval_runtime': 3.4225, 'eval_samples_per_second': 292.184, 'eval_steps_per_second': 18.408, 'epoch': 0.56}
{'loss': 0.852, 'grad_norm': 0.26019227504730225, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.337455153465271, 'eval_runtime': 3.423, 'eval_samples_per_second': 292.14, 'eval_steps_per_second': 18.405, 'epoch': 0.6}
{'loss': 0.8362, 'grad_norm': 0.33235013484954834, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.3159853518009186, 'eval_runtime': 3.4402, 'eval_samples_per_second': 290.679, 'eval_steps_per_second': 18.313, 'epoch': 0.64}
{'loss': 0.7736, 'grad_norm': 0.2602136731147766, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2871400713920593, 'eval_runtime': 3.441, 'eval_samples_per_second': 290.614, 'eval_steps_per_second': 18.309, 'epoch': 0.68}
{'loss': 0.8011, 'grad_norm': 0.26098084449768066, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.2691076099872589, 'eval_runtime': 3.4368, 'eval_samples_per_second': 290.97, 'eval_steps_per_second': 18.331, 'epoch': 0.72}
{'loss': 0.7601, 'grad_norm': 0.2614925503730774, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2566573917865753, 'eval_runtime': 3.424, 'eval_samples_per_second': 292.053, 'eval_steps_per_second': 18.399, 'epoch': 0.76}
{'loss': 0.7594, 'grad_norm': 0.2510565519332886, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.24311886727809906, 'eval_runtime': 3.424, 'eval_samples_per_second': 292.057, 'eval_steps_per_second': 18.4, 'epoch': 0.8}
{'loss': 0.7614, 'grad_norm': 0.2041548192501068, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2319277822971344, 'eval_runtime': 3.4275, 'eval_samples_per_second': 291.761, 'eval_steps_per_second': 18.381, 'epoch': 0.84}
{'loss': 0.7766, 'grad_norm': 0.2711876332759857, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.22134929895401, 'eval_runtime': 3.426, 'eval_samples_per_second': 291.882, 'eval_steps_per_second': 18.389, 'epoch': 0.88}
{'loss': 0.7494, 'grad_norm': 0.2511221468448639, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.21449390053749084, 'eval_runtime': 3.4338, 'eval_samples_per_second': 291.221, 'eval_steps_per_second': 18.347, 'epoch': 0.92}
{'loss': 0.7487, 'grad_norm': 0.1802409142255783, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2060217708349228, 'eval_runtime': 3.4263, 'eval_samples_per_second': 291.858, 'eval_steps_per_second': 18.387, 'epoch': 0.96}
{'loss': 0.8035, 'grad_norm': 0.2200336456298828, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.20387734472751617, 'eval_runtime': 3.4274, 'eval_samples_per_second': 291.763, 'eval_steps_per_second': 18.381, 'epoch': 1.0}
{'train_runtime': 295.5388, 'train_samples_per_second': 33.83, 'train_steps_per_second': 2.115, 'train_loss': 0.9650498413085937, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4899771213531494, 0.9284393787384033, 0.8320671319961548, 0.7197682857513428, 0.6437605023384094, 0.6044655442237854, 0.5656479597091675, 0.5426539182662964, 0.5057578086853027, 0.47674688696861267, 0.4270426630973816, 0.4053479731082916, 0.3903208076953888, 0.35863593220710754, 0.337455153465271, 0.3159853518009186, 0.2871400713920593, 0.2691076099872589, 0.2566573917865753, 0.24311886727809906, 0.2319277822971344, 0.22134929895401, 0.21449390053749084, 0.2060217708349228, 0.20387734472751617], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4899771213531494, 0.9284393787384033, 0.8320671319961548, 0.7197682857513428, 0.6437605023384094, 0.6044655442237854, 0.5656479597091675, 0.5426539182662964, 0.5057578086853027, 0.47674688696861267, 0.4270426630973816, 0.4053479731082916, 0.3903208076953888, 0.35863593220710754, 0.337455153465271, 0.3159853518009186, 0.2871400713920593, 0.2691076099872589, 0.2566573917865753, 0.24311886727809906, 0.2319277822971344, 0.22134929895401, 0.21449390053749084, 0.2060217708349228, 0.20387734472751617]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.20387734472751617
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.5555 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -1.0664692128436672
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -1.0666237553299762
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -1.0664697369594416
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -1.0664692128436672
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -1.0927681071951088
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1815, dtype=torch.float64), tensor(0.0247, dtype=torch.float64), tensor(0.0696, dtype=torch.float64), tensor(0.3074, dtype=torch.float64), 0, tensor(0.2305, dtype=torch.float64), 0, 0, tensor(0.1834, dtype=torch.float64), 14, 0, 1, 0, 1, 1, 42, 0.003087319466877446, 34.08581533436097, 1]
normalized proposed parameters for next round by BO: [tensor(0.1815, dtype=torch.float64), tensor(0.0247, dtype=torch.float64), tensor(0.0696, dtype=torch.float64), tensor(0.3074, dtype=torch.float64), tensor(0.0028, dtype=torch.float64), tensor(0.2305, dtype=torch.float64), tensor(5.3659e-21, dtype=torch.float64), tensor(2.7962e-18, dtype=torch.float64), tensor(0.1834, dtype=torch.float64), tensor(0.4262, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3295, dtype=torch.float64), tensor(0.0309, dtype=torch.float64), tensor(0.7101, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.182
  gsm8k: 0.025
  rowan_hellaswag: 0.07
  sciq: 0.307
  triviaqa: 0
  truthfulqa_gen: 0.23
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.183

LoRA Parameters:
  lora_r: (42,)
  lora_dropout: (0.003087319466877446,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (34.08581533436097,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  42
lora dropout:  0.003087319466877446
lora alpha:  34.08581533436097
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 24,686,592 || all params: 8,054,947,840 || trainable%: 0.3065
length of training data:  9969
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4222, 'grad_norm': 2.077544689178467, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6105037927627563, 'eval_runtime': 3.3641, 'eval_samples_per_second': 297.259, 'eval_steps_per_second': 18.727, 'epoch': 0.04}
{'loss': 1.4852, 'grad_norm': 0.6860864162445068, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.024410367012024, 'eval_runtime': 3.3648, 'eval_samples_per_second': 297.196, 'eval_steps_per_second': 18.723, 'epoch': 0.08}
{'loss': 1.127, 'grad_norm': 0.45861655473709106, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.8955426812171936, 'eval_runtime': 3.3581, 'eval_samples_per_second': 297.788, 'eval_steps_per_second': 18.761, 'epoch': 0.12}
{'loss': 1.0711, 'grad_norm': 0.6143309473991394, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.8251703977584839, 'eval_runtime': 3.3643, 'eval_samples_per_second': 297.239, 'eval_steps_per_second': 18.726, 'epoch': 0.16}
{'loss': 1.027, 'grad_norm': 0.5841630697250366, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.772462010383606, 'eval_runtime': 3.3683, 'eval_samples_per_second': 296.89, 'eval_steps_per_second': 18.704, 'epoch': 0.2}
{'loss': 0.9992, 'grad_norm': 0.38581302762031555, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.7658939957618713, 'eval_runtime': 3.3764, 'eval_samples_per_second': 296.169, 'eval_steps_per_second': 18.659, 'epoch': 0.24}
{'loss': 0.9872, 'grad_norm': 0.4751585125923157, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.7467046976089478, 'eval_runtime': 3.3834, 'eval_samples_per_second': 295.556, 'eval_steps_per_second': 18.62, 'epoch': 0.28}
{'loss': 1.0009, 'grad_norm': 0.7704126834869385, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.7128943204879761, 'eval_runtime': 3.3802, 'eval_samples_per_second': 295.84, 'eval_steps_per_second': 18.638, 'epoch': 0.32}
{'loss': 0.9846, 'grad_norm': 0.4525282680988312, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.6877458095550537, 'eval_runtime': 3.3826, 'eval_samples_per_second': 295.634, 'eval_steps_per_second': 18.625, 'epoch': 0.36}
{'loss': 0.9631, 'grad_norm': 0.4520926773548126, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.6841703653335571, 'eval_runtime': 3.3864, 'eval_samples_per_second': 295.296, 'eval_steps_per_second': 18.604, 'epoch': 0.4}
{'loss': 0.9692, 'grad_norm': 0.3514339327812195, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.6667031049728394, 'eval_runtime': 3.3838, 'eval_samples_per_second': 295.524, 'eval_steps_per_second': 18.618, 'epoch': 0.44}
{'loss': 0.9477, 'grad_norm': 0.46009525656700134, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.6531424522399902, 'eval_runtime': 3.3881, 'eval_samples_per_second': 295.147, 'eval_steps_per_second': 18.594, 'epoch': 0.48}
{'loss': 0.9124, 'grad_norm': 0.45610517263412476, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.6534901261329651, 'eval_runtime': 3.3899, 'eval_samples_per_second': 294.994, 'eval_steps_per_second': 18.585, 'epoch': 0.52}
{'loss': 0.9599, 'grad_norm': 0.3482326865196228, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.6321009397506714, 'eval_runtime': 3.3854, 'eval_samples_per_second': 295.389, 'eval_steps_per_second': 18.61, 'epoch': 0.56}
{'loss': 0.918, 'grad_norm': 0.40559694170951843, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.5996502637863159, 'eval_runtime': 3.3967, 'eval_samples_per_second': 294.4, 'eval_steps_per_second': 18.547, 'epoch': 0.6}
{'loss': 1.0133, 'grad_norm': 0.458878755569458, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.5830539464950562, 'eval_runtime': 3.3999, 'eval_samples_per_second': 294.128, 'eval_steps_per_second': 18.53, 'epoch': 0.64}
{'loss': 0.9728, 'grad_norm': 0.4057084918022156, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.5748377442359924, 'eval_runtime': 3.3955, 'eval_samples_per_second': 294.512, 'eval_steps_per_second': 18.554, 'epoch': 0.68}
{'loss': 0.9278, 'grad_norm': 0.4360564053058624, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.5562170743942261, 'eval_runtime': 3.3929, 'eval_samples_per_second': 294.737, 'eval_steps_per_second': 18.568, 'epoch': 0.72}
{'loss': 0.9269, 'grad_norm': 0.4681251049041748, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.5505187511444092, 'eval_runtime': 3.3954, 'eval_samples_per_second': 294.515, 'eval_steps_per_second': 18.554, 'epoch': 0.76}
{'loss': 0.896, 'grad_norm': 0.5433309078216553, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.5355561375617981, 'eval_runtime': 3.3946, 'eval_samples_per_second': 294.588, 'eval_steps_per_second': 18.559, 'epoch': 0.8}
{'loss': 0.9445, 'grad_norm': 0.40762653946876526, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.5284256339073181, 'eval_runtime': 3.3913, 'eval_samples_per_second': 294.87, 'eval_steps_per_second': 18.577, 'epoch': 0.84}
{'loss': 0.9146, 'grad_norm': 0.42409488558769226, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.5175257325172424, 'eval_runtime': 3.3928, 'eval_samples_per_second': 294.738, 'eval_steps_per_second': 18.568, 'epoch': 0.88}
{'loss': 0.8951, 'grad_norm': 0.4192006587982178, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.5023469924926758, 'eval_runtime': 3.4127, 'eval_samples_per_second': 293.025, 'eval_steps_per_second': 18.461, 'epoch': 0.92}
{'loss': 0.9181, 'grad_norm': 0.3624390959739685, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.4983088970184326, 'eval_runtime': 3.3873, 'eval_samples_per_second': 295.216, 'eval_steps_per_second': 18.599, 'epoch': 0.96}
{'train_runtime': 263.4894, 'train_samples_per_second': 37.835, 'train_steps_per_second': 2.368, 'train_loss': 1.0832475576645288, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6105037927627563, 1.024410367012024, 0.8955426812171936, 0.8251703977584839, 0.772462010383606, 0.7658939957618713, 0.7467046976089478, 0.7128943204879761, 0.6877458095550537, 0.6841703653335571, 0.6667031049728394, 0.6531424522399902, 0.6534901261329651, 0.6321009397506714, 0.5996502637863159, 0.5830539464950562, 0.5748377442359924, 0.5562170743942261, 0.5505187511444092, 0.5355561375617981, 0.5284256339073181, 0.5175257325172424, 0.5023469924926758, 0.4983088970184326], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.6105037927627563, 1.024410367012024, 0.8955426812171936, 0.8251703977584839, 0.772462010383606, 0.7658939957618713, 0.7467046976089478, 0.7128943204879761, 0.6877458095550537, 0.6841703653335571, 0.6667031049728394, 0.6531424522399902, 0.6534901261329651, 0.6321009397506714, 0.5996502637863159, 0.5830539464950562, 0.5748377442359924, 0.5562170743942261, 0.5505187511444092, 0.5355561375617981, 0.5284256339073181, 0.5175257325172424, 0.5023469924926758, 0.4983088970184326]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1276923418045044
current iteration best possible eval_loss (full train run):  -0.4983088970184326
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778, -1.1277821063995361, -1.1276923418045044]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.5005 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7418680787086487, 0.34305238723754883, 0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.24965649843215942, 0.4866626262664795, 0.6463397145271301, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.6175775527954102, 0.9655588865280151, 0.30760660767555237, 0.14850598573684692]  ‚Üí  acq = -1.0616756519802837
X = [0.5951085090637207, 0.6337894201278687, 0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.14596298336982727, 0.1414751410484314, 0.14421969652175903, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2858731746673584, 0.8522514700889587, 0.9279651641845703, 0.5002951622009277]  ‚Üí  acq = -1.060382028265296
X = [0.5818041563034058, 0.9915117621421814, 0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.12225708365440369, 0.8182916045188904, 0.2420574426651001, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6762534976005554, 0.9445821046829224, 0.28039222955703735, 0.026326775550842285]  ‚Üí  acq = -1.0603820282657943
X = [0.375633180141449, 0.817596435546875, 0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9535425305366516, 0.6659542918205261, 0.4328734874725342, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6761766076087952, 0.7598890066146851, 0.8752217292785645, 0.10180890560150146]  ‚Üí  acq = -1.0328696442809722
X = [0.13599658012390137, 0.6423792243003845, 0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9463992118835449, 0.9866945743560791, 0.903969407081604, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.46579673886299133, 0.231636643409729, 0.8483097553253174, 0.5441073775291443]  ‚Üí  acq = -1.0603820282652954
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0657, dtype=torch.float64), tensor(0.0638, dtype=torch.float64), 0, tensor(0.2334, dtype=torch.float64), tensor(0.0584, dtype=torch.float64), tensor(0.3366, dtype=torch.float64), tensor(0.0656, dtype=torch.float64), tensor(0.0854, dtype=torch.float64), tensor(0.0911, dtype=torch.float64), 9, 0, 1, 0, 1, 1, 50, 0.009617390437377265, 28.52139335036519, 1]
normalized proposed parameters for next round by BO: [tensor(0.0657, dtype=torch.float64), tensor(0.0638, dtype=torch.float64), tensor(1.6668e-18, dtype=torch.float64), tensor(0.2334, dtype=torch.float64), tensor(0.0584, dtype=torch.float64), tensor(0.3366, dtype=torch.float64), tensor(0.0656, dtype=torch.float64), tensor(0.0854, dtype=torch.float64), tensor(0.0911, dtype=torch.float64), tensor(0.2963, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3941, dtype=torch.float64), tensor(0.0962, dtype=torch.float64), tensor(0.5942, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.066
  gsm8k: 0.064
  rowan_hellaswag: 0
  sciq: 0.233
  triviaqa: 0.058
  truthfulqa_gen: 0.337
  wikitext: 0.066
  mmlu: 0.085
  arc_challenge: 0.091

LoRA Parameters:
  lora_r: (50,)
  lora_dropout: (0.009617390437377265,)
  num_layers_to_apply: (9,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (28.52139335036519,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  9
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  50
lora dropout:  0.009617390437377265
lora alpha:  28.52139335036519
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 18,892,800 || all params: 8,049,154,048 || trainable%: 0.2347
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4828, 'grad_norm': 2.0424697399139404, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0427050590515137, 'eval_runtime': 3.2555, 'eval_samples_per_second': 307.17, 'eval_steps_per_second': 19.352, 'epoch': 0.04}
{'loss': 1.5827, 'grad_norm': 1.4943475723266602, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0204737186431885, 'eval_runtime': 3.2577, 'eval_samples_per_second': 306.964, 'eval_steps_per_second': 19.339, 'epoch': 0.08}
{'loss': 1.223, 'grad_norm': 0.6547184586524963, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9183493852615356, 'eval_runtime': 3.2582, 'eval_samples_per_second': 306.921, 'eval_steps_per_second': 19.336, 'epoch': 0.12}
{'loss': 1.1287, 'grad_norm': 0.6242235898971558, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8175116181373596, 'eval_runtime': 3.2623, 'eval_samples_per_second': 306.529, 'eval_steps_per_second': 19.311, 'epoch': 0.16}
{'loss': 1.0697, 'grad_norm': 0.7009077072143555, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7756397128105164, 'eval_runtime': 3.2755, 'eval_samples_per_second': 305.3, 'eval_steps_per_second': 19.234, 'epoch': 0.2}
{'loss': 1.0044, 'grad_norm': 0.5445554852485657, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7515534162521362, 'eval_runtime': 3.2906, 'eval_samples_per_second': 303.892, 'eval_steps_per_second': 19.145, 'epoch': 0.24}
{'loss': 0.9465, 'grad_norm': 0.3860110342502594, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7268632650375366, 'eval_runtime': 3.3012, 'eval_samples_per_second': 302.922, 'eval_steps_per_second': 19.084, 'epoch': 0.28}
{'loss': 0.9692, 'grad_norm': 0.3681512176990509, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7154716849327087, 'eval_runtime': 3.3133, 'eval_samples_per_second': 301.811, 'eval_steps_per_second': 19.014, 'epoch': 0.32}
{'loss': 1.0647, 'grad_norm': 0.7825015187263489, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.707422137260437, 'eval_runtime': 3.2845, 'eval_samples_per_second': 304.464, 'eval_steps_per_second': 19.181, 'epoch': 0.36}
{'loss': 0.9982, 'grad_norm': 0.39225006103515625, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6888418793678284, 'eval_runtime': 3.2821, 'eval_samples_per_second': 304.68, 'eval_steps_per_second': 19.195, 'epoch': 0.4}
{'loss': 0.9776, 'grad_norm': 0.44620606303215027, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6747209429740906, 'eval_runtime': 3.2828, 'eval_samples_per_second': 304.619, 'eval_steps_per_second': 19.191, 'epoch': 0.44}
{'loss': 0.9709, 'grad_norm': 0.40045076608657837, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6659923195838928, 'eval_runtime': 3.2842, 'eval_samples_per_second': 304.492, 'eval_steps_per_second': 19.183, 'epoch': 0.48}
{'loss': 0.9546, 'grad_norm': 0.46123823523521423, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6608325839042664, 'eval_runtime': 3.2777, 'eval_samples_per_second': 305.094, 'eval_steps_per_second': 19.221, 'epoch': 0.52}
{'loss': 1.0493, 'grad_norm': 0.6178484559059143, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.648378312587738, 'eval_runtime': 3.285, 'eval_samples_per_second': 304.414, 'eval_steps_per_second': 19.178, 'epoch': 0.56}
{'loss': 0.9652, 'grad_norm': 1.201886773109436, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6271716356277466, 'eval_runtime': 3.2809, 'eval_samples_per_second': 304.791, 'eval_steps_per_second': 19.202, 'epoch': 0.6}
{'loss': 0.936, 'grad_norm': 0.40919065475463867, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6199643611907959, 'eval_runtime': 3.2805, 'eval_samples_per_second': 304.827, 'eval_steps_per_second': 19.204, 'epoch': 0.64}
{'loss': 0.9265, 'grad_norm': 0.4295068681240082, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6018844842910767, 'eval_runtime': 3.2822, 'eval_samples_per_second': 304.678, 'eval_steps_per_second': 19.195, 'epoch': 0.68}
{'loss': 0.9467, 'grad_norm': 0.3980000913143158, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5877730250358582, 'eval_runtime': 3.2847, 'eval_samples_per_second': 304.444, 'eval_steps_per_second': 19.18, 'epoch': 0.72}
{'loss': 0.8786, 'grad_norm': 0.7297804355621338, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5805466175079346, 'eval_runtime': 3.2906, 'eval_samples_per_second': 303.899, 'eval_steps_per_second': 19.146, 'epoch': 0.76}
{'loss': 0.9123, 'grad_norm': 0.45955121517181396, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5781846046447754, 'eval_runtime': 3.2794, 'eval_samples_per_second': 304.938, 'eval_steps_per_second': 19.211, 'epoch': 0.8}
{'loss': 0.8811, 'grad_norm': 0.6059137582778931, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5457966327667236, 'eval_runtime': 3.2831, 'eval_samples_per_second': 304.593, 'eval_steps_per_second': 19.189, 'epoch': 0.84}
{'loss': 0.98, 'grad_norm': 0.5620477795600891, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5439285635948181, 'eval_runtime': 3.283, 'eval_samples_per_second': 304.602, 'eval_steps_per_second': 19.19, 'epoch': 0.88}
{'loss': 0.8748, 'grad_norm': 0.40800634026527405, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5344739556312561, 'eval_runtime': 3.2902, 'eval_samples_per_second': 303.934, 'eval_steps_per_second': 19.148, 'epoch': 0.92}
{'loss': 0.8666, 'grad_norm': 0.47317424416542053, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.527199387550354, 'eval_runtime': 3.2969, 'eval_samples_per_second': 303.313, 'eval_steps_per_second': 19.109, 'epoch': 0.96}
{'loss': 0.9101, 'grad_norm': 0.473736971616745, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5253576636314392, 'eval_runtime': 3.3093, 'eval_samples_per_second': 302.179, 'eval_steps_per_second': 19.037, 'epoch': 1.0}
{'train_runtime': 250.8087, 'train_samples_per_second': 39.859, 'train_steps_per_second': 2.492, 'train_loss': 1.1000020660400391, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0427050590515137, 1.0204737186431885, 0.9183493852615356, 0.8175116181373596, 0.7756397128105164, 0.7515534162521362, 0.7268632650375366, 0.7154716849327087, 0.707422137260437, 0.6888418793678284, 0.6747209429740906, 0.6659923195838928, 0.6608325839042664, 0.648378312587738, 0.6271716356277466, 0.6199643611907959, 0.6018844842910767, 0.5877730250358582, 0.5805466175079346, 0.5781846046447754, 0.5457966327667236, 0.5439285635948181, 0.5344739556312561, 0.527199387550354, 0.5253576636314392], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0427050590515137, 1.0204737186431885, 0.9183493852615356, 0.8175116181373596, 0.7756397128105164, 0.7515534162521362, 0.7268632650375366, 0.7154716849327087, 0.707422137260437, 0.6888418793678284, 0.6747209429740906, 0.6659923195838928, 0.6608325839042664, 0.648378312587738, 0.6271716356277466, 0.6199643611907959, 0.6018844842910767, 0.5877730250358582, 0.5805466175079346, 0.5781846046447754, 0.5457966327667236, 0.5439285635948181, 0.5344739556312561, 0.527199387550354, 0.5253576636314392]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1273939609527588
current iteration best possible eval_loss (full train run):  -0.5253576636314392
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778, -1.1277821063995361, -1.1276923418045044, -1.1273939609527588]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.1952 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -1.0648722657777998
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -1.0648722611790984
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -1.0648722611790984
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -1.0653600437488147
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -1.065615947456593
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1784, dtype=torch.float64), tensor(0.0705, dtype=torch.float64), tensor(0.2945, dtype=torch.float64), 0, tensor(0.1771, dtype=torch.float64), 0, tensor(0.0898, dtype=torch.float64), tensor(0.1898, dtype=torch.float64), 16, 0, 1, 0, 1, 1, 43, 1.852789555777943e-18, 35.89486233293965, 1]
normalized proposed parameters for next round by BO: [tensor(4.7957e-18, dtype=torch.float64), tensor(0.1784, dtype=torch.float64), tensor(0.0705, dtype=torch.float64), tensor(0.2945, dtype=torch.float64), tensor(1.9754e-18, dtype=torch.float64), tensor(0.1771, dtype=torch.float64), tensor(2.0372e-18, dtype=torch.float64), tensor(0.0898, dtype=torch.float64), tensor(0.1898, dtype=torch.float64), tensor(0.5149, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3322, dtype=torch.float64), tensor(1.8528e-17, dtype=torch.float64), tensor(0.7478, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.178
  rowan_hellaswag: 0.071
  sciq: 0.294
  triviaqa: 0
  truthfulqa_gen: 0.177
  wikitext: 0
  mmlu: 0.09
  arc_challenge: 0.19

LoRA Parameters:
  lora_r: (43,)
  lora_dropout: (1.852789555777943e-18,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (35.89486233293965,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  43
lora dropout:  1.852789555777943e-18
lora alpha:  35.89486233293965
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 28,884,992 || all params: 8,059,146,240 || trainable%: 0.3584
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8833, 'grad_norm': 0.875725269317627, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5825812816619873, 'eval_runtime': 3.4752, 'eval_samples_per_second': 287.757, 'eval_steps_per_second': 18.129, 'epoch': 0.04}
{'loss': 1.4212, 'grad_norm': 0.7663745880126953, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0135563611984253, 'eval_runtime': 3.4732, 'eval_samples_per_second': 287.918, 'eval_steps_per_second': 18.139, 'epoch': 0.08}
{'loss': 1.1111, 'grad_norm': 0.4893481731414795, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8957225680351257, 'eval_runtime': 3.4811, 'eval_samples_per_second': 287.267, 'eval_steps_per_second': 18.098, 'epoch': 0.12}
{'loss': 1.0666, 'grad_norm': 0.44271111488342285, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8015267848968506, 'eval_runtime': 3.476, 'eval_samples_per_second': 287.689, 'eval_steps_per_second': 18.124, 'epoch': 0.16}
{'loss': 1.046, 'grad_norm': 0.4208453297615051, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7881672978401184, 'eval_runtime': 3.4612, 'eval_samples_per_second': 288.913, 'eval_steps_per_second': 18.202, 'epoch': 0.2}
{'loss': 1.0411, 'grad_norm': 0.3927481770515442, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7502092719078064, 'eval_runtime': 3.4674, 'eval_samples_per_second': 288.404, 'eval_steps_per_second': 18.169, 'epoch': 0.24}
{'loss': 1.0008, 'grad_norm': 0.37041452527046204, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7352234125137329, 'eval_runtime': 3.4685, 'eval_samples_per_second': 288.311, 'eval_steps_per_second': 18.164, 'epoch': 0.28}
{'loss': 0.996, 'grad_norm': 0.39321368932724, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7373629808425903, 'eval_runtime': 3.4762, 'eval_samples_per_second': 287.67, 'eval_steps_per_second': 18.123, 'epoch': 0.32}
{'loss': 1.0441, 'grad_norm': 0.4656032919883728, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.699881911277771, 'eval_runtime': 3.4808, 'eval_samples_per_second': 287.289, 'eval_steps_per_second': 18.099, 'epoch': 0.36}
{'loss': 0.9394, 'grad_norm': 0.3853438198566437, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6785273551940918, 'eval_runtime': 3.4776, 'eval_samples_per_second': 287.556, 'eval_steps_per_second': 18.116, 'epoch': 0.4}
{'loss': 0.9816, 'grad_norm': 0.4098576009273529, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6735014915466309, 'eval_runtime': 3.5014, 'eval_samples_per_second': 285.599, 'eval_steps_per_second': 17.993, 'epoch': 0.44}
{'loss': 0.9818, 'grad_norm': 0.3883444666862488, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6488457918167114, 'eval_runtime': 3.4945, 'eval_samples_per_second': 286.167, 'eval_steps_per_second': 18.028, 'epoch': 0.48}
{'loss': 0.9837, 'grad_norm': 0.3745484948158264, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6495477557182312, 'eval_runtime': 3.4946, 'eval_samples_per_second': 286.153, 'eval_steps_per_second': 18.028, 'epoch': 0.52}
{'loss': 1.0006, 'grad_norm': 0.43364641070365906, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6284360885620117, 'eval_runtime': 3.4957, 'eval_samples_per_second': 286.062, 'eval_steps_per_second': 18.022, 'epoch': 0.56}
{'loss': 1.01, 'grad_norm': 0.40434131026268005, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6140307784080505, 'eval_runtime': 3.4952, 'eval_samples_per_second': 286.103, 'eval_steps_per_second': 18.024, 'epoch': 0.6}
{'loss': 0.9959, 'grad_norm': 0.40989065170288086, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5987064838409424, 'eval_runtime': 3.4939, 'eval_samples_per_second': 286.21, 'eval_steps_per_second': 18.031, 'epoch': 0.64}
{'loss': 0.9792, 'grad_norm': 0.42532363533973694, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5996410846710205, 'eval_runtime': 3.4938, 'eval_samples_per_second': 286.224, 'eval_steps_per_second': 18.032, 'epoch': 0.68}
{'loss': 0.9894, 'grad_norm': 0.4506339430809021, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5806177258491516, 'eval_runtime': 3.5083, 'eval_samples_per_second': 285.034, 'eval_steps_per_second': 17.957, 'epoch': 0.72}
{'loss': 0.8857, 'grad_norm': 1.198961853981018, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5704180002212524, 'eval_runtime': 3.5024, 'eval_samples_per_second': 285.523, 'eval_steps_per_second': 17.988, 'epoch': 0.76}
{'loss': 1.0025, 'grad_norm': 0.4616371691226959, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5759605169296265, 'eval_runtime': 3.5035, 'eval_samples_per_second': 285.428, 'eval_steps_per_second': 17.982, 'epoch': 0.8}
{'loss': 0.933, 'grad_norm': 0.43276000022888184, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5513791441917419, 'eval_runtime': 3.5078, 'eval_samples_per_second': 285.081, 'eval_steps_per_second': 17.96, 'epoch': 0.84}
{'loss': 0.9996, 'grad_norm': 0.3859255313873291, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5462590456008911, 'eval_runtime': 3.5059, 'eval_samples_per_second': 285.237, 'eval_steps_per_second': 17.97, 'epoch': 0.88}
{'loss': 0.9395, 'grad_norm': 0.45982250571250916, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5381657481193542, 'eval_runtime': 3.5014, 'eval_samples_per_second': 285.602, 'eval_steps_per_second': 17.993, 'epoch': 0.92}
{'loss': 0.9769, 'grad_norm': 0.34670430421829224, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5361679792404175, 'eval_runtime': 3.4847, 'eval_samples_per_second': 286.969, 'eval_steps_per_second': 18.079, 'epoch': 0.96}
{'loss': 0.9471, 'grad_norm': 0.5024393200874329, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5338330268859863, 'eval_runtime': 3.4921, 'eval_samples_per_second': 286.359, 'eval_steps_per_second': 18.041, 'epoch': 1.0}
{'train_runtime': 300.6734, 'train_samples_per_second': 33.245, 'train_steps_per_second': 2.079, 'train_loss': 1.0862413635253907, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5825812816619873, 1.0135563611984253, 0.8957225680351257, 0.8015267848968506, 0.7881672978401184, 0.7502092719078064, 0.7352234125137329, 0.7373629808425903, 0.699881911277771, 0.6785273551940918, 0.6735014915466309, 0.6488457918167114, 0.6495477557182312, 0.6284360885620117, 0.6140307784080505, 0.5987064838409424, 0.5996410846710205, 0.5806177258491516, 0.5704180002212524, 0.5759605169296265, 0.5513791441917419, 0.5462590456008911, 0.5381657481193542, 0.5361679792404175, 0.5338330268859863], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5825812816619873, 1.0135563611984253, 0.8957225680351257, 0.8015267848968506, 0.7881672978401184, 0.7502092719078064, 0.7352234125137329, 0.7373629808425903, 0.699881911277771, 0.6785273551940918, 0.6735014915466309, 0.6488457918167114, 0.6495477557182312, 0.6284360885620117, 0.6140307784080505, 0.5987064838409424, 0.5996410846710205, 0.5806177258491516, 0.5704180002212524, 0.5759605169296265, 0.5513791441917419, 0.5462590456008911, 0.5381657481193542, 0.5361679792404175, 0.5338330268859863]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1274949312210083
current iteration best possible eval_loss (full train run):  -0.5338330268859863
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778, -1.1277821063995361, -1.1276923418045044, -1.1273939609527588, -1.1274949312210083]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.4068 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -1.072937551794157
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -1.0730505733325455
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -1.0819537417126848
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -1.0729375517331734
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -1.0729375517331365
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1828, dtype=torch.float64), tensor(0.3281, dtype=torch.float64), 0, 0, tensor(0.0908, dtype=torch.float64), tensor(0.0374, dtype=torch.float64), tensor(0.0257, dtype=torch.float64), tensor(0.0970, dtype=torch.float64), tensor(0.2334, dtype=torch.float64), 5, 1, 1, 1, 1, 1, 128, 0.01168445909828972, 42.543152964789385, 1]
normalized proposed parameters for next round by BO: [tensor(0.1828, dtype=torch.float64), tensor(0.3281, dtype=torch.float64), tensor(0.0046, dtype=torch.float64), tensor(2.7105e-20, dtype=torch.float64), tensor(0.0908, dtype=torch.float64), tensor(0.0374, dtype=torch.float64), tensor(0.0257, dtype=torch.float64), tensor(0.0970, dtype=torch.float64), tensor(0.2334, dtype=torch.float64), tensor(0.1687, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1168, dtype=torch.float64), tensor(0.8863, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.183
  gsm8k: 0.328
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.091
  truthfulqa_gen: 0.037
  wikitext: 0.026
  mmlu: 0.097
  arc_challenge: 0.233

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.01168445909828972,)
  num_layers_to_apply: (5,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (42.543152964789385,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  5
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.01168445909828972
lora alpha:  42.543152964789385
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 43,909,120 || all params: 8,074,170,368 || trainable%: 0.5438
length of training data:  9952
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7784, 'grad_norm': 0.8043280243873596, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.305622100830078, 'eval_runtime': 3.2309, 'eval_samples_per_second': 309.515, 'eval_steps_per_second': 19.499, 'epoch': 0.04}
{'loss': 1.4805, 'grad_norm': 0.5506716966629028, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.385448694229126, 'eval_runtime': 3.2554, 'eval_samples_per_second': 307.18, 'eval_steps_per_second': 19.352, 'epoch': 0.08}
{'loss': 1.1376, 'grad_norm': 0.3663599193096161, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.0707111358642578, 'eval_runtime': 3.2505, 'eval_samples_per_second': 307.649, 'eval_steps_per_second': 19.382, 'epoch': 0.12}
{'loss': 1.1112, 'grad_norm': 0.3593319356441498, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.037358045578003, 'eval_runtime': 3.2705, 'eval_samples_per_second': 305.766, 'eval_steps_per_second': 19.263, 'epoch': 0.16}
{'loss': 0.983, 'grad_norm': 0.2664240300655365, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 0.9479321837425232, 'eval_runtime': 3.2847, 'eval_samples_per_second': 304.445, 'eval_steps_per_second': 19.18, 'epoch': 0.2}
{'loss': 1.0004, 'grad_norm': 0.2552022337913513, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 0.9296783208847046, 'eval_runtime': 3.2486, 'eval_samples_per_second': 307.824, 'eval_steps_per_second': 19.393, 'epoch': 0.24}
{'loss': 0.9923, 'grad_norm': 0.8705703616142273, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 0.8777025938034058, 'eval_runtime': 3.2505, 'eval_samples_per_second': 307.647, 'eval_steps_per_second': 19.382, 'epoch': 0.28}
{'loss': 0.9748, 'grad_norm': 0.23702919483184814, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 0.8999518752098083, 'eval_runtime': 3.2503, 'eval_samples_per_second': 307.664, 'eval_steps_per_second': 19.383, 'epoch': 0.32}
{'loss': 1.0099, 'grad_norm': 0.2458650767803192, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 0.8631898760795593, 'eval_runtime': 3.2499, 'eval_samples_per_second': 307.704, 'eval_steps_per_second': 19.385, 'epoch': 0.36}
{'loss': 0.923, 'grad_norm': 0.2354588657617569, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 0.8624538779258728, 'eval_runtime': 3.2526, 'eval_samples_per_second': 307.45, 'eval_steps_per_second': 19.369, 'epoch': 0.4}
{'loss': 0.9715, 'grad_norm': 0.2438371330499649, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 0.8445640802383423, 'eval_runtime': 3.2568, 'eval_samples_per_second': 307.046, 'eval_steps_per_second': 19.344, 'epoch': 0.44}
{'loss': 0.9582, 'grad_norm': 0.22934573888778687, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 0.8800897002220154, 'eval_runtime': 3.2543, 'eval_samples_per_second': 307.282, 'eval_steps_per_second': 19.359, 'epoch': 0.48}
{'loss': 0.9582, 'grad_norm': 0.23858404159545898, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 0.8494259119033813, 'eval_runtime': 3.2603, 'eval_samples_per_second': 306.717, 'eval_steps_per_second': 19.323, 'epoch': 0.52}
{'loss': 0.9034, 'grad_norm': 0.22551888227462769, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 0.8391369581222534, 'eval_runtime': 3.2509, 'eval_samples_per_second': 307.611, 'eval_steps_per_second': 19.38, 'epoch': 0.56}
{'loss': 0.9366, 'grad_norm': 0.19951778650283813, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 0.8242475390434265, 'eval_runtime': 3.2541, 'eval_samples_per_second': 307.306, 'eval_steps_per_second': 19.36, 'epoch': 0.6}
{'loss': 0.9495, 'grad_norm': 0.23876042664051056, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 0.8353114128112793, 'eval_runtime': 3.2549, 'eval_samples_per_second': 307.23, 'eval_steps_per_second': 19.356, 'epoch': 0.64}
{'loss': 0.9446, 'grad_norm': 0.17788821458816528, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 0.8450077176094055, 'eval_runtime': 3.257, 'eval_samples_per_second': 307.031, 'eval_steps_per_second': 19.343, 'epoch': 0.68}
{'loss': 0.9647, 'grad_norm': 0.29330453276634216, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 0.8623262643814087, 'eval_runtime': 3.2553, 'eval_samples_per_second': 307.189, 'eval_steps_per_second': 19.353, 'epoch': 0.72}
{'loss': 0.9548, 'grad_norm': 0.21558259427547455, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 0.8394719362258911, 'eval_runtime': 3.2564, 'eval_samples_per_second': 307.084, 'eval_steps_per_second': 19.346, 'epoch': 0.76}
{'loss': 0.9602, 'grad_norm': 0.2589031159877777, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.8366174101829529, 'eval_runtime': 3.2544, 'eval_samples_per_second': 307.279, 'eval_steps_per_second': 19.359, 'epoch': 0.8}
{'loss': 0.8954, 'grad_norm': 0.268004834651947, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 0.8169845938682556, 'eval_runtime': 3.2604, 'eval_samples_per_second': 306.706, 'eval_steps_per_second': 19.322, 'epoch': 0.84}
{'loss': 0.9568, 'grad_norm': 0.19881312549114227, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.8429160714149475, 'eval_runtime': 3.2499, 'eval_samples_per_second': 307.7, 'eval_steps_per_second': 19.385, 'epoch': 0.88}
{'loss': 0.9386, 'grad_norm': 0.22853122651576996, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.8340502977371216, 'eval_runtime': 3.2538, 'eval_samples_per_second': 307.329, 'eval_steps_per_second': 19.362, 'epoch': 0.92}
{'loss': 0.9044, 'grad_norm': 0.25463274121284485, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.8163672089576721, 'eval_runtime': 3.2593, 'eval_samples_per_second': 306.815, 'eval_steps_per_second': 19.329, 'epoch': 0.96}
{'train_runtime': 274.1961, 'train_samples_per_second': 36.295, 'train_steps_per_second': 2.268, 'train_loss': 1.0611902218539615, 'epoch': 1.0}
train_results:  {'eval_loss': [2.305622100830078, 1.385448694229126, 1.0707111358642578, 1.037358045578003, 0.9479321837425232, 0.9296783208847046, 0.8777025938034058, 0.8999518752098083, 0.8631898760795593, 0.8624538779258728, 0.8445640802383423, 0.8800897002220154, 0.8494259119033813, 0.8391369581222534, 0.8242475390434265, 0.8353114128112793, 0.8450077176094055, 0.8623262643814087, 0.8394719362258911, 0.8366174101829529, 0.8169845938682556, 0.8429160714149475, 0.8340502977371216, 0.8163672089576721], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.305622100830078, 1.385448694229126, 1.0707111358642578, 1.037358045578003, 0.9479321837425232, 0.9296783208847046, 0.8777025938034058, 0.8999518752098083, 0.8631898760795593, 0.8624538779258728, 0.8445640802383423, 0.8800897002220154, 0.8494259119033813, 0.8391369581222534, 0.8242475390434265, 0.8353114128112793, 0.8450077176094055, 0.8623262643814087, 0.8394719362258911, 0.8366174101829529, 0.8169845938682556, 0.8429160714149475, 0.8340502977371216, 0.8163672089576721]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.8163672089576721
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778, -1.1277821063995361, -1.1276923418045044, -1.1273939609527588, -1.1274949312210083, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1344 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.038794755935668945, 0.5530740022659302, 0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.32418566942214966, 0.03446310758590698, 0.8077076077461243, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5978026986122131, 0.24681228399276733, 0.3197188079357147, 0.43591630458831787]  ‚Üí  acq = -1.0754662499641257
X = [0.06298112869262695, 0.4111078381538391, 0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.2800779640674591, 0.1305062174797058, 0.7458388209342957, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2502773702144623, 0.26675117015838623, 0.721631646156311, 0.1087694764137268]  ‚Üí  acq = -1.0754662499641257
X = [0.7721713781356812, 0.36252284049987793, 0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8363065719604492, 0.2568463683128357, 0.6493399143218994, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7879849672317505, 0.044145405292510986, 0.17305481433868408, 0.9866487383842468]  ‚Üí  acq = -1.067370138512545
X = [0.5755511522293091, 0.49270403385162354, 0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2550579309463501, 0.3171401023864746, 0.8667199611663818, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6549527049064636, 0.34602898359298706, 0.037224020808935165, 0.7405623197555542]  ‚Üí  acq = -1.0898236881036358
X = [0.7848239541053772, 0.7650286555290222, 0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.2828661799430847, 0.7089584469795227, 0.5923287272453308, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.9000691771507263, 0.3606852889060974, 0.7490195035934448, 0.591159462928772]  ‚Üí  acq = -1.075466250145002
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0941, dtype=torch.float64), tensor(0.4253, dtype=torch.float64), 0, tensor(0.0396, dtype=torch.float64), 0, tensor(0.0154, dtype=torch.float64), 0, tensor(0.1902, dtype=torch.float64), tensor(0.2304, dtype=torch.float64), 7, 1, 1, 1, 1, 1, 110, 0.022074579542030015, 22.751181326422063, 1]
normalized proposed parameters for next round by BO: [tensor(0.0941, dtype=torch.float64), tensor(0.4253, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0396, dtype=torch.float64), tensor(0.0050, dtype=torch.float64), tensor(0.0154, dtype=torch.float64), tensor(2.6212e-17, dtype=torch.float64), tensor(0.1902, dtype=torch.float64), tensor(0.2304, dtype=torch.float64), tensor(0.2177, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8560, dtype=torch.float64), tensor(0.2207, dtype=torch.float64), tensor(0.4740, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.094
  gsm8k: 0.425
  rowan_hellaswag: 0
  sciq: 0.04
  triviaqa: 0
  truthfulqa_gen: 0.015
  wikitext: 0
  mmlu: 0.19
  arc_challenge: 0.23

LoRA Parameters:
  lora_r: (110,)
  lora_dropout: (0.022074579542030015,)
  num_layers_to_apply: (7,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (22.751181326422063,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  7
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  110
lora dropout:  0.022074579542030015
lora alpha:  22.751181326422063
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 52,828,160 || all params: 8,083,089,408 || trainable%: 0.6536
length of training data:  9947
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6822, 'grad_norm': 0.9479410648345947, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4113266468048096, 'eval_runtime': 3.3395, 'eval_samples_per_second': 299.442, 'eval_steps_per_second': 18.865, 'epoch': 0.04}
{'loss': 1.5083, 'grad_norm': 0.5865222215652466, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3703852891921997, 'eval_runtime': 3.3426, 'eval_samples_per_second': 299.168, 'eval_steps_per_second': 18.848, 'epoch': 0.08}
{'loss': 1.1187, 'grad_norm': 0.23779496550559998, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.1509264707565308, 'eval_runtime': 3.3462, 'eval_samples_per_second': 298.851, 'eval_steps_per_second': 18.828, 'epoch': 0.12}
{'loss': 1.0631, 'grad_norm': 0.376695454120636, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.1347131729125977, 'eval_runtime': 3.361, 'eval_samples_per_second': 297.533, 'eval_steps_per_second': 18.745, 'epoch': 0.16}
{'loss': 1.0085, 'grad_norm': 0.25789833068847656, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.11277174949646, 'eval_runtime': 3.3652, 'eval_samples_per_second': 297.159, 'eval_steps_per_second': 18.721, 'epoch': 0.2}
{'loss': 1.006, 'grad_norm': 0.19346089661121368, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.0331358909606934, 'eval_runtime': 3.3655, 'eval_samples_per_second': 297.133, 'eval_steps_per_second': 18.719, 'epoch': 0.24}
{'loss': 0.9571, 'grad_norm': 0.30212247371673584, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 0.9950640797615051, 'eval_runtime': 3.3681, 'eval_samples_per_second': 296.907, 'eval_steps_per_second': 18.705, 'epoch': 0.28}
{'loss': 0.9628, 'grad_norm': 0.20461928844451904, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 0.9894245862960815, 'eval_runtime': 3.3676, 'eval_samples_per_second': 296.944, 'eval_steps_per_second': 18.707, 'epoch': 0.32}
{'loss': 0.9886, 'grad_norm': 0.2077692151069641, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 0.9996845126152039, 'eval_runtime': 3.374, 'eval_samples_per_second': 296.38, 'eval_steps_per_second': 18.672, 'epoch': 0.36}
{'loss': 0.9786, 'grad_norm': 0.1864139586687088, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 0.9739980697631836, 'eval_runtime': 3.3823, 'eval_samples_per_second': 295.657, 'eval_steps_per_second': 18.626, 'epoch': 0.4}
{'loss': 0.9805, 'grad_norm': 0.19467638432979584, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 0.9416074752807617, 'eval_runtime': 3.3869, 'eval_samples_per_second': 295.255, 'eval_steps_per_second': 18.601, 'epoch': 0.44}
{'loss': 0.9601, 'grad_norm': 0.24480217695236206, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 1.0484453439712524, 'eval_runtime': 3.3976, 'eval_samples_per_second': 294.325, 'eval_steps_per_second': 18.542, 'epoch': 0.48}
{'loss': 0.9421, 'grad_norm': 0.16727693378925323, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 0.9596720933914185, 'eval_runtime': 3.4051, 'eval_samples_per_second': 293.681, 'eval_steps_per_second': 18.502, 'epoch': 0.52}
{'loss': 0.9486, 'grad_norm': 0.15526531636714935, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 0.954775869846344, 'eval_runtime': 3.3796, 'eval_samples_per_second': 295.894, 'eval_steps_per_second': 18.641, 'epoch': 0.56}
{'loss': 0.9558, 'grad_norm': 0.3963775038719177, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 0.9414180517196655, 'eval_runtime': 3.3984, 'eval_samples_per_second': 294.258, 'eval_steps_per_second': 18.538, 'epoch': 0.6}
{'loss': 0.9398, 'grad_norm': 0.17721430957317352, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 0.9248806834220886, 'eval_runtime': 3.3737, 'eval_samples_per_second': 296.408, 'eval_steps_per_second': 18.674, 'epoch': 0.64}
{'loss': 0.9202, 'grad_norm': 0.18162643909454346, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 0.9207578897476196, 'eval_runtime': 3.3744, 'eval_samples_per_second': 296.345, 'eval_steps_per_second': 18.67, 'epoch': 0.68}
{'loss': 0.9392, 'grad_norm': 0.17991213500499725, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 0.9623022675514221, 'eval_runtime': 3.3686, 'eval_samples_per_second': 296.857, 'eval_steps_per_second': 18.702, 'epoch': 0.72}
{'loss': 0.926, 'grad_norm': 0.16828013956546783, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 0.939276933670044, 'eval_runtime': 3.3699, 'eval_samples_per_second': 296.742, 'eval_steps_per_second': 18.695, 'epoch': 0.76}
{'loss': 0.9162, 'grad_norm': 0.21018671989440918, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.9478324055671692, 'eval_runtime': 3.3741, 'eval_samples_per_second': 296.372, 'eval_steps_per_second': 18.671, 'epoch': 0.8}
{'loss': 0.9039, 'grad_norm': 0.19669528305530548, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 0.9445891380310059, 'eval_runtime': 3.3708, 'eval_samples_per_second': 296.669, 'eval_steps_per_second': 18.69, 'epoch': 0.84}
{'loss': 0.9427, 'grad_norm': 0.18581166863441467, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.9333194494247437, 'eval_runtime': 3.3786, 'eval_samples_per_second': 295.98, 'eval_steps_per_second': 18.647, 'epoch': 0.88}
{'loss': 0.9084, 'grad_norm': 0.17104452848434448, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.9238488674163818, 'eval_runtime': 3.3737, 'eval_samples_per_second': 296.414, 'eval_steps_per_second': 18.674, 'epoch': 0.92}
{'loss': 0.9149, 'grad_norm': 0.16674581170082092, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.9191880822181702, 'eval_runtime': 3.3799, 'eval_samples_per_second': 295.865, 'eval_steps_per_second': 18.639, 'epoch': 0.96}
{'train_runtime': 288.9317, 'train_samples_per_second': 34.427, 'train_steps_per_second': 2.153, 'train_loss': 1.0530552143452636, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4113266468048096, 1.3703852891921997, 1.1509264707565308, 1.1347131729125977, 1.11277174949646, 1.0331358909606934, 0.9950640797615051, 0.9894245862960815, 0.9996845126152039, 0.9739980697631836, 0.9416074752807617, 1.0484453439712524, 0.9596720933914185, 0.954775869846344, 0.9414180517196655, 0.9248806834220886, 0.9207578897476196, 0.9623022675514221, 0.939276933670044, 0.9478324055671692, 0.9445891380310059, 0.9333194494247437, 0.9238488674163818, 0.9191880822181702], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.4113266468048096, 1.3703852891921997, 1.1509264707565308, 1.1347131729125977, 1.11277174949646, 1.0331358909606934, 0.9950640797615051, 0.9894245862960815, 0.9996845126152039, 0.9739980697631836, 0.9416074752807617, 1.0484453439712524, 0.9596720933914185, 0.954775869846344, 0.9414180517196655, 0.9248806834220886, 0.9207578897476196, 0.9623022675514221, 0.939276933670044, 0.9478324055671692, 0.9445891380310059, 0.9333194494247437, 0.9238488674163818, 0.9191880822181702]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1387369632720947
current iteration best possible eval_loss (full train run):  -0.9191880822181702
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778, -1.1277821063995361, -1.1276923418045044, -1.1273939609527588, -1.1274949312210083, -1.1277821063995361, -1.1387369632720947]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.7400 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.647038459777832, 0.3565073609352112, 0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.6441194415092468, 0.6647308468818665, 0.9432829022407532, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.47025445103645325, 0.5915921330451965, 0.563302755355835, 0.236403226852417]  ‚Üí  acq = -1.1033840495807727
X = [0.7882768511772156, 0.0865660309791565, 0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.06684881448745728, 0.476356565952301, 0.49730604887008667, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.06370062381029129, 0.8247895836830139, 0.642969012260437, 0.2869639992713928]  ‚Üí  acq = -1.1033840235545935
X = [0.9537772536277771, 0.5254051685333252, 0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5333559513092041, 0.6058185696601868, 0.9840016961097717, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.07915887981653214, 0.6141131520271301, 0.8602699041366577, 0.6692355275154114]  ‚Üí  acq = -1.1033838824336617
X = [0.10701495409011841, 0.8236895203590393, 0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6551105976104736, 0.23645484447479248, 0.007582306861877441, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5136890411376953, 0.0011958479881286621, 0.378928005695343, 0.21825557947158813]  ‚Üí  acq = -1.0965024665885361
X = [0.37967562675476074, 0.21945631504058838, 0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4969014823436737, 0.5960436463356018, 0.011648118495941162, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.918420135974884, 0.3473445177078247, 0.7110291719436646, 0.30779868364334106]  ‚Üí  acq = -1.103384020092931
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0958, dtype=torch.float64), tensor(0.1444, dtype=torch.float64), 0, tensor(0.4334, dtype=torch.float64), tensor(0.0360, dtype=torch.float64), tensor(0.0408, dtype=torch.float64), tensor(0.2496, dtype=torch.float64), 16, 0, 1, 0, 1, 1, 43, 1.6817378298790544e-19, 35.25979431342027, 1]
normalized proposed parameters for next round by BO: [tensor(1.2120e-17, dtype=torch.float64), tensor(4.5249e-18, dtype=torch.float64), tensor(0.0958, dtype=torch.float64), tensor(0.1444, dtype=torch.float64), tensor(4.2058e-17, dtype=torch.float64), tensor(0.4334, dtype=torch.float64), tensor(0.0360, dtype=torch.float64), tensor(0.0408, dtype=torch.float64), tensor(0.2496, dtype=torch.float64), tensor(0.4889, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3334, dtype=torch.float64), tensor(1.6817e-18, dtype=torch.float64), tensor(0.7346, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.096
  sciq: 0.144
  triviaqa: 0
  truthfulqa_gen: 0.433
  wikitext: 0.036
  mmlu: 0.041
  arc_challenge: 0.25

LoRA Parameters:
  lora_r: (43,)
  lora_dropout: (1.6817378298790544e-19,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (35.25979431342027,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  43
lora dropout:  1.6817378298790544e-19
lora alpha:  35.25979431342027
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 28,884,992 || all params: 8,059,146,240 || trainable%: 0.3584
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3065, 'grad_norm': 1.1059209108352661, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.535562515258789, 'eval_runtime': 3.4687, 'eval_samples_per_second': 288.293, 'eval_steps_per_second': 18.162, 'epoch': 0.04}
{'loss': 1.5105, 'grad_norm': 0.6737749576568604, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9591919183731079, 'eval_runtime': 3.4701, 'eval_samples_per_second': 288.179, 'eval_steps_per_second': 18.155, 'epoch': 0.08}
{'loss': 1.2438, 'grad_norm': 0.4964194893836975, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8075239658355713, 'eval_runtime': 3.4686, 'eval_samples_per_second': 288.3, 'eval_steps_per_second': 18.163, 'epoch': 0.12}
{'loss': 1.1151, 'grad_norm': 0.4898515045642853, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7173730731010437, 'eval_runtime': 3.4642, 'eval_samples_per_second': 288.666, 'eval_steps_per_second': 18.186, 'epoch': 0.16}
{'loss': 1.069, 'grad_norm': 0.4054414629936218, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6848963499069214, 'eval_runtime': 3.4701, 'eval_samples_per_second': 288.178, 'eval_steps_per_second': 18.155, 'epoch': 0.2}
{'loss': 1.0976, 'grad_norm': 0.5554425120353699, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.650311291217804, 'eval_runtime': 3.476, 'eval_samples_per_second': 287.685, 'eval_steps_per_second': 18.124, 'epoch': 0.24}
{'loss': 1.08, 'grad_norm': 0.442617267370224, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6088168621063232, 'eval_runtime': 3.4763, 'eval_samples_per_second': 287.658, 'eval_steps_per_second': 18.122, 'epoch': 0.28}
{'loss': 1.0357, 'grad_norm': 0.5206329226493835, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.568954348564148, 'eval_runtime': 3.4855, 'eval_samples_per_second': 286.901, 'eval_steps_per_second': 18.075, 'epoch': 0.32}
{'loss': 0.9843, 'grad_norm': 0.43148061633110046, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.55810546875, 'eval_runtime': 3.4969, 'eval_samples_per_second': 285.964, 'eval_steps_per_second': 18.016, 'epoch': 0.36}
{'loss': 0.9957, 'grad_norm': 0.4307172894477844, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5114827752113342, 'eval_runtime': 3.4891, 'eval_samples_per_second': 286.606, 'eval_steps_per_second': 18.056, 'epoch': 0.4}
{'loss': 0.9783, 'grad_norm': 0.7509987950325012, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.497367799282074, 'eval_runtime': 3.4855, 'eval_samples_per_second': 286.899, 'eval_steps_per_second': 18.075, 'epoch': 0.44}
{'loss': 1.0131, 'grad_norm': 0.5971590876579285, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.466462641954422, 'eval_runtime': 3.49, 'eval_samples_per_second': 286.534, 'eval_steps_per_second': 18.052, 'epoch': 0.48}
{'loss': 0.9675, 'grad_norm': 0.5719438195228577, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4527442455291748, 'eval_runtime': 3.4846, 'eval_samples_per_second': 286.973, 'eval_steps_per_second': 18.079, 'epoch': 0.52}
{'loss': 0.9711, 'grad_norm': 0.41639432311058044, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.42816248536109924, 'eval_runtime': 3.4876, 'eval_samples_per_second': 286.732, 'eval_steps_per_second': 18.064, 'epoch': 0.56}
{'loss': 0.9646, 'grad_norm': 0.47402849793434143, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.40191540122032166, 'eval_runtime': 3.4885, 'eval_samples_per_second': 286.655, 'eval_steps_per_second': 18.059, 'epoch': 0.6}
{'loss': 0.972, 'grad_norm': 0.4972238540649414, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.3887007236480713, 'eval_runtime': 3.5131, 'eval_samples_per_second': 284.647, 'eval_steps_per_second': 17.933, 'epoch': 0.64}
{'loss': 0.924, 'grad_norm': 0.6214794516563416, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.361267626285553, 'eval_runtime': 3.5178, 'eval_samples_per_second': 284.271, 'eval_steps_per_second': 17.909, 'epoch': 0.68}
{'loss': 0.977, 'grad_norm': 0.622882604598999, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.344003289937973, 'eval_runtime': 3.5125, 'eval_samples_per_second': 284.697, 'eval_steps_per_second': 17.936, 'epoch': 0.72}
{'loss': 0.9343, 'grad_norm': 0.6474730968475342, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3194112479686737, 'eval_runtime': 3.4999, 'eval_samples_per_second': 285.722, 'eval_steps_per_second': 18.001, 'epoch': 0.76}
{'loss': 0.9314, 'grad_norm': 0.8477445840835571, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.30424633622169495, 'eval_runtime': 3.4867, 'eval_samples_per_second': 286.808, 'eval_steps_per_second': 18.069, 'epoch': 0.8}
{'loss': 0.8683, 'grad_norm': 0.5066640973091125, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2892361283302307, 'eval_runtime': 3.4929, 'eval_samples_per_second': 286.298, 'eval_steps_per_second': 18.037, 'epoch': 0.84}
{'loss': 0.9375, 'grad_norm': 0.6478501558303833, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.27942854166030884, 'eval_runtime': 3.4931, 'eval_samples_per_second': 286.277, 'eval_steps_per_second': 18.035, 'epoch': 0.88}
{'loss': 0.8792, 'grad_norm': 0.584629476070404, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2640882730484009, 'eval_runtime': 3.5021, 'eval_samples_per_second': 285.544, 'eval_steps_per_second': 17.989, 'epoch': 0.92}
{'loss': 0.8175, 'grad_norm': 0.7814427614212036, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.25566810369491577, 'eval_runtime': 3.4943, 'eval_samples_per_second': 286.181, 'eval_steps_per_second': 18.029, 'epoch': 0.96}
{'loss': 0.8848, 'grad_norm': 0.7488240599632263, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2526765465736389, 'eval_runtime': 3.5023, 'eval_samples_per_second': 285.53, 'eval_steps_per_second': 17.988, 'epoch': 1.0}
{'train_runtime': 285.6552, 'train_samples_per_second': 34.997, 'train_steps_per_second': 2.188, 'train_loss': 1.098352554321289, 'epoch': 1.0}
train_results:  {'eval_loss': [1.535562515258789, 0.9591919183731079, 0.8075239658355713, 0.7173730731010437, 0.6848963499069214, 0.650311291217804, 0.6088168621063232, 0.568954348564148, 0.55810546875, 0.5114827752113342, 0.497367799282074, 0.466462641954422, 0.4527442455291748, 0.42816248536109924, 0.40191540122032166, 0.3887007236480713, 0.361267626285553, 0.344003289937973, 0.3194112479686737, 0.30424633622169495, 0.2892361283302307, 0.27942854166030884, 0.2640882730484009, 0.25566810369491577, 0.2526765465736389], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.535562515258789, 0.9591919183731079, 0.8075239658355713, 0.7173730731010437, 0.6848963499069214, 0.650311291217804, 0.6088168621063232, 0.568954348564148, 0.55810546875, 0.5114827752113342, 0.497367799282074, 0.466462641954422, 0.4527442455291748, 0.42816248536109924, 0.40191540122032166, 0.3887007236480713, 0.361267626285553, 0.344003289937973, 0.3194112479686737, 0.30424633622169495, 0.2892361283302307, 0.27942854166030884, 0.2640882730484009, 0.25566810369491577, 0.2526765465736389]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1274372339248657
current iteration best possible eval_loss (full train run):  -0.2526765465736389
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778, -1.1277821063995361, -1.1276923418045044, -1.1273939609527588, -1.1274949312210083, -1.1277821063995361, -1.1387369632720947, -1.1274372339248657]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.4267 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.0130869549032762
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.0130664569392234
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.0130840968551802
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.01274066069544
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.0168071634689668
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0692, dtype=torch.float64), tensor(0.1646, dtype=torch.float64), tensor(0.0313, dtype=torch.float64), tensor(0.5449, dtype=torch.float64), 0, tensor(0.1899, dtype=torch.float64), 0, 15, 1, 0, 1, 0, 1, 26, 0.02180473165587239, 19.04472187207947, 1]
normalized proposed parameters for next round by BO: [tensor(1.0318e-17, dtype=torch.float64), tensor(4.4713e-18, dtype=torch.float64), tensor(0.0692, dtype=torch.float64), tensor(0.1646, dtype=torch.float64), tensor(0.0313, dtype=torch.float64), tensor(0.5449, dtype=torch.float64), tensor(6.9208e-18, dtype=torch.float64), tensor(0.1899, dtype=torch.float64), tensor(1.3581e-17, dtype=torch.float64), tensor(0.4725, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1994, dtype=torch.float64), tensor(0.2180, dtype=torch.float64), tensor(0.3968, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.069
  sciq: 0.165
  triviaqa: 0.031
  truthfulqa_gen: 0.545
  wikitext: 0
  mmlu: 0.19
  arc_challenge: 0

LoRA Parameters:
  lora_r: (26,)
  lora_dropout: (0.02180473165587239,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (19.04472187207947,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  26
lora dropout:  0.02180473165587239
lora alpha:  19.04472187207947
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 17,571,840 || all params: 8,047,833,088 || trainable%: 0.2183
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.724, 'grad_norm': 0.8381350040435791, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2069642543792725, 'eval_runtime': 3.3662, 'eval_samples_per_second': 297.072, 'eval_steps_per_second': 18.716, 'epoch': 0.04}
{'loss': 1.9593, 'grad_norm': 0.700067937374115, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3739759922027588, 'eval_runtime': 3.3757, 'eval_samples_per_second': 296.237, 'eval_steps_per_second': 18.663, 'epoch': 0.08}
{'loss': 1.5575, 'grad_norm': 0.2846662402153015, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1085988283157349, 'eval_runtime': 3.3755, 'eval_samples_per_second': 296.256, 'eval_steps_per_second': 18.664, 'epoch': 0.12}
{'loss': 1.3661, 'grad_norm': 0.41911032795906067, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.011689305305481, 'eval_runtime': 3.3754, 'eval_samples_per_second': 296.261, 'eval_steps_per_second': 18.664, 'epoch': 0.16}
{'loss': 1.2109, 'grad_norm': 0.4072117507457733, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8660532236099243, 'eval_runtime': 3.3867, 'eval_samples_per_second': 295.271, 'eval_steps_per_second': 18.602, 'epoch': 0.2}
{'loss': 1.2705, 'grad_norm': 0.34828895330429077, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8343839645385742, 'eval_runtime': 3.393, 'eval_samples_per_second': 294.721, 'eval_steps_per_second': 18.567, 'epoch': 0.24}
{'loss': 1.2396, 'grad_norm': 0.3272649645805359, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7969549298286438, 'eval_runtime': 3.39, 'eval_samples_per_second': 294.985, 'eval_steps_per_second': 18.584, 'epoch': 0.28}
{'loss': 1.1977, 'grad_norm': 0.4300471842288971, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7566119432449341, 'eval_runtime': 3.41, 'eval_samples_per_second': 293.258, 'eval_steps_per_second': 18.475, 'epoch': 0.32}
{'loss': 1.1238, 'grad_norm': 0.3904656767845154, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7221112251281738, 'eval_runtime': 3.4017, 'eval_samples_per_second': 293.974, 'eval_steps_per_second': 18.52, 'epoch': 0.36}
{'loss': 1.1077, 'grad_norm': 0.3924311399459839, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6851724982261658, 'eval_runtime': 3.3948, 'eval_samples_per_second': 294.568, 'eval_steps_per_second': 18.558, 'epoch': 0.4}
{'loss': 1.1603, 'grad_norm': 0.4438806474208832, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6461691856384277, 'eval_runtime': 3.399, 'eval_samples_per_second': 294.208, 'eval_steps_per_second': 18.535, 'epoch': 0.44}
{'loss': 1.0773, 'grad_norm': 0.540077269077301, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6077120304107666, 'eval_runtime': 3.4027, 'eval_samples_per_second': 293.888, 'eval_steps_per_second': 18.515, 'epoch': 0.48}
{'loss': 1.0857, 'grad_norm': 0.4921770989894867, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5691806674003601, 'eval_runtime': 3.4031, 'eval_samples_per_second': 293.852, 'eval_steps_per_second': 18.513, 'epoch': 0.52}
{'loss': 1.0362, 'grad_norm': 0.4546360373497009, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5169358849525452, 'eval_runtime': 3.4019, 'eval_samples_per_second': 293.955, 'eval_steps_per_second': 18.519, 'epoch': 0.56}
{'loss': 1.0271, 'grad_norm': 0.528831958770752, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4693503975868225, 'eval_runtime': 3.4013, 'eval_samples_per_second': 294.007, 'eval_steps_per_second': 18.522, 'epoch': 0.6}
{'loss': 0.9839, 'grad_norm': 0.487621545791626, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4364496171474457, 'eval_runtime': 3.3969, 'eval_samples_per_second': 294.389, 'eval_steps_per_second': 18.546, 'epoch': 0.64}
{'loss': 0.9267, 'grad_norm': 0.49929770827293396, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4138316214084625, 'eval_runtime': 3.401, 'eval_samples_per_second': 294.03, 'eval_steps_per_second': 18.524, 'epoch': 0.68}
{'loss': 0.9682, 'grad_norm': 0.6737123727798462, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.38444915413856506, 'eval_runtime': 3.3955, 'eval_samples_per_second': 294.506, 'eval_steps_per_second': 18.554, 'epoch': 0.72}
{'loss': 0.919, 'grad_norm': 0.696607768535614, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.36282533407211304, 'eval_runtime': 3.4002, 'eval_samples_per_second': 294.104, 'eval_steps_per_second': 18.529, 'epoch': 0.76}
{'loss': 0.9489, 'grad_norm': 0.6949924230575562, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3321244418621063, 'eval_runtime': 3.3944, 'eval_samples_per_second': 294.605, 'eval_steps_per_second': 18.56, 'epoch': 0.8}
{'loss': 0.89, 'grad_norm': 0.5244771838188171, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.31857454776763916, 'eval_runtime': 3.3962, 'eval_samples_per_second': 294.448, 'eval_steps_per_second': 18.55, 'epoch': 0.84}
{'loss': 0.9582, 'grad_norm': 0.5535454750061035, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.30012109875679016, 'eval_runtime': 3.3978, 'eval_samples_per_second': 294.306, 'eval_steps_per_second': 18.541, 'epoch': 0.88}
{'loss': 0.956, 'grad_norm': 0.5842093229293823, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2868533432483673, 'eval_runtime': 3.3968, 'eval_samples_per_second': 294.393, 'eval_steps_per_second': 18.547, 'epoch': 0.92}
{'loss': 0.8608, 'grad_norm': 0.6771138906478882, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2776218056678772, 'eval_runtime': 3.398, 'eval_samples_per_second': 294.292, 'eval_steps_per_second': 18.54, 'epoch': 0.96}
{'loss': 0.9094, 'grad_norm': 0.44819578528404236, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.27309906482696533, 'eval_runtime': 3.4038, 'eval_samples_per_second': 293.786, 'eval_steps_per_second': 18.509, 'epoch': 1.0}
{'train_runtime': 275.3594, 'train_samples_per_second': 36.305, 'train_steps_per_second': 2.27, 'train_loss': 1.218584033203125, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2069642543792725, 1.3739759922027588, 1.1085988283157349, 1.011689305305481, 0.8660532236099243, 0.8343839645385742, 0.7969549298286438, 0.7566119432449341, 0.7221112251281738, 0.6851724982261658, 0.6461691856384277, 0.6077120304107666, 0.5691806674003601, 0.5169358849525452, 0.4693503975868225, 0.4364496171474457, 0.4138316214084625, 0.38444915413856506, 0.36282533407211304, 0.3321244418621063, 0.31857454776763916, 0.30012109875679016, 0.2868533432483673, 0.2776218056678772, 0.27309906482696533], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2069642543792725, 1.3739759922027588, 1.1085988283157349, 1.011689305305481, 0.8660532236099243, 0.8343839645385742, 0.7969549298286438, 0.7566119432449341, 0.7221112251281738, 0.6851724982261658, 0.6461691856384277, 0.6077120304107666, 0.5691806674003601, 0.5169358849525452, 0.4693503975868225, 0.4364496171474457, 0.4138316214084625, 0.38444915413856506, 0.36282533407211304, 0.3321244418621063, 0.31857454776763916, 0.30012109875679016, 0.2868533432483673, 0.2776218056678772, 0.27309906482696533]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.27309906482696533
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778, -1.1277821063995361, -1.1276923418045044, -1.1273939609527588, -1.1274949312210083, -1.1277821063995361, -1.1387369632720947, -1.1274372339248657, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.5172 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -1.0979863805420385
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -1.098021211039021
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -1.1019449597604907
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -1.0980212108116345
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -1.0980212375762828
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0916, dtype=torch.float64), tensor(0.0320, dtype=torch.float64), tensor(0.0775, dtype=torch.float64), tensor(0.1203, dtype=torch.float64), 0, tensor(0.4385, dtype=torch.float64), tensor(0.0734, dtype=torch.float64), tensor(0.1015, dtype=torch.float64), tensor(0.0652, dtype=torch.float64), 22, 0, 1, 0, 1, 1, 54, 0.0025115401742376938, 25.544838878784336, 1]
normalized proposed parameters for next round by BO: [tensor(0.0916, dtype=torch.float64), tensor(0.0320, dtype=torch.float64), tensor(0.0775, dtype=torch.float64), tensor(0.1203, dtype=torch.float64), tensor(6.8429e-19, dtype=torch.float64), tensor(0.4385, dtype=torch.float64), tensor(0.0734, dtype=torch.float64), tensor(0.1015, dtype=torch.float64), tensor(0.0652, dtype=torch.float64), tensor(0.7019, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4236, dtype=torch.float64), tensor(0.0251, dtype=torch.float64), tensor(0.5322, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.092
  gsm8k: 0.032
  rowan_hellaswag: 0.077
  sciq: 0.12
  triviaqa: 0
  truthfulqa_gen: 0.438
  wikitext: 0.073
  mmlu: 0.101
  arc_challenge: 0.065

LoRA Parameters:
  lora_r: (54,)
  lora_dropout: (0.0025115401742376938,)
  num_layers_to_apply: (22,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (25.544838878784336,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  22
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  54
lora dropout:  0.0025115401742376938
lora alpha:  25.544838878784336
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 49,876,992 || all params: 8,080,138,240 || trainable%: 0.6173
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3588, 'grad_norm': 1.125457763671875, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5418950319290161, 'eval_runtime': 3.5372, 'eval_samples_per_second': 282.708, 'eval_steps_per_second': 17.811, 'epoch': 0.04}
{'loss': 1.6031, 'grad_norm': 0.9363077282905579, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9391429424285889, 'eval_runtime': 3.5374, 'eval_samples_per_second': 282.696, 'eval_steps_per_second': 17.81, 'epoch': 0.08}
{'loss': 1.2745, 'grad_norm': 0.7009369730949402, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8437284231185913, 'eval_runtime': 3.5414, 'eval_samples_per_second': 282.377, 'eval_steps_per_second': 17.79, 'epoch': 0.12}
{'loss': 1.2116, 'grad_norm': 0.4282093346118927, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7246763706207275, 'eval_runtime': 3.5328, 'eval_samples_per_second': 283.063, 'eval_steps_per_second': 17.833, 'epoch': 0.16}
{'loss': 1.162, 'grad_norm': 0.3629000782966614, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6605271697044373, 'eval_runtime': 3.5328, 'eval_samples_per_second': 283.065, 'eval_steps_per_second': 17.833, 'epoch': 0.2}
{'loss': 1.1631, 'grad_norm': 0.45963379740715027, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6160292029380798, 'eval_runtime': 3.5367, 'eval_samples_per_second': 282.75, 'eval_steps_per_second': 17.813, 'epoch': 0.24}
{'loss': 1.1648, 'grad_norm': 0.4194659888744354, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5783936977386475, 'eval_runtime': 3.5465, 'eval_samples_per_second': 281.966, 'eval_steps_per_second': 17.764, 'epoch': 0.28}
{'loss': 1.0721, 'grad_norm': 0.39657825231552124, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5377328991889954, 'eval_runtime': 3.5484, 'eval_samples_per_second': 281.819, 'eval_steps_per_second': 17.755, 'epoch': 0.32}
{'loss': 1.111, 'grad_norm': 0.3735261857509613, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.49742016196250916, 'eval_runtime': 3.5526, 'eval_samples_per_second': 281.487, 'eval_steps_per_second': 17.734, 'epoch': 0.36}
{'loss': 1.0924, 'grad_norm': 0.39920201897621155, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.46924445033073425, 'eval_runtime': 3.5473, 'eval_samples_per_second': 281.902, 'eval_steps_per_second': 17.76, 'epoch': 0.4}
{'loss': 1.0754, 'grad_norm': 0.4317356050014496, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.44389453530311584, 'eval_runtime': 3.5421, 'eval_samples_per_second': 282.321, 'eval_steps_per_second': 17.786, 'epoch': 0.44}
{'loss': 1.0167, 'grad_norm': 0.31471487879753113, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4153408110141754, 'eval_runtime': 3.5573, 'eval_samples_per_second': 281.114, 'eval_steps_per_second': 17.71, 'epoch': 0.48}
{'loss': 1.0962, 'grad_norm': 0.4692366421222687, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.37393900752067566, 'eval_runtime': 3.5491, 'eval_samples_per_second': 281.764, 'eval_steps_per_second': 17.751, 'epoch': 0.52}
{'loss': 0.985, 'grad_norm': 0.45315495133399963, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.35130226612091064, 'eval_runtime': 3.5454, 'eval_samples_per_second': 282.054, 'eval_steps_per_second': 17.769, 'epoch': 0.56}
{'loss': 0.9472, 'grad_norm': 0.46648773550987244, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.32947081327438354, 'eval_runtime': 3.544, 'eval_samples_per_second': 282.166, 'eval_steps_per_second': 17.776, 'epoch': 0.6}
{'loss': 1.0408, 'grad_norm': 0.3603678345680237, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.31210851669311523, 'eval_runtime': 3.5496, 'eval_samples_per_second': 281.725, 'eval_steps_per_second': 17.749, 'epoch': 0.64}
{'loss': 1.0054, 'grad_norm': 0.418026864528656, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.29465290904045105, 'eval_runtime': 3.546, 'eval_samples_per_second': 282.009, 'eval_steps_per_second': 17.767, 'epoch': 0.68}
{'loss': 0.9224, 'grad_norm': 0.29756608605384827, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.27637913823127747, 'eval_runtime': 3.5564, 'eval_samples_per_second': 281.185, 'eval_steps_per_second': 17.715, 'epoch': 0.72}
{'loss': 0.9623, 'grad_norm': 0.4952702224254608, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.263796865940094, 'eval_runtime': 3.5441, 'eval_samples_per_second': 282.156, 'eval_steps_per_second': 17.776, 'epoch': 0.76}
{'loss': 1.0681, 'grad_norm': 0.3840027451515198, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.2549574673175812, 'eval_runtime': 3.5534, 'eval_samples_per_second': 281.419, 'eval_steps_per_second': 17.729, 'epoch': 0.8}
{'loss': 0.95, 'grad_norm': 0.4490227699279785, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2387435883283615, 'eval_runtime': 3.559, 'eval_samples_per_second': 280.976, 'eval_steps_per_second': 17.702, 'epoch': 0.84}
{'loss': 0.9461, 'grad_norm': 0.3676661252975464, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2296910285949707, 'eval_runtime': 3.5512, 'eval_samples_per_second': 281.592, 'eval_steps_per_second': 17.74, 'epoch': 0.88}
{'loss': 0.906, 'grad_norm': 0.336605429649353, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2201119363307953, 'eval_runtime': 3.5593, 'eval_samples_per_second': 280.954, 'eval_steps_per_second': 17.7, 'epoch': 0.92}
{'loss': 0.9992, 'grad_norm': 0.30574271082878113, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.21511545777320862, 'eval_runtime': 3.5523, 'eval_samples_per_second': 281.506, 'eval_steps_per_second': 17.735, 'epoch': 0.96}
{'loss': 0.9359, 'grad_norm': 0.5119795203208923, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.21351663768291473, 'eval_runtime': 3.5504, 'eval_samples_per_second': 281.661, 'eval_steps_per_second': 17.745, 'epoch': 1.0}
{'train_runtime': 298.7282, 'train_samples_per_second': 33.459, 'train_steps_per_second': 2.092, 'train_loss': 1.1628101348876954, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5418950319290161, 0.9391429424285889, 0.8437284231185913, 0.7246763706207275, 0.6605271697044373, 0.6160292029380798, 0.5783936977386475, 0.5377328991889954, 0.49742016196250916, 0.46924445033073425, 0.44389453530311584, 0.4153408110141754, 0.37393900752067566, 0.35130226612091064, 0.32947081327438354, 0.31210851669311523, 0.29465290904045105, 0.27637913823127747, 0.263796865940094, 0.2549574673175812, 0.2387435883283615, 0.2296910285949707, 0.2201119363307953, 0.21511545777320862, 0.21351663768291473], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5418950319290161, 0.9391429424285889, 0.8437284231185913, 0.7246763706207275, 0.6605271697044373, 0.6160292029380798, 0.5783936977386475, 0.5377328991889954, 0.49742016196250916, 0.46924445033073425, 0.44389453530311584, 0.4153408110141754, 0.37393900752067566, 0.35130226612091064, 0.32947081327438354, 0.31210851669311523, 0.29465290904045105, 0.27637913823127747, 0.263796865940094, 0.2549574673175812, 0.2387435883283615, 0.2296910285949707, 0.2201119363307953, 0.21511545777320862, 0.21351663768291473]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1267887353897095
current iteration best possible eval_loss (full train run):  -0.21351663768291473
max eval_loss so far:  -0.16776354610919952
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.124546766281128, -1.125186800956726, -1.1277821063995361, -1.1277821063995361, -1.1255121231079102, -1.121098518371582, -1.1213496923446655, -1.1264408826828003, -1.1277821063995361, -1.1277821063995361, -1.3225500583648682, -1.1277821063995361, -1.1273829936981201, -1.1263079643249512, -1.1277821063995361, -1.124749779701233, -1.1266225576400757, -1.1249819993972778, -1.1277821063995361, -1.1276923418045044, -1.1273939609527588, -1.1274949312210083, -1.1277821063995361, -1.1387369632720947, -1.1274372339248657, -1.1277821063995361, -1.1267887353897095]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 26.6640 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -1.1050281400324808
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -1.1050367482937826
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -1.1050192346632148
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -1.1234049428724753
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -1.1050281400324813
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0641, dtype=torch.float64), tensor(0.2193, dtype=torch.float64), tensor(0.0810, dtype=torch.float64), tensor(0.0992, dtype=torch.float64), tensor(0.0976, dtype=torch.float64), tensor(0.2975, dtype=torch.float64), tensor(0.0326, dtype=torch.float64), tensor(0.0499, dtype=torch.float64), tensor(0.0588, dtype=torch.float64), 13, 1, 1, 0, 1, 1, 44, 0.012000271085219014, 30.523882972166508, 1]
normalized proposed parameters for next round by BO: [tensor(0.0641, dtype=torch.float64), tensor(0.2193, dtype=torch.float64), tensor(0.0810, dtype=torch.float64), tensor(0.0992, dtype=torch.float64), tensor(0.0976, dtype=torch.float64), tensor(0.2975, dtype=torch.float64), tensor(0.0326, dtype=torch.float64), tensor(0.0499, dtype=torch.float64), tensor(0.0588, dtype=torch.float64), tensor(0.4140, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3399, dtype=torch.float64), tensor(0.1200, dtype=torch.float64), tensor(0.6359, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-1.1763049364089966, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
truthfulqa_gen
evaluation dataset:
data domain:  truthfulqa_gen  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/truthfulqa_gen/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 20, 1, 1, 1, 1, 1, 18, 0.05137445104835325, 2, 1]
Checking history sample input_X_between_0_1:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.5137445104835324, 0.041666666666666664, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2949618101119995
Checking history sample input_X:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 22, 1, 1, 0, 0, 1, 9, 0.09530863992118319, 22, 1]
Checking history sample input_X_between_0_1:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 0.6875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0703125, 0.9530863992118318, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.064080834388733
Checking history sample input_X:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 7, 0, 0, 0, 0, 1, 45, 0.012049704078718804, 22, 1]
Checking history sample input_X_between_0_1:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 0.21875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3515625, 0.12049704078718804, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.052124261856079
Checking history sample input_X:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 2, 1, 1, 1, 0, 0, 32, 0.03789572912213354, 26, 1]
Checking history sample input_X_between_0_1:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.3789572912213354, 0.5416666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4835323095321655
Checking history sample input_X:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 8, 0, 0, 1, 1, 1, 57, 0.05639372568359048, 47, 0]
Checking history sample input_X_between_0_1:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4453125, 0.5639372568359048, 0.9791666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2198251485824585
Checking history sample input_X:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 18, 1, 1, 0, 0, 1, 112, 0.0011300351648876107, 2, 1]
Checking history sample input_X_between_0_1:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 0.5625, 1.0, 1.0, 0.0, 0.0, 1.0, 0.875, 0.011300351648876106, 0.041666666666666664, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1017459630966187
Checking history sample input_X:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 11, 0, 1, 1, 0, 1, 121, 0.04409228366491266, 38, 0]
Checking history sample input_X_between_0_1:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 0.34375, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9453125, 0.4409228366491266, 0.7916666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0767676830291748
Checking history sample input_X:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 19, 1, 0, 0, 1, 0, 20, 0.057419890903339765, 17, 1]
Checking history sample input_X_between_0_1:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 0.59375, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15625, 0.5741989090333977, 0.3541666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0715935230255127
Checking history sample input_X:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 13, 0, 1, 0, 0, 1, 54, 0.07044921211215552, 48, 0]
Checking history sample input_X_between_0_1:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 0.40625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.421875, 0.7044921211215551, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3453316688537598
Checking history sample input_X:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 9, 1, 0, 1, 1, 0, 17, 0.07776680881547844, 40, 0]
Checking history sample input_X_between_0_1:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 0.28125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1328125, 0.7776680881547844, 0.8333333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.148055076599121
Checking history sample input_X:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 1, 0, 0, 0, 1, 0, 60, 0.001514616808966751, 14, 1]
Checking history sample input_X_between_0_1:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.46875, 0.015146168089667511, 0.2916666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4578598737716675
Checking history sample input_X:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 18, 1, 1, 1, 0, 1, 45, 0.001287895623877422, 34, 1]
Checking history sample input_X_between_0_1:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 0.5625, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3515625, 0.01287895623877422, 0.7083333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9687060117721558
Checking history sample input_X:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 8, 0, 0, 0, 1, 1, 51, 0.08835721159033366, 35, 0]
Checking history sample input_X_between_0_1:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3984375, 0.8835721159033366, 0.7291666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2380133867263794
Checking history sample input_X:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 2, 0, 0, 0, 1, 1, 28, 0.09450434861769766, 16, 1]
Checking history sample input_X_between_0_1:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 0.0625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.21875, 0.9450434861769765, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.283164620399475
Checking history sample input_X:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 2, 0, 0, 0, 1, 0, 27, 0.08742966606550949, 14, 0]
Checking history sample input_X_between_0_1:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2109375, 0.8742966606550948, 0.2916666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.5741345882415771
Checking history sample input_X:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 30, 0, 0, 0, 1, 0, 18, 0.07914275308569024, 23, 0]
Checking history sample input_X_between_0_1:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.140625, 0.7914275308569024, 0.4791666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0483520030975342
Checking history sample input_X:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 4, 1, 1, 1, 1, 1, 126, 0.005789639303569194, 25, 1]
Checking history sample input_X_between_0_1:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.05789639303569194, 0.5208333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9796491861343384
Checking history sample input_X:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 9, 1, 1, 1, 1, 1, 31, 0.03322680456132531, 22, 1]
Checking history sample input_X_between_0_1:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2421875, 0.33226804561325307, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1166555881500244
Checking history sample input_X:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 15, 0, 1, 0, 1, 1, 49, 0.0008783405064032635, 29, 1]
Checking history sample input_X_between_0_1:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 0.46875, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3828125, 0.008783405064032634, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8782313466072083
Checking history sample input_X:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 1, 0, 1, 1, 0, 1, 106, 0.08696702158391928, 5, 0]
Checking history sample input_X_between_0_1:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 0.03125, 0.0, 1.0, 1.0, 0.0, 1.0, 0.828125, 0.8696702158391928, 0.10416666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.4728790521621704
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4017 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16031867265701294, 0.38416755199432373, 0.43190622329711914, 0.5052624940872192, 0.03402602672576904, 0.30002135038375854, 0.25499939918518066, 0.37621599435806274, 0.39879488945007324, 0.9157447218894958, 0.8840097784996033, 0.10385668277740479, 0.044097840785980225, 0.7504631280899048, 0.43580615520477295, 0.14621058106422424, 0.03360581398010254, 0.6811758279800415, 0.6288644075393677]  ‚Üí  acq = -0.6940806706535148
X = [0.5054563879966736, 0.9175823926925659, 0.72557133436203, 0.49456363916397095, 0.8813178539276123, 0.645788848400116, 0.8690239787101746, 0.35576480627059937, 0.7927037477493286, 0.3084469139575958, 0.25986069440841675, 0.65398108959198, 0.6918710470199585, 0.15115982294082642, 0.40588051080703735, 0.37669065594673157, 0.9656259417533875, 0.28104373812675476, 0.9393320083618164]  ‚Üí  acq = -0.6791588352442407
X = [0.2356998324394226, 0.7436355948448181, 0.024914085865020752, 0.5332534909248352, 0.40452879667282104, 0.7091929316520691, 0.5338171720504761, 0.877155601978302, 0.8259592652320862, 0.552446186542511, 0.09636795520782471, 0.5429786443710327, 0.9750219583511353, 0.4863256812095642, 0.9258865714073181, 0.9982288479804993, 0.3321690559387207, 0.4649343192577362, 0.14576447010040283]  ‚Üí  acq = -0.6508138430491403
X = [0.9784659743309021, 0.2890373468399048, 0.2788410782814026, 0.9119408130645752, 0.06163877248764038, 0.01322394609451294, 0.11561739444732666, 0.46361202001571655, 0.8592676520347595, 0.39803001284599304, 0.19337624311447144, 0.18421852588653564, 0.3048694133758545, 0.8771745562553406, 0.8225249648094177, 0.23737065494060516, 0.04702770709991455, 0.3519912660121918, 0.9805734753608704]  ‚Üí  acq = -0.6849462044929081
X = [0.6941823959350586, 0.19157367944717407, 0.976674497127533, 0.2738240957260132, 0.017247378826141357, 0.9475875496864319, 0.23092156648635864, 0.6219609379768372, 0.08978629112243652, 0.8371531963348389, 0.7114340662956238, 0.7263268232345581, 0.36689281463623047, 0.8925881385803223, 0.819909930229187, 0.4585444927215576, 0.431087851524353, 0.8892704248428345, 0.7772882580757141]  ‚Üí  acq = -0.6791507852056051
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.0217, dtype=torch.float64), 0, tensor(0.2060, dtype=torch.float64), 0, 0, 0, tensor(0.1947, dtype=torch.float64), tensor(0.5776, dtype=torch.float64), 20, 1, 1, 1, 1, 1, 2, 1.2663481374630693e-17, 47.99999999999999, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(6.6764e-17, dtype=torch.float64), tensor(0.0217, dtype=torch.float64), tensor(4.2170e-17, dtype=torch.float64), tensor(0.2060, dtype=torch.float64), tensor(4.5998e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0123e-16, dtype=torch.float64), tensor(0.1947, dtype=torch.float64), tensor(0.5776, dtype=torch.float64), tensor(0.6170, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.2663e-16, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.022
  rowan_hellaswag: 0
  sciq: 0.206
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.195
  arc_challenge: 0.578

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.2663481374630693e-17,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  2
lora dropout:  1.2663481374630693e-17
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,744,320 || all params: 8,033,005,568 || trainable%: 0.0342
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 2.4692, 'grad_norm': 4.79401969909668, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3690892457962036, 'eval_runtime': 3.6985, 'eval_samples_per_second': 270.378, 'eval_steps_per_second': 17.034, 'epoch': 0.04}
{'loss': 1.1152, 'grad_norm': 2.5220000743865967, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2344310283660889, 'eval_runtime': 3.7023, 'eval_samples_per_second': 270.099, 'eval_steps_per_second': 17.016, 'epoch': 0.08}
{'loss': 1.0164, 'grad_norm': 2.7974236011505127, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1542541980743408, 'eval_runtime': 3.6876, 'eval_samples_per_second': 271.182, 'eval_steps_per_second': 17.084, 'epoch': 0.12}
{'loss': 0.9057, 'grad_norm': 2.099456787109375, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1565438508987427, 'eval_runtime': 3.6821, 'eval_samples_per_second': 271.586, 'eval_steps_per_second': 17.11, 'epoch': 0.16}
{'loss': 0.9038, 'grad_norm': 2.0632355213165283, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1255148649215698, 'eval_runtime': 3.6797, 'eval_samples_per_second': 271.764, 'eval_steps_per_second': 17.121, 'epoch': 0.2}
{'loss': 0.8647, 'grad_norm': 2.123157024383545, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.133722186088562, 'eval_runtime': 3.6862, 'eval_samples_per_second': 271.28, 'eval_steps_per_second': 17.091, 'epoch': 0.24}
{'loss': 0.847, 'grad_norm': 2.7955856323242188, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1125378608703613, 'eval_runtime': 3.6843, 'eval_samples_per_second': 271.425, 'eval_steps_per_second': 17.1, 'epoch': 0.28}
{'loss': 0.8539, 'grad_norm': 2.7688286304473877, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.206335186958313, 'eval_runtime': 3.6819, 'eval_samples_per_second': 271.602, 'eval_steps_per_second': 17.111, 'epoch': 0.32}
{'loss': 0.7863, 'grad_norm': 3.1786415576934814, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1531709432601929, 'eval_runtime': 3.6947, 'eval_samples_per_second': 270.659, 'eval_steps_per_second': 17.052, 'epoch': 0.36}
{'loss': 0.7736, 'grad_norm': 2.3135271072387695, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.189895510673523, 'eval_runtime': 3.6885, 'eval_samples_per_second': 271.113, 'eval_steps_per_second': 17.08, 'epoch': 0.4}
{'loss': 0.7648, 'grad_norm': 2.7749855518341064, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.150434136390686, 'eval_runtime': 3.688, 'eval_samples_per_second': 271.147, 'eval_steps_per_second': 17.082, 'epoch': 0.44}
{'loss': 0.7637, 'grad_norm': 2.3221468925476074, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.194745659828186, 'eval_runtime': 3.6865, 'eval_samples_per_second': 271.26, 'eval_steps_per_second': 17.089, 'epoch': 0.48}
{'loss': 0.7226, 'grad_norm': 3.1629574298858643, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.156964898109436, 'eval_runtime': 3.6896, 'eval_samples_per_second': 271.029, 'eval_steps_per_second': 17.075, 'epoch': 0.52}
{'loss': 0.6668, 'grad_norm': 2.3280656337738037, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2118384838104248, 'eval_runtime': 3.692, 'eval_samples_per_second': 270.858, 'eval_steps_per_second': 17.064, 'epoch': 0.56}
{'loss': 0.6777, 'grad_norm': 3.279195785522461, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1400233507156372, 'eval_runtime': 3.6844, 'eval_samples_per_second': 271.416, 'eval_steps_per_second': 17.099, 'epoch': 0.6}
{'loss': 0.6775, 'grad_norm': 3.235938549041748, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1776447296142578, 'eval_runtime': 3.6915, 'eval_samples_per_second': 270.896, 'eval_steps_per_second': 17.066, 'epoch': 0.64}
{'loss': 0.6347, 'grad_norm': 2.723588705062866, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1844263076782227, 'eval_runtime': 3.6952, 'eval_samples_per_second': 270.623, 'eval_steps_per_second': 17.049, 'epoch': 0.68}
{'loss': 0.6416, 'grad_norm': 5.006833553314209, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1944849491119385, 'eval_runtime': 3.6922, 'eval_samples_per_second': 270.838, 'eval_steps_per_second': 17.063, 'epoch': 0.72}
{'loss': 0.6156, 'grad_norm': 3.3366992473602295, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1812429428100586, 'eval_runtime': 3.6842, 'eval_samples_per_second': 271.432, 'eval_steps_per_second': 17.1, 'epoch': 0.76}
{'loss': 0.6156, 'grad_norm': 2.790201425552368, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2065942287445068, 'eval_runtime': 3.693, 'eval_samples_per_second': 270.782, 'eval_steps_per_second': 17.059, 'epoch': 0.8}
{'loss': 0.5418, 'grad_norm': 2.3762032985687256, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.201864242553711, 'eval_runtime': 3.6927, 'eval_samples_per_second': 270.801, 'eval_steps_per_second': 17.06, 'epoch': 0.84}
{'loss': 0.5881, 'grad_norm': 2.481858968734741, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2052624225616455, 'eval_runtime': 3.717, 'eval_samples_per_second': 269.034, 'eval_steps_per_second': 16.949, 'epoch': 0.88}
{'loss': 0.5629, 'grad_norm': 2.3629956245422363, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.19490647315979, 'eval_runtime': 3.695, 'eval_samples_per_second': 270.637, 'eval_steps_per_second': 17.05, 'epoch': 0.92}
{'loss': 0.5207, 'grad_norm': 1.9572144746780396, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.183465600013733, 'eval_runtime': 3.6948, 'eval_samples_per_second': 270.651, 'eval_steps_per_second': 17.051, 'epoch': 0.96}
{'loss': 0.5346, 'grad_norm': 2.4146015644073486, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.192497968673706, 'eval_runtime': 3.6994, 'eval_samples_per_second': 270.312, 'eval_steps_per_second': 17.03, 'epoch': 1.0}
{'train_runtime': 296.8251, 'train_samples_per_second': 33.683, 'train_steps_per_second': 2.106, 'train_loss': 0.8025854934692382, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3690892457962036, 1.2344310283660889, 1.1542541980743408, 1.1565438508987427, 1.1255148649215698, 1.133722186088562, 1.1125378608703613, 1.206335186958313, 1.1531709432601929, 1.189895510673523, 1.150434136390686, 1.194745659828186, 1.156964898109436, 1.2118384838104248, 1.1400233507156372, 1.1776447296142578, 1.1844263076782227, 1.1944849491119385, 1.1812429428100586, 1.2065942287445068, 1.201864242553711, 1.2052624225616455, 1.19490647315979, 1.183465600013733, 1.192497968673706], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3690892457962036, 1.2344310283660889, 1.1542541980743408, 1.1565438508987427, 1.1255148649215698, 1.133722186088562, 1.1125378608703613, 1.206335186958313, 1.1531709432601929, 1.189895510673523, 1.150434136390686, 1.194745659828186, 1.156964898109436, 1.2118384838104248, 1.1400233507156372, 1.1776447296142578, 1.1844263076782227, 1.1944849491119385, 1.1812429428100586, 1.2065942287445068, 1.201864242553711, 1.2052624225616455, 1.19490647315979, 1.183465600013733, 1.192497968673706]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -1.192497968673706
max eval_loss so far:  -1.192497968673706
BO observations:  [-1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8462 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.8403239869164603
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.8413314977268667
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.8413043590282945
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.8414969395549388
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.8712040114209862
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4746, dtype=torch.float64), 0, 0, 0, tensor(0.1256, dtype=torch.float64), 0, tensor(0.1512, dtype=torch.float64), tensor(0.2486, dtype=torch.float64), 21, 0, 0, 0, 1, 1, 128, 7.301197257607335e-20, 29.195137616014208, 1]
normalized proposed parameters for next round by BO: [tensor(2.8257e-17, dtype=torch.float64), tensor(0.4746, dtype=torch.float64), tensor(1.8527e-18, dtype=torch.float64), tensor(1.8972e-17, dtype=torch.float64), tensor(2.7336e-17, dtype=torch.float64), tensor(0.1256, dtype=torch.float64), tensor(2.6084e-18, dtype=torch.float64), tensor(0.1512, dtype=torch.float64), tensor(0.2486, dtype=torch.float64), tensor(0.6540, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(7.3012e-19, dtype=torch.float64), tensor(0.6082, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.475
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.126
  wikitext: 0
  mmlu: 0.151
  arc_challenge: 0.249

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (7.301197257607335e-20,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (29.195137616014208,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  128
lora dropout:  7.301197257607335e-20
lora alpha:  29.195137616014208
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 99,090,432 || all params: 8,129,351,680 || trainable%: 1.2189
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4343, 'grad_norm': 0.6152333617210388, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.912038803100586, 'eval_runtime': 3.3348, 'eval_samples_per_second': 299.868, 'eval_steps_per_second': 18.892, 'epoch': 0.04}
{'loss': 1.2463, 'grad_norm': 0.40032434463500977, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1468650102615356, 'eval_runtime': 3.3266, 'eval_samples_per_second': 300.603, 'eval_steps_per_second': 18.938, 'epoch': 0.08}
{'loss': 1.0231, 'grad_norm': 0.1481902152299881, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0053797960281372, 'eval_runtime': 3.3323, 'eval_samples_per_second': 300.092, 'eval_steps_per_second': 18.906, 'epoch': 0.12}
{'loss': 0.9834, 'grad_norm': 0.17899517714977264, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9151690602302551, 'eval_runtime': 3.3775, 'eval_samples_per_second': 296.074, 'eval_steps_per_second': 18.653, 'epoch': 0.16}
{'loss': 0.9764, 'grad_norm': 0.19349312782287598, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8445462584495544, 'eval_runtime': 3.3846, 'eval_samples_per_second': 295.458, 'eval_steps_per_second': 18.614, 'epoch': 0.2}
{'loss': 0.9571, 'grad_norm': 0.4202999174594879, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7991851568222046, 'eval_runtime': 3.3805, 'eval_samples_per_second': 295.81, 'eval_steps_per_second': 18.636, 'epoch': 0.24}
{'loss': 0.8951, 'grad_norm': 0.18757264316082, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7879631519317627, 'eval_runtime': 3.3951, 'eval_samples_per_second': 294.545, 'eval_steps_per_second': 18.556, 'epoch': 0.28}
{'loss': 0.8828, 'grad_norm': 0.14918065071105957, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.765739917755127, 'eval_runtime': 3.4, 'eval_samples_per_second': 294.113, 'eval_steps_per_second': 18.529, 'epoch': 0.32}
{'loss': 0.9159, 'grad_norm': 0.1496417373418808, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7478600740432739, 'eval_runtime': 3.3813, 'eval_samples_per_second': 295.747, 'eval_steps_per_second': 18.632, 'epoch': 0.36}
{'loss': 0.8694, 'grad_norm': 0.14975443482398987, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7487215995788574, 'eval_runtime': 3.3873, 'eval_samples_per_second': 295.22, 'eval_steps_per_second': 18.599, 'epoch': 0.4}
{'loss': 0.896, 'grad_norm': 0.17854976654052734, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7238878011703491, 'eval_runtime': 3.4222, 'eval_samples_per_second': 292.211, 'eval_steps_per_second': 18.409, 'epoch': 0.44}
{'loss': 0.8629, 'grad_norm': 0.1636449694633484, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7084328532218933, 'eval_runtime': 3.3962, 'eval_samples_per_second': 294.446, 'eval_steps_per_second': 18.55, 'epoch': 0.48}
{'loss': 0.8921, 'grad_norm': 0.1714484989643097, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6971399188041687, 'eval_runtime': 3.388, 'eval_samples_per_second': 295.163, 'eval_steps_per_second': 18.595, 'epoch': 0.52}
{'loss': 0.8672, 'grad_norm': 0.17422981560230255, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6949000358581543, 'eval_runtime': 3.3788, 'eval_samples_per_second': 295.96, 'eval_steps_per_second': 18.645, 'epoch': 0.56}
{'loss': 0.8829, 'grad_norm': 0.17819271981716156, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6842817664146423, 'eval_runtime': 3.376, 'eval_samples_per_second': 296.207, 'eval_steps_per_second': 18.661, 'epoch': 0.6}
{'loss': 0.8651, 'grad_norm': 0.199750617146492, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6853549480438232, 'eval_runtime': 3.3675, 'eval_samples_per_second': 296.954, 'eval_steps_per_second': 18.708, 'epoch': 0.64}
{'loss': 0.8504, 'grad_norm': 0.16811123490333557, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6714736223220825, 'eval_runtime': 3.3649, 'eval_samples_per_second': 297.187, 'eval_steps_per_second': 18.723, 'epoch': 0.68}
{'loss': 0.8419, 'grad_norm': 0.17223654687404633, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6569732427597046, 'eval_runtime': 3.367, 'eval_samples_per_second': 296.999, 'eval_steps_per_second': 18.711, 'epoch': 0.72}
{'loss': 0.8546, 'grad_norm': 0.16640831530094147, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6415058374404907, 'eval_runtime': 3.3771, 'eval_samples_per_second': 296.108, 'eval_steps_per_second': 18.655, 'epoch': 0.76}
{'loss': 0.8704, 'grad_norm': 0.16751952469348907, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6443633437156677, 'eval_runtime': 3.3734, 'eval_samples_per_second': 296.44, 'eval_steps_per_second': 18.676, 'epoch': 0.8}
{'loss': 0.8226, 'grad_norm': 0.1708376705646515, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6373868584632874, 'eval_runtime': 3.3718, 'eval_samples_per_second': 296.579, 'eval_steps_per_second': 18.684, 'epoch': 0.84}
{'loss': 0.868, 'grad_norm': 0.19902004301548004, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6295393109321594, 'eval_runtime': 3.3682, 'eval_samples_per_second': 296.897, 'eval_steps_per_second': 18.704, 'epoch': 0.88}
{'loss': 0.8307, 'grad_norm': 0.5414171814918518, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6228663325309753, 'eval_runtime': 3.3585, 'eval_samples_per_second': 297.75, 'eval_steps_per_second': 18.758, 'epoch': 0.92}
{'loss': 0.8276, 'grad_norm': 0.18103468418121338, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6184463500976562, 'eval_runtime': 3.3541, 'eval_samples_per_second': 298.145, 'eval_steps_per_second': 18.783, 'epoch': 0.96}
{'loss': 0.8308, 'grad_norm': 0.18843387067317963, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6183189153671265, 'eval_runtime': 3.3526, 'eval_samples_per_second': 298.277, 'eval_steps_per_second': 18.791, 'epoch': 1.0}
{'train_runtime': 300.4597, 'train_samples_per_second': 33.276, 'train_steps_per_second': 2.08, 'train_loss': 0.9618887969970703, 'epoch': 1.0}
train_results:  {'eval_loss': [1.912038803100586, 1.1468650102615356, 1.0053797960281372, 0.9151690602302551, 0.8445462584495544, 0.7991851568222046, 0.7879631519317627, 0.765739917755127, 0.7478600740432739, 0.7487215995788574, 0.7238878011703491, 0.7084328532218933, 0.6971399188041687, 0.6949000358581543, 0.6842817664146423, 0.6853549480438232, 0.6714736223220825, 0.6569732427597046, 0.6415058374404907, 0.6443633437156677, 0.6373868584632874, 0.6295393109321594, 0.6228663325309753, 0.6184463500976562, 0.6183189153671265], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.912038803100586, 1.1468650102615356, 1.0053797960281372, 0.9151690602302551, 0.8445462584495544, 0.7991851568222046, 0.7879631519317627, 0.765739917755127, 0.7478600740432739, 0.7487215995788574, 0.7238878011703491, 0.7084328532218933, 0.6971399188041687, 0.6949000358581543, 0.6842817664146423, 0.6853549480438232, 0.6714736223220825, 0.6569732427597046, 0.6415058374404907, 0.6443633437156677, 0.6373868584632874, 0.6295393109321594, 0.6228663325309753, 0.6184463500976562, 0.6183189153671265]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.6183189153671265
max eval_loss so far:  -0.6183189153671265
BO observations:  [-1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.7927 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.8560256870894472
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.8519944483000964
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.8519944480398347
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.8558552940447399
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.8521118112209283
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0233, dtype=torch.float64), tensor(0.3468, dtype=torch.float64), tensor(0.0107, dtype=torch.float64), tensor(0.2944, dtype=torch.float64), 0, tensor(0.0648, dtype=torch.float64), tensor(0.2600, dtype=torch.float64), 15, 0, 0, 0, 1, 1, 128, 0.08155121370978102, 27.559310884646173, 1]
normalized proposed parameters for next round by BO: [tensor(1.0446e-17, dtype=torch.float64), tensor(1.7544e-18, dtype=torch.float64), tensor(0.0233, dtype=torch.float64), tensor(0.3468, dtype=torch.float64), tensor(0.0107, dtype=torch.float64), tensor(0.2944, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0648, dtype=torch.float64), tensor(0.2600, dtype=torch.float64), tensor(0.4721, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8155, dtype=torch.float64), tensor(0.5742, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.023
  sciq: 0.347
  triviaqa: 0.011
  truthfulqa_gen: 0.294
  wikitext: 0
  mmlu: 0.065
  arc_challenge: 0.26

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.08155121370978102,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (27.559310884646173,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  128
lora dropout:  0.08155121370978102
lora alpha:  27.559310884646173
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 70,778,880 || all params: 8,101,040,128 || trainable%: 0.8737
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5728, 'grad_norm': 1.1924549341201782, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.912475824356079, 'eval_runtime': 3.2441, 'eval_samples_per_second': 308.254, 'eval_steps_per_second': 19.42, 'epoch': 0.04}
{'loss': 1.4466, 'grad_norm': 0.6093529462814331, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0842255353927612, 'eval_runtime': 3.2482, 'eval_samples_per_second': 307.86, 'eval_steps_per_second': 19.395, 'epoch': 0.08}
{'loss': 1.1496, 'grad_norm': 0.30051499605178833, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9399561882019043, 'eval_runtime': 3.2419, 'eval_samples_per_second': 308.466, 'eval_steps_per_second': 19.433, 'epoch': 0.12}
{'loss': 1.063, 'grad_norm': 0.276994526386261, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8451589941978455, 'eval_runtime': 3.2493, 'eval_samples_per_second': 307.76, 'eval_steps_per_second': 19.389, 'epoch': 0.16}
{'loss': 1.0086, 'grad_norm': 0.3945399820804596, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.792733907699585, 'eval_runtime': 3.2578, 'eval_samples_per_second': 306.957, 'eval_steps_per_second': 19.338, 'epoch': 0.2}
{'loss': 0.9047, 'grad_norm': 0.29496610164642334, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7542892098426819, 'eval_runtime': 3.2564, 'eval_samples_per_second': 307.085, 'eval_steps_per_second': 19.346, 'epoch': 0.24}
{'loss': 0.9452, 'grad_norm': 0.25802749395370483, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7066146731376648, 'eval_runtime': 3.2635, 'eval_samples_per_second': 306.419, 'eval_steps_per_second': 19.304, 'epoch': 0.28}
{'loss': 0.9437, 'grad_norm': 0.25556474924087524, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.712241530418396, 'eval_runtime': 3.2633, 'eval_samples_per_second': 306.438, 'eval_steps_per_second': 19.306, 'epoch': 0.32}
{'loss': 0.8851, 'grad_norm': 0.20448318123817444, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.68074631690979, 'eval_runtime': 3.2687, 'eval_samples_per_second': 305.927, 'eval_steps_per_second': 19.273, 'epoch': 0.36}
{'loss': 0.8742, 'grad_norm': 0.21109935641288757, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6610645651817322, 'eval_runtime': 3.2631, 'eval_samples_per_second': 306.457, 'eval_steps_per_second': 19.307, 'epoch': 0.4}
{'loss': 0.9052, 'grad_norm': 0.23676545917987823, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6415525674819946, 'eval_runtime': 3.2639, 'eval_samples_per_second': 306.382, 'eval_steps_per_second': 19.302, 'epoch': 0.44}
{'loss': 0.8619, 'grad_norm': 0.23077556490898132, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6284247040748596, 'eval_runtime': 3.2685, 'eval_samples_per_second': 305.954, 'eval_steps_per_second': 19.275, 'epoch': 0.48}
{'loss': 0.8597, 'grad_norm': 0.30132046341896057, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6242413520812988, 'eval_runtime': 3.2675, 'eval_samples_per_second': 306.046, 'eval_steps_per_second': 19.281, 'epoch': 0.52}
{'loss': 0.8312, 'grad_norm': 0.24207758903503418, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5915195345878601, 'eval_runtime': 3.2682, 'eval_samples_per_second': 305.983, 'eval_steps_per_second': 19.277, 'epoch': 0.56}
{'loss': 0.8554, 'grad_norm': 0.2089274823665619, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5832812786102295, 'eval_runtime': 3.2642, 'eval_samples_per_second': 306.35, 'eval_steps_per_second': 19.3, 'epoch': 0.6}
{'loss': 0.8592, 'grad_norm': 0.2633516490459442, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5757092237472534, 'eval_runtime': 3.2735, 'eval_samples_per_second': 305.486, 'eval_steps_per_second': 19.246, 'epoch': 0.64}
{'loss': 0.8112, 'grad_norm': 0.25097811222076416, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5473941564559937, 'eval_runtime': 3.2702, 'eval_samples_per_second': 305.791, 'eval_steps_per_second': 19.265, 'epoch': 0.68}
{'loss': 0.8296, 'grad_norm': 0.24497728049755096, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5409479141235352, 'eval_runtime': 3.2721, 'eval_samples_per_second': 305.611, 'eval_steps_per_second': 19.253, 'epoch': 0.72}
{'loss': 0.8707, 'grad_norm': 0.22015409171581268, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5153408050537109, 'eval_runtime': 3.2698, 'eval_samples_per_second': 305.826, 'eval_steps_per_second': 19.267, 'epoch': 0.76}
{'loss': 0.7954, 'grad_norm': 0.265238881111145, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5062401294708252, 'eval_runtime': 3.271, 'eval_samples_per_second': 305.717, 'eval_steps_per_second': 19.26, 'epoch': 0.8}
{'loss': 0.7966, 'grad_norm': 0.300800621509552, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4897579252719879, 'eval_runtime': 3.2771, 'eval_samples_per_second': 305.144, 'eval_steps_per_second': 19.224, 'epoch': 0.84}
{'loss': 0.7741, 'grad_norm': 0.2683536112308502, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.47484290599823, 'eval_runtime': 3.2752, 'eval_samples_per_second': 305.327, 'eval_steps_per_second': 19.236, 'epoch': 0.88}
{'loss': 0.7288, 'grad_norm': 0.2765739858150482, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.46766194701194763, 'eval_runtime': 3.2929, 'eval_samples_per_second': 303.682, 'eval_steps_per_second': 19.132, 'epoch': 0.92}
{'loss': 0.7909, 'grad_norm': 0.2488533854484558, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.46109288930892944, 'eval_runtime': 3.2707, 'eval_samples_per_second': 305.745, 'eval_steps_per_second': 19.262, 'epoch': 0.96}
{'loss': 0.8072, 'grad_norm': 0.3451295495033264, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.4600749611854553, 'eval_runtime': 3.2704, 'eval_samples_per_second': 305.775, 'eval_steps_per_second': 19.264, 'epoch': 1.0}
{'train_runtime': 241.3061, 'train_samples_per_second': 41.425, 'train_steps_per_second': 2.59, 'train_loss': 1.006820297241211, 'epoch': 1.0}
train_results:  {'eval_loss': [1.912475824356079, 1.0842255353927612, 0.9399561882019043, 0.8451589941978455, 0.792733907699585, 0.7542892098426819, 0.7066146731376648, 0.712241530418396, 0.68074631690979, 0.6610645651817322, 0.6415525674819946, 0.6284247040748596, 0.6242413520812988, 0.5915195345878601, 0.5832812786102295, 0.5757092237472534, 0.5473941564559937, 0.5409479141235352, 0.5153408050537109, 0.5062401294708252, 0.4897579252719879, 0.47484290599823, 0.46766194701194763, 0.46109288930892944, 0.4600749611854553], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.912475824356079, 1.0842255353927612, 0.9399561882019043, 0.8451589941978455, 0.792733907699585, 0.7542892098426819, 0.7066146731376648, 0.712241530418396, 0.68074631690979, 0.6610645651817322, 0.6415525674819946, 0.6284247040748596, 0.6242413520812988, 0.5915195345878601, 0.5832812786102295, 0.5757092237472534, 0.5473941564559937, 0.5409479141235352, 0.5153408050537109, 0.5062401294708252, 0.4897579252719879, 0.47484290599823, 0.46766194701194763, 0.46109288930892944, 0.4600749611854553]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.4600749611854553
max eval_loss so far:  -0.4600749611854553
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.1607 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.9064753921955158
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.9138413997672173
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.9138676246280073
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.9138406288729586
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.9138410494596885
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2940, dtype=torch.float64), tensor(0.1183, dtype=torch.float64), tensor(0.0474, dtype=torch.float64), tensor(0.1349, dtype=torch.float64), tensor(0.0444, dtype=torch.float64), tensor(0.1206, dtype=torch.float64), 0, 0, tensor(0.2405, dtype=torch.float64), 23, 1, 1, 1, 0, 1, 75, 1.7347234759768072e-19, 34.290363248714364, 1]
normalized proposed parameters for next round by BO: [tensor(0.2940, dtype=torch.float64), tensor(0.1183, dtype=torch.float64), tensor(0.0474, dtype=torch.float64), tensor(0.1349, dtype=torch.float64), tensor(0.0444, dtype=torch.float64), tensor(0.1206, dtype=torch.float64), tensor(7.4576e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2405, dtype=torch.float64), tensor(0.7055, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5857, dtype=torch.float64), tensor(1.7347e-18, dtype=torch.float64), tensor(0.7144, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.294
  gsm8k: 0.118
  rowan_hellaswag: 0.047
  sciq: 0.135
  triviaqa: 0.044
  truthfulqa_gen: 0.121
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.241

LoRA Parameters:
  lora_r: (75,)
  lora_dropout: (1.7347234759768072e-19,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (34.290363248714364,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  75
lora dropout:  1.7347234759768072e-19
lora alpha:  34.290363248714364
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 86,553,600 || all params: 8,116,814,848 || trainable%: 1.0663
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9686, 'grad_norm': 0.6356469988822937, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7300180196762085, 'eval_runtime': 3.7233, 'eval_samples_per_second': 268.576, 'eval_steps_per_second': 16.92, 'epoch': 0.04}
{'loss': 1.3406, 'grad_norm': 0.27473151683807373, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1975834369659424, 'eval_runtime': 3.7207, 'eval_samples_per_second': 268.765, 'eval_steps_per_second': 16.932, 'epoch': 0.08}
{'loss': 1.201, 'grad_norm': 0.29271718859672546, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0687048435211182, 'eval_runtime': 3.7242, 'eval_samples_per_second': 268.511, 'eval_steps_per_second': 16.916, 'epoch': 0.12}
{'loss': 1.1216, 'grad_norm': 0.3070634603500366, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9455744624137878, 'eval_runtime': 3.7236, 'eval_samples_per_second': 268.556, 'eval_steps_per_second': 16.919, 'epoch': 0.16}
{'loss': 1.0332, 'grad_norm': 0.2298533022403717, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9502925276756287, 'eval_runtime': 3.7163, 'eval_samples_per_second': 269.085, 'eval_steps_per_second': 16.952, 'epoch': 0.2}
{'loss': 1.094, 'grad_norm': 0.2120051085948944, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9020302891731262, 'eval_runtime': 3.7184, 'eval_samples_per_second': 268.934, 'eval_steps_per_second': 16.943, 'epoch': 0.24}
{'loss': 0.9959, 'grad_norm': 0.2331387996673584, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8687598705291748, 'eval_runtime': 3.7142, 'eval_samples_per_second': 269.24, 'eval_steps_per_second': 16.962, 'epoch': 0.28}
{'loss': 0.9952, 'grad_norm': 0.2926267981529236, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8292068243026733, 'eval_runtime': 3.7206, 'eval_samples_per_second': 268.776, 'eval_steps_per_second': 16.933, 'epoch': 0.32}
{'loss': 0.9627, 'grad_norm': 0.2657451033592224, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7415000796318054, 'eval_runtime': 3.7434, 'eval_samples_per_second': 267.135, 'eval_steps_per_second': 16.83, 'epoch': 0.36}
{'loss': 0.9807, 'grad_norm': 0.22434960305690765, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7134114503860474, 'eval_runtime': 3.72, 'eval_samples_per_second': 268.815, 'eval_steps_per_second': 16.935, 'epoch': 0.4}
{'loss': 0.8906, 'grad_norm': 0.2600264251232147, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6975146532058716, 'eval_runtime': 3.7203, 'eval_samples_per_second': 268.799, 'eval_steps_per_second': 16.934, 'epoch': 0.44}
{'loss': 0.8994, 'grad_norm': 0.2011175900697708, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6713140606880188, 'eval_runtime': 3.7225, 'eval_samples_per_second': 268.635, 'eval_steps_per_second': 16.924, 'epoch': 0.48}
{'loss': 0.8812, 'grad_norm': 0.25091567635536194, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6628962755203247, 'eval_runtime': 3.738, 'eval_samples_per_second': 267.526, 'eval_steps_per_second': 16.854, 'epoch': 0.52}
{'loss': 0.9044, 'grad_norm': 0.2756977081298828, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6679626703262329, 'eval_runtime': 3.7219, 'eval_samples_per_second': 268.677, 'eval_steps_per_second': 16.927, 'epoch': 0.56}
{'loss': 0.8663, 'grad_norm': 0.26610708236694336, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6513899564743042, 'eval_runtime': 3.7256, 'eval_samples_per_second': 268.415, 'eval_steps_per_second': 16.91, 'epoch': 0.6}
{'loss': 0.8629, 'grad_norm': 0.23426510393619537, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6253066658973694, 'eval_runtime': 3.7186, 'eval_samples_per_second': 268.919, 'eval_steps_per_second': 16.942, 'epoch': 0.64}
{'loss': 0.8676, 'grad_norm': 0.2722325623035431, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6163128614425659, 'eval_runtime': 3.7214, 'eval_samples_per_second': 268.719, 'eval_steps_per_second': 16.929, 'epoch': 0.68}
{'loss': 0.8921, 'grad_norm': 0.30097970366477966, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6177875995635986, 'eval_runtime': 3.745, 'eval_samples_per_second': 267.023, 'eval_steps_per_second': 16.822, 'epoch': 0.72}
{'loss': 0.8579, 'grad_norm': 0.29541662335395813, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5983685851097107, 'eval_runtime': 3.7484, 'eval_samples_per_second': 266.784, 'eval_steps_per_second': 16.807, 'epoch': 0.76}
{'loss': 0.8306, 'grad_norm': 0.34709882736206055, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5941746234893799, 'eval_runtime': 3.7358, 'eval_samples_per_second': 267.678, 'eval_steps_per_second': 16.864, 'epoch': 0.8}
{'loss': 0.8572, 'grad_norm': 0.25320884585380554, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5866680145263672, 'eval_runtime': 3.7317, 'eval_samples_per_second': 267.975, 'eval_steps_per_second': 16.882, 'epoch': 0.84}
{'loss': 0.8203, 'grad_norm': 0.27471649646759033, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5755992531776428, 'eval_runtime': 3.7562, 'eval_samples_per_second': 266.229, 'eval_steps_per_second': 16.772, 'epoch': 0.88}
{'loss': 0.8324, 'grad_norm': 0.27912139892578125, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5766872763633728, 'eval_runtime': 3.7651, 'eval_samples_per_second': 265.598, 'eval_steps_per_second': 16.733, 'epoch': 0.92}
{'loss': 0.8625, 'grad_norm': 0.3453005254268646, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5683674216270447, 'eval_runtime': 3.7456, 'eval_samples_per_second': 266.979, 'eval_steps_per_second': 16.82, 'epoch': 0.96}
{'loss': 0.862, 'grad_norm': 0.3259231746196747, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.565803050994873, 'eval_runtime': 3.7339, 'eval_samples_per_second': 267.815, 'eval_steps_per_second': 16.872, 'epoch': 1.0}
{'train_runtime': 313.462, 'train_samples_per_second': 31.889, 'train_steps_per_second': 1.994, 'train_loss': 1.0272394622802734, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7300180196762085, 1.1975834369659424, 1.0687048435211182, 0.9455744624137878, 0.9502925276756287, 0.9020302891731262, 0.8687598705291748, 0.8292068243026733, 0.7415000796318054, 0.7134114503860474, 0.6975146532058716, 0.6713140606880188, 0.6628962755203247, 0.6679626703262329, 0.6513899564743042, 0.6253066658973694, 0.6163128614425659, 0.6177875995635986, 0.5983685851097107, 0.5941746234893799, 0.5866680145263672, 0.5755992531776428, 0.5766872763633728, 0.5683674216270447, 0.565803050994873], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7300180196762085, 1.1975834369659424, 1.0687048435211182, 0.9455744624137878, 0.9502925276756287, 0.9020302891731262, 0.8687598705291748, 0.8292068243026733, 0.7415000796318054, 0.7134114503860474, 0.6975146532058716, 0.6713140606880188, 0.6628962755203247, 0.6679626703262329, 0.6513899564743042, 0.6253066658973694, 0.6163128614425659, 0.6177875995635986, 0.5983685851097107, 0.5941746234893799, 0.5866680145263672, 0.5755992531776428, 0.5766872763633728, 0.5683674216270447, 0.565803050994873]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1238232851028442
current iteration best possible eval_loss (full train run):  -0.565803050994873
max eval_loss so far:  -0.4600749611854553
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2124 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.9040784216065116
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.9041168147179508
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.9039818394269459
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.9039818157897328
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.9039818949877332
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5179, dtype=torch.float64), tensor(0.0307, dtype=torch.float64), tensor(0.2096, dtype=torch.float64), 0, tensor(0.0976, dtype=torch.float64), 0, tensor(0.0868, dtype=torch.float64), tensor(0.0574, dtype=torch.float64), 14, 1, 1, 0, 1, 1, 56, 0.0, 29.643640833149007, 1]
normalized proposed parameters for next round by BO: [tensor(2.2467e-17, dtype=torch.float64), tensor(0.5179, dtype=torch.float64), tensor(0.0307, dtype=torch.float64), tensor(0.2096, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0976, dtype=torch.float64), tensor(5.1547e-17, dtype=torch.float64), tensor(0.0868, dtype=torch.float64), tensor(0.0574, dtype=torch.float64), tensor(0.4380, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4351, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6176, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.518
  rowan_hellaswag: 0.031
  sciq: 0.21
  triviaqa: 0
  truthfulqa_gen: 0.098
  wikitext: 0
  mmlu: 0.087
  arc_challenge: 0.057

LoRA Parameters:
  lora_r: (56,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (29.643640833149007,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  56
lora dropout:  0.0
lora alpha:  29.643640833149007
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 39,337,984 || all params: 8,069,599,232 || trainable%: 0.4875
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.458, 'grad_norm': 1.056628704071045, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8205963373184204, 'eval_runtime': 3.3775, 'eval_samples_per_second': 296.076, 'eval_steps_per_second': 18.653, 'epoch': 0.04}
{'loss': 1.2641, 'grad_norm': 0.628660261631012, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1299880743026733, 'eval_runtime': 3.3232, 'eval_samples_per_second': 300.917, 'eval_steps_per_second': 18.958, 'epoch': 0.08}
{'loss': 1.0672, 'grad_norm': 0.39403146505355835, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9948384761810303, 'eval_runtime': 3.3315, 'eval_samples_per_second': 300.163, 'eval_steps_per_second': 18.91, 'epoch': 0.12}
{'loss': 1.0158, 'grad_norm': 0.3234192728996277, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9625282883644104, 'eval_runtime': 3.3386, 'eval_samples_per_second': 299.527, 'eval_steps_per_second': 18.87, 'epoch': 0.16}
{'loss': 1.0194, 'grad_norm': 0.3743339776992798, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8626078963279724, 'eval_runtime': 3.3422, 'eval_samples_per_second': 299.208, 'eval_steps_per_second': 18.85, 'epoch': 0.2}
{'loss': 0.9374, 'grad_norm': 0.29477235674858093, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8250328302383423, 'eval_runtime': 3.3465, 'eval_samples_per_second': 298.823, 'eval_steps_per_second': 18.826, 'epoch': 0.24}
{'loss': 0.9324, 'grad_norm': 0.2612835168838501, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8213654160499573, 'eval_runtime': 3.3491, 'eval_samples_per_second': 298.584, 'eval_steps_per_second': 18.811, 'epoch': 0.28}
{'loss': 0.9141, 'grad_norm': 0.2725121080875397, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8267653584480286, 'eval_runtime': 3.352, 'eval_samples_per_second': 298.329, 'eval_steps_per_second': 18.795, 'epoch': 0.32}
{'loss': 0.9395, 'grad_norm': 0.2765251398086548, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7955832481384277, 'eval_runtime': 3.3486, 'eval_samples_per_second': 298.636, 'eval_steps_per_second': 18.814, 'epoch': 0.36}
{'loss': 0.9137, 'grad_norm': 0.31465551257133484, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7843531370162964, 'eval_runtime': 3.3472, 'eval_samples_per_second': 298.754, 'eval_steps_per_second': 18.821, 'epoch': 0.4}
{'loss': 1.0079, 'grad_norm': 0.2852129340171814, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7738449573516846, 'eval_runtime': 3.3527, 'eval_samples_per_second': 298.269, 'eval_steps_per_second': 18.791, 'epoch': 0.44}
{'loss': 0.8937, 'grad_norm': 0.5104098320007324, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7981851696968079, 'eval_runtime': 3.3474, 'eval_samples_per_second': 298.739, 'eval_steps_per_second': 18.821, 'epoch': 0.48}
{'loss': 0.9217, 'grad_norm': 0.23490090668201447, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7657380104064941, 'eval_runtime': 3.3536, 'eval_samples_per_second': 298.19, 'eval_steps_per_second': 18.786, 'epoch': 0.52}
{'loss': 0.8856, 'grad_norm': 0.22409288585186005, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7513087391853333, 'eval_runtime': 3.3477, 'eval_samples_per_second': 298.711, 'eval_steps_per_second': 18.819, 'epoch': 0.56}
{'loss': 0.8739, 'grad_norm': 0.3218670189380646, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7514719367027283, 'eval_runtime': 3.3468, 'eval_samples_per_second': 298.796, 'eval_steps_per_second': 18.824, 'epoch': 0.6}
{'loss': 0.9163, 'grad_norm': 0.2291259616613388, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7501835823059082, 'eval_runtime': 3.3563, 'eval_samples_per_second': 297.944, 'eval_steps_per_second': 18.77, 'epoch': 0.64}
{'loss': 0.8514, 'grad_norm': 0.24820666015148163, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7364563941955566, 'eval_runtime': 3.3615, 'eval_samples_per_second': 297.483, 'eval_steps_per_second': 18.741, 'epoch': 0.68}
{'loss': 0.8828, 'grad_norm': 0.27419131994247437, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7244972586631775, 'eval_runtime': 3.3529, 'eval_samples_per_second': 298.25, 'eval_steps_per_second': 18.79, 'epoch': 0.72}
{'loss': 0.8809, 'grad_norm': 0.24344468116760254, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7361034154891968, 'eval_runtime': 3.3577, 'eval_samples_per_second': 297.823, 'eval_steps_per_second': 18.763, 'epoch': 0.76}
{'loss': 0.9284, 'grad_norm': 0.2891395092010498, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7350890040397644, 'eval_runtime': 3.354, 'eval_samples_per_second': 298.154, 'eval_steps_per_second': 18.784, 'epoch': 0.8}
{'loss': 0.8693, 'grad_norm': 0.2855519652366638, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7174395322799683, 'eval_runtime': 3.3523, 'eval_samples_per_second': 298.301, 'eval_steps_per_second': 18.793, 'epoch': 0.84}
{'loss': 0.8689, 'grad_norm': 0.23277463018894196, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7106981873512268, 'eval_runtime': 3.3514, 'eval_samples_per_second': 298.385, 'eval_steps_per_second': 18.798, 'epoch': 0.88}
{'loss': 0.873, 'grad_norm': 0.26101818680763245, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7113212943077087, 'eval_runtime': 3.3501, 'eval_samples_per_second': 298.499, 'eval_steps_per_second': 18.805, 'epoch': 0.92}
{'loss': 0.9103, 'grad_norm': 0.25166502594947815, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7059839963912964, 'eval_runtime': 3.3718, 'eval_samples_per_second': 296.577, 'eval_steps_per_second': 18.684, 'epoch': 0.96}
{'loss': 0.8995, 'grad_norm': 0.29325684905052185, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7073624134063721, 'eval_runtime': 3.3536, 'eval_samples_per_second': 298.183, 'eval_steps_per_second': 18.786, 'epoch': 1.0}
{'train_runtime': 296.4191, 'train_samples_per_second': 33.726, 'train_steps_per_second': 2.109, 'train_loss': 0.9970061309814453, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8205963373184204, 1.1299880743026733, 0.9948384761810303, 0.9625282883644104, 0.8626078963279724, 0.8250328302383423, 0.8213654160499573, 0.8267653584480286, 0.7955832481384277, 0.7843531370162964, 0.7738449573516846, 0.7981851696968079, 0.7657380104064941, 0.7513087391853333, 0.7514719367027283, 0.7501835823059082, 0.7364563941955566, 0.7244972586631775, 0.7361034154891968, 0.7350890040397644, 0.7174395322799683, 0.7106981873512268, 0.7113212943077087, 0.7059839963912964, 0.7073624134063721], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8205963373184204, 1.1299880743026733, 0.9948384761810303, 0.9625282883644104, 0.8626078963279724, 0.8250328302383423, 0.8213654160499573, 0.8267653584480286, 0.7955832481384277, 0.7843531370162964, 0.7738449573516846, 0.7981851696968079, 0.7657380104064941, 0.7513087391853333, 0.7514719367027283, 0.7501835823059082, 0.7364563941955566, 0.7244972586631775, 0.7361034154891968, 0.7350890040397644, 0.7174395322799683, 0.7106981873512268, 0.7113212943077087, 0.7059839963912964, 0.7073624134063721]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1264610290527344
current iteration best possible eval_loss (full train run):  -0.7073624134063721
max eval_loss so far:  -0.4600749611854553
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3250 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.9246162554921076
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.9246213604112179
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.9246213603869036
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.9246163297659692
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.9246213605160002
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1265, dtype=torch.float64), 0, tensor(0.0527, dtype=torch.float64), tensor(0.1190, dtype=torch.float64), tensor(0.2474, dtype=torch.float64), 0, tensor(0.0962, dtype=torch.float64), tensor(0.3553, dtype=torch.float64), 13, 0, 1, 0, 1, 1, 75, 4.4651782271643e-19, 27.96127110891487, 1]
normalized proposed parameters for next round by BO: [tensor(2.3003e-17, dtype=torch.float64), tensor(0.1265, dtype=torch.float64), tensor(0.0029, dtype=torch.float64), tensor(0.0527, dtype=torch.float64), tensor(0.1190, dtype=torch.float64), tensor(0.2474, dtype=torch.float64), tensor(2.7314e-18, dtype=torch.float64), tensor(0.0962, dtype=torch.float64), tensor(0.3553, dtype=torch.float64), tensor(0.4098, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5873, dtype=torch.float64), tensor(4.4652e-18, dtype=torch.float64), tensor(0.5825, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.127
  rowan_hellaswag: 0
  sciq: 0.053
  triviaqa: 0.119
  truthfulqa_gen: 0.247
  wikitext: 0
  mmlu: 0.096
  arc_challenge: 0.355

LoRA Parameters:
  lora_r: (75,)
  lora_dropout: (4.4651782271643e-19,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (27.96127110891487,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  75
lora dropout:  4.4651782271643e-19
lora alpha:  27.96127110891487
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 40,934,400 || all params: 8,071,195,648 || trainable%: 0.5072
length of training data:  9968
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0055, 'grad_norm': 1.4631600379943848, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8429206609725952, 'eval_runtime': 3.3604, 'eval_samples_per_second': 297.588, 'eval_steps_per_second': 18.748, 'epoch': 0.04}
{'loss': 1.3631, 'grad_norm': 0.606415867805481, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0461562871932983, 'eval_runtime': 3.3642, 'eval_samples_per_second': 297.251, 'eval_steps_per_second': 18.727, 'epoch': 0.08}
{'loss': 1.0684, 'grad_norm': 0.3536101281642914, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 0.9320973753929138, 'eval_runtime': 3.3588, 'eval_samples_per_second': 297.721, 'eval_steps_per_second': 18.756, 'epoch': 0.12}
{'loss': 0.9896, 'grad_norm': 0.4580650329589844, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 0.8577127456665039, 'eval_runtime': 3.3742, 'eval_samples_per_second': 296.371, 'eval_steps_per_second': 18.671, 'epoch': 0.16}
{'loss': 0.9486, 'grad_norm': 0.4182998538017273, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 0.793915867805481, 'eval_runtime': 3.3777, 'eval_samples_per_second': 296.056, 'eval_steps_per_second': 18.652, 'epoch': 0.2}
{'loss': 0.9558, 'grad_norm': 0.29601505398750305, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 0.7826612591743469, 'eval_runtime': 3.3887, 'eval_samples_per_second': 295.1, 'eval_steps_per_second': 18.591, 'epoch': 0.24}
{'loss': 0.897, 'grad_norm': 0.27242857217788696, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 0.744118869304657, 'eval_runtime': 3.3853, 'eval_samples_per_second': 295.392, 'eval_steps_per_second': 18.61, 'epoch': 0.28}
{'loss': 0.9359, 'grad_norm': 0.24639160931110382, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 0.734343409538269, 'eval_runtime': 3.3758, 'eval_samples_per_second': 296.225, 'eval_steps_per_second': 18.662, 'epoch': 0.32}
{'loss': 0.9068, 'grad_norm': 0.25229108333587646, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 0.7187365889549255, 'eval_runtime': 3.3934, 'eval_samples_per_second': 294.686, 'eval_steps_per_second': 18.565, 'epoch': 0.36}
{'loss': 0.8879, 'grad_norm': 0.28419193625450134, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 0.7051598429679871, 'eval_runtime': 3.3875, 'eval_samples_per_second': 295.204, 'eval_steps_per_second': 18.598, 'epoch': 0.4}
{'loss': 0.8398, 'grad_norm': 0.3312443196773529, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 0.686034083366394, 'eval_runtime': 3.3738, 'eval_samples_per_second': 296.401, 'eval_steps_per_second': 18.673, 'epoch': 0.44}
{'loss': 0.8523, 'grad_norm': 0.29736292362213135, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 0.6760320663452148, 'eval_runtime': 3.3636, 'eval_samples_per_second': 297.297, 'eval_steps_per_second': 18.73, 'epoch': 0.48}
{'loss': 0.8658, 'grad_norm': 0.2841196656227112, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 0.6587684750556946, 'eval_runtime': 3.3679, 'eval_samples_per_second': 296.922, 'eval_steps_per_second': 18.706, 'epoch': 0.52}
{'loss': 0.878, 'grad_norm': 0.3435504734516144, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 0.6533830761909485, 'eval_runtime': 3.3844, 'eval_samples_per_second': 295.469, 'eval_steps_per_second': 18.615, 'epoch': 0.56}
{'loss': 0.8731, 'grad_norm': 0.2871229946613312, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 0.6403359174728394, 'eval_runtime': 3.3741, 'eval_samples_per_second': 296.373, 'eval_steps_per_second': 18.671, 'epoch': 0.6}
{'loss': 0.887, 'grad_norm': 0.3268006443977356, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 0.6252829432487488, 'eval_runtime': 3.3743, 'eval_samples_per_second': 296.354, 'eval_steps_per_second': 18.67, 'epoch': 0.64}
{'loss': 0.8468, 'grad_norm': 0.33350247144699097, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 0.6152923703193665, 'eval_runtime': 3.3729, 'eval_samples_per_second': 296.485, 'eval_steps_per_second': 18.679, 'epoch': 0.68}
{'loss': 0.854, 'grad_norm': 0.3500558137893677, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 0.6038931608200073, 'eval_runtime': 3.3734, 'eval_samples_per_second': 296.44, 'eval_steps_per_second': 18.676, 'epoch': 0.72}
{'loss': 0.8463, 'grad_norm': 0.37040799856185913, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 0.5978508591651917, 'eval_runtime': 3.3763, 'eval_samples_per_second': 296.182, 'eval_steps_per_second': 18.659, 'epoch': 0.76}
{'loss': 0.8251, 'grad_norm': 0.3411223292350769, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.5869280099868774, 'eval_runtime': 3.3727, 'eval_samples_per_second': 296.494, 'eval_steps_per_second': 18.679, 'epoch': 0.8}
{'loss': 0.837, 'grad_norm': 0.4373306334018707, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.5736532807350159, 'eval_runtime': 3.3738, 'eval_samples_per_second': 296.401, 'eval_steps_per_second': 18.673, 'epoch': 0.84}
{'loss': 0.8261, 'grad_norm': 0.4321157932281494, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.5744709968566895, 'eval_runtime': 3.3718, 'eval_samples_per_second': 296.579, 'eval_steps_per_second': 18.684, 'epoch': 0.88}
{'loss': 0.809, 'grad_norm': 0.3684261441230774, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.561689555644989, 'eval_runtime': 3.3731, 'eval_samples_per_second': 296.46, 'eval_steps_per_second': 18.677, 'epoch': 0.92}
{'loss': 0.7635, 'grad_norm': 0.43825989961624146, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.5582028031349182, 'eval_runtime': 3.3744, 'eval_samples_per_second': 296.347, 'eval_steps_per_second': 18.67, 'epoch': 0.96}
{'train_runtime': 272.686, 'train_samples_per_second': 36.555, 'train_steps_per_second': 2.285, 'train_loss': 0.9827659478348293, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8429206609725952, 1.0461562871932983, 0.9320973753929138, 0.8577127456665039, 0.793915867805481, 0.7826612591743469, 0.744118869304657, 0.734343409538269, 0.7187365889549255, 0.7051598429679871, 0.686034083366394, 0.6760320663452148, 0.6587684750556946, 0.6533830761909485, 0.6403359174728394, 0.6252829432487488, 0.6152923703193665, 0.6038931608200073, 0.5978508591651917, 0.5869280099868774, 0.5736532807350159, 0.5744709968566895, 0.561689555644989, 0.5582028031349182], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.8429206609725952, 1.0461562871932983, 0.9320973753929138, 0.8577127456665039, 0.793915867805481, 0.7826612591743469, 0.744118869304657, 0.734343409538269, 0.7187365889549255, 0.7051598429679871, 0.686034083366394, 0.6760320663452148, 0.6587684750556946, 0.6533830761909485, 0.6403359174728394, 0.6252829432487488, 0.6152923703193665, 0.6038931608200073, 0.5978508591651917, 0.5869280099868774, 0.5736532807350159, 0.5744709968566895, 0.561689555644989, 0.5582028031349182]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1268006563186646
current iteration best possible eval_loss (full train run):  -0.5582028031349182
max eval_loss so far:  -0.4600749611854553
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.5856 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.20075875520706177, 0.2661306858062744, 0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8751383423805237, 0.10287010669708252, 0.1595216989517212, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.02520020864903927, 0.987917423248291, 0.7536473274230957, 0.3050987720489502]  ‚Üí  acq = -0.9203513224917836
X = [0.37903499603271484, 0.49315524101257324, 0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.2505585253238678, 0.6847650408744812, 0.7922602295875549, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.23907232284545898, 0.9114632606506348, 0.9043515920639038, 0.9609596729278564]  ‚Üí  acq = -0.9203513224917836
X = [0.9641072154045105, 0.4331236481666565, 0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.4157118797302246, 0.0375140905380249, 0.23879104852676392, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.47058141231536865, 0.22714883089065552, 0.2083243578672409, 0.21395939588546753]  ‚Üí  acq = -0.9203513224917836
X = [0.8797377347946167, 0.2856326103210449, 0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2817014753818512, 0.0661017894744873, 0.16436314582824707, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5602686405181885, 0.5082452893257141, 0.49900704622268677, 0.5792016983032227]  ‚Üí  acq = -0.9203513224917836
X = [0.30917781591415405, 0.6994286775588989, 0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.7001289129257202, 0.6122615933418274, 0.49610579013824463, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8769228458404541, 0.1870233416557312, 0.19417396187782288, 0.6010990738868713]  ‚Üí  acq = -0.9203513224917836
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0965, dtype=torch.float64), 0, 0, tensor(0.7051, dtype=torch.float64), 0, 0, tensor(0.1984, dtype=torch.float64), 16, 1, 1, 1, 1, 1, 41, 0.0143584141831305, 38.47568258425922, 1]
normalized proposed parameters for next round by BO: [tensor(4.2510e-18, dtype=torch.float64), tensor(4.4307e-18, dtype=torch.float64), tensor(0.0965, dtype=torch.float64), tensor(1.7896e-18, dtype=torch.float64), tensor(1.1856e-18, dtype=torch.float64), tensor(0.7051, dtype=torch.float64), tensor(1.6007e-18, dtype=torch.float64), tensor(2.8752e-18, dtype=torch.float64), tensor(0.1984, dtype=torch.float64), tensor(0.5008, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3218, dtype=torch.float64), tensor(0.1436, dtype=torch.float64), tensor(0.8016, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.097
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.705
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.198

LoRA Parameters:
  lora_r: (41,)
  lora_dropout: (0.0143584141831305,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (38.47568258425922,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  41
lora dropout:  0.0143584141831305
lora alpha:  38.47568258425922
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 45,006,848 || all params: 8,075,268,096 || trainable%: 0.5573
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1786, 'grad_norm': 1.2645517587661743, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.364652156829834, 'eval_runtime': 3.653, 'eval_samples_per_second': 273.744, 'eval_steps_per_second': 17.246, 'epoch': 0.04}
{'loss': 1.3342, 'grad_norm': 1.07784104347229, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.8747363090515137, 'eval_runtime': 3.6589, 'eval_samples_per_second': 273.308, 'eval_steps_per_second': 17.218, 'epoch': 0.08}
{'loss': 1.0517, 'grad_norm': 0.5895917415618896, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7056472301483154, 'eval_runtime': 3.6391, 'eval_samples_per_second': 274.793, 'eval_steps_per_second': 17.312, 'epoch': 0.12}
{'loss': 1.0229, 'grad_norm': 0.5989059805870056, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6108918190002441, 'eval_runtime': 3.6488, 'eval_samples_per_second': 274.061, 'eval_steps_per_second': 17.266, 'epoch': 0.16}
{'loss': 0.9263, 'grad_norm': 0.6010977029800415, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.5611245632171631, 'eval_runtime': 3.656, 'eval_samples_per_second': 273.524, 'eval_steps_per_second': 17.232, 'epoch': 0.2}
{'loss': 0.9139, 'grad_norm': 0.6937625408172607, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5041304230690002, 'eval_runtime': 3.6721, 'eval_samples_per_second': 272.325, 'eval_steps_per_second': 17.157, 'epoch': 0.24}
{'loss': 0.8023, 'grad_norm': 0.5682664513587952, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.44740843772888184, 'eval_runtime': 3.6617, 'eval_samples_per_second': 273.098, 'eval_steps_per_second': 17.205, 'epoch': 0.28}
{'loss': 0.8079, 'grad_norm': 0.5840073227882385, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.40678855776786804, 'eval_runtime': 3.6811, 'eval_samples_per_second': 271.661, 'eval_steps_per_second': 17.115, 'epoch': 0.32}
{'loss': 0.7361, 'grad_norm': 0.6168819069862366, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.3751204311847687, 'eval_runtime': 3.6615, 'eval_samples_per_second': 273.115, 'eval_steps_per_second': 17.206, 'epoch': 0.36}
{'loss': 0.7684, 'grad_norm': 0.4574166238307953, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.33850976824760437, 'eval_runtime': 3.6685, 'eval_samples_per_second': 272.589, 'eval_steps_per_second': 17.173, 'epoch': 0.4}
{'loss': 0.8047, 'grad_norm': 0.4527684450149536, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.3205353319644928, 'eval_runtime': 3.6629, 'eval_samples_per_second': 273.008, 'eval_steps_per_second': 17.199, 'epoch': 0.44}
{'loss': 0.8098, 'grad_norm': 0.4912611246109009, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.28648078441619873, 'eval_runtime': 3.662, 'eval_samples_per_second': 273.075, 'eval_steps_per_second': 17.204, 'epoch': 0.48}
{'loss': 0.7451, 'grad_norm': 0.6369784474372864, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.2627996802330017, 'eval_runtime': 3.6612, 'eval_samples_per_second': 273.132, 'eval_steps_per_second': 17.207, 'epoch': 0.52}
{'loss': 0.6757, 'grad_norm': 0.6297604441642761, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.2416948825120926, 'eval_runtime': 3.6706, 'eval_samples_per_second': 272.438, 'eval_steps_per_second': 17.164, 'epoch': 0.56}
{'loss': 0.6597, 'grad_norm': 0.4095219671726227, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.22701972723007202, 'eval_runtime': 3.7066, 'eval_samples_per_second': 269.79, 'eval_steps_per_second': 16.997, 'epoch': 0.6}
{'loss': 0.751, 'grad_norm': 0.4601713716983795, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.2162666916847229, 'eval_runtime': 3.6714, 'eval_samples_per_second': 272.377, 'eval_steps_per_second': 17.16, 'epoch': 0.64}
{'loss': 0.6818, 'grad_norm': 0.504231870174408, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.20102651417255402, 'eval_runtime': 3.6592, 'eval_samples_per_second': 273.282, 'eval_steps_per_second': 17.217, 'epoch': 0.68}
{'loss': 0.6933, 'grad_norm': 0.36239010095596313, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.18983951210975647, 'eval_runtime': 3.6607, 'eval_samples_per_second': 273.175, 'eval_steps_per_second': 17.21, 'epoch': 0.72}
{'loss': 0.7645, 'grad_norm': 0.476046085357666, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.18529684841632843, 'eval_runtime': 3.6639, 'eval_samples_per_second': 272.93, 'eval_steps_per_second': 17.195, 'epoch': 0.76}
{'loss': 0.6457, 'grad_norm': 0.5762762427330017, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.1797712743282318, 'eval_runtime': 3.661, 'eval_samples_per_second': 273.149, 'eval_steps_per_second': 17.208, 'epoch': 0.8}
{'loss': 0.7431, 'grad_norm': 0.5829368233680725, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.17443722486495972, 'eval_runtime': 3.6587, 'eval_samples_per_second': 273.321, 'eval_steps_per_second': 17.219, 'epoch': 0.84}
{'loss': 0.7239, 'grad_norm': 0.4899755120277405, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.16751837730407715, 'eval_runtime': 3.6689, 'eval_samples_per_second': 272.565, 'eval_steps_per_second': 17.172, 'epoch': 0.88}
{'loss': 0.7524, 'grad_norm': 0.40358054637908936, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.1645248681306839, 'eval_runtime': 3.67, 'eval_samples_per_second': 272.479, 'eval_steps_per_second': 17.166, 'epoch': 0.92}
{'loss': 0.6483, 'grad_norm': 0.5772675275802612, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.16042019426822662, 'eval_runtime': 3.6661, 'eval_samples_per_second': 272.769, 'eval_steps_per_second': 17.184, 'epoch': 0.96}
{'loss': 0.601, 'grad_norm': 0.47559162974357605, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.15899628400802612, 'eval_runtime': 3.6593, 'eval_samples_per_second': 273.278, 'eval_steps_per_second': 17.217, 'epoch': 1.0}
{'train_runtime': 302.6855, 'train_samples_per_second': 33.034, 'train_steps_per_second': 2.065, 'train_loss': 0.8896788024902343, 'epoch': 1.0}
train_results:  {'eval_loss': [1.364652156829834, 0.8747363090515137, 0.7056472301483154, 0.6108918190002441, 0.5611245632171631, 0.5041304230690002, 0.44740843772888184, 0.40678855776786804, 0.3751204311847687, 0.33850976824760437, 0.3205353319644928, 0.28648078441619873, 0.2627996802330017, 0.2416948825120926, 0.22701972723007202, 0.2162666916847229, 0.20102651417255402, 0.18983951210975647, 0.18529684841632843, 0.1797712743282318, 0.17443722486495972, 0.16751837730407715, 0.1645248681306839, 0.16042019426822662, 0.15899628400802612], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.364652156829834, 0.8747363090515137, 0.7056472301483154, 0.6108918190002441, 0.5611245632171631, 0.5041304230690002, 0.44740843772888184, 0.40678855776786804, 0.3751204311847687, 0.33850976824760437, 0.3205353319644928, 0.28648078441619873, 0.2627996802330017, 0.2416948825120926, 0.22701972723007202, 0.2162666916847229, 0.20102651417255402, 0.18983951210975647, 0.18529684841632843, 0.1797712743282318, 0.17443722486495972, 0.16751837730407715, 0.1645248681306839, 0.16042019426822662, 0.15899628400802612]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.15899628400802612
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.0020 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.9754239735263893
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.959341368006029
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.9593413680060288
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.9593447689702165
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.959406448042366
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0530, dtype=torch.float64), tensor(0.0318, dtype=torch.float64), tensor(0.3505, dtype=torch.float64), 0, tensor(0.4377, dtype=torch.float64), tensor(0.0578, dtype=torch.float64), tensor(0.0173, dtype=torch.float64), tensor(0.0503, dtype=torch.float64), 18, 0, 1, 1, 1, 1, 128, 6.938893903907229e-19, 22.2990310904418, 1]
normalized proposed parameters for next round by BO: [tensor(7.7873e-18, dtype=torch.float64), tensor(0.0530, dtype=torch.float64), tensor(0.0318, dtype=torch.float64), tensor(0.3505, dtype=torch.float64), tensor(0.0015, dtype=torch.float64), tensor(0.4377, dtype=torch.float64), tensor(0.0578, dtype=torch.float64), tensor(0.0173, dtype=torch.float64), tensor(0.0503, dtype=torch.float64), tensor(0.5599, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(6.9389e-18, dtype=torch.float64), tensor(0.4646, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.053
  rowan_hellaswag: 0.032
  sciq: 0.35
  triviaqa: 0
  truthfulqa_gen: 0.438
  wikitext: 0.058
  mmlu: 0.017
  arc_challenge: 0.05

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (6.938893903907229e-19,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (22.2990310904418,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  6.938893903907229e-19
lora alpha:  22.2990310904418
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 139,198,464 || all params: 8,169,459,712 || trainable%: 1.7039
length of training data:  9981
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2989, 'grad_norm': 1.1521493196487427, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5368218421936035, 'eval_runtime': 3.4779, 'eval_samples_per_second': 287.532, 'eval_steps_per_second': 18.115, 'epoch': 0.04}
{'loss': 1.4143, 'grad_norm': 0.7346249222755432, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9751988649368286, 'eval_runtime': 3.4788, 'eval_samples_per_second': 287.455, 'eval_steps_per_second': 18.11, 'epoch': 0.08}
{'loss': 1.1284, 'grad_norm': 0.28803175687789917, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.8462069034576416, 'eval_runtime': 3.472, 'eval_samples_per_second': 288.021, 'eval_steps_per_second': 18.145, 'epoch': 0.12}
{'loss': 1.1402, 'grad_norm': 0.31902000308036804, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.7559235692024231, 'eval_runtime': 3.473, 'eval_samples_per_second': 287.939, 'eval_steps_per_second': 18.14, 'epoch': 0.16}
{'loss': 0.9426, 'grad_norm': 0.2342357039451599, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.6787084937095642, 'eval_runtime': 3.4793, 'eval_samples_per_second': 287.411, 'eval_steps_per_second': 18.107, 'epoch': 0.2}
{'loss': 0.9268, 'grad_norm': 0.2821052074432373, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.6260197162628174, 'eval_runtime': 3.4869, 'eval_samples_per_second': 286.787, 'eval_steps_per_second': 18.068, 'epoch': 0.24}
{'loss': 0.9388, 'grad_norm': 0.23712512850761414, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.602168083190918, 'eval_runtime': 3.4848, 'eval_samples_per_second': 286.958, 'eval_steps_per_second': 18.078, 'epoch': 0.28}
{'loss': 0.974, 'grad_norm': 0.2607186436653137, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.5584887862205505, 'eval_runtime': 3.4863, 'eval_samples_per_second': 286.837, 'eval_steps_per_second': 18.071, 'epoch': 0.32}
{'loss': 0.8629, 'grad_norm': 0.3647983968257904, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.524014949798584, 'eval_runtime': 3.491, 'eval_samples_per_second': 286.452, 'eval_steps_per_second': 18.046, 'epoch': 0.36}
{'loss': 0.963, 'grad_norm': 0.32693785429000854, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.5119323134422302, 'eval_runtime': 3.4878, 'eval_samples_per_second': 286.711, 'eval_steps_per_second': 18.063, 'epoch': 0.4}
{'loss': 0.9038, 'grad_norm': 0.2998366951942444, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.48446813225746155, 'eval_runtime': 3.4939, 'eval_samples_per_second': 286.216, 'eval_steps_per_second': 18.032, 'epoch': 0.44}
{'loss': 0.8379, 'grad_norm': 0.2896932065486908, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.44149157404899597, 'eval_runtime': 3.5213, 'eval_samples_per_second': 283.984, 'eval_steps_per_second': 17.891, 'epoch': 0.48}
{'loss': 0.8807, 'grad_norm': 0.3136177062988281, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.4428316056728363, 'eval_runtime': 3.5102, 'eval_samples_per_second': 284.88, 'eval_steps_per_second': 17.947, 'epoch': 0.52}
{'loss': 0.8669, 'grad_norm': 0.2644009590148926, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.397579550743103, 'eval_runtime': 3.5003, 'eval_samples_per_second': 285.692, 'eval_steps_per_second': 17.999, 'epoch': 0.56}
{'loss': 0.8583, 'grad_norm': 0.32024019956588745, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.3688338100910187, 'eval_runtime': 3.4935, 'eval_samples_per_second': 286.245, 'eval_steps_per_second': 18.033, 'epoch': 0.6}
{'loss': 0.7769, 'grad_norm': 0.30397239327430725, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.3510643243789673, 'eval_runtime': 3.5074, 'eval_samples_per_second': 285.111, 'eval_steps_per_second': 17.962, 'epoch': 0.64}
{'loss': 0.8991, 'grad_norm': 0.26871412992477417, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.3326900899410248, 'eval_runtime': 3.5086, 'eval_samples_per_second': 285.014, 'eval_steps_per_second': 17.956, 'epoch': 0.68}
{'loss': 0.7945, 'grad_norm': 0.28262829780578613, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.31385675072669983, 'eval_runtime': 3.493, 'eval_samples_per_second': 286.288, 'eval_steps_per_second': 18.036, 'epoch': 0.72}
{'loss': 0.7968, 'grad_norm': 0.286291241645813, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.2961094379425049, 'eval_runtime': 3.4936, 'eval_samples_per_second': 286.238, 'eval_steps_per_second': 18.033, 'epoch': 0.76}
{'loss': 0.8407, 'grad_norm': 0.2609632611274719, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.27940112352371216, 'eval_runtime': 3.4944, 'eval_samples_per_second': 286.168, 'eval_steps_per_second': 18.029, 'epoch': 0.8}
{'loss': 0.7616, 'grad_norm': 0.3287028670310974, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.2756642997264862, 'eval_runtime': 3.4947, 'eval_samples_per_second': 286.145, 'eval_steps_per_second': 18.027, 'epoch': 0.84}
{'loss': 0.769, 'grad_norm': 0.26428109407424927, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.2603854835033417, 'eval_runtime': 3.5, 'eval_samples_per_second': 285.715, 'eval_steps_per_second': 18.0, 'epoch': 0.88}
{'loss': 0.8682, 'grad_norm': 0.43357911705970764, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.24903708696365356, 'eval_runtime': 3.4973, 'eval_samples_per_second': 285.931, 'eval_steps_per_second': 18.014, 'epoch': 0.92}
{'loss': 0.7271, 'grad_norm': 0.23331810534000397, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.23959694802761078, 'eval_runtime': 3.4999, 'eval_samples_per_second': 285.72, 'eval_steps_per_second': 18.0, 'epoch': 0.96}
{'train_runtime': 268.2906, 'train_samples_per_second': 37.202, 'train_steps_per_second': 2.326, 'train_loss': 0.9961313773424197, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5368218421936035, 0.9751988649368286, 0.8462069034576416, 0.7559235692024231, 0.6787084937095642, 0.6260197162628174, 0.602168083190918, 0.5584887862205505, 0.524014949798584, 0.5119323134422302, 0.48446813225746155, 0.44149157404899597, 0.4428316056728363, 0.397579550743103, 0.3688338100910187, 0.3510643243789673, 0.3326900899410248, 0.31385675072669983, 0.2961094379425049, 0.27940112352371216, 0.2756642997264862, 0.2603854835033417, 0.24903708696365356, 0.23959694802761078], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.5368218421936035, 0.9751988649368286, 0.8462069034576416, 0.7559235692024231, 0.6787084937095642, 0.6260197162628174, 0.602168083190918, 0.5584887862205505, 0.524014949798584, 0.5119323134422302, 0.48446813225746155, 0.44149157404899597, 0.4428316056728363, 0.397579550743103, 0.3688338100910187, 0.3510643243789673, 0.3326900899410248, 0.31385675072669983, 0.2961094379425049, 0.27940112352371216, 0.2756642997264862, 0.2603854835033417, 0.24903708696365356, 0.23959694802761078]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1300971508026123
current iteration best possible eval_loss (full train run):  -0.23959694802761078
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6994 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1560913324356079, 0.29771536588668823, 0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.4007338583469391, 0.8963236808776855, 0.5591384768486023, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.42323532700538635, 0.003319978713989258, 0.666995644569397, 0.6627236008644104]  ‚Üí  acq = -0.9378847488992197
X = [0.6140679717063904, 0.7597888708114624, 0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.5511543154716492, 0.10114860534667969, 0.46051692962646484, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6117270588874817, 0.06090492010116577, 0.29019129276275635, 0.544792890548706]  ‚Üí  acq = -0.9378847345007387
X = [0.4161165952682495, 0.6534545421600342, 0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9124553799629211, 0.5554662346839905, 0.09747803211212158, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9407435655593872, 0.9420399069786072, 0.717787504196167, 0.5669505000114441]  ‚Üí  acq = -0.9378847345098535
X = [0.11413401365280151, 0.7658460736274719, 0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.20504699647426605, 0.40315020084381104, 0.9066953659057617, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7133153080940247, 0.3376033306121826, 0.9497083425521851, 0.04203289747238159]  ‚Üí  acq = -0.9378848911286692
X = [0.7649766802787781, 0.17325276136398315, 0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7676264643669128, 0.04699522256851196, 0.9299392104148865, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.02683671936392784, 0.7503892779350281, 0.20525000989437103, 0.7006362080574036]  ‚Üí  acq = -0.9378847338899101
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0616, dtype=torch.float64), tensor(0.2456, dtype=torch.float64), 0, tensor(0.5622, dtype=torch.float64), 0, tensor(0.1306, dtype=torch.float64), 0, 19, 0, 0, 0, 1, 1, 39, 6.505213034913028e-19, 26.601311074356037, 1]
normalized proposed parameters for next round by BO: [tensor(4.5545e-18, dtype=torch.float64), tensor(1.6261e-17, dtype=torch.float64), tensor(0.0616, dtype=torch.float64), tensor(0.2456, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5622, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1306, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6091, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3062, dtype=torch.float64), tensor(6.5052e-18, dtype=torch.float64), tensor(0.5542, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.062
  sciq: 0.246
  triviaqa: 0
  truthfulqa_gen: 0.562
  wikitext: 0
  mmlu: 0.131
  arc_challenge: 0

LoRA Parameters:
  lora_r: (39,)
  lora_dropout: (6.505213034913028e-19,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (26.601311074356037,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  39
lora dropout:  6.505213034913028e-19
lora alpha:  26.601311074356037
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 27,316,224 || all params: 8,057,577,472 || trainable%: 0.3390
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6184, 'grad_norm': 2.115495204925537, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5567100048065186, 'eval_runtime': 3.3893, 'eval_samples_per_second': 295.042, 'eval_steps_per_second': 18.588, 'epoch': 0.04}
{'loss': 1.504, 'grad_norm': 1.477091908454895, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9527994394302368, 'eval_runtime': 3.4015, 'eval_samples_per_second': 293.991, 'eval_steps_per_second': 18.521, 'epoch': 0.08}
{'loss': 1.1671, 'grad_norm': 0.701896607875824, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.807196855545044, 'eval_runtime': 3.3902, 'eval_samples_per_second': 294.97, 'eval_steps_per_second': 18.583, 'epoch': 0.12}
{'loss': 1.069, 'grad_norm': 0.4400199353694916, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6906510591506958, 'eval_runtime': 3.3887, 'eval_samples_per_second': 295.098, 'eval_steps_per_second': 18.591, 'epoch': 0.16}
{'loss': 0.9548, 'grad_norm': 0.43809008598327637, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6289605498313904, 'eval_runtime': 3.3962, 'eval_samples_per_second': 294.451, 'eval_steps_per_second': 18.55, 'epoch': 0.2}
{'loss': 0.9653, 'grad_norm': 0.4689501225948334, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5739685893058777, 'eval_runtime': 3.39, 'eval_samples_per_second': 294.985, 'eval_steps_per_second': 18.584, 'epoch': 0.24}
{'loss': 0.9309, 'grad_norm': 0.3843795657157898, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5447024703025818, 'eval_runtime': 3.3979, 'eval_samples_per_second': 294.296, 'eval_steps_per_second': 18.541, 'epoch': 0.28}
{'loss': 0.9568, 'grad_norm': 0.4379737377166748, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5055639147758484, 'eval_runtime': 3.3985, 'eval_samples_per_second': 294.246, 'eval_steps_per_second': 18.537, 'epoch': 0.32}
{'loss': 0.9692, 'grad_norm': 0.5042871832847595, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.4540415108203888, 'eval_runtime': 3.4, 'eval_samples_per_second': 294.119, 'eval_steps_per_second': 18.529, 'epoch': 0.36}
{'loss': 0.928, 'grad_norm': 0.4388978183269501, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.43623223900794983, 'eval_runtime': 3.3995, 'eval_samples_per_second': 294.161, 'eval_steps_per_second': 18.532, 'epoch': 0.4}
{'loss': 0.9179, 'grad_norm': 0.47965794801712036, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.4056777358055115, 'eval_runtime': 3.4062, 'eval_samples_per_second': 293.582, 'eval_steps_per_second': 18.496, 'epoch': 0.44}
{'loss': 0.832, 'grad_norm': 0.46580034494400024, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3717217445373535, 'eval_runtime': 3.4109, 'eval_samples_per_second': 293.177, 'eval_steps_per_second': 18.47, 'epoch': 0.48}
{'loss': 0.8353, 'grad_norm': 0.6066275835037231, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.33640578389167786, 'eval_runtime': 3.4312, 'eval_samples_per_second': 291.446, 'eval_steps_per_second': 18.361, 'epoch': 0.52}
{'loss': 0.8315, 'grad_norm': 0.7074594497680664, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3090428113937378, 'eval_runtime': 3.4199, 'eval_samples_per_second': 292.408, 'eval_steps_per_second': 18.422, 'epoch': 0.56}
{'loss': 0.9293, 'grad_norm': 0.5666963458061218, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2849189341068268, 'eval_runtime': 3.4147, 'eval_samples_per_second': 292.848, 'eval_steps_per_second': 18.449, 'epoch': 0.6}
{'loss': 0.869, 'grad_norm': 0.5987365245819092, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.27376788854599, 'eval_runtime': 3.4241, 'eval_samples_per_second': 292.051, 'eval_steps_per_second': 18.399, 'epoch': 0.64}
{'loss': 0.9168, 'grad_norm': 0.3772699534893036, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.25534510612487793, 'eval_runtime': 3.4377, 'eval_samples_per_second': 290.889, 'eval_steps_per_second': 18.326, 'epoch': 0.68}
{'loss': 0.7226, 'grad_norm': 0.5452908873558044, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.23835988342761993, 'eval_runtime': 3.417, 'eval_samples_per_second': 292.651, 'eval_steps_per_second': 18.437, 'epoch': 0.72}
{'loss': 0.7989, 'grad_norm': 0.45036542415618896, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2283257395029068, 'eval_runtime': 3.4153, 'eval_samples_per_second': 292.8, 'eval_steps_per_second': 18.446, 'epoch': 0.76}
{'loss': 0.8299, 'grad_norm': 0.5356042981147766, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.22048485279083252, 'eval_runtime': 3.4156, 'eval_samples_per_second': 292.772, 'eval_steps_per_second': 18.445, 'epoch': 0.8}
{'loss': 0.7799, 'grad_norm': 0.5174864530563354, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2123720347881317, 'eval_runtime': 3.4141, 'eval_samples_per_second': 292.905, 'eval_steps_per_second': 18.453, 'epoch': 0.84}
{'loss': 0.7511, 'grad_norm': 0.40129169821739197, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.204456627368927, 'eval_runtime': 3.4145, 'eval_samples_per_second': 292.869, 'eval_steps_per_second': 18.451, 'epoch': 0.88}
{'loss': 0.8276, 'grad_norm': 0.6590999960899353, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.20154643058776855, 'eval_runtime': 3.4163, 'eval_samples_per_second': 292.717, 'eval_steps_per_second': 18.441, 'epoch': 0.92}
{'loss': 0.7494, 'grad_norm': 0.499063640832901, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.19488872587680817, 'eval_runtime': 3.4197, 'eval_samples_per_second': 292.423, 'eval_steps_per_second': 18.423, 'epoch': 0.96}
{'loss': 0.7496, 'grad_norm': 0.3992571532726288, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.19416816532611847, 'eval_runtime': 3.4206, 'eval_samples_per_second': 292.343, 'eval_steps_per_second': 18.418, 'epoch': 1.0}
{'train_runtime': 268.986, 'train_samples_per_second': 37.169, 'train_steps_per_second': 2.324, 'train_loss': 1.0161644012451172, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5567100048065186, 0.9527994394302368, 0.807196855545044, 0.6906510591506958, 0.6289605498313904, 0.5739685893058777, 0.5447024703025818, 0.5055639147758484, 0.4540415108203888, 0.43623223900794983, 0.4056777358055115, 0.3717217445373535, 0.33640578389167786, 0.3090428113937378, 0.2849189341068268, 0.27376788854599, 0.25534510612487793, 0.23835988342761993, 0.2283257395029068, 0.22048485279083252, 0.2123720347881317, 0.204456627368927, 0.20154643058776855, 0.19488872587680817, 0.19416816532611847], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5567100048065186, 0.9527994394302368, 0.807196855545044, 0.6906510591506958, 0.6289605498313904, 0.5739685893058777, 0.5447024703025818, 0.5055639147758484, 0.4540415108203888, 0.43623223900794983, 0.4056777358055115, 0.3717217445373535, 0.33640578389167786, 0.3090428113937378, 0.2849189341068268, 0.27376788854599, 0.25534510612487793, 0.23835988342761993, 0.2283257395029068, 0.22048485279083252, 0.2123720347881317, 0.204456627368927, 0.20154643058776855, 0.19488872587680817, 0.19416816532611847]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.12752366065979
current iteration best possible eval_loss (full train run):  -0.19416816532611847
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.2280 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.9835075438366481
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.9831824641911093
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.9836623775275561
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.9835071405147174
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.9835075251300056
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3062, dtype=torch.float64), tensor(0.1788, dtype=torch.float64), tensor(0.0957, dtype=torch.float64), tensor(0.0258, dtype=torch.float64), 0, tensor(0.3355, dtype=torch.float64), 0, 0, tensor(0.0542, dtype=torch.float64), 14, 0, 1, 0, 0, 1, 62, 4.611574626291439e-21, 42.78201362927016, 1]
normalized proposed parameters for next round by BO: [tensor(0.3062, dtype=torch.float64), tensor(0.1788, dtype=torch.float64), tensor(0.0957, dtype=torch.float64), tensor(0.0258, dtype=torch.float64), tensor(1.2058e-18, dtype=torch.float64), tensor(0.3355, dtype=torch.float64), tensor(5.5168e-19, dtype=torch.float64), tensor(0.0038, dtype=torch.float64), tensor(0.0542, dtype=torch.float64), tensor(0.4408, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4845, dtype=torch.float64), tensor(4.6116e-20, dtype=torch.float64), tensor(0.8913, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.306
  gsm8k: 0.179
  rowan_hellaswag: 0.096
  sciq: 0.026
  triviaqa: 0
  truthfulqa_gen: 0.335
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.054

LoRA Parameters:
  lora_r: (62,)
  lora_dropout: (4.611574626291439e-21,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (42.78201362927016,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  62
lora dropout:  4.611574626291439e-21
lora alpha:  42.78201362927016
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 20,443,136 || all params: 8,050,704,384 || trainable%: 0.2539
length of training data:  9959
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.281, 'grad_norm': 1.0912091732025146, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1520705223083496, 'eval_runtime': 3.1715, 'eval_samples_per_second': 315.303, 'eval_steps_per_second': 19.864, 'epoch': 0.04}
{'loss': 1.7064, 'grad_norm': 0.5279049873352051, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.260018229484558, 'eval_runtime': 3.1758, 'eval_samples_per_second': 314.884, 'eval_steps_per_second': 19.838, 'epoch': 0.08}
{'loss': 1.3102, 'grad_norm': 0.5336705446243286, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.0812824964523315, 'eval_runtime': 3.1853, 'eval_samples_per_second': 313.938, 'eval_steps_per_second': 19.778, 'epoch': 0.12}
{'loss': 1.194, 'grad_norm': 0.45821601152420044, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 0.980938732624054, 'eval_runtime': 3.1771, 'eval_samples_per_second': 314.755, 'eval_steps_per_second': 19.83, 'epoch': 0.16}
{'loss': 1.1769, 'grad_norm': 0.37792184948921204, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 0.9224525094032288, 'eval_runtime': 3.1835, 'eval_samples_per_second': 314.119, 'eval_steps_per_second': 19.79, 'epoch': 0.2}
{'loss': 1.1115, 'grad_norm': 0.37848100066185, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 0.8912991285324097, 'eval_runtime': 3.1894, 'eval_samples_per_second': 313.536, 'eval_steps_per_second': 19.753, 'epoch': 0.24}
{'loss': 1.1394, 'grad_norm': 0.33148273825645447, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 0.893462061882019, 'eval_runtime': 3.1986, 'eval_samples_per_second': 312.637, 'eval_steps_per_second': 19.696, 'epoch': 0.28}
{'loss': 1.0881, 'grad_norm': 0.28880575299263, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 0.8512019515037537, 'eval_runtime': 3.1997, 'eval_samples_per_second': 312.526, 'eval_steps_per_second': 19.689, 'epoch': 0.32}
{'loss': 1.0969, 'grad_norm': 0.3092050850391388, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 0.8315428495407104, 'eval_runtime': 3.1993, 'eval_samples_per_second': 312.569, 'eval_steps_per_second': 19.692, 'epoch': 0.36}
{'loss': 1.1344, 'grad_norm': 0.3189716339111328, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 0.7880237102508545, 'eval_runtime': 3.2047, 'eval_samples_per_second': 312.038, 'eval_steps_per_second': 19.658, 'epoch': 0.4}
{'loss': 1.0983, 'grad_norm': 0.33329710364341736, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 0.76873779296875, 'eval_runtime': 3.2032, 'eval_samples_per_second': 312.188, 'eval_steps_per_second': 19.668, 'epoch': 0.44}
{'loss': 1.0907, 'grad_norm': 0.30448678135871887, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 0.7317528128623962, 'eval_runtime': 3.2038, 'eval_samples_per_second': 312.13, 'eval_steps_per_second': 19.664, 'epoch': 0.48}
{'loss': 1.0622, 'grad_norm': 0.3188828229904175, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 0.7129438519477844, 'eval_runtime': 3.2114, 'eval_samples_per_second': 311.395, 'eval_steps_per_second': 19.618, 'epoch': 0.52}
{'loss': 1.0293, 'grad_norm': 0.35710954666137695, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 0.6675931215286255, 'eval_runtime': 3.2216, 'eval_samples_per_second': 310.402, 'eval_steps_per_second': 19.555, 'epoch': 0.56}
{'loss': 1.045, 'grad_norm': 0.42219817638397217, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 0.6173984408378601, 'eval_runtime': 3.2051, 'eval_samples_per_second': 312.006, 'eval_steps_per_second': 19.656, 'epoch': 0.6}
{'loss': 1.0102, 'grad_norm': 0.36993351578712463, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 0.5677059292793274, 'eval_runtime': 3.206, 'eval_samples_per_second': 311.915, 'eval_steps_per_second': 19.651, 'epoch': 0.64}
{'loss': 0.9763, 'grad_norm': 0.3240036368370056, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 0.5525602102279663, 'eval_runtime': 3.2101, 'eval_samples_per_second': 311.517, 'eval_steps_per_second': 19.626, 'epoch': 0.68}
{'loss': 0.9394, 'grad_norm': 0.39523717761039734, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 0.5341611504554749, 'eval_runtime': 3.1979, 'eval_samples_per_second': 312.71, 'eval_steps_per_second': 19.701, 'epoch': 0.72}
{'loss': 0.9592, 'grad_norm': 0.3151860535144806, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 0.524205207824707, 'eval_runtime': 3.1973, 'eval_samples_per_second': 312.76, 'eval_steps_per_second': 19.704, 'epoch': 0.76}
{'loss': 0.9836, 'grad_norm': 0.4308498501777649, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.5094395875930786, 'eval_runtime': 3.1983, 'eval_samples_per_second': 312.67, 'eval_steps_per_second': 19.698, 'epoch': 0.8}
{'loss': 0.9489, 'grad_norm': 0.35519644618034363, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.49606093764305115, 'eval_runtime': 3.2001, 'eval_samples_per_second': 312.487, 'eval_steps_per_second': 19.687, 'epoch': 0.84}
{'loss': 0.9553, 'grad_norm': 0.33407971262931824, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.48384615778923035, 'eval_runtime': 3.197, 'eval_samples_per_second': 312.791, 'eval_steps_per_second': 19.706, 'epoch': 0.88}
{'loss': 0.9918, 'grad_norm': 0.38777822256088257, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.4702405631542206, 'eval_runtime': 3.2016, 'eval_samples_per_second': 312.341, 'eval_steps_per_second': 19.678, 'epoch': 0.92}
{'loss': 0.8925, 'grad_norm': 0.4162021279335022, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.4601367712020874, 'eval_runtime': 3.2034, 'eval_samples_per_second': 312.168, 'eval_steps_per_second': 19.667, 'epoch': 0.96}
{'train_runtime': 271.8479, 'train_samples_per_second': 36.634, 'train_steps_per_second': 2.292, 'train_loss': 1.1688424840593414, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1520705223083496, 1.260018229484558, 1.0812824964523315, 0.980938732624054, 0.9224525094032288, 0.8912991285324097, 0.893462061882019, 0.8512019515037537, 0.8315428495407104, 0.7880237102508545, 0.76873779296875, 0.7317528128623962, 0.7129438519477844, 0.6675931215286255, 0.6173984408378601, 0.5677059292793274, 0.5525602102279663, 0.5341611504554749, 0.524205207824707, 0.5094395875930786, 0.49606093764305115, 0.48384615778923035, 0.4702405631542206, 0.4601367712020874], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.1520705223083496, 1.260018229484558, 1.0812824964523315, 0.980938732624054, 0.9224525094032288, 0.8912991285324097, 0.893462061882019, 0.8512019515037537, 0.8315428495407104, 0.7880237102508545, 0.76873779296875, 0.7317528128623962, 0.7129438519477844, 0.6675931215286255, 0.6173984408378601, 0.5677059292793274, 0.5525602102279663, 0.5341611504554749, 0.524205207824707, 0.5094395875930786, 0.49606093764305115, 0.48384615778923035, 0.4702405631542206, 0.4601367712020874]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1246788501739502
current iteration best possible eval_loss (full train run):  -0.4601367712020874
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9303 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5646550059318542, 0.9397340416908264, 0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.2664431631565094, 0.2574614882469177, 0.06447935104370117, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.30395135283470154, 0.8692778944969177, 0.17745260894298553, 0.919781506061554]  ‚Üí  acq = -0.9772305417524437
X = [0.9089382886886597, 0.2508368492126465, 0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6453872919082642, 0.29114675521850586, 0.1965603232383728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6267572045326233, 0.9949815273284912, 0.7691781520843506, 0.9943764209747314]  ‚Üí  acq = -0.9772304835455567
X = [0.8503569960594177, 0.5030623078346252, 0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.18871216475963593, 0.41532599925994873, 0.3550078272819519, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8731500506401062, 0.6550924777984619, 0.047274813055992126, 0.9721140265464783]  ‚Üí  acq = -0.9772304835846097
X = [0.044066667556762695, 0.3236018419265747, 0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8502388000488281, 0.7014566659927368, 0.3285099267959595, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.33359673619270325, 0.12362712621688843, 0.09682413935661316, 0.7159355282783508]  ‚Üí  acq = -0.9772304835454524
X = [0.8981380462646484, 0.41329818964004517, 0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8451046347618103, 0.6754447817802429, 0.39419329166412354, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9611881971359253, 0.1054425835609436, 0.06551053375005722, 0.9501203298568726]  ‚Üí  acq = -0.977210647827089
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0380, dtype=torch.float64), tensor(0.0744, dtype=torch.float64), tensor(0.2756, dtype=torch.float64), tensor(0.1277, dtype=torch.float64), tensor(0.3007, dtype=torch.float64), 0, tensor(0.0937, dtype=torch.float64), tensor(0.0897, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 41, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0002, dtype=torch.float64), tensor(0.0380, dtype=torch.float64), tensor(0.0744, dtype=torch.float64), tensor(0.2756, dtype=torch.float64), tensor(0.1277, dtype=torch.float64), tensor(0.3007, dtype=torch.float64), tensor(4.4727e-18, dtype=torch.float64), tensor(0.0937, dtype=torch.float64), tensor(0.0897, dtype=torch.float64), tensor(0.4631, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3229, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.038
  rowan_hellaswag: 0.074
  sciq: 0.276
  triviaqa: 0.128
  truthfulqa_gen: 0.301
  wikitext: 0
  mmlu: 0.094
  arc_challenge: 0.09

LoRA Parameters:
  lora_r: (41,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  41
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 25,820,160 || all params: 8,056,081,408 || trainable%: 0.3205
length of training data:  9994
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1597, 'grad_norm': 2.407426118850708, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5521694421768188, 'eval_runtime': 3.3838, 'eval_samples_per_second': 295.527, 'eval_steps_per_second': 18.618, 'epoch': 0.04}
{'loss': 1.4713, 'grad_norm': 1.259427547454834, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0160374641418457, 'eval_runtime': 3.389, 'eval_samples_per_second': 295.075, 'eval_steps_per_second': 18.59, 'epoch': 0.08}
{'loss': 1.1822, 'grad_norm': 0.7122302651405334, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8294122219085693, 'eval_runtime': 3.3718, 'eval_samples_per_second': 296.579, 'eval_steps_per_second': 18.684, 'epoch': 0.12}
{'loss': 1.0862, 'grad_norm': 0.5614544153213501, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7566434144973755, 'eval_runtime': 3.3838, 'eval_samples_per_second': 295.528, 'eval_steps_per_second': 18.618, 'epoch': 0.16}
{'loss': 1.069, 'grad_norm': 0.5911340117454529, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.718081533908844, 'eval_runtime': 3.3889, 'eval_samples_per_second': 295.079, 'eval_steps_per_second': 18.59, 'epoch': 0.2}
{'loss': 1.0983, 'grad_norm': 1.0006827116012573, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7035762667655945, 'eval_runtime': 3.3929, 'eval_samples_per_second': 294.732, 'eval_steps_per_second': 18.568, 'epoch': 0.24}
{'loss': 1.0954, 'grad_norm': 0.5818526148796082, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6686474680900574, 'eval_runtime': 3.3954, 'eval_samples_per_second': 294.518, 'eval_steps_per_second': 18.555, 'epoch': 0.28}
{'loss': 0.9838, 'grad_norm': 0.45577186346054077, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6505792140960693, 'eval_runtime': 3.3899, 'eval_samples_per_second': 294.997, 'eval_steps_per_second': 18.585, 'epoch': 0.32}
{'loss': 1.0327, 'grad_norm': 0.6146013736724854, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6279429793357849, 'eval_runtime': 3.3976, 'eval_samples_per_second': 294.324, 'eval_steps_per_second': 18.542, 'epoch': 0.36}
{'loss': 1.0483, 'grad_norm': 0.7764137983322144, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6014884114265442, 'eval_runtime': 3.4045, 'eval_samples_per_second': 293.732, 'eval_steps_per_second': 18.505, 'epoch': 0.4}
{'loss': 1.062, 'grad_norm': 0.44808101654052734, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.600091814994812, 'eval_runtime': 3.3933, 'eval_samples_per_second': 294.699, 'eval_steps_per_second': 18.566, 'epoch': 0.44}
{'loss': 1.0156, 'grad_norm': 0.49039357900619507, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5730599164962769, 'eval_runtime': 3.4007, 'eval_samples_per_second': 294.056, 'eval_steps_per_second': 18.526, 'epoch': 0.48}
{'loss': 1.0494, 'grad_norm': 0.4713055491447449, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5567514896392822, 'eval_runtime': 3.4002, 'eval_samples_per_second': 294.098, 'eval_steps_per_second': 18.528, 'epoch': 0.52}
{'loss': 1.0437, 'grad_norm': 0.623069703578949, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.538025975227356, 'eval_runtime': 3.3996, 'eval_samples_per_second': 294.155, 'eval_steps_per_second': 18.532, 'epoch': 0.56}
{'loss': 0.9661, 'grad_norm': 0.5979686975479126, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.507811427116394, 'eval_runtime': 3.4127, 'eval_samples_per_second': 293.022, 'eval_steps_per_second': 18.46, 'epoch': 0.6}
{'loss': 0.9468, 'grad_norm': 0.5088217258453369, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4884752035140991, 'eval_runtime': 3.3937, 'eval_samples_per_second': 294.659, 'eval_steps_per_second': 18.564, 'epoch': 0.64}
{'loss': 0.9374, 'grad_norm': 0.5420474410057068, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4657754898071289, 'eval_runtime': 3.4348, 'eval_samples_per_second': 291.14, 'eval_steps_per_second': 18.342, 'epoch': 0.68}
{'loss': 0.9983, 'grad_norm': 0.4887621998786926, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.4507666528224945, 'eval_runtime': 3.4366, 'eval_samples_per_second': 290.989, 'eval_steps_per_second': 18.332, 'epoch': 0.72}
{'loss': 0.9451, 'grad_norm': 0.7277199625968933, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.42957785725593567, 'eval_runtime': 3.4398, 'eval_samples_per_second': 290.712, 'eval_steps_per_second': 18.315, 'epoch': 0.76}
{'loss': 0.987, 'grad_norm': 0.6681450009346008, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.4163263440132141, 'eval_runtime': 3.4245, 'eval_samples_per_second': 292.012, 'eval_steps_per_second': 18.397, 'epoch': 0.8}
{'loss': 0.9764, 'grad_norm': 0.599687933921814, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.40754958987236023, 'eval_runtime': 3.4172, 'eval_samples_per_second': 292.634, 'eval_steps_per_second': 18.436, 'epoch': 0.84}
{'loss': 0.9899, 'grad_norm': 0.576433002948761, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3844982981681824, 'eval_runtime': 3.4237, 'eval_samples_per_second': 292.084, 'eval_steps_per_second': 18.401, 'epoch': 0.88}
{'loss': 0.9585, 'grad_norm': 0.40497323870658875, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3727814555168152, 'eval_runtime': 3.4007, 'eval_samples_per_second': 294.056, 'eval_steps_per_second': 18.526, 'epoch': 0.92}
{'loss': 1.0521, 'grad_norm': 0.5216739177703857, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3651191294193268, 'eval_runtime': 3.4039, 'eval_samples_per_second': 293.785, 'eval_steps_per_second': 18.508, 'epoch': 0.96}
{'loss': 0.9598, 'grad_norm': 1.163308024406433, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.36292925477027893, 'eval_runtime': 3.4004, 'eval_samples_per_second': 294.079, 'eval_steps_per_second': 18.527, 'epoch': 1.0}
{'train_runtime': 278.112, 'train_samples_per_second': 35.935, 'train_steps_per_second': 2.247, 'train_loss': 1.1245947509765626, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5521694421768188, 1.0160374641418457, 0.8294122219085693, 0.7566434144973755, 0.718081533908844, 0.7035762667655945, 0.6686474680900574, 0.6505792140960693, 0.6279429793357849, 0.6014884114265442, 0.600091814994812, 0.5730599164962769, 0.5567514896392822, 0.538025975227356, 0.507811427116394, 0.4884752035140991, 0.4657754898071289, 0.4507666528224945, 0.42957785725593567, 0.4163263440132141, 0.40754958987236023, 0.3844982981681824, 0.3727814555168152, 0.3651191294193268, 0.36292925477027893], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5521694421768188, 1.0160374641418457, 0.8294122219085693, 0.7566434144973755, 0.718081533908844, 0.7035762667655945, 0.6686474680900574, 0.6505792140960693, 0.6279429793357849, 0.6014884114265442, 0.600091814994812, 0.5730599164962769, 0.5567514896392822, 0.538025975227356, 0.507811427116394, 0.4884752035140991, 0.4657754898071289, 0.4507666528224945, 0.42957785725593567, 0.4163263440132141, 0.40754958987236023, 0.3844982981681824, 0.3727814555168152, 0.3651191294193268, 0.36292925477027893]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.36292925477027893
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.6245 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.9136347966275354
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.913491655760231
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.9136347966275354
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.9136347966275354
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.9136347916557743
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1567, dtype=torch.float64), 0, tensor(0.5744, dtype=torch.float64), 0, tensor(0.1524, dtype=torch.float64), tensor(0.1165, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 91, 3.469446951953616e-19, 38.21624730487854, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(4.8559e-18, dtype=torch.float64), tensor(1.7931e-18, dtype=torch.float64), tensor(0.1567, dtype=torch.float64), tensor(6.0214e-18, dtype=torch.float64), tensor(0.5744, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1524, dtype=torch.float64), tensor(0.1165, dtype=torch.float64), tensor(0.4744, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7090, dtype=torch.float64), tensor(3.4694e-18, dtype=torch.float64), tensor(0.7962, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.157
  triviaqa: 0
  truthfulqa_gen: 0.574
  wikitext: 0
  mmlu: 0.152
  arc_challenge: 0.117

LoRA Parameters:
  lora_r: (91,)
  lora_dropout: (3.469446951953616e-19,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (38.21624730487854,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  91
lora dropout:  3.469446951953616e-19
lora alpha:  38.21624730487854
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 57,308,160 || all params: 8,087,569,408 || trainable%: 0.7086
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3238, 'grad_norm': 1.4543498754501343, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4804638624191284, 'eval_runtime': 3.414, 'eval_samples_per_second': 292.913, 'eval_steps_per_second': 18.453, 'epoch': 0.04}
{'loss': 1.2752, 'grad_norm': 0.6824029088020325, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9348036646842957, 'eval_runtime': 3.4199, 'eval_samples_per_second': 292.406, 'eval_steps_per_second': 18.422, 'epoch': 0.08}
{'loss': 1.0471, 'grad_norm': 0.42689886689186096, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7877190113067627, 'eval_runtime': 3.4265, 'eval_samples_per_second': 291.847, 'eval_steps_per_second': 18.386, 'epoch': 0.12}
{'loss': 0.9241, 'grad_norm': 1.1186801195144653, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6953754425048828, 'eval_runtime': 3.413, 'eval_samples_per_second': 293.001, 'eval_steps_per_second': 18.459, 'epoch': 0.16}
{'loss': 0.9158, 'grad_norm': 0.4120483100414276, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6538378000259399, 'eval_runtime': 3.4179, 'eval_samples_per_second': 292.581, 'eval_steps_per_second': 18.433, 'epoch': 0.2}
{'loss': 0.9016, 'grad_norm': 0.43490108847618103, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6048555374145508, 'eval_runtime': 3.4231, 'eval_samples_per_second': 292.131, 'eval_steps_per_second': 18.404, 'epoch': 0.24}
{'loss': 0.8439, 'grad_norm': 0.35812053084373474, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5755658149719238, 'eval_runtime': 3.425, 'eval_samples_per_second': 291.972, 'eval_steps_per_second': 18.394, 'epoch': 0.28}
{'loss': 0.8006, 'grad_norm': 0.3369508683681488, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5427535176277161, 'eval_runtime': 3.4259, 'eval_samples_per_second': 291.897, 'eval_steps_per_second': 18.389, 'epoch': 0.32}
{'loss': 0.7733, 'grad_norm': 0.3443175256252289, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.49503955245018005, 'eval_runtime': 3.4332, 'eval_samples_per_second': 291.273, 'eval_steps_per_second': 18.35, 'epoch': 0.36}
{'loss': 0.7794, 'grad_norm': 0.48608827590942383, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.46160322427749634, 'eval_runtime': 3.4256, 'eval_samples_per_second': 291.922, 'eval_steps_per_second': 18.391, 'epoch': 0.4}
{'loss': 0.7253, 'grad_norm': 0.40131452679634094, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.43103674054145813, 'eval_runtime': 3.4333, 'eval_samples_per_second': 291.265, 'eval_steps_per_second': 18.35, 'epoch': 0.44}
{'loss': 0.8214, 'grad_norm': 0.44355711340904236, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.40581071376800537, 'eval_runtime': 3.4325, 'eval_samples_per_second': 291.329, 'eval_steps_per_second': 18.354, 'epoch': 0.48}
{'loss': 0.7771, 'grad_norm': 0.4148462414741516, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.37623393535614014, 'eval_runtime': 3.4446, 'eval_samples_per_second': 290.311, 'eval_steps_per_second': 18.29, 'epoch': 0.52}
{'loss': 0.7492, 'grad_norm': 0.5913669466972351, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3472340703010559, 'eval_runtime': 3.437, 'eval_samples_per_second': 290.951, 'eval_steps_per_second': 18.33, 'epoch': 0.56}
{'loss': 0.74, 'grad_norm': 0.3710002899169922, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.32824012637138367, 'eval_runtime': 3.4395, 'eval_samples_per_second': 290.739, 'eval_steps_per_second': 18.317, 'epoch': 0.6}
{'loss': 0.7086, 'grad_norm': 0.3691803216934204, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.31651225686073303, 'eval_runtime': 3.4328, 'eval_samples_per_second': 291.304, 'eval_steps_per_second': 18.352, 'epoch': 0.64}
{'loss': 0.719, 'grad_norm': 0.4810296595096588, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.30420252680778503, 'eval_runtime': 3.4305, 'eval_samples_per_second': 291.507, 'eval_steps_per_second': 18.365, 'epoch': 0.68}
{'loss': 0.6888, 'grad_norm': 0.470568984746933, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.2789837718009949, 'eval_runtime': 3.4345, 'eval_samples_per_second': 291.16, 'eval_steps_per_second': 18.343, 'epoch': 0.72}
{'loss': 0.6737, 'grad_norm': 0.42130833864212036, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2600378692150116, 'eval_runtime': 3.4366, 'eval_samples_per_second': 290.983, 'eval_steps_per_second': 18.332, 'epoch': 0.76}
{'loss': 0.7067, 'grad_norm': 0.38213050365448, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.24738650023937225, 'eval_runtime': 3.4449, 'eval_samples_per_second': 290.282, 'eval_steps_per_second': 18.288, 'epoch': 0.8}
{'loss': 0.6855, 'grad_norm': 0.3952389061450958, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2353975623846054, 'eval_runtime': 3.4369, 'eval_samples_per_second': 290.956, 'eval_steps_per_second': 18.33, 'epoch': 0.84}
{'loss': 0.6675, 'grad_norm': 0.3257063925266266, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2259446084499359, 'eval_runtime': 3.4367, 'eval_samples_per_second': 290.978, 'eval_steps_per_second': 18.332, 'epoch': 0.88}
{'loss': 0.6758, 'grad_norm': 0.4208163917064667, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.21676082909107208, 'eval_runtime': 3.4406, 'eval_samples_per_second': 290.643, 'eval_steps_per_second': 18.311, 'epoch': 0.92}
{'loss': 0.6797, 'grad_norm': 0.5912423729896545, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.21055038273334503, 'eval_runtime': 3.4377, 'eval_samples_per_second': 290.89, 'eval_steps_per_second': 18.326, 'epoch': 0.96}
{'loss': 0.6274, 'grad_norm': 0.6644884347915649, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.20690536499023438, 'eval_runtime': 3.4332, 'eval_samples_per_second': 291.276, 'eval_steps_per_second': 18.35, 'epoch': 1.0}
{'train_runtime': 249.5321, 'train_samples_per_second': 40.067, 'train_steps_per_second': 2.505, 'train_loss': 0.8892146865844727, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4804638624191284, 0.9348036646842957, 0.7877190113067627, 0.6953754425048828, 0.6538378000259399, 0.6048555374145508, 0.5755658149719238, 0.5427535176277161, 0.49503955245018005, 0.46160322427749634, 0.43103674054145813, 0.40581071376800537, 0.37623393535614014, 0.3472340703010559, 0.32824012637138367, 0.31651225686073303, 0.30420252680778503, 0.2789837718009949, 0.2600378692150116, 0.24738650023937225, 0.2353975623846054, 0.2259446084499359, 0.21676082909107208, 0.21055038273334503, 0.20690536499023438], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4804638624191284, 0.9348036646842957, 0.7877190113067627, 0.6953754425048828, 0.6538378000259399, 0.6048555374145508, 0.5755658149719238, 0.5427535176277161, 0.49503955245018005, 0.46160322427749634, 0.43103674054145813, 0.40581071376800537, 0.37623393535614014, 0.3472340703010559, 0.32824012637138367, 0.31651225686073303, 0.30420252680778503, 0.2789837718009949, 0.2600378692150116, 0.24738650023937225, 0.2353975623846054, 0.2259446084499359, 0.21676082909107208, 0.21055038273334503, 0.20690536499023438]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1241222620010376
current iteration best possible eval_loss (full train run):  -0.20690536499023438
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.2835 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -0.9925904393281206
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -0.9925242940894016
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -0.9925903336788168
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -0.9925832992554113
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -0.9925903336784535
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0605, dtype=torch.float64), tensor(0.1374, dtype=torch.float64), tensor(0.5728, dtype=torch.float64), 0, 0, tensor(0.0291, dtype=torch.float64), 0, tensor(0.2002, dtype=torch.float64), 16, 1, 1, 1, 0, 1, 128, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0605, dtype=torch.float64), tensor(0.1374, dtype=torch.float64), tensor(0.5728, dtype=torch.float64), tensor(1.0590e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(6.0013e-18, dtype=torch.float64), tensor(0.2002, dtype=torch.float64), tensor(0.4994, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.06
  rowan_hellaswag: 0.137
  sciq: 0.573
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.029
  mmlu: 0
  arc_challenge: 0.2

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 102,760,448 || all params: 8,133,021,696 || trainable%: 1.2635
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2374, 'grad_norm': 0.5576761364936829, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8974387645721436, 'eval_runtime': 3.4018, 'eval_samples_per_second': 293.96, 'eval_steps_per_second': 18.519, 'epoch': 0.04}
{'loss': 1.5462, 'grad_norm': 0.3629874289035797, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4635496139526367, 'eval_runtime': 3.3997, 'eval_samples_per_second': 294.144, 'eval_steps_per_second': 18.531, 'epoch': 0.08}
{'loss': 1.3106, 'grad_norm': 0.32056692242622375, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3904153108596802, 'eval_runtime': 3.3947, 'eval_samples_per_second': 294.577, 'eval_steps_per_second': 18.558, 'epoch': 0.12}
{'loss': 1.2191, 'grad_norm': 0.2436503916978836, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2783375978469849, 'eval_runtime': 3.3816, 'eval_samples_per_second': 295.716, 'eval_steps_per_second': 18.63, 'epoch': 0.16}
{'loss': 1.2959, 'grad_norm': 0.29327741265296936, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3283625841140747, 'eval_runtime': 3.3748, 'eval_samples_per_second': 296.312, 'eval_steps_per_second': 18.668, 'epoch': 0.2}
{'loss': 1.2293, 'grad_norm': 0.2638336420059204, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3139946460723877, 'eval_runtime': 3.3778, 'eval_samples_per_second': 296.048, 'eval_steps_per_second': 18.651, 'epoch': 0.24}
{'loss': 1.1695, 'grad_norm': 0.21758124232292175, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3102002143859863, 'eval_runtime': 3.3808, 'eval_samples_per_second': 295.784, 'eval_steps_per_second': 18.634, 'epoch': 0.28}
{'loss': 1.1484, 'grad_norm': 0.24195142090320587, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.17563796043396, 'eval_runtime': 3.3763, 'eval_samples_per_second': 296.179, 'eval_steps_per_second': 18.659, 'epoch': 0.32}
{'loss': 1.1538, 'grad_norm': 0.1974429339170456, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1737699508666992, 'eval_runtime': 3.4107, 'eval_samples_per_second': 293.191, 'eval_steps_per_second': 18.471, 'epoch': 0.36}
{'loss': 1.0836, 'grad_norm': 0.24409936368465424, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1761447191238403, 'eval_runtime': 3.4014, 'eval_samples_per_second': 293.995, 'eval_steps_per_second': 18.522, 'epoch': 0.4}
{'loss': 1.0909, 'grad_norm': 0.23430828750133514, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2397080659866333, 'eval_runtime': 3.4041, 'eval_samples_per_second': 293.763, 'eval_steps_per_second': 18.507, 'epoch': 0.44}
{'loss': 1.1297, 'grad_norm': 0.2372012883424759, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.160930871963501, 'eval_runtime': 3.4181, 'eval_samples_per_second': 292.558, 'eval_steps_per_second': 18.431, 'epoch': 0.48}
{'loss': 1.108, 'grad_norm': 0.267953485250473, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1822104454040527, 'eval_runtime': 3.4153, 'eval_samples_per_second': 292.799, 'eval_steps_per_second': 18.446, 'epoch': 0.52}
{'loss': 1.1237, 'grad_norm': 0.2476697713136673, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1984680891036987, 'eval_runtime': 3.4009, 'eval_samples_per_second': 294.041, 'eval_steps_per_second': 18.525, 'epoch': 0.56}
{'loss': 1.1498, 'grad_norm': 0.23383964598178864, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1938445568084717, 'eval_runtime': 3.3958, 'eval_samples_per_second': 294.481, 'eval_steps_per_second': 18.552, 'epoch': 0.6}
{'loss': 1.0633, 'grad_norm': 0.24870829284191132, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1714726686477661, 'eval_runtime': 3.3842, 'eval_samples_per_second': 295.489, 'eval_steps_per_second': 18.616, 'epoch': 0.64}
{'loss': 1.0132, 'grad_norm': 0.3123481273651123, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1241286993026733, 'eval_runtime': 3.3819, 'eval_samples_per_second': 295.69, 'eval_steps_per_second': 18.628, 'epoch': 0.68}
{'loss': 1.0425, 'grad_norm': 0.23121199011802673, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2086158990859985, 'eval_runtime': 3.3818, 'eval_samples_per_second': 295.702, 'eval_steps_per_second': 18.629, 'epoch': 0.72}
{'loss': 1.0805, 'grad_norm': 0.25423458218574524, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2341506481170654, 'eval_runtime': 3.3788, 'eval_samples_per_second': 295.96, 'eval_steps_per_second': 18.646, 'epoch': 0.76}
{'loss': 1.0172, 'grad_norm': 0.29344478249549866, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1705557107925415, 'eval_runtime': 3.3771, 'eval_samples_per_second': 296.115, 'eval_steps_per_second': 18.655, 'epoch': 0.8}
{'loss': 1.0786, 'grad_norm': 0.2725018262863159, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1980407238006592, 'eval_runtime': 3.3789, 'eval_samples_per_second': 295.956, 'eval_steps_per_second': 18.645, 'epoch': 0.84}
{'loss': 1.1043, 'grad_norm': 0.2559821605682373, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.218984603881836, 'eval_runtime': 3.3771, 'eval_samples_per_second': 296.114, 'eval_steps_per_second': 18.655, 'epoch': 0.88}
{'loss': 0.9758, 'grad_norm': 0.2599380612373352, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2054380178451538, 'eval_runtime': 3.3752, 'eval_samples_per_second': 296.275, 'eval_steps_per_second': 18.665, 'epoch': 0.92}
{'loss': 1.0138, 'grad_norm': 0.27560821175575256, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1924608945846558, 'eval_runtime': 3.3769, 'eval_samples_per_second': 296.134, 'eval_steps_per_second': 18.656, 'epoch': 0.96}
{'loss': 1.0337, 'grad_norm': 0.25285017490386963, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1953213214874268, 'eval_runtime': 3.3819, 'eval_samples_per_second': 295.689, 'eval_steps_per_second': 18.628, 'epoch': 1.0}
{'train_runtime': 287.595, 'train_samples_per_second': 34.761, 'train_steps_per_second': 2.173, 'train_loss': 1.216745803833008, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8974387645721436, 1.4635496139526367, 1.3904153108596802, 1.2783375978469849, 1.3283625841140747, 1.3139946460723877, 1.3102002143859863, 1.17563796043396, 1.1737699508666992, 1.1761447191238403, 1.2397080659866333, 1.160930871963501, 1.1822104454040527, 1.1984680891036987, 1.1938445568084717, 1.1714726686477661, 1.1241286993026733, 1.2086158990859985, 1.2341506481170654, 1.1705557107925415, 1.1980407238006592, 1.218984603881836, 1.2054380178451538, 1.1924608945846558, 1.1953213214874268], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8974387645721436, 1.4635496139526367, 1.3904153108596802, 1.2783375978469849, 1.3283625841140747, 1.3139946460723877, 1.3102002143859863, 1.17563796043396, 1.1737699508666992, 1.1761447191238403, 1.2397080659866333, 1.160930871963501, 1.1822104454040527, 1.1984680891036987, 1.1938445568084717, 1.1714726686477661, 1.1241286993026733, 1.2086158990859985, 1.2341506481170654, 1.1705557107925415, 1.1980407238006592, 1.218984603881836, 1.2054380178451538, 1.1924608945846558, 1.1953213214874268]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1211590766906738
current iteration best possible eval_loss (full train run):  -1.1953213214874268
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.4635 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -1.0359274337526183
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -1.0397703290734537
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -1.0359282349241625
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -1.0277558647174916
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -1.0455607936993652
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0582, dtype=torch.float64), tensor(0.3245, dtype=torch.float64), 0, 0, tensor(0.0594, dtype=torch.float64), tensor(0.0530, dtype=torch.float64), tensor(0.0838, dtype=torch.float64), tensor(0.0988, dtype=torch.float64), tensor(0.3101, dtype=torch.float64), 17, 1, 1, 1, 1, 1, 73, 0.05337732623639885, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0582, dtype=torch.float64), tensor(0.3245, dtype=torch.float64), tensor(0.0099, dtype=torch.float64), tensor(0.0023, dtype=torch.float64), tensor(0.0594, dtype=torch.float64), tensor(0.0530, dtype=torch.float64), tensor(0.0838, dtype=torch.float64), tensor(0.0988, dtype=torch.float64), tensor(0.3101, dtype=torch.float64), tensor(0.5161, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5705, dtype=torch.float64), tensor(0.5338, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.058
  gsm8k: 0.324
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.059
  truthfulqa_gen: 0.053
  wikitext: 0.084
  mmlu: 0.099
  arc_challenge: 0.31

LoRA Parameters:
  lora_r: (73,)
  lora_dropout: (0.05337732623639885,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  73
lora dropout:  0.05337732623639885
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 85,142,528 || all params: 8,115,403,776 || trainable%: 1.0491
length of training data:  9875
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.2629, 'grad_norm': 0.8781651258468628, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8022164106369019, 'eval_runtime': 3.7551, 'eval_samples_per_second': 266.303, 'eval_steps_per_second': 16.777, 'epoch': 0.04}
{'loss': 1.1914, 'grad_norm': 0.6464482545852661, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1196328401565552, 'eval_runtime': 3.751, 'eval_samples_per_second': 266.596, 'eval_steps_per_second': 16.796, 'epoch': 0.08}
{'loss': 1.134, 'grad_norm': 0.3821820020675659, 'learning_rate': 0.0002873239436619718, 'epoch': 0.12}
{'eval_loss': 0.9932860732078552, 'eval_runtime': 3.7418, 'eval_samples_per_second': 267.253, 'eval_steps_per_second': 16.837, 'epoch': 0.12}
{'loss': 1.0214, 'grad_norm': 0.3407645523548126, 'learning_rate': 0.0002741197183098591, 'epoch': 0.16}
{'eval_loss': 0.9217448830604553, 'eval_runtime': 3.744, 'eval_samples_per_second': 267.092, 'eval_steps_per_second': 16.827, 'epoch': 0.16}
{'loss': 1.0689, 'grad_norm': 0.33149605989456177, 'learning_rate': 0.0002609154929577464, 'epoch': 0.2}
{'eval_loss': 0.8769576549530029, 'eval_runtime': 3.7496, 'eval_samples_per_second': 266.693, 'eval_steps_per_second': 16.802, 'epoch': 0.2}
{'loss': 1.0115, 'grad_norm': 0.3517293930053711, 'learning_rate': 0.0002477112676056338, 'epoch': 0.24}
{'eval_loss': 0.8427855372428894, 'eval_runtime': 3.768, 'eval_samples_per_second': 265.396, 'eval_steps_per_second': 16.72, 'epoch': 0.24}
{'loss': 0.9461, 'grad_norm': 0.35571885108947754, 'learning_rate': 0.00023450704225352109, 'epoch': 0.28}
{'eval_loss': 0.8083934783935547, 'eval_runtime': 3.7585, 'eval_samples_per_second': 266.066, 'eval_steps_per_second': 16.762, 'epoch': 0.28}
{'loss': 0.9344, 'grad_norm': 0.2660094201564789, 'learning_rate': 0.00022130281690140843, 'epoch': 0.32}
{'eval_loss': 0.7897730469703674, 'eval_runtime': 3.7568, 'eval_samples_per_second': 266.181, 'eval_steps_per_second': 16.769, 'epoch': 0.32}
{'loss': 0.8972, 'grad_norm': 0.2971700429916382, 'learning_rate': 0.00020809859154929575, 'epoch': 0.36}
{'eval_loss': 0.7882336378097534, 'eval_runtime': 3.7806, 'eval_samples_per_second': 264.512, 'eval_steps_per_second': 16.664, 'epoch': 0.36}
{'loss': 0.9345, 'grad_norm': 0.3639146685600281, 'learning_rate': 0.00019489436619718307, 'epoch': 0.4}
{'eval_loss': 0.7774616479873657, 'eval_runtime': 3.7927, 'eval_samples_per_second': 263.663, 'eval_steps_per_second': 16.611, 'epoch': 0.4}
{'loss': 0.9052, 'grad_norm': 0.2549578547477722, 'learning_rate': 0.0001816901408450704, 'epoch': 0.44}
{'eval_loss': 0.7662363052368164, 'eval_runtime': 3.8062, 'eval_samples_per_second': 262.731, 'eval_steps_per_second': 16.552, 'epoch': 0.44}
{'loss': 0.8735, 'grad_norm': 0.34300291538238525, 'learning_rate': 0.0001684859154929577, 'epoch': 0.49}
{'eval_loss': 0.769973874092102, 'eval_runtime': 3.7672, 'eval_samples_per_second': 265.446, 'eval_steps_per_second': 16.723, 'epoch': 0.49}
{'loss': 0.8761, 'grad_norm': 0.4206363260746002, 'learning_rate': 0.00015528169014084506, 'epoch': 0.53}
{'eval_loss': 0.7440425753593445, 'eval_runtime': 3.7918, 'eval_samples_per_second': 263.729, 'eval_steps_per_second': 16.615, 'epoch': 0.53}
{'loss': 0.9159, 'grad_norm': 0.3102604150772095, 'learning_rate': 0.00014207746478873238, 'epoch': 0.57}
{'eval_loss': 0.7512960433959961, 'eval_runtime': 3.8036, 'eval_samples_per_second': 262.906, 'eval_steps_per_second': 16.563, 'epoch': 0.57}
{'loss': 0.871, 'grad_norm': 0.3125855326652527, 'learning_rate': 0.0001288732394366197, 'epoch': 0.61}
{'eval_loss': 0.7288727760314941, 'eval_runtime': 3.7933, 'eval_samples_per_second': 263.621, 'eval_steps_per_second': 16.608, 'epoch': 0.61}
{'loss': 0.8835, 'grad_norm': 0.4061504006385803, 'learning_rate': 0.00011566901408450703, 'epoch': 0.65}
{'eval_loss': 0.7203865647315979, 'eval_runtime': 3.7878, 'eval_samples_per_second': 264.005, 'eval_steps_per_second': 16.632, 'epoch': 0.65}
{'loss': 0.9326, 'grad_norm': 0.5249751210212708, 'learning_rate': 0.00010246478873239435, 'epoch': 0.69}
{'eval_loss': 0.7186152338981628, 'eval_runtime': 3.7827, 'eval_samples_per_second': 264.361, 'eval_steps_per_second': 16.655, 'epoch': 0.69}
{'loss': 0.8743, 'grad_norm': 0.2612811028957367, 'learning_rate': 8.926056338028169e-05, 'epoch': 0.73}
{'eval_loss': 0.7161896228790283, 'eval_runtime': 3.7866, 'eval_samples_per_second': 264.091, 'eval_steps_per_second': 16.638, 'epoch': 0.73}
{'loss': 0.8326, 'grad_norm': 0.3946996331214905, 'learning_rate': 7.6056338028169e-05, 'epoch': 0.77}
{'eval_loss': 0.701842725276947, 'eval_runtime': 3.787, 'eval_samples_per_second': 264.06, 'eval_steps_per_second': 16.636, 'epoch': 0.77}
{'loss': 0.8567, 'grad_norm': 0.48096007108688354, 'learning_rate': 6.285211267605634e-05, 'epoch': 0.81}
{'eval_loss': 0.6981362104415894, 'eval_runtime': 3.7678, 'eval_samples_per_second': 265.406, 'eval_steps_per_second': 16.721, 'epoch': 0.81}
{'loss': 0.8342, 'grad_norm': 0.5314590334892273, 'learning_rate': 4.964788732394366e-05, 'epoch': 0.85}
{'eval_loss': 0.6982713341712952, 'eval_runtime': 3.7685, 'eval_samples_per_second': 265.359, 'eval_steps_per_second': 16.718, 'epoch': 0.85}
{'loss': 0.8238, 'grad_norm': 0.35617348551750183, 'learning_rate': 3.6443661971830985e-05, 'epoch': 0.89}
{'eval_loss': 0.6955497860908508, 'eval_runtime': 3.763, 'eval_samples_per_second': 265.742, 'eval_steps_per_second': 16.742, 'epoch': 0.89}
{'loss': 0.8226, 'grad_norm': 0.5936594605445862, 'learning_rate': 2.3239436619718305e-05, 'epoch': 0.93}
{'eval_loss': 0.6900307536125183, 'eval_runtime': 3.7663, 'eval_samples_per_second': 265.51, 'eval_steps_per_second': 16.727, 'epoch': 0.93}
{'loss': 0.8764, 'grad_norm': 0.5631688833236694, 'learning_rate': 1.0035211267605631e-05, 'epoch': 0.97}
{'eval_loss': 0.6891473531723022, 'eval_runtime': 3.7631, 'eval_samples_per_second': 265.742, 'eval_steps_per_second': 16.742, 'epoch': 0.97}
{'train_runtime': 326.5262, 'train_samples_per_second': 30.243, 'train_steps_per_second': 1.893, 'train_loss': 0.9794323698988239, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8022164106369019, 1.1196328401565552, 0.9932860732078552, 0.9217448830604553, 0.8769576549530029, 0.8427855372428894, 0.8083934783935547, 0.7897730469703674, 0.7882336378097534, 0.7774616479873657, 0.7662363052368164, 0.769973874092102, 0.7440425753593445, 0.7512960433959961, 0.7288727760314941, 0.7203865647315979, 0.7186152338981628, 0.7161896228790283, 0.701842725276947, 0.6981362104415894, 0.6982713341712952, 0.6955497860908508, 0.6900307536125183, 0.6891473531723022], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.8022164106369019, 1.1196328401565552, 0.9932860732078552, 0.9217448830604553, 0.8769576549530029, 0.8427855372428894, 0.8083934783935547, 0.7897730469703674, 0.7882336378097534, 0.7774616479873657, 0.7662363052368164, 0.769973874092102, 0.7440425753593445, 0.7512960433959961, 0.7288727760314941, 0.7203865647315979, 0.7186152338981628, 0.7161896228790283, 0.701842725276947, 0.6981362104415894, 0.6982713341712952, 0.6955497860908508, 0.6900307536125183, 0.6891473531723022]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1222003698349
current iteration best possible eval_loss (full train run):  -0.6891473531723022
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.7055 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5840210318565369, 0.5403404831886292, 0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.49047714471817017, 0.8636659979820251, 0.10396885871887207, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.09389074146747589, 0.5328817963600159, 0.1704922765493393, 0.8951139450073242]  ‚Üí  acq = -1.039748355926274
X = [0.20226186513900757, 0.6831576228141785, 0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.24446256458759308, 0.7839422821998596, 0.6810406446456909, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.06929440051317215, 0.9363643527030945, 0.4914133846759796, 0.07482516765594482]  ‚Üí  acq = -1.1516418325279552
X = [0.7401218414306641, 0.8363668322563171, 0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.5837264060974121, 0.37286609411239624, 0.8461750149726868, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.32410305738449097, 0.8762340545654297, 0.1423635184764862, 0.6519256234169006]  ‚Üí  acq = -1.0398954108195966
X = [0.5809492468833923, 0.9983730316162109, 0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.5152989625930786, 0.0025587081909179688, 0.5287423133850098, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.04051867127418518, 0.9936825633049011, 0.13591282069683075, 0.053788065910339355]  ‚Üí  acq = -1.0397483559263645
X = [0.3528999090194702, 0.5949426293373108, 0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.40121349692344666, 0.7189667820930481, 0.7950335144996643, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4900456964969635, 0.8887658715248108, 0.9805734157562256, 0.013015925884246826]  ‚Üí  acq = -1.0407161887974887
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0642, dtype=torch.float64), 0, tensor(0.0189, dtype=torch.float64), tensor(0.2169, dtype=torch.float64), tensor(0.0222, dtype=torch.float64), 0, tensor(0.0482, dtype=torch.float64), tensor(0.1576, dtype=torch.float64), tensor(0.4718, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 2, 1.458443567833615e-20, 26.290412349218936, 1]
normalized proposed parameters for next round by BO: [tensor(0.0642, dtype=torch.float64), tensor(8.3451e-19, dtype=torch.float64), tensor(0.0189, dtype=torch.float64), tensor(0.2169, dtype=torch.float64), tensor(0.0222, dtype=torch.float64), tensor(1.4898e-19, dtype=torch.float64), tensor(0.0482, dtype=torch.float64), tensor(0.1576, dtype=torch.float64), tensor(0.4718, dtype=torch.float64), tensor(0.4731, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.4584e-19, dtype=torch.float64), tensor(0.5477, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.064
  gsm8k: 0
  rowan_hellaswag: 0.019
  sciq: 0.217
  triviaqa: 0.022
  truthfulqa_gen: 0
  wikitext: 0.048
  mmlu: 0.158
  arc_challenge: 0.472

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.458443567833615e-20,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (26.290412349218936,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  2
lora dropout:  1.458443567833615e-20
lora alpha:  26.290412349218936
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,259,520 || all params: 8,031,520,768 || trainable%: 0.0157
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0404, 'grad_norm': 11.164600372314453, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0375819206237793, 'eval_runtime': 3.3153, 'eval_samples_per_second': 301.633, 'eval_steps_per_second': 19.003, 'epoch': 0.04}
{'loss': 1.3863, 'grad_norm': 3.279250383377075, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2654597759246826, 'eval_runtime': 3.3165, 'eval_samples_per_second': 301.527, 'eval_steps_per_second': 18.996, 'epoch': 0.08}
{'loss': 1.1203, 'grad_norm': 1.7321557998657227, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2285258769989014, 'eval_runtime': 3.3041, 'eval_samples_per_second': 302.655, 'eval_steps_per_second': 19.067, 'epoch': 0.12}
{'loss': 1.0588, 'grad_norm': 2.1278159618377686, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1373910903930664, 'eval_runtime': 3.3124, 'eval_samples_per_second': 301.898, 'eval_steps_per_second': 19.02, 'epoch': 0.16}
{'loss': 1.0062, 'grad_norm': 1.3587366342544556, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1470012664794922, 'eval_runtime': 3.324, 'eval_samples_per_second': 300.844, 'eval_steps_per_second': 18.953, 'epoch': 0.2}
{'loss': 1.0395, 'grad_norm': 1.4491593837738037, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.182523488998413, 'eval_runtime': 3.3291, 'eval_samples_per_second': 300.383, 'eval_steps_per_second': 18.924, 'epoch': 0.24}
{'loss': 1.0462, 'grad_norm': 1.6148604154586792, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.216732382774353, 'eval_runtime': 3.3345, 'eval_samples_per_second': 299.898, 'eval_steps_per_second': 18.894, 'epoch': 0.28}
{'loss': 0.9916, 'grad_norm': 1.482954740524292, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1517215967178345, 'eval_runtime': 3.328, 'eval_samples_per_second': 300.485, 'eval_steps_per_second': 18.931, 'epoch': 0.32}
{'loss': 0.9635, 'grad_norm': 1.8816866874694824, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2253581285476685, 'eval_runtime': 3.3296, 'eval_samples_per_second': 300.34, 'eval_steps_per_second': 18.921, 'epoch': 0.36}
{'loss': 0.9962, 'grad_norm': 1.50014066696167, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1849210262298584, 'eval_runtime': 3.3307, 'eval_samples_per_second': 300.236, 'eval_steps_per_second': 18.915, 'epoch': 0.4}
{'loss': 0.9313, 'grad_norm': 1.6357027292251587, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1816014051437378, 'eval_runtime': 3.3403, 'eval_samples_per_second': 299.376, 'eval_steps_per_second': 18.861, 'epoch': 0.44}
{'loss': 0.9227, 'grad_norm': 1.7730129957199097, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1969178915023804, 'eval_runtime': 3.3339, 'eval_samples_per_second': 299.948, 'eval_steps_per_second': 18.897, 'epoch': 0.48}
{'loss': 0.9418, 'grad_norm': 2.261946678161621, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2044445276260376, 'eval_runtime': 3.3294, 'eval_samples_per_second': 300.353, 'eval_steps_per_second': 18.922, 'epoch': 0.52}
{'loss': 0.9279, 'grad_norm': 4.349666118621826, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.202663779258728, 'eval_runtime': 3.3349, 'eval_samples_per_second': 299.862, 'eval_steps_per_second': 18.891, 'epoch': 0.56}
{'loss': 0.9535, 'grad_norm': 1.74429190158844, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.189389705657959, 'eval_runtime': 3.3295, 'eval_samples_per_second': 300.342, 'eval_steps_per_second': 18.922, 'epoch': 0.6}
{'loss': 0.904, 'grad_norm': 1.8649989366531372, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2205970287322998, 'eval_runtime': 3.3315, 'eval_samples_per_second': 300.162, 'eval_steps_per_second': 18.91, 'epoch': 0.64}
{'loss': 0.9052, 'grad_norm': 2.411860942840576, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2081937789916992, 'eval_runtime': 3.3299, 'eval_samples_per_second': 300.309, 'eval_steps_per_second': 18.919, 'epoch': 0.68}
{'loss': 0.9284, 'grad_norm': 2.216942310333252, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1683759689331055, 'eval_runtime': 3.3307, 'eval_samples_per_second': 300.236, 'eval_steps_per_second': 18.915, 'epoch': 0.72}
{'loss': 0.944, 'grad_norm': 2.0300838947296143, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.182250738143921, 'eval_runtime': 3.3332, 'eval_samples_per_second': 300.011, 'eval_steps_per_second': 18.901, 'epoch': 0.76}
{'loss': 0.8925, 'grad_norm': 2.266040802001953, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1702638864517212, 'eval_runtime': 3.3395, 'eval_samples_per_second': 299.446, 'eval_steps_per_second': 18.865, 'epoch': 0.8}
{'loss': 0.8865, 'grad_norm': 2.4357705116271973, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.197325587272644, 'eval_runtime': 3.3314, 'eval_samples_per_second': 300.178, 'eval_steps_per_second': 18.911, 'epoch': 0.84}
{'loss': 0.8187, 'grad_norm': 2.316761016845703, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1891570091247559, 'eval_runtime': 3.3335, 'eval_samples_per_second': 299.989, 'eval_steps_per_second': 18.899, 'epoch': 0.88}
{'loss': 0.8426, 'grad_norm': 3.2797467708587646, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1867566108703613, 'eval_runtime': 3.3348, 'eval_samples_per_second': 299.869, 'eval_steps_per_second': 18.892, 'epoch': 0.92}
{'loss': 0.843, 'grad_norm': 3.5526397228240967, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2069836854934692, 'eval_runtime': 3.3322, 'eval_samples_per_second': 300.099, 'eval_steps_per_second': 18.906, 'epoch': 0.96}
{'loss': 0.8584, 'grad_norm': 3.529258966445923, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2037627696990967, 'eval_runtime': 3.3285, 'eval_samples_per_second': 300.438, 'eval_steps_per_second': 18.928, 'epoch': 1.0}
{'train_runtime': 269.2236, 'train_samples_per_second': 37.133, 'train_steps_per_second': 2.321, 'train_loss': 1.0459817810058594, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0375819206237793, 1.2654597759246826, 1.2285258769989014, 1.1373910903930664, 1.1470012664794922, 1.182523488998413, 1.216732382774353, 1.1517215967178345, 1.2253581285476685, 1.1849210262298584, 1.1816014051437378, 1.1969178915023804, 1.2044445276260376, 1.202663779258728, 1.189389705657959, 1.2205970287322998, 1.2081937789916992, 1.1683759689331055, 1.182250738143921, 1.1702638864517212, 1.197325587272644, 1.1891570091247559, 1.1867566108703613, 1.2069836854934692, 1.2037627696990967], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0375819206237793, 1.2654597759246826, 1.2285258769989014, 1.1373910903930664, 1.1470012664794922, 1.182523488998413, 1.216732382774353, 1.1517215967178345, 1.2253581285476685, 1.1849210262298584, 1.1816014051437378, 1.1969178915023804, 1.2044445276260376, 1.202663779258728, 1.189389705657959, 1.2205970287322998, 1.2081937789916992, 1.1683759689331055, 1.182250738143921, 1.1702638864517212, 1.197325587272644, 1.1891570091247559, 1.1867566108703613, 1.2069836854934692, 1.2037627696990967]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -1.2037627696990967
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.6435 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -1.0245110035450702
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -1.0246653594281454
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -1.0245110032166722
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -1.0245110050183606
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -1.0240304295738254
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0704, dtype=torch.float64), tensor(0.1490, dtype=torch.float64), tensor(0.0838, dtype=torch.float64), tensor(0.3575, dtype=torch.float64), tensor(0.1028, dtype=torch.float64), tensor(0.0828, dtype=torch.float64), 0, tensor(0.0417, dtype=torch.float64), tensor(0.1121, dtype=torch.float64), 15, 1, 1, 1, 1, 1, 39, 0.0, 25.333526542594925, 1]
normalized proposed parameters for next round by BO: [tensor(0.0704, dtype=torch.float64), tensor(0.1490, dtype=torch.float64), tensor(0.0838, dtype=torch.float64), tensor(0.3575, dtype=torch.float64), tensor(0.1028, dtype=torch.float64), tensor(0.0828, dtype=torch.float64), tensor(3.1364e-17, dtype=torch.float64), tensor(0.0417, dtype=torch.float64), tensor(0.1121, dtype=torch.float64), tensor(0.4606, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3075, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5278, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.07
  gsm8k: 0.149
  rowan_hellaswag: 0.084
  sciq: 0.357
  triviaqa: 0.103
  truthfulqa_gen: 0.083
  wikitext: 0
  mmlu: 0.042
  arc_challenge: 0.112

LoRA Parameters:
  lora_r: (39,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (25.333526542594925,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  39
lora dropout:  0.0
lora alpha:  25.333526542594925
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 40,135,680 || all params: 8,070,396,928 || trainable%: 0.4973
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1725, 'grad_norm': 1.7359719276428223, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7131413221359253, 'eval_runtime': 3.6046, 'eval_samples_per_second': 277.422, 'eval_steps_per_second': 17.478, 'epoch': 0.04}
{'loss': 1.47, 'grad_norm': 1.098677158355713, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0829604864120483, 'eval_runtime': 3.5972, 'eval_samples_per_second': 277.993, 'eval_steps_per_second': 17.514, 'epoch': 0.08}
{'loss': 1.1853, 'grad_norm': 0.46380943059921265, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0298283100128174, 'eval_runtime': 3.6021, 'eval_samples_per_second': 277.618, 'eval_steps_per_second': 17.49, 'epoch': 0.12}
{'loss': 1.1392, 'grad_norm': 0.5747172236442566, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9180911779403687, 'eval_runtime': 3.6085, 'eval_samples_per_second': 277.12, 'eval_steps_per_second': 17.459, 'epoch': 0.16}
{'loss': 1.0787, 'grad_norm': 0.34039753675460815, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8539113998413086, 'eval_runtime': 3.6258, 'eval_samples_per_second': 275.798, 'eval_steps_per_second': 17.375, 'epoch': 0.2}
{'loss': 1.0288, 'grad_norm': 0.330169677734375, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8382697701454163, 'eval_runtime': 3.6275, 'eval_samples_per_second': 275.672, 'eval_steps_per_second': 17.367, 'epoch': 0.24}
{'loss': 1.0393, 'grad_norm': 0.2986477315425873, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.856791079044342, 'eval_runtime': 3.6427, 'eval_samples_per_second': 274.521, 'eval_steps_per_second': 17.295, 'epoch': 0.28}
{'loss': 1.0742, 'grad_norm': 0.31541818380355835, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8258465528488159, 'eval_runtime': 3.6214, 'eval_samples_per_second': 276.135, 'eval_steps_per_second': 17.397, 'epoch': 0.32}
{'loss': 1.0481, 'grad_norm': 0.34234094619750977, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8262057900428772, 'eval_runtime': 3.6238, 'eval_samples_per_second': 275.953, 'eval_steps_per_second': 17.385, 'epoch': 0.36}
{'loss': 1.0175, 'grad_norm': 0.32988500595092773, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8112305402755737, 'eval_runtime': 3.6183, 'eval_samples_per_second': 276.375, 'eval_steps_per_second': 17.412, 'epoch': 0.4}
{'loss': 1.0688, 'grad_norm': 0.3817150890827179, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8109940886497498, 'eval_runtime': 3.6251, 'eval_samples_per_second': 275.855, 'eval_steps_per_second': 17.379, 'epoch': 0.44}
{'loss': 0.9868, 'grad_norm': 0.2851085066795349, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.78138267993927, 'eval_runtime': 3.6247, 'eval_samples_per_second': 275.882, 'eval_steps_per_second': 17.381, 'epoch': 0.48}
{'loss': 1.0322, 'grad_norm': 0.32592007517814636, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7789611220359802, 'eval_runtime': 3.6225, 'eval_samples_per_second': 276.053, 'eval_steps_per_second': 17.391, 'epoch': 0.52}
{'loss': 1.0263, 'grad_norm': 0.3074278235435486, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.778573215007782, 'eval_runtime': 3.6213, 'eval_samples_per_second': 276.147, 'eval_steps_per_second': 17.397, 'epoch': 0.56}
{'loss': 0.9993, 'grad_norm': 0.3166505694389343, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.770495593547821, 'eval_runtime': 3.6261, 'eval_samples_per_second': 275.782, 'eval_steps_per_second': 17.374, 'epoch': 0.6}
{'loss': 1.0124, 'grad_norm': 0.36455854773521423, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7529719471931458, 'eval_runtime': 3.6255, 'eval_samples_per_second': 275.823, 'eval_steps_per_second': 17.377, 'epoch': 0.64}
{'loss': 0.998, 'grad_norm': 0.2989348769187927, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.746412992477417, 'eval_runtime': 3.6179, 'eval_samples_per_second': 276.406, 'eval_steps_per_second': 17.414, 'epoch': 0.68}
{'loss': 1.0105, 'grad_norm': 0.3165794610977173, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7350975871086121, 'eval_runtime': 3.6181, 'eval_samples_per_second': 276.387, 'eval_steps_per_second': 17.412, 'epoch': 0.72}
{'loss': 1.0163, 'grad_norm': 0.3697338402271271, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7297319173812866, 'eval_runtime': 3.6204, 'eval_samples_per_second': 276.209, 'eval_steps_per_second': 17.401, 'epoch': 0.76}
{'loss': 1.0623, 'grad_norm': 0.3275863230228424, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7259464263916016, 'eval_runtime': 3.6207, 'eval_samples_per_second': 276.192, 'eval_steps_per_second': 17.4, 'epoch': 0.8}
{'loss': 1.0264, 'grad_norm': 0.31931641697883606, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7153395414352417, 'eval_runtime': 3.626, 'eval_samples_per_second': 275.783, 'eval_steps_per_second': 17.374, 'epoch': 0.84}
{'loss': 1.0314, 'grad_norm': 0.2828311026096344, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7182042598724365, 'eval_runtime': 3.6206, 'eval_samples_per_second': 276.201, 'eval_steps_per_second': 17.401, 'epoch': 0.88}
{'loss': 1.0357, 'grad_norm': 0.4067313075065613, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7117748260498047, 'eval_runtime': 3.6252, 'eval_samples_per_second': 275.847, 'eval_steps_per_second': 17.378, 'epoch': 0.92}
{'loss': 0.9831, 'grad_norm': 0.3117029666900635, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7107958793640137, 'eval_runtime': 3.6187, 'eval_samples_per_second': 276.339, 'eval_steps_per_second': 17.409, 'epoch': 0.96}
{'loss': 0.9827, 'grad_norm': 0.37074536085128784, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7116898894309998, 'eval_runtime': 3.6217, 'eval_samples_per_second': 276.114, 'eval_steps_per_second': 17.395, 'epoch': 1.0}
{'train_runtime': 310.2345, 'train_samples_per_second': 32.221, 'train_steps_per_second': 2.015, 'train_loss': 1.1410343536376952, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7131413221359253, 1.0829604864120483, 1.0298283100128174, 0.9180911779403687, 0.8539113998413086, 0.8382697701454163, 0.856791079044342, 0.8258465528488159, 0.8262057900428772, 0.8112305402755737, 0.8109940886497498, 0.78138267993927, 0.7789611220359802, 0.778573215007782, 0.770495593547821, 0.7529719471931458, 0.746412992477417, 0.7350975871086121, 0.7297319173812866, 0.7259464263916016, 0.7153395414352417, 0.7182042598724365, 0.7117748260498047, 0.7107958793640137, 0.7116898894309998], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7131413221359253, 1.0829604864120483, 1.0298283100128174, 0.9180911779403687, 0.8539113998413086, 0.8382697701454163, 0.856791079044342, 0.8258465528488159, 0.8262057900428772, 0.8112305402755737, 0.8109940886497498, 0.78138267993927, 0.7789611220359802, 0.778573215007782, 0.770495593547821, 0.7529719471931458, 0.746412992477417, 0.7350975871086121, 0.7297319173812866, 0.7259464263916016, 0.7153395414352417, 0.7182042598724365, 0.7117748260498047, 0.7107958793640137, 0.7116898894309998]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.7116898894309998
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.0898 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -1.0104956983469715
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -1.0099171082176255
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -1.0104956983933255
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -1.0101784777255733
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -1.0104956983471212
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.9204, dtype=torch.float64), tensor(0.0574, dtype=torch.float64), 0, 0, 0, tensor(0.0222, dtype=torch.float64), 0, 0, 16, 0, 1, 1, 0, 1, 48, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.1806e-20, dtype=torch.float64), tensor(0.9204, dtype=torch.float64), tensor(0.0574, dtype=torch.float64), tensor(5.5500e-18, dtype=torch.float64), tensor(1.2201e-20, dtype=torch.float64), tensor(5.9433e-21, dtype=torch.float64), tensor(0.0222, dtype=torch.float64), tensor(1.4167e-17, dtype=torch.float64), tensor(8.1351e-18, dtype=torch.float64), tensor(0.5139, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3766, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.92
  rowan_hellaswag: 0.057
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.022
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (48,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  48
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 32,243,712 || all params: 8,062,504,960 || trainable%: 0.3999
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.0309, 'grad_norm': 0.48948031663894653, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.5390543937683105, 'eval_runtime': 3.2875, 'eval_samples_per_second': 304.184, 'eval_steps_per_second': 19.164, 'epoch': 0.04}
{'loss': 1.1848, 'grad_norm': 0.39899152517318726, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.287865161895752, 'eval_runtime': 3.2859, 'eval_samples_per_second': 304.329, 'eval_steps_per_second': 19.173, 'epoch': 0.08}
{'loss': 1.0397, 'grad_norm': 0.331020325422287, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0666630268096924, 'eval_runtime': 3.2844, 'eval_samples_per_second': 304.469, 'eval_steps_per_second': 19.182, 'epoch': 0.12}
{'loss': 0.9795, 'grad_norm': 0.248253732919693, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0352399349212646, 'eval_runtime': 3.2865, 'eval_samples_per_second': 304.272, 'eval_steps_per_second': 19.169, 'epoch': 0.16}
{'loss': 0.9801, 'grad_norm': 0.261568546295166, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0614514350891113, 'eval_runtime': 3.2903, 'eval_samples_per_second': 303.927, 'eval_steps_per_second': 19.147, 'epoch': 0.2}
{'loss': 0.9909, 'grad_norm': 0.25024330615997314, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.990441918373108, 'eval_runtime': 3.3067, 'eval_samples_per_second': 302.414, 'eval_steps_per_second': 19.052, 'epoch': 0.24}
{'loss': 0.9664, 'grad_norm': 0.2738696336746216, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.000392436981201, 'eval_runtime': 3.3053, 'eval_samples_per_second': 302.548, 'eval_steps_per_second': 19.061, 'epoch': 0.28}
{'loss': 0.9252, 'grad_norm': 0.29840946197509766, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8990073204040527, 'eval_runtime': 3.3113, 'eval_samples_per_second': 301.995, 'eval_steps_per_second': 19.026, 'epoch': 0.32}
{'loss': 0.923, 'grad_norm': 0.28636273741722107, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.987967610359192, 'eval_runtime': 3.3025, 'eval_samples_per_second': 302.805, 'eval_steps_per_second': 19.077, 'epoch': 0.36}
{'loss': 0.8705, 'grad_norm': 0.28482991456985474, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9442607164382935, 'eval_runtime': 3.3072, 'eval_samples_per_second': 302.369, 'eval_steps_per_second': 19.049, 'epoch': 0.4}
{'loss': 0.9134, 'grad_norm': 0.25103262066841125, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9293068647384644, 'eval_runtime': 3.3032, 'eval_samples_per_second': 302.733, 'eval_steps_per_second': 19.072, 'epoch': 0.44}
{'loss': 0.8611, 'grad_norm': 0.2794366478919983, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9212852716445923, 'eval_runtime': 3.3044, 'eval_samples_per_second': 302.623, 'eval_steps_per_second': 19.065, 'epoch': 0.48}
{'loss': 0.8842, 'grad_norm': 0.3027561604976654, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9486757516860962, 'eval_runtime': 3.304, 'eval_samples_per_second': 302.666, 'eval_steps_per_second': 19.068, 'epoch': 0.52}
{'loss': 0.8377, 'grad_norm': 0.27182310819625854, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9787222146987915, 'eval_runtime': 3.3051, 'eval_samples_per_second': 302.562, 'eval_steps_per_second': 19.061, 'epoch': 0.56}
{'loss': 0.8876, 'grad_norm': 0.2822878658771515, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.928459644317627, 'eval_runtime': 3.3066, 'eval_samples_per_second': 302.422, 'eval_steps_per_second': 19.053, 'epoch': 0.6}
{'loss': 0.8476, 'grad_norm': 0.2956795394420624, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9518547058105469, 'eval_runtime': 3.3043, 'eval_samples_per_second': 302.637, 'eval_steps_per_second': 19.066, 'epoch': 0.64}
{'loss': 0.8501, 'grad_norm': 0.2852124571800232, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9488811492919922, 'eval_runtime': 3.3049, 'eval_samples_per_second': 302.583, 'eval_steps_per_second': 19.063, 'epoch': 0.68}
{'loss': 0.8514, 'grad_norm': 0.3168815076351166, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9829623699188232, 'eval_runtime': 3.3051, 'eval_samples_per_second': 302.563, 'eval_steps_per_second': 19.061, 'epoch': 0.72}
{'loss': 0.8306, 'grad_norm': 0.2964005768299103, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.037003755569458, 'eval_runtime': 3.3132, 'eval_samples_per_second': 301.823, 'eval_steps_per_second': 19.015, 'epoch': 0.76}
{'loss': 0.8296, 'grad_norm': 0.31922730803489685, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.084566831588745, 'eval_runtime': 3.3094, 'eval_samples_per_second': 302.167, 'eval_steps_per_second': 19.037, 'epoch': 0.8}
{'loss': 0.8355, 'grad_norm': 0.3415546715259552, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0669116973876953, 'eval_runtime': 3.3135, 'eval_samples_per_second': 301.793, 'eval_steps_per_second': 19.013, 'epoch': 0.84}
{'loss': 0.809, 'grad_norm': 0.32777512073516846, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0486977100372314, 'eval_runtime': 3.3175, 'eval_samples_per_second': 301.427, 'eval_steps_per_second': 18.99, 'epoch': 0.88}
{'loss': 0.8111, 'grad_norm': 0.3906526267528534, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.056276798248291, 'eval_runtime': 3.3173, 'eval_samples_per_second': 301.454, 'eval_steps_per_second': 18.992, 'epoch': 0.92}
{'loss': 0.8033, 'grad_norm': 0.3680844306945801, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0519185066223145, 'eval_runtime': 3.3121, 'eval_samples_per_second': 301.927, 'eval_steps_per_second': 19.021, 'epoch': 0.96}
{'loss': 0.7972, 'grad_norm': 0.34493789076805115, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.0552961826324463, 'eval_runtime': 3.339, 'eval_samples_per_second': 299.49, 'eval_steps_per_second': 18.868, 'epoch': 1.0}
{'train_runtime': 293.3648, 'train_samples_per_second': 34.084, 'train_steps_per_second': 2.13, 'train_loss': 0.9416145324707031, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5390543937683105, 2.287865161895752, 2.0666630268096924, 2.0352399349212646, 2.0614514350891113, 1.990441918373108, 2.000392436981201, 1.8990073204040527, 1.987967610359192, 1.9442607164382935, 1.9293068647384644, 1.9212852716445923, 1.9486757516860962, 1.9787222146987915, 1.928459644317627, 1.9518547058105469, 1.9488811492919922, 1.9829623699188232, 2.037003755569458, 2.084566831588745, 2.0669116973876953, 2.0486977100372314, 2.056276798248291, 2.0519185066223145, 2.0552961826324463], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.5390543937683105, 2.287865161895752, 2.0666630268096924, 2.0352399349212646, 2.0614514350891113, 1.990441918373108, 2.000392436981201, 1.8990073204040527, 1.987967610359192, 1.9442607164382935, 1.9293068647384644, 1.9212852716445923, 1.9486757516860962, 1.9787222146987915, 1.928459644317627, 1.9518547058105469, 1.9488811492919922, 1.9829623699188232, 2.037003755569458, 2.084566831588745, 2.0669116973876953, 2.0486977100372314, 2.056276798248291, 2.0519185066223145, 2.0552961826324463]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -2.0552961826324463
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.3444 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -1.037904607627266
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -1.037904572198804
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -1.037904728054608
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -1.037905831478609
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -1.0379045697550435
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1239, dtype=torch.float64), tensor(0.1217, dtype=torch.float64), tensor(0.2317, dtype=torch.float64), tensor(0.4848, dtype=torch.float64), tensor(0.0312, dtype=torch.float64), 0, 0, 15, 1, 1, 1, 0, 1, 46, 6.939684807731537e-22, 33.91714501823304, 1]
normalized proposed parameters for next round by BO: [tensor(0.0067, dtype=torch.float64), tensor(3.1265e-21, dtype=torch.float64), tensor(0.1239, dtype=torch.float64), tensor(0.1217, dtype=torch.float64), tensor(0.2317, dtype=torch.float64), tensor(0.4848, dtype=torch.float64), tensor(0.0312, dtype=torch.float64), tensor(1.3428e-20, dtype=torch.float64), tensor(4.7970e-18, dtype=torch.float64), tensor(0.4831, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3631, dtype=torch.float64), tensor(6.9397e-21, dtype=torch.float64), tensor(0.7066, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.124
  sciq: 0.122
  triviaqa: 0.232
  truthfulqa_gen: 0.485
  wikitext: 0.031
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (46,)
  lora_dropout: (6.939684807731537e-22,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (33.91714501823304,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  46
lora dropout:  6.939684807731537e-22
lora alpha:  33.91714501823304
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 34,621,440 || all params: 8,064,882,688 || trainable%: 0.4293
length of training data:  9931
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.7156, 'grad_norm': 1.1988834142684937, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7756205797195435, 'eval_runtime': 3.3787, 'eval_samples_per_second': 295.975, 'eval_steps_per_second': 18.646, 'epoch': 0.04}
{'loss': 1.8132, 'grad_norm': 0.5097869634628296, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1516531705856323, 'eval_runtime': 3.3775, 'eval_samples_per_second': 296.08, 'eval_steps_per_second': 18.653, 'epoch': 0.08}
{'loss': 1.474, 'grad_norm': 0.6037443280220032, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.9790587425231934, 'eval_runtime': 3.3669, 'eval_samples_per_second': 297.006, 'eval_steps_per_second': 18.711, 'epoch': 0.12}
{'loss': 1.3627, 'grad_norm': 0.43101412057876587, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.8626520037651062, 'eval_runtime': 3.3745, 'eval_samples_per_second': 296.34, 'eval_steps_per_second': 18.669, 'epoch': 0.16}
{'loss': 1.3247, 'grad_norm': 0.6442770957946777, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.8139736652374268, 'eval_runtime': 3.3806, 'eval_samples_per_second': 295.801, 'eval_steps_per_second': 18.635, 'epoch': 0.2}
{'loss': 1.223, 'grad_norm': 0.48526498675346375, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.7673095464706421, 'eval_runtime': 3.3813, 'eval_samples_per_second': 295.742, 'eval_steps_per_second': 18.632, 'epoch': 0.24}
{'loss': 1.1921, 'grad_norm': 0.41998809576034546, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.7018065452575684, 'eval_runtime': 3.3846, 'eval_samples_per_second': 295.458, 'eval_steps_per_second': 18.614, 'epoch': 0.28}
{'loss': 1.2127, 'grad_norm': 0.5107192397117615, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.6397210955619812, 'eval_runtime': 3.3931, 'eval_samples_per_second': 294.714, 'eval_steps_per_second': 18.567, 'epoch': 0.32}
{'loss': 1.1675, 'grad_norm': 0.40098902583122253, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.5552298426628113, 'eval_runtime': 3.3932, 'eval_samples_per_second': 294.706, 'eval_steps_per_second': 18.566, 'epoch': 0.36}
{'loss': 1.1186, 'grad_norm': 0.33390119671821594, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.5078177452087402, 'eval_runtime': 3.3963, 'eval_samples_per_second': 294.436, 'eval_steps_per_second': 18.549, 'epoch': 0.4}
{'loss': 1.1303, 'grad_norm': 0.4477141499519348, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.4762914478778839, 'eval_runtime': 3.3894, 'eval_samples_per_second': 295.034, 'eval_steps_per_second': 18.587, 'epoch': 0.44}
{'loss': 1.0878, 'grad_norm': 0.375657856464386, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.4561332166194916, 'eval_runtime': 3.3959, 'eval_samples_per_second': 294.471, 'eval_steps_per_second': 18.552, 'epoch': 0.48}
{'loss': 1.0229, 'grad_norm': 0.6056481599807739, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.421772837638855, 'eval_runtime': 3.3926, 'eval_samples_per_second': 294.759, 'eval_steps_per_second': 18.57, 'epoch': 0.52}
{'loss': 1.1517, 'grad_norm': 0.7563872337341309, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.40843915939331055, 'eval_runtime': 3.3935, 'eval_samples_per_second': 294.685, 'eval_steps_per_second': 18.565, 'epoch': 0.56}
{'loss': 0.9633, 'grad_norm': 0.5959874987602234, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.37913939356803894, 'eval_runtime': 3.3948, 'eval_samples_per_second': 294.565, 'eval_steps_per_second': 18.558, 'epoch': 0.6}
{'loss': 0.9978, 'grad_norm': 0.5310029983520508, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.3561338186264038, 'eval_runtime': 3.3914, 'eval_samples_per_second': 294.865, 'eval_steps_per_second': 18.576, 'epoch': 0.64}
{'loss': 0.9694, 'grad_norm': 0.533406138420105, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.3333529531955719, 'eval_runtime': 3.3935, 'eval_samples_per_second': 294.684, 'eval_steps_per_second': 18.565, 'epoch': 0.68}
{'loss': 1.0367, 'grad_norm': 0.5450934767723083, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.3214821219444275, 'eval_runtime': 3.4183, 'eval_samples_per_second': 292.544, 'eval_steps_per_second': 18.43, 'epoch': 0.72}
{'loss': 1.0543, 'grad_norm': 0.4289900064468384, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.3043963611125946, 'eval_runtime': 3.395, 'eval_samples_per_second': 294.547, 'eval_steps_per_second': 18.556, 'epoch': 0.76}
{'loss': 0.9566, 'grad_norm': 0.47541317343711853, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.28598955273628235, 'eval_runtime': 3.4039, 'eval_samples_per_second': 293.785, 'eval_steps_per_second': 18.508, 'epoch': 0.81}
{'loss': 1.0259, 'grad_norm': 0.7369003295898438, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.2700449824333191, 'eval_runtime': 3.4344, 'eval_samples_per_second': 291.173, 'eval_steps_per_second': 18.344, 'epoch': 0.85}
{'loss': 0.9133, 'grad_norm': 0.47169479727745056, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.2538231611251831, 'eval_runtime': 3.3901, 'eval_samples_per_second': 294.973, 'eval_steps_per_second': 18.583, 'epoch': 0.89}
{'loss': 0.9175, 'grad_norm': 0.4559206962585449, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.2450820952653885, 'eval_runtime': 3.3982, 'eval_samples_per_second': 294.275, 'eval_steps_per_second': 18.539, 'epoch': 0.93}
{'loss': 0.9559, 'grad_norm': 0.4805713891983032, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.23865436017513275, 'eval_runtime': 3.401, 'eval_samples_per_second': 294.03, 'eval_steps_per_second': 18.524, 'epoch': 0.97}
{'train_runtime': 268.8295, 'train_samples_per_second': 36.942, 'train_steps_per_second': 2.31, 'train_loss': 1.2324417224828748, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7756205797195435, 1.1516531705856323, 0.9790587425231934, 0.8626520037651062, 0.8139736652374268, 0.7673095464706421, 0.7018065452575684, 0.6397210955619812, 0.5552298426628113, 0.5078177452087402, 0.4762914478778839, 0.4561332166194916, 0.421772837638855, 0.40843915939331055, 0.37913939356803894, 0.3561338186264038, 0.3333529531955719, 0.3214821219444275, 0.3043963611125946, 0.28598955273628235, 0.2700449824333191, 0.2538231611251831, 0.2450820952653885, 0.23865436017513275], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.7756205797195435, 1.1516531705856323, 0.9790587425231934, 0.8626520037651062, 0.8139736652374268, 0.7673095464706421, 0.7018065452575684, 0.6397210955619812, 0.5552298426628113, 0.5078177452087402, 0.4762914478778839, 0.4561332166194916, 0.421772837638855, 0.40843915939331055, 0.37913939356803894, 0.3561338186264038, 0.3333529531955719, 0.3214821219444275, 0.3043963611125946, 0.28598955273628235, 0.2700449824333191, 0.2538231611251831, 0.2450820952653885, 0.23865436017513275]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1270310878753662
current iteration best possible eval_loss (full train run):  -0.23865436017513275
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.0892 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9994975924491882, 0.07707124948501587, 0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5594547390937805, 0.1537771224975586, 0.817682683467865, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.14512693881988525, 0.883480966091156, 0.9173967838287354, 0.8848122358322144]  ‚Üí  acq = -1.0390174906990401
X = [0.9514877796173096, 0.9132158160209656, 0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.41754159331321716, 0.28531861305236816, 0.5411667227745056, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.23015734553337097, 0.9975385665893555, 0.2174660712480545, 0.13808739185333252]  ‚Üí  acq = -1.0390091381359725
X = [0.39850908517837524, 0.8170029520988464, 0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5720815062522888, 0.12135124206542969, 0.050965309143066406, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8528783321380615, 0.15156877040863037, 0.1581156700849533, 0.8321003913879395]  ‚Üí  acq = -1.038999135607795
X = [0.28970110416412354, 0.6408820152282715, 0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7433760762214661, 0.9123662114143372, 0.8424274921417236, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5417225956916809, 0.7911179065704346, 0.9902287721633911, 0.020802080631256104]  ‚Üí  acq = -1.0390091352732245
X = [0.5538846254348755, 0.17085957527160645, 0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6919615864753723, 0.1586219072341919, 0.2204926609992981, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.45917418599128723, 0.6860421299934387, 0.8713036775588989, 0.5007997155189514]  ‚Üí  acq = -1.039009135272266
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0586, dtype=torch.float64), 0, 0, tensor(0.0977, dtype=torch.float64), 0, tensor(0.4465, dtype=torch.float64), tensor(0.0674, dtype=torch.float64), tensor(0.1074, dtype=torch.float64), tensor(0.2178, dtype=torch.float64), 11, 0, 1, 0, 1, 1, 121, 1.4785051720441246e-20, 20.85961675984914, 1]
normalized proposed parameters for next round by BO: [tensor(0.0586, dtype=torch.float64), tensor(1.3175e-17, dtype=torch.float64), tensor(1.8231e-17, dtype=torch.float64), tensor(0.0977, dtype=torch.float64), tensor(0.0046, dtype=torch.float64), tensor(0.4465, dtype=torch.float64), tensor(0.0674, dtype=torch.float64), tensor(0.1074, dtype=torch.float64), tensor(0.2178, dtype=torch.float64), tensor(0.3422, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9467, dtype=torch.float64), tensor(1.4785e-19, dtype=torch.float64), tensor(0.4346, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.059
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.098
  triviaqa: 0
  truthfulqa_gen: 0.447
  wikitext: 0.067
  mmlu: 0.107
  arc_challenge: 0.218

LoRA Parameters:
  lora_r: (121,)
  lora_dropout: (1.4785051720441246e-20,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (20.85961675984914,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  121
lora dropout:  1.4785051720441246e-20
lora alpha:  20.85961675984914
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 55,880,704 || all params: 8,086,141,952 || trainable%: 0.6911
length of training data:  9952
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5731, 'grad_norm': 1.712029218673706, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.161433458328247, 'eval_runtime': 3.3313, 'eval_samples_per_second': 300.179, 'eval_steps_per_second': 18.911, 'epoch': 0.04}
{'loss': 1.633, 'grad_norm': 1.0846318006515503, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1010476350784302, 'eval_runtime': 3.3515, 'eval_samples_per_second': 298.375, 'eval_steps_per_second': 18.798, 'epoch': 0.08}
{'loss': 1.2006, 'grad_norm': 0.28199586272239685, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 0.9263797402381897, 'eval_runtime': 3.3438, 'eval_samples_per_second': 299.062, 'eval_steps_per_second': 18.841, 'epoch': 0.12}
{'loss': 1.1371, 'grad_norm': 0.2521904408931732, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 0.8625893592834473, 'eval_runtime': 3.3419, 'eval_samples_per_second': 299.23, 'eval_steps_per_second': 18.852, 'epoch': 0.16}
{'loss': 1.0071, 'grad_norm': 0.4175567328929901, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 0.7939117550849915, 'eval_runtime': 3.342, 'eval_samples_per_second': 299.221, 'eval_steps_per_second': 18.851, 'epoch': 0.2}
{'loss': 1.1053, 'grad_norm': 0.3592875301837921, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 0.7458698749542236, 'eval_runtime': 3.3456, 'eval_samples_per_second': 298.899, 'eval_steps_per_second': 18.831, 'epoch': 0.24}
{'loss': 1.0006, 'grad_norm': 0.22220684587955475, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 0.7223190069198608, 'eval_runtime': 3.3446, 'eval_samples_per_second': 298.985, 'eval_steps_per_second': 18.836, 'epoch': 0.28}
{'loss': 0.9555, 'grad_norm': 0.27277857065200806, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 0.7005809545516968, 'eval_runtime': 3.3357, 'eval_samples_per_second': 299.788, 'eval_steps_per_second': 18.887, 'epoch': 0.32}
{'loss': 1.0138, 'grad_norm': 0.20780621469020844, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 0.6833561062812805, 'eval_runtime': 3.3387, 'eval_samples_per_second': 299.519, 'eval_steps_per_second': 18.87, 'epoch': 0.36}
{'loss': 0.9337, 'grad_norm': 0.2940222918987274, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 0.6625080108642578, 'eval_runtime': 3.3458, 'eval_samples_per_second': 298.884, 'eval_steps_per_second': 18.83, 'epoch': 0.4}
{'loss': 1.0241, 'grad_norm': 0.25212910771369934, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 0.648818850517273, 'eval_runtime': 3.3388, 'eval_samples_per_second': 299.511, 'eval_steps_per_second': 18.869, 'epoch': 0.44}
{'loss': 0.9737, 'grad_norm': 0.28023573756217957, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 0.6319892406463623, 'eval_runtime': 3.3404, 'eval_samples_per_second': 299.366, 'eval_steps_per_second': 18.86, 'epoch': 0.48}
{'loss': 0.9676, 'grad_norm': 0.2662525773048401, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 0.6146039962768555, 'eval_runtime': 3.3451, 'eval_samples_per_second': 298.948, 'eval_steps_per_second': 18.834, 'epoch': 0.52}
{'loss': 0.9428, 'grad_norm': 0.30239713191986084, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 0.5886927247047424, 'eval_runtime': 3.3581, 'eval_samples_per_second': 297.784, 'eval_steps_per_second': 18.76, 'epoch': 0.56}
{'loss': 0.9236, 'grad_norm': 0.3136973977088928, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 0.5757840871810913, 'eval_runtime': 3.3576, 'eval_samples_per_second': 297.833, 'eval_steps_per_second': 18.763, 'epoch': 0.6}
{'loss': 0.9046, 'grad_norm': 0.38602814078330994, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 0.5566356182098389, 'eval_runtime': 3.3543, 'eval_samples_per_second': 298.129, 'eval_steps_per_second': 18.782, 'epoch': 0.64}
{'loss': 0.8684, 'grad_norm': 0.35907477140426636, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 0.5433769822120667, 'eval_runtime': 3.3544, 'eval_samples_per_second': 298.119, 'eval_steps_per_second': 18.781, 'epoch': 0.68}
{'loss': 0.9763, 'grad_norm': 0.3007972836494446, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 0.5268831849098206, 'eval_runtime': 3.358, 'eval_samples_per_second': 297.796, 'eval_steps_per_second': 18.761, 'epoch': 0.72}
{'loss': 0.8451, 'grad_norm': 0.3390761613845825, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 0.5059796571731567, 'eval_runtime': 3.3554, 'eval_samples_per_second': 298.025, 'eval_steps_per_second': 18.776, 'epoch': 0.76}
{'loss': 0.8791, 'grad_norm': 0.3558943569660187, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.49777865409851074, 'eval_runtime': 3.3581, 'eval_samples_per_second': 297.787, 'eval_steps_per_second': 18.761, 'epoch': 0.8}
{'loss': 0.8322, 'grad_norm': 0.321096807718277, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 0.4779999256134033, 'eval_runtime': 3.3601, 'eval_samples_per_second': 297.609, 'eval_steps_per_second': 18.749, 'epoch': 0.84}
{'loss': 0.9259, 'grad_norm': 0.3558059334754944, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.46322670578956604, 'eval_runtime': 3.3677, 'eval_samples_per_second': 296.94, 'eval_steps_per_second': 18.707, 'epoch': 0.88}
{'loss': 0.8236, 'grad_norm': 0.38197219371795654, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.45366984605789185, 'eval_runtime': 3.3658, 'eval_samples_per_second': 297.104, 'eval_steps_per_second': 18.718, 'epoch': 0.92}
{'loss': 0.8728, 'grad_norm': 0.2524662911891937, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.4409463405609131, 'eval_runtime': 3.3621, 'eval_samples_per_second': 297.432, 'eval_steps_per_second': 18.738, 'epoch': 0.96}
{'train_runtime': 243.8284, 'train_samples_per_second': 40.816, 'train_steps_per_second': 2.551, 'train_loss': 1.0877758743678643, 'epoch': 1.0}
train_results:  {'eval_loss': [2.161433458328247, 1.1010476350784302, 0.9263797402381897, 0.8625893592834473, 0.7939117550849915, 0.7458698749542236, 0.7223190069198608, 0.7005809545516968, 0.6833561062812805, 0.6625080108642578, 0.648818850517273, 0.6319892406463623, 0.6146039962768555, 0.5886927247047424, 0.5757840871810913, 0.5566356182098389, 0.5433769822120667, 0.5268831849098206, 0.5059796571731567, 0.49777865409851074, 0.4779999256134033, 0.46322670578956604, 0.45366984605789185, 0.4409463405609131], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.161433458328247, 1.1010476350784302, 0.9263797402381897, 0.8625893592834473, 0.7939117550849915, 0.7458698749542236, 0.7223190069198608, 0.7005809545516968, 0.6833561062812805, 0.6625080108642578, 0.648818850517273, 0.6319892406463623, 0.6146039962768555, 0.5886927247047424, 0.5757840871810913, 0.5566356182098389, 0.5433769822120667, 0.5268831849098206, 0.5059796571731567, 0.49777865409851074, 0.4779999256134033, 0.46322670578956604, 0.45366984605789185, 0.4409463405609131]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1437886953353882
current iteration best possible eval_loss (full train run):  -0.4409463405609131
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9733 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.769386351108551, 0.48460155725479126, 0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.871427059173584, 0.9379879832267761, 0.9101236462593079, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.43461495637893677, 0.05005854368209839, 0.09836432337760925, 0.9400144815444946]  ‚Üí  acq = -1.0556993689269811
X = [0.9526360034942627, 0.8012327551841736, 0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5360714197158813, 0.12353461980819702, 0.3528776168823242, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5302047729492188, 0.31215590238571167, 0.6163817644119263, 0.9380749464035034]  ‚Üí  acq = -1.0574445080208366
X = [0.34730666875839233, 0.06246674060821533, 0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3383713960647583, 0.12947320938110352, 0.50956130027771, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.48614978790283203, 0.43715083599090576, 0.6119240522384644, 0.47161221504211426]  ‚Üí  acq = -1.057467709314171
X = [0.45309585332870483, 0.0678371787071228, 0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7435163855552673, 0.12291663885116577, 0.4224488139152527, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.23402123153209686, 0.37334394454956055, 0.5466699600219727, 0.06654441356658936]  ‚Üí  acq = -1.0574449510907598
X = [0.18636363744735718, 0.6823987364768982, 0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9869434833526611, 0.8804963827133179, 0.9389960169792175, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.33730313181877136, 0.484236478805542, 0.44418343901634216, 0.7705504298210144]  ‚Üí  acq = -1.0573658375098054
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0360, dtype=torch.float64), tensor(0.3171, dtype=torch.float64), tensor(0.0674, dtype=torch.float64), tensor(0.1767, dtype=torch.float64), 0, tensor(0.1918, dtype=torch.float64), tensor(0.0731, dtype=torch.float64), tensor(0.0444, dtype=torch.float64), tensor(0.0936, dtype=torch.float64), 16, 1, 1, 1, 0, 1, 40, 1.0962736143162835e-19, 30.33111201499967, 1]
normalized proposed parameters for next round by BO: [tensor(0.0360, dtype=torch.float64), tensor(0.3171, dtype=torch.float64), tensor(0.0674, dtype=torch.float64), tensor(0.1767, dtype=torch.float64), tensor(4.3413e-19, dtype=torch.float64), tensor(0.1918, dtype=torch.float64), tensor(0.0731, dtype=torch.float64), tensor(0.0444, dtype=torch.float64), tensor(0.0936, dtype=torch.float64), tensor(0.5115, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3143, dtype=torch.float64), tensor(1.0963e-18, dtype=torch.float64), tensor(0.6319, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.036
  gsm8k: 0.317
  rowan_hellaswag: 0.067
  sciq: 0.177
  triviaqa: 0
  truthfulqa_gen: 0.192
  wikitext: 0.073
  mmlu: 0.044
  arc_challenge: 0.094

LoRA Parameters:
  lora_r: (40,)
  lora_dropout: (1.0962736143162835e-19,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (30.33111201499967,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  40
lora dropout:  1.0962736143162835e-19
lora alpha:  30.33111201499967
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 32,112,640 || all params: 8,062,373,888 || trainable%: 0.3983
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8456, 'grad_norm': 0.4606389105319977, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.83457612991333, 'eval_runtime': 3.3568, 'eval_samples_per_second': 297.905, 'eval_steps_per_second': 18.768, 'epoch': 0.04}
{'loss': 1.5419, 'grad_norm': 0.4767366647720337, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2262033224105835, 'eval_runtime': 3.3416, 'eval_samples_per_second': 299.255, 'eval_steps_per_second': 18.853, 'epoch': 0.08}
{'loss': 1.3169, 'grad_norm': 0.3423927426338196, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0985610485076904, 'eval_runtime': 3.3456, 'eval_samples_per_second': 298.899, 'eval_steps_per_second': 18.831, 'epoch': 0.12}
{'loss': 1.1841, 'grad_norm': 0.27609285712242126, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9685070514678955, 'eval_runtime': 3.3587, 'eval_samples_per_second': 297.735, 'eval_steps_per_second': 18.757, 'epoch': 0.16}
{'loss': 1.1901, 'grad_norm': 0.34073206782341003, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9277114272117615, 'eval_runtime': 3.3614, 'eval_samples_per_second': 297.498, 'eval_steps_per_second': 18.742, 'epoch': 0.2}
{'loss': 1.094, 'grad_norm': 0.2786584496498108, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.913189172744751, 'eval_runtime': 3.3634, 'eval_samples_per_second': 297.315, 'eval_steps_per_second': 18.731, 'epoch': 0.24}
{'loss': 1.1045, 'grad_norm': 0.2941215932369232, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8957016468048096, 'eval_runtime': 3.3605, 'eval_samples_per_second': 297.571, 'eval_steps_per_second': 18.747, 'epoch': 0.28}
{'loss': 1.0919, 'grad_norm': 0.3334115743637085, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8662660717964172, 'eval_runtime': 3.3662, 'eval_samples_per_second': 297.075, 'eval_steps_per_second': 18.716, 'epoch': 0.32}
{'loss': 1.1104, 'grad_norm': 0.2894477844238281, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8108343482017517, 'eval_runtime': 3.3669, 'eval_samples_per_second': 297.006, 'eval_steps_per_second': 18.711, 'epoch': 0.36}
{'loss': 1.0228, 'grad_norm': 0.2978733777999878, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7499552965164185, 'eval_runtime': 3.3633, 'eval_samples_per_second': 297.328, 'eval_steps_per_second': 18.732, 'epoch': 0.4}
{'loss': 1.0748, 'grad_norm': 0.2692027986049652, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6967531442642212, 'eval_runtime': 3.365, 'eval_samples_per_second': 297.176, 'eval_steps_per_second': 18.722, 'epoch': 0.44}
{'loss': 1.0755, 'grad_norm': 0.2696302533149719, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6852772235870361, 'eval_runtime': 3.3666, 'eval_samples_per_second': 297.039, 'eval_steps_per_second': 18.713, 'epoch': 0.48}
{'loss': 1.0376, 'grad_norm': 0.37084662914276123, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6612979769706726, 'eval_runtime': 3.3684, 'eval_samples_per_second': 296.877, 'eval_steps_per_second': 18.703, 'epoch': 0.52}
{'loss': 1.0077, 'grad_norm': 0.3976380527019501, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6571422219276428, 'eval_runtime': 3.3691, 'eval_samples_per_second': 296.815, 'eval_steps_per_second': 18.699, 'epoch': 0.56}
{'loss': 1.0023, 'grad_norm': 0.2971135675907135, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6369789838790894, 'eval_runtime': 3.3673, 'eval_samples_per_second': 296.973, 'eval_steps_per_second': 18.709, 'epoch': 0.6}
{'loss': 1.0444, 'grad_norm': 0.3855253756046295, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6213327646255493, 'eval_runtime': 3.3695, 'eval_samples_per_second': 296.781, 'eval_steps_per_second': 18.697, 'epoch': 0.64}
{'loss': 1.0562, 'grad_norm': 0.2820594906806946, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6149088740348816, 'eval_runtime': 3.3637, 'eval_samples_per_second': 297.292, 'eval_steps_per_second': 18.729, 'epoch': 0.68}
{'loss': 0.9818, 'grad_norm': 0.28428253531455994, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6060969829559326, 'eval_runtime': 3.3932, 'eval_samples_per_second': 294.706, 'eval_steps_per_second': 18.566, 'epoch': 0.72}
{'loss': 1.0509, 'grad_norm': 0.45068302750587463, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5985831022262573, 'eval_runtime': 3.3947, 'eval_samples_per_second': 294.58, 'eval_steps_per_second': 18.559, 'epoch': 0.76}
{'loss': 0.9975, 'grad_norm': 0.3615439832210541, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5783330202102661, 'eval_runtime': 3.3707, 'eval_samples_per_second': 296.676, 'eval_steps_per_second': 18.691, 'epoch': 0.8}
{'loss': 1.0099, 'grad_norm': 0.3970883786678314, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.572820782661438, 'eval_runtime': 3.3672, 'eval_samples_per_second': 296.982, 'eval_steps_per_second': 18.71, 'epoch': 0.84}
{'loss': 0.9303, 'grad_norm': 0.378783255815506, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5630520582199097, 'eval_runtime': 3.3711, 'eval_samples_per_second': 296.643, 'eval_steps_per_second': 18.689, 'epoch': 0.88}
{'loss': 0.9239, 'grad_norm': 0.3257029056549072, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5658854842185974, 'eval_runtime': 3.3897, 'eval_samples_per_second': 295.01, 'eval_steps_per_second': 18.586, 'epoch': 0.92}
{'loss': 1.0085, 'grad_norm': 0.33064424991607666, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5572121739387512, 'eval_runtime': 3.3698, 'eval_samples_per_second': 296.752, 'eval_steps_per_second': 18.695, 'epoch': 0.96}
{'loss': 0.9667, 'grad_norm': 0.4800107181072235, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5512655973434448, 'eval_runtime': 3.3703, 'eval_samples_per_second': 296.706, 'eval_steps_per_second': 18.692, 'epoch': 1.0}
{'train_runtime': 299.0352, 'train_samples_per_second': 33.424, 'train_steps_per_second': 2.09, 'train_loss': 1.1468072998046874, 'epoch': 1.0}
train_results:  {'eval_loss': [1.83457612991333, 1.2262033224105835, 1.0985610485076904, 0.9685070514678955, 0.9277114272117615, 0.913189172744751, 0.8957016468048096, 0.8662660717964172, 0.8108343482017517, 0.7499552965164185, 0.6967531442642212, 0.6852772235870361, 0.6612979769706726, 0.6571422219276428, 0.6369789838790894, 0.6213327646255493, 0.6149088740348816, 0.6060969829559326, 0.5985831022262573, 0.5783330202102661, 0.572820782661438, 0.5630520582199097, 0.5658854842185974, 0.5572121739387512, 0.5512655973434448], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.83457612991333, 1.2262033224105835, 1.0985610485076904, 0.9685070514678955, 0.9277114272117615, 0.913189172744751, 0.8957016468048096, 0.8662660717964172, 0.8108343482017517, 0.7499552965164185, 0.6967531442642212, 0.6852772235870361, 0.6612979769706726, 0.6571422219276428, 0.6369789838790894, 0.6213327646255493, 0.6149088740348816, 0.6060969829559326, 0.5985831022262573, 0.5783330202102661, 0.572820782661438, 0.5630520582199097, 0.5658854842185974, 0.5572121739387512, 0.5512655973434448]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.5512655973434448
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.4275 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -1.0073327647720962
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -1.054383275871369
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -1.041275209317405
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -1.0543832757141556
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -1.0584374068097449
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0429, dtype=torch.float64), tensor(0.0136, dtype=torch.float64), tensor(0.0745, dtype=torch.float64), tensor(0.1027, dtype=torch.float64), 0, tensor(0.4423, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), tensor(0.1192, dtype=torch.float64), tensor(0.1649, dtype=torch.float64), 25, 0, 1, 0, 1, 1, 15, 0.005826087646127717, 26.676437939496665, 1]
normalized proposed parameters for next round by BO: [tensor(0.0429, dtype=torch.float64), tensor(0.0136, dtype=torch.float64), tensor(0.0745, dtype=torch.float64), tensor(0.1027, dtype=torch.float64), tensor(4.7883e-18, dtype=torch.float64), tensor(0.4423, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), tensor(0.1192, dtype=torch.float64), tensor(0.1649, dtype=torch.float64), tensor(0.7848, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1203, dtype=torch.float64), tensor(0.0583, dtype=torch.float64), tensor(0.5558, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.043
  gsm8k: 0.014
  rowan_hellaswag: 0.075
  sciq: 0.103
  triviaqa: 0
  truthfulqa_gen: 0.442
  wikitext: 0.04
  mmlu: 0.119
  arc_challenge: 0.165

LoRA Parameters:
  lora_r: (15,)
  lora_dropout: (0.005826087646127717,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (26.676437939496665,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  15
lora dropout:  0.005826087646127717
lora alpha:  26.676437939496665
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 15,744,000 || all params: 8,046,005,248 || trainable%: 0.1957
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.251, 'grad_norm': 2.794888734817505, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.554711937904358, 'eval_runtime': 3.5913, 'eval_samples_per_second': 278.454, 'eval_steps_per_second': 17.543, 'epoch': 0.04}
{'loss': 1.448, 'grad_norm': 0.7313879132270813, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9240517020225525, 'eval_runtime': 3.5948, 'eval_samples_per_second': 278.178, 'eval_steps_per_second': 17.525, 'epoch': 0.08}
{'loss': 1.2436, 'grad_norm': 0.8473045825958252, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8031504154205322, 'eval_runtime': 3.5903, 'eval_samples_per_second': 278.529, 'eval_steps_per_second': 17.547, 'epoch': 0.12}
{'loss': 1.1424, 'grad_norm': 0.6314034461975098, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6905758380889893, 'eval_runtime': 3.5915, 'eval_samples_per_second': 278.432, 'eval_steps_per_second': 17.541, 'epoch': 0.16}
{'loss': 1.1222, 'grad_norm': 0.7755779027938843, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6415578126907349, 'eval_runtime': 3.6233, 'eval_samples_per_second': 275.994, 'eval_steps_per_second': 17.388, 'epoch': 0.2}
{'loss': 1.061, 'grad_norm': 0.6279890537261963, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.585745632648468, 'eval_runtime': 3.619, 'eval_samples_per_second': 276.32, 'eval_steps_per_second': 17.408, 'epoch': 0.24}
{'loss': 1.0755, 'grad_norm': 0.6216492652893066, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5326986312866211, 'eval_runtime': 3.6345, 'eval_samples_per_second': 275.145, 'eval_steps_per_second': 17.334, 'epoch': 0.28}
{'loss': 1.0603, 'grad_norm': 1.2329524755477905, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.4793541729450226, 'eval_runtime': 3.6428, 'eval_samples_per_second': 274.515, 'eval_steps_per_second': 17.294, 'epoch': 0.32}
{'loss': 1.0193, 'grad_norm': 0.5674772262573242, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.45634955167770386, 'eval_runtime': 3.6226, 'eval_samples_per_second': 276.044, 'eval_steps_per_second': 17.391, 'epoch': 0.36}
{'loss': 0.9971, 'grad_norm': 0.8034203052520752, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.4188788831233978, 'eval_runtime': 3.6047, 'eval_samples_per_second': 277.418, 'eval_steps_per_second': 17.477, 'epoch': 0.4}
{'loss': 0.9182, 'grad_norm': 0.5828487873077393, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.3904860317707062, 'eval_runtime': 3.6064, 'eval_samples_per_second': 277.286, 'eval_steps_per_second': 17.469, 'epoch': 0.44}
{'loss': 1.015, 'grad_norm': 0.6151161789894104, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3668234646320343, 'eval_runtime': 3.6042, 'eval_samples_per_second': 277.455, 'eval_steps_per_second': 17.48, 'epoch': 0.48}
{'loss': 0.9263, 'grad_norm': 0.7990243434906006, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3351830840110779, 'eval_runtime': 3.6009, 'eval_samples_per_second': 277.707, 'eval_steps_per_second': 17.496, 'epoch': 0.52}
{'loss': 0.9594, 'grad_norm': 0.7508260011672974, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3147394359111786, 'eval_runtime': 3.6003, 'eval_samples_per_second': 277.751, 'eval_steps_per_second': 17.498, 'epoch': 0.56}
{'loss': 0.9565, 'grad_norm': 0.732719361782074, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.29379358887672424, 'eval_runtime': 3.6065, 'eval_samples_per_second': 277.276, 'eval_steps_per_second': 17.468, 'epoch': 0.6}
{'loss': 0.9606, 'grad_norm': 0.6991211771965027, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.27264851331710815, 'eval_runtime': 3.6043, 'eval_samples_per_second': 277.448, 'eval_steps_per_second': 17.479, 'epoch': 0.64}
{'loss': 0.9002, 'grad_norm': 0.7690393328666687, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.25568658113479614, 'eval_runtime': 3.6059, 'eval_samples_per_second': 277.327, 'eval_steps_per_second': 17.472, 'epoch': 0.68}
{'loss': 0.9473, 'grad_norm': 0.6857167482376099, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.24390347301959991, 'eval_runtime': 3.6009, 'eval_samples_per_second': 277.709, 'eval_steps_per_second': 17.496, 'epoch': 0.72}
{'loss': 0.894, 'grad_norm': 0.6901564002037048, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.23301221430301666, 'eval_runtime': 3.6038, 'eval_samples_per_second': 277.486, 'eval_steps_per_second': 17.482, 'epoch': 0.76}
{'loss': 0.9444, 'grad_norm': 0.4528582692146301, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.22023113071918488, 'eval_runtime': 3.5989, 'eval_samples_per_second': 277.863, 'eval_steps_per_second': 17.505, 'epoch': 0.8}
{'loss': 0.9082, 'grad_norm': 0.851348876953125, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2120361328125, 'eval_runtime': 3.5997, 'eval_samples_per_second': 277.802, 'eval_steps_per_second': 17.502, 'epoch': 0.84}
{'loss': 1.003, 'grad_norm': 0.7160696983337402, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2043340504169464, 'eval_runtime': 3.6023, 'eval_samples_per_second': 277.603, 'eval_steps_per_second': 17.489, 'epoch': 0.88}
{'loss': 0.8611, 'grad_norm': 0.5475873947143555, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.19864948093891144, 'eval_runtime': 3.6019, 'eval_samples_per_second': 277.632, 'eval_steps_per_second': 17.491, 'epoch': 0.92}
{'loss': 0.8528, 'grad_norm': 1.0754871368408203, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.19388189911842346, 'eval_runtime': 3.6141, 'eval_samples_per_second': 276.692, 'eval_steps_per_second': 17.432, 'epoch': 0.96}
{'loss': 0.8452, 'grad_norm': 0.7283768653869629, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.1922573447227478, 'eval_runtime': 3.5995, 'eval_samples_per_second': 277.818, 'eval_steps_per_second': 17.503, 'epoch': 1.0}
{'train_runtime': 305.9929, 'train_samples_per_second': 32.671, 'train_steps_per_second': 2.043, 'train_loss': 1.0925028869628906, 'epoch': 1.0}
train_results:  {'eval_loss': [1.554711937904358, 0.9240517020225525, 0.8031504154205322, 0.6905758380889893, 0.6415578126907349, 0.585745632648468, 0.5326986312866211, 0.4793541729450226, 0.45634955167770386, 0.4188788831233978, 0.3904860317707062, 0.3668234646320343, 0.3351830840110779, 0.3147394359111786, 0.29379358887672424, 0.27264851331710815, 0.25568658113479614, 0.24390347301959991, 0.23301221430301666, 0.22023113071918488, 0.2120361328125, 0.2043340504169464, 0.19864948093891144, 0.19388189911842346, 0.1922573447227478], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.554711937904358, 0.9240517020225525, 0.8031504154205322, 0.6905758380889893, 0.6415578126907349, 0.585745632648468, 0.5326986312866211, 0.4793541729450226, 0.45634955167770386, 0.4188788831233978, 0.3904860317707062, 0.3668234646320343, 0.3351830840110779, 0.3147394359111786, 0.29379358887672424, 0.27264851331710815, 0.25568658113479614, 0.24390347301959991, 0.23301221430301666, 0.22023113071918488, 0.2120361328125, 0.2043340504169464, 0.19864948093891144, 0.19388189911842346, 0.1922573447227478]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.1922573447227478
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.4262 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -1.0644821976255123
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -1.0658260318192823
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -1.065774180061772
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -1.0657729139649939
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -1.066219312638813
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0335, dtype=torch.float64), 0, tensor(0.0725, dtype=torch.float64), tensor(0.1190, dtype=torch.float64), 0, tensor(0.3558, dtype=torch.float64), 0, tensor(0.0549, dtype=torch.float64), tensor(0.3643, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 88, 0.02475173938289693, 27.818850229064807, 1]
normalized proposed parameters for next round by BO: [tensor(0.0335, dtype=torch.float64), tensor(1.2668e-18, dtype=torch.float64), tensor(0.0725, dtype=torch.float64), tensor(0.1190, dtype=torch.float64), tensor(6.0941e-19, dtype=torch.float64), tensor(0.3558, dtype=torch.float64), tensor(3.7027e-18, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), tensor(0.3643, dtype=torch.float64), tensor(0.4805, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6838, dtype=torch.float64), tensor(0.2475, dtype=torch.float64), tensor(0.5796, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.034
  gsm8k: 0
  rowan_hellaswag: 0.072
  sciq: 0.119
  triviaqa: 0
  truthfulqa_gen: 0.356
  wikitext: 0
  mmlu: 0.055
  arc_challenge: 0.364

LoRA Parameters:
  lora_r: (88,)
  lora_dropout: (0.02475173938289693,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (27.818850229064807,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  88
lora dropout:  0.02475173938289693
lora alpha:  27.818850229064807
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 55,418,880 || all params: 8,085,680,128 || trainable%: 0.6854
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3718, 'grad_norm': 1.9654539823532104, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6546761989593506, 'eval_runtime': 3.3146, 'eval_samples_per_second': 301.693, 'eval_steps_per_second': 19.007, 'epoch': 0.04}
{'loss': 1.4238, 'grad_norm': 0.5411715507507324, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0277155637741089, 'eval_runtime': 3.3017, 'eval_samples_per_second': 302.874, 'eval_steps_per_second': 19.081, 'epoch': 0.08}
{'loss': 1.2513, 'grad_norm': 0.37373673915863037, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.91554194688797, 'eval_runtime': 3.3041, 'eval_samples_per_second': 302.653, 'eval_steps_per_second': 19.067, 'epoch': 0.12}
{'loss': 1.0708, 'grad_norm': 0.35640665888786316, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8137623071670532, 'eval_runtime': 3.2991, 'eval_samples_per_second': 303.115, 'eval_steps_per_second': 19.096, 'epoch': 0.16}
{'loss': 1.009, 'grad_norm': 0.361230731010437, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7484917044639587, 'eval_runtime': 3.3212, 'eval_samples_per_second': 301.092, 'eval_steps_per_second': 18.969, 'epoch': 0.2}
{'loss': 1.0106, 'grad_norm': 0.3535749614238739, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7144174575805664, 'eval_runtime': 3.3313, 'eval_samples_per_second': 300.187, 'eval_steps_per_second': 18.912, 'epoch': 0.24}
{'loss': 0.995, 'grad_norm': 0.30547836422920227, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6858547329902649, 'eval_runtime': 3.3119, 'eval_samples_per_second': 301.937, 'eval_steps_per_second': 19.022, 'epoch': 0.28}
{'loss': 0.9962, 'grad_norm': 0.4848081171512604, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6712177991867065, 'eval_runtime': 3.3219, 'eval_samples_per_second': 301.03, 'eval_steps_per_second': 18.965, 'epoch': 0.32}
{'loss': 1.0073, 'grad_norm': 0.28086692094802856, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6457098722457886, 'eval_runtime': 3.3122, 'eval_samples_per_second': 301.916, 'eval_steps_per_second': 19.021, 'epoch': 0.36}
{'loss': 0.9316, 'grad_norm': 0.43200021982192993, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6206483840942383, 'eval_runtime': 3.3163, 'eval_samples_per_second': 301.542, 'eval_steps_per_second': 18.997, 'epoch': 0.4}
{'loss': 0.8822, 'grad_norm': 0.31679466366767883, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6053958535194397, 'eval_runtime': 3.3183, 'eval_samples_per_second': 301.363, 'eval_steps_per_second': 18.986, 'epoch': 0.44}
{'loss': 0.9645, 'grad_norm': 0.2848007082939148, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5814312696456909, 'eval_runtime': 3.3141, 'eval_samples_per_second': 301.744, 'eval_steps_per_second': 19.01, 'epoch': 0.48}
{'loss': 0.9436, 'grad_norm': 0.3036254644393921, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5636838674545288, 'eval_runtime': 3.3304, 'eval_samples_per_second': 300.265, 'eval_steps_per_second': 18.917, 'epoch': 0.52}
{'loss': 0.8859, 'grad_norm': 0.36840254068374634, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5523933172225952, 'eval_runtime': 3.3299, 'eval_samples_per_second': 300.31, 'eval_steps_per_second': 18.92, 'epoch': 0.56}
{'loss': 0.9338, 'grad_norm': 0.33672794699668884, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5186561346054077, 'eval_runtime': 3.3186, 'eval_samples_per_second': 301.329, 'eval_steps_per_second': 18.984, 'epoch': 0.6}
{'loss': 0.8956, 'grad_norm': 0.35622093081474304, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.49985677003860474, 'eval_runtime': 3.3255, 'eval_samples_per_second': 300.71, 'eval_steps_per_second': 18.945, 'epoch': 0.64}
{'loss': 0.9177, 'grad_norm': 0.3511817455291748, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4904269874095917, 'eval_runtime': 3.331, 'eval_samples_per_second': 300.213, 'eval_steps_per_second': 18.913, 'epoch': 0.68}
{'loss': 0.8582, 'grad_norm': 0.4442692995071411, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.4614432454109192, 'eval_runtime': 3.3191, 'eval_samples_per_second': 301.288, 'eval_steps_per_second': 18.981, 'epoch': 0.72}
{'loss': 0.9084, 'grad_norm': 0.34361836314201355, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4418676793575287, 'eval_runtime': 3.318, 'eval_samples_per_second': 301.386, 'eval_steps_per_second': 18.987, 'epoch': 0.76}
{'loss': 0.9074, 'grad_norm': 0.4732779860496521, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.42497584223747253, 'eval_runtime': 3.3315, 'eval_samples_per_second': 300.169, 'eval_steps_per_second': 18.911, 'epoch': 0.8}
{'loss': 0.8458, 'grad_norm': 0.47191956639289856, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4183252155780792, 'eval_runtime': 3.3298, 'eval_samples_per_second': 300.32, 'eval_steps_per_second': 18.92, 'epoch': 0.84}
{'loss': 0.8211, 'grad_norm': 0.4965904653072357, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.4065130650997162, 'eval_runtime': 3.3232, 'eval_samples_per_second': 300.913, 'eval_steps_per_second': 18.958, 'epoch': 0.88}
{'loss': 0.8357, 'grad_norm': 0.4113210141658783, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.39490488171577454, 'eval_runtime': 3.3222, 'eval_samples_per_second': 301.002, 'eval_steps_per_second': 18.963, 'epoch': 0.92}
{'loss': 0.8214, 'grad_norm': 0.4464765787124634, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.38449859619140625, 'eval_runtime': 3.3186, 'eval_samples_per_second': 301.334, 'eval_steps_per_second': 18.984, 'epoch': 0.96}
{'loss': 0.8167, 'grad_norm': 0.47215408086776733, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3817158043384552, 'eval_runtime': 3.3217, 'eval_samples_per_second': 301.054, 'eval_steps_per_second': 18.966, 'epoch': 1.0}
{'train_runtime': 270.0998, 'train_samples_per_second': 37.016, 'train_steps_per_second': 2.314, 'train_loss': 1.0522112091064453, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6546761989593506, 1.0277155637741089, 0.91554194688797, 0.8137623071670532, 0.7484917044639587, 0.7144174575805664, 0.6858547329902649, 0.6712177991867065, 0.6457098722457886, 0.6206483840942383, 0.6053958535194397, 0.5814312696456909, 0.5636838674545288, 0.5523933172225952, 0.5186561346054077, 0.49985677003860474, 0.4904269874095917, 0.4614432454109192, 0.4418676793575287, 0.42497584223747253, 0.4183252155780792, 0.4065130650997162, 0.39490488171577454, 0.38449859619140625, 0.3817158043384552], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6546761989593506, 1.0277155637741089, 0.91554194688797, 0.8137623071670532, 0.7484917044639587, 0.7144174575805664, 0.6858547329902649, 0.6712177991867065, 0.6457098722457886, 0.6206483840942383, 0.6053958535194397, 0.5814312696456909, 0.5636838674545288, 0.5523933172225952, 0.5186561346054077, 0.49985677003860474, 0.4904269874095917, 0.4614432454109192, 0.4418676793575287, 0.42497584223747253, 0.4183252155780792, 0.4065130650997162, 0.39490488171577454, 0.38449859619140625, 0.3817158043384552]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.127687931060791
current iteration best possible eval_loss (full train run):  -0.3817158043384552
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361, -1.127687931060791]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.1600 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -1.0666166012508038
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -1.0666166012508038
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -1.0666753772795943
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -1.0666166012508038
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -1.1378833319191208
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0837, dtype=torch.float64), tensor(0.2985, dtype=torch.float64), tensor(0.0594, dtype=torch.float64), tensor(0.1809, dtype=torch.float64), tensor(0.0991, dtype=torch.float64), tensor(0.1099, dtype=torch.float64), tensor(0.0314, dtype=torch.float64), tensor(0.0950, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 42, 8.673617379884034e-20, 27.587974575079556, 1]
normalized proposed parameters for next round by BO: [tensor(0.0837, dtype=torch.float64), tensor(0.2985, dtype=torch.float64), tensor(0.0594, dtype=torch.float64), tensor(0.1809, dtype=torch.float64), tensor(0.0991, dtype=torch.float64), tensor(0.1099, dtype=torch.float64), tensor(0.0314, dtype=torch.float64), tensor(0.0950, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(0.4663, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3283, dtype=torch.float64), tensor(8.6736e-19, dtype=torch.float64), tensor(0.5747, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.084
  gsm8k: 0.299
  rowan_hellaswag: 0.059
  sciq: 0.181
  triviaqa: 0.099
  truthfulqa_gen: 0.11
  wikitext: 0.031
  mmlu: 0.095
  arc_challenge: 0.042

LoRA Parameters:
  lora_r: (42,)
  lora_dropout: (8.673617379884034e-20,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (27.587974575079556,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  42
lora dropout:  8.673617379884034e-20
lora alpha:  27.587974575079556
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 26,449,920 || all params: 8,056,711,168 || trainable%: 0.3283
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8916, 'grad_norm': 1.5341265201568604, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8186557292938232, 'eval_runtime': 3.3299, 'eval_samples_per_second': 300.309, 'eval_steps_per_second': 18.919, 'epoch': 0.04}
{'loss': 1.4243, 'grad_norm': 0.8848626613616943, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0580461025238037, 'eval_runtime': 3.3221, 'eval_samples_per_second': 301.011, 'eval_steps_per_second': 18.964, 'epoch': 0.08}
{'loss': 1.2138, 'grad_norm': 0.5539250373840332, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9833473563194275, 'eval_runtime': 3.3166, 'eval_samples_per_second': 301.51, 'eval_steps_per_second': 18.995, 'epoch': 0.12}
{'loss': 1.1305, 'grad_norm': 0.4220196604728699, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9092323184013367, 'eval_runtime': 3.3203, 'eval_samples_per_second': 301.176, 'eval_steps_per_second': 18.974, 'epoch': 0.16}
{'loss': 1.1452, 'grad_norm': 0.39687442779541016, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.850070595741272, 'eval_runtime': 3.3609, 'eval_samples_per_second': 297.543, 'eval_steps_per_second': 18.745, 'epoch': 0.2}
{'loss': 1.0989, 'grad_norm': 0.5590335130691528, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8181085586547852, 'eval_runtime': 3.3344, 'eval_samples_per_second': 299.903, 'eval_steps_per_second': 18.894, 'epoch': 0.24}
{'loss': 1.0562, 'grad_norm': 0.33159977197647095, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8211745619773865, 'eval_runtime': 3.3365, 'eval_samples_per_second': 299.714, 'eval_steps_per_second': 18.882, 'epoch': 0.28}
{'loss': 1.041, 'grad_norm': 0.2953090965747833, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8039538860321045, 'eval_runtime': 3.3281, 'eval_samples_per_second': 300.467, 'eval_steps_per_second': 18.929, 'epoch': 0.32}
{'loss': 1.0749, 'grad_norm': 0.34836146235466003, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7897942662239075, 'eval_runtime': 3.3277, 'eval_samples_per_second': 300.511, 'eval_steps_per_second': 18.932, 'epoch': 0.36}
{'loss': 1.0487, 'grad_norm': 0.3250866234302521, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7840501070022583, 'eval_runtime': 3.3327, 'eval_samples_per_second': 300.057, 'eval_steps_per_second': 18.904, 'epoch': 0.4}
{'loss': 1.0426, 'grad_norm': 0.2921285331249237, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7645691633224487, 'eval_runtime': 3.3306, 'eval_samples_per_second': 300.244, 'eval_steps_per_second': 18.915, 'epoch': 0.44}
{'loss': 1.0708, 'grad_norm': 0.2823794186115265, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7637510895729065, 'eval_runtime': 3.3292, 'eval_samples_per_second': 300.372, 'eval_steps_per_second': 18.923, 'epoch': 0.48}
{'loss': 1.0251, 'grad_norm': 0.3252837061882019, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7537089586257935, 'eval_runtime': 3.3305, 'eval_samples_per_second': 300.254, 'eval_steps_per_second': 18.916, 'epoch': 0.52}
{'loss': 1.0689, 'grad_norm': 0.400988906621933, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7686045169830322, 'eval_runtime': 3.3336, 'eval_samples_per_second': 299.973, 'eval_steps_per_second': 18.898, 'epoch': 0.56}
{'loss': 1.0283, 'grad_norm': 0.4248473346233368, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7504671216011047, 'eval_runtime': 3.3375, 'eval_samples_per_second': 299.626, 'eval_steps_per_second': 18.876, 'epoch': 0.6}
{'loss': 1.0204, 'grad_norm': 0.3412815034389496, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7455710172653198, 'eval_runtime': 3.3359, 'eval_samples_per_second': 299.766, 'eval_steps_per_second': 18.885, 'epoch': 0.64}
{'loss': 1.0492, 'grad_norm': 0.31328877806663513, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7306458353996277, 'eval_runtime': 3.3346, 'eval_samples_per_second': 299.886, 'eval_steps_per_second': 18.893, 'epoch': 0.68}
{'loss': 0.9628, 'grad_norm': 0.3200135827064514, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7133492827415466, 'eval_runtime': 3.3382, 'eval_samples_per_second': 299.561, 'eval_steps_per_second': 18.872, 'epoch': 0.72}
{'loss': 1.0854, 'grad_norm': 0.42980992794036865, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7215815186500549, 'eval_runtime': 3.3377, 'eval_samples_per_second': 299.606, 'eval_steps_per_second': 18.875, 'epoch': 0.76}
{'loss': 0.9943, 'grad_norm': 0.3514214754104614, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7014222741127014, 'eval_runtime': 3.3366, 'eval_samples_per_second': 299.706, 'eval_steps_per_second': 18.881, 'epoch': 0.8}
{'loss': 0.9993, 'grad_norm': 0.28253501653671265, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7067722082138062, 'eval_runtime': 3.3366, 'eval_samples_per_second': 299.706, 'eval_steps_per_second': 18.881, 'epoch': 0.84}
{'loss': 0.9751, 'grad_norm': 0.2681182324886322, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7019224762916565, 'eval_runtime': 3.3353, 'eval_samples_per_second': 299.82, 'eval_steps_per_second': 18.889, 'epoch': 0.88}
{'loss': 1.006, 'grad_norm': 0.3368162512779236, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6971232891082764, 'eval_runtime': 3.3374, 'eval_samples_per_second': 299.632, 'eval_steps_per_second': 18.877, 'epoch': 0.92}
{'loss': 0.9794, 'grad_norm': 0.33504214882850647, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6983942985534668, 'eval_runtime': 3.3379, 'eval_samples_per_second': 299.589, 'eval_steps_per_second': 18.874, 'epoch': 0.96}
{'loss': 1.0343, 'grad_norm': 0.6428831815719604, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.699491560459137, 'eval_runtime': 3.3364, 'eval_samples_per_second': 299.727, 'eval_steps_per_second': 18.883, 'epoch': 1.0}
{'train_runtime': 296.0868, 'train_samples_per_second': 33.76, 'train_steps_per_second': 2.111, 'train_loss': 1.1386787200927735, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8186557292938232, 1.0580461025238037, 0.9833473563194275, 0.9092323184013367, 0.850070595741272, 0.8181085586547852, 0.8211745619773865, 0.8039538860321045, 0.7897942662239075, 0.7840501070022583, 0.7645691633224487, 0.7637510895729065, 0.7537089586257935, 0.7686045169830322, 0.7504671216011047, 0.7455710172653198, 0.7306458353996277, 0.7133492827415466, 0.7215815186500549, 0.7014222741127014, 0.7067722082138062, 0.7019224762916565, 0.6971232891082764, 0.6983942985534668, 0.699491560459137], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8186557292938232, 1.0580461025238037, 0.9833473563194275, 0.9092323184013367, 0.850070595741272, 0.8181085586547852, 0.8211745619773865, 0.8039538860321045, 0.7897942662239075, 0.7840501070022583, 0.7645691633224487, 0.7637510895729065, 0.7537089586257935, 0.7686045169830322, 0.7504671216011047, 0.7455710172653198, 0.7306458353996277, 0.7133492827415466, 0.7215815186500549, 0.7014222741127014, 0.7067722082138062, 0.7019224762916565, 0.6971232891082764, 0.6983942985534668, 0.699491560459137]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1276519298553467
current iteration best possible eval_loss (full train run):  -0.699491560459137
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361, -1.127687931060791, -1.1276519298553467]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.9841 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -1.067205404563053
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -1.0672054045630532
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -1.0672054045630532
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -1.0674193035706985
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -1.0673777240701492
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1321, dtype=torch.float64), tensor(0.2143, dtype=torch.float64), tensor(0.0115, dtype=torch.float64), tensor(0.2549, dtype=torch.float64), tensor(0.0529, dtype=torch.float64), 0, tensor(0.0569, dtype=torch.float64), tensor(0.0134, dtype=torch.float64), tensor(0.2639, dtype=torch.float64), 10, 1, 1, 1, 1, 1, 126, 0.020038803278428136, 24.953352824473036, 1]
normalized proposed parameters for next round by BO: [tensor(0.1321, dtype=torch.float64), tensor(0.2143, dtype=torch.float64), tensor(0.0115, dtype=torch.float64), tensor(0.2549, dtype=torch.float64), tensor(0.0529, dtype=torch.float64), tensor(5.1955e-19, dtype=torch.float64), tensor(0.0569, dtype=torch.float64), tensor(0.0134, dtype=torch.float64), tensor(0.2639, dtype=torch.float64), tensor(0.3250, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9875, dtype=torch.float64), tensor(0.2004, dtype=torch.float64), tensor(0.5199, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.132
  gsm8k: 0.214
  rowan_hellaswag: 0.012
  sciq: 0.255
  triviaqa: 0.053
  truthfulqa_gen: 0
  wikitext: 0.057
  mmlu: 0.013
  arc_challenge: 0.264

LoRA Parameters:
  lora_r: (126,)
  lora_dropout: (0.020038803278428136,)
  num_layers_to_apply: (10,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (24.953352824473036,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  10
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  126
lora dropout:  0.020038803278428136
lora alpha:  24.953352824473036
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 86,446,080 || all params: 8,116,707,328 || trainable%: 1.0650
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0255, 'grad_norm': 0.9465417861938477, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.071526288986206, 'eval_runtime': 3.3915, 'eval_samples_per_second': 294.855, 'eval_steps_per_second': 18.576, 'epoch': 0.04}
{'loss': 1.4519, 'grad_norm': 0.43159741163253784, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3381363153457642, 'eval_runtime': 3.3946, 'eval_samples_per_second': 294.583, 'eval_steps_per_second': 18.559, 'epoch': 0.08}
{'loss': 1.0867, 'grad_norm': 0.20485904812812805, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2590126991271973, 'eval_runtime': 3.3903, 'eval_samples_per_second': 294.963, 'eval_steps_per_second': 18.583, 'epoch': 0.12}
{'loss': 1.0341, 'grad_norm': 0.2828660309314728, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1743518114089966, 'eval_runtime': 3.3946, 'eval_samples_per_second': 294.586, 'eval_steps_per_second': 18.559, 'epoch': 0.16}
{'loss': 1.0445, 'grad_norm': 0.21621666848659515, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1602106094360352, 'eval_runtime': 3.4052, 'eval_samples_per_second': 293.672, 'eval_steps_per_second': 18.501, 'epoch': 0.2}
{'loss': 1.0033, 'grad_norm': 0.2613929808139801, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1780297756195068, 'eval_runtime': 3.4026, 'eval_samples_per_second': 293.89, 'eval_steps_per_second': 18.515, 'epoch': 0.24}
{'loss': 0.945, 'grad_norm': 0.17354923486709595, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1288161277770996, 'eval_runtime': 3.4111, 'eval_samples_per_second': 293.162, 'eval_steps_per_second': 18.469, 'epoch': 0.28}
{'loss': 0.9673, 'grad_norm': 0.1983877420425415, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.164240837097168, 'eval_runtime': 3.4145, 'eval_samples_per_second': 292.87, 'eval_steps_per_second': 18.451, 'epoch': 0.32}
{'loss': 0.9636, 'grad_norm': 0.19629403948783875, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1875346899032593, 'eval_runtime': 3.4182, 'eval_samples_per_second': 292.552, 'eval_steps_per_second': 18.431, 'epoch': 0.36}
{'loss': 0.9464, 'grad_norm': 0.25531211495399475, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1650080680847168, 'eval_runtime': 3.431, 'eval_samples_per_second': 291.458, 'eval_steps_per_second': 18.362, 'epoch': 0.4}
{'loss': 0.9454, 'grad_norm': 0.22206340730190277, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1709527969360352, 'eval_runtime': 3.427, 'eval_samples_per_second': 291.8, 'eval_steps_per_second': 18.383, 'epoch': 0.44}
{'loss': 0.9436, 'grad_norm': 0.22394999861717224, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1515583992004395, 'eval_runtime': 3.4168, 'eval_samples_per_second': 292.669, 'eval_steps_per_second': 18.438, 'epoch': 0.48}
{'loss': 0.9349, 'grad_norm': 0.23156699538230896, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.187180519104004, 'eval_runtime': 3.4162, 'eval_samples_per_second': 292.719, 'eval_steps_per_second': 18.441, 'epoch': 0.52}
{'loss': 0.9109, 'grad_norm': 0.18746167421340942, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1948344707489014, 'eval_runtime': 3.4118, 'eval_samples_per_second': 293.096, 'eval_steps_per_second': 18.465, 'epoch': 0.56}
{'loss': 0.8997, 'grad_norm': 0.16609974205493927, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1665898561477661, 'eval_runtime': 3.4169, 'eval_samples_per_second': 292.66, 'eval_steps_per_second': 18.438, 'epoch': 0.6}
{'loss': 1.009, 'grad_norm': 0.22229278087615967, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1868548393249512, 'eval_runtime': 3.4127, 'eval_samples_per_second': 293.022, 'eval_steps_per_second': 18.46, 'epoch': 0.64}
{'loss': 0.9122, 'grad_norm': 0.2539622485637665, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2015641927719116, 'eval_runtime': 3.4187, 'eval_samples_per_second': 292.508, 'eval_steps_per_second': 18.428, 'epoch': 0.68}
{'loss': 0.9916, 'grad_norm': 0.22926226258277893, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2479506731033325, 'eval_runtime': 3.4218, 'eval_samples_per_second': 292.241, 'eval_steps_per_second': 18.411, 'epoch': 0.72}
{'loss': 0.914, 'grad_norm': 0.17361143231391907, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2278220653533936, 'eval_runtime': 3.4189, 'eval_samples_per_second': 292.493, 'eval_steps_per_second': 18.427, 'epoch': 0.76}
{'loss': 0.9505, 'grad_norm': 0.2134595364332199, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2245179414749146, 'eval_runtime': 3.424, 'eval_samples_per_second': 292.058, 'eval_steps_per_second': 18.4, 'epoch': 0.8}
{'loss': 0.9381, 'grad_norm': 0.18511098623275757, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2035375833511353, 'eval_runtime': 3.4248, 'eval_samples_per_second': 291.992, 'eval_steps_per_second': 18.395, 'epoch': 0.84}
{'loss': 0.897, 'grad_norm': 0.2237369865179062, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.19782555103302, 'eval_runtime': 3.4273, 'eval_samples_per_second': 291.776, 'eval_steps_per_second': 18.382, 'epoch': 0.88}
{'loss': 0.8983, 'grad_norm': 0.19228249788284302, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1883752346038818, 'eval_runtime': 3.4337, 'eval_samples_per_second': 291.23, 'eval_steps_per_second': 18.348, 'epoch': 0.92}
{'loss': 0.9117, 'grad_norm': 0.23272903263568878, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2047463655471802, 'eval_runtime': 3.4241, 'eval_samples_per_second': 292.048, 'eval_steps_per_second': 18.399, 'epoch': 0.96}
{'loss': 0.9266, 'grad_norm': 0.22598391771316528, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2115036249160767, 'eval_runtime': 3.4234, 'eval_samples_per_second': 292.109, 'eval_steps_per_second': 18.403, 'epoch': 1.0}
{'train_runtime': 289.2876, 'train_samples_per_second': 34.557, 'train_steps_per_second': 2.16, 'train_loss': 1.0580760223388672, 'epoch': 1.0}
train_results:  {'eval_loss': [2.071526288986206, 1.3381363153457642, 1.2590126991271973, 1.1743518114089966, 1.1602106094360352, 1.1780297756195068, 1.1288161277770996, 1.164240837097168, 1.1875346899032593, 1.1650080680847168, 1.1709527969360352, 1.1515583992004395, 1.187180519104004, 1.1948344707489014, 1.1665898561477661, 1.1868548393249512, 1.2015641927719116, 1.2479506731033325, 1.2278220653533936, 1.2245179414749146, 1.2035375833511353, 1.19782555103302, 1.1883752346038818, 1.2047463655471802, 1.2115036249160767], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.071526288986206, 1.3381363153457642, 1.2590126991271973, 1.1743518114089966, 1.1602106094360352, 1.1780297756195068, 1.1288161277770996, 1.164240837097168, 1.1875346899032593, 1.1650080680847168, 1.1709527969360352, 1.1515583992004395, 1.187180519104004, 1.1948344707489014, 1.1665898561477661, 1.1868548393249512, 1.2015641927719116, 1.2479506731033325, 1.2278220653533936, 1.2245179414749146, 1.2035375833511353, 1.19782555103302, 1.1883752346038818, 1.2047463655471802, 1.2115036249160767]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1364693641662598
current iteration best possible eval_loss (full train run):  -1.2115036249160767
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361, -1.127687931060791, -1.1276519298553467, -1.1364693641662598]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.5265 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -1.0721944767090739
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -1.0721776308892008
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -1.072177628332099
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -1.0755785247177274
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -1.0723648578974703
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0930, dtype=torch.float64), 0, tensor(0.0688, dtype=torch.float64), tensor(0.1897, dtype=torch.float64), 0, tensor(0.4230, dtype=torch.float64), 0, tensor(0.1777, dtype=torch.float64), tensor(0.0478, dtype=torch.float64), 15, 0, 1, 0, 1, 1, 61, 0.015151749030145345, 28.885634596466573, 1]
normalized proposed parameters for next round by BO: [tensor(0.0930, dtype=torch.float64), tensor(3.2207e-18, dtype=torch.float64), tensor(0.0688, dtype=torch.float64), tensor(0.1897, dtype=torch.float64), tensor(7.1084e-19, dtype=torch.float64), tensor(0.4230, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1777, dtype=torch.float64), tensor(0.0478, dtype=torch.float64), tensor(0.4616, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4796, dtype=torch.float64), tensor(0.1515, dtype=torch.float64), tensor(0.6018, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.093
  gsm8k: 0
  rowan_hellaswag: 0.069
  sciq: 0.19
  triviaqa: 0
  truthfulqa_gen: 0.423
  wikitext: 0
  mmlu: 0.178
  arc_challenge: 0.048

LoRA Parameters:
  lora_r: (61,)
  lora_dropout: (0.015151749030145345,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (28.885634596466573,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  61
lora dropout:  0.015151749030145345
lora alpha:  28.885634596466573
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 38,415,360 || all params: 8,068,676,608 || trainable%: 0.4761
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4954, 'grad_norm': 2.309309482574463, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6192522048950195, 'eval_runtime': 3.3825, 'eval_samples_per_second': 295.642, 'eval_steps_per_second': 18.625, 'epoch': 0.04}
{'loss': 1.5882, 'grad_norm': 0.8818458914756775, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9757028222084045, 'eval_runtime': 3.3886, 'eval_samples_per_second': 295.104, 'eval_steps_per_second': 18.592, 'epoch': 0.08}
{'loss': 1.2729, 'grad_norm': 0.5168965458869934, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8708574175834656, 'eval_runtime': 3.3799, 'eval_samples_per_second': 295.869, 'eval_steps_per_second': 18.64, 'epoch': 0.12}
{'loss': 1.1636, 'grad_norm': 0.5169105529785156, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7760811448097229, 'eval_runtime': 3.3822, 'eval_samples_per_second': 295.665, 'eval_steps_per_second': 18.627, 'epoch': 0.16}
{'loss': 1.1742, 'grad_norm': 0.5211976766586304, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7114393711090088, 'eval_runtime': 3.3978, 'eval_samples_per_second': 294.312, 'eval_steps_per_second': 18.542, 'epoch': 0.2}
{'loss': 1.0952, 'grad_norm': 0.3922971487045288, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6885201334953308, 'eval_runtime': 3.4063, 'eval_samples_per_second': 293.574, 'eval_steps_per_second': 18.495, 'epoch': 0.24}
{'loss': 0.9959, 'grad_norm': 0.4020339250564575, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6578740477561951, 'eval_runtime': 3.418, 'eval_samples_per_second': 292.573, 'eval_steps_per_second': 18.432, 'epoch': 0.28}
{'loss': 1.0757, 'grad_norm': 0.3503194749355316, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.636758029460907, 'eval_runtime': 3.4226, 'eval_samples_per_second': 292.171, 'eval_steps_per_second': 18.407, 'epoch': 0.32}
{'loss': 1.0192, 'grad_norm': 0.3153865933418274, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6137113571166992, 'eval_runtime': 3.4062, 'eval_samples_per_second': 293.583, 'eval_steps_per_second': 18.496, 'epoch': 0.36}
{'loss': 1.0433, 'grad_norm': 0.3609653115272522, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5874791145324707, 'eval_runtime': 3.4184, 'eval_samples_per_second': 292.531, 'eval_steps_per_second': 18.429, 'epoch': 0.4}
{'loss': 1.0637, 'grad_norm': 0.35616472363471985, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5500942468643188, 'eval_runtime': 3.4122, 'eval_samples_per_second': 293.069, 'eval_steps_per_second': 18.463, 'epoch': 0.44}
{'loss': 1.0993, 'grad_norm': 0.30794450640678406, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5390512943267822, 'eval_runtime': 3.4107, 'eval_samples_per_second': 293.191, 'eval_steps_per_second': 18.471, 'epoch': 0.48}
{'loss': 1.0013, 'grad_norm': 0.3968457579612732, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5083407163619995, 'eval_runtime': 3.4102, 'eval_samples_per_second': 293.237, 'eval_steps_per_second': 18.474, 'epoch': 0.52}
{'loss': 1.0081, 'grad_norm': 0.37431272864341736, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.49131980538368225, 'eval_runtime': 3.4163, 'eval_samples_per_second': 292.711, 'eval_steps_per_second': 18.441, 'epoch': 0.56}
{'loss': 1.0142, 'grad_norm': 0.44926169514656067, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4691379964351654, 'eval_runtime': 3.4079, 'eval_samples_per_second': 293.434, 'eval_steps_per_second': 18.486, 'epoch': 0.6}
{'loss': 1.0098, 'grad_norm': 0.4410068392753601, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4494490325450897, 'eval_runtime': 3.4075, 'eval_samples_per_second': 293.47, 'eval_steps_per_second': 18.489, 'epoch': 0.64}
{'loss': 0.9388, 'grad_norm': 0.484433114528656, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4248267412185669, 'eval_runtime': 3.4133, 'eval_samples_per_second': 292.975, 'eval_steps_per_second': 18.457, 'epoch': 0.68}
{'loss': 0.9659, 'grad_norm': 0.44222205877304077, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.4097867012023926, 'eval_runtime': 3.4112, 'eval_samples_per_second': 293.148, 'eval_steps_per_second': 18.468, 'epoch': 0.72}
{'loss': 0.9816, 'grad_norm': 0.6098341345787048, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.39314624667167664, 'eval_runtime': 3.4183, 'eval_samples_per_second': 292.541, 'eval_steps_per_second': 18.43, 'epoch': 0.76}
{'loss': 0.9888, 'grad_norm': 0.44456470012664795, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.38232293725013733, 'eval_runtime': 3.4146, 'eval_samples_per_second': 292.861, 'eval_steps_per_second': 18.45, 'epoch': 0.8}
{'loss': 0.9535, 'grad_norm': 0.4361053705215454, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3551148772239685, 'eval_runtime': 3.4196, 'eval_samples_per_second': 292.43, 'eval_steps_per_second': 18.423, 'epoch': 0.84}
{'loss': 0.9903, 'grad_norm': 0.4010036289691925, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.33922773599624634, 'eval_runtime': 3.4265, 'eval_samples_per_second': 291.847, 'eval_steps_per_second': 18.386, 'epoch': 0.88}
{'loss': 0.9103, 'grad_norm': 0.31514033675193787, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.32428351044654846, 'eval_runtime': 3.4299, 'eval_samples_per_second': 291.552, 'eval_steps_per_second': 18.368, 'epoch': 0.92}
{'loss': 0.9848, 'grad_norm': 0.5857155323028564, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3172661364078522, 'eval_runtime': 3.4472, 'eval_samples_per_second': 290.088, 'eval_steps_per_second': 18.276, 'epoch': 0.96}
{'loss': 0.8964, 'grad_norm': 0.7719460725784302, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.310642272233963, 'eval_runtime': 3.4092, 'eval_samples_per_second': 293.323, 'eval_steps_per_second': 18.479, 'epoch': 1.0}
{'train_runtime': 281.3038, 'train_samples_per_second': 35.538, 'train_steps_per_second': 2.222, 'train_loss': 1.14921142578125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6192522048950195, 0.9757028222084045, 0.8708574175834656, 0.7760811448097229, 0.7114393711090088, 0.6885201334953308, 0.6578740477561951, 0.636758029460907, 0.6137113571166992, 0.5874791145324707, 0.5500942468643188, 0.5390512943267822, 0.5083407163619995, 0.49131980538368225, 0.4691379964351654, 0.4494490325450897, 0.4248267412185669, 0.4097867012023926, 0.39314624667167664, 0.38232293725013733, 0.3551148772239685, 0.33922773599624634, 0.32428351044654846, 0.3172661364078522, 0.310642272233963], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6192522048950195, 0.9757028222084045, 0.8708574175834656, 0.7760811448097229, 0.7114393711090088, 0.6885201334953308, 0.6578740477561951, 0.636758029460907, 0.6137113571166992, 0.5874791145324707, 0.5500942468643188, 0.5390512943267822, 0.5083407163619995, 0.49131980538368225, 0.4691379964351654, 0.4494490325450897, 0.4248267412185669, 0.4097867012023926, 0.39314624667167664, 0.38232293725013733, 0.3551148772239685, 0.33922773599624634, 0.32428351044654846, 0.3172661364078522, 0.310642272233963]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1265597343444824
current iteration best possible eval_loss (full train run):  -0.310642272233963
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361, -1.127687931060791, -1.1276519298553467, -1.1364693641662598, -1.1265597343444824]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.8696 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -1.0705830346228697
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -1.0708671600209207
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -1.0834469007916303
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -1.0705830340993707
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -1.0705830340977798
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0583, dtype=torch.float64), tensor(0.2171, dtype=torch.float64), tensor(0.0700, dtype=torch.float64), tensor(0.1520, dtype=torch.float64), tensor(0.0487, dtype=torch.float64), tensor(0.0139, dtype=torch.float64), tensor(0.0348, dtype=torch.float64), tensor(0.1115, dtype=torch.float64), tensor(0.2937, dtype=torch.float64), 18, 1, 1, 1, 1, 1, 126, 0.01424793424646767, 28.401902929776703, 1]
normalized proposed parameters for next round by BO: [tensor(0.0583, dtype=torch.float64), tensor(0.2171, dtype=torch.float64), tensor(0.0700, dtype=torch.float64), tensor(0.1520, dtype=torch.float64), tensor(0.0487, dtype=torch.float64), tensor(0.0139, dtype=torch.float64), tensor(0.0348, dtype=torch.float64), tensor(0.1115, dtype=torch.float64), tensor(0.2937, dtype=torch.float64), tensor(0.5478, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9853, dtype=torch.float64), tensor(0.1425, dtype=torch.float64), tensor(0.5917, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.058
  gsm8k: 0.217
  rowan_hellaswag: 0.07
  sciq: 0.152
  triviaqa: 0.049
  truthfulqa_gen: 0.014
  wikitext: 0.035
  mmlu: 0.112
  arc_challenge: 0.294

LoRA Parameters:
  lora_r: (126,)
  lora_dropout: (0.01424793424646767,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (28.401902929776703,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  126
lora dropout:  0.01424793424646767
lora alpha:  28.401902929776703
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 155,602,944 || all params: 8,185,864,192 || trainable%: 1.9009
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8074, 'grad_norm': 0.6560759544372559, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6626367568969727, 'eval_runtime': 3.7092, 'eval_samples_per_second': 269.603, 'eval_steps_per_second': 16.985, 'epoch': 0.04}
{'loss': 1.3838, 'grad_norm': 0.4351235032081604, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.238726019859314, 'eval_runtime': 3.7055, 'eval_samples_per_second': 269.869, 'eval_steps_per_second': 17.002, 'epoch': 0.08}
{'loss': 1.1266, 'grad_norm': 0.2717134356498718, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1639728546142578, 'eval_runtime': 3.7117, 'eval_samples_per_second': 269.416, 'eval_steps_per_second': 16.973, 'epoch': 0.12}
{'loss': 1.1587, 'grad_norm': 0.19994127750396729, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0544689893722534, 'eval_runtime': 3.7047, 'eval_samples_per_second': 269.931, 'eval_steps_per_second': 17.006, 'epoch': 0.16}
{'loss': 1.0946, 'grad_norm': 0.19504833221435547, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0358718633651733, 'eval_runtime': 3.7121, 'eval_samples_per_second': 269.387, 'eval_steps_per_second': 16.971, 'epoch': 0.2}
{'loss': 1.088, 'grad_norm': 0.292001336812973, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0346449613571167, 'eval_runtime': 3.7149, 'eval_samples_per_second': 269.19, 'eval_steps_per_second': 16.959, 'epoch': 0.24}
{'loss': 1.0885, 'grad_norm': 0.22440391778945923, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9804900884628296, 'eval_runtime': 3.7307, 'eval_samples_per_second': 268.048, 'eval_steps_per_second': 16.887, 'epoch': 0.28}
{'loss': 1.0778, 'grad_norm': 0.19929155707359314, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.963996946811676, 'eval_runtime': 3.7086, 'eval_samples_per_second': 269.647, 'eval_steps_per_second': 16.988, 'epoch': 0.32}
{'loss': 1.0234, 'grad_norm': 0.18746745586395264, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9604501128196716, 'eval_runtime': 3.7158, 'eval_samples_per_second': 269.123, 'eval_steps_per_second': 16.955, 'epoch': 0.36}
{'loss': 1.0468, 'grad_norm': 0.1886620670557022, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9744981527328491, 'eval_runtime': 3.7126, 'eval_samples_per_second': 269.352, 'eval_steps_per_second': 16.969, 'epoch': 0.4}
{'loss': 1.0427, 'grad_norm': 0.1834845393896103, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.965941309928894, 'eval_runtime': 3.7069, 'eval_samples_per_second': 269.764, 'eval_steps_per_second': 16.995, 'epoch': 0.44}
{'loss': 0.9973, 'grad_norm': 0.1947864592075348, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9188628196716309, 'eval_runtime': 3.7141, 'eval_samples_per_second': 269.242, 'eval_steps_per_second': 16.962, 'epoch': 0.48}
{'loss': 1.0086, 'grad_norm': 0.25544390082359314, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9334094524383545, 'eval_runtime': 3.7078, 'eval_samples_per_second': 269.703, 'eval_steps_per_second': 16.991, 'epoch': 0.52}
{'loss': 1.0277, 'grad_norm': 0.24629755318164825, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9378665685653687, 'eval_runtime': 3.7074, 'eval_samples_per_second': 269.73, 'eval_steps_per_second': 16.993, 'epoch': 0.56}
{'loss': 1.0242, 'grad_norm': 0.291839599609375, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9230697154998779, 'eval_runtime': 3.7324, 'eval_samples_per_second': 267.923, 'eval_steps_per_second': 16.879, 'epoch': 0.6}
{'loss': 0.972, 'grad_norm': 0.31262269616127014, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9449280500411987, 'eval_runtime': 3.732, 'eval_samples_per_second': 267.953, 'eval_steps_per_second': 16.881, 'epoch': 0.64}
{'loss': 1.0369, 'grad_norm': 0.1943669617176056, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9321170449256897, 'eval_runtime': 3.7374, 'eval_samples_per_second': 267.567, 'eval_steps_per_second': 16.857, 'epoch': 0.68}
{'loss': 0.9317, 'grad_norm': 0.21748553216457367, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9088571667671204, 'eval_runtime': 3.7142, 'eval_samples_per_second': 269.238, 'eval_steps_per_second': 16.962, 'epoch': 0.72}
{'loss': 0.9944, 'grad_norm': 0.31574490666389465, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8920855522155762, 'eval_runtime': 3.7264, 'eval_samples_per_second': 268.357, 'eval_steps_per_second': 16.906, 'epoch': 0.76}
{'loss': 0.9567, 'grad_norm': 0.30190351605415344, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9080197811126709, 'eval_runtime': 3.7362, 'eval_samples_per_second': 267.651, 'eval_steps_per_second': 16.862, 'epoch': 0.8}
{'loss': 0.9605, 'grad_norm': 0.24009795486927032, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9029589891433716, 'eval_runtime': 3.7208, 'eval_samples_per_second': 268.756, 'eval_steps_per_second': 16.932, 'epoch': 0.84}
{'loss': 1.0026, 'grad_norm': 0.25511717796325684, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9003231525421143, 'eval_runtime': 3.7164, 'eval_samples_per_second': 269.077, 'eval_steps_per_second': 16.952, 'epoch': 0.88}
{'loss': 0.9791, 'grad_norm': 0.2464524358510971, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.898804783821106, 'eval_runtime': 3.7073, 'eval_samples_per_second': 269.737, 'eval_steps_per_second': 16.993, 'epoch': 0.92}
{'loss': 0.992, 'grad_norm': 0.2584258019924164, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8977094888687134, 'eval_runtime': 3.7133, 'eval_samples_per_second': 269.3, 'eval_steps_per_second': 16.966, 'epoch': 0.96}
{'loss': 0.9702, 'grad_norm': 0.28080275654792786, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8998646140098572, 'eval_runtime': 3.7198, 'eval_samples_per_second': 268.831, 'eval_steps_per_second': 16.936, 'epoch': 1.0}
{'train_runtime': 328.7211, 'train_samples_per_second': 30.406, 'train_steps_per_second': 1.901, 'train_loss': 1.1116807403564453, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6626367568969727, 1.238726019859314, 1.1639728546142578, 1.0544689893722534, 1.0358718633651733, 1.0346449613571167, 0.9804900884628296, 0.963996946811676, 0.9604501128196716, 0.9744981527328491, 0.965941309928894, 0.9188628196716309, 0.9334094524383545, 0.9378665685653687, 0.9230697154998779, 0.9449280500411987, 0.9321170449256897, 0.9088571667671204, 0.8920855522155762, 0.9080197811126709, 0.9029589891433716, 0.9003231525421143, 0.898804783821106, 0.8977094888687134, 0.8998646140098572], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6626367568969727, 1.238726019859314, 1.1639728546142578, 1.0544689893722534, 1.0358718633651733, 1.0346449613571167, 0.9804900884628296, 0.963996946811676, 0.9604501128196716, 0.9744981527328491, 0.965941309928894, 0.9188628196716309, 0.9334094524383545, 0.9378665685653687, 0.9230697154998779, 0.9449280500411987, 0.9321170449256897, 0.9088571667671204, 0.8920855522155762, 0.9080197811126709, 0.9029589891433716, 0.9003231525421143, 0.898804783821106, 0.8977094888687134, 0.8998646140098572]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.8998646140098572
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361, -1.127687931060791, -1.1276519298553467, -1.1364693641662598, -1.1265597343444824, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.8511 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -1.0719598981532559
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -1.0719553813168905
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -1.0717501984943008
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -1.0717501985008588
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -1.0700835887817899
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0998, dtype=torch.float64), 0, tensor(0.1164, dtype=torch.float64), tensor(0.4783, dtype=torch.float64), tensor(0.0377, dtype=torch.float64), tensor(0.2050, dtype=torch.float64), tensor(0.0244, dtype=torch.float64), 0, tensor(0.0252, dtype=torch.float64), 18, 1, 1, 1, 0, 1, 30, 0.008263475530649676, 20.82225633237526, 1]
normalized proposed parameters for next round by BO: [tensor(0.0998, dtype=torch.float64), tensor(0.0044, dtype=torch.float64), tensor(0.1164, dtype=torch.float64), tensor(0.4783, dtype=torch.float64), tensor(0.0377, dtype=torch.float64), tensor(0.2050, dtype=torch.float64), tensor(0.0244, dtype=torch.float64), tensor(0.0087, dtype=torch.float64), tensor(0.0252, dtype=torch.float64), tensor(0.5631, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2344, dtype=torch.float64), tensor(0.0826, dtype=torch.float64), tensor(0.4338, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.1
  gsm8k: 0
  rowan_hellaswag: 0.116
  sciq: 0.478
  triviaqa: 0.038
  truthfulqa_gen: 0.205
  wikitext: 0.024
  mmlu: 0
  arc_challenge: 0.025

LoRA Parameters:
  lora_r: (30,)
  lora_dropout: (0.008263475530649676,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (20.82225633237526,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  30
lora dropout:  0.008263475530649676
lora alpha:  20.82225633237526
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 27,095,040 || all params: 8,057,356,288 || trainable%: 0.3363
length of training data:  9866
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8159, 'grad_norm': 0.6911422610282898, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.848743200302124, 'eval_runtime': 3.4294, 'eval_samples_per_second': 291.593, 'eval_steps_per_second': 18.37, 'epoch': 0.04}
{'loss': 1.7994, 'grad_norm': 0.4309696555137634, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2752597332000732, 'eval_runtime': 3.4348, 'eval_samples_per_second': 291.137, 'eval_steps_per_second': 18.342, 'epoch': 0.08}
{'loss': 1.5092, 'grad_norm': 0.5259755849838257, 'learning_rate': 0.00028730158730158725, 'epoch': 0.12}
{'eval_loss': 1.1389341354370117, 'eval_runtime': 3.4306, 'eval_samples_per_second': 291.491, 'eval_steps_per_second': 18.364, 'epoch': 0.12}
{'loss': 1.3808, 'grad_norm': 0.41044145822525024, 'learning_rate': 0.00027407407407407404, 'epoch': 0.16}
{'eval_loss': 0.9948157072067261, 'eval_runtime': 3.4397, 'eval_samples_per_second': 290.726, 'eval_steps_per_second': 18.316, 'epoch': 0.16}
{'loss': 1.1852, 'grad_norm': 0.34272584319114685, 'learning_rate': 0.00026084656084656083, 'epoch': 0.2}
{'eval_loss': 0.9269999861717224, 'eval_runtime': 3.453, 'eval_samples_per_second': 289.602, 'eval_steps_per_second': 18.245, 'epoch': 0.2}
{'loss': 1.2405, 'grad_norm': 0.42395830154418945, 'learning_rate': 0.00024761904761904757, 'epoch': 0.24}
{'eval_loss': 0.9275956749916077, 'eval_runtime': 3.4607, 'eval_samples_per_second': 288.955, 'eval_steps_per_second': 18.204, 'epoch': 0.24}
{'loss': 1.1877, 'grad_norm': 0.40599605441093445, 'learning_rate': 0.0002343915343915344, 'epoch': 0.28}
{'eval_loss': 0.8759823441505432, 'eval_runtime': 3.4625, 'eval_samples_per_second': 288.81, 'eval_steps_per_second': 18.195, 'epoch': 0.28}
{'loss': 1.2351, 'grad_norm': 0.35490599274635315, 'learning_rate': 0.00022116402116402112, 'epoch': 0.32}
{'eval_loss': 0.8382698893547058, 'eval_runtime': 3.4683, 'eval_samples_per_second': 288.329, 'eval_steps_per_second': 18.165, 'epoch': 0.32}
{'loss': 1.1419, 'grad_norm': 0.374979704618454, 'learning_rate': 0.00020793650793650791, 'epoch': 0.36}
{'eval_loss': 0.7880239486694336, 'eval_runtime': 3.4681, 'eval_samples_per_second': 288.343, 'eval_steps_per_second': 18.166, 'epoch': 0.36}
{'loss': 1.1096, 'grad_norm': 0.35656973719596863, 'learning_rate': 0.00019470899470899468, 'epoch': 0.41}
{'eval_loss': 0.7105380296707153, 'eval_runtime': 3.4705, 'eval_samples_per_second': 288.143, 'eval_steps_per_second': 18.153, 'epoch': 0.41}
{'loss': 1.1711, 'grad_norm': 0.3134819567203522, 'learning_rate': 0.00018148148148148147, 'epoch': 0.45}
{'eval_loss': 0.682183027267456, 'eval_runtime': 3.4488, 'eval_samples_per_second': 289.955, 'eval_steps_per_second': 18.267, 'epoch': 0.45}
{'loss': 1.0929, 'grad_norm': 0.3292525112628937, 'learning_rate': 0.00016825396825396823, 'epoch': 0.49}
{'eval_loss': 0.6736654043197632, 'eval_runtime': 3.4583, 'eval_samples_per_second': 289.157, 'eval_steps_per_second': 18.217, 'epoch': 0.49}
{'loss': 1.1217, 'grad_norm': 0.3058297336101532, 'learning_rate': 0.00015502645502645502, 'epoch': 0.53}
{'eval_loss': 0.6453297138214111, 'eval_runtime': 3.4479, 'eval_samples_per_second': 290.036, 'eval_steps_per_second': 18.272, 'epoch': 0.53}
{'loss': 1.066, 'grad_norm': 0.3426320254802704, 'learning_rate': 0.00014179894179894179, 'epoch': 0.57}
{'eval_loss': 0.6237307786941528, 'eval_runtime': 3.448, 'eval_samples_per_second': 290.024, 'eval_steps_per_second': 18.272, 'epoch': 0.57}
{'loss': 1.0403, 'grad_norm': 0.36676907539367676, 'learning_rate': 0.00012857142857142855, 'epoch': 0.61}
{'eval_loss': 0.6173633337020874, 'eval_runtime': 3.4485, 'eval_samples_per_second': 289.983, 'eval_steps_per_second': 18.269, 'epoch': 0.61}
{'loss': 1.081, 'grad_norm': 0.33750829100608826, 'learning_rate': 0.00011534391534391533, 'epoch': 0.65}
{'eval_loss': 0.5977442860603333, 'eval_runtime': 3.4443, 'eval_samples_per_second': 290.335, 'eval_steps_per_second': 18.291, 'epoch': 0.65}
{'loss': 1.0969, 'grad_norm': 0.44632285833358765, 'learning_rate': 0.0001021164021164021, 'epoch': 0.69}
{'eval_loss': 0.5811585187911987, 'eval_runtime': 3.4555, 'eval_samples_per_second': 289.396, 'eval_steps_per_second': 18.232, 'epoch': 0.69}
{'loss': 1.0315, 'grad_norm': 0.4114847481250763, 'learning_rate': 8.888888888888888e-05, 'epoch': 0.73}
{'eval_loss': 0.5599890947341919, 'eval_runtime': 3.4459, 'eval_samples_per_second': 290.199, 'eval_steps_per_second': 18.283, 'epoch': 0.73}
{'loss': 1.0938, 'grad_norm': 0.3968054950237274, 'learning_rate': 7.566137566137566e-05, 'epoch': 0.77}
{'eval_loss': 0.5608617663383484, 'eval_runtime': 3.4409, 'eval_samples_per_second': 290.62, 'eval_steps_per_second': 18.309, 'epoch': 0.77}
{'loss': 1.1378, 'grad_norm': 0.48109450936317444, 'learning_rate': 6.243386243386242e-05, 'epoch': 0.81}
{'eval_loss': 0.5383588671684265, 'eval_runtime': 3.4495, 'eval_samples_per_second': 289.898, 'eval_steps_per_second': 18.264, 'epoch': 0.81}
{'loss': 1.1067, 'grad_norm': 0.36566975712776184, 'learning_rate': 4.92063492063492e-05, 'epoch': 0.85}
{'eval_loss': 0.5283299088478088, 'eval_runtime': 3.4636, 'eval_samples_per_second': 288.719, 'eval_steps_per_second': 18.189, 'epoch': 0.85}
{'loss': 1.0876, 'grad_norm': 0.4451082944869995, 'learning_rate': 3.5978835978835974e-05, 'epoch': 0.89}
{'eval_loss': 0.5219843983650208, 'eval_runtime': 3.4561, 'eval_samples_per_second': 289.34, 'eval_steps_per_second': 18.228, 'epoch': 0.89}
{'loss': 1.041, 'grad_norm': 0.40968483686447144, 'learning_rate': 2.2751322751322748e-05, 'epoch': 0.93}
{'eval_loss': 0.5126363635063171, 'eval_runtime': 3.4522, 'eval_samples_per_second': 289.67, 'eval_steps_per_second': 18.249, 'epoch': 0.93}
{'loss': 1.0932, 'grad_norm': 0.44152626395225525, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.97}
{'eval_loss': 0.5116100311279297, 'eval_runtime': 3.4483, 'eval_samples_per_second': 289.996, 'eval_steps_per_second': 18.27, 'epoch': 0.97}
{'train_runtime': 281.0468, 'train_samples_per_second': 35.104, 'train_steps_per_second': 2.195, 'train_loss': 1.2826510604042287, 'epoch': 1.0}
train_results:  {'eval_loss': [1.848743200302124, 1.2752597332000732, 1.1389341354370117, 0.9948157072067261, 0.9269999861717224, 0.9275956749916077, 0.8759823441505432, 0.8382698893547058, 0.7880239486694336, 0.7105380296707153, 0.682183027267456, 0.6736654043197632, 0.6453297138214111, 0.6237307786941528, 0.6173633337020874, 0.5977442860603333, 0.5811585187911987, 0.5599890947341919, 0.5608617663383484, 0.5383588671684265, 0.5283299088478088, 0.5219843983650208, 0.5126363635063171, 0.5116100311279297], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.848743200302124, 1.2752597332000732, 1.1389341354370117, 0.9948157072067261, 0.9269999861717224, 0.9275956749916077, 0.8759823441505432, 0.8382698893547058, 0.7880239486694336, 0.7105380296707153, 0.682183027267456, 0.6736654043197632, 0.6453297138214111, 0.6237307786941528, 0.6173633337020874, 0.5977442860603333, 0.5811585187911987, 0.5599890947341919, 0.5608617663383484, 0.5383588671684265, 0.5283299088478088, 0.5219843983650208, 0.5126363635063171, 0.5116100311279297]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.5116100311279297
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361, -1.127687931060791, -1.1276519298553467, -1.1364693641662598, -1.1265597343444824, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.8748 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.647038459777832, 0.3565073609352112, 0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.6441194415092468, 0.6647308468818665, 0.9432829022407532, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.47025445103645325, 0.5915921330451965, 0.563302755355835, 0.236403226852417]  ‚Üí  acq = -1.07344305576615
X = [0.7882768511772156, 0.0865660309791565, 0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.06684881448745728, 0.476356565952301, 0.49730604887008667, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.06370062381029129, 0.8247895836830139, 0.642969012260437, 0.2869639992713928]  ‚Üí  acq = -1.0734430388531602
X = [0.9537772536277771, 0.5254051685333252, 0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5333559513092041, 0.6058185696601868, 0.9840016961097717, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.07915887981653214, 0.6141131520271301, 0.8602699041366577, 0.6692355275154114]  ‚Üí  acq = -1.0734429510136456
X = [0.10701495409011841, 0.8236895203590393, 0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6551105976104736, 0.23645484447479248, 0.007582306861877441, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5136890411376953, 0.0011958479881286621, 0.378928005695343, 0.21825557947158813]  ‚Üí  acq = -1.0692942161060879
X = [0.37967562675476074, 0.21945631504058838, 0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4969014823436737, 0.5960436463356018, 0.011648118495941162, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.918420135974884, 0.3473445177078247, 0.7110291719436646, 0.30779868364334106]  ‚Üí  acq = -1.0734430366955152
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0676, dtype=torch.float64), tensor(0.1033, dtype=torch.float64), tensor(0.0884, dtype=torch.float64), tensor(0.1485, dtype=torch.float64), 0, tensor(0.4445, dtype=torch.float64), tensor(0.0238, dtype=torch.float64), tensor(0.0890, dtype=torch.float64), tensor(0.0349, dtype=torch.float64), 17, 0, 1, 0, 1, 1, 44, 0.013149368976085984, 29.48533481138056, 1]
normalized proposed parameters for next round by BO: [tensor(0.0676, dtype=torch.float64), tensor(0.1033, dtype=torch.float64), tensor(0.0884, dtype=torch.float64), tensor(0.1485, dtype=torch.float64), tensor(2.4489e-19, dtype=torch.float64), tensor(0.4445, dtype=torch.float64), tensor(0.0238, dtype=torch.float64), tensor(0.0890, dtype=torch.float64), tensor(0.0349, dtype=torch.float64), tensor(0.5424, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3441, dtype=torch.float64), tensor(0.1315, dtype=torch.float64), tensor(0.6143, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.068
  gsm8k: 0.103
  rowan_hellaswag: 0.088
  sciq: 0.148
  triviaqa: 0
  truthfulqa_gen: 0.444
  wikitext: 0.024
  mmlu: 0.089
  arc_challenge: 0.035

LoRA Parameters:
  lora_r: (44,)
  lora_dropout: (0.013149368976085984,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (29.48533481138056,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  44
lora dropout:  0.013149368976085984
lora alpha:  29.48533481138056
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 31,404,032 || all params: 8,061,665,280 || trainable%: 0.3895
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2548, 'grad_norm': 1.4258685111999512, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5426682233810425, 'eval_runtime': 3.3971, 'eval_samples_per_second': 294.365, 'eval_steps_per_second': 18.545, 'epoch': 0.04}
{'loss': 1.5712, 'grad_norm': 0.4913516044616699, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9755889773368835, 'eval_runtime': 3.3937, 'eval_samples_per_second': 294.665, 'eval_steps_per_second': 18.564, 'epoch': 0.08}
{'loss': 1.2692, 'grad_norm': 0.5262919068336487, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8468565940856934, 'eval_runtime': 3.3546, 'eval_samples_per_second': 298.096, 'eval_steps_per_second': 18.78, 'epoch': 0.12}
{'loss': 1.1079, 'grad_norm': 0.4993971586227417, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7382317781448364, 'eval_runtime': 3.367, 'eval_samples_per_second': 297.004, 'eval_steps_per_second': 18.711, 'epoch': 0.16}
{'loss': 1.0872, 'grad_norm': 0.4580386281013489, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6982558965682983, 'eval_runtime': 3.3694, 'eval_samples_per_second': 296.788, 'eval_steps_per_second': 18.698, 'epoch': 0.2}
{'loss': 1.143, 'grad_norm': 0.47410809993743896, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6552702784538269, 'eval_runtime': 3.3676, 'eval_samples_per_second': 296.95, 'eval_steps_per_second': 18.708, 'epoch': 0.24}
{'loss': 1.0986, 'grad_norm': 0.5334129333496094, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.632576048374176, 'eval_runtime': 3.3697, 'eval_samples_per_second': 296.762, 'eval_steps_per_second': 18.696, 'epoch': 0.28}
{'loss': 1.0712, 'grad_norm': 0.4584682583808899, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6000382304191589, 'eval_runtime': 3.3774, 'eval_samples_per_second': 296.085, 'eval_steps_per_second': 18.653, 'epoch': 0.32}
{'loss': 1.0566, 'grad_norm': 0.37240928411483765, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5662089586257935, 'eval_runtime': 3.374, 'eval_samples_per_second': 296.385, 'eval_steps_per_second': 18.672, 'epoch': 0.36}
{'loss': 1.0714, 'grad_norm': 0.5753783583641052, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5383731126785278, 'eval_runtime': 3.3795, 'eval_samples_per_second': 295.904, 'eval_steps_per_second': 18.642, 'epoch': 0.4}
{'loss': 1.0831, 'grad_norm': 0.47735968232154846, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5065947771072388, 'eval_runtime': 3.3789, 'eval_samples_per_second': 295.957, 'eval_steps_per_second': 18.645, 'epoch': 0.44}
{'loss': 1.0073, 'grad_norm': 0.4320317208766937, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.48236846923828125, 'eval_runtime': 3.3777, 'eval_samples_per_second': 296.064, 'eval_steps_per_second': 18.652, 'epoch': 0.48}
{'loss': 0.9915, 'grad_norm': 0.42225968837738037, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.44522321224212646, 'eval_runtime': 3.3788, 'eval_samples_per_second': 295.964, 'eval_steps_per_second': 18.646, 'epoch': 0.52}
{'loss': 0.9789, 'grad_norm': 0.436109334230423, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.42081910371780396, 'eval_runtime': 3.3767, 'eval_samples_per_second': 296.149, 'eval_steps_per_second': 18.657, 'epoch': 0.56}
{'loss': 0.9885, 'grad_norm': 0.48745816946029663, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.39995935559272766, 'eval_runtime': 3.3735, 'eval_samples_per_second': 296.426, 'eval_steps_per_second': 18.675, 'epoch': 0.6}
{'loss': 1.0499, 'grad_norm': 0.4190738797187805, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.37653112411499023, 'eval_runtime': 3.3812, 'eval_samples_per_second': 295.749, 'eval_steps_per_second': 18.632, 'epoch': 0.64}
{'loss': 0.9346, 'grad_norm': 0.4750591814517975, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3631610572338104, 'eval_runtime': 3.3823, 'eval_samples_per_second': 295.654, 'eval_steps_per_second': 18.626, 'epoch': 0.68}
{'loss': 0.9088, 'grad_norm': 0.41102728247642517, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.33754709362983704, 'eval_runtime': 3.3808, 'eval_samples_per_second': 295.786, 'eval_steps_per_second': 18.635, 'epoch': 0.72}
{'loss': 0.9635, 'grad_norm': 0.5406652092933655, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.32077041268348694, 'eval_runtime': 3.38, 'eval_samples_per_second': 295.859, 'eval_steps_per_second': 18.639, 'epoch': 0.76}
{'loss': 0.9215, 'grad_norm': 0.4863815903663635, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3073481321334839, 'eval_runtime': 3.3771, 'eval_samples_per_second': 296.116, 'eval_steps_per_second': 18.655, 'epoch': 0.8}
{'loss': 0.9223, 'grad_norm': 0.5060970783233643, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2867858409881592, 'eval_runtime': 3.3916, 'eval_samples_per_second': 294.85, 'eval_steps_per_second': 18.576, 'epoch': 0.84}
{'loss': 0.925, 'grad_norm': 0.4526115953922272, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2752039134502411, 'eval_runtime': 3.3913, 'eval_samples_per_second': 294.876, 'eval_steps_per_second': 18.577, 'epoch': 0.88}
{'loss': 0.913, 'grad_norm': 0.48561960458755493, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2614172697067261, 'eval_runtime': 3.3755, 'eval_samples_per_second': 296.256, 'eval_steps_per_second': 18.664, 'epoch': 0.92}
{'loss': 0.9397, 'grad_norm': 0.504675030708313, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2563028633594513, 'eval_runtime': 3.3796, 'eval_samples_per_second': 295.897, 'eval_steps_per_second': 18.642, 'epoch': 0.96}
{'loss': 0.9635, 'grad_norm': 0.5190557837486267, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.253040075302124, 'eval_runtime': 3.3751, 'eval_samples_per_second': 296.287, 'eval_steps_per_second': 18.666, 'epoch': 1.0}
{'train_runtime': 291.6917, 'train_samples_per_second': 34.266, 'train_steps_per_second': 2.143, 'train_loss': 1.128892935180664, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5426682233810425, 0.9755889773368835, 0.8468565940856934, 0.7382317781448364, 0.6982558965682983, 0.6552702784538269, 0.632576048374176, 0.6000382304191589, 0.5662089586257935, 0.5383731126785278, 0.5065947771072388, 0.48236846923828125, 0.44522321224212646, 0.42081910371780396, 0.39995935559272766, 0.37653112411499023, 0.3631610572338104, 0.33754709362983704, 0.32077041268348694, 0.3073481321334839, 0.2867858409881592, 0.2752039134502411, 0.2614172697067261, 0.2563028633594513, 0.253040075302124], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5426682233810425, 0.9755889773368835, 0.8468565940856934, 0.7382317781448364, 0.6982558965682983, 0.6552702784538269, 0.632576048374176, 0.6000382304191589, 0.5662089586257935, 0.5383731126785278, 0.5065947771072388, 0.48236846923828125, 0.44522321224212646, 0.42081910371780396, 0.39995935559272766, 0.37653112411499023, 0.3631610572338104, 0.33754709362983704, 0.32077041268348694, 0.3073481321334839, 0.2867858409881592, 0.2752039134502411, 0.2614172697067261, 0.2563028633594513, 0.253040075302124]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1270087957382202
current iteration best possible eval_loss (full train run):  -0.253040075302124
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361, -1.127687931060791, -1.1276519298553467, -1.1364693641662598, -1.1265597343444824, -1.1277821063995361, -1.1277821063995361, -1.1270087957382202]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.7806 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.0916202571528004
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.0916169350273086
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.0916169947432888
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.0916169353440295
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.0916159367831486
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2449, dtype=torch.float64), 0, 0, tensor(0.2926, dtype=torch.float64), tensor(0.0640, dtype=torch.float64), tensor(0.1421, dtype=torch.float64), tensor(0.0464, dtype=torch.float64), tensor(0.1960, dtype=torch.float64), 29, 1, 0, 1, 0, 1, 3, 0.1, 26.895620650015008, 1]
normalized proposed parameters for next round by BO: [tensor(0.0040, dtype=torch.float64), tensor(0.2449, dtype=torch.float64), tensor(0.0099, dtype=torch.float64), tensor(2.2368e-19, dtype=torch.float64), tensor(0.2926, dtype=torch.float64), tensor(0.0640, dtype=torch.float64), tensor(0.1421, dtype=torch.float64), tensor(0.0464, dtype=torch.float64), tensor(0.1960, dtype=torch.float64), tensor(0.9180, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0230, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5603, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.245
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.293
  truthfulqa_gen: 0.064
  wikitext: 0.142
  mmlu: 0.046
  arc_challenge: 0.196

LoRA Parameters:
  lora_r: (3,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (26.895620650015008,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  3
lora dropout:  0.1
lora alpha:  26.895620650015008
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,919,872 || all params: 8,034,181,120 || trainable%: 0.0488
length of training data:  9859
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.674, 'grad_norm': 1.2726261615753174, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9789798259735107, 'eval_runtime': 3.6541, 'eval_samples_per_second': 273.668, 'eval_steps_per_second': 17.241, 'epoch': 0.04}
{'loss': 1.4888, 'grad_norm': 0.9841310381889343, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.239869475364685, 'eval_runtime': 3.6263, 'eval_samples_per_second': 275.766, 'eval_steps_per_second': 17.373, 'epoch': 0.08}
{'loss': 1.292, 'grad_norm': 0.8214097023010254, 'learning_rate': 0.00028730158730158725, 'epoch': 0.12}
{'eval_loss': 1.0824668407440186, 'eval_runtime': 3.6499, 'eval_samples_per_second': 273.983, 'eval_steps_per_second': 17.261, 'epoch': 0.12}
{'loss': 1.1628, 'grad_norm': 0.9366148114204407, 'learning_rate': 0.00027407407407407404, 'epoch': 0.16}
{'eval_loss': 0.9823972582817078, 'eval_runtime': 3.6735, 'eval_samples_per_second': 272.224, 'eval_steps_per_second': 17.15, 'epoch': 0.16}
{'loss': 1.1526, 'grad_norm': 0.8018771409988403, 'learning_rate': 0.00026084656084656083, 'epoch': 0.2}
{'eval_loss': 0.9582257270812988, 'eval_runtime': 3.6778, 'eval_samples_per_second': 271.902, 'eval_steps_per_second': 17.13, 'epoch': 0.2}
{'loss': 1.0763, 'grad_norm': 0.8161506652832031, 'learning_rate': 0.00024761904761904757, 'epoch': 0.24}
{'eval_loss': 0.9458054304122925, 'eval_runtime': 3.6771, 'eval_samples_per_second': 271.953, 'eval_steps_per_second': 17.133, 'epoch': 0.24}
{'loss': 1.0661, 'grad_norm': 0.7698141932487488, 'learning_rate': 0.0002343915343915344, 'epoch': 0.28}
{'eval_loss': 0.901885986328125, 'eval_runtime': 3.6737, 'eval_samples_per_second': 272.206, 'eval_steps_per_second': 17.149, 'epoch': 0.28}
{'loss': 1.0395, 'grad_norm': 0.8903026580810547, 'learning_rate': 0.00022116402116402112, 'epoch': 0.32}
{'eval_loss': 0.8385186791419983, 'eval_runtime': 3.6823, 'eval_samples_per_second': 271.569, 'eval_steps_per_second': 17.109, 'epoch': 0.32}
{'loss': 1.003, 'grad_norm': 0.7452676296234131, 'learning_rate': 0.00020793650793650791, 'epoch': 0.36}
{'eval_loss': 0.7674580216407776, 'eval_runtime': 3.6853, 'eval_samples_per_second': 271.345, 'eval_steps_per_second': 17.095, 'epoch': 0.36}
{'loss': 1.0632, 'grad_norm': 1.0761786699295044, 'learning_rate': 0.00019470899470899468, 'epoch': 0.41}
{'eval_loss': 0.7567803263664246, 'eval_runtime': 3.6589, 'eval_samples_per_second': 273.308, 'eval_steps_per_second': 17.218, 'epoch': 0.41}
{'loss': 1.0477, 'grad_norm': 0.9468571543693542, 'learning_rate': 0.00018148148148148147, 'epoch': 0.45}
{'eval_loss': 0.7280081510543823, 'eval_runtime': 3.6514, 'eval_samples_per_second': 273.867, 'eval_steps_per_second': 17.254, 'epoch': 0.45}
{'loss': 1.0067, 'grad_norm': 0.8985490202903748, 'learning_rate': 0.00016825396825396823, 'epoch': 0.49}
{'eval_loss': 0.7235704064369202, 'eval_runtime': 3.6562, 'eval_samples_per_second': 273.504, 'eval_steps_per_second': 17.231, 'epoch': 0.49}
{'loss': 0.9977, 'grad_norm': 0.9531058669090271, 'learning_rate': 0.00015502645502645502, 'epoch': 0.53}
{'eval_loss': 0.731576681137085, 'eval_runtime': 3.6651, 'eval_samples_per_second': 272.843, 'eval_steps_per_second': 17.189, 'epoch': 0.53}
{'loss': 1.0476, 'grad_norm': 0.8986237645149231, 'learning_rate': 0.00014179894179894179, 'epoch': 0.57}
{'eval_loss': 0.7172276973724365, 'eval_runtime': 3.6576, 'eval_samples_per_second': 273.4, 'eval_steps_per_second': 17.224, 'epoch': 0.57}
{'loss': 0.9706, 'grad_norm': 0.8897405862808228, 'learning_rate': 0.00012857142857142855, 'epoch': 0.61}
{'eval_loss': 0.7035494446754456, 'eval_runtime': 3.6459, 'eval_samples_per_second': 274.28, 'eval_steps_per_second': 17.28, 'epoch': 0.61}
{'loss': 0.9601, 'grad_norm': 0.9668024778366089, 'learning_rate': 0.00011534391534391533, 'epoch': 0.65}
{'eval_loss': 0.7022160887718201, 'eval_runtime': 3.6473, 'eval_samples_per_second': 274.173, 'eval_steps_per_second': 17.273, 'epoch': 0.65}
{'loss': 0.9538, 'grad_norm': 0.9153101444244385, 'learning_rate': 0.0001021164021164021, 'epoch': 0.69}
{'eval_loss': 0.7014326453208923, 'eval_runtime': 3.6464, 'eval_samples_per_second': 274.247, 'eval_steps_per_second': 17.278, 'epoch': 0.69}
{'loss': 0.9524, 'grad_norm': 0.7607466578483582, 'learning_rate': 8.888888888888888e-05, 'epoch': 0.73}
{'eval_loss': 0.6900097131729126, 'eval_runtime': 3.6499, 'eval_samples_per_second': 273.982, 'eval_steps_per_second': 17.261, 'epoch': 0.73}
{'loss': 0.9705, 'grad_norm': 0.9031733870506287, 'learning_rate': 7.566137566137566e-05, 'epoch': 0.77}
{'eval_loss': 0.6737554669380188, 'eval_runtime': 3.6546, 'eval_samples_per_second': 273.63, 'eval_steps_per_second': 17.239, 'epoch': 0.77}
{'loss': 0.9697, 'grad_norm': 0.8606465458869934, 'learning_rate': 6.243386243386242e-05, 'epoch': 0.81}
{'eval_loss': 0.6834720373153687, 'eval_runtime': 3.6467, 'eval_samples_per_second': 274.221, 'eval_steps_per_second': 17.276, 'epoch': 0.81}
{'loss': 0.9694, 'grad_norm': 1.0872907638549805, 'learning_rate': 4.92063492063492e-05, 'epoch': 0.85}
{'eval_loss': 0.6725850105285645, 'eval_runtime': 3.6487, 'eval_samples_per_second': 274.07, 'eval_steps_per_second': 17.266, 'epoch': 0.85}
{'loss': 0.9973, 'grad_norm': 1.0837936401367188, 'learning_rate': 3.5978835978835974e-05, 'epoch': 0.89}
{'eval_loss': 0.6712011098861694, 'eval_runtime': 3.6452, 'eval_samples_per_second': 274.333, 'eval_steps_per_second': 17.283, 'epoch': 0.89}
{'loss': 0.9323, 'grad_norm': 0.8762140870094299, 'learning_rate': 2.2751322751322748e-05, 'epoch': 0.93}
{'eval_loss': 0.6642669439315796, 'eval_runtime': 3.6654, 'eval_samples_per_second': 272.819, 'eval_steps_per_second': 17.188, 'epoch': 0.93}
{'loss': 0.9802, 'grad_norm': 0.8818483948707581, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.97}
{'eval_loss': 0.660702645778656, 'eval_runtime': 3.6487, 'eval_samples_per_second': 274.069, 'eval_steps_per_second': 17.266, 'epoch': 0.97}
{'train_runtime': 312.36, 'train_samples_per_second': 31.563, 'train_steps_per_second': 1.975, 'train_loss': 1.1101184239070652, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9789798259735107, 1.239869475364685, 1.0824668407440186, 0.9823972582817078, 0.9582257270812988, 0.9458054304122925, 0.901885986328125, 0.8385186791419983, 0.7674580216407776, 0.7567803263664246, 0.7280081510543823, 0.7235704064369202, 0.731576681137085, 0.7172276973724365, 0.7035494446754456, 0.7022160887718201, 0.7014326453208923, 0.6900097131729126, 0.6737554669380188, 0.6834720373153687, 0.6725850105285645, 0.6712011098861694, 0.6642669439315796, 0.660702645778656], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9789798259735107, 1.239869475364685, 1.0824668407440186, 0.9823972582817078, 0.9582257270812988, 0.9458054304122925, 0.901885986328125, 0.8385186791419983, 0.7674580216407776, 0.7567803263664246, 0.7280081510543823, 0.7235704064369202, 0.731576681137085, 0.7172276973724365, 0.7035494446754456, 0.7022160887718201, 0.7014326453208923, 0.6900097131729126, 0.6737554669380188, 0.6834720373153687, 0.6725850105285645, 0.6712011098861694, 0.6642669439315796, 0.660702645778656]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -0.660702645778656
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361, -1.127687931060791, -1.1276519298553467, -1.1364693641662598, -1.1265597343444824, -1.1277821063995361, -1.1277821063995361, -1.1270087957382202, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2073 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.01858508586883545, 0.27087509632110596, 0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3516203463077545, 0.6742079257965088, 0.8425379991531372, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.3618133068084717, 0.971827507019043, 0.9077439308166504, 0.298198401927948]  ‚Üí  acq = -1.0942247270673267
X = [0.8427011370658875, 0.498738169670105, 0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.949406087398529, 0.2025597095489502, 0.1727500557899475, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6172329187393188, 0.7515861392021179, 0.5611653327941895, 0.20585447549819946]  ‚Üí  acq = -1.0941379769957165
X = [0.2902684807777405, 0.8506918549537659, 0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.44211840629577637, 0.9807340502738953, 0.05173856019973755, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5744302272796631, 0.3193025588989258, 0.46983620524406433, 0.4887549877166748]  ‚Üí  acq = -1.0941379769957165
X = [0.4835529327392578, 0.9963791966438293, 0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.5757967829704285, 0.605756938457489, 0.38045990467071533, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.12970638275146484, 0.7689643502235413, 0.6867401599884033, 0.6047636270523071]  ‚Üí  acq = -1.094155966870643
X = [0.500048816204071, 0.24933886528015137, 0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.07476793229579926, 0.3871777653694153, 0.952457845211029, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.24435395002365112, 0.10478633642196655, 0.9867004156112671, 0.0338512659072876]  ‚Üí  acq = -1.137171951289341
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2016, dtype=torch.float64), 0, tensor(0.1330, dtype=torch.float64), tensor(0.0519, dtype=torch.float64), 0, 0, tensor(0.1635, dtype=torch.float64), tensor(0.4499, dtype=torch.float64), 2, 0, 0, 1, 0, 1, 128, 0.00728556510107497, 38.42708686934615, 0]
normalized proposed parameters for next round by BO: [tensor(6.4729e-17, dtype=torch.float64), tensor(0.2016, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1330, dtype=torch.float64), tensor(0.0519, dtype=torch.float64), tensor(1.7332e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1635, dtype=torch.float64), tensor(0.4499, dtype=torch.float64), tensor(0.0711, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0729, dtype=torch.float64), tensor(0.8006, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.202
  rowan_hellaswag: 0
  sciq: 0.133
  triviaqa: 0.052
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.164
  arc_challenge: 0.45

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.00728556510107497,)
  num_layers_to_apply: (2,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (38.42708686934615,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  2
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.00728556510107497
lora alpha:  38.42708686934615
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 9,437,184 || all params: 8,039,698,432 || trainable%: 0.1174
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4274, 'grad_norm': 0.5600027441978455, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.209188938140869, 'eval_runtime': 3.0453, 'eval_samples_per_second': 328.372, 'eval_steps_per_second': 20.687, 'epoch': 0.04}
{'loss': 1.8933, 'grad_norm': 0.35616469383239746, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.8455158472061157, 'eval_runtime': 3.055, 'eval_samples_per_second': 327.332, 'eval_steps_per_second': 20.622, 'epoch': 0.08}
{'loss': 1.3947, 'grad_norm': 0.22631192207336426, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6005514860153198, 'eval_runtime': 3.0647, 'eval_samples_per_second': 326.297, 'eval_steps_per_second': 20.557, 'epoch': 0.12}
{'loss': 1.2967, 'grad_norm': 0.1999841332435608, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5026533603668213, 'eval_runtime': 3.0575, 'eval_samples_per_second': 327.064, 'eval_steps_per_second': 20.605, 'epoch': 0.16}
{'loss': 1.1933, 'grad_norm': 0.39051640033721924, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.353378176689148, 'eval_runtime': 3.0634, 'eval_samples_per_second': 326.435, 'eval_steps_per_second': 20.565, 'epoch': 0.2}
{'loss': 1.1806, 'grad_norm': 0.23795469105243683, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.22343111038208, 'eval_runtime': 3.0612, 'eval_samples_per_second': 326.674, 'eval_steps_per_second': 20.58, 'epoch': 0.24}
{'loss': 1.1094, 'grad_norm': 0.2068905234336853, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1788828372955322, 'eval_runtime': 3.0652, 'eval_samples_per_second': 326.248, 'eval_steps_per_second': 20.554, 'epoch': 0.28}
{'loss': 1.0462, 'grad_norm': 0.17936089634895325, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1790658235549927, 'eval_runtime': 3.0639, 'eval_samples_per_second': 326.384, 'eval_steps_per_second': 20.562, 'epoch': 0.32}
{'loss': 1.0848, 'grad_norm': 0.19359023869037628, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2142525911331177, 'eval_runtime': 3.0783, 'eval_samples_per_second': 324.854, 'eval_steps_per_second': 20.466, 'epoch': 0.36}
{'loss': 1.0572, 'grad_norm': 0.18902677297592163, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1637099981307983, 'eval_runtime': 3.0864, 'eval_samples_per_second': 324.003, 'eval_steps_per_second': 20.412, 'epoch': 0.4}
{'loss': 1.0298, 'grad_norm': 0.19418583810329437, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.151057481765747, 'eval_runtime': 3.0873, 'eval_samples_per_second': 323.903, 'eval_steps_per_second': 20.406, 'epoch': 0.44}
{'loss': 1.0041, 'grad_norm': 0.18192251026630402, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1420481204986572, 'eval_runtime': 3.0915, 'eval_samples_per_second': 323.472, 'eval_steps_per_second': 20.379, 'epoch': 0.48}
{'loss': 1.0077, 'grad_norm': 0.2243846356868744, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1434968709945679, 'eval_runtime': 3.0882, 'eval_samples_per_second': 323.811, 'eval_steps_per_second': 20.4, 'epoch': 0.52}
{'loss': 1.0353, 'grad_norm': 0.1952757090330124, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1525734663009644, 'eval_runtime': 3.0955, 'eval_samples_per_second': 323.052, 'eval_steps_per_second': 20.352, 'epoch': 0.56}
{'loss': 1.004, 'grad_norm': 0.1930498331785202, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1370576620101929, 'eval_runtime': 3.0735, 'eval_samples_per_second': 325.36, 'eval_steps_per_second': 20.498, 'epoch': 0.6}
{'loss': 1.0174, 'grad_norm': 0.19494542479515076, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1376371383666992, 'eval_runtime': 3.0804, 'eval_samples_per_second': 324.633, 'eval_steps_per_second': 20.452, 'epoch': 0.64}
{'loss': 1.0392, 'grad_norm': 0.20521323382854462, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1293776035308838, 'eval_runtime': 3.0808, 'eval_samples_per_second': 324.591, 'eval_steps_per_second': 20.449, 'epoch': 0.68}
{'loss': 0.9907, 'grad_norm': 0.19560012221336365, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.138588786125183, 'eval_runtime': 3.0784, 'eval_samples_per_second': 324.84, 'eval_steps_per_second': 20.465, 'epoch': 0.72}
{'loss': 0.9843, 'grad_norm': 0.18641215562820435, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1352734565734863, 'eval_runtime': 3.0837, 'eval_samples_per_second': 324.288, 'eval_steps_per_second': 20.43, 'epoch': 0.76}
{'loss': 1.0253, 'grad_norm': 0.2622343599796295, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1436877250671387, 'eval_runtime': 3.0904, 'eval_samples_per_second': 323.581, 'eval_steps_per_second': 20.386, 'epoch': 0.8}
{'loss': 1.0053, 'grad_norm': 0.22965370118618011, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1366180181503296, 'eval_runtime': 3.0822, 'eval_samples_per_second': 324.442, 'eval_steps_per_second': 20.44, 'epoch': 0.84}
{'loss': 1.0205, 'grad_norm': 0.2000846117734909, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.13932466506958, 'eval_runtime': 3.0759, 'eval_samples_per_second': 325.111, 'eval_steps_per_second': 20.482, 'epoch': 0.88}
{'loss': 0.9967, 'grad_norm': 0.20548591017723083, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.141279935836792, 'eval_runtime': 3.0769, 'eval_samples_per_second': 325.004, 'eval_steps_per_second': 20.475, 'epoch': 0.92}
{'loss': 1.0467, 'grad_norm': 0.2062712013721466, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1382641792297363, 'eval_runtime': 3.0785, 'eval_samples_per_second': 324.83, 'eval_steps_per_second': 20.464, 'epoch': 0.96}
{'loss': 1.0013, 'grad_norm': 0.25743451714515686, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1363232135772705, 'eval_runtime': 3.079, 'eval_samples_per_second': 324.783, 'eval_steps_per_second': 20.461, 'epoch': 1.0}
{'train_runtime': 171.6556, 'train_samples_per_second': 58.25, 'train_steps_per_second': 3.641, 'train_loss': 1.1956652221679687, 'epoch': 1.0}
train_results:  {'eval_loss': [3.209188938140869, 1.8455158472061157, 1.6005514860153198, 1.5026533603668213, 1.353378176689148, 1.22343111038208, 1.1788828372955322, 1.1790658235549927, 1.2142525911331177, 1.1637099981307983, 1.151057481765747, 1.1420481204986572, 1.1434968709945679, 1.1525734663009644, 1.1370576620101929, 1.1376371383666992, 1.1293776035308838, 1.138588786125183, 1.1352734565734863, 1.1436877250671387, 1.1366180181503296, 1.13932466506958, 1.141279935836792, 1.1382641792297363, 1.1363232135772705], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.209188938140869, 1.8455158472061157, 1.6005514860153198, 1.5026533603668213, 1.353378176689148, 1.22343111038208, 1.1788828372955322, 1.1790658235549927, 1.2142525911331177, 1.1637099981307983, 1.151057481765747, 1.1420481204986572, 1.1434968709945679, 1.1525734663009644, 1.1370576620101929, 1.1376371383666992, 1.1293776035308838, 1.138588786125183, 1.1352734565734863, 1.1436877250671387, 1.1366180181503296, 1.13932466506958, 1.141279935836792, 1.1382641792297363, 1.1363232135772705]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1277821063995361
current iteration best possible eval_loss (full train run):  -1.1363232135772705
max eval_loss so far:  -0.15899628400802612
BO observations:  [-1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1238232851028442, -1.1264610290527344, -1.1268006563186646, -1.1277821063995361, -1.1300971508026123, -1.12752366065979, -1.1246788501739502, -1.1277821063995361, -1.1241222620010376, -1.1211590766906738, -1.1222003698349, -1.1277821063995361, -1.1277821063995361, -1.1277821063995361, -1.1270310878753662, -1.1437886953353882, -1.1277821063995361, -1.1277821063995361, -1.127687931060791, -1.1276519298553467, -1.1364693641662598, -1.1265597343444824, -1.1277821063995361, -1.1277821063995361, -1.1270087957382202, -1.1277821063995361, -1.1277821063995361]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9889 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -1.0986609618132332
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -1.0986618056062165
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -1.0982572412544118
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -1.1962190241102533
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -1.098660999553124
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.0805, dtype=torch.float64), tensor(0.1605, dtype=torch.float64), tensor(0.7590, dtype=torch.float64), 2, 0, 0, 0, 0, 1, 128, 0.007492115602086321, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(3.8780e-17, dtype=torch.float64), tensor(9.0071e-18, dtype=torch.float64), tensor(1.7076e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0805, dtype=torch.float64), tensor(0.1605, dtype=torch.float64), tensor(0.7590, dtype=torch.float64), tensor(0.0575, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0749, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-1.192497968673706, -0.6183189153671265, -0.4600749611854553, -0.4600749611854553, -0.4600749611854553, -0.4600749611854553, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'truthfulqa_gen', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_truthfulqa_gen_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['truthfulqa_gen'], 'weight': [1.0], 'random': [[-1.2251425981521606, -0.6216726303100586, -0.22662609815597534, -0.22662609815597534, -0.22662609815597534, -0.22662609815597534, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203, -0.16279813647270203], [-1.1763049364089966, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.24569274485111237, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952, -0.16776354610919952], [-1.192497968673706, -0.6183189153671265, -0.4600749611854553, -0.4600749611854553, -0.4600749611854553, -0.4600749611854553, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612, -0.15899628400802612]], 'random_full_inputs': [[[0, 0.021561556136190662, 0, 0.20553649905007443, 0, 0, 0, 0.19561845043793985, 0.5772834943757948, 20, 1, 1, 1, 1, 1, 2, 1.0093173416231136e-17, 48.0, 1], [0, 0.4797029648228365, 0, 0, 0, 0.12297743237501833, 0, 0.14866799049606674, 0.2486516123060784, 21, 0, 0, 0, 1, 1, 128, 0.0, 29.275239470650614, 1], [0, 0.027993084042825835, 0, 0.23281972467958165, 0, 0.35773209836419834, 0, 0.11277580258015586, 0.26867929033323834, 21, 1, 1, 1, 1, 1, 128, 0.09731501319020856, 26.801795577226084, 1], [0, 0.3535687765903328, 0.04614152962583302, 0.43345390905464665, 0, 0.03441448341114126, 0, 0, 0.13242130131804644, 13, 0, 0, 0, 1, 1, 111, 0.0, 31.737160320822, 1], [0.3674226825232981, 0, 0.05160708240192936, 0.12344872510092648, 0, 0.0965444247931614, 0, 0.067994417637498, 0.2929826675431865, 20, 0, 1, 0, 0, 1, 47, 0.0, 32.106209822146326, 1], [0, 0, 0.025775104543649518, 0.16617237218416556, 0.16063473708087433, 0.12993872670313006, 0, 0.06188854960435471, 0.45559050988382593, 14, 0, 1, 0, 1, 1, 63, 5.177956050417777e-19, 26.611116691575894, 1], [0, 0.06671238863236903, 0.06006402557410644, 0.17452526547016364, 0.01031476691652826, 0.6831612454032585, 0, 0, 0, 18, 1, 0, 1, 1, 1, 2, 4.689311273921698e-19, 33.35421995035911, 1], [0, 0.30013063989288136, 0.12672659515206217, 0, 0.10724659876508229, 0.3996913978881711, 0, 0, 0.0639046674003578, 16, 1, 1, 1, 0, 1, 128, 0.005118820700197063, 1.4800000190734863, 1], [0, 0.2043651821028904, 0.04230802963341273, 0.3117896684064489, 0, 0.1421176434648922, 0.08747351696036645, 0.12256786831359201, 0.08115826253040204, 18, 0, 1, 0, 0, 1, 50, 7.121334955216545e-20, 34.600233411904355, 1], [0, 0.02506756387432659, 0.039924158965628884, 0.12348556422083982, 0.20462414374280352, 0.28818954670954505, 0.04896126122018452, 0.023831482535100687, 0.24591627873157101, 21, 1, 1, 0, 1, 1, 128, 5.1573515439894996e-20, 42.12371485967695, 1], [0, 0, 0.08143653076474816, 0.48780355615645177, 0, 0.20518816767486603, 0.03257093721587698, 0.18631035528533146, 0, 15, 1, 1, 1, 1, 1, 9, 2.168404344971009e-19, 31.298170553890856, 1], [0.15720357792815762, 0.2997186424805247, 0.021988093196371038, 0.13692141833005994, 0, 0.19934556973566903, 0.04621669216338653, 0.13860600616583119, 0, 13, 1, 1, 1, 1, 1, 36, 9.144437483899277e-19, 26.482701633633493, 1], [0.0725965154605662, 0.02985040634894141, 0.12912607658166564, 0.1453086365726042, 0.02834415645441421, 0.4136881470367985, 0, 0.04886468072833945, 0.12565341002675165, 19, 0, 1, 0, 1, 1, 21, 0.0006026085895487142, 32.47128059077284, 1], [0, 0.11268025817975949, 0.07030231513030571, 0.1413592574481926, 0, 0.42400870339596464, 0.05577564589875332, 0.07520500798243882, 0.11829663541416684, 25, 0, 1, 0, 1, 1, 36, 0.030656145413005098, 27.51657897465981, 1], [0, 0.0751791585847235, 0.06734551716049059, 0.22287210233062338, 0, 0.43298765625058855, 0, 0.08609169378739602, 0.11552387188617799, 13, 0, 1, 0, 1, 1, 128, 0.006926009508305727, 35.5637175061527, 1], [0.0983000677920687, 0.057677952998168976, 0.017447151517033504, 0.19624908237130798, 0.13189039899619653, 0.30897489873270273, 0.01473292037609274, 0.15987207023405295, 0.014855456982375883, 21, 0, 1, 0, 1, 1, 50, 0.010746782261566902, 24.25231909001765, 1], [0.03735347711187067, 0.14062904208578522, 0.08820311043006994, 0.15971965645279598, 0.20323558195273098, 0.25130346468456055, 0.02357192944683908, 0, 0.08913797396351827, 15, 0, 1, 0, 1, 1, 57, 0.05747561757857964, 32.360253262882196, 1], [0.11277702178951936, 0.04592035635873476, 0.060907858258340095, 0.218368761222734, 0, 0.3374844900664259, 0, 0.07683890253700507, 0.14770260976724087, 15, 1, 1, 0, 1, 1, 54, 9.145307880490847e-19, 32.7729818677275, 1], [0.12264224944353569, 0.03346983254959673, 0.038244190596703934, 0.21848833880396734, 0.034932607185137375, 0.14559699818740013, 0.06696400570193803, 0.0675134229051203, 0.27214835462660053, 14, 0, 1, 0, 1, 1, 49, 0.0, 29.296249977228467, 1], [0, 0.1343625150696407, 0.012686487452494904, 0.1544169634060855, 0.13423833802556215, 0.1578414788298493, 0.08218825405407963, 0.05199778305728708, 0.27226818010500076, 19, 1, 1, 1, 0, 1, 48, 0.005691973419476557, 26.395946670281205, 1], [0.10214476143456518, 0.1832028581026839, 0.01028673527770296, 0.024872594583185963, 0.07050439038864023, 0, 0, 0.1917704267460088, 0.417218233467213, 12, 1, 1, 1, 1, 1, 125, 0.005753349568666645, 28.989827631196647, 1], [0.20930568107420758, 0.18598684956588418, 0.02750809105200561, 0, 0.10965524442135752, 0.0346435405940704, 0.06615595659139445, 0.14487007676953975, 0.21251820424305962, 18, 1, 1, 0, 0, 1, 99, 0.03894786867881531, 27.583586557138233, 1], [0.034126535739198806, 0.11475122909559292, 0.0959523863891462, 0.20809461886442207, 0, 0.2996803069426379, 0, 0.055524839715893566, 0.19027195581472722, 15, 0, 1, 0, 1, 1, 40, 4.792406786123038e-19, 36.93298264054999, 1], [0, 0.05554593322717673, 0.08676136133120844, 0.18131686805179487, 0, 0.4788748745779206, 0, 0.050696272112327, 0.14680469069957236, 15, 0, 1, 0, 1, 1, 128, 6.130700940266754e-21, 28.170077246134205, 1], [0, 0.14147633329362022, 0.06593751920250716, 0.18373210731480302, 0.027334546062080755, 0.449399814307123, 0, 0.02942545357245824, 0.10269422624740762, 14, 0, 1, 0, 1, 1, 50, 0.004471980819431203, 38.30824810975315, 1], [0.11775606342112224, 0.0807302401011516, 0.08752991589866407, 0.17648425423301675, 0.0356309376416858, 0.08468382032521536, 0.05098558834998644, 0.09173774454327531, 0.2744614354858824, 19, 0, 1, 0, 1, 1, 42, 0.0, 27.859450368071677, 1], [0.136079460214977, 0.20640525856047803, 0, 0.0924946755619553, 0.08777359043957246, 0.1015325317675779, 0, 0.1310701325973913, 0.24464435085804798, 13, 1, 0, 0, 1, 0, 22, 0.07692924628218017, 16.720352388457265, 1], [0.06170155052890307, 0.040986057144855016, 0, 0.11056031480760863, 0.022109391436653594, 0.2598008632980057, 0.05513885823191499, 0.1185518480023339, 0.33115111654972507, 13, 1, 1, 1, 1, 1, 128, 0.0, 25.783373216892613, 1], [0.14290317256806023, 0.05941352572553055, 0.10529450292614892, 0.1279802764407148, 0.05371909035242574, 0.3498683143916352, 0, 0.092216540269351, 0.0686045773261336, 24, 0, 1, 0, 1, 1, 64, 0.0, 29.971032931310212, 1], [0.22255988793550932, 0.03133357694263833, 0.041990113078241596, 0.13440842229068806, 0.056451783526283314, 0.32868679968743775, 0, 0.11426502597345793, 0.07030439056574368, 15, 0, 1, 0, 1, 1, 2, 0.006758202125242452, 25.19007385455864, 1]], [[0, 0.02134830002950278, 0, 0.20530105042017613, 0, 0, 0, 0.19622083920514793, 0.5771298103451732, 20, 1, 1, 1, 1, 1, 2, 3.469446951953617e-18, 48.0, 1], [0, 0.27246279846809235, 0, 0.20567724731323705, 0, 0.3318222514064164, 0, 0, 0.1900377028122542, 21, 1, 0, 1, 1, 1, 128, 0.1, 32.79679622748782, 1], [0, 0.22223389094011903, 0, 0.35509117289235326, 0, 0, 0, 0.14058456189003868, 0.2820903742774892, 20, 0, 1, 0, 0, 1, 118, 0.0, 24.338731766076165, 1], [0, 0.428869947511847, 0.05541723316512428, 0, 0, 0.2112578584182118, 0, 0.0314055751105655, 0.2730493857942516, 13, 0, 0, 0, 1, 1, 61, 3.4480645183212413e-19, 39.41010416081197, 1], [0.31042071686712575, 0, 0.0877156107542139, 0.1426256766452828, 0.015011121566102835, 0.16750126860580924, 0, 0.11217175758583445, 0.1645538479756309, 23, 1, 1, 0, 0, 1, 58, 0.0, 30.49685604180935, 1], [0, 0, 0.02430454360261405, 0.18212739079047074, 0.09897867886616574, 0.10790362799123976, 0, 0.11052874998605003, 0.47615700876345973, 13, 0, 0, 0, 1, 1, 83, 1.6216989780348246e-19, 24.502031543125447, 1], [0, 0, 0.025267354075160033, 0.4028873960364454, 0.13633404785608544, 0.35722774732011897, 0, 0.024868918829722975, 0.05341453588246717, 15, 1, 1, 1, 1, 1, 2, 8.673617379884038e-20, 25.987460987107283, 1], [0.07756228641415211, 0.36500582502978773, 0.04318117667621176, 0.18998979998017135, 0, 0.11615086517667231, 0, 0.05560526901413261, 0.1525047777088722, 13, 0, 1, 1, 1, 1, 69, 4.3172370583502765e-20, 34.04989285200343, 1], [0, 0.1232468925950199, 0.034380086598851375, 0.34157286032221756, 0, 0.2815543638132771, 0.05554079611805452, 0, 0.16370500055257936, 17, 1, 0, 1, 1, 1, 128, 2.5930463851202934e-18, 47.8954707142289, 1], [0, 0, 0.14125906250540224, 0.07016069280274675, 0, 0.7253171548957502, 0.026035440530919725, 0.03722764926518103, 0, 15, 1, 1, 1, 0, 1, 128, 0.0, 48.0, 1], [0, 0.05529194315678087, 0.03868182446725941, 0.20776302287142764, 0, 0.400051796502122, 0.05646212086211791, 0.02827449479925128, 0.21347479734104086, 18, 1, 1, 0, 0, 1, 59, 2.781952198396969e-19, 26.55594176355511, 1], [0, 0.021657395472330886, 0.089848868944376, 0.35250048475448437, 0, 0.266863741116278, 0, 0.2691295097125308, 0, 16, 1, 1, 1, 1, 1, 2, 4.444464148472774e-21, 40.69577891992306, 1], [0.020381563280636325, 0.1343476229058217, 0.09019053559238699, 0, 0.2725139032565056, 0.4264871292844123, 0, 0.05607924568023723, 0, 15, 0, 0, 0, 1, 1, 31, 0.05397133499040581, 48.0, 1], [0, 0, 0.08636908730677324, 0.5337084742544295, 0, 0.237847766142234, 0.029634891231235555, 0.034487486535931526, 0.07795229452939625, 14, 0, 1, 0, 1, 1, 94, 2.3534919234240353e-19, 1.4800000190734866, 1], [0, 0.06799980345484967, 0.10858862879187078, 0.09208586228663374, 0, 0.5977933092422676, 0, 0.031990790109764, 0.10154160611461424, 19, 0, 1, 0, 1, 1, 3, 0.025368363008023573, 31.485834545520206, 1], [0.019447158193951694, 0.1812257822178173, 0.047854502590662824, 0.21532297181140364, 0.24169450603182588, 0.14164619807541945, 0, 0.1510612254920169, 0, 17, 0, 0, 0, 1, 0, 44, 0.017951878864132364, 29.794565958083233, 0], [0.08028633313972265, 0.2766645582978027, 0.04559340660071699, 0.08449212608797518, 0, 0.26043895650648524, 0.07686976928383439, 0.09302812453442497, 0.08262672554903787, 17, 0, 1, 0, 1, 1, 65, 0.006917288113729968, 28.993457433915104, 1], [0, 0, 0.1540473517388249, 0.1678990688748707, 0.28947283348927827, 0, 0.026124734003428383, 0, 0.36245601189359783, 17, 1, 0, 1, 0, 1, 8, 0.02997305159457278, 32.84718857312018, 1], [0, 0.06672225379568779, 0.03411067266115397, 0.47105447183422616, 0.06572298515754067, 0, 0.025931607978035055, 0, 0.33645800857335634, 16, 1, 1, 0, 1, 1, 75, 3.4694469519536153e-19, 34.297930073478895, 1], [0.22076873678556821, 0.08817295322396695, 0, 0.24368128617013926, 0, 0.26926560647104714, 0.04801978209636115, 0.09622467113695467, 0.02640123406134651, 14, 0, 1, 0, 1, 1, 63, 1.823477323089251e-19, 28.891492099938546, 1], [0.05568246788210142, 0.13632719102830768, 0.029086991485043543, 0.04950444956046127, 0.12264243873506611, 0.10303271605123898, 0, 0.1200788682008199, 0.383644877056961, 20, 1, 1, 0, 1, 1, 101, 8.656664920849258e-20, 32.281455366633956, 1], [0, 0.2612109218883443, 0.0716745698645916, 0.03372709808096027, 0.07598124923148529, 0.5574061609346181, 0, 0, 0, 17, 0, 1, 0, 1, 1, 128, 0.029528209240282818, 30.52929371085512, 1], [0.1815439865181526, 0.024714864868077994, 0.06963256221398914, 0.30737170579307393, 0, 0.23047926034698862, 0, 0, 0.18344245119750008, 14, 0, 1, 0, 1, 1, 42, 0.003087319466877446, 34.08581533436097, 1], [0.0656861406294812, 0.06376468067304018, 0, 0.23340475457261603, 0.058420225955057685, 0.336582439561792, 0.06563354801019267, 0.08540069709015026, 0.09110751350767003, 9, 0, 1, 0, 1, 1, 50, 0.009617390437377265, 28.52139335036519, 1], [0, 0.17837079923835344, 0.07053921717239513, 0.29448494363847266, 0, 0.17706289842743897, 0, 0.08976716960976912, 0.18977497191357057, 16, 0, 1, 0, 1, 1, 43, 1.852789555777943e-18, 35.89486233293965, 1], [0.1828065631850709, 0.3281222010351775, 0, 0, 0.09083699734759308, 0.037416788093903044, 0.02574934797214742, 0.09700962948280076, 0.23341599765040874, 5, 1, 1, 1, 1, 1, 128, 0.01168445909828972, 42.543152964789385, 1], [0.09413108280623274, 0.42525572682908913, 0, 0.03963716856885403, 0, 0.015382387227293137, 0, 0.19020880246287009, 0.23039862653582693, 7, 1, 1, 1, 1, 1, 110, 0.022074579542030015, 22.751181326422063, 1], [0, 0, 0.09578114003035579, 0.14443378846981483, 0, 0.4333791644033507, 0.03600196477781797, 0.04078617259334184, 0.2496177697253188, 16, 0, 1, 0, 1, 1, 43, 1.6817378298790544e-19, 35.25979431342027, 1], [0, 0, 0.069236668038109, 0.16464968305710906, 0.0313377690163251, 0.5448983088558389, 0, 0.18987757103261804, 0, 15, 1, 0, 1, 0, 1, 26, 0.02180473165587239, 19.04472187207947, 1], [0.09157568427314704, 0.03200961600002953, 0.07749560905668891, 0.1202800523807722, 0, 0.43849626832160604, 0.0734471709440397, 0.10149035708181786, 0.06520524194189868, 22, 0, 1, 0, 1, 1, 54, 0.0025115401742376938, 25.544838878784336, 1]], [[0, 0.021658519022546997, 0, 0.20597147049208245, 0, 0, 0, 0.19472128542832953, 0.577648725057041, 20, 1, 1, 1, 1, 1, 2, 1.2663481374630693e-17, 47.99999999999999, 1], [0, 0.47456664988190306, 0, 0, 0, 0.1256162927557584, 0, 0.15124363541743613, 0.24857342194490242, 21, 0, 0, 0, 1, 1, 128, 7.301197257607335e-20, 29.195137616014208, 1], [0, 0, 0.02329103332183575, 0.3468185651571559, 0.010713360280472432, 0.2943994761226241, 0, 0.06477865729634585, 0.25999890782156604, 15, 0, 0, 0, 1, 1, 128, 0.08155121370978102, 27.559310884646173, 1], [0.2939502053918837, 0.11828205979595419, 0.04737790507152565, 0.13487932284028326, 0.04442112657178404, 0.1205714234305019, 0, 0, 0.2405179568980673, 23, 1, 1, 1, 0, 1, 75, 1.7347234759768072e-19, 34.290363248714364, 1], [0, 0.5178662536735403, 0.03074503714230629, 0.20957521733226817, 0, 0.0975680973007262, 0, 0.08683921623769002, 0.057406178313469, 14, 1, 1, 0, 1, 1, 56, 0.0, 29.643640833149007, 1], [0, 0.12654891866670187, 0, 0.05265736281643136, 0.11899295435697144, 0.2474231589590815, 0, 0.09621687870326895, 0.35529211889840495, 13, 0, 1, 0, 1, 1, 75, 4.4651782271643e-19, 27.96127110891487, 1], [0, 0, 0.09650640803378398, 0, 0, 0.7051242677889109, 0, 0, 0.19836932417730513, 16, 1, 1, 1, 1, 1, 41, 0.0143584141831305, 38.47568258425922, 1], [0, 0.05298248130287865, 0.031837603430588036, 0.35047488169192903, 0, 0.4376952904763016, 0.05782904194464415, 0.01732532357688864, 0.050312568948945986, 18, 0, 1, 1, 1, 1, 128, 6.938893903907229e-19, 22.2990310904418, 1], [0, 0, 0.06158082479540719, 0.24559584018052225, 0, 0.5622114129317877, 0, 0.13061192209228298, 0, 19, 0, 0, 0, 1, 1, 39, 6.505213034913028e-19, 26.601311074356037, 1], [0.30621821373465136, 0.17875800563037733, 0.0957457659553367, 0.025790412786318283, 0, 0.3354765691197846, 0, 0, 0.054217628228234824, 14, 0, 1, 0, 0, 1, 62, 4.611574626291439e-21, 42.78201362927016, 1], [0, 0.03800655113687039, 0.07438922578816026, 0.2756152976131629, 0.1276810224476854, 0.30066974935235863, 0, 0.09369526795203466, 0.0897499039768538, 15, 0, 1, 0, 1, 1, 41, 0.0, 48.0, 1], [0, 0, 0, 0.1566969603521037, 0, 0.5744044101552194, 0, 0.15239772485174374, 0.1165009046409333, 15, 0, 1, 0, 1, 1, 91, 3.469446951953616e-19, 38.21624730487854, 1], [0, 0.06048125754225091, 0.13736505341081867, 0.572846242780969, 0, 0, 0.02908082283763126, 0, 0.20022662342833036, 16, 1, 1, 1, 0, 1, 128, 0.0, 48.0, 1], [0.05821794312981393, 0.3244845021165996, 0, 0, 0.0593551552614098, 0.05301865326179223, 0.08381023212151506, 0.09880127010680087, 0.3100677847584683, 17, 1, 1, 1, 1, 1, 73, 0.05337732623639885, 48.0, 1], [0.06419904509259675, 0, 0.018941840764587877, 0.21694076083524502, 0.02222151768288978, 0, 0.048228625210402276, 0.1576468338408621, 0.47182137657341633, 15, 0, 1, 0, 1, 1, 2, 1.458443567833615e-20, 26.290412349218936, 1], [0.07037198058865107, 0.14901661999337826, 0.08381696300449785, 0.3574880985422777, 0.1027676423995216, 0.08280447186572006, 0, 0.041677829058102155, 0.11205639454785124, 15, 1, 1, 1, 1, 1, 39, 0.0, 25.333526542594925, 1], [0, 0.9204347718599646, 0.05735852530231882, 0, 0, 0, 0.02220670283771666, 0, 0, 16, 0, 1, 1, 0, 1, 48, 0.0, 48.0, 1], [0, 0, 0.12392724264814728, 0.12165093549038578, 0.23174453779690837, 0.48483033994547087, 0.031191781769376942, 0, 0, 15, 1, 1, 1, 0, 1, 46, 6.939684807731537e-22, 33.91714501823304, 1], [0.05861149555183563, 0, 0, 0.09772005094827024, 0, 0.4465193701405455, 0.06737993414695136, 0.10735890585261566, 0.21783335620059696, 11, 0, 1, 0, 1, 1, 121, 1.4785051720441246e-20, 20.85961675984914, 1], [0.035959581537809454, 0.3170637198209435, 0.06743968712619755, 0.17669195921372347, 0, 0.19175367379075287, 0.07308658398901473, 0.04435563109674498, 0.09364916342481346, 16, 1, 1, 1, 0, 1, 40, 1.0962736143162835e-19, 30.33111201499967, 1], [0.042871309070237236, 0.013561349678663768, 0.07450063551425619, 0.10272858179376024, 0, 0.4422560142425123, 0.03994722888948038, 0.11922681125130938, 0.16490806955978055, 25, 0, 1, 0, 1, 1, 15, 0.005826087646127717, 26.676437939496665, 1], [0.03352440360006393, 0, 0.07245686031348128, 0.11903149769159763, 0, 0.35580014539902305, 0, 0.0549080147290604, 0.36427907826677386, 15, 0, 1, 0, 1, 1, 88, 0.02475173938289693, 27.818850229064807, 1], [0.08374292271887444, 0.2985387347615575, 0.059415064925999454, 0.18088524471118383, 0.09906779598032942, 0.1098790249413113, 0.031415119407731194, 0.09504736977735707, 0.042008722775655837, 15, 0, 1, 0, 1, 1, 42, 8.673617379884034e-20, 27.587974575079556, 1], [0.13211851371144348, 0.21428386763399276, 0.011528745154390447, 0.25493360005616045, 0.05293737332898547, 0, 0.056878658649426234, 0.013416197772839488, 0.2639030436927617, 10, 1, 1, 1, 1, 1, 126, 0.020038803278428136, 24.953352824473036, 1], [0.09297670908850178, 0, 0.06878146678373938, 0.18974699266787678, 0, 0.42300832851895376, 0, 0.17770436138727477, 0.047782141553653595, 15, 0, 1, 0, 1, 1, 61, 0.015151749030145345, 28.885634596466573, 1], [0.05834344993858891, 0.21714333828180574, 0.0699867444897952, 0.15197420508295068, 0.04866474180266279, 0.013924538996311894, 0.034791091916140875, 0.11150908696349375, 0.2936628025282502, 18, 1, 1, 1, 1, 1, 126, 0.01424793424646767, 28.401902929776703, 1], [0.09979982311974302, 0, 0.11642178128886087, 0.47833745066360706, 0.03771732393116378, 0.20502923928689715, 0.024413224284331958, 0, 0.02519734101836089, 18, 1, 1, 1, 0, 1, 30, 0.008263475530649676, 20.82225633237526, 1], [0.06764745988244494, 0.10326135273887804, 0.08839865571615682, 0.14848041645049662, 0, 0.44448829217891933, 0.023848914174697443, 0.08895542724894549, 0.03491948160946142, 17, 0, 1, 0, 1, 1, 44, 0.013149368976085984, 29.48533481138056, 1], [0, 0.24486656134908033, 0, 0, 0.292606676134082, 0.06400933341440383, 0.14210986503508466, 0.046431157521305964, 0.19603817391638126, 29, 1, 0, 1, 0, 1, 3, 0.1, 26.895620650015008, 1], [0, 0.20163224940121946, 0, 0.1330010622193701, 0.05193479861558027, 0, 0, 0.16350378588944736, 0.44992810387438287, 2, 0, 0, 1, 0, 1, 128, 0.00728556510107497, 38.42708686934615, 0]]], 'random_full_train_performance': [-1.192497968673706, -0.6183189153671265, -0.4600749611854553, -0.565803050994873, -0.7073624134063721, -0.5582028031349182, -0.15899628400802612, -0.23959694802761078, -0.19416816532611847, -0.4601367712020874, -0.36292925477027893, -0.20690536499023438, -1.1953213214874268, -0.6891473531723022, -1.2037627696990967, -0.7116898894309998, -2.0552961826324463, -0.23865436017513275, -0.4409463405609131, -0.5512655973434448, -0.1922573447227478, -0.3817158043384552, -0.699491560459137, -1.2115036249160767, -0.310642272233963, -0.8998646140098572, -0.5116100311279297, -0.253040075302124, -0.660702645778656, -1.1363232135772705]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.055 MB of 0.061 MB uploadedwandb: - 1.265 MB of 1.265 MB uploadedwandb: \ 1.265 MB of 1.265 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÑ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñá‚ñÉ‚ñà
wandb:            eval/runtime ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÅ
wandb: eval/samples_per_second ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñà
wandb:   eval/steps_per_second ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñà
wandb:             train/epoch ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÖ‚ñá‚ñÉ‚ñá‚ñÑ‚ñà
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÖ‚ñá‚ñÉ‚ñá‚ñÑ‚ñà
wandb:         train/grad_norm ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñà‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÅ‚ñá‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÇ‚ñÖ‚ñÅ‚ñá‚ñÇ‚ñÖ‚ñà‚ñÜ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñÅ‚ñÑ‚ñá
wandb:              train/loss ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñá‚ñÜ‚ñÇ‚ñÑ‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÜ
wandb: 
wandb: Run summary:
wandb:                eval/loss 1.13632
wandb:             eval/runtime 3.079
wandb:  eval/samples_per_second 324.783
wandb:    eval/steps_per_second 20.461
wandb:               total_flos 1.0652022723654451e+17
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 0.25743
wandb:      train/learning_rate 0.0
wandb:               train/loss 1.0013
wandb:               train_loss 1.19567
wandb:            train_runtime 171.6556
wandb: train_samples_per_second 58.25
wandb:   train_steps_per_second 3.641
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/62d2o85c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260101_185454-62d2o85c/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

2026-01-02 05:52:03.308690: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-02 05:52:03.340290: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-02 05:52:03.340336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-02 05:52:03.341296: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-02 05:52:03.346299: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-02 05:52:04.331100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'mmlu', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_mmlu_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['mmlu']
evaluation tasks and weights:  {'mmlu': (1.0, 'acc,none')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
mmlu
evaluation dataset:
data domain:  mmlu  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/mmlu/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.3161191096028816, 0.11426349731754912, 0.08683311140461922, 0.010648816854681904, 0.21071291050026175, 0.09848450659553602, 0.03252722732441375, 0.03858444462759559, 0.09182637577246108, 14, 1, 1, 1, 1, 0, 22, 0.04375408122258204, 13, 0]
Checking history sample input_X_between_0_1:  [0.3161191096028816, 0.11426349731754912, 0.08683311140461922, 0.010648816854681904, 0.21071291050026175, 0.09848450659553602, 0.03252722732441375, 0.03858444462759559, 0.09182637577246108, 0.4375, 1.0, 1.0, 1.0, 1.0, 0.0, 0.171875, 0.4375408122258204, 0.2708333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1066114902496338
Checking history sample input_X:  [0.17001858590456562, 0.06482605897458003, 0.0552231822789883, 0.2005042167465889, 0.1402610441334454, 0.22905697283425983, 0.05485942451788231, 0.033879744874935516, 0.05137076973475401, 21, 0, 1, 1, 1, 1, 71, 0.008194930202178574, 20, 0]
Checking history sample input_X_between_0_1:  [0.17001858590456562, 0.06482605897458003, 0.0552231822789883, 0.2005042167465889, 0.1402610441334454, 0.22905697283425983, 0.05485942451788231, 0.033879744874935516, 0.05137076973475401, 0.65625, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5546875, 0.08194930202178573, 0.4166666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9399064183235168
Checking history sample input_X:  [0.08457531624975921, 0.03444227766767646, 0.15033113821265595, 0.2888075957037953, 0.10483406652162627, 0.11125953049519431, 0.08585722091652723, 0.13154360559344933, 0.008349248639315885, 4, 0, 1, 1, 0, 0, 111, 0.08837128439087288, 42, 1]
Checking history sample input_X_between_0_1:  [0.08457531624975921, 0.03444227766767646, 0.15033113821265595, 0.2888075957037953, 0.10483406652162627, 0.11125953049519431, 0.08585722091652723, 0.13154360559344933, 0.008349248639315885, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8671875, 0.8837128439087288, 0.875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4136319160461426
Checking history sample input_X:  [0.16965502636986396, 0.1632541269761418, 0.06536948116802524, 0.22931460918771437, 0.07235389537017577, 0.07948837750247255, 0.09856044446849346, 0.10613959878213879, 0.015864440174974067, 2, 1, 0, 0, 0, 0, 52, 0.02440109885663705, 37, 1]
Checking history sample input_X_between_0_1:  [0.16965502636986396, 0.1632541269761418, 0.06536948116802524, 0.22931460918771437, 0.07235389537017577, 0.07948837750247255, 0.09856044446849346, 0.10613959878213879, 0.015864440174974067, 0.0625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.40625, 0.2440109885663705, 0.7708333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -2.0770633220672607
Checking history sample input_X:  [0.36131685118949053, 0.2612655996132517, 0.09728973792827665, 0.016352402241110658, 0.0637664750231403, 0.01680248270633843, 0.137283452814417, 0.01400964559956043, 0.03191335288441422, 16, 1, 1, 1, 0, 0, 70, 0.02082741206626623, 38, 1]
Checking history sample input_X_between_0_1:  [0.36131685118949053, 0.2612655996132517, 0.09728973792827665, 0.016352402241110658, 0.0637664750231403, 0.01680248270633843, 0.137283452814417, 0.01400964559956043, 0.03191335288441422, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.546875, 0.20827412066266227, 0.7916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0819246768951416
Checking history sample input_X:  [0.1471945258628963, 0.09048865243304925, 0.0028432699437709185, 0.03648812445816234, 0.15879745758736136, 0.08392207963907294, 0.3559804206469414, 0.1163518135768591, 0.007933655851886249, 30, 1, 1, 0, 1, 1, 27, 0.03341373220395105, 3, 1]
Checking history sample input_X_between_0_1:  [0.1471945258628963, 0.09048865243304925, 0.0028432699437709185, 0.03648812445816234, 0.15879745758736136, 0.08392207963907294, 0.3559804206469414, 0.1163518135768591, 0.007933655851886249, 0.9375, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2109375, 0.3341373220395105, 0.0625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2804617881774902
Checking history sample input_X:  [0.09708970671019405, 0.13689092369695835, 0.13726832030202735, 0.03956836294376928, 0.09044580719401238, 0.041564370215183784, 0.0018749764538878487, 0.24111683879487406, 0.21418069368909284, 25, 1, 0, 0, 1, 1, 72, 0.07793912935485114, 34, 0]
Checking history sample input_X_between_0_1:  [0.09708970671019405, 0.13689092369695835, 0.13726832030202735, 0.03956836294376928, 0.09044580719401238, 0.041564370215183784, 0.0018749764538878487, 0.24111683879487406, 0.21418069368909284, 0.78125, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5625, 0.7793912935485114, 0.7083333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0335614681243896
Checking history sample input_X:  [0.10072133130350767, 0.061195722672967294, 0.15043397081227575, 0.09909493084207656, 0.14517766942001123, 0.09911157330699087, 0.06709075752452866, 0.1444407486582126, 0.13273329545942936, 12, 1, 0, 0, 1, 0, 48, 0.029635134547486387, 5, 0]
Checking history sample input_X_between_0_1:  [0.10072133130350767, 0.061195722672967294, 0.15043397081227575, 0.09909493084207656, 0.14517766942001123, 0.09911157330699087, 0.06709075752452866, 0.1444407486582126, 0.13273329545942936, 0.375, 1.0, 0.0, 0.0, 1.0, 0.0, 0.375, 0.29635134547486386, 0.10416666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3930084705352783
Checking history sample input_X:  [0.0056408233326652405, 0.045454006708258686, 0.02419314968847306, 0.2621062198301061, 0.2543756705541004, 0.20901339392353954, 0.058849705392657874, 0.062447696140449545, 0.07791933442974946, 10, 0, 1, 0, 1, 1, 118, 0.004765499761682824, 9, 1]
Checking history sample input_X_between_0_1:  [0.0056408233326652405, 0.045454006708258686, 0.02419314968847306, 0.2621062198301061, 0.2543756705541004, 0.20901339392353954, 0.058849705392657874, 0.062447696140449545, 0.07791933442974946, 0.3125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.921875, 0.04765499761682824, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0495586395263672
Checking history sample input_X:  [0.03488693528172687, 0.35652639517245394, 0.07778500850664789, 0.10513445995831482, 0.36325923927607257, 0.0020496056495831597, 0.04514940236956389, 0.009292499052857398, 0.005916454732779405, 12, 1, 0, 1, 0, 0, 126, 0.042101384736383855, 9, 1]
Checking history sample input_X_between_0_1:  [0.03488693528172687, 0.35652639517245394, 0.07778500850664789, 0.10513445995831482, 0.36325923927607257, 0.0020496056495831597, 0.04514940236956389, 0.009292499052857398, 0.005916454732779405, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.984375, 0.4210138473638385, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.156050682067871
Checking history sample input_X:  [0.10948692372840407, 0.21669077939904466, 0.16290113684821536, 0.06632389273412838, 0.023331634670867053, 0.04511875862710639, 0.06417984157378358, 0.04013429279943936, 0.2718327396190111, 1, 1, 1, 1, 1, 0, 86, 0.0106663316581613, 38, 1]
Checking history sample input_X_between_0_1:  [0.10948692372840407, 0.21669077939904466, 0.16290113684821536, 0.06632389273412838, 0.023331634670867053, 0.04511875862710639, 0.06417984157378358, 0.04013429279943936, 0.2718327396190111, 0.03125, 1.0, 1.0, 1.0, 1.0, 0.0, 0.671875, 0.10666331658161299, 0.7916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.288783311843872
Checking history sample input_X:  [0.01438882831028789, 0.12997755569201166, 0.04075163354943151, 0.023595154255388905, 0.35477131795304295, 0.016630964685680923, 0.05509145809339371, 0.17242366325727926, 0.19236942420348319, 23, 1, 0, 1, 1, 0, 44, 0.0856132560172227, 24, 0]
Checking history sample input_X_between_0_1:  [0.01438882831028789, 0.12997755569201166, 0.04075163354943151, 0.023595154255388905, 0.35477131795304295, 0.016630964685680923, 0.05509145809339371, 0.17242366325727926, 0.19236942420348319, 0.71875, 1.0, 0.0, 1.0, 1.0, 0.0, 0.34375, 0.8561325601722269, 0.5, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9945324659347534
Checking history sample input_X:  [0.24449605892239556, 0.03207833927765533, 0.03240197712149855, 0.1790966164420265, 0.0030442733927761897, 0.05196332423567327, 0.23192639161149287, 0.20689610574355566, 0.018096913252926106, 27, 1, 0, 1, 0, 1, 88, 0.010689967975687609, 46, 0]
Checking history sample input_X_between_0_1:  [0.24449605892239556, 0.03207833927765533, 0.03240197712149855, 0.1790966164420265, 0.0030442733927761897, 0.05196332423567327, 0.23192639161149287, 0.20689610574355566, 0.018096913252926106, 0.84375, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6875, 0.10689967975687609, 0.9583333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0903468132019043
Checking history sample input_X:  [0.02102103063057685, 0.04371291849458268, 0.03642670439125302, 0.1177710527319927, 0.001407831158738137, 0.07877731789412545, 0.023108436790582124, 0.2712565713655495, 0.4065181365425995, 14, 0, 0, 1, 0, 1, 116, 0.0043609167679767745, 32, 0]
Checking history sample input_X_between_0_1:  [0.02102103063057685, 0.04371291849458268, 0.03642670439125302, 0.1177710527319927, 0.001407831158738137, 0.07877731789412545, 0.023108436790582124, 0.2712565713655495, 0.4065181365425995, 0.4375, 0.0, 0.0, 1.0, 0.0, 1.0, 0.90625, 0.04360916767976774, 0.6666666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.8854451179504395
Checking history sample input_X:  [0.07412507163006393, 0.2445504968671524, 0.13622879351830353, 0.04483166378469822, 0.22478374969436524, 0.03378463940701705, 0.23468787806930036, 0.0038162629362842922, 0.00319144409281491, 3, 1, 1, 0, 1, 1, 42, 0.09004868565085311, 14, 1]
Checking history sample input_X_between_0_1:  [0.07412507163006393, 0.2445504968671524, 0.13622879351830353, 0.04483166378469822, 0.22478374969436524, 0.03378463940701705, 0.23468787806930036, 0.0038162629362842922, 0.00319144409281491, 0.09375, 1.0, 1.0, 0.0, 1.0, 1.0, 0.328125, 0.9004868565085311, 0.2916666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.3376717567443848
Checking history sample input_X:  [0.13979331608413714, 0.22567075739867196, 0.047220640032135254, 0.04226304004460249, 0.20524041855401653, 0.1714340493110747, 0.08299650343653195, 0.04377008261845374, 0.04161119252037616, 32, 1, 1, 0, 1, 0, 29, 0.006821658518724117, 29, 0]
Checking history sample input_X_between_0_1:  [0.13979331608413714, 0.22567075739867196, 0.047220640032135254, 0.04226304004460249, 0.20524041855401653, 0.1714340493110747, 0.08299650343653195, 0.04377008261845374, 0.04161119252037616, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2265625, 0.06821658518724116, 0.6041666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9732747077941895
Checking history sample input_X:  [0.05744047965548223, 0.289973214141178, 0.20610036282289132, 0.008161921516160834, 0.05654145345652092, 0.0039259100145156494, 0.08013508882995081, 0.2050713532761911, 0.09265021628710923, 30, 1, 0, 1, 0, 1, 112, 0.003324070102424537, 36, 1]
Checking history sample input_X_between_0_1:  [0.05744047965548223, 0.289973214141178, 0.20610036282289132, 0.008161921516160834, 0.05654145345652092, 0.0039259100145156494, 0.08013508882995081, 0.2050713532761911, 0.09265021628710923, 0.9375, 1.0, 0.0, 1.0, 0.0, 1.0, 0.875, 0.03324070102424537, 0.75, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1381622552871704
Checking history sample input_X:  [0.011148142522340278, 0.2280230847756128, 0.289512358234605, 0.006770020691989324, 0.024707092605269564, 0.08731213423792487, 0.005804310855452206, 0.08464343941500369, 0.26207941666180234, 9, 1, 1, 1, 1, 0, 105, 0.09544186461572818, 12, 0]
Checking history sample input_X_between_0_1:  [0.011148142522340278, 0.2280230847756128, 0.289512358234605, 0.006770020691989324, 0.024707092605269564, 0.08731213423792487, 0.005804310855452206, 0.08464343941500369, 0.26207941666180234, 0.28125, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8203125, 0.9544186461572818, 0.25, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3062412738800049
Checking history sample input_X:  [0.018288746378659515, 0.11384303280049743, 0.03547167747709447, 0.12207096189867232, 0.06447946973899343, 0.18231445052447756, 0.1522399185681228, 0.13557467183201224, 0.17571707078147022, 4, 0, 1, 0, 1, 0, 53, 0.035644857853155076, 11, 0]
Checking history sample input_X_between_0_1:  [0.018288746378659515, 0.11384303280049743, 0.03547167747709447, 0.12207096189867232, 0.06447946973899343, 0.18231445052447756, 0.1522399185681228, 0.13557467183201224, 0.17571707078147022, 0.125, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4140625, 0.35644857853155076, 0.22916666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2854622602462769
Checking history sample input_X:  [0.511268635219867, 0.08759324792444484, 0.1034528140114814, 0.07300440359718659, 0.03242123832197673, 0.031167825877860832, 0.0622431292646515, 0.05784239394761446, 0.04100631183491673, 15, 0, 0, 1, 1, 0, 125, 0.022242287070241652, 48, 0]
Checking history sample input_X_between_0_1:  [0.511268635219867, 0.08759324792444484, 0.1034528140114814, 0.07300440359718659, 0.03242123832197673, 0.031167825877860832, 0.0622431292646515, 0.05784239394761446, 0.04100631183491673, 0.46875, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9765625, 0.2224228707024165, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0917696952819824
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.3671 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9089505672454834, 0.44492530822753906, 0.8335408568382263, 0.7245227694511414, 0.2723979949951172, 0.162494957447052, 0.5682340860366821, 0.9048105478286743, 0.8643947839736938, 0.8833190202713013, 0.5946435332298279, 0.8510079383850098, 0.058124840259552, 0.7151941061019897, 0.8033846616744995, 0.8343492746353149, 0.3631654381752014, 0.6703011989593506, 0.9739857912063599]  ‚Üí  acq = -0.18028301342376896
X = [0.7326999306678772, 0.7046439051628113, 0.41322898864746094, 0.724590003490448, 0.8838324546813965, 0.058849990367889404, 0.30059295892715454, 0.33979588747024536, 0.17974424362182617, 0.6537359952926636, 0.13360995054244995, 0.36228930950164795, 0.8030505776405334, 0.40566837787628174, 0.5906584858894348, 0.548897385597229, 0.7220836877822876, 0.32235175371170044, 0.6208162903785706]  ‚Üí  acq = -0.18078853639237558
X = [0.8685704469680786, 0.7159191370010376, 0.39079737663269043, 0.23804521560668945, 0.5261849761009216, 0.9893805384635925, 0.9529777765274048, 0.8923686742782593, 0.09791028499603271, 0.7687748074531555, 0.28195977210998535, 0.26437973976135254, 0.3991502523422241, 0.7524023056030273, 0.8315271139144897, 0.5485687255859375, 0.007722675800323486, 0.8424153327941895, 0.004985511302947998]  ‚Üí  acq = -0.18028301354381138
X = [0.16479992866516113, 0.35027605295181274, 0.7179569005966187, 0.018716096878051758, 0.4647156596183777, 0.1686762571334839, 0.8876041769981384, 0.356232225894928, 0.1957554817199707, 0.20810921490192413, 0.6647956967353821, 0.4634368419647217, 0.6886604428291321, 0.07205325365066528, 0.3337745666503906, 0.25389644503593445, 0.1762792468070984, 0.5369749069213867, 0.1016191840171814]  ‚Üí  acq = -0.18028301342390418
X = [0.5750206112861633, 0.004155576229095459, 0.5298779606819153, 0.9502435326576233, 0.18796515464782715, 0.4408145546913147, 0.38329416513442993, 0.6765797138214111, 0.6411966681480408, 0.8946990370750427, 0.37997615337371826, 0.2138270139694214, 0.7950549721717834, 0.5158151984214783, 0.4622836112976074, 0.5427699089050293, 0.4303721785545349, 0.17861013114452362, 0.21848684549331665]  ‚Üí  acq = -0.18028834352815815
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.4636, dtype=torch.float64), 0, tensor(0.0296, dtype=torch.float64), 0, 0, tensor(0.0751, dtype=torch.float64), 0, 0, tensor(0.4317, dtype=torch.float64), 27, 1, 1, 1, 0, 1, 107, 0.008828946339714285, 47.99999999999999, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.4636, dtype=torch.float64), tensor(8.8726e-18, dtype=torch.float64), tensor(0.0296, dtype=torch.float64), tensor(4.1266e-19, dtype=torch.float64), tensor(1.4112e-17, dtype=torch.float64), tensor(0.0751, dtype=torch.float64), tensor(6.9170e-17, dtype=torch.float64), tensor(6.8570e-18, dtype=torch.float64), tensor(0.4317, dtype=torch.float64), tensor(0.8315, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8367, dtype=torch.float64), tensor(0.0883, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.464
  gsm8k: 0
  rowan_hellaswag: 0.03
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.075
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.432

LoRA Parameters:
  lora_r: (107,)
  lora_dropout: (0.008828946339714285,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  107
lora dropout:  0.008828946339714285
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 144,958,464 || all params: 8,175,219,712 || trainable%: 1.7731
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260102_055425-ff9wxrri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/ff9wxrri
{'loss': 2.7352, 'grad_norm': 0.6535512804985046, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0134522914886475, 'eval_runtime': 10.6847, 'eval_samples_per_second': 93.592, 'eval_steps_per_second': 5.896, 'epoch': 0.04}
{'loss': 1.1617, 'grad_norm': 0.24620001018047333, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9379302263259888, 'eval_runtime': 10.6892, 'eval_samples_per_second': 93.553, 'eval_steps_per_second': 5.894, 'epoch': 0.08}
{'loss': 1.0503, 'grad_norm': 0.25289011001586914, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8568893671035767, 'eval_runtime': 10.7317, 'eval_samples_per_second': 93.182, 'eval_steps_per_second': 5.87, 'epoch': 0.12}
{'loss': 0.9565, 'grad_norm': 0.22152704000473022, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8152790069580078, 'eval_runtime': 10.7416, 'eval_samples_per_second': 93.096, 'eval_steps_per_second': 5.865, 'epoch': 0.16}
{'loss': 0.881, 'grad_norm': 0.22316497564315796, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8166604042053223, 'eval_runtime': 10.7613, 'eval_samples_per_second': 92.926, 'eval_steps_per_second': 5.854, 'epoch': 0.2}
{'loss': 0.8428, 'grad_norm': 0.25224533677101135, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8867781162261963, 'eval_runtime': 10.7564, 'eval_samples_per_second': 92.968, 'eval_steps_per_second': 5.857, 'epoch': 0.24}
{'loss': 0.841, 'grad_norm': 0.28851959109306335, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9043667316436768, 'eval_runtime': 10.7701, 'eval_samples_per_second': 92.85, 'eval_steps_per_second': 5.85, 'epoch': 0.28}
{'loss': 0.8235, 'grad_norm': 0.22464796900749207, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.885538101196289, 'eval_runtime': 10.7621, 'eval_samples_per_second': 92.918, 'eval_steps_per_second': 5.854, 'epoch': 0.32}
{'loss': 0.786, 'grad_norm': 0.25128254294395447, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9402776956558228, 'eval_runtime': 10.7923, 'eval_samples_per_second': 92.659, 'eval_steps_per_second': 5.838, 'epoch': 0.36}
{'loss': 0.7986, 'grad_norm': 0.31155264377593994, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9967637062072754, 'eval_runtime': 10.8302, 'eval_samples_per_second': 92.334, 'eval_steps_per_second': 5.817, 'epoch': 0.4}
{'loss': 0.7588, 'grad_norm': 0.25880903005599976, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9578485488891602, 'eval_runtime': 10.7856, 'eval_samples_per_second': 92.716, 'eval_steps_per_second': 5.841, 'epoch': 0.44}
{'loss': 0.727, 'grad_norm': 0.3485107123851776, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9872783422470093, 'eval_runtime': 10.7774, 'eval_samples_per_second': 92.787, 'eval_steps_per_second': 5.846, 'epoch': 0.48}
{'loss': 0.7206, 'grad_norm': 0.35340097546577454, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.959859013557434, 'eval_runtime': 10.7753, 'eval_samples_per_second': 92.805, 'eval_steps_per_second': 5.847, 'epoch': 0.52}
{'loss': 0.6527, 'grad_norm': 0.28525158762931824, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9855036735534668, 'eval_runtime': 10.7879, 'eval_samples_per_second': 92.696, 'eval_steps_per_second': 5.84, 'epoch': 0.56}
{'loss': 0.73, 'grad_norm': 0.3793862462043762, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.0027928352355957, 'eval_runtime': 10.7905, 'eval_samples_per_second': 92.675, 'eval_steps_per_second': 5.838, 'epoch': 0.6}
{'loss': 0.721, 'grad_norm': 0.31012749671936035, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9749037027359009, 'eval_runtime': 10.7856, 'eval_samples_per_second': 92.716, 'eval_steps_per_second': 5.841, 'epoch': 0.64}
{'loss': 0.6509, 'grad_norm': 0.30382582545280457, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.0110080242156982, 'eval_runtime': 10.8158, 'eval_samples_per_second': 92.458, 'eval_steps_per_second': 5.825, 'epoch': 0.68}
{'loss': 0.6354, 'grad_norm': 0.4409220516681671, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.020087957382202, 'eval_runtime': 10.8024, 'eval_samples_per_second': 92.572, 'eval_steps_per_second': 5.832, 'epoch': 0.72}
{'loss': 0.679, 'grad_norm': 0.3281126916408539, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.0570502281188965, 'eval_runtime': 10.7962, 'eval_samples_per_second': 92.625, 'eval_steps_per_second': 5.835, 'epoch': 0.76}
{'loss': 0.602, 'grad_norm': 0.23895171284675598, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.0432398319244385, 'eval_runtime': 10.7962, 'eval_samples_per_second': 92.626, 'eval_steps_per_second': 5.835, 'epoch': 0.8}
{'loss': 0.6374, 'grad_norm': 0.2427445650100708, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.042750358581543, 'eval_runtime': 10.7871, 'eval_samples_per_second': 92.703, 'eval_steps_per_second': 5.84, 'epoch': 0.84}
{'loss': 0.5954, 'grad_norm': 0.38422343134880066, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0613532066345215, 'eval_runtime': 10.8229, 'eval_samples_per_second': 92.396, 'eval_steps_per_second': 5.821, 'epoch': 0.88}
{'loss': 0.5949, 'grad_norm': 0.29781585931777954, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0981411933898926, 'eval_runtime': 10.8108, 'eval_samples_per_second': 92.5, 'eval_steps_per_second': 5.828, 'epoch': 0.92}
{'loss': 0.599, 'grad_norm': 0.2865787148475647, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1282451152801514, 'eval_runtime': 10.8209, 'eval_samples_per_second': 92.414, 'eval_steps_per_second': 5.822, 'epoch': 0.96}
{'loss': 0.5899, 'grad_norm': 0.4474049210548401, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.1192641258239746, 'eval_runtime': 10.7959, 'eval_samples_per_second': 92.628, 'eval_steps_per_second': 5.836, 'epoch': 1.0}
{'train_runtime': 458.8281, 'train_samples_per_second': 21.79, 'train_steps_per_second': 1.362, 'train_loss': 0.8308131637573242, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0134522914886475, 1.9379302263259888, 1.8568893671035767, 1.8152790069580078, 1.8166604042053223, 1.8867781162261963, 1.9043667316436768, 1.885538101196289, 1.9402776956558228, 1.9967637062072754, 1.9578485488891602, 1.9872783422470093, 1.959859013557434, 1.9855036735534668, 2.0027928352355957, 1.9749037027359009, 2.0110080242156982, 2.020087957382202, 2.0570502281188965, 2.0432398319244385, 2.042750358581543, 2.0613532066345215, 2.0981411933898926, 2.1282451152801514, 2.1192641258239746], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0134522914886475, 1.9379302263259888, 1.8568893671035767, 1.8152790069580078, 1.8166604042053223, 1.8867781162261963, 1.9043667316436768, 1.885538101196289, 1.9402776956558228, 1.9967637062072754, 1.9578485488891602, 1.9872783422470093, 1.959859013557434, 1.9855036735534668, 2.0027928352355957, 1.9749037027359009, 2.0110080242156982, 2.020087957382202, 2.0570502281188965, 2.0432398319244385, 2.042750358581543, 2.0613532066345215, 2.0981411933898926, 2.1282451152801514, 2.1192641258239746]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.1192641258239746
max eval_loss so far:  -2.1192641258239746
BO observations:  [-1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8930 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.46494268835564423
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.4649284771046147
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.4649284771042168
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.4649284771042197
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.4650316104214244
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2376, dtype=torch.float64), 0, 0, 0, tensor(0.1918, dtype=torch.float64), 0, tensor(0.5201, dtype=torch.float64), tensor(0.0506, dtype=torch.float64), 16, 0, 1, 1, 1, 1, 128, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2376, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5406e-17, dtype=torch.float64), tensor(1.0508e-17, dtype=torch.float64), tensor(0.1918, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5201, dtype=torch.float64), tensor(0.0506, dtype=torch.float64), tensor(0.4980, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.238
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.192
  wikitext: 0
  mmlu: 0.52
  arc_challenge: 0.051

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 123,731,968 || all params: 8,153,993,216 || trainable%: 1.5174
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4578, 'grad_norm': 0.658519446849823, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6217265129089355, 'eval_runtime': 9.5282, 'eval_samples_per_second': 104.951, 'eval_steps_per_second': 6.612, 'epoch': 0.04}
{'loss': 1.2961, 'grad_norm': 0.5533523559570312, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.359008550643921, 'eval_runtime': 9.5401, 'eval_samples_per_second': 104.82, 'eval_steps_per_second': 6.604, 'epoch': 0.08}
{'loss': 1.2116, 'grad_norm': 0.3195295035839081, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3198782205581665, 'eval_runtime': 9.5691, 'eval_samples_per_second': 104.503, 'eval_steps_per_second': 6.584, 'epoch': 0.12}
{'loss': 1.0812, 'grad_norm': 0.3983325958251953, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2875715494155884, 'eval_runtime': 9.5778, 'eval_samples_per_second': 104.408, 'eval_steps_per_second': 6.578, 'epoch': 0.16}
{'loss': 1.1093, 'grad_norm': 0.27561524510383606, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2686593532562256, 'eval_runtime': 9.5983, 'eval_samples_per_second': 104.185, 'eval_steps_per_second': 6.564, 'epoch': 0.2}
{'loss': 1.0757, 'grad_norm': 0.24304687976837158, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2587343454360962, 'eval_runtime': 9.5695, 'eval_samples_per_second': 104.498, 'eval_steps_per_second': 6.583, 'epoch': 0.24}
{'loss': 1.0692, 'grad_norm': 0.2488294541835785, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.251889705657959, 'eval_runtime': 9.5512, 'eval_samples_per_second': 104.699, 'eval_steps_per_second': 6.596, 'epoch': 0.28}
{'loss': 1.0504, 'grad_norm': 0.23223380744457245, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2476811408996582, 'eval_runtime': 9.5407, 'eval_samples_per_second': 104.814, 'eval_steps_per_second': 6.603, 'epoch': 0.32}
{'loss': 1.1115, 'grad_norm': 0.23741284012794495, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.239545464515686, 'eval_runtime': 9.5451, 'eval_samples_per_second': 104.765, 'eval_steps_per_second': 6.6, 'epoch': 0.36}
{'loss': 1.0341, 'grad_norm': 0.22413189709186554, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.237099289894104, 'eval_runtime': 9.5634, 'eval_samples_per_second': 104.565, 'eval_steps_per_second': 6.588, 'epoch': 0.4}
{'loss': 1.0176, 'grad_norm': 0.22991448640823364, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.239504337310791, 'eval_runtime': 9.5615, 'eval_samples_per_second': 104.586, 'eval_steps_per_second': 6.589, 'epoch': 0.44}
{'loss': 1.0341, 'grad_norm': 0.22493723034858704, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2345917224884033, 'eval_runtime': 9.5766, 'eval_samples_per_second': 104.421, 'eval_steps_per_second': 6.579, 'epoch': 0.48}
{'loss': 0.9956, 'grad_norm': 0.31756868958473206, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.227047324180603, 'eval_runtime': 9.6475, 'eval_samples_per_second': 103.653, 'eval_steps_per_second': 6.53, 'epoch': 0.52}
{'loss': 1.0488, 'grad_norm': 0.2516613006591797, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2269915342330933, 'eval_runtime': 9.6451, 'eval_samples_per_second': 103.679, 'eval_steps_per_second': 6.532, 'epoch': 0.56}
{'loss': 1.0, 'grad_norm': 0.21630050241947174, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2232577800750732, 'eval_runtime': 9.6244, 'eval_samples_per_second': 103.902, 'eval_steps_per_second': 6.546, 'epoch': 0.6}
{'loss': 0.9873, 'grad_norm': 0.23719815909862518, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2231338024139404, 'eval_runtime': 9.6721, 'eval_samples_per_second': 103.39, 'eval_steps_per_second': 6.514, 'epoch': 0.64}
{'loss': 1.0359, 'grad_norm': 0.26548105478286743, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.217104434967041, 'eval_runtime': 9.6404, 'eval_samples_per_second': 103.73, 'eval_steps_per_second': 6.535, 'epoch': 0.68}
{'loss': 1.0094, 'grad_norm': 0.24153627455234528, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2159228324890137, 'eval_runtime': 9.6291, 'eval_samples_per_second': 103.851, 'eval_steps_per_second': 6.543, 'epoch': 0.72}
{'loss': 1.0009, 'grad_norm': 0.20911747217178345, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2127150297164917, 'eval_runtime': 9.6257, 'eval_samples_per_second': 103.889, 'eval_steps_per_second': 6.545, 'epoch': 0.76}
{'loss': 0.9977, 'grad_norm': 0.24799717962741852, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2109943628311157, 'eval_runtime': 9.635, 'eval_samples_per_second': 103.788, 'eval_steps_per_second': 6.539, 'epoch': 0.8}
{'loss': 1.0411, 'grad_norm': 0.23058751225471497, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.208935022354126, 'eval_runtime': 9.6182, 'eval_samples_per_second': 103.969, 'eval_steps_per_second': 6.55, 'epoch': 0.84}
{'loss': 0.9994, 'grad_norm': 0.28454330563545227, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2076035737991333, 'eval_runtime': 9.6037, 'eval_samples_per_second': 104.127, 'eval_steps_per_second': 6.56, 'epoch': 0.88}
{'loss': 0.9781, 'grad_norm': 0.2504023313522339, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2065761089324951, 'eval_runtime': 9.6045, 'eval_samples_per_second': 104.117, 'eval_steps_per_second': 6.559, 'epoch': 0.92}
{'loss': 0.96, 'grad_norm': 0.2885981798171997, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2056565284729004, 'eval_runtime': 9.6138, 'eval_samples_per_second': 104.017, 'eval_steps_per_second': 6.553, 'epoch': 0.96}
{'loss': 0.9567, 'grad_norm': 0.32563307881355286, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2048003673553467, 'eval_runtime': 9.6236, 'eval_samples_per_second': 103.911, 'eval_steps_per_second': 6.546, 'epoch': 1.0}
{'train_runtime': 453.1309, 'train_samples_per_second': 22.064, 'train_steps_per_second': 1.379, 'train_loss': 1.1023724731445312, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6217265129089355, 1.359008550643921, 1.3198782205581665, 1.2875715494155884, 1.2686593532562256, 1.2587343454360962, 1.251889705657959, 1.2476811408996582, 1.239545464515686, 1.237099289894104, 1.239504337310791, 1.2345917224884033, 1.227047324180603, 1.2269915342330933, 1.2232577800750732, 1.2231338024139404, 1.217104434967041, 1.2159228324890137, 1.2127150297164917, 1.2109943628311157, 1.208935022354126, 1.2076035737991333, 1.2065761089324951, 1.2056565284729004, 1.2048003673553467], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6217265129089355, 1.359008550643921, 1.3198782205581665, 1.2875715494155884, 1.2686593532562256, 1.2587343454360962, 1.251889705657959, 1.2476811408996582, 1.239545464515686, 1.237099289894104, 1.239504337310791, 1.2345917224884033, 1.227047324180603, 1.2269915342330933, 1.2232577800750732, 1.2231338024139404, 1.217104434967041, 1.2159228324890137, 1.2127150297164917, 1.2109943628311157, 1.208935022354126, 1.2076035737991333, 1.2065761089324951, 1.2056565284729004, 1.2048003673553467]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2048003673553467
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.4289 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.5258397238341235
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.5255438571699762
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.5255438571702886
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.5271367498131455
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.5222930690893982
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3629, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6340, dtype=torch.float64), 15, 0, 0, 1, 1, 1, 128, 0.0, 16.88194075791525, 1]
normalized proposed parameters for next round by BO: [tensor(3.8851e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0031, dtype=torch.float64), tensor(0.3629, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9740e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6340, dtype=torch.float64), tensor(0.4634, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3517, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.363
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.634

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (16.88194075791525,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  16.88194075791525
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 106,168,320 || all params: 8,136,429,568 || trainable%: 1.3049
length of training data:  9968
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3434, 'grad_norm': 1.3514145612716675, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9607970714569092, 'eval_runtime': 9.3679, 'eval_samples_per_second': 106.747, 'eval_steps_per_second': 6.725, 'epoch': 0.04}
{'loss': 1.1982, 'grad_norm': 0.2945128381252289, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9494342803955078, 'eval_runtime': 9.3924, 'eval_samples_per_second': 106.469, 'eval_steps_per_second': 6.708, 'epoch': 0.08}
{'loss': 0.9751, 'grad_norm': 0.1817256510257721, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.900661587715149, 'eval_runtime': 9.3828, 'eval_samples_per_second': 106.578, 'eval_steps_per_second': 6.714, 'epoch': 0.12}
{'loss': 0.8969, 'grad_norm': 0.2283623218536377, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.9109601974487305, 'eval_runtime': 9.413, 'eval_samples_per_second': 106.237, 'eval_steps_per_second': 6.693, 'epoch': 0.16}
{'loss': 0.8648, 'grad_norm': 0.20851194858551025, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.9138802289962769, 'eval_runtime': 9.4402, 'eval_samples_per_second': 105.93, 'eval_steps_per_second': 6.674, 'epoch': 0.2}
{'loss': 0.8342, 'grad_norm': 0.2139967381954193, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.8749524354934692, 'eval_runtime': 9.4191, 'eval_samples_per_second': 106.167, 'eval_steps_per_second': 6.689, 'epoch': 0.24}
{'loss': 0.8151, 'grad_norm': 0.1743168830871582, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.9286869764328003, 'eval_runtime': 9.4001, 'eval_samples_per_second': 106.382, 'eval_steps_per_second': 6.702, 'epoch': 0.28}
{'loss': 0.7584, 'grad_norm': 0.21943476796150208, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.9102729558944702, 'eval_runtime': 9.3985, 'eval_samples_per_second': 106.4, 'eval_steps_per_second': 6.703, 'epoch': 0.32}
{'loss': 0.7651, 'grad_norm': 0.20820800960063934, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.9403495788574219, 'eval_runtime': 9.4183, 'eval_samples_per_second': 106.176, 'eval_steps_per_second': 6.689, 'epoch': 0.36}
{'loss': 0.7476, 'grad_norm': 0.17564377188682556, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.9865959882736206, 'eval_runtime': 9.404, 'eval_samples_per_second': 106.338, 'eval_steps_per_second': 6.699, 'epoch': 0.4}
{'loss': 0.7181, 'grad_norm': 0.23008248209953308, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.9446965456008911, 'eval_runtime': 9.4313, 'eval_samples_per_second': 106.029, 'eval_steps_per_second': 6.68, 'epoch': 0.44}
{'loss': 0.7111, 'grad_norm': 0.3326577842235565, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 2.002967119216919, 'eval_runtime': 9.4303, 'eval_samples_per_second': 106.041, 'eval_steps_per_second': 6.681, 'epoch': 0.48}
{'loss': 0.6764, 'grad_norm': 0.25849342346191406, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.9877759218215942, 'eval_runtime': 9.4215, 'eval_samples_per_second': 106.14, 'eval_steps_per_second': 6.687, 'epoch': 0.52}
{'loss': 0.6743, 'grad_norm': 0.2694610357284546, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.985834002494812, 'eval_runtime': 9.4194, 'eval_samples_per_second': 106.164, 'eval_steps_per_second': 6.688, 'epoch': 0.56}
{'loss': 0.6561, 'grad_norm': 0.3425680100917816, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 2.0237016677856445, 'eval_runtime': 9.422, 'eval_samples_per_second': 106.134, 'eval_steps_per_second': 6.686, 'epoch': 0.6}
{'loss': 0.6421, 'grad_norm': 0.34501802921295166, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 2.066679000854492, 'eval_runtime': 9.4753, 'eval_samples_per_second': 105.537, 'eval_steps_per_second': 6.649, 'epoch': 0.64}
{'loss': 0.6495, 'grad_norm': 0.35943830013275146, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 2.0353920459747314, 'eval_runtime': 9.4763, 'eval_samples_per_second': 105.526, 'eval_steps_per_second': 6.648, 'epoch': 0.68}
{'loss': 0.5983, 'grad_norm': 0.36031824350357056, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 2.0476315021514893, 'eval_runtime': 9.4942, 'eval_samples_per_second': 105.328, 'eval_steps_per_second': 6.636, 'epoch': 0.72}
{'loss': 0.5857, 'grad_norm': 0.43507957458496094, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 2.057386875152588, 'eval_runtime': 9.4661, 'eval_samples_per_second': 105.64, 'eval_steps_per_second': 6.655, 'epoch': 0.76}
{'loss': 0.5548, 'grad_norm': 0.46542859077453613, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 2.0979082584381104, 'eval_runtime': 9.47, 'eval_samples_per_second': 105.597, 'eval_steps_per_second': 6.653, 'epoch': 0.8}
{'loss': 0.5804, 'grad_norm': 0.4357614815235138, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 2.0943312644958496, 'eval_runtime': 9.4617, 'eval_samples_per_second': 105.689, 'eval_steps_per_second': 6.658, 'epoch': 0.84}
{'loss': 0.5078, 'grad_norm': 0.46367159485816956, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 2.131863594055176, 'eval_runtime': 9.4822, 'eval_samples_per_second': 105.461, 'eval_steps_per_second': 6.644, 'epoch': 0.88}
{'loss': 0.5114, 'grad_norm': 0.5356863141059875, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 2.1486692428588867, 'eval_runtime': 9.5268, 'eval_samples_per_second': 104.967, 'eval_steps_per_second': 6.613, 'epoch': 0.92}
{'loss': 0.4768, 'grad_norm': 0.47700902819633484, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 2.176797389984131, 'eval_runtime': 9.5084, 'eval_samples_per_second': 105.17, 'eval_steps_per_second': 6.626, 'epoch': 0.96}
{'train_runtime': 373.7509, 'train_samples_per_second': 26.67, 'train_steps_per_second': 1.667, 'train_loss': 0.8104881742792757, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9607970714569092, 1.9494342803955078, 1.900661587715149, 1.9109601974487305, 1.9138802289962769, 1.8749524354934692, 1.9286869764328003, 1.9102729558944702, 1.9403495788574219, 1.9865959882736206, 1.9446965456008911, 2.002967119216919, 1.9877759218215942, 1.985834002494812, 2.0237016677856445, 2.066679000854492, 2.0353920459747314, 2.0476315021514893, 2.057386875152588, 2.0979082584381104, 2.0943312644958496, 2.131863594055176, 2.1486692428588867, 2.176797389984131], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9607970714569092, 1.9494342803955078, 1.900661587715149, 1.9109601974487305, 1.9138802289962769, 1.8749524354934692, 1.9286869764328003, 1.9102729558944702, 1.9403495788574219, 1.9865959882736206, 1.9446965456008911, 2.002967119216919, 1.9877759218215942, 1.985834002494812, 2.0237016677856445, 2.066679000854492, 2.0353920459747314, 2.0476315021514893, 2.057386875152588, 2.0979082584381104, 2.0943312644958496, 2.131863594055176, 2.1486692428588867, 2.176797389984131]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.176797389984131
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.1344 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.04070591926574707, 0.2670907974243164, 0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.11440835893154144, 0.8343492150306702, 0.6285832524299622, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5520853400230408, 0.8785960674285889, 0.7897642850875854, 0.10385686159133911]  ‚Üí  acq = -0.6022029815538635
X = [0.1669445037841797, 0.07206535339355469, 0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.29802340269088745, 0.02810537815093994, 0.37732040882110596, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4432501196861267, 0.8628382086753845, 0.2149072289466858, 0.16291123628616333]  ‚Üí  acq = -0.6022029815538635
X = [0.5642975568771362, 0.34905433654785156, 0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.45446842908859253, 0.9301089644432068, 0.8571122884750366, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9834365248680115, 0.5003925561904907, 0.9843506813049316, 0.11054939031600952]  ‚Üí  acq = -0.6022029815538635
X = [0.16956406831741333, 0.7232905030250549, 0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.11824709177017212, 0.728330671787262, 0.602367103099823, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4289284646511078, 0.0390055775642395, 0.09640960395336151, 0.9996876120567322]  ‚Üí  acq = -0.6022029815978117
X = [0.7488081455230713, 0.8432244062423706, 0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.5128886699676514, 0.7522366642951965, 0.6226845979690552, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.46897920966148376, 0.9671209454536438, 0.12299968302249908, 0.006412029266357422]  ‚Üí  acq = -0.6022029815538638
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2476, dtype=torch.float64), tensor(0.0793, dtype=torch.float64), tensor(0.0339, dtype=torch.float64), tensor(0.0454, dtype=torch.float64), tensor(0.2474, dtype=torch.float64), 0, tensor(0.3464, dtype=torch.float64), 0, 27, 0, 0, 1, 0, 1, 7, 6.573674024565671e-20, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(3.7135e-18, dtype=torch.float64), tensor(0.2476, dtype=torch.float64), tensor(0.0793, dtype=torch.float64), tensor(0.0339, dtype=torch.float64), tensor(0.0454, dtype=torch.float64), tensor(0.2474, dtype=torch.float64), tensor(5.6750e-18, dtype=torch.float64), tensor(0.3464, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8512, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0574, dtype=torch.float64), tensor(6.5737e-19, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.248
  rowan_hellaswag: 0.079
  sciq: 0.034
  triviaqa: 0.045
  truthfulqa_gen: 0.247
  wikitext: 0
  mmlu: 0.346
  arc_challenge: 0

LoRA Parameters:
  lora_r: (7,)
  lora_dropout: (6.573674024565671e-20,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  7
lora dropout:  6.573674024565671e-20
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 6,967,296 || all params: 8,037,228,544 || trainable%: 0.0867
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6885, 'grad_norm': 1.866288185119629, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6806139945983887, 'eval_runtime': 9.7055, 'eval_samples_per_second': 103.034, 'eval_steps_per_second': 6.491, 'epoch': 0.04}
{'loss': 1.4559, 'grad_norm': 1.0466399192810059, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4259334802627563, 'eval_runtime': 9.7363, 'eval_samples_per_second': 102.709, 'eval_steps_per_second': 6.471, 'epoch': 0.08}
{'loss': 1.2529, 'grad_norm': 0.736943781375885, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3271030187606812, 'eval_runtime': 9.7706, 'eval_samples_per_second': 102.347, 'eval_steps_per_second': 6.448, 'epoch': 0.12}
{'loss': 1.1532, 'grad_norm': 0.7088835835456848, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2698537111282349, 'eval_runtime': 9.7861, 'eval_samples_per_second': 102.186, 'eval_steps_per_second': 6.438, 'epoch': 0.16}
{'loss': 1.1749, 'grad_norm': 0.6225773692131042, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.262701392173767, 'eval_runtime': 9.816, 'eval_samples_per_second': 101.875, 'eval_steps_per_second': 6.418, 'epoch': 0.2}
{'loss': 1.1809, 'grad_norm': 0.6664459109306335, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2619397640228271, 'eval_runtime': 9.8457, 'eval_samples_per_second': 101.567, 'eval_steps_per_second': 6.399, 'epoch': 0.24}
{'loss': 1.0948, 'grad_norm': 0.66966313123703, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2532414197921753, 'eval_runtime': 9.837, 'eval_samples_per_second': 101.657, 'eval_steps_per_second': 6.404, 'epoch': 0.28}
{'loss': 1.0908, 'grad_norm': 0.6517709493637085, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2481999397277832, 'eval_runtime': 9.8137, 'eval_samples_per_second': 101.898, 'eval_steps_per_second': 6.42, 'epoch': 0.32}
{'loss': 1.1015, 'grad_norm': 0.6811518669128418, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.246633529663086, 'eval_runtime': 9.7988, 'eval_samples_per_second': 102.053, 'eval_steps_per_second': 6.429, 'epoch': 0.36}
{'loss': 1.0463, 'grad_norm': 0.71079421043396, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2455111742019653, 'eval_runtime': 9.8215, 'eval_samples_per_second': 101.818, 'eval_steps_per_second': 6.415, 'epoch': 0.4}
{'loss': 1.0799, 'grad_norm': 0.7701975703239441, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2455111742019653, 'eval_runtime': 9.8193, 'eval_samples_per_second': 101.84, 'eval_steps_per_second': 6.416, 'epoch': 0.44}
{'loss': 1.0985, 'grad_norm': 0.6313552856445312, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2390668392181396, 'eval_runtime': 9.8005, 'eval_samples_per_second': 102.035, 'eval_steps_per_second': 6.428, 'epoch': 0.48}
{'loss': 1.0465, 'grad_norm': 0.861198365688324, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2412201166152954, 'eval_runtime': 9.8314, 'eval_samples_per_second': 101.715, 'eval_steps_per_second': 6.408, 'epoch': 0.52}
{'loss': 1.0734, 'grad_norm': 0.7983377575874329, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.237356185913086, 'eval_runtime': 9.8258, 'eval_samples_per_second': 101.773, 'eval_steps_per_second': 6.412, 'epoch': 0.56}
{'loss': 1.0275, 'grad_norm': 0.7543638348579407, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2358171939849854, 'eval_runtime': 9.8327, 'eval_samples_per_second': 101.701, 'eval_steps_per_second': 6.407, 'epoch': 0.6}
{'loss': 1.0265, 'grad_norm': 0.7785837054252625, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2309741973876953, 'eval_runtime': 9.8234, 'eval_samples_per_second': 101.798, 'eval_steps_per_second': 6.413, 'epoch': 0.64}
{'loss': 0.9792, 'grad_norm': 1.0879379510879517, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.227899193763733, 'eval_runtime': 9.8077, 'eval_samples_per_second': 101.961, 'eval_steps_per_second': 6.424, 'epoch': 0.68}
{'loss': 1.0751, 'grad_norm': 0.8035308718681335, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2234225273132324, 'eval_runtime': 9.8384, 'eval_samples_per_second': 101.643, 'eval_steps_per_second': 6.403, 'epoch': 0.72}
{'loss': 0.9805, 'grad_norm': 0.7819077372550964, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2235517501831055, 'eval_runtime': 9.8152, 'eval_samples_per_second': 101.883, 'eval_steps_per_second': 6.419, 'epoch': 0.76}
{'loss': 1.0118, 'grad_norm': 0.6355531215667725, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2208999395370483, 'eval_runtime': 9.8195, 'eval_samples_per_second': 101.838, 'eval_steps_per_second': 6.416, 'epoch': 0.8}
{'loss': 1.0406, 'grad_norm': 0.6546351909637451, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2189133167266846, 'eval_runtime': 9.8287, 'eval_samples_per_second': 101.742, 'eval_steps_per_second': 6.41, 'epoch': 0.84}
{'loss': 1.0691, 'grad_norm': 0.7783607244491577, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2173032760620117, 'eval_runtime': 9.8294, 'eval_samples_per_second': 101.736, 'eval_steps_per_second': 6.409, 'epoch': 0.88}
{'loss': 1.0207, 'grad_norm': 0.6585214734077454, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2170778512954712, 'eval_runtime': 9.8285, 'eval_samples_per_second': 101.745, 'eval_steps_per_second': 6.41, 'epoch': 0.92}
{'loss': 1.003, 'grad_norm': 0.6567872762680054, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2163312435150146, 'eval_runtime': 9.8373, 'eval_samples_per_second': 101.654, 'eval_steps_per_second': 6.404, 'epoch': 0.96}
{'loss': 1.0492, 'grad_norm': 0.863447368144989, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2164639234542847, 'eval_runtime': 9.8347, 'eval_samples_per_second': 101.681, 'eval_steps_per_second': 6.406, 'epoch': 1.0}
{'train_runtime': 450.8926, 'train_samples_per_second': 22.174, 'train_steps_per_second': 1.386, 'train_loss': 1.1528567138671875, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6806139945983887, 1.4259334802627563, 1.3271030187606812, 1.2698537111282349, 1.262701392173767, 1.2619397640228271, 1.2532414197921753, 1.2481999397277832, 1.246633529663086, 1.2455111742019653, 1.2455111742019653, 1.2390668392181396, 1.2412201166152954, 1.237356185913086, 1.2358171939849854, 1.2309741973876953, 1.227899193763733, 1.2234225273132324, 1.2235517501831055, 1.2208999395370483, 1.2189133167266846, 1.2173032760620117, 1.2170778512954712, 1.2163312435150146, 1.2164639234542847], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6806139945983887, 1.4259334802627563, 1.3271030187606812, 1.2698537111282349, 1.262701392173767, 1.2619397640228271, 1.2532414197921753, 1.2481999397277832, 1.246633529663086, 1.2455111742019653, 1.2455111742019653, 1.2390668392181396, 1.2412201166152954, 1.237356185913086, 1.2358171939849854, 1.2309741973876953, 1.227899193763733, 1.2234225273132324, 1.2235517501831055, 1.2208999395370483, 1.2189133167266846, 1.2173032760620117, 1.2170778512954712, 1.2163312435150146, 1.2164639234542847]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2164639234542847
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.0688 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.6421300208122158
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.6380003097798409
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.6380002271740352
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.6382310248819395
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.6380040238095767
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4924, dtype=torch.float64), 0, tensor(0.0336, dtype=torch.float64), tensor(0.0933, dtype=torch.float64), 0, 0, tensor(0.0228, dtype=torch.float64), tensor(0.2664, dtype=torch.float64), tensor(0.0914, dtype=torch.float64), 18, 0, 0, 1, 0, 1, 128, 5.118389433593509e-20, 9.85882197002225, 0]
normalized proposed parameters for next round by BO: [tensor(0.4924, dtype=torch.float64), tensor(2.7813e-19, dtype=torch.float64), tensor(0.0336, dtype=torch.float64), tensor(0.0933, dtype=torch.float64), tensor(1.8040e-19, dtype=torch.float64), tensor(3.9123e-19, dtype=torch.float64), tensor(0.0228, dtype=torch.float64), tensor(0.2664, dtype=torch.float64), tensor(0.0914, dtype=torch.float64), tensor(0.5509, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(5.1184e-19, dtype=torch.float64), tensor(0.2054, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.492
  gsm8k: 0
  rowan_hellaswag: 0.034
  sciq: 0.093
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.023
  mmlu: 0.266
  arc_challenge: 0.091

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.118389433593509e-20,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (9.85882197002225,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  128
lora dropout:  5.118389433593509e-20
lora alpha:  9.85882197002225
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 84,934,656 || all params: 8,115,195,904 || trainable%: 1.0466
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8162, 'grad_norm': 0.24134045839309692, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1357779502868652, 'eval_runtime': 9.1751, 'eval_samples_per_second': 108.991, 'eval_steps_per_second': 6.866, 'epoch': 0.04}
{'loss': 1.8631, 'grad_norm': 0.17610135674476624, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6384904384613037, 'eval_runtime': 9.2211, 'eval_samples_per_second': 108.447, 'eval_steps_per_second': 6.832, 'epoch': 0.08}
{'loss': 1.4205, 'grad_norm': 0.08115188032388687, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5452033281326294, 'eval_runtime': 9.2786, 'eval_samples_per_second': 107.775, 'eval_steps_per_second': 6.79, 'epoch': 0.12}
{'loss': 1.3255, 'grad_norm': 0.0952671617269516, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.468956470489502, 'eval_runtime': 9.3447, 'eval_samples_per_second': 107.013, 'eval_steps_per_second': 6.742, 'epoch': 0.16}
{'loss': 1.3268, 'grad_norm': 0.11500836163759232, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4257490634918213, 'eval_runtime': 9.369, 'eval_samples_per_second': 106.734, 'eval_steps_per_second': 6.724, 'epoch': 0.2}
{'loss': 1.2267, 'grad_norm': 0.11346272379159927, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3569767475128174, 'eval_runtime': 9.3899, 'eval_samples_per_second': 106.498, 'eval_steps_per_second': 6.709, 'epoch': 0.24}
{'loss': 1.1712, 'grad_norm': 0.10727657377719879, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3137458562850952, 'eval_runtime': 9.3857, 'eval_samples_per_second': 106.545, 'eval_steps_per_second': 6.712, 'epoch': 0.28}
{'loss': 1.1386, 'grad_norm': 0.08385223895311356, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3079965114593506, 'eval_runtime': 9.3703, 'eval_samples_per_second': 106.72, 'eval_steps_per_second': 6.723, 'epoch': 0.32}
{'loss': 1.1038, 'grad_norm': 0.09285560995340347, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2999160289764404, 'eval_runtime': 9.3737, 'eval_samples_per_second': 106.682, 'eval_steps_per_second': 6.721, 'epoch': 0.36}
{'loss': 1.0623, 'grad_norm': 0.10105323791503906, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3019345998764038, 'eval_runtime': 9.3445, 'eval_samples_per_second': 107.015, 'eval_steps_per_second': 6.742, 'epoch': 0.4}
{'loss': 1.1361, 'grad_norm': 0.09017430245876312, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2995387315750122, 'eval_runtime': 9.3862, 'eval_samples_per_second': 106.54, 'eval_steps_per_second': 6.712, 'epoch': 0.44}
{'loss': 1.1671, 'grad_norm': 0.0923759788274765, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2963957786560059, 'eval_runtime': 9.394, 'eval_samples_per_second': 106.45, 'eval_steps_per_second': 6.706, 'epoch': 0.48}
{'loss': 1.0746, 'grad_norm': 0.08844108134508133, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.297637939453125, 'eval_runtime': 9.3711, 'eval_samples_per_second': 106.712, 'eval_steps_per_second': 6.723, 'epoch': 0.52}
{'loss': 1.1589, 'grad_norm': 0.09644421935081482, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2885236740112305, 'eval_runtime': 9.3624, 'eval_samples_per_second': 106.81, 'eval_steps_per_second': 6.729, 'epoch': 0.56}
{'loss': 1.0631, 'grad_norm': 0.09997115284204483, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2900251150131226, 'eval_runtime': 9.33, 'eval_samples_per_second': 107.181, 'eval_steps_per_second': 6.752, 'epoch': 0.6}
{'loss': 1.0839, 'grad_norm': 0.10462025552988052, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2902570962905884, 'eval_runtime': 9.2986, 'eval_samples_per_second': 107.544, 'eval_steps_per_second': 6.775, 'epoch': 0.64}
{'loss': 1.1049, 'grad_norm': 0.09819628298282623, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.284914255142212, 'eval_runtime': 9.3067, 'eval_samples_per_second': 107.45, 'eval_steps_per_second': 6.769, 'epoch': 0.68}
{'loss': 1.1236, 'grad_norm': 0.09436146914958954, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2849516868591309, 'eval_runtime': 9.2908, 'eval_samples_per_second': 107.634, 'eval_steps_per_second': 6.781, 'epoch': 0.72}
{'loss': 1.0503, 'grad_norm': 0.09846926480531693, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.285815715789795, 'eval_runtime': 9.3066, 'eval_samples_per_second': 107.45, 'eval_steps_per_second': 6.769, 'epoch': 0.76}
{'loss': 1.0438, 'grad_norm': 0.09899646043777466, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2839608192443848, 'eval_runtime': 9.3319, 'eval_samples_per_second': 107.159, 'eval_steps_per_second': 6.751, 'epoch': 0.8}
{'loss': 1.0853, 'grad_norm': 0.10181606560945511, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.281557559967041, 'eval_runtime': 9.3171, 'eval_samples_per_second': 107.329, 'eval_steps_per_second': 6.762, 'epoch': 0.84}
{'loss': 1.0837, 'grad_norm': 0.09578587114810944, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2814688682556152, 'eval_runtime': 9.3064, 'eval_samples_per_second': 107.453, 'eval_steps_per_second': 6.77, 'epoch': 0.88}
{'loss': 1.0846, 'grad_norm': 0.11234886944293976, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2811298370361328, 'eval_runtime': 9.3029, 'eval_samples_per_second': 107.493, 'eval_steps_per_second': 6.772, 'epoch': 0.92}
{'loss': 1.1247, 'grad_norm': 0.1018887460231781, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2802162170410156, 'eval_runtime': 9.3077, 'eval_samples_per_second': 107.438, 'eval_steps_per_second': 6.769, 'epoch': 0.96}
{'loss': 1.0668, 'grad_norm': 0.11612568795681, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.280200481414795, 'eval_runtime': 9.2861, 'eval_samples_per_second': 107.688, 'eval_steps_per_second': 6.784, 'epoch': 1.0}
{'train_runtime': 379.1175, 'train_samples_per_second': 26.372, 'train_steps_per_second': 1.649, 'train_loss': 1.2762442596435546, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1357779502868652, 1.6384904384613037, 1.5452033281326294, 1.468956470489502, 1.4257490634918213, 1.3569767475128174, 1.3137458562850952, 1.3079965114593506, 1.2999160289764404, 1.3019345998764038, 1.2995387315750122, 1.2963957786560059, 1.297637939453125, 1.2885236740112305, 1.2900251150131226, 1.2902570962905884, 1.284914255142212, 1.2849516868591309, 1.285815715789795, 1.2839608192443848, 1.281557559967041, 1.2814688682556152, 1.2811298370361328, 1.2802162170410156, 1.280200481414795], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1357779502868652, 1.6384904384613037, 1.5452033281326294, 1.468956470489502, 1.4257490634918213, 1.3569767475128174, 1.3137458562850952, 1.3079965114593506, 1.2999160289764404, 1.3019345998764038, 1.2995387315750122, 1.2963957786560059, 1.297637939453125, 1.2885236740112305, 1.2900251150131226, 1.2902570962905884, 1.284914255142212, 1.2849516868591309, 1.285815715789795, 1.2839608192443848, 1.281557559967041, 1.2814688682556152, 1.2811298370361328, 1.2802162170410156, 1.280200481414795]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.280200481414795
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9011 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.6530764330212088
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.6530720382622632
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.6530720369794747
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.6530720384801604
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.6530723144926771
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0169, dtype=torch.float64), tensor(0.0207, dtype=torch.float64), tensor(0.0555, dtype=torch.float64), tensor(0.3053, dtype=torch.float64), tensor(0.2878, dtype=torch.float64), 0, 0, tensor(0.3138, dtype=torch.float64), 19, 1, 0, 1, 0, 1, 128, 0.0, 26.163130094488803, 0]
normalized proposed parameters for next round by BO: [tensor(4.2001e-18, dtype=torch.float64), tensor(0.0169, dtype=torch.float64), tensor(0.0207, dtype=torch.float64), tensor(0.0555, dtype=torch.float64), tensor(0.3053, dtype=torch.float64), tensor(0.2878, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3138, dtype=torch.float64), tensor(0.5953, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5451, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.017
  rowan_hellaswag: 0.021
  sciq: 0.055
  triviaqa: 0.305
  truthfulqa_gen: 0.288
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.314

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (26.163130094488803,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  26.163130094488803
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 109,576,192 || all params: 8,139,837,440 || trainable%: 1.3462
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5122, 'grad_norm': 0.3926394581794739, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.025517225265503, 'eval_runtime': 9.3886, 'eval_samples_per_second': 106.513, 'eval_steps_per_second': 6.71, 'epoch': 0.04}
{'loss': 1.5117, 'grad_norm': 0.1718675047159195, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.918662428855896, 'eval_runtime': 9.4237, 'eval_samples_per_second': 106.115, 'eval_steps_per_second': 6.685, 'epoch': 0.08}
{'loss': 1.212, 'grad_norm': 0.17074917256832123, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8788342475891113, 'eval_runtime': 9.3987, 'eval_samples_per_second': 106.397, 'eval_steps_per_second': 6.703, 'epoch': 0.12}
{'loss': 1.0888, 'grad_norm': 0.22345910966396332, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.797029733657837, 'eval_runtime': 9.4375, 'eval_samples_per_second': 105.96, 'eval_steps_per_second': 6.675, 'epoch': 0.16}
{'loss': 0.9787, 'grad_norm': 0.1686483919620514, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8073588609695435, 'eval_runtime': 9.4687, 'eval_samples_per_second': 105.611, 'eval_steps_per_second': 6.653, 'epoch': 0.2}
{'loss': 0.944, 'grad_norm': 0.1898709088563919, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8266897201538086, 'eval_runtime': 9.5118, 'eval_samples_per_second': 105.132, 'eval_steps_per_second': 6.623, 'epoch': 0.24}
{'loss': 0.9132, 'grad_norm': 0.1797206848859787, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.786117672920227, 'eval_runtime': 9.5159, 'eval_samples_per_second': 105.087, 'eval_steps_per_second': 6.62, 'epoch': 0.28}
{'loss': 0.8887, 'grad_norm': 0.19563697278499603, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.877236008644104, 'eval_runtime': 9.5914, 'eval_samples_per_second': 104.26, 'eval_steps_per_second': 6.568, 'epoch': 0.32}
{'loss': 0.8352, 'grad_norm': 0.1944466531276703, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8282835483551025, 'eval_runtime': 9.5458, 'eval_samples_per_second': 104.758, 'eval_steps_per_second': 6.6, 'epoch': 0.36}
{'loss': 0.8553, 'grad_norm': 0.18258032202720642, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8398669958114624, 'eval_runtime': 9.6108, 'eval_samples_per_second': 104.05, 'eval_steps_per_second': 6.555, 'epoch': 0.4}
{'loss': 0.8961, 'grad_norm': 0.21793518960475922, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8589656352996826, 'eval_runtime': 9.617, 'eval_samples_per_second': 103.983, 'eval_steps_per_second': 6.551, 'epoch': 0.44}
{'loss': 0.8334, 'grad_norm': 0.23518095910549164, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9011783599853516, 'eval_runtime': 9.6338, 'eval_samples_per_second': 103.802, 'eval_steps_per_second': 6.54, 'epoch': 0.48}
{'loss': 0.8704, 'grad_norm': 0.27840957045555115, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9040156602859497, 'eval_runtime': 9.6362, 'eval_samples_per_second': 103.775, 'eval_steps_per_second': 6.538, 'epoch': 0.52}
{'loss': 0.7959, 'grad_norm': 0.26772353053092957, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9139587879180908, 'eval_runtime': 9.5879, 'eval_samples_per_second': 104.298, 'eval_steps_per_second': 6.571, 'epoch': 0.56}
{'loss': 0.729, 'grad_norm': 0.24752721190452576, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8516277074813843, 'eval_runtime': 9.5559, 'eval_samples_per_second': 104.647, 'eval_steps_per_second': 6.593, 'epoch': 0.6}
{'loss': 0.7932, 'grad_norm': 0.23057572543621063, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8172621726989746, 'eval_runtime': 9.543, 'eval_samples_per_second': 104.789, 'eval_steps_per_second': 6.602, 'epoch': 0.64}
{'loss': 0.7658, 'grad_norm': 0.27645307779312134, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9039186239242554, 'eval_runtime': 9.5437, 'eval_samples_per_second': 104.781, 'eval_steps_per_second': 6.601, 'epoch': 0.68}
{'loss': 0.7699, 'grad_norm': 0.270952045917511, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9462567567825317, 'eval_runtime': 9.5475, 'eval_samples_per_second': 104.739, 'eval_steps_per_second': 6.599, 'epoch': 0.72}
{'loss': 0.762, 'grad_norm': 0.3029029071331024, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9376682043075562, 'eval_runtime': 9.5898, 'eval_samples_per_second': 104.278, 'eval_steps_per_second': 6.569, 'epoch': 0.76}
{'loss': 0.7099, 'grad_norm': 0.2843334376811981, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9653637409210205, 'eval_runtime': 9.5937, 'eval_samples_per_second': 104.235, 'eval_steps_per_second': 6.567, 'epoch': 0.8}
{'loss': 0.6939, 'grad_norm': 0.27825653553009033, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9930251836776733, 'eval_runtime': 9.5614, 'eval_samples_per_second': 104.587, 'eval_steps_per_second': 6.589, 'epoch': 0.84}
{'loss': 0.7341, 'grad_norm': 0.3425137400627136, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.002711534500122, 'eval_runtime': 9.5327, 'eval_samples_per_second': 104.902, 'eval_steps_per_second': 6.609, 'epoch': 0.88}
{'loss': 0.7198, 'grad_norm': 0.31973809003829956, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.008018732070923, 'eval_runtime': 9.5421, 'eval_samples_per_second': 104.799, 'eval_steps_per_second': 6.602, 'epoch': 0.92}
{'loss': 0.6739, 'grad_norm': 0.3531646430492401, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0062899589538574, 'eval_runtime': 9.5411, 'eval_samples_per_second': 104.81, 'eval_steps_per_second': 6.603, 'epoch': 0.96}
{'loss': 0.6717, 'grad_norm': 0.3664506673812866, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.014878749847412, 'eval_runtime': 9.5368, 'eval_samples_per_second': 104.857, 'eval_steps_per_second': 6.606, 'epoch': 1.0}
{'train_runtime': 367.7744, 'train_samples_per_second': 27.182, 'train_steps_per_second': 1.699, 'train_loss': 0.9663534088134765, 'epoch': 1.0}
train_results:  {'eval_loss': [2.025517225265503, 1.918662428855896, 1.8788342475891113, 1.797029733657837, 1.8073588609695435, 1.8266897201538086, 1.786117672920227, 1.877236008644104, 1.8282835483551025, 1.8398669958114624, 1.8589656352996826, 1.9011783599853516, 1.9040156602859497, 1.9139587879180908, 1.8516277074813843, 1.8172621726989746, 1.9039186239242554, 1.9462567567825317, 1.9376682043075562, 1.9653637409210205, 1.9930251836776733, 2.002711534500122, 2.008018732070923, 2.0062899589538574, 2.014878749847412], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.025517225265503, 1.918662428855896, 1.8788342475891113, 1.797029733657837, 1.8073588609695435, 1.8266897201538086, 1.786117672920227, 1.877236008644104, 1.8282835483551025, 1.8398669958114624, 1.8589656352996826, 1.9011783599853516, 1.9040156602859497, 1.9139587879180908, 1.8516277074813843, 1.8172621726989746, 1.9039186239242554, 1.9462567567825317, 1.9376682043075562, 1.9653637409210205, 1.9930251836776733, 2.002711534500122, 2.008018732070923, 2.0062899589538574, 2.014878749847412]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.014878749847412
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.0266 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.7136726019533431
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.7136726019533409
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.7136789292645932
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.7136720364552136
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.7136726019533409
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1619, dtype=torch.float64), tensor(0.2435, dtype=torch.float64), tensor(0.0917, dtype=torch.float64), tensor(0.0476, dtype=torch.float64), tensor(0.0219, dtype=torch.float64), tensor(0.0496, dtype=torch.float64), tensor(0.0812, dtype=torch.float64), 0, tensor(0.3026, dtype=torch.float64), 28, 1, 1, 1, 1, 0, 101, 0.021786671198701354, 35.69135507938533, 1]
normalized proposed parameters for next round by BO: [tensor(0.1619, dtype=torch.float64), tensor(0.2435, dtype=torch.float64), tensor(0.0917, dtype=torch.float64), tensor(0.0476, dtype=torch.float64), tensor(0.0219, dtype=torch.float64), tensor(0.0496, dtype=torch.float64), tensor(0.0812, dtype=torch.float64), tensor(1.3045e-18, dtype=torch.float64), tensor(0.3026, dtype=torch.float64), tensor(0.8873, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7878, dtype=torch.float64), tensor(0.2179, dtype=torch.float64), tensor(0.7436, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.162
  gsm8k: 0.243
  rowan_hellaswag: 0.092
  sciq: 0.048
  triviaqa: 0.022
  truthfulqa_gen: 0.05
  wikitext: 0.081
  mmlu: 0
  arc_challenge: 0.303

LoRA Parameters:
  lora_r: (101,)
  lora_dropout: (0.021786671198701354,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (35.69135507938533,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  101
lora dropout:  0.021786671198701354
lora alpha:  35.69135507938533
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 141,897,728 || all params: 8,172,158,976 || trainable%: 1.7364
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6495, 'grad_norm': 0.9023305177688599, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9235705137252808, 'eval_runtime': 10.8446, 'eval_samples_per_second': 92.212, 'eval_steps_per_second': 5.809, 'epoch': 0.04}
{'loss': 1.3522, 'grad_norm': 0.2800149619579315, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7980600595474243, 'eval_runtime': 10.8301, 'eval_samples_per_second': 92.335, 'eval_steps_per_second': 5.817, 'epoch': 0.08}
{'loss': 1.2066, 'grad_norm': 0.27184468507766724, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8102710247039795, 'eval_runtime': 10.861, 'eval_samples_per_second': 92.073, 'eval_steps_per_second': 5.801, 'epoch': 0.12}
{'loss': 1.1864, 'grad_norm': 0.3007792830467224, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7519440650939941, 'eval_runtime': 10.8594, 'eval_samples_per_second': 92.086, 'eval_steps_per_second': 5.801, 'epoch': 0.16}
{'loss': 1.0687, 'grad_norm': 0.2370384782552719, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7589209079742432, 'eval_runtime': 10.9149, 'eval_samples_per_second': 91.618, 'eval_steps_per_second': 5.772, 'epoch': 0.2}
{'loss': 1.155, 'grad_norm': 0.24932408332824707, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.760009765625, 'eval_runtime': 10.9757, 'eval_samples_per_second': 91.111, 'eval_steps_per_second': 5.74, 'epoch': 0.24}
{'loss': 1.0754, 'grad_norm': 0.2601288855075836, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7562062740325928, 'eval_runtime': 10.9291, 'eval_samples_per_second': 91.499, 'eval_steps_per_second': 5.764, 'epoch': 0.28}
{'loss': 1.0065, 'grad_norm': 0.3016337752342224, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7453465461730957, 'eval_runtime': 10.9448, 'eval_samples_per_second': 91.367, 'eval_steps_per_second': 5.756, 'epoch': 0.32}
{'loss': 1.0118, 'grad_norm': 0.30756497383117676, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7564932107925415, 'eval_runtime': 10.9101, 'eval_samples_per_second': 91.658, 'eval_steps_per_second': 5.774, 'epoch': 0.36}
{'loss': 1.0139, 'grad_norm': 0.23275940120220184, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8191554546356201, 'eval_runtime': 10.8714, 'eval_samples_per_second': 91.984, 'eval_steps_per_second': 5.795, 'epoch': 0.4}
{'loss': 1.0503, 'grad_norm': 0.24033138155937195, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7936910390853882, 'eval_runtime': 10.8747, 'eval_samples_per_second': 91.956, 'eval_steps_per_second': 5.793, 'epoch': 0.44}
{'loss': 1.0078, 'grad_norm': 0.24940361082553864, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.788845181465149, 'eval_runtime': 10.8648, 'eval_samples_per_second': 92.041, 'eval_steps_per_second': 5.799, 'epoch': 0.48}
{'loss': 0.9899, 'grad_norm': 0.24947595596313477, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7972328662872314, 'eval_runtime': 10.868, 'eval_samples_per_second': 92.014, 'eval_steps_per_second': 5.797, 'epoch': 0.52}
{'loss': 0.9783, 'grad_norm': 0.3128991425037384, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7620737552642822, 'eval_runtime': 10.8669, 'eval_samples_per_second': 92.023, 'eval_steps_per_second': 5.797, 'epoch': 0.56}
{'loss': 0.974, 'grad_norm': 0.2557860314846039, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7942698001861572, 'eval_runtime': 10.8788, 'eval_samples_per_second': 91.922, 'eval_steps_per_second': 5.791, 'epoch': 0.6}
{'loss': 0.948, 'grad_norm': 0.1983208954334259, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.798399567604065, 'eval_runtime': 10.8788, 'eval_samples_per_second': 91.922, 'eval_steps_per_second': 5.791, 'epoch': 0.64}
{'loss': 1.0044, 'grad_norm': 0.2855677008628845, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8443769216537476, 'eval_runtime': 10.8736, 'eval_samples_per_second': 91.966, 'eval_steps_per_second': 5.794, 'epoch': 0.68}
{'loss': 0.9252, 'grad_norm': 0.2823418080806732, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.840678095817566, 'eval_runtime': 10.8932, 'eval_samples_per_second': 91.801, 'eval_steps_per_second': 5.783, 'epoch': 0.72}
{'loss': 0.956, 'grad_norm': 0.3363551199436188, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8558666706085205, 'eval_runtime': 10.9325, 'eval_samples_per_second': 91.47, 'eval_steps_per_second': 5.763, 'epoch': 0.76}
{'loss': 0.9058, 'grad_norm': 0.28825730085372925, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8452332019805908, 'eval_runtime': 10.9438, 'eval_samples_per_second': 91.376, 'eval_steps_per_second': 5.757, 'epoch': 0.8}
{'loss': 0.9455, 'grad_norm': 0.22329451143741608, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.837081789970398, 'eval_runtime': 10.9965, 'eval_samples_per_second': 90.938, 'eval_steps_per_second': 5.729, 'epoch': 0.84}
{'loss': 0.9333, 'grad_norm': 0.24897083640098572, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8359066247940063, 'eval_runtime': 11.016, 'eval_samples_per_second': 90.777, 'eval_steps_per_second': 5.719, 'epoch': 0.88}
{'loss': 0.9335, 'grad_norm': 0.27562394738197327, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.876070261001587, 'eval_runtime': 11.021, 'eval_samples_per_second': 90.736, 'eval_steps_per_second': 5.716, 'epoch': 0.92}
{'loss': 0.9267, 'grad_norm': 0.28547120094299316, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8559550046920776, 'eval_runtime': 11.0046, 'eval_samples_per_second': 90.871, 'eval_steps_per_second': 5.725, 'epoch': 0.96}
{'loss': 0.9441, 'grad_norm': 0.3079119026660919, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8587568998336792, 'eval_runtime': 11.0205, 'eval_samples_per_second': 90.74, 'eval_steps_per_second': 5.717, 'epoch': 1.0}
{'train_runtime': 533.5436, 'train_samples_per_second': 18.735, 'train_steps_per_second': 1.171, 'train_loss': 1.0859496826171875, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9235705137252808, 1.7980600595474243, 1.8102710247039795, 1.7519440650939941, 1.7589209079742432, 1.760009765625, 1.7562062740325928, 1.7453465461730957, 1.7564932107925415, 1.8191554546356201, 1.7936910390853882, 1.788845181465149, 1.7972328662872314, 1.7620737552642822, 1.7942698001861572, 1.798399567604065, 1.8443769216537476, 1.840678095817566, 1.8558666706085205, 1.8452332019805908, 1.837081789970398, 1.8359066247940063, 1.876070261001587, 1.8559550046920776, 1.8587568998336792], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9235705137252808, 1.7980600595474243, 1.8102710247039795, 1.7519440650939941, 1.7589209079742432, 1.760009765625, 1.7562062740325928, 1.7453465461730957, 1.7564932107925415, 1.8191554546356201, 1.7936910390853882, 1.788845181465149, 1.7972328662872314, 1.7620737552642822, 1.7942698001861572, 1.798399567604065, 1.8443769216537476, 1.840678095817566, 1.8558666706085205, 1.8452332019805908, 1.837081789970398, 1.8359066247940063, 1.876070261001587, 1.8559550046920776, 1.8587568998336792]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.8587568998336792
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 18.5476 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.7405564165721876
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.7327079734533378
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.7327079734533378
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.7327079734533378
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.7327079748489093
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0485, dtype=torch.float64), tensor(0.0509, dtype=torch.float64), tensor(0.1373, dtype=torch.float64), 0, tensor(0.0217, dtype=torch.float64), tensor(0.0578, dtype=torch.float64), tensor(0.2444, dtype=torch.float64), tensor(0.4394, dtype=torch.float64), 18, 0, 1, 0, 1, 1, 72, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0485, dtype=torch.float64), tensor(0.0509, dtype=torch.float64), tensor(0.1373, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0217, dtype=torch.float64), tensor(0.0578, dtype=torch.float64), tensor(0.2444, dtype=torch.float64), tensor(0.4394, dtype=torch.float64), tensor(0.5636, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5654, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.049
  rowan_hellaswag: 0.051
  sciq: 0.137
  triviaqa: 0
  truthfulqa_gen: 0.022
  wikitext: 0.058
  mmlu: 0.244
  arc_challenge: 0.439

LoRA Parameters:
  lora_r: (72,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  72
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 54,411,264 || all params: 8,084,672,512 || trainable%: 0.6730
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7798, 'grad_norm': 0.8885067105293274, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6554135084152222, 'eval_runtime': 9.3327, 'eval_samples_per_second': 107.15, 'eval_steps_per_second': 6.75, 'epoch': 0.04}
{'loss': 1.3549, 'grad_norm': 0.7953764200210571, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.440061330795288, 'eval_runtime': 9.3064, 'eval_samples_per_second': 107.453, 'eval_steps_per_second': 6.77, 'epoch': 0.08}
{'loss': 1.2212, 'grad_norm': 0.4318729341030121, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3506743907928467, 'eval_runtime': 9.3532, 'eval_samples_per_second': 106.916, 'eval_steps_per_second': 6.736, 'epoch': 0.12}
{'loss': 1.0957, 'grad_norm': 0.42830604314804077, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.30670964717865, 'eval_runtime': 9.3713, 'eval_samples_per_second': 106.709, 'eval_steps_per_second': 6.723, 'epoch': 0.16}
{'loss': 1.0538, 'grad_norm': 0.3272027373313904, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.295656442642212, 'eval_runtime': 9.382, 'eval_samples_per_second': 106.587, 'eval_steps_per_second': 6.715, 'epoch': 0.2}
{'loss': 1.1328, 'grad_norm': 0.43482351303100586, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2813884019851685, 'eval_runtime': 9.3932, 'eval_samples_per_second': 106.46, 'eval_steps_per_second': 6.707, 'epoch': 0.24}
{'loss': 1.1254, 'grad_norm': 0.5096169710159302, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2837541103363037, 'eval_runtime': 9.3862, 'eval_samples_per_second': 106.54, 'eval_steps_per_second': 6.712, 'epoch': 0.28}
{'loss': 1.0837, 'grad_norm': 0.5131167769432068, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2763863801956177, 'eval_runtime': 9.3851, 'eval_samples_per_second': 106.552, 'eval_steps_per_second': 6.713, 'epoch': 0.32}
{'loss': 1.0103, 'grad_norm': 0.2946717441082001, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2698537111282349, 'eval_runtime': 9.3827, 'eval_samples_per_second': 106.579, 'eval_steps_per_second': 6.715, 'epoch': 0.36}
{'loss': 1.0032, 'grad_norm': 0.41219252347946167, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2622627019882202, 'eval_runtime': 9.3691, 'eval_samples_per_second': 106.734, 'eval_steps_per_second': 6.724, 'epoch': 0.4}
{'loss': 0.9785, 'grad_norm': 0.4448908567428589, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.264791488647461, 'eval_runtime': 9.3326, 'eval_samples_per_second': 107.151, 'eval_steps_per_second': 6.751, 'epoch': 0.44}
{'loss': 1.0409, 'grad_norm': 0.5282301902770996, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2690098285675049, 'eval_runtime': 9.3267, 'eval_samples_per_second': 107.219, 'eval_steps_per_second': 6.755, 'epoch': 0.48}
{'loss': 1.0226, 'grad_norm': 0.6467791795730591, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2601673603057861, 'eval_runtime': 9.3273, 'eval_samples_per_second': 107.212, 'eval_steps_per_second': 6.754, 'epoch': 0.52}
{'loss': 0.998, 'grad_norm': 0.3884435296058655, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2554455995559692, 'eval_runtime': 9.3379, 'eval_samples_per_second': 107.091, 'eval_steps_per_second': 6.747, 'epoch': 0.56}
{'loss': 1.0105, 'grad_norm': 0.5802960395812988, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2528257369995117, 'eval_runtime': 9.339, 'eval_samples_per_second': 107.077, 'eval_steps_per_second': 6.746, 'epoch': 0.6}
{'loss': 0.945, 'grad_norm': 0.3544342815876007, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2526066303253174, 'eval_runtime': 9.334, 'eval_samples_per_second': 107.136, 'eval_steps_per_second': 6.75, 'epoch': 0.64}
{'loss': 0.9357, 'grad_norm': 0.4972800314426422, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2480971813201904, 'eval_runtime': 9.3346, 'eval_samples_per_second': 107.128, 'eval_steps_per_second': 6.749, 'epoch': 0.68}
{'loss': 0.9166, 'grad_norm': 0.49343594908714294, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2452133893966675, 'eval_runtime': 9.3299, 'eval_samples_per_second': 107.182, 'eval_steps_per_second': 6.752, 'epoch': 0.72}
{'loss': 0.9965, 'grad_norm': 0.6061158776283264, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2408939599990845, 'eval_runtime': 9.3409, 'eval_samples_per_second': 107.056, 'eval_steps_per_second': 6.745, 'epoch': 0.76}
{'loss': 0.8821, 'grad_norm': 0.5462622046470642, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2406508922576904, 'eval_runtime': 9.3816, 'eval_samples_per_second': 106.592, 'eval_steps_per_second': 6.715, 'epoch': 0.8}
{'loss': 0.9366, 'grad_norm': 0.5487757921218872, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2415287494659424, 'eval_runtime': 9.3943, 'eval_samples_per_second': 106.447, 'eval_steps_per_second': 6.706, 'epoch': 0.84}
{'loss': 0.8951, 'grad_norm': 0.7889862656593323, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2404991388320923, 'eval_runtime': 9.3869, 'eval_samples_per_second': 106.531, 'eval_steps_per_second': 6.711, 'epoch': 0.88}
{'loss': 0.9093, 'grad_norm': 0.5949177742004395, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2391618490219116, 'eval_runtime': 9.39, 'eval_samples_per_second': 106.496, 'eval_steps_per_second': 6.709, 'epoch': 0.92}
{'loss': 0.9026, 'grad_norm': 0.41705527901649475, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2378100156784058, 'eval_runtime': 9.3829, 'eval_samples_per_second': 106.577, 'eval_steps_per_second': 6.714, 'epoch': 0.96}
{'loss': 0.9089, 'grad_norm': 0.6550034284591675, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.23720383644104, 'eval_runtime': 9.3916, 'eval_samples_per_second': 106.478, 'eval_steps_per_second': 6.708, 'epoch': 1.0}
{'train_runtime': 433.4612, 'train_samples_per_second': 23.063, 'train_steps_per_second': 1.442, 'train_loss': 1.0855824462890624, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6554135084152222, 1.440061330795288, 1.3506743907928467, 1.30670964717865, 1.295656442642212, 1.2813884019851685, 1.2837541103363037, 1.2763863801956177, 1.2698537111282349, 1.2622627019882202, 1.264791488647461, 1.2690098285675049, 1.2601673603057861, 1.2554455995559692, 1.2528257369995117, 1.2526066303253174, 1.2480971813201904, 1.2452133893966675, 1.2408939599990845, 1.2406508922576904, 1.2415287494659424, 1.2404991388320923, 1.2391618490219116, 1.2378100156784058, 1.23720383644104], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6554135084152222, 1.440061330795288, 1.3506743907928467, 1.30670964717865, 1.295656442642212, 1.2813884019851685, 1.2837541103363037, 1.2763863801956177, 1.2698537111282349, 1.2622627019882202, 1.264791488647461, 1.2690098285675049, 1.2601673603057861, 1.2554455995559692, 1.2528257369995117, 1.2526066303253174, 1.2480971813201904, 1.2452133893966675, 1.2408939599990845, 1.2406508922576904, 1.2415287494659424, 1.2404991388320923, 1.2391618490219116, 1.2378100156784058, 1.23720383644104]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.23720383644104
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.5241 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.7212435862743755
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.7212435863150725
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.7212435862743939
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.7212435862746359
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.7212435862745213
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1223, dtype=torch.float64), tensor(0.0208, dtype=torch.float64), tensor(0.3410, dtype=torch.float64), tensor(0.0721, dtype=torch.float64), tensor(0.1357, dtype=torch.float64), tensor(0.0272, dtype=torch.float64), tensor(0.1044, dtype=torch.float64), tensor(0.1765, dtype=torch.float64), 21, 0, 0, 1, 0, 1, 83, 0.029164550120442296, 22.156323452673107, 0]
normalized proposed parameters for next round by BO: [tensor(1.6846e-18, dtype=torch.float64), tensor(0.1223, dtype=torch.float64), tensor(0.0208, dtype=torch.float64), tensor(0.3410, dtype=torch.float64), tensor(0.0721, dtype=torch.float64), tensor(0.1357, dtype=torch.float64), tensor(0.0272, dtype=torch.float64), tensor(0.1044, dtype=torch.float64), tensor(0.1765, dtype=torch.float64), tensor(0.6479, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6457, dtype=torch.float64), tensor(0.2916, dtype=torch.float64), tensor(0.4616, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.122
  rowan_hellaswag: 0.021
  sciq: 0.341
  triviaqa: 0.072
  truthfulqa_gen: 0.136
  wikitext: 0.027
  mmlu: 0.104
  arc_challenge: 0.176

LoRA Parameters:
  lora_r: (83,)
  lora_dropout: (0.029164550120442296,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (22.156323452673107,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  83
lora dropout:  0.029164550120442296
lora alpha:  22.156323452673107
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 64,253,952 || all params: 8,094,515,200 || trainable%: 0.7938
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1069, 'grad_norm': 0.28976598381996155, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.008108377456665, 'eval_runtime': 9.6629, 'eval_samples_per_second': 103.489, 'eval_steps_per_second': 6.52, 'epoch': 0.04}
{'loss': 1.5296, 'grad_norm': 0.2152402698993683, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6312236785888672, 'eval_runtime': 9.7343, 'eval_samples_per_second': 102.729, 'eval_steps_per_second': 6.472, 'epoch': 0.08}
{'loss': 1.2717, 'grad_norm': 0.1883864402770996, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.544956088066101, 'eval_runtime': 9.7933, 'eval_samples_per_second': 102.11, 'eval_steps_per_second': 6.433, 'epoch': 0.12}
{'loss': 1.1982, 'grad_norm': 0.19472739100456238, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.465754747390747, 'eval_runtime': 9.7946, 'eval_samples_per_second': 102.097, 'eval_steps_per_second': 6.432, 'epoch': 0.16}
{'loss': 1.038, 'grad_norm': 0.18392831087112427, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3619717359542847, 'eval_runtime': 9.84, 'eval_samples_per_second': 101.626, 'eval_steps_per_second': 6.402, 'epoch': 0.2}
{'loss': 1.0094, 'grad_norm': 0.167770117521286, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3291501998901367, 'eval_runtime': 9.8094, 'eval_samples_per_second': 101.943, 'eval_steps_per_second': 6.422, 'epoch': 0.24}
{'loss': 0.992, 'grad_norm': 0.15417379140853882, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3236879110336304, 'eval_runtime': 9.8057, 'eval_samples_per_second': 101.982, 'eval_steps_per_second': 6.425, 'epoch': 0.28}
{'loss': 1.0294, 'grad_norm': 0.18884776532649994, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3152018785476685, 'eval_runtime': 9.7819, 'eval_samples_per_second': 102.23, 'eval_steps_per_second': 6.44, 'epoch': 0.32}
{'loss': 1.0261, 'grad_norm': 0.16457253694534302, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3080222606658936, 'eval_runtime': 9.7903, 'eval_samples_per_second': 102.142, 'eval_steps_per_second': 6.435, 'epoch': 0.36}
{'loss': 1.0117, 'grad_norm': 0.1985338181257248, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3087295293807983, 'eval_runtime': 9.7529, 'eval_samples_per_second': 102.533, 'eval_steps_per_second': 6.46, 'epoch': 0.4}
{'loss': 0.957, 'grad_norm': 0.18832212686538696, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.30134117603302, 'eval_runtime': 9.7511, 'eval_samples_per_second': 102.553, 'eval_steps_per_second': 6.461, 'epoch': 0.44}
{'loss': 0.9751, 'grad_norm': 0.1628355234861374, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3001797199249268, 'eval_runtime': 9.7521, 'eval_samples_per_second': 102.542, 'eval_steps_per_second': 6.46, 'epoch': 0.48}
{'loss': 0.9859, 'grad_norm': 0.16384314000606537, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2993841171264648, 'eval_runtime': 9.7549, 'eval_samples_per_second': 102.512, 'eval_steps_per_second': 6.458, 'epoch': 0.52}
{'loss': 0.9557, 'grad_norm': 0.2139558047056198, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.295345425605774, 'eval_runtime': 9.7494, 'eval_samples_per_second': 102.57, 'eval_steps_per_second': 6.462, 'epoch': 0.56}
{'loss': 0.9517, 'grad_norm': 0.23078155517578125, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2953498363494873, 'eval_runtime': 9.7502, 'eval_samples_per_second': 102.562, 'eval_steps_per_second': 6.461, 'epoch': 0.6}
{'loss': 0.9632, 'grad_norm': 0.19011437892913818, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2898056507110596, 'eval_runtime': 9.7464, 'eval_samples_per_second': 102.602, 'eval_steps_per_second': 6.464, 'epoch': 0.64}
{'loss': 0.9126, 'grad_norm': 0.17506374418735504, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.291217565536499, 'eval_runtime': 9.7566, 'eval_samples_per_second': 102.495, 'eval_steps_per_second': 6.457, 'epoch': 0.68}
{'loss': 1.0112, 'grad_norm': 0.1911860555410385, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2885949611663818, 'eval_runtime': 9.757, 'eval_samples_per_second': 102.49, 'eval_steps_per_second': 6.457, 'epoch': 0.72}
{'loss': 0.9735, 'grad_norm': 0.22440265119075775, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.287560224533081, 'eval_runtime': 9.7251, 'eval_samples_per_second': 102.826, 'eval_steps_per_second': 6.478, 'epoch': 0.76}
{'loss': 0.9327, 'grad_norm': 0.2007356584072113, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.285140037536621, 'eval_runtime': 9.7259, 'eval_samples_per_second': 102.818, 'eval_steps_per_second': 6.478, 'epoch': 0.8}
{'loss': 0.8884, 'grad_norm': 0.2222716510295868, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2871383428573608, 'eval_runtime': 9.7179, 'eval_samples_per_second': 102.903, 'eval_steps_per_second': 6.483, 'epoch': 0.84}
{'loss': 0.9356, 'grad_norm': 0.2122303545475006, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2864021062850952, 'eval_runtime': 9.7197, 'eval_samples_per_second': 102.883, 'eval_steps_per_second': 6.482, 'epoch': 0.88}
{'loss': 0.9556, 'grad_norm': 0.21223603188991547, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2854433059692383, 'eval_runtime': 9.7367, 'eval_samples_per_second': 102.705, 'eval_steps_per_second': 6.47, 'epoch': 0.92}
{'loss': 0.9989, 'grad_norm': 0.18304608762264252, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2852418422698975, 'eval_runtime': 9.7267, 'eval_samples_per_second': 102.809, 'eval_steps_per_second': 6.477, 'epoch': 0.96}
{'loss': 0.9883, 'grad_norm': 0.25121092796325684, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2850857973098755, 'eval_runtime': 9.7449, 'eval_samples_per_second': 102.618, 'eval_steps_per_second': 6.465, 'epoch': 1.0}
{'train_runtime': 411.9324, 'train_samples_per_second': 24.266, 'train_steps_per_second': 1.517, 'train_loss': 1.1039330078125, 'epoch': 1.0}
train_results:  {'eval_loss': [2.008108377456665, 1.6312236785888672, 1.544956088066101, 1.465754747390747, 1.3619717359542847, 1.3291501998901367, 1.3236879110336304, 1.3152018785476685, 1.3080222606658936, 1.3087295293807983, 1.30134117603302, 1.3001797199249268, 1.2993841171264648, 1.295345425605774, 1.2953498363494873, 1.2898056507110596, 1.291217565536499, 1.2885949611663818, 1.287560224533081, 1.285140037536621, 1.2871383428573608, 1.2864021062850952, 1.2854433059692383, 1.2852418422698975, 1.2850857973098755], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.008108377456665, 1.6312236785888672, 1.544956088066101, 1.465754747390747, 1.3619717359542847, 1.3291501998901367, 1.3236879110336304, 1.3152018785476685, 1.3080222606658936, 1.3087295293807983, 1.30134117603302, 1.3001797199249268, 1.2993841171264648, 1.295345425605774, 1.2953498363494873, 1.2898056507110596, 1.291217565536499, 1.2885949611663818, 1.287560224533081, 1.285140037536621, 1.2871383428573608, 1.2864021062850952, 1.2854433059692383, 1.2852418422698975, 1.2850857973098755]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2850857973098755
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.6897 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.7601711917868013
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.7551110201059044
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.7602463879865763
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.7601694297924595
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.7602840054633216
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3034, dtype=torch.float64), 0, tensor(0.0722, dtype=torch.float64), 0, tensor(0.3484, dtype=torch.float64), tensor(0.2351, dtype=torch.float64), tensor(0.0408, dtype=torch.float64), 0, 0, 27, 1, 0, 1, 0, 1, 2, 0.06607897402843078, 9.100479302100386, 0]
normalized proposed parameters for next round by BO: [tensor(0.3034, dtype=torch.float64), tensor(1.3170e-17, dtype=torch.float64), tensor(0.0722, dtype=torch.float64), tensor(3.9264e-19, dtype=torch.float64), tensor(0.3484, dtype=torch.float64), tensor(0.2351, dtype=torch.float64), tensor(0.0408, dtype=torch.float64), tensor(8.0626e-18, dtype=torch.float64), tensor(8.3206e-19, dtype=torch.float64), tensor(0.8588, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.6608, dtype=torch.float64), tensor(0.1896, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.303
  gsm8k: 0
  rowan_hellaswag: 0.072
  sciq: 0
  triviaqa: 0.348
  truthfulqa_gen: 0.235
  wikitext: 0.041
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.06607897402843078,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (9.100479302100386,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.06607897402843078
lora alpha:  9.100479302100386
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,433,024 || all params: 8,032,694,272 || trainable%: 0.0303
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9772, 'grad_norm': 1.7031618356704712, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1719465255737305, 'eval_runtime': 9.8703, 'eval_samples_per_second': 101.314, 'eval_steps_per_second': 6.383, 'epoch': 0.04}
{'loss': 1.9214, 'grad_norm': 1.7070668935775757, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.1986212730407715, 'eval_runtime': 9.8532, 'eval_samples_per_second': 101.49, 'eval_steps_per_second': 6.394, 'epoch': 0.08}
{'loss': 1.4341, 'grad_norm': 0.8096768856048584, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.14298152923584, 'eval_runtime': 9.8798, 'eval_samples_per_second': 101.216, 'eval_steps_per_second': 6.377, 'epoch': 0.12}
{'loss': 1.3798, 'grad_norm': 0.8850988745689392, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.056251049041748, 'eval_runtime': 9.9022, 'eval_samples_per_second': 100.988, 'eval_steps_per_second': 6.362, 'epoch': 0.16}
{'loss': 1.2371, 'grad_norm': 0.7422035336494446, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.97542405128479, 'eval_runtime': 9.9104, 'eval_samples_per_second': 100.904, 'eval_steps_per_second': 6.357, 'epoch': 0.2}
{'loss': 1.1723, 'grad_norm': 0.7091148495674133, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.981665849685669, 'eval_runtime': 9.9143, 'eval_samples_per_second': 100.865, 'eval_steps_per_second': 6.354, 'epoch': 0.24}
{'loss': 1.0744, 'grad_norm': 0.9245380163192749, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.038084030151367, 'eval_runtime': 9.9158, 'eval_samples_per_second': 100.849, 'eval_steps_per_second': 6.353, 'epoch': 0.28}
{'loss': 1.0949, 'grad_norm': 0.7109397649765015, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.0096054077148438, 'eval_runtime': 9.9161, 'eval_samples_per_second': 100.846, 'eval_steps_per_second': 6.353, 'epoch': 0.32}
{'loss': 1.0942, 'grad_norm': 0.7087544202804565, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.0023574829101562, 'eval_runtime': 9.9178, 'eval_samples_per_second': 100.829, 'eval_steps_per_second': 6.352, 'epoch': 0.36}
{'loss': 1.1201, 'grad_norm': 0.6582496166229248, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.0483384132385254, 'eval_runtime': 9.9124, 'eval_samples_per_second': 100.884, 'eval_steps_per_second': 6.356, 'epoch': 0.4}
{'loss': 1.0539, 'grad_norm': 0.8906557559967041, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0038344860076904, 'eval_runtime': 9.9049, 'eval_samples_per_second': 100.96, 'eval_steps_per_second': 6.36, 'epoch': 0.44}
{'loss': 1.0687, 'grad_norm': 0.7680702209472656, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.0441339015960693, 'eval_runtime': 9.9033, 'eval_samples_per_second': 100.976, 'eval_steps_per_second': 6.361, 'epoch': 0.48}
{'loss': 1.0496, 'grad_norm': 0.7560916543006897, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.0874085426330566, 'eval_runtime': 9.902, 'eval_samples_per_second': 100.99, 'eval_steps_per_second': 6.362, 'epoch': 0.52}
{'loss': 1.1064, 'grad_norm': 0.7777241468429565, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.050246238708496, 'eval_runtime': 9.8984, 'eval_samples_per_second': 101.026, 'eval_steps_per_second': 6.365, 'epoch': 0.56}
{'loss': 1.0393, 'grad_norm': 0.8883019685745239, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.011460542678833, 'eval_runtime': 9.9373, 'eval_samples_per_second': 100.631, 'eval_steps_per_second': 6.34, 'epoch': 0.6}
{'loss': 1.0506, 'grad_norm': 1.3176125288009644, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.036320447921753, 'eval_runtime': 9.9407, 'eval_samples_per_second': 100.596, 'eval_steps_per_second': 6.338, 'epoch': 0.64}
{'loss': 1.0507, 'grad_norm': 0.902127742767334, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.012209177017212, 'eval_runtime': 9.9205, 'eval_samples_per_second': 100.801, 'eval_steps_per_second': 6.35, 'epoch': 0.68}
{'loss': 1.0533, 'grad_norm': 0.8888317346572876, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.013654947280884, 'eval_runtime': 9.9075, 'eval_samples_per_second': 100.934, 'eval_steps_per_second': 6.359, 'epoch': 0.72}
{'loss': 1.0124, 'grad_norm': 1.0243016481399536, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.0553090572357178, 'eval_runtime': 9.9146, 'eval_samples_per_second': 100.861, 'eval_steps_per_second': 6.354, 'epoch': 0.76}
{'loss': 1.0276, 'grad_norm': 0.7857850193977356, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.024564266204834, 'eval_runtime': 9.9054, 'eval_samples_per_second': 100.955, 'eval_steps_per_second': 6.36, 'epoch': 0.8}
{'loss': 0.9782, 'grad_norm': 0.9072828888893127, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0477726459503174, 'eval_runtime': 9.9057, 'eval_samples_per_second': 100.952, 'eval_steps_per_second': 6.36, 'epoch': 0.84}
{'loss': 1.0493, 'grad_norm': 1.1511799097061157, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.04677677154541, 'eval_runtime': 9.9177, 'eval_samples_per_second': 100.83, 'eval_steps_per_second': 6.352, 'epoch': 0.88}
{'loss': 0.9479, 'grad_norm': 1.0155996084213257, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0597314834594727, 'eval_runtime': 9.9166, 'eval_samples_per_second': 100.841, 'eval_steps_per_second': 6.353, 'epoch': 0.92}
{'loss': 0.9469, 'grad_norm': 0.8481687903404236, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0503346920013428, 'eval_runtime': 9.9051, 'eval_samples_per_second': 100.958, 'eval_steps_per_second': 6.36, 'epoch': 0.96}
{'loss': 1.0185, 'grad_norm': 0.866837739944458, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.0537195205688477, 'eval_runtime': 9.9212, 'eval_samples_per_second': 100.794, 'eval_steps_per_second': 6.35, 'epoch': 1.0}
{'train_runtime': 419.8248, 'train_samples_per_second': 23.815, 'train_steps_per_second': 1.489, 'train_loss': 1.2383492614746094, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1719465255737305, 2.1986212730407715, 2.14298152923584, 2.056251049041748, 1.97542405128479, 1.981665849685669, 2.038084030151367, 2.0096054077148438, 2.0023574829101562, 2.0483384132385254, 2.0038344860076904, 2.0441339015960693, 2.0874085426330566, 2.050246238708496, 2.011460542678833, 2.036320447921753, 2.012209177017212, 2.013654947280884, 2.0553090572357178, 2.024564266204834, 2.0477726459503174, 2.04677677154541, 2.0597314834594727, 2.0503346920013428, 2.0537195205688477], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1719465255737305, 2.1986212730407715, 2.14298152923584, 2.056251049041748, 1.97542405128479, 1.981665849685669, 2.038084030151367, 2.0096054077148438, 2.0023574829101562, 2.0483384132385254, 2.0038344860076904, 2.0441339015960693, 2.0874085426330566, 2.050246238708496, 2.011460542678833, 2.036320447921753, 2.012209177017212, 2.013654947280884, 2.0553090572357178, 2.024564266204834, 2.0477726459503174, 2.04677677154541, 2.0597314834594727, 2.0503346920013428, 2.0537195205688477]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.0537195205688477
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0943 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.8269160222633625
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.829361489258272
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.8273745856638821
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.8267376806789869
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.8267376767032614
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0184, dtype=torch.float64), tensor(0.1370, dtype=torch.float64), tensor(0.0855, dtype=torch.float64), tensor(0.0978, dtype=torch.float64), tensor(0.0392, dtype=torch.float64), tensor(0.2895, dtype=torch.float64), tensor(0.3260, dtype=torch.float64), 14, 1, 1, 0, 0, 0, 81, 5.737496960664978e-20, 6.765812343970218, 1]
normalized proposed parameters for next round by BO: [tensor(0.0067, dtype=torch.float64), tensor(1.0308e-17, dtype=torch.float64), tensor(0.0184, dtype=torch.float64), tensor(0.1370, dtype=torch.float64), tensor(0.0855, dtype=torch.float64), tensor(0.0978, dtype=torch.float64), tensor(0.0392, dtype=torch.float64), tensor(0.2895, dtype=torch.float64), tensor(0.3260, dtype=torch.float64), tensor(0.4261, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6321, dtype=torch.float64), tensor(5.7375e-19, dtype=torch.float64), tensor(0.1410, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.018
  sciq: 0.137
  triviaqa: 0.085
  truthfulqa_gen: 0.098
  wikitext: 0.039
  mmlu: 0.29
  arc_challenge: 0.326

LoRA Parameters:
  lora_r: (81,)
  lora_dropout: (5.737496960664978e-20,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 1, 0, 0, 0],)
  lora_alpha: (6.765812343970218,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 0]
lora rank:  81
lora dropout:  5.737496960664978e-20
lora alpha:  6.765812343970218
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 15,095,808 || all params: 8,045,357,056 || trainable%: 0.1876
length of training data:  9928
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.0177, 'grad_norm': 0.8158038854598999, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.7594776153564453, 'eval_runtime': 8.8546, 'eval_samples_per_second': 112.935, 'eval_steps_per_second': 7.115, 'epoch': 0.04}
{'loss': 2.5136, 'grad_norm': 0.7320202589035034, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.911974310874939, 'eval_runtime': 8.8615, 'eval_samples_per_second': 112.848, 'eval_steps_per_second': 7.109, 'epoch': 0.08}
{'loss': 1.7946, 'grad_norm': 0.5892168283462524, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.7714252471923828, 'eval_runtime': 8.8959, 'eval_samples_per_second': 112.411, 'eval_steps_per_second': 7.082, 'epoch': 0.12}
{'loss': 1.5895, 'grad_norm': 0.34794852137565613, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.6880801916122437, 'eval_runtime': 8.9167, 'eval_samples_per_second': 112.149, 'eval_steps_per_second': 7.065, 'epoch': 0.16}
{'loss': 1.5907, 'grad_norm': 0.3583909273147583, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.6511919498443604, 'eval_runtime': 8.9138, 'eval_samples_per_second': 112.186, 'eval_steps_per_second': 7.068, 'epoch': 0.2}
{'loss': 1.4848, 'grad_norm': 0.306205153465271, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.5953798294067383, 'eval_runtime': 8.9138, 'eval_samples_per_second': 112.186, 'eval_steps_per_second': 7.068, 'epoch': 0.24}
{'loss': 1.4455, 'grad_norm': 0.3102678954601288, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.5719208717346191, 'eval_runtime': 8.9212, 'eval_samples_per_second': 112.092, 'eval_steps_per_second': 7.062, 'epoch': 0.28}
{'loss': 1.4417, 'grad_norm': 0.2749094069004059, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.5551459789276123, 'eval_runtime': 8.9273, 'eval_samples_per_second': 112.016, 'eval_steps_per_second': 7.057, 'epoch': 0.32}
{'loss': 1.3723, 'grad_norm': 0.335486501455307, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.5438982248306274, 'eval_runtime': 8.9359, 'eval_samples_per_second': 111.908, 'eval_steps_per_second': 7.05, 'epoch': 0.36}
{'loss': 1.3598, 'grad_norm': 0.33516326546669006, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.5289113521575928, 'eval_runtime': 8.9536, 'eval_samples_per_second': 111.687, 'eval_steps_per_second': 7.036, 'epoch': 0.4}
{'loss': 1.3793, 'grad_norm': 0.28954991698265076, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.5179038047790527, 'eval_runtime': 8.9532, 'eval_samples_per_second': 111.691, 'eval_steps_per_second': 7.037, 'epoch': 0.44}
{'loss': 1.2904, 'grad_norm': 0.2872556149959564, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.498262882232666, 'eval_runtime': 8.9576, 'eval_samples_per_second': 111.637, 'eval_steps_per_second': 7.033, 'epoch': 0.48}
{'loss': 1.3488, 'grad_norm': 0.28173691034317017, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.4915227890014648, 'eval_runtime': 8.9469, 'eval_samples_per_second': 111.771, 'eval_steps_per_second': 7.042, 'epoch': 0.52}
{'loss': 1.3906, 'grad_norm': 0.30703192949295044, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.4695754051208496, 'eval_runtime': 8.967, 'eval_samples_per_second': 111.52, 'eval_steps_per_second': 7.026, 'epoch': 0.56}
{'loss': 1.3807, 'grad_norm': 0.3134571313858032, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.4648710489273071, 'eval_runtime': 8.9397, 'eval_samples_per_second': 111.861, 'eval_steps_per_second': 7.047, 'epoch': 0.6}
{'loss': 1.3191, 'grad_norm': 0.27721378207206726, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.445211410522461, 'eval_runtime': 8.9287, 'eval_samples_per_second': 111.998, 'eval_steps_per_second': 7.056, 'epoch': 0.64}
{'loss': 1.2716, 'grad_norm': 0.29614749550819397, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.4317865371704102, 'eval_runtime': 8.9308, 'eval_samples_per_second': 111.972, 'eval_steps_per_second': 7.054, 'epoch': 0.68}
{'loss': 1.3602, 'grad_norm': 0.26314854621887207, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.424124002456665, 'eval_runtime': 8.9259, 'eval_samples_per_second': 112.033, 'eval_steps_per_second': 7.058, 'epoch': 0.72}
{'loss': 1.319, 'grad_norm': 0.26620692014694214, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.4213752746582031, 'eval_runtime': 8.9134, 'eval_samples_per_second': 112.19, 'eval_steps_per_second': 7.068, 'epoch': 0.76}
{'loss': 1.2583, 'grad_norm': 0.3335011601448059, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.4194214344024658, 'eval_runtime': 8.9134, 'eval_samples_per_second': 112.191, 'eval_steps_per_second': 7.068, 'epoch': 0.81}
{'loss': 1.3191, 'grad_norm': 0.2641487121582031, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.4179584980010986, 'eval_runtime': 8.8679, 'eval_samples_per_second': 112.766, 'eval_steps_per_second': 7.104, 'epoch': 0.85}
{'loss': 1.2532, 'grad_norm': 0.346527099609375, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.4152616262435913, 'eval_runtime': 8.86, 'eval_samples_per_second': 112.867, 'eval_steps_per_second': 7.111, 'epoch': 0.89}
{'loss': 1.2621, 'grad_norm': 0.2953106462955475, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.4144350290298462, 'eval_runtime': 8.8652, 'eval_samples_per_second': 112.8, 'eval_steps_per_second': 7.106, 'epoch': 0.93}
{'loss': 1.2931, 'grad_norm': 0.29003840684890747, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.4127216339111328, 'eval_runtime': 8.8681, 'eval_samples_per_second': 112.764, 'eval_steps_per_second': 7.104, 'epoch': 0.97}
{'train_runtime': 393.5433, 'train_samples_per_second': 25.227, 'train_steps_per_second': 1.578, 'train_loss': 1.533117211383322, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7594776153564453, 1.911974310874939, 1.7714252471923828, 1.6880801916122437, 1.6511919498443604, 1.5953798294067383, 1.5719208717346191, 1.5551459789276123, 1.5438982248306274, 1.5289113521575928, 1.5179038047790527, 1.498262882232666, 1.4915227890014648, 1.4695754051208496, 1.4648710489273071, 1.445211410522461, 1.4317865371704102, 1.424124002456665, 1.4213752746582031, 1.4194214344024658, 1.4179584980010986, 1.4152616262435913, 1.4144350290298462, 1.4127216339111328], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.7594776153564453, 1.911974310874939, 1.7714252471923828, 1.6880801916122437, 1.6511919498443604, 1.5953798294067383, 1.5719208717346191, 1.5551459789276123, 1.5438982248306274, 1.5289113521575928, 1.5179038047790527, 1.498262882232666, 1.4915227890014648, 1.4695754051208496, 1.4648710489273071, 1.445211410522461, 1.4317865371704102, 1.424124002456665, 1.4213752746582031, 1.4194214344024658, 1.4179584980010986, 1.4152616262435913, 1.4144350290298462, 1.4127216339111328]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.4127216339111328
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4175 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3405855894088745, 0.2697674036026001, 0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2527220845222473, 0.2958891987800598, 0.798973798751831, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.42427369952201843, 0.12062281370162964, 0.7909995317459106, 0.5773974061012268]  ‚Üí  acq = -0.842434467859736
X = [0.33804601430892944, 0.07152366638183594, 0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.505533754825592, 0.6193363070487976, 0.5416111350059509, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9287145733833313, 0.6309018731117249, 0.6364489793777466, 0.2499348521232605]  ‚Üí  acq = -0.8374194488559967
X = [0.6847147345542908, 0.7650787830352783, 0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5654566884040833, 0.984917938709259, 0.907849907875061, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4765317142009735, 0.10925418138504028, 0.4907895624637604, 0.022005975246429443]  ‚Üí  acq = -0.8424110370386768
X = [0.15384602546691895, 0.49969446659088135, 0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7945950031280518, 0.5358787775039673, 0.48621666431427, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9266265034675598, 0.8326297998428345, 0.6380935907363892, 0.630294680595398]  ‚Üí  acq = -0.8425120666498083
X = [0.4769304394721985, 0.06750988960266113, 0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6422049403190613, 0.6458637714385986, 0.9798121452331543, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.285779744386673, 0.47408175468444824, 0.7827727794647217, 0.2598608732223511]  ‚Üí  acq = -0.8275287254082304
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1398, dtype=torch.float64), tensor(0.0617, dtype=torch.float64), tensor(0.0745, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(0.2870, dtype=torch.float64), tensor(0.3834, dtype=torch.float64), 14, 0, 0, 0, 0, 1, 87, 3.4694469519536134e-19, 8.072574667720746, 1]
normalized proposed parameters for next round by BO: [tensor(0.0035, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0080, dtype=torch.float64), tensor(0.1398, dtype=torch.float64), tensor(0.0617, dtype=torch.float64), tensor(0.0745, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(0.2870, dtype=torch.float64), tensor(0.3834, dtype=torch.float64), tensor(0.4272, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6787, dtype=torch.float64), tensor(3.4694e-18, dtype=torch.float64), tensor(0.1682, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.14
  triviaqa: 0.062
  truthfulqa_gen: 0.075
  wikitext: 0.042
  mmlu: 0.287
  arc_challenge: 0.383

LoRA Parameters:
  lora_r: (87,)
  lora_dropout: (3.4694469519536134e-19,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (8.072574667720746,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  87
lora dropout:  3.4694469519536134e-19
lora alpha:  8.072574667720746
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 22,450,176 || all params: 8,052,711,424 || trainable%: 0.2788
length of training data:  9883
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8707, 'grad_norm': 0.3885209560394287, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.576023817062378, 'eval_runtime': 8.8841, 'eval_samples_per_second': 112.561, 'eval_steps_per_second': 7.091, 'epoch': 0.04}
{'loss': 2.2308, 'grad_norm': 0.23980076611042023, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.855689287185669, 'eval_runtime': 8.8679, 'eval_samples_per_second': 112.766, 'eval_steps_per_second': 7.104, 'epoch': 0.08}
{'loss': 1.5474, 'grad_norm': 0.1387614905834198, 'learning_rate': 0.0002873239436619718, 'epoch': 0.12}
{'eval_loss': 1.7096672058105469, 'eval_runtime': 8.893, 'eval_samples_per_second': 112.448, 'eval_steps_per_second': 7.084, 'epoch': 0.12}
{'loss': 1.4896, 'grad_norm': 0.12155000120401382, 'learning_rate': 0.0002741197183098591, 'epoch': 0.16}
{'eval_loss': 1.6515836715698242, 'eval_runtime': 8.9006, 'eval_samples_per_second': 112.352, 'eval_steps_per_second': 7.078, 'epoch': 0.16}
{'loss': 1.4016, 'grad_norm': 0.12345044314861298, 'learning_rate': 0.0002609154929577464, 'epoch': 0.2}
{'eval_loss': 1.5749646425247192, 'eval_runtime': 8.9166, 'eval_samples_per_second': 112.15, 'eval_steps_per_second': 7.065, 'epoch': 0.2}
{'loss': 1.3267, 'grad_norm': 0.10064050555229187, 'learning_rate': 0.0002477112676056338, 'epoch': 0.24}
{'eval_loss': 1.5227620601654053, 'eval_runtime': 8.9389, 'eval_samples_per_second': 111.87, 'eval_steps_per_second': 7.048, 'epoch': 0.24}
{'loss': 1.3474, 'grad_norm': 0.11815071851015091, 'learning_rate': 0.00023450704225352109, 'epoch': 0.28}
{'eval_loss': 1.5018291473388672, 'eval_runtime': 8.9686, 'eval_samples_per_second': 111.5, 'eval_steps_per_second': 7.025, 'epoch': 0.28}
{'loss': 1.2967, 'grad_norm': 0.1081119105219841, 'learning_rate': 0.00022130281690140843, 'epoch': 0.32}
{'eval_loss': 1.4618737697601318, 'eval_runtime': 8.9767, 'eval_samples_per_second': 111.4, 'eval_steps_per_second': 7.018, 'epoch': 0.32}
{'loss': 1.2914, 'grad_norm': 0.15357176959514618, 'learning_rate': 0.00020809859154929575, 'epoch': 0.36}
{'eval_loss': 1.4187366962432861, 'eval_runtime': 9.0066, 'eval_samples_per_second': 111.03, 'eval_steps_per_second': 6.995, 'epoch': 0.36}
{'loss': 1.2056, 'grad_norm': 0.10285737365484238, 'learning_rate': 0.00019489436619718307, 'epoch': 0.4}
{'eval_loss': 1.399458885192871, 'eval_runtime': 9.0393, 'eval_samples_per_second': 110.627, 'eval_steps_per_second': 6.97, 'epoch': 0.4}
{'loss': 1.2685, 'grad_norm': 0.12126803398132324, 'learning_rate': 0.0001816901408450704, 'epoch': 0.44}
{'eval_loss': 1.3832052946090698, 'eval_runtime': 8.9991, 'eval_samples_per_second': 111.122, 'eval_steps_per_second': 7.001, 'epoch': 0.44}
{'loss': 1.1818, 'grad_norm': 0.16438992321491241, 'learning_rate': 0.0001684859154929577, 'epoch': 0.49}
{'eval_loss': 1.3723676204681396, 'eval_runtime': 9.0083, 'eval_samples_per_second': 111.009, 'eval_steps_per_second': 6.994, 'epoch': 0.49}
{'loss': 1.1426, 'grad_norm': 0.15121057629585266, 'learning_rate': 0.00015528169014084506, 'epoch': 0.53}
{'eval_loss': 1.3615212440490723, 'eval_runtime': 9.0121, 'eval_samples_per_second': 110.962, 'eval_steps_per_second': 6.991, 'epoch': 0.53}
{'loss': 1.2108, 'grad_norm': 0.15418080985546112, 'learning_rate': 0.00014207746478873238, 'epoch': 0.57}
{'eval_loss': 1.354834794998169, 'eval_runtime': 9.0112, 'eval_samples_per_second': 110.974, 'eval_steps_per_second': 6.991, 'epoch': 0.57}
{'loss': 1.129, 'grad_norm': 0.10979674756526947, 'learning_rate': 0.0001288732394366197, 'epoch': 0.61}
{'eval_loss': 1.3488301038742065, 'eval_runtime': 9.0355, 'eval_samples_per_second': 110.674, 'eval_steps_per_second': 6.972, 'epoch': 0.61}
{'loss': 1.143, 'grad_norm': 0.12820008397102356, 'learning_rate': 0.00011566901408450703, 'epoch': 0.65}
{'eval_loss': 1.3478060960769653, 'eval_runtime': 9.0027, 'eval_samples_per_second': 111.078, 'eval_steps_per_second': 6.998, 'epoch': 0.65}
{'loss': 1.1254, 'grad_norm': 0.13096442818641663, 'learning_rate': 0.00010246478873239435, 'epoch': 0.69}
{'eval_loss': 1.3448294401168823, 'eval_runtime': 9.0075, 'eval_samples_per_second': 111.019, 'eval_steps_per_second': 6.994, 'epoch': 0.69}
{'loss': 1.1466, 'grad_norm': 0.12066734582185745, 'learning_rate': 8.926056338028169e-05, 'epoch': 0.73}
{'eval_loss': 1.3440049886703491, 'eval_runtime': 9.0261, 'eval_samples_per_second': 110.79, 'eval_steps_per_second': 6.98, 'epoch': 0.73}
{'loss': 1.1171, 'grad_norm': 0.12308046966791153, 'learning_rate': 7.6056338028169e-05, 'epoch': 0.77}
{'eval_loss': 1.3421374559402466, 'eval_runtime': 9.0154, 'eval_samples_per_second': 110.922, 'eval_steps_per_second': 6.988, 'epoch': 0.77}
{'loss': 1.1397, 'grad_norm': 0.11640407890081406, 'learning_rate': 6.285211267605634e-05, 'epoch': 0.81}
{'eval_loss': 1.3398959636688232, 'eval_runtime': 9.0247, 'eval_samples_per_second': 110.807, 'eval_steps_per_second': 6.981, 'epoch': 0.81}
{'loss': 1.1464, 'grad_norm': 0.10506096482276917, 'learning_rate': 4.964788732394366e-05, 'epoch': 0.85}
{'eval_loss': 1.3397996425628662, 'eval_runtime': 9.0049, 'eval_samples_per_second': 111.05, 'eval_steps_per_second': 6.996, 'epoch': 0.85}
{'loss': 1.1218, 'grad_norm': 0.12135610729455948, 'learning_rate': 3.6443661971830985e-05, 'epoch': 0.89}
{'eval_loss': 1.337839126586914, 'eval_runtime': 9.0085, 'eval_samples_per_second': 111.006, 'eval_steps_per_second': 6.993, 'epoch': 0.89}
{'loss': 1.1643, 'grad_norm': 0.14093773066997528, 'learning_rate': 2.3239436619718305e-05, 'epoch': 0.93}
{'eval_loss': 1.3369100093841553, 'eval_runtime': 8.9897, 'eval_samples_per_second': 111.239, 'eval_steps_per_second': 7.008, 'epoch': 0.93}
{'loss': 1.1247, 'grad_norm': 0.10555660724639893, 'learning_rate': 1.0035211267605631e-05, 'epoch': 0.97}
{'eval_loss': 1.3369179964065552, 'eval_runtime': 8.9892, 'eval_samples_per_second': 111.245, 'eval_steps_per_second': 7.008, 'epoch': 0.97}
{'train_runtime': 386.2709, 'train_samples_per_second': 25.586, 'train_steps_per_second': 1.6, 'train_loss': 1.3747947131160008, 'epoch': 1.0}
train_results:  {'eval_loss': [2.576023817062378, 1.855689287185669, 1.7096672058105469, 1.6515836715698242, 1.5749646425247192, 1.5227620601654053, 1.5018291473388672, 1.4618737697601318, 1.4187366962432861, 1.399458885192871, 1.3832052946090698, 1.3723676204681396, 1.3615212440490723, 1.354834794998169, 1.3488301038742065, 1.3478060960769653, 1.3448294401168823, 1.3440049886703491, 1.3421374559402466, 1.3398959636688232, 1.3397996425628662, 1.337839126586914, 1.3369100093841553, 1.3369179964065552], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.576023817062378, 1.855689287185669, 1.7096672058105469, 1.6515836715698242, 1.5749646425247192, 1.5227620601654053, 1.5018291473388672, 1.4618737697601318, 1.4187366962432861, 1.399458885192871, 1.3832052946090698, 1.3723676204681396, 1.3615212440490723, 1.354834794998169, 1.3488301038742065, 1.3478060960769653, 1.3448294401168823, 1.3440049886703491, 1.3421374559402466, 1.3398959636688232, 1.3397996425628662, 1.337839126586914, 1.3369100093841553, 1.3369179964065552]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3369179964065552
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5211 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1795889139175415, 0.5639668703079224, 0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.6092051267623901, 0.8282220363616943, 0.8343875408172607, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.22647850215435028, 0.034817516803741455, 0.2369377166032791, 0.8301550149917603]  ‚Üí  acq = -0.9273299498677561
X = [0.3627225160598755, 0.4275026321411133, 0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.20957553386688232, 0.29544466733932495, 0.45958811044692993, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7996509075164795, 0.16252559423446655, 0.23583632707595825, 0.46233898401260376]  ‚Üí  acq = -0.9287099622733348
X = [0.3497363328933716, 0.9076562523841858, 0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.451727032661438, 0.6221595406532288, 0.021270394325256348, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.990592360496521, 0.6705264449119568, 0.5292245149612427, 0.4145408272743225]  ‚Üí  acq = -0.9247035206112764
X = [0.8545095920562744, 0.7290428876876831, 0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.1896294802427292, 0.13708847761154175, 0.2295888066291809, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.42133286595344543, 0.9889391660690308, 0.27955883741378784, 0.647262454032898]  ‚Üí  acq = -0.9267184963096651
X = [0.25802141427993774, 0.15090978145599365, 0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.16085723042488098, 0.13615381717681885, 0.3176853060722351, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7902160882949829, 0.012897372245788574, 0.5332701206207275, 0.154333233833313]  ‚Üí  acq = -0.9271582891680499
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.2185, dtype=torch.float64), 0, tensor(0.2500, dtype=torch.float64), 0, tensor(0.0589, dtype=torch.float64), tensor(0.4726, dtype=torch.float64), 24, 0, 1, 0, 1, 0, 112, 0.06807334599599184, 14.94814645888734, 0]
normalized proposed parameters for next round by BO: [tensor(1.2919e-17, dtype=torch.float64), tensor(7.3509e-17, dtype=torch.float64), tensor(1.0576e-17, dtype=torch.float64), tensor(0.2185, dtype=torch.float64), tensor(1.7598e-17, dtype=torch.float64), tensor(0.2500, dtype=torch.float64), tensor(1.1873e-17, dtype=torch.float64), tensor(0.0589, dtype=torch.float64), tensor(0.4726, dtype=torch.float64), tensor(0.7582, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8789, dtype=torch.float64), tensor(0.6807, dtype=torch.float64), tensor(0.3114, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.219
  triviaqa: 0
  truthfulqa_gen: 0.25
  wikitext: 0
  mmlu: 0.059
  arc_challenge: 0.473

LoRA Parameters:
  lora_r: (112,)
  lora_dropout: (0.06807334599599184,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (14.94814645888734,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  112
lora dropout:  0.06807334599599184
lora alpha:  14.94814645888734
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 63,307,776 || all params: 8,093,569,024 || trainable%: 0.7822
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6442, 'grad_norm': 0.7230589985847473, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.025118350982666, 'eval_runtime': 9.2517, 'eval_samples_per_second': 108.088, 'eval_steps_per_second': 6.81, 'epoch': 0.04}
{'loss': 1.2589, 'grad_norm': 0.27327272295951843, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5659960508346558, 'eval_runtime': 9.2915, 'eval_samples_per_second': 107.625, 'eval_steps_per_second': 6.78, 'epoch': 0.08}
{'loss': 0.9253, 'grad_norm': 0.17340247333049774, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.508648157119751, 'eval_runtime': 9.3317, 'eval_samples_per_second': 107.161, 'eval_steps_per_second': 6.751, 'epoch': 0.12}
{'loss': 0.9183, 'grad_norm': 0.1577281653881073, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4766877889633179, 'eval_runtime': 9.3453, 'eval_samples_per_second': 107.006, 'eval_steps_per_second': 6.741, 'epoch': 0.16}
{'loss': 0.9064, 'grad_norm': 0.14953011274337769, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4514578580856323, 'eval_runtime': 9.3513, 'eval_samples_per_second': 106.937, 'eval_steps_per_second': 6.737, 'epoch': 0.2}
{'loss': 0.8933, 'grad_norm': 0.17096315324306488, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4417003393173218, 'eval_runtime': 9.3546, 'eval_samples_per_second': 106.899, 'eval_steps_per_second': 6.735, 'epoch': 0.24}
{'loss': 0.8866, 'grad_norm': 0.18376067280769348, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4425612688064575, 'eval_runtime': 9.4119, 'eval_samples_per_second': 106.249, 'eval_steps_per_second': 6.694, 'epoch': 0.28}
{'loss': 0.8703, 'grad_norm': 0.15037071704864502, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4207819700241089, 'eval_runtime': 9.3855, 'eval_samples_per_second': 106.547, 'eval_steps_per_second': 6.712, 'epoch': 0.32}
{'loss': 0.8591, 'grad_norm': 0.17624430358409882, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4011497497558594, 'eval_runtime': 9.3449, 'eval_samples_per_second': 107.011, 'eval_steps_per_second': 6.742, 'epoch': 0.36}
{'loss': 0.8416, 'grad_norm': 0.17898663878440857, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4195635318756104, 'eval_runtime': 9.3446, 'eval_samples_per_second': 107.013, 'eval_steps_per_second': 6.742, 'epoch': 0.4}
{'loss': 0.8257, 'grad_norm': 0.18849556148052216, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4120451211929321, 'eval_runtime': 9.3531, 'eval_samples_per_second': 106.916, 'eval_steps_per_second': 6.736, 'epoch': 0.44}
{'loss': 0.829, 'grad_norm': 0.17970426380634308, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3843095302581787, 'eval_runtime': 9.3658, 'eval_samples_per_second': 106.772, 'eval_steps_per_second': 6.727, 'epoch': 0.48}
{'loss': 0.8248, 'grad_norm': 0.18220607936382294, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3671237230300903, 'eval_runtime': 9.3608, 'eval_samples_per_second': 106.828, 'eval_steps_per_second': 6.73, 'epoch': 0.52}
{'loss': 0.7976, 'grad_norm': 0.18164367973804474, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3683584928512573, 'eval_runtime': 9.3061, 'eval_samples_per_second': 107.456, 'eval_steps_per_second': 6.77, 'epoch': 0.56}
{'loss': 0.8255, 'grad_norm': 0.18692833185195923, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3517619371414185, 'eval_runtime': 9.3681, 'eval_samples_per_second': 106.746, 'eval_steps_per_second': 6.725, 'epoch': 0.6}
{'loss': 0.8194, 'grad_norm': 0.19775089621543884, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.346315622329712, 'eval_runtime': 9.3767, 'eval_samples_per_second': 106.648, 'eval_steps_per_second': 6.719, 'epoch': 0.64}
{'loss': 0.835, 'grad_norm': 0.20712904632091522, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3532238006591797, 'eval_runtime': 9.3729, 'eval_samples_per_second': 106.691, 'eval_steps_per_second': 6.722, 'epoch': 0.68}
{'loss': 0.8039, 'grad_norm': 0.19919775426387787, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.342703104019165, 'eval_runtime': 9.3823, 'eval_samples_per_second': 106.583, 'eval_steps_per_second': 6.715, 'epoch': 0.72}
{'loss': 0.7861, 'grad_norm': 0.2156275361776352, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.333183765411377, 'eval_runtime': 9.3733, 'eval_samples_per_second': 106.686, 'eval_steps_per_second': 6.721, 'epoch': 0.76}
{'loss': 0.7685, 'grad_norm': 0.21633224189281464, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3325248956680298, 'eval_runtime': 9.3764, 'eval_samples_per_second': 106.65, 'eval_steps_per_second': 6.719, 'epoch': 0.8}
{'loss': 0.7664, 'grad_norm': 0.20956569910049438, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3282641172409058, 'eval_runtime': 9.3825, 'eval_samples_per_second': 106.582, 'eval_steps_per_second': 6.715, 'epoch': 0.84}
{'loss': 0.7468, 'grad_norm': 0.22913803160190582, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3290164470672607, 'eval_runtime': 9.3731, 'eval_samples_per_second': 106.688, 'eval_steps_per_second': 6.721, 'epoch': 0.88}
{'loss': 0.8, 'grad_norm': 0.24204851686954498, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.323976993560791, 'eval_runtime': 9.372, 'eval_samples_per_second': 106.701, 'eval_steps_per_second': 6.722, 'epoch': 0.92}
{'loss': 0.7851, 'grad_norm': 0.24959023296833038, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.324552059173584, 'eval_runtime': 9.3651, 'eval_samples_per_second': 106.78, 'eval_steps_per_second': 6.727, 'epoch': 0.96}
{'loss': 0.7888, 'grad_norm': 0.26952287554740906, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3247251510620117, 'eval_runtime': 9.3634, 'eval_samples_per_second': 106.799, 'eval_steps_per_second': 6.728, 'epoch': 1.0}
{'train_runtime': 374.1049, 'train_samples_per_second': 26.725, 'train_steps_per_second': 1.671, 'train_loss': 0.9602727966308594, 'epoch': 1.0}
train_results:  {'eval_loss': [2.025118350982666, 1.5659960508346558, 1.508648157119751, 1.4766877889633179, 1.4514578580856323, 1.4417003393173218, 1.4425612688064575, 1.4207819700241089, 1.4011497497558594, 1.4195635318756104, 1.4120451211929321, 1.3843095302581787, 1.3671237230300903, 1.3683584928512573, 1.3517619371414185, 1.346315622329712, 1.3532238006591797, 1.342703104019165, 1.333183765411377, 1.3325248956680298, 1.3282641172409058, 1.3290164470672607, 1.323976993560791, 1.324552059173584, 1.3247251510620117], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.025118350982666, 1.5659960508346558, 1.508648157119751, 1.4766877889633179, 1.4514578580856323, 1.4417003393173218, 1.4425612688064575, 1.4207819700241089, 1.4011497497558594, 1.4195635318756104, 1.4120451211929321, 1.3843095302581787, 1.3671237230300903, 1.3683584928512573, 1.3517619371414185, 1.346315622329712, 1.3532238006591797, 1.342703104019165, 1.333183765411377, 1.3325248956680298, 1.3282641172409058, 1.3290164470672607, 1.323976993560791, 1.324552059173584, 1.3247251510620117]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3247251510620117
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1876 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -0.9673452020570255
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -0.9602411664655826
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -0.9604652687139784
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.9396638427399844
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -0.9663354226565379
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0174, dtype=torch.float64), tensor(0.1844, dtype=torch.float64), 0, tensor(0.1073, dtype=torch.float64), 0, tensor(0.0302, dtype=torch.float64), tensor(0.0151, dtype=torch.float64), 0, tensor(0.6456, dtype=torch.float64), 26, 0, 0, 1, 1, 1, 53, 0.0, 26.28572216499061, 0]
normalized proposed parameters for next round by BO: [tensor(0.0174, dtype=torch.float64), tensor(0.1844, dtype=torch.float64), tensor(7.9647e-18, dtype=torch.float64), tensor(0.1073, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0302, dtype=torch.float64), tensor(0.0151, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6456, dtype=torch.float64), tensor(0.8179, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4153, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5476, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.017
  gsm8k: 0.184
  rowan_hellaswag: 0
  sciq: 0.107
  triviaqa: 0
  truthfulqa_gen: 0.03
  wikitext: 0.015
  mmlu: 0
  arc_challenge: 0.646

LoRA Parameters:
  lora_r: (53,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (26.28572216499061,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  53
lora dropout:  0.0
lora alpha:  26.28572216499061
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 76,197,888 || all params: 8,106,459,136 || trainable%: 0.9400
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5912, 'grad_norm': 0.873375415802002, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.957175850868225, 'eval_runtime': 10.5511, 'eval_samples_per_second': 94.777, 'eval_steps_per_second': 5.971, 'epoch': 0.04}
{'loss': 1.0294, 'grad_norm': 0.36040106415748596, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7904555797576904, 'eval_runtime': 10.5771, 'eval_samples_per_second': 94.544, 'eval_steps_per_second': 5.956, 'epoch': 0.08}
{'loss': 0.8636, 'grad_norm': 0.2394922524690628, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8152449131011963, 'eval_runtime': 10.5968, 'eval_samples_per_second': 94.368, 'eval_steps_per_second': 5.945, 'epoch': 0.12}
{'loss': 0.8339, 'grad_norm': 0.21251656115055084, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8487508296966553, 'eval_runtime': 10.6147, 'eval_samples_per_second': 94.209, 'eval_steps_per_second': 5.935, 'epoch': 0.16}
{'loss': 0.7964, 'grad_norm': 0.25915101170539856, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8393017053604126, 'eval_runtime': 10.6228, 'eval_samples_per_second': 94.137, 'eval_steps_per_second': 5.931, 'epoch': 0.2}
{'loss': 0.7716, 'grad_norm': 0.26764294505119324, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8415417671203613, 'eval_runtime': 10.6316, 'eval_samples_per_second': 94.06, 'eval_steps_per_second': 5.926, 'epoch': 0.24}
{'loss': 0.785, 'grad_norm': 0.25898173451423645, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.897768259048462, 'eval_runtime': 10.6401, 'eval_samples_per_second': 93.984, 'eval_steps_per_second': 5.921, 'epoch': 0.28}
{'loss': 0.7092, 'grad_norm': 0.3483045995235443, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9206693172454834, 'eval_runtime': 10.6397, 'eval_samples_per_second': 93.988, 'eval_steps_per_second': 5.921, 'epoch': 0.32}
{'loss': 0.6863, 'grad_norm': 0.3766590356826782, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9379531145095825, 'eval_runtime': 10.6393, 'eval_samples_per_second': 93.991, 'eval_steps_per_second': 5.921, 'epoch': 0.36}
{'loss': 0.6647, 'grad_norm': 0.31054267287254333, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8986525535583496, 'eval_runtime': 10.642, 'eval_samples_per_second': 93.968, 'eval_steps_per_second': 5.92, 'epoch': 0.4}
{'loss': 0.6258, 'grad_norm': 0.4318868815898895, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9652190208435059, 'eval_runtime': 10.6489, 'eval_samples_per_second': 93.906, 'eval_steps_per_second': 5.916, 'epoch': 0.44}
{'loss': 0.6082, 'grad_norm': 0.36621764302253723, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9747307300567627, 'eval_runtime': 10.6571, 'eval_samples_per_second': 93.834, 'eval_steps_per_second': 5.912, 'epoch': 0.48}
{'loss': 0.5983, 'grad_norm': 0.29005497694015503, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9771660566329956, 'eval_runtime': 10.6666, 'eval_samples_per_second': 93.751, 'eval_steps_per_second': 5.906, 'epoch': 0.52}
{'loss': 0.5641, 'grad_norm': 0.41265591979026794, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.929193377494812, 'eval_runtime': 10.7416, 'eval_samples_per_second': 93.096, 'eval_steps_per_second': 5.865, 'epoch': 0.56}
{'loss': 0.5641, 'grad_norm': 0.3574737310409546, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9921153783798218, 'eval_runtime': 10.7217, 'eval_samples_per_second': 93.269, 'eval_steps_per_second': 5.876, 'epoch': 0.6}
{'loss': 0.5239, 'grad_norm': 0.3889623284339905, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.0194358825683594, 'eval_runtime': 10.7233, 'eval_samples_per_second': 93.255, 'eval_steps_per_second': 5.875, 'epoch': 0.64}
{'loss': 0.5006, 'grad_norm': 0.31319165229797363, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9872158765792847, 'eval_runtime': 10.7361, 'eval_samples_per_second': 93.143, 'eval_steps_per_second': 5.868, 'epoch': 0.68}
{'loss': 0.4847, 'grad_norm': 0.40157538652420044, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.1054375171661377, 'eval_runtime': 10.6866, 'eval_samples_per_second': 93.575, 'eval_steps_per_second': 5.895, 'epoch': 0.72}
{'loss': 0.4514, 'grad_norm': 0.4302561283111572, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.031536102294922, 'eval_runtime': 10.6949, 'eval_samples_per_second': 93.503, 'eval_steps_per_second': 5.891, 'epoch': 0.76}
{'loss': 0.4369, 'grad_norm': 0.44388025999069214, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.1209311485290527, 'eval_runtime': 10.6867, 'eval_samples_per_second': 93.574, 'eval_steps_per_second': 5.895, 'epoch': 0.8}
{'loss': 0.4358, 'grad_norm': 0.5298389792442322, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.149207830429077, 'eval_runtime': 10.6872, 'eval_samples_per_second': 93.57, 'eval_steps_per_second': 5.895, 'epoch': 0.84}
{'loss': 0.4217, 'grad_norm': 0.40117907524108887, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.177765130996704, 'eval_runtime': 10.6795, 'eval_samples_per_second': 93.637, 'eval_steps_per_second': 5.899, 'epoch': 0.88}
{'loss': 0.3957, 'grad_norm': 0.2900409400463104, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1814122200012207, 'eval_runtime': 10.7168, 'eval_samples_per_second': 93.312, 'eval_steps_per_second': 5.879, 'epoch': 0.92}
{'loss': 0.419, 'grad_norm': 0.28411105275154114, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1750638484954834, 'eval_runtime': 10.7195, 'eval_samples_per_second': 93.288, 'eval_steps_per_second': 5.877, 'epoch': 0.96}
{'loss': 0.4254, 'grad_norm': 0.2879466116428375, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.186971426010132, 'eval_runtime': 10.693, 'eval_samples_per_second': 93.519, 'eval_steps_per_second': 5.892, 'epoch': 1.0}
{'train_runtime': 472.0347, 'train_samples_per_second': 21.176, 'train_steps_per_second': 1.324, 'train_loss': 0.6874815689086914, 'epoch': 1.0}
train_results:  {'eval_loss': [1.957175850868225, 1.7904555797576904, 1.8152449131011963, 1.8487508296966553, 1.8393017053604126, 1.8415417671203613, 1.897768259048462, 1.9206693172454834, 1.9379531145095825, 1.8986525535583496, 1.9652190208435059, 1.9747307300567627, 1.9771660566329956, 1.929193377494812, 1.9921153783798218, 2.0194358825683594, 1.9872158765792847, 2.1054375171661377, 2.031536102294922, 2.1209311485290527, 2.149207830429077, 2.177765130996704, 2.1814122200012207, 2.1750638484954834, 2.186971426010132], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.957175850868225, 1.7904555797576904, 1.8152449131011963, 1.8487508296966553, 1.8393017053604126, 1.8415417671203613, 1.897768259048462, 1.9206693172454834, 1.9379531145095825, 1.8986525535583496, 1.9652190208435059, 1.9747307300567627, 1.9771660566329956, 1.929193377494812, 1.9921153783798218, 2.0194358825683594, 1.9872158765792847, 2.1054375171661377, 2.031536102294922, 2.1209311485290527, 2.149207830429077, 2.177765130996704, 2.1814122200012207, 2.1750638484954834, 2.186971426010132]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.186971426010132
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.0343 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.8741994099159385
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.848055932568891
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.8630664504827427
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.8736805508547683
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.8742432368498607
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0248, dtype=torch.float64), 0, 0, tensor(0.1465, dtype=torch.float64), tensor(0.0378, dtype=torch.float64), tensor(0.0313, dtype=torch.float64), tensor(0.0288, dtype=torch.float64), tensor(0.2812, dtype=torch.float64), tensor(0.4432, dtype=torch.float64), 14, 0, 1, 0, 1, 1, 96, 0.0, 8.154815143544639, 0]
normalized proposed parameters for next round by BO: [tensor(0.0248, dtype=torch.float64), tensor(2.9178e-18, dtype=torch.float64), tensor(0.0064, dtype=torch.float64), tensor(0.1465, dtype=torch.float64), tensor(0.0378, dtype=torch.float64), tensor(0.0313, dtype=torch.float64), tensor(0.0288, dtype=torch.float64), tensor(0.2812, dtype=torch.float64), tensor(0.4432, dtype=torch.float64), tensor(0.4271, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7499, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1699, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.025
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.146
  triviaqa: 0.038
  truthfulqa_gen: 0.031
  wikitext: 0.029
  mmlu: 0.281
  arc_challenge: 0.443

LoRA Parameters:
  lora_r: (96,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (8.154815143544639,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  96
lora dropout:  0.0
lora alpha:  8.154815143544639
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 56,426,496 || all params: 8,086,687,744 || trainable%: 0.6978
length of training data:  9930
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6563, 'grad_norm': 0.5686875581741333, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.153384208679199, 'eval_runtime': 9.2039, 'eval_samples_per_second': 108.65, 'eval_steps_per_second': 6.845, 'epoch': 0.04}
{'loss': 1.729, 'grad_norm': 0.21356233954429626, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5869089365005493, 'eval_runtime': 9.229, 'eval_samples_per_second': 108.354, 'eval_steps_per_second': 6.826, 'epoch': 0.08}
{'loss': 1.1896, 'grad_norm': 0.10962959378957748, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.4543291330337524, 'eval_runtime': 9.2645, 'eval_samples_per_second': 107.939, 'eval_steps_per_second': 6.8, 'epoch': 0.12}
{'loss': 1.1617, 'grad_norm': 0.09229007363319397, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.4189523458480835, 'eval_runtime': 9.295, 'eval_samples_per_second': 107.585, 'eval_steps_per_second': 6.778, 'epoch': 0.16}
{'loss': 1.1899, 'grad_norm': 0.1024797335267067, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.3905041217803955, 'eval_runtime': 9.3088, 'eval_samples_per_second': 107.426, 'eval_steps_per_second': 6.768, 'epoch': 0.2}
{'loss': 1.1266, 'grad_norm': 0.10443875938653946, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.3546407222747803, 'eval_runtime': 9.3174, 'eval_samples_per_second': 107.326, 'eval_steps_per_second': 6.762, 'epoch': 0.24}
{'loss': 1.0718, 'grad_norm': 0.09723503887653351, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.348232388496399, 'eval_runtime': 9.3187, 'eval_samples_per_second': 107.311, 'eval_steps_per_second': 6.761, 'epoch': 0.28}
{'loss': 1.0451, 'grad_norm': 0.11314205825328827, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.3420772552490234, 'eval_runtime': 9.2972, 'eval_samples_per_second': 107.559, 'eval_steps_per_second': 6.776, 'epoch': 0.32}
{'loss': 1.0866, 'grad_norm': 0.10548321157693863, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.3301212787628174, 'eval_runtime': 9.3044, 'eval_samples_per_second': 107.476, 'eval_steps_per_second': 6.771, 'epoch': 0.36}
{'loss': 1.0364, 'grad_norm': 0.11900777369737625, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.3274548053741455, 'eval_runtime': 9.3492, 'eval_samples_per_second': 106.961, 'eval_steps_per_second': 6.739, 'epoch': 0.4}
{'loss': 1.0432, 'grad_norm': 0.1247110441327095, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.3301957845687866, 'eval_runtime': 9.335, 'eval_samples_per_second': 107.124, 'eval_steps_per_second': 6.749, 'epoch': 0.44}
{'loss': 1.0587, 'grad_norm': 0.12716691195964813, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.3293673992156982, 'eval_runtime': 9.3254, 'eval_samples_per_second': 107.234, 'eval_steps_per_second': 6.756, 'epoch': 0.48}
{'loss': 1.0244, 'grad_norm': 0.14345331490039825, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.3246831893920898, 'eval_runtime': 9.3596, 'eval_samples_per_second': 106.842, 'eval_steps_per_second': 6.731, 'epoch': 0.52}
{'loss': 1.0083, 'grad_norm': 0.12478707730770111, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.3295661211013794, 'eval_runtime': 9.3613, 'eval_samples_per_second': 106.823, 'eval_steps_per_second': 6.73, 'epoch': 0.56}
{'loss': 0.9976, 'grad_norm': 0.13023632764816284, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.3268595933914185, 'eval_runtime': 9.3346, 'eval_samples_per_second': 107.128, 'eval_steps_per_second': 6.749, 'epoch': 0.6}
{'loss': 1.0337, 'grad_norm': 0.1417524516582489, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.32375168800354, 'eval_runtime': 9.3328, 'eval_samples_per_second': 107.149, 'eval_steps_per_second': 6.75, 'epoch': 0.64}
{'loss': 0.994, 'grad_norm': 0.1377246230840683, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.3250089883804321, 'eval_runtime': 9.338, 'eval_samples_per_second': 107.089, 'eval_steps_per_second': 6.747, 'epoch': 0.68}
{'loss': 1.0032, 'grad_norm': 0.17676956951618195, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.3244682550430298, 'eval_runtime': 9.3466, 'eval_samples_per_second': 106.991, 'eval_steps_per_second': 6.74, 'epoch': 0.72}
{'loss': 1.0063, 'grad_norm': 0.1616060584783554, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.3234418630599976, 'eval_runtime': 9.343, 'eval_samples_per_second': 107.032, 'eval_steps_per_second': 6.743, 'epoch': 0.76}
{'loss': 0.9816, 'grad_norm': 0.16956114768981934, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.3238646984100342, 'eval_runtime': 9.3729, 'eval_samples_per_second': 106.69, 'eval_steps_per_second': 6.721, 'epoch': 0.81}
{'loss': 0.9937, 'grad_norm': 0.17406725883483887, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.321885108947754, 'eval_runtime': 9.3653, 'eval_samples_per_second': 106.777, 'eval_steps_per_second': 6.727, 'epoch': 0.85}
{'loss': 1.0052, 'grad_norm': 0.15849584341049194, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.322830080986023, 'eval_runtime': 9.3487, 'eval_samples_per_second': 106.967, 'eval_steps_per_second': 6.739, 'epoch': 0.89}
{'loss': 1.0419, 'grad_norm': 0.1585891991853714, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.3206015825271606, 'eval_runtime': 9.3475, 'eval_samples_per_second': 106.98, 'eval_steps_per_second': 6.74, 'epoch': 0.93}
{'loss': 0.936, 'grad_norm': 0.20349334180355072, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.3225841522216797, 'eval_runtime': 9.3347, 'eval_samples_per_second': 107.127, 'eval_steps_per_second': 6.749, 'epoch': 0.97}
{'train_runtime': 355.0199, 'train_samples_per_second': 27.97, 'train_steps_per_second': 1.749, 'train_loss': 1.1786745750193819, 'epoch': 1.0}
train_results:  {'eval_loss': [2.153384208679199, 1.5869089365005493, 1.4543291330337524, 1.4189523458480835, 1.3905041217803955, 1.3546407222747803, 1.348232388496399, 1.3420772552490234, 1.3301212787628174, 1.3274548053741455, 1.3301957845687866, 1.3293673992156982, 1.3246831893920898, 1.3295661211013794, 1.3268595933914185, 1.32375168800354, 1.3250089883804321, 1.3244682550430298, 1.3234418630599976, 1.3238646984100342, 1.321885108947754, 1.322830080986023, 1.3206015825271606, 1.3225841522216797], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.153384208679199, 1.5869089365005493, 1.4543291330337524, 1.4189523458480835, 1.3905041217803955, 1.3546407222747803, 1.348232388496399, 1.3420772552490234, 1.3301212787628174, 1.3274548053741455, 1.3301957845687866, 1.3293673992156982, 1.3246831893920898, 1.3295661211013794, 1.3268595933914185, 1.32375168800354, 1.3250089883804321, 1.3244682550430298, 1.3234418630599976, 1.3238646984100342, 1.321885108947754, 1.322830080986023, 1.3206015825271606, 1.3225841522216797]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3225841522216797
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3754 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4017324447631836, 0.8098095655441284, 0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.9023115634918213, 0.3290713429450989, 0.8067486882209778, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.7156268954277039, 0.5243701934814453, 0.35216549038887024, 0.006528973579406738]  ‚Üí  acq = -0.8713404549816588
X = [0.083668053150177, 0.21730327606201172, 0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4629462659358978, 0.49664074182510376, 0.07627946138381958, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.09527727216482162, 0.25031810998916626, 0.8219367265701294, 0.7197057604789734]  ‚Üí  acq = -0.8719294831030188
X = [0.2491104006767273, 0.20842111110687256, 0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.4023188650608063, 0.07552939653396606, 0.27186697721481323, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.9325355887413025, 0.1334356665611267, 0.16606317460536957, 0.5709797143936157]  ‚Üí  acq = -0.8712993162328524
X = [0.6106637716293335, 0.9664523601531982, 0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.13759151101112366, 0.28943711519241333, 0.03323030471801758, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8348419070243835, 0.620255172252655, 0.1275952160358429, 0.9232467412948608]  ‚Üí  acq = -0.8751670638260696
X = [0.332327663898468, 0.6604408025741577, 0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6904752850532532, 0.5186182260513306, 0.2655754089355469, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.19332276284694672, 0.0007928609848022461, 0.8442171812057495, 0.3439427614212036]  ‚Üí  acq = -0.8719294945889244
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.0295, dtype=torch.float64), 0, tensor(0.1911, dtype=torch.float64), tensor(0.1573, dtype=torch.float64), 0, tensor(0.6222, dtype=torch.float64), 11, 1, 1, 0, 0, 0, 86, 1.3010426069826058e-19, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(7.1946e-17, dtype=torch.float64), tensor(1.8597e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0295, dtype=torch.float64), tensor(1.5711e-17, dtype=torch.float64), tensor(0.1911, dtype=torch.float64), tensor(0.1573, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6222, dtype=torch.float64), tensor(0.3502, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6701, dtype=torch.float64), tensor(1.3010e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.029
  triviaqa: 0
  truthfulqa_gen: 0.191
  wikitext: 0.157
  mmlu: 0
  arc_challenge: 0.622

LoRA Parameters:
  lora_r: (86,)
  lora_dropout: (1.3010426069826058e-19,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([1, 1, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 0]
lora rank:  86
lora dropout:  1.3010426069826058e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 12,593,152 || all params: 8,042,854,400 || trainable%: 0.1566
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4858, 'grad_norm': 1.9337927103042603, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.288261890411377, 'eval_runtime': 8.7434, 'eval_samples_per_second': 114.373, 'eval_steps_per_second': 7.205, 'epoch': 0.04}
{'loss': 1.8053, 'grad_norm': 0.888217031955719, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.0430333614349365, 'eval_runtime': 8.7432, 'eval_samples_per_second': 114.374, 'eval_steps_per_second': 7.206, 'epoch': 0.08}
{'loss': 1.3022, 'grad_norm': 0.6759671568870544, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9958947896957397, 'eval_runtime': 8.7681, 'eval_samples_per_second': 114.05, 'eval_steps_per_second': 7.185, 'epoch': 0.12}
{'loss': 1.1866, 'grad_norm': 0.6032541990280151, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9657760858535767, 'eval_runtime': 8.7811, 'eval_samples_per_second': 113.881, 'eval_steps_per_second': 7.175, 'epoch': 0.16}
{'loss': 1.101, 'grad_norm': 0.4690297842025757, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.915757656097412, 'eval_runtime': 8.7899, 'eval_samples_per_second': 113.766, 'eval_steps_per_second': 7.167, 'epoch': 0.2}
{'loss': 1.1989, 'grad_norm': 0.40862324833869934, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.923491358757019, 'eval_runtime': 8.7806, 'eval_samples_per_second': 113.887, 'eval_steps_per_second': 7.175, 'epoch': 0.24}
{'loss': 1.1648, 'grad_norm': 0.6079533696174622, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8917131423950195, 'eval_runtime': 8.7753, 'eval_samples_per_second': 113.957, 'eval_steps_per_second': 7.179, 'epoch': 0.28}
{'loss': 1.1029, 'grad_norm': 0.5494617223739624, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9526101350784302, 'eval_runtime': 8.7903, 'eval_samples_per_second': 113.762, 'eval_steps_per_second': 7.167, 'epoch': 0.32}
{'loss': 1.0661, 'grad_norm': 0.422972172498703, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9161309003829956, 'eval_runtime': 8.7905, 'eval_samples_per_second': 113.76, 'eval_steps_per_second': 7.167, 'epoch': 0.36}
{'loss': 1.0832, 'grad_norm': 0.3850165009498596, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9525395631790161, 'eval_runtime': 8.7938, 'eval_samples_per_second': 113.716, 'eval_steps_per_second': 7.164, 'epoch': 0.4}
{'loss': 1.0192, 'grad_norm': 0.46806275844573975, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8830024003982544, 'eval_runtime': 8.7681, 'eval_samples_per_second': 114.05, 'eval_steps_per_second': 7.185, 'epoch': 0.44}
{'loss': 1.0364, 'grad_norm': 0.4178428053855896, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9444806575775146, 'eval_runtime': 8.7489, 'eval_samples_per_second': 114.301, 'eval_steps_per_second': 7.201, 'epoch': 0.48}
{'loss': 1.0813, 'grad_norm': 0.436244398355484, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9441581964492798, 'eval_runtime': 8.7456, 'eval_samples_per_second': 114.344, 'eval_steps_per_second': 7.204, 'epoch': 0.52}
{'loss': 1.0246, 'grad_norm': 0.6184238195419312, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8972171545028687, 'eval_runtime': 8.7371, 'eval_samples_per_second': 114.455, 'eval_steps_per_second': 7.211, 'epoch': 0.56}
{'loss': 1.055, 'grad_norm': 0.47939255833625793, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.0059752464294434, 'eval_runtime': 8.745, 'eval_samples_per_second': 114.351, 'eval_steps_per_second': 7.204, 'epoch': 0.6}
{'loss': 1.0329, 'grad_norm': 0.5410639047622681, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9696459770202637, 'eval_runtime': 8.7208, 'eval_samples_per_second': 114.668, 'eval_steps_per_second': 7.224, 'epoch': 0.64}
{'loss': 1.059, 'grad_norm': 0.504637598991394, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9387270212173462, 'eval_runtime': 8.7333, 'eval_samples_per_second': 114.505, 'eval_steps_per_second': 7.214, 'epoch': 0.68}
{'loss': 1.0835, 'grad_norm': 0.5351465344429016, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.965795874595642, 'eval_runtime': 8.7501, 'eval_samples_per_second': 114.284, 'eval_steps_per_second': 7.2, 'epoch': 0.72}
{'loss': 1.1167, 'grad_norm': 0.5531848669052124, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9584919214248657, 'eval_runtime': 8.7484, 'eval_samples_per_second': 114.306, 'eval_steps_per_second': 7.201, 'epoch': 0.76}
{'loss': 1.0175, 'grad_norm': 0.5335591435432434, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9485347270965576, 'eval_runtime': 8.7927, 'eval_samples_per_second': 113.73, 'eval_steps_per_second': 7.165, 'epoch': 0.8}
{'loss': 1.011, 'grad_norm': 0.5798565745353699, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9891984462738037, 'eval_runtime': 8.8006, 'eval_samples_per_second': 113.628, 'eval_steps_per_second': 7.159, 'epoch': 0.84}
{'loss': 0.9739, 'grad_norm': 0.576931357383728, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.985592246055603, 'eval_runtime': 8.7891, 'eval_samples_per_second': 113.777, 'eval_steps_per_second': 7.168, 'epoch': 0.88}
{'loss': 0.9932, 'grad_norm': 0.6105040311813354, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.970471978187561, 'eval_runtime': 8.7899, 'eval_samples_per_second': 113.767, 'eval_steps_per_second': 7.167, 'epoch': 0.92}
{'loss': 1.0102, 'grad_norm': 0.6464706659317017, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.997296929359436, 'eval_runtime': 8.7988, 'eval_samples_per_second': 113.651, 'eval_steps_per_second': 7.16, 'epoch': 0.96}
{'loss': 1.0374, 'grad_norm': 0.696259617805481, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9991611242294312, 'eval_runtime': 8.8353, 'eval_samples_per_second': 113.182, 'eval_steps_per_second': 7.13, 'epoch': 1.0}
{'train_runtime': 372.2415, 'train_samples_per_second': 26.856, 'train_steps_per_second': 1.679, 'train_loss': 1.2019470947265625, 'epoch': 1.0}
train_results:  {'eval_loss': [2.288261890411377, 2.0430333614349365, 1.9958947896957397, 1.9657760858535767, 1.915757656097412, 1.923491358757019, 1.8917131423950195, 1.9526101350784302, 1.9161309003829956, 1.9525395631790161, 1.8830024003982544, 1.9444806575775146, 1.9441581964492798, 1.8972171545028687, 2.0059752464294434, 1.9696459770202637, 1.9387270212173462, 1.965795874595642, 1.9584919214248657, 1.9485347270965576, 1.9891984462738037, 1.985592246055603, 1.970471978187561, 1.997296929359436, 1.9991611242294312], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.288261890411377, 2.0430333614349365, 1.9958947896957397, 1.9657760858535767, 1.915757656097412, 1.923491358757019, 1.8917131423950195, 1.9526101350784302, 1.9161309003829956, 1.9525395631790161, 1.8830024003982544, 1.9444806575775146, 1.9441581964492798, 1.8972171545028687, 2.0059752464294434, 1.9696459770202637, 1.9387270212173462, 1.965795874595642, 1.9584919214248657, 1.9485347270965576, 1.9891984462738037, 1.985592246055603, 1.970471978187561, 1.997296929359436, 1.9991611242294312]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.9991611242294312
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0056 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -0.8966359274533939
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -0.8966359274533939
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -0.8966359274533939
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.8966359274533939
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -0.8966359274533939
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, tensor(0.8850, dtype=torch.float64), tensor(0.1150, dtype=torch.float64), 15, 0, 0, 1, 1, 0, 128, 0.01965596956562212, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0524e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5898e-17, dtype=torch.float64), tensor(1.0840e-17, dtype=torch.float64), tensor(0.8850, dtype=torch.float64), tensor(0.1150, dtype=torch.float64), tensor(0.4665, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1966, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.885
  arc_challenge: 0.115

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.01965596956562212,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.01965596956562212
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 70,778,880 || all params: 8,101,040,128 || trainable%: 0.8737
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7537, 'grad_norm': 0.672616183757782, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6723599433898926, 'eval_runtime': 9.1289, 'eval_samples_per_second': 109.543, 'eval_steps_per_second': 6.901, 'epoch': 0.04}
{'loss': 1.4647, 'grad_norm': 0.28991395235061646, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.352750301361084, 'eval_runtime': 9.1736, 'eval_samples_per_second': 109.008, 'eval_steps_per_second': 6.868, 'epoch': 0.08}
{'loss': 1.3282, 'grad_norm': 0.22347702085971832, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3116567134857178, 'eval_runtime': 9.2344, 'eval_samples_per_second': 108.29, 'eval_steps_per_second': 6.822, 'epoch': 0.12}
{'loss': 1.2335, 'grad_norm': 0.23344126343727112, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2968252897262573, 'eval_runtime': 9.2664, 'eval_samples_per_second': 107.916, 'eval_steps_per_second': 6.799, 'epoch': 0.16}
{'loss': 1.2327, 'grad_norm': 0.22972580790519714, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2888696193695068, 'eval_runtime': 9.2937, 'eval_samples_per_second': 107.599, 'eval_steps_per_second': 6.779, 'epoch': 0.2}
{'loss': 1.2633, 'grad_norm': 0.19510750472545624, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2833172082901, 'eval_runtime': 9.2769, 'eval_samples_per_second': 107.795, 'eval_steps_per_second': 6.791, 'epoch': 0.24}
{'loss': 1.2526, 'grad_norm': 0.17230631411075592, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.279290795326233, 'eval_runtime': 9.2803, 'eval_samples_per_second': 107.755, 'eval_steps_per_second': 6.789, 'epoch': 0.28}
{'loss': 1.2516, 'grad_norm': 0.2887060046195984, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.276498794555664, 'eval_runtime': 9.299, 'eval_samples_per_second': 107.539, 'eval_steps_per_second': 6.775, 'epoch': 0.32}
{'loss': 1.2059, 'grad_norm': 0.20878760516643524, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2721301317214966, 'eval_runtime': 9.2763, 'eval_samples_per_second': 107.801, 'eval_steps_per_second': 6.791, 'epoch': 0.36}
{'loss': 1.2407, 'grad_norm': 0.1859302818775177, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.266830563545227, 'eval_runtime': 9.2948, 'eval_samples_per_second': 107.587, 'eval_steps_per_second': 6.778, 'epoch': 0.4}
{'loss': 1.2115, 'grad_norm': 0.19697163999080658, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.265769124031067, 'eval_runtime': 9.2951, 'eval_samples_per_second': 107.584, 'eval_steps_per_second': 6.778, 'epoch': 0.44}
{'loss': 1.182, 'grad_norm': 0.21054649353027344, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2609204053878784, 'eval_runtime': 9.2895, 'eval_samples_per_second': 107.649, 'eval_steps_per_second': 6.782, 'epoch': 0.48}
{'loss': 1.2564, 'grad_norm': 0.18689337372779846, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2591333389282227, 'eval_runtime': 9.2669, 'eval_samples_per_second': 107.911, 'eval_steps_per_second': 6.798, 'epoch': 0.52}
{'loss': 1.2324, 'grad_norm': 0.2078799605369568, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2573802471160889, 'eval_runtime': 9.2773, 'eval_samples_per_second': 107.79, 'eval_steps_per_second': 6.791, 'epoch': 0.56}
{'loss': 1.1988, 'grad_norm': 0.20146869122982025, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.25412118434906, 'eval_runtime': 9.2851, 'eval_samples_per_second': 107.699, 'eval_steps_per_second': 6.785, 'epoch': 0.6}
{'loss': 1.2056, 'grad_norm': 0.21139195561408997, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2533241510391235, 'eval_runtime': 9.2767, 'eval_samples_per_second': 107.797, 'eval_steps_per_second': 6.791, 'epoch': 0.64}
{'loss': 1.1951, 'grad_norm': 0.19340388476848602, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2509289979934692, 'eval_runtime': 9.2786, 'eval_samples_per_second': 107.774, 'eval_steps_per_second': 6.79, 'epoch': 0.68}
{'loss': 1.1475, 'grad_norm': 0.21814873814582825, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.249843716621399, 'eval_runtime': 9.2692, 'eval_samples_per_second': 107.884, 'eval_steps_per_second': 6.797, 'epoch': 0.72}
{'loss': 1.1782, 'grad_norm': 0.2396644800901413, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2464492321014404, 'eval_runtime': 9.2719, 'eval_samples_per_second': 107.853, 'eval_steps_per_second': 6.795, 'epoch': 0.76}
{'loss': 1.1733, 'grad_norm': 0.24496452510356903, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2431697845458984, 'eval_runtime': 9.2783, 'eval_samples_per_second': 107.778, 'eval_steps_per_second': 6.79, 'epoch': 0.8}
{'loss': 1.1244, 'grad_norm': 0.2145460546016693, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2422715425491333, 'eval_runtime': 9.2579, 'eval_samples_per_second': 108.016, 'eval_steps_per_second': 6.805, 'epoch': 0.84}
{'loss': 1.1501, 'grad_norm': 0.21941246092319489, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2436115741729736, 'eval_runtime': 9.2673, 'eval_samples_per_second': 107.907, 'eval_steps_per_second': 6.798, 'epoch': 0.88}
{'loss': 1.178, 'grad_norm': 0.21937693655490875, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2416092157363892, 'eval_runtime': 9.2744, 'eval_samples_per_second': 107.824, 'eval_steps_per_second': 6.793, 'epoch': 0.92}
{'loss': 1.1311, 'grad_norm': 0.2252403348684311, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2423110008239746, 'eval_runtime': 9.2763, 'eval_samples_per_second': 107.801, 'eval_steps_per_second': 6.791, 'epoch': 0.96}
{'loss': 1.161, 'grad_norm': 0.23461920022964478, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2414144277572632, 'eval_runtime': 9.2756, 'eval_samples_per_second': 107.809, 'eval_steps_per_second': 6.792, 'epoch': 1.0}
{'train_runtime': 385.6733, 'train_samples_per_second': 25.926, 'train_steps_per_second': 1.621, 'train_loss': 1.2780924530029296, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6723599433898926, 1.352750301361084, 1.3116567134857178, 1.2968252897262573, 1.2888696193695068, 1.2833172082901, 1.279290795326233, 1.276498794555664, 1.2721301317214966, 1.266830563545227, 1.265769124031067, 1.2609204053878784, 1.2591333389282227, 1.2573802471160889, 1.25412118434906, 1.2533241510391235, 1.2509289979934692, 1.249843716621399, 1.2464492321014404, 1.2431697845458984, 1.2422715425491333, 1.2436115741729736, 1.2416092157363892, 1.2423110008239746, 1.2414144277572632], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6723599433898926, 1.352750301361084, 1.3116567134857178, 1.2968252897262573, 1.2888696193695068, 1.2833172082901, 1.279290795326233, 1.276498794555664, 1.2721301317214966, 1.266830563545227, 1.265769124031067, 1.2609204053878784, 1.2591333389282227, 1.2573802471160889, 1.25412118434906, 1.2533241510391235, 1.2509289979934692, 1.249843716621399, 1.2464492321014404, 1.2431697845458984, 1.2422715425491333, 1.2436115741729736, 1.2416092157363892, 1.2423110008239746, 1.2414144277572632]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2414144277572632
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6422 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -0.9286189485861394
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -0.9286189485861394
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -0.9286189485861394
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -0.9286189485861394
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -0.9286189485861394
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.1947, dtype=torch.float64), tensor(0.3951, dtype=torch.float64), tensor(0.4102, dtype=torch.float64), 16, 0, 0, 1, 0, 1, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(2.7768e-17, dtype=torch.float64), tensor(1.8644e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.9649e-18, dtype=torch.float64), tensor(1.1581e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1947, dtype=torch.float64), tensor(0.3951, dtype=torch.float64), tensor(0.4102, dtype=torch.float64), tensor(0.5107, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.195
  mmlu: 0.395
  arc_challenge: 0.41

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8929, 'grad_norm': 0.43967530131340027, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8186628818511963, 'eval_runtime': 9.0893, 'eval_samples_per_second': 110.019, 'eval_steps_per_second': 6.931, 'epoch': 0.04}
{'loss': 1.5719, 'grad_norm': 0.2215055376291275, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.566101312637329, 'eval_runtime': 9.0975, 'eval_samples_per_second': 109.92, 'eval_steps_per_second': 6.925, 'epoch': 0.08}
{'loss': 1.4416, 'grad_norm': 0.21815437078475952, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4500224590301514, 'eval_runtime': 9.1122, 'eval_samples_per_second': 109.743, 'eval_steps_per_second': 6.914, 'epoch': 0.12}
{'loss': 1.3359, 'grad_norm': 0.20179323852062225, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3756229877471924, 'eval_runtime': 9.144, 'eval_samples_per_second': 109.362, 'eval_steps_per_second': 6.89, 'epoch': 0.16}
{'loss': 1.2353, 'grad_norm': 0.2117004245519638, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.329310655593872, 'eval_runtime': 9.1475, 'eval_samples_per_second': 109.32, 'eval_steps_per_second': 6.887, 'epoch': 0.2}
{'loss': 1.1735, 'grad_norm': 0.23850783705711365, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3056737184524536, 'eval_runtime': 9.1627, 'eval_samples_per_second': 109.138, 'eval_steps_per_second': 6.876, 'epoch': 0.24}
{'loss': 1.2212, 'grad_norm': 0.2342938333749771, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3034957647323608, 'eval_runtime': 9.1652, 'eval_samples_per_second': 109.108, 'eval_steps_per_second': 6.874, 'epoch': 0.28}
{'loss': 1.1977, 'grad_norm': 0.23252873122692108, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3027536869049072, 'eval_runtime': 9.1551, 'eval_samples_per_second': 109.229, 'eval_steps_per_second': 6.881, 'epoch': 0.32}
{'loss': 1.2403, 'grad_norm': 0.22015078365802765, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2955338954925537, 'eval_runtime': 9.1651, 'eval_samples_per_second': 109.109, 'eval_steps_per_second': 6.874, 'epoch': 0.36}
{'loss': 1.2471, 'grad_norm': 0.22568722069263458, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2926201820373535, 'eval_runtime': 9.1606, 'eval_samples_per_second': 109.163, 'eval_steps_per_second': 6.877, 'epoch': 0.4}
{'loss': 1.1549, 'grad_norm': 0.2666197419166565, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2907629013061523, 'eval_runtime': 9.1701, 'eval_samples_per_second': 109.05, 'eval_steps_per_second': 6.87, 'epoch': 0.44}
{'loss': 1.1713, 'grad_norm': 0.26181572675704956, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2896181344985962, 'eval_runtime': 9.1652, 'eval_samples_per_second': 109.109, 'eval_steps_per_second': 6.874, 'epoch': 0.48}
{'loss': 1.1116, 'grad_norm': 0.26519936323165894, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2864375114440918, 'eval_runtime': 9.1677, 'eval_samples_per_second': 109.079, 'eval_steps_per_second': 6.872, 'epoch': 0.52}
{'loss': 1.0517, 'grad_norm': 0.3065202236175537, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2879365682601929, 'eval_runtime': 9.1632, 'eval_samples_per_second': 109.132, 'eval_steps_per_second': 6.875, 'epoch': 0.56}
{'loss': 1.1692, 'grad_norm': 0.2680870592594147, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.28013277053833, 'eval_runtime': 9.228, 'eval_samples_per_second': 108.366, 'eval_steps_per_second': 6.827, 'epoch': 0.6}
{'loss': 1.0977, 'grad_norm': 0.255859911441803, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2788325548171997, 'eval_runtime': 9.2504, 'eval_samples_per_second': 108.103, 'eval_steps_per_second': 6.811, 'epoch': 0.64}
{'loss': 1.0414, 'grad_norm': 0.23339366912841797, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2795947790145874, 'eval_runtime': 9.2489, 'eval_samples_per_second': 108.121, 'eval_steps_per_second': 6.812, 'epoch': 0.68}
{'loss': 1.043, 'grad_norm': 0.2789060175418854, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2761688232421875, 'eval_runtime': 9.2455, 'eval_samples_per_second': 108.16, 'eval_steps_per_second': 6.814, 'epoch': 0.72}
{'loss': 1.1487, 'grad_norm': 0.35402023792266846, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2729597091674805, 'eval_runtime': 9.242, 'eval_samples_per_second': 108.202, 'eval_steps_per_second': 6.817, 'epoch': 0.76}
{'loss': 1.0369, 'grad_norm': 0.27381715178489685, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2733571529388428, 'eval_runtime': 9.2545, 'eval_samples_per_second': 108.055, 'eval_steps_per_second': 6.807, 'epoch': 0.8}
{'loss': 0.9869, 'grad_norm': 0.3470132648944855, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2717515230178833, 'eval_runtime': 9.2468, 'eval_samples_per_second': 108.145, 'eval_steps_per_second': 6.813, 'epoch': 0.84}
{'loss': 1.0214, 'grad_norm': 0.3137573003768921, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2712681293487549, 'eval_runtime': 9.2451, 'eval_samples_per_second': 108.165, 'eval_steps_per_second': 6.814, 'epoch': 0.88}
{'loss': 0.9425, 'grad_norm': 0.3348712623119354, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2715789079666138, 'eval_runtime': 9.2438, 'eval_samples_per_second': 108.181, 'eval_steps_per_second': 6.815, 'epoch': 0.92}
{'loss': 1.0519, 'grad_norm': 0.29012951254844666, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2702573537826538, 'eval_runtime': 9.2867, 'eval_samples_per_second': 107.681, 'eval_steps_per_second': 6.784, 'epoch': 0.96}
{'loss': 0.9979, 'grad_norm': 0.3372131586074829, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.269762396812439, 'eval_runtime': 9.3131, 'eval_samples_per_second': 107.376, 'eval_steps_per_second': 6.765, 'epoch': 1.0}
{'train_runtime': 373.5142, 'train_samples_per_second': 26.767, 'train_steps_per_second': 1.673, 'train_loss': 1.2233834075927734, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8186628818511963, 1.566101312637329, 1.4500224590301514, 1.3756229877471924, 1.329310655593872, 1.3056737184524536, 1.3034957647323608, 1.3027536869049072, 1.2955338954925537, 1.2926201820373535, 1.2907629013061523, 1.2896181344985962, 1.2864375114440918, 1.2879365682601929, 1.28013277053833, 1.2788325548171997, 1.2795947790145874, 1.2761688232421875, 1.2729597091674805, 1.2733571529388428, 1.2717515230178833, 1.2712681293487549, 1.2715789079666138, 1.2702573537826538, 1.269762396812439], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8186628818511963, 1.566101312637329, 1.4500224590301514, 1.3756229877471924, 1.329310655593872, 1.3056737184524536, 1.3034957647323608, 1.3027536869049072, 1.2955338954925537, 1.2926201820373535, 1.2907629013061523, 1.2896181344985962, 1.2864375114440918, 1.2879365682601929, 1.28013277053833, 1.2788325548171997, 1.2795947790145874, 1.2761688232421875, 1.2729597091674805, 1.2733571529388428, 1.2717515230178833, 1.2712681293487549, 1.2715789079666138, 1.2702573537826538, 1.269762396812439]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.269762396812439
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4750 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -1.0032609713753584
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.973257750275939
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -1.0056983024297634
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -1.004121283946617
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -1.0070832563753331
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1433, dtype=torch.float64), 0, tensor(0.0814, dtype=torch.float64), 0, tensor(0.0139, dtype=torch.float64), 0, tensor(0.0653, dtype=torch.float64), tensor(0.6961, dtype=torch.float64), 16, 0, 0, 1, 0, 1, 119, 0.09458529616067521, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(2.9156e-17, dtype=torch.float64), tensor(0.1433, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0814, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0139, dtype=torch.float64), tensor(1.0359e-18, dtype=torch.float64), tensor(0.0653, dtype=torch.float64), tensor(0.6961, dtype=torch.float64), tensor(0.4857, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9265, dtype=torch.float64), tensor(0.9459, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.143
  rowan_hellaswag: 0
  sciq: 0.081
  triviaqa: 0
  truthfulqa_gen: 0.014
  wikitext: 0
  mmlu: 0.065
  arc_challenge: 0.696

LoRA Parameters:
  lora_r: (119,)
  lora_dropout: (0.09458529616067521,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  119
lora dropout:  0.09458529616067521
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 70,189,056 || all params: 8,100,450,304 || trainable%: 0.8665
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7619, 'grad_norm': 0.5493211150169373, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0515191555023193, 'eval_runtime': 9.5048, 'eval_samples_per_second': 105.21, 'eval_steps_per_second': 6.628, 'epoch': 0.04}
{'loss': 1.2551, 'grad_norm': 0.1998947709798813, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6866624355316162, 'eval_runtime': 9.5715, 'eval_samples_per_second': 104.477, 'eval_steps_per_second': 6.582, 'epoch': 0.08}
{'loss': 1.1023, 'grad_norm': 0.19855186343193054, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5830552577972412, 'eval_runtime': 9.5932, 'eval_samples_per_second': 104.24, 'eval_steps_per_second': 6.567, 'epoch': 0.12}
{'loss': 0.9529, 'grad_norm': 0.21712709963321686, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5040462017059326, 'eval_runtime': 9.5998, 'eval_samples_per_second': 104.169, 'eval_steps_per_second': 6.563, 'epoch': 0.16}
{'loss': 0.8639, 'grad_norm': 0.2344837784767151, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4635910987854004, 'eval_runtime': 9.6004, 'eval_samples_per_second': 104.163, 'eval_steps_per_second': 6.562, 'epoch': 0.2}
{'loss': 0.8642, 'grad_norm': 0.20513980090618134, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4087780714035034, 'eval_runtime': 9.6218, 'eval_samples_per_second': 103.93, 'eval_steps_per_second': 6.548, 'epoch': 0.24}
{'loss': 0.8364, 'grad_norm': 0.246663436293602, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3967331647872925, 'eval_runtime': 9.631, 'eval_samples_per_second': 103.831, 'eval_steps_per_second': 6.541, 'epoch': 0.28}
{'loss': 0.7957, 'grad_norm': 0.28191494941711426, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3832507133483887, 'eval_runtime': 9.6214, 'eval_samples_per_second': 103.935, 'eval_steps_per_second': 6.548, 'epoch': 0.32}
{'loss': 0.7266, 'grad_norm': 0.3242659568786621, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3985984325408936, 'eval_runtime': 9.6094, 'eval_samples_per_second': 104.064, 'eval_steps_per_second': 6.556, 'epoch': 0.36}
{'loss': 0.7335, 'grad_norm': 0.2806302607059479, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.398271918296814, 'eval_runtime': 9.63, 'eval_samples_per_second': 103.842, 'eval_steps_per_second': 6.542, 'epoch': 0.4}
{'loss': 0.6822, 'grad_norm': 0.38203829526901245, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3955981731414795, 'eval_runtime': 9.6256, 'eval_samples_per_second': 103.89, 'eval_steps_per_second': 6.545, 'epoch': 0.44}
{'loss': 0.6724, 'grad_norm': 0.3201584219932556, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.389560341835022, 'eval_runtime': 9.6334, 'eval_samples_per_second': 103.806, 'eval_steps_per_second': 6.54, 'epoch': 0.48}
{'loss': 0.6459, 'grad_norm': 0.31927821040153503, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3830578327178955, 'eval_runtime': 9.6319, 'eval_samples_per_second': 103.822, 'eval_steps_per_second': 6.541, 'epoch': 0.52}
{'loss': 0.576, 'grad_norm': 0.31673988699913025, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3944664001464844, 'eval_runtime': 9.6175, 'eval_samples_per_second': 103.977, 'eval_steps_per_second': 6.551, 'epoch': 0.56}
{'loss': 0.6048, 'grad_norm': 0.4272102415561676, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3825117349624634, 'eval_runtime': 9.6164, 'eval_samples_per_second': 103.989, 'eval_steps_per_second': 6.551, 'epoch': 0.6}
{'loss': 0.5299, 'grad_norm': 0.2870856821537018, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3826369047164917, 'eval_runtime': 9.6095, 'eval_samples_per_second': 104.064, 'eval_steps_per_second': 6.556, 'epoch': 0.64}
{'loss': 0.533, 'grad_norm': 0.3153589367866516, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3785167932510376, 'eval_runtime': 9.6085, 'eval_samples_per_second': 104.074, 'eval_steps_per_second': 6.557, 'epoch': 0.68}
{'loss': 0.5011, 'grad_norm': 0.28375494480133057, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.385970950126648, 'eval_runtime': 9.6134, 'eval_samples_per_second': 104.022, 'eval_steps_per_second': 6.553, 'epoch': 0.72}
{'loss': 0.4942, 'grad_norm': 0.3038899898529053, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3754074573516846, 'eval_runtime': 9.5831, 'eval_samples_per_second': 104.35, 'eval_steps_per_second': 6.574, 'epoch': 0.76}
{'loss': 0.4601, 'grad_norm': 0.29902753233909607, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.383745551109314, 'eval_runtime': 9.5682, 'eval_samples_per_second': 104.513, 'eval_steps_per_second': 6.584, 'epoch': 0.8}
{'loss': 0.4961, 'grad_norm': 0.2548331618309021, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.374489665031433, 'eval_runtime': 9.5485, 'eval_samples_per_second': 104.729, 'eval_steps_per_second': 6.598, 'epoch': 0.84}
{'loss': 0.4515, 'grad_norm': 0.36743825674057007, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3761937618255615, 'eval_runtime': 9.5612, 'eval_samples_per_second': 104.59, 'eval_steps_per_second': 6.589, 'epoch': 0.88}
{'loss': 0.4679, 'grad_norm': 0.30580776929855347, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.370518445968628, 'eval_runtime': 9.5655, 'eval_samples_per_second': 104.543, 'eval_steps_per_second': 6.586, 'epoch': 0.92}
{'loss': 0.4024, 'grad_norm': 0.42684778571128845, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3730275630950928, 'eval_runtime': 9.5617, 'eval_samples_per_second': 104.584, 'eval_steps_per_second': 6.589, 'epoch': 0.96}
{'loss': 0.4125, 'grad_norm': 0.22727040946483612, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3724653720855713, 'eval_runtime': 9.5682, 'eval_samples_per_second': 104.513, 'eval_steps_per_second': 6.584, 'epoch': 1.0}
{'train_runtime': 387.5658, 'train_samples_per_second': 25.794, 'train_steps_per_second': 1.613, 'train_loss': 0.7528987731933594, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0515191555023193, 1.6866624355316162, 1.5830552577972412, 1.5040462017059326, 1.4635910987854004, 1.4087780714035034, 1.3967331647872925, 1.3832507133483887, 1.3985984325408936, 1.398271918296814, 1.3955981731414795, 1.389560341835022, 1.3830578327178955, 1.3944664001464844, 1.3825117349624634, 1.3826369047164917, 1.3785167932510376, 1.385970950126648, 1.3754074573516846, 1.383745551109314, 1.374489665031433, 1.3761937618255615, 1.370518445968628, 1.3730275630950928, 1.3724653720855713], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0515191555023193, 1.6866624355316162, 1.5830552577972412, 1.5040462017059326, 1.4635910987854004, 1.4087780714035034, 1.3967331647872925, 1.3832507133483887, 1.3985984325408936, 1.398271918296814, 1.3955981731414795, 1.389560341835022, 1.3830578327178955, 1.3944664001464844, 1.3825117349624634, 1.3826369047164917, 1.3785167932510376, 1.385970950126648, 1.3754074573516846, 1.383745551109314, 1.374489665031433, 1.3761937618255615, 1.370518445968628, 1.3730275630950928, 1.3724653720855713]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3724653720855713
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.6723 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.9425134119662872
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.9767334987750346
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.9755110223515391
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.9759701021830445
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -0.9553206602638673
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1899, dtype=torch.float64), 0, tensor(0.2392, dtype=torch.float64), tensor(0.1682, dtype=torch.float64), tensor(0.2670, dtype=torch.float64), tensor(0.1357, dtype=torch.float64), 17, 0, 1, 1, 1, 0, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(8.1171e-18, dtype=torch.float64), tensor(2.4433e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1899, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2392, dtype=torch.float64), tensor(0.1682, dtype=torch.float64), tensor(0.2670, dtype=torch.float64), tensor(0.1357, dtype=torch.float64), tensor(0.5259, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.19
  triviaqa: 0
  truthfulqa_gen: 0.239
  wikitext: 0.168
  mmlu: 0.267
  arc_challenge: 0.136

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 91,357,184 || all params: 8,121,618,432 || trainable%: 1.1249
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0184, 'grad_norm': 0.5807996988296509, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6605548858642578, 'eval_runtime': 9.4044, 'eval_samples_per_second': 106.333, 'eval_steps_per_second': 6.699, 'epoch': 0.04}
{'loss': 1.4261, 'grad_norm': 0.3342457413673401, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3856470584869385, 'eval_runtime': 9.4002, 'eval_samples_per_second': 106.381, 'eval_steps_per_second': 6.702, 'epoch': 0.08}
{'loss': 1.287, 'grad_norm': 0.331574946641922, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3493671417236328, 'eval_runtime': 9.4183, 'eval_samples_per_second': 106.176, 'eval_steps_per_second': 6.689, 'epoch': 0.12}
{'loss': 1.2107, 'grad_norm': 0.22754816710948944, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3216907978057861, 'eval_runtime': 9.4588, 'eval_samples_per_second': 105.722, 'eval_steps_per_second': 6.66, 'epoch': 0.16}
{'loss': 1.2208, 'grad_norm': 0.24876683950424194, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.313245415687561, 'eval_runtime': 9.4833, 'eval_samples_per_second': 105.448, 'eval_steps_per_second': 6.643, 'epoch': 0.2}
{'loss': 1.1607, 'grad_norm': 0.2547651529312134, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2997241020202637, 'eval_runtime': 9.4875, 'eval_samples_per_second': 105.402, 'eval_steps_per_second': 6.64, 'epoch': 0.24}
{'loss': 1.2471, 'grad_norm': 0.35561811923980713, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3019633293151855, 'eval_runtime': 9.4944, 'eval_samples_per_second': 105.326, 'eval_steps_per_second': 6.636, 'epoch': 0.28}
{'loss': 1.1276, 'grad_norm': 0.23418369889259338, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2967792749404907, 'eval_runtime': 9.4823, 'eval_samples_per_second': 105.459, 'eval_steps_per_second': 6.644, 'epoch': 0.32}
{'loss': 1.1555, 'grad_norm': 0.24172110855579376, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.289613127708435, 'eval_runtime': 9.4953, 'eval_samples_per_second': 105.315, 'eval_steps_per_second': 6.635, 'epoch': 0.36}
{'loss': 1.112, 'grad_norm': 0.30229589343070984, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.290987491607666, 'eval_runtime': 9.4762, 'eval_samples_per_second': 105.528, 'eval_steps_per_second': 6.648, 'epoch': 0.4}
{'loss': 1.179, 'grad_norm': 0.30445247888565063, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2887287139892578, 'eval_runtime': 9.4817, 'eval_samples_per_second': 105.467, 'eval_steps_per_second': 6.644, 'epoch': 0.44}
{'loss': 1.1503, 'grad_norm': 0.2585035562515259, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.287250280380249, 'eval_runtime': 9.4827, 'eval_samples_per_second': 105.455, 'eval_steps_per_second': 6.644, 'epoch': 0.48}
{'loss': 1.221, 'grad_norm': 0.29581162333488464, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2849386930465698, 'eval_runtime': 9.4748, 'eval_samples_per_second': 105.543, 'eval_steps_per_second': 6.649, 'epoch': 0.52}
{'loss': 1.1771, 'grad_norm': 0.3261357247829437, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2764760255813599, 'eval_runtime': 9.4839, 'eval_samples_per_second': 105.442, 'eval_steps_per_second': 6.643, 'epoch': 0.56}
{'loss': 1.0904, 'grad_norm': 0.26286083459854126, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2752407789230347, 'eval_runtime': 9.4825, 'eval_samples_per_second': 105.457, 'eval_steps_per_second': 6.644, 'epoch': 0.6}
{'loss': 1.1122, 'grad_norm': 0.34098994731903076, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2753682136535645, 'eval_runtime': 9.4777, 'eval_samples_per_second': 105.511, 'eval_steps_per_second': 6.647, 'epoch': 0.64}
{'loss': 1.1672, 'grad_norm': 0.3145224452018738, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.272647500038147, 'eval_runtime': 9.4712, 'eval_samples_per_second': 105.584, 'eval_steps_per_second': 6.652, 'epoch': 0.68}
{'loss': 1.1151, 'grad_norm': 0.31829574704170227, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2711118459701538, 'eval_runtime': 9.4956, 'eval_samples_per_second': 105.312, 'eval_steps_per_second': 6.635, 'epoch': 0.72}
{'loss': 1.1882, 'grad_norm': 0.3145483434200287, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2714450359344482, 'eval_runtime': 9.487, 'eval_samples_per_second': 105.408, 'eval_steps_per_second': 6.641, 'epoch': 0.76}
{'loss': 1.1307, 'grad_norm': 0.26041728258132935, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2680555582046509, 'eval_runtime': 9.4984, 'eval_samples_per_second': 105.281, 'eval_steps_per_second': 6.633, 'epoch': 0.8}
{'loss': 1.137, 'grad_norm': 0.32822898030281067, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2678595781326294, 'eval_runtime': 9.479, 'eval_samples_per_second': 105.497, 'eval_steps_per_second': 6.646, 'epoch': 0.84}
{'loss': 1.0951, 'grad_norm': 0.2600683569908142, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2654306888580322, 'eval_runtime': 9.4896, 'eval_samples_per_second': 105.379, 'eval_steps_per_second': 6.639, 'epoch': 0.88}
{'loss': 1.0196, 'grad_norm': 0.2924101650714874, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2646933794021606, 'eval_runtime': 9.5127, 'eval_samples_per_second': 105.123, 'eval_steps_per_second': 6.623, 'epoch': 0.92}
{'loss': 1.0718, 'grad_norm': 0.3541671633720398, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.26473069190979, 'eval_runtime': 9.4927, 'eval_samples_per_second': 105.344, 'eval_steps_per_second': 6.637, 'epoch': 0.96}
{'loss': 1.0896, 'grad_norm': 0.3182075619697571, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2634965181350708, 'eval_runtime': 9.5007, 'eval_samples_per_second': 105.256, 'eval_steps_per_second': 6.631, 'epoch': 1.0}
{'train_runtime': 384.155, 'train_samples_per_second': 26.026, 'train_steps_per_second': 1.627, 'train_loss': 1.2364036651611328, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6605548858642578, 1.3856470584869385, 1.3493671417236328, 1.3216907978057861, 1.313245415687561, 1.2997241020202637, 1.3019633293151855, 1.2967792749404907, 1.289613127708435, 1.290987491607666, 1.2887287139892578, 1.287250280380249, 1.2849386930465698, 1.2764760255813599, 1.2752407789230347, 1.2753682136535645, 1.272647500038147, 1.2711118459701538, 1.2714450359344482, 1.2680555582046509, 1.2678595781326294, 1.2654306888580322, 1.2646933794021606, 1.26473069190979, 1.2634965181350708], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6605548858642578, 1.3856470584869385, 1.3493671417236328, 1.3216907978057861, 1.313245415687561, 1.2997241020202637, 1.3019633293151855, 1.2967792749404907, 1.289613127708435, 1.290987491607666, 1.2887287139892578, 1.287250280380249, 1.2849386930465698, 1.2764760255813599, 1.2752407789230347, 1.2753682136535645, 1.272647500038147, 1.2711118459701538, 1.2714450359344482, 1.2680555582046509, 1.2678595781326294, 1.2654306888580322, 1.2646933794021606, 1.26473069190979, 1.2634965181350708]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2634965181350708
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 44.8015 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.9692869476672241
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -0.9692869476672241
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.9692869476672241
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -0.9692869476672241
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -0.9692869476672241
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.6120, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.3880, dtype=torch.float64), 0, 15, 0, 0, 1, 1, 1, 128, 0.0, 47.999999999999986, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.6120, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3772e-16, dtype=torch.float64), tensor(0.3880, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4698, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.612
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.388
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (47.999999999999986,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  47.999999999999986
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 106,168,320 || all params: 8,136,429,568 || trainable%: 1.3049
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.2763, 'grad_norm': 0.6481289863586426, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6700574159622192, 'eval_runtime': 16.9694, 'eval_samples_per_second': 58.93, 'eval_steps_per_second': 3.713, 'epoch': 0.04}
{'loss': 1.1851, 'grad_norm': 0.21947191655635834, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4167869091033936, 'eval_runtime': 14.8871, 'eval_samples_per_second': 67.172, 'eval_steps_per_second': 4.232, 'epoch': 0.08}
{'loss': 1.0676, 'grad_norm': 0.16614565253257751, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3357341289520264, 'eval_runtime': 15.2174, 'eval_samples_per_second': 65.714, 'eval_steps_per_second': 4.14, 'epoch': 0.12}
{'loss': 1.048, 'grad_norm': 0.1522921323776245, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3224183320999146, 'eval_runtime': 15.5026, 'eval_samples_per_second': 64.505, 'eval_steps_per_second': 4.064, 'epoch': 0.16}
{'loss': 1.06, 'grad_norm': 0.14705732464790344, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3087105751037598, 'eval_runtime': 16.6544, 'eval_samples_per_second': 60.044, 'eval_steps_per_second': 3.783, 'epoch': 0.2}
{'loss': 1.0115, 'grad_norm': 0.1490524262189865, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3051769733428955, 'eval_runtime': 16.3282, 'eval_samples_per_second': 61.244, 'eval_steps_per_second': 3.858, 'epoch': 0.24}
{'loss': 1.0305, 'grad_norm': 0.1545698493719101, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.301025629043579, 'eval_runtime': 15.0717, 'eval_samples_per_second': 66.35, 'eval_steps_per_second': 4.18, 'epoch': 0.28}
{'loss': 1.0308, 'grad_norm': 0.14691197872161865, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2927134037017822, 'eval_runtime': 16.4409, 'eval_samples_per_second': 60.824, 'eval_steps_per_second': 3.832, 'epoch': 0.32}
{'loss': 1.0024, 'grad_norm': 0.1421576887369156, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2915103435516357, 'eval_runtime': 13.5169, 'eval_samples_per_second': 73.982, 'eval_steps_per_second': 4.661, 'epoch': 0.36}
{'loss': 0.9842, 'grad_norm': 0.16427452862262726, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2888915538787842, 'eval_runtime': 15.6934, 'eval_samples_per_second': 63.721, 'eval_steps_per_second': 4.014, 'epoch': 0.4}
{'loss': 0.9746, 'grad_norm': 0.15098510682582855, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2871776819229126, 'eval_runtime': 15.0872, 'eval_samples_per_second': 66.281, 'eval_steps_per_second': 4.176, 'epoch': 0.44}
{'loss': 1.013, 'grad_norm': 0.15935511887073517, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2812772989273071, 'eval_runtime': 15.2017, 'eval_samples_per_second': 65.782, 'eval_steps_per_second': 4.144, 'epoch': 0.48}
{'loss': 0.958, 'grad_norm': 0.14564886689186096, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2809783220291138, 'eval_runtime': 14.6189, 'eval_samples_per_second': 68.404, 'eval_steps_per_second': 4.309, 'epoch': 0.52}
{'loss': 0.9679, 'grad_norm': 0.144860178232193, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2787110805511475, 'eval_runtime': 13.5281, 'eval_samples_per_second': 73.92, 'eval_steps_per_second': 4.657, 'epoch': 0.56}
{'loss': 1.0062, 'grad_norm': 0.16856494545936584, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2740434408187866, 'eval_runtime': 13.475, 'eval_samples_per_second': 74.211, 'eval_steps_per_second': 4.675, 'epoch': 0.6}
{'loss': 0.9359, 'grad_norm': 0.15220339596271515, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2750179767608643, 'eval_runtime': 11.6753, 'eval_samples_per_second': 85.651, 'eval_steps_per_second': 5.396, 'epoch': 0.64}
{'loss': 0.9369, 'grad_norm': 0.15473037958145142, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2716467380523682, 'eval_runtime': 13.0236, 'eval_samples_per_second': 76.784, 'eval_steps_per_second': 4.837, 'epoch': 0.68}
{'loss': 0.957, 'grad_norm': 0.1532914936542511, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2681244611740112, 'eval_runtime': 11.8779, 'eval_samples_per_second': 84.19, 'eval_steps_per_second': 5.304, 'epoch': 0.72}
{'loss': 0.9188, 'grad_norm': 0.18367329239845276, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2663216590881348, 'eval_runtime': 11.2691, 'eval_samples_per_second': 88.738, 'eval_steps_per_second': 5.59, 'epoch': 0.76}
{'loss': 0.9309, 'grad_norm': 0.17426440119743347, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2678872346878052, 'eval_runtime': 9.7885, 'eval_samples_per_second': 102.161, 'eval_steps_per_second': 6.436, 'epoch': 0.8}
{'loss': 0.9617, 'grad_norm': 0.182281032204628, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.264736294746399, 'eval_runtime': 9.6352, 'eval_samples_per_second': 103.786, 'eval_steps_per_second': 6.539, 'epoch': 0.84}
{'loss': 0.9681, 'grad_norm': 0.16072486340999603, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2641762495040894, 'eval_runtime': 9.6183, 'eval_samples_per_second': 103.968, 'eval_steps_per_second': 6.55, 'epoch': 0.88}
{'loss': 0.9431, 'grad_norm': 0.16477008163928986, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2625782489776611, 'eval_runtime': 9.6255, 'eval_samples_per_second': 103.89, 'eval_steps_per_second': 6.545, 'epoch': 0.92}
{'loss': 0.9107, 'grad_norm': 0.1761041283607483, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.262802243232727, 'eval_runtime': 9.6307, 'eval_samples_per_second': 103.835, 'eval_steps_per_second': 6.542, 'epoch': 0.96}
{'loss': 0.9411, 'grad_norm': 0.18109814822673798, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2626203298568726, 'eval_runtime': 9.6356, 'eval_samples_per_second': 103.782, 'eval_steps_per_second': 6.538, 'epoch': 1.0}
{'train_runtime': 608.5248, 'train_samples_per_second': 16.432, 'train_steps_per_second': 1.027, 'train_loss': 1.0408160278320313, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6700574159622192, 1.4167869091033936, 1.3357341289520264, 1.3224183320999146, 1.3087105751037598, 1.3051769733428955, 1.301025629043579, 1.2927134037017822, 1.2915103435516357, 1.2888915538787842, 1.2871776819229126, 1.2812772989273071, 1.2809783220291138, 1.2787110805511475, 1.2740434408187866, 1.2750179767608643, 1.2716467380523682, 1.2681244611740112, 1.2663216590881348, 1.2678872346878052, 1.264736294746399, 1.2641762495040894, 1.2625782489776611, 1.262802243232727, 1.2626203298568726], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6700574159622192, 1.4167869091033936, 1.3357341289520264, 1.3224183320999146, 1.3087105751037598, 1.3051769733428955, 1.301025629043579, 1.2927134037017822, 1.2915103435516357, 1.2888915538787842, 1.2871776819229126, 1.2812772989273071, 1.2809783220291138, 1.2787110805511475, 1.2740434408187866, 1.2750179767608643, 1.2716467380523682, 1.2681244611740112, 1.2663216590881348, 1.2678872346878052, 1.264736294746399, 1.2641762495040894, 1.2625782489776611, 1.262802243232727, 1.2626203298568726]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2626203298568726
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7381 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -1.0168587917179073
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -1.0179112861932356
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -1.0155431233841696
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -1.0179898917267733
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.9984721357801714
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1880, dtype=torch.float64), 0, 0, 0, tensor(0.5384, dtype=torch.float64), tensor(0.2736, dtype=torch.float64), 19, 1, 1, 1, 0, 1, 120, 0.0, 30.793208021204926, 0]
normalized proposed parameters for next round by BO: [tensor(2.1263e-17, dtype=torch.float64), tensor(4.1962e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1880, dtype=torch.float64), tensor(1.9583e-17, dtype=torch.float64), tensor(9.1103e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5384, dtype=torch.float64), tensor(0.2736, dtype=torch.float64), tensor(0.6013, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9336, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6415, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.188
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.538
  arc_challenge: 0.274

LoRA Parameters:
  lora_r: (120,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (30.793208021204926,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  120
lora dropout:  0.0
lora alpha:  30.793208021204926
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 114,401,280 || all params: 8,144,662,528 || trainable%: 1.4046
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8872, 'grad_norm': 0.4654143452644348, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7772529125213623, 'eval_runtime': 9.5214, 'eval_samples_per_second': 105.027, 'eval_steps_per_second': 6.617, 'epoch': 0.04}
{'loss': 1.4965, 'grad_norm': 0.17129194736480713, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4980589151382446, 'eval_runtime': 9.5595, 'eval_samples_per_second': 104.608, 'eval_steps_per_second': 6.59, 'epoch': 0.08}
{'loss': 1.3042, 'grad_norm': 0.17446960508823395, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3974406719207764, 'eval_runtime': 9.5814, 'eval_samples_per_second': 104.369, 'eval_steps_per_second': 6.575, 'epoch': 0.12}
{'loss': 1.2179, 'grad_norm': 0.20477358996868134, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3494704961776733, 'eval_runtime': 9.5645, 'eval_samples_per_second': 104.554, 'eval_steps_per_second': 6.587, 'epoch': 0.16}
{'loss': 1.1392, 'grad_norm': 0.1606036275625229, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.289075493812561, 'eval_runtime': 9.6333, 'eval_samples_per_second': 103.806, 'eval_steps_per_second': 6.54, 'epoch': 0.2}
{'loss': 1.1071, 'grad_norm': 0.17304696142673492, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2779719829559326, 'eval_runtime': 9.6544, 'eval_samples_per_second': 103.579, 'eval_steps_per_second': 6.526, 'epoch': 0.24}
{'loss': 1.0789, 'grad_norm': 0.16942492127418518, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2704968452453613, 'eval_runtime': 9.6952, 'eval_samples_per_second': 103.144, 'eval_steps_per_second': 6.498, 'epoch': 0.28}
{'loss': 1.11, 'grad_norm': 0.1803802102804184, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2665055990219116, 'eval_runtime': 9.6745, 'eval_samples_per_second': 103.365, 'eval_steps_per_second': 6.512, 'epoch': 0.32}
{'loss': 1.0902, 'grad_norm': 0.1828988939523697, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2611117362976074, 'eval_runtime': 9.6787, 'eval_samples_per_second': 103.32, 'eval_steps_per_second': 6.509, 'epoch': 0.36}
{'loss': 1.0566, 'grad_norm': 0.1826806664466858, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2581576108932495, 'eval_runtime': 9.6743, 'eval_samples_per_second': 103.366, 'eval_steps_per_second': 6.512, 'epoch': 0.4}
{'loss': 1.0822, 'grad_norm': 0.17156781256198883, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2548037767410278, 'eval_runtime': 9.6311, 'eval_samples_per_second': 103.83, 'eval_steps_per_second': 6.541, 'epoch': 0.44}
{'loss': 1.0595, 'grad_norm': 0.21145489811897278, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2559138536453247, 'eval_runtime': 9.6193, 'eval_samples_per_second': 103.957, 'eval_steps_per_second': 6.549, 'epoch': 0.48}
{'loss': 1.055, 'grad_norm': 0.1880553513765335, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2526845932006836, 'eval_runtime': 9.6377, 'eval_samples_per_second': 103.759, 'eval_steps_per_second': 6.537, 'epoch': 0.52}
{'loss': 1.0365, 'grad_norm': 0.196573406457901, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.247817873954773, 'eval_runtime': 9.6338, 'eval_samples_per_second': 103.801, 'eval_steps_per_second': 6.539, 'epoch': 0.56}
{'loss': 1.0108, 'grad_norm': 0.2311558574438095, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.246069073677063, 'eval_runtime': 9.6254, 'eval_samples_per_second': 103.892, 'eval_steps_per_second': 6.545, 'epoch': 0.6}
{'loss': 1.0562, 'grad_norm': 0.20234885811805725, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2454160451889038, 'eval_runtime': 9.6204, 'eval_samples_per_second': 103.946, 'eval_steps_per_second': 6.549, 'epoch': 0.64}
{'loss': 0.9878, 'grad_norm': 0.1988181173801422, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2461141347885132, 'eval_runtime': 9.6208, 'eval_samples_per_second': 103.941, 'eval_steps_per_second': 6.548, 'epoch': 0.68}
{'loss': 1.0713, 'grad_norm': 0.21933092176914215, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2407723665237427, 'eval_runtime': 9.6274, 'eval_samples_per_second': 103.87, 'eval_steps_per_second': 6.544, 'epoch': 0.72}
{'loss': 1.073, 'grad_norm': 0.23839785158634186, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2363600730895996, 'eval_runtime': 9.6124, 'eval_samples_per_second': 104.032, 'eval_steps_per_second': 6.554, 'epoch': 0.76}
{'loss': 1.0186, 'grad_norm': 0.2356971949338913, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2367318868637085, 'eval_runtime': 9.603, 'eval_samples_per_second': 104.134, 'eval_steps_per_second': 6.56, 'epoch': 0.8}
{'loss': 0.9747, 'grad_norm': 0.2348577231168747, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2338799238204956, 'eval_runtime': 9.6079, 'eval_samples_per_second': 104.081, 'eval_steps_per_second': 6.557, 'epoch': 0.84}
{'loss': 1.0065, 'grad_norm': 0.22059780359268188, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2336235046386719, 'eval_runtime': 9.6613, 'eval_samples_per_second': 103.506, 'eval_steps_per_second': 6.521, 'epoch': 0.88}
{'loss': 1.0499, 'grad_norm': 0.2651301324367523, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2334564924240112, 'eval_runtime': 9.6741, 'eval_samples_per_second': 103.369, 'eval_steps_per_second': 6.512, 'epoch': 0.92}
{'loss': 0.989, 'grad_norm': 0.20691430568695068, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2315443754196167, 'eval_runtime': 9.6835, 'eval_samples_per_second': 103.269, 'eval_steps_per_second': 6.506, 'epoch': 0.96}
{'loss': 1.0265, 'grad_norm': 0.2461269199848175, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2312357425689697, 'eval_runtime': 9.7136, 'eval_samples_per_second': 102.948, 'eval_steps_per_second': 6.486, 'epoch': 1.0}
{'train_runtime': 409.1696, 'train_samples_per_second': 24.435, 'train_steps_per_second': 1.527, 'train_loss': 1.1594130859375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7772529125213623, 1.4980589151382446, 1.3974406719207764, 1.3494704961776733, 1.289075493812561, 1.2779719829559326, 1.2704968452453613, 1.2665055990219116, 1.2611117362976074, 1.2581576108932495, 1.2548037767410278, 1.2559138536453247, 1.2526845932006836, 1.247817873954773, 1.246069073677063, 1.2454160451889038, 1.2461141347885132, 1.2407723665237427, 1.2363600730895996, 1.2367318868637085, 1.2338799238204956, 1.2336235046386719, 1.2334564924240112, 1.2315443754196167, 1.2312357425689697], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7772529125213623, 1.4980589151382446, 1.3974406719207764, 1.3494704961776733, 1.289075493812561, 1.2779719829559326, 1.2704968452453613, 1.2665055990219116, 1.2611117362976074, 1.2581576108932495, 1.2548037767410278, 1.2559138536453247, 1.2526845932006836, 1.247817873954773, 1.246069073677063, 1.2454160451889038, 1.2461141347885132, 1.2407723665237427, 1.2363600730895996, 1.2367318868637085, 1.2338799238204956, 1.2336235046386719, 1.2334564924240112, 1.2315443754196167, 1.2312357425689697]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2312357425689697
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2230 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -1.0376052256629555
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -1.0384811360871307
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -1.020164110396676
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -1.0375919565069576
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -1.0121573951021525
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0232, dtype=torch.float64), 0, tensor(0.0355, dtype=torch.float64), tensor(0.0671, dtype=torch.float64), tensor(0.6359, dtype=torch.float64), tensor(0.2383, dtype=torch.float64), 0, 0, 0, 25, 1, 1, 0, 0, 0, 49, 0.034983514628449536, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0232, dtype=torch.float64), tensor(1.4024e-18, dtype=torch.float64), tensor(0.0355, dtype=torch.float64), tensor(0.0671, dtype=torch.float64), tensor(0.6359, dtype=torch.float64), tensor(0.2383, dtype=torch.float64), tensor(3.7644e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7923, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3863, dtype=torch.float64), tensor(0.3498, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.023
  gsm8k: 0
  rowan_hellaswag: 0.035
  sciq: 0.067
  triviaqa: 0.636
  truthfulqa_gen: 0.238
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (49,)
  lora_dropout: (0.034983514628449536,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([1, 1, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 0]
lora rank:  49
lora dropout:  0.034983514628449536
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,307,200 || all params: 8,046,568,448 || trainable%: 0.2027
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.0851, 'grad_norm': 1.4778488874435425, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1768271923065186, 'eval_runtime': 9.0662, 'eval_samples_per_second': 110.3, 'eval_steps_per_second': 6.949, 'epoch': 0.04}
{'loss': 1.7368, 'grad_norm': 0.7062540054321289, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9556758403778076, 'eval_runtime': 9.1266, 'eval_samples_per_second': 109.57, 'eval_steps_per_second': 6.903, 'epoch': 0.08}
{'loss': 1.3626, 'grad_norm': 0.5583416819572449, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9808235168457031, 'eval_runtime': 9.1459, 'eval_samples_per_second': 109.339, 'eval_steps_per_second': 6.888, 'epoch': 0.12}
{'loss': 1.2321, 'grad_norm': 0.7144489288330078, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9599689245224, 'eval_runtime': 9.1783, 'eval_samples_per_second': 108.952, 'eval_steps_per_second': 6.864, 'epoch': 0.16}
{'loss': 1.1374, 'grad_norm': 0.6236286163330078, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.948373794555664, 'eval_runtime': 9.2097, 'eval_samples_per_second': 108.581, 'eval_steps_per_second': 6.841, 'epoch': 0.2}
{'loss': 1.1444, 'grad_norm': 0.5441719889640808, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.890882134437561, 'eval_runtime': 9.2039, 'eval_samples_per_second': 108.649, 'eval_steps_per_second': 6.845, 'epoch': 0.24}
{'loss': 1.05, 'grad_norm': 0.6278647780418396, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.854945182800293, 'eval_runtime': 9.2154, 'eval_samples_per_second': 108.515, 'eval_steps_per_second': 6.836, 'epoch': 0.28}
{'loss': 1.0651, 'grad_norm': 0.5696614384651184, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8300583362579346, 'eval_runtime': 9.2116, 'eval_samples_per_second': 108.559, 'eval_steps_per_second': 6.839, 'epoch': 0.32}
{'loss': 1.0869, 'grad_norm': 0.5624131560325623, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8774504661560059, 'eval_runtime': 9.2215, 'eval_samples_per_second': 108.442, 'eval_steps_per_second': 6.832, 'epoch': 0.36}
{'loss': 1.0048, 'grad_norm': 0.6197253465652466, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8805079460144043, 'eval_runtime': 9.2124, 'eval_samples_per_second': 108.549, 'eval_steps_per_second': 6.839, 'epoch': 0.4}
{'loss': 0.9979, 'grad_norm': 0.5390893220901489, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8599618673324585, 'eval_runtime': 9.2659, 'eval_samples_per_second': 107.923, 'eval_steps_per_second': 6.799, 'epoch': 0.44}
{'loss': 1.0218, 'grad_norm': 0.4862896800041199, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9635776281356812, 'eval_runtime': 9.2656, 'eval_samples_per_second': 107.926, 'eval_steps_per_second': 6.799, 'epoch': 0.48}
{'loss': 1.0189, 'grad_norm': 0.5038248300552368, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9413338899612427, 'eval_runtime': 9.2084, 'eval_samples_per_second': 108.597, 'eval_steps_per_second': 6.842, 'epoch': 0.52}
{'loss': 1.0161, 'grad_norm': 0.6055641770362854, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9120513200759888, 'eval_runtime': 9.2039, 'eval_samples_per_second': 108.65, 'eval_steps_per_second': 6.845, 'epoch': 0.56}
{'loss': 1.1006, 'grad_norm': 0.5739465951919556, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8810005187988281, 'eval_runtime': 9.2038, 'eval_samples_per_second': 108.65, 'eval_steps_per_second': 6.845, 'epoch': 0.6}
{'loss': 0.9637, 'grad_norm': 0.5307783484458923, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9131108522415161, 'eval_runtime': 9.1992, 'eval_samples_per_second': 108.705, 'eval_steps_per_second': 6.848, 'epoch': 0.64}
{'loss': 1.0539, 'grad_norm': 0.5729550123214722, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8816859722137451, 'eval_runtime': 9.2032, 'eval_samples_per_second': 108.658, 'eval_steps_per_second': 6.845, 'epoch': 0.68}
{'loss': 0.9378, 'grad_norm': 0.5417277216911316, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8954752683639526, 'eval_runtime': 9.2035, 'eval_samples_per_second': 108.655, 'eval_steps_per_second': 6.845, 'epoch': 0.72}
{'loss': 1.0332, 'grad_norm': 0.5002303123474121, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8795197010040283, 'eval_runtime': 9.1822, 'eval_samples_per_second': 108.906, 'eval_steps_per_second': 6.861, 'epoch': 0.76}
{'loss': 1.0512, 'grad_norm': 0.5031742453575134, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9036650657653809, 'eval_runtime': 9.1862, 'eval_samples_per_second': 108.859, 'eval_steps_per_second': 6.858, 'epoch': 0.8}
{'loss': 0.9621, 'grad_norm': 0.5496558547019958, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9332419633865356, 'eval_runtime': 9.1869, 'eval_samples_per_second': 108.851, 'eval_steps_per_second': 6.858, 'epoch': 0.84}
{'loss': 0.9961, 'grad_norm': 0.560958206653595, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9174797534942627, 'eval_runtime': 9.2145, 'eval_samples_per_second': 108.524, 'eval_steps_per_second': 6.837, 'epoch': 0.88}
{'loss': 0.9995, 'grad_norm': 0.5649930238723755, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9107062816619873, 'eval_runtime': 9.2084, 'eval_samples_per_second': 108.596, 'eval_steps_per_second': 6.842, 'epoch': 0.92}
{'loss': 0.9771, 'grad_norm': 0.6133633852005005, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9055715799331665, 'eval_runtime': 9.1757, 'eval_samples_per_second': 108.983, 'eval_steps_per_second': 6.866, 'epoch': 0.96}
{'loss': 0.9359, 'grad_norm': 0.6927974224090576, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.900427222251892, 'eval_runtime': 9.2019, 'eval_samples_per_second': 108.674, 'eval_steps_per_second': 6.846, 'epoch': 1.0}
{'train_runtime': 345.4235, 'train_samples_per_second': 28.944, 'train_steps_per_second': 1.809, 'train_loss': 1.1988420349121094, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1768271923065186, 1.9556758403778076, 1.9808235168457031, 1.9599689245224, 1.948373794555664, 1.890882134437561, 1.854945182800293, 1.8300583362579346, 1.8774504661560059, 1.8805079460144043, 1.8599618673324585, 1.9635776281356812, 1.9413338899612427, 1.9120513200759888, 1.8810005187988281, 1.9131108522415161, 1.8816859722137451, 1.8954752683639526, 1.8795197010040283, 1.9036650657653809, 1.9332419633865356, 1.9174797534942627, 1.9107062816619873, 1.9055715799331665, 1.900427222251892], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1768271923065186, 1.9556758403778076, 1.9808235168457031, 1.9599689245224, 1.948373794555664, 1.890882134437561, 1.854945182800293, 1.8300583362579346, 1.8774504661560059, 1.8805079460144043, 1.8599618673324585, 1.9635776281356812, 1.9413338899612427, 1.9120513200759888, 1.8810005187988281, 1.9131108522415161, 1.8816859722137451, 1.8954752683639526, 1.8795197010040283, 1.9036650657653809, 1.9332419633865356, 1.9174797534942627, 1.9107062816619873, 1.9055715799331665, 1.900427222251892]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.900427222251892
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 52.5719 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -1.014573664288646
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -1.0145713786432242
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -1.0145713765728692
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -1.0133009673087567
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -1.0145713780406411
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0385, dtype=torch.float64), 0, tensor(0.0865, dtype=torch.float64), 0, tensor(0.8750, dtype=torch.float64), 0, 0, 0, 18, 0, 1, 1, 1, 1, 84, 0.0, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0385, dtype=torch.float64), tensor(1.3797e-17, dtype=torch.float64), tensor(0.0865, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8750, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3696e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5590, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6553, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.039
  rowan_hellaswag: 0
  sciq: 0.086
  triviaqa: 0
  truthfulqa_gen: 0.875
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (84,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  84
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 91,348,992 || all params: 8,121,610,240 || trainable%: 1.1248
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.716, 'grad_norm': 0.45373040437698364, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.6976635456085205, 'eval_runtime': 11.7684, 'eval_samples_per_second': 84.973, 'eval_steps_per_second': 5.353, 'epoch': 0.04}
{'loss': 2.2128, 'grad_norm': 0.9175252914428711, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.2143359184265137, 'eval_runtime': 11.5357, 'eval_samples_per_second': 86.687, 'eval_steps_per_second': 5.461, 'epoch': 0.08}
{'loss': 1.2472, 'grad_norm': 0.25393015146255493, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.160076856613159, 'eval_runtime': 10.4147, 'eval_samples_per_second': 96.018, 'eval_steps_per_second': 6.049, 'epoch': 0.12}
{'loss': 1.1266, 'grad_norm': 0.19731201231479645, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1477322578430176, 'eval_runtime': 10.1054, 'eval_samples_per_second': 98.957, 'eval_steps_per_second': 6.234, 'epoch': 0.16}
{'loss': 1.0397, 'grad_norm': 0.25727754831314087, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.1933085918426514, 'eval_runtime': 10.15, 'eval_samples_per_second': 98.522, 'eval_steps_per_second': 6.207, 'epoch': 0.2}
{'loss': 0.9188, 'grad_norm': 0.11924327909946442, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.2163121700286865, 'eval_runtime': 10.1547, 'eval_samples_per_second': 98.477, 'eval_steps_per_second': 6.204, 'epoch': 0.24}
{'loss': 0.8268, 'grad_norm': 0.16914363205432892, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.243891954421997, 'eval_runtime': 10.1455, 'eval_samples_per_second': 98.566, 'eval_steps_per_second': 6.21, 'epoch': 0.28}
{'loss': 0.7397, 'grad_norm': 0.08704181015491486, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.2169361114501953, 'eval_runtime': 10.1129, 'eval_samples_per_second': 98.883, 'eval_steps_per_second': 6.23, 'epoch': 0.32}
{'loss': 0.7205, 'grad_norm': 0.10174811631441116, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.2685790061950684, 'eval_runtime': 10.2143, 'eval_samples_per_second': 97.901, 'eval_steps_per_second': 6.168, 'epoch': 0.36}
{'loss': 0.6821, 'grad_norm': 0.10058122128248215, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.2551958560943604, 'eval_runtime': 10.089, 'eval_samples_per_second': 99.118, 'eval_steps_per_second': 6.244, 'epoch': 0.4}
{'loss': 0.6487, 'grad_norm': 0.1001301109790802, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.2602646350860596, 'eval_runtime': 10.1269, 'eval_samples_per_second': 98.747, 'eval_steps_per_second': 6.221, 'epoch': 0.44}
{'loss': 0.6241, 'grad_norm': 0.1304222196340561, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.272895097732544, 'eval_runtime': 10.2624, 'eval_samples_per_second': 97.443, 'eval_steps_per_second': 6.139, 'epoch': 0.48}
{'loss': 0.6336, 'grad_norm': 0.1610521525144577, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.267021656036377, 'eval_runtime': 10.0637, 'eval_samples_per_second': 99.367, 'eval_steps_per_second': 6.26, 'epoch': 0.52}
{'loss': 0.5862, 'grad_norm': 0.16370373964309692, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.2982499599456787, 'eval_runtime': 10.0752, 'eval_samples_per_second': 99.253, 'eval_steps_per_second': 6.253, 'epoch': 0.56}
{'loss': 0.5645, 'grad_norm': 0.15248730778694153, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.307159185409546, 'eval_runtime': 10.0367, 'eval_samples_per_second': 99.634, 'eval_steps_per_second': 6.277, 'epoch': 0.6}
{'loss': 0.5685, 'grad_norm': 0.22052794694900513, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.3577821254730225, 'eval_runtime': 10.0276, 'eval_samples_per_second': 99.725, 'eval_steps_per_second': 6.283, 'epoch': 0.64}
{'loss': 0.5225, 'grad_norm': 0.22793903946876526, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.3632490634918213, 'eval_runtime': 10.0077, 'eval_samples_per_second': 99.923, 'eval_steps_per_second': 6.295, 'epoch': 0.68}
{'loss': 0.5163, 'grad_norm': 0.21408328413963318, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.4071898460388184, 'eval_runtime': 10.0211, 'eval_samples_per_second': 99.789, 'eval_steps_per_second': 6.287, 'epoch': 0.72}
{'loss': 0.5077, 'grad_norm': 0.19765056669712067, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.3841474056243896, 'eval_runtime': 10.009, 'eval_samples_per_second': 99.91, 'eval_steps_per_second': 6.294, 'epoch': 0.76}
{'loss': 0.4579, 'grad_norm': 0.27177664637565613, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.4085769653320312, 'eval_runtime': 10.0066, 'eval_samples_per_second': 99.935, 'eval_steps_per_second': 6.296, 'epoch': 0.8}
{'loss': 0.4368, 'grad_norm': 0.3040759265422821, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.4094159603118896, 'eval_runtime': 10.0235, 'eval_samples_per_second': 99.766, 'eval_steps_per_second': 6.285, 'epoch': 0.84}
{'loss': 0.4281, 'grad_norm': 0.2573443055152893, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.4278042316436768, 'eval_runtime': 10.0183, 'eval_samples_per_second': 99.818, 'eval_steps_per_second': 6.289, 'epoch': 0.88}
{'loss': 0.4191, 'grad_norm': 0.35504648089408875, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.4417495727539062, 'eval_runtime': 10.0227, 'eval_samples_per_second': 99.773, 'eval_steps_per_second': 6.286, 'epoch': 0.92}
{'loss': 0.3854, 'grad_norm': 0.28748443722724915, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.444873809814453, 'eval_runtime': 10.0229, 'eval_samples_per_second': 99.771, 'eval_steps_per_second': 6.286, 'epoch': 0.96}
{'loss': 0.3913, 'grad_norm': 0.2289970964193344, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.4472742080688477, 'eval_runtime': 9.9948, 'eval_samples_per_second': 100.052, 'eval_steps_per_second': 6.303, 'epoch': 1.0}
{'train_runtime': 405.3123, 'train_samples_per_second': 24.67, 'train_steps_per_second': 1.542, 'train_loss': 0.8768311508178711, 'epoch': 1.0}
train_results:  {'eval_loss': [2.6976635456085205, 2.2143359184265137, 2.160076856613159, 2.1477322578430176, 2.1933085918426514, 2.2163121700286865, 2.243891954421997, 2.2169361114501953, 2.2685790061950684, 2.2551958560943604, 2.2602646350860596, 2.272895097732544, 2.267021656036377, 2.2982499599456787, 2.307159185409546, 2.3577821254730225, 2.3632490634918213, 2.4071898460388184, 2.3841474056243896, 2.4085769653320312, 2.4094159603118896, 2.4278042316436768, 2.4417495727539062, 2.444873809814453, 2.4472742080688477], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.6976635456085205, 2.2143359184265137, 2.160076856613159, 2.1477322578430176, 2.1933085918426514, 2.2163121700286865, 2.243891954421997, 2.2169361114501953, 2.2685790061950684, 2.2551958560943604, 2.2602646350860596, 2.272895097732544, 2.267021656036377, 2.2982499599456787, 2.307159185409546, 2.3577821254730225, 2.3632490634918213, 2.4071898460388184, 2.3841474056243896, 2.4085769653320312, 2.4094159603118896, 2.4278042316436768, 2.4417495727539062, 2.444873809814453, 2.4472742080688477]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.4472742080688477
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.5927 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -1.002237356564662
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -1.002682964081949
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -1.0026829646985997
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -1.0135551288538633
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -1.0111149495734069
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0373, dtype=torch.float64), 0, 0, tensor(0.9627, dtype=torch.float64), 0, 0, 0, 14, 0, 1, 0, 0, 1, 128, 0.046733813955840336, 31.253608184671947, 0]
normalized proposed parameters for next round by BO: [tensor(9.3299e-18, dtype=torch.float64), tensor(6.9981e-18, dtype=torch.float64), tensor(0.0373, dtype=torch.float64), tensor(1.5699e-17, dtype=torch.float64), tensor(1.3364e-17, dtype=torch.float64), tensor(0.9627, dtype=torch.float64), tensor(6.2944e-18, dtype=torch.float64), tensor(9.8931e-18, dtype=torch.float64), tensor(7.3335e-18, dtype=torch.float64), tensor(0.4389, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4673, dtype=torch.float64), tensor(0.6511, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.037
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.963
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.046733813955840336,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (31.253608184671947,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  128
lora dropout:  0.046733813955840336
lora alpha:  31.253608184671947
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 42,205,184 || all params: 8,072,466,432 || trainable%: 0.5228
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.287, 'grad_norm': 0.5066545605659485, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.404721260070801, 'eval_runtime': 8.856, 'eval_samples_per_second': 112.917, 'eval_steps_per_second': 7.114, 'epoch': 0.04}
{'loss': 1.7669, 'grad_norm': 0.16899123787879944, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.228703498840332, 'eval_runtime': 8.9057, 'eval_samples_per_second': 112.288, 'eval_steps_per_second': 7.074, 'epoch': 0.08}
{'loss': 1.2944, 'grad_norm': 0.19054745137691498, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.2559380531311035, 'eval_runtime': 8.921, 'eval_samples_per_second': 112.095, 'eval_steps_per_second': 7.062, 'epoch': 0.12}
{'loss': 1.0992, 'grad_norm': 0.23420917987823486, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.2811474800109863, 'eval_runtime': 9.0362, 'eval_samples_per_second': 110.666, 'eval_steps_per_second': 6.972, 'epoch': 0.16}
{'loss': 0.9724, 'grad_norm': 0.2703292667865753, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.295039176940918, 'eval_runtime': 9.0097, 'eval_samples_per_second': 110.991, 'eval_steps_per_second': 6.992, 'epoch': 0.2}
{'loss': 0.7957, 'grad_norm': 0.2758059501647949, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.3344037532806396, 'eval_runtime': 9.0292, 'eval_samples_per_second': 110.752, 'eval_steps_per_second': 6.977, 'epoch': 0.24}
{'loss': 0.6846, 'grad_norm': 0.3221879303455353, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.373081922531128, 'eval_runtime': 9.0133, 'eval_samples_per_second': 110.947, 'eval_steps_per_second': 6.99, 'epoch': 0.28}
{'loss': 0.525, 'grad_norm': 0.32106849551200867, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.4173054695129395, 'eval_runtime': 9.0097, 'eval_samples_per_second': 110.992, 'eval_steps_per_second': 6.992, 'epoch': 0.32}
{'loss': 0.5305, 'grad_norm': 0.2636560797691345, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.466078758239746, 'eval_runtime': 9.0231, 'eval_samples_per_second': 110.826, 'eval_steps_per_second': 6.982, 'epoch': 0.36}
{'loss': 0.5104, 'grad_norm': 0.3142748773097992, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.511772871017456, 'eval_runtime': 9.0291, 'eval_samples_per_second': 110.754, 'eval_steps_per_second': 6.977, 'epoch': 0.4}
{'loss': 0.4749, 'grad_norm': 0.35419711470603943, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.5617785453796387, 'eval_runtime': 9.0209, 'eval_samples_per_second': 110.854, 'eval_steps_per_second': 6.984, 'epoch': 0.44}
{'loss': 0.4566, 'grad_norm': 0.2804528772830963, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.5217537879943848, 'eval_runtime': 9.0229, 'eval_samples_per_second': 110.83, 'eval_steps_per_second': 6.982, 'epoch': 0.48}
{'loss': 0.4696, 'grad_norm': 0.2725455164909363, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.515092372894287, 'eval_runtime': 9.0245, 'eval_samples_per_second': 110.809, 'eval_steps_per_second': 6.981, 'epoch': 0.52}
{'loss': 0.4336, 'grad_norm': 0.2687539756298065, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.608806610107422, 'eval_runtime': 9.0219, 'eval_samples_per_second': 110.841, 'eval_steps_per_second': 6.983, 'epoch': 0.56}
{'loss': 0.3992, 'grad_norm': 0.3237588107585907, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.601635694503784, 'eval_runtime': 9.0125, 'eval_samples_per_second': 110.957, 'eval_steps_per_second': 6.99, 'epoch': 0.6}
{'loss': 0.5037, 'grad_norm': 0.19184967875480652, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.575953960418701, 'eval_runtime': 9.0184, 'eval_samples_per_second': 110.884, 'eval_steps_per_second': 6.986, 'epoch': 0.64}
{'loss': 0.4285, 'grad_norm': 0.24036671221256256, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.526946783065796, 'eval_runtime': 9.0305, 'eval_samples_per_second': 110.735, 'eval_steps_per_second': 6.976, 'epoch': 0.68}
{'loss': 0.3463, 'grad_norm': 0.2753390073776245, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.565464735031128, 'eval_runtime': 9.0125, 'eval_samples_per_second': 110.957, 'eval_steps_per_second': 6.99, 'epoch': 0.72}
{'loss': 0.4413, 'grad_norm': 0.22406363487243652, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.6411452293395996, 'eval_runtime': 9.0102, 'eval_samples_per_second': 110.986, 'eval_steps_per_second': 6.992, 'epoch': 0.76}
{'loss': 0.3127, 'grad_norm': 0.2441467046737671, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.647862434387207, 'eval_runtime': 9.0473, 'eval_samples_per_second': 110.53, 'eval_steps_per_second': 6.963, 'epoch': 0.8}
{'loss': 0.3268, 'grad_norm': 0.1829775720834732, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.6968164443969727, 'eval_runtime': 9.0249, 'eval_samples_per_second': 110.804, 'eval_steps_per_second': 6.981, 'epoch': 0.84}
{'loss': 0.3938, 'grad_norm': 0.21372127532958984, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.7132081985473633, 'eval_runtime': 9.027, 'eval_samples_per_second': 110.779, 'eval_steps_per_second': 6.979, 'epoch': 0.88}
{'loss': 0.3597, 'grad_norm': 0.34113970398902893, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.6800224781036377, 'eval_runtime': 9.0156, 'eval_samples_per_second': 110.919, 'eval_steps_per_second': 6.988, 'epoch': 0.92}
{'loss': 0.2938, 'grad_norm': 0.23263607919216156, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.702643871307373, 'eval_runtime': 9.0114, 'eval_samples_per_second': 110.97, 'eval_steps_per_second': 6.991, 'epoch': 0.96}
{'loss': 0.3384, 'grad_norm': 0.21758611500263214, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.7203564643859863, 'eval_runtime': 9.0087, 'eval_samples_per_second': 111.003, 'eval_steps_per_second': 6.993, 'epoch': 1.0}
{'train_runtime': 313.5537, 'train_samples_per_second': 31.889, 'train_steps_per_second': 1.993, 'train_loss': 0.73779482421875, 'epoch': 1.0}
train_results:  {'eval_loss': [2.404721260070801, 2.228703498840332, 2.2559380531311035, 2.2811474800109863, 2.295039176940918, 2.3344037532806396, 2.373081922531128, 2.4173054695129395, 2.466078758239746, 2.511772871017456, 2.5617785453796387, 2.5217537879943848, 2.515092372894287, 2.608806610107422, 2.601635694503784, 2.575953960418701, 2.526946783065796, 2.565464735031128, 2.6411452293395996, 2.647862434387207, 2.6968164443969727, 2.7132081985473633, 2.6800224781036377, 2.702643871307373, 2.7203564643859863], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.404721260070801, 2.228703498840332, 2.2559380531311035, 2.2811474800109863, 2.295039176940918, 2.3344037532806396, 2.373081922531128, 2.4173054695129395, 2.466078758239746, 2.511772871017456, 2.5617785453796387, 2.5217537879943848, 2.515092372894287, 2.608806610107422, 2.601635694503784, 2.575953960418701, 2.526946783065796, 2.565464735031128, 2.6411452293395996, 2.647862434387207, 2.6968164443969727, 2.7132081985473633, 2.6800224781036377, 2.702643871307373, 2.7203564643859863]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.7203564643859863
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7732 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -0.9888389763924581
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -0.9893991695227387
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -1.092556255795056
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -0.988838978818016
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -0.9888389788180422
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.9461, dtype=torch.float64), tensor(0.0539, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 21, 1, 1, 1, 1, 0, 128, 0.0, 1.4800000190734885, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.9461, dtype=torch.float64), tensor(0.0539, dtype=torch.float64), tensor(9.2059e-18, dtype=torch.float64), tensor(2.7842e-17, dtype=torch.float64), tensor(1.8486e-17, dtype=torch.float64), tensor(6.8788e-18, dtype=torch.float64), tensor(2.4873e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6477, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.946
  rowan_hellaswag: 0.054
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734885,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734885
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 134,873,088 || all params: 8,165,134,336 || trainable%: 1.6518
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6326, 'grad_norm': 0.10846500098705292, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.832519054412842, 'eval_runtime': 9.7115, 'eval_samples_per_second': 102.97, 'eval_steps_per_second': 6.487, 'epoch': 0.04}
{'loss': 1.5805, 'grad_norm': 0.10929553955793381, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.2754268646240234, 'eval_runtime': 9.7022, 'eval_samples_per_second': 103.07, 'eval_steps_per_second': 6.493, 'epoch': 0.08}
{'loss': 1.1411, 'grad_norm': 0.055796828120946884, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0877113342285156, 'eval_runtime': 9.7239, 'eval_samples_per_second': 102.839, 'eval_steps_per_second': 6.479, 'epoch': 0.12}
{'loss': 1.0162, 'grad_norm': 0.03488151729106903, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9753234386444092, 'eval_runtime': 9.7329, 'eval_samples_per_second': 102.744, 'eval_steps_per_second': 6.473, 'epoch': 0.16}
{'loss': 0.9545, 'grad_norm': 0.035234369337558746, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0154173374176025, 'eval_runtime': 9.7582, 'eval_samples_per_second': 102.478, 'eval_steps_per_second': 6.456, 'epoch': 0.2}
{'loss': 0.9523, 'grad_norm': 0.030241619795560837, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.034843683242798, 'eval_runtime': 9.7694, 'eval_samples_per_second': 102.36, 'eval_steps_per_second': 6.449, 'epoch': 0.24}
{'loss': 0.9409, 'grad_norm': 0.035205088555812836, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.0379271507263184, 'eval_runtime': 9.7931, 'eval_samples_per_second': 102.112, 'eval_steps_per_second': 6.433, 'epoch': 0.28}
{'loss': 0.9318, 'grad_norm': 0.038966067135334015, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.101329803466797, 'eval_runtime': 9.7879, 'eval_samples_per_second': 102.167, 'eval_steps_per_second': 6.437, 'epoch': 0.32}
{'loss': 0.9357, 'grad_norm': 0.036292046308517456, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.099876880645752, 'eval_runtime': 9.8151, 'eval_samples_per_second': 101.884, 'eval_steps_per_second': 6.419, 'epoch': 0.36}
{'loss': 0.8993, 'grad_norm': 0.04336932301521301, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.1079189777374268, 'eval_runtime': 9.8436, 'eval_samples_per_second': 101.589, 'eval_steps_per_second': 6.4, 'epoch': 0.4}
{'loss': 0.9222, 'grad_norm': 0.03672574460506439, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.042649030685425, 'eval_runtime': 9.8608, 'eval_samples_per_second': 101.411, 'eval_steps_per_second': 6.389, 'epoch': 0.44}
{'loss': 0.9065, 'grad_norm': 0.03516297787427902, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.1236133575439453, 'eval_runtime': 9.895, 'eval_samples_per_second': 101.061, 'eval_steps_per_second': 6.367, 'epoch': 0.48}
{'loss': 0.9076, 'grad_norm': 0.03842072933912277, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.1281957626342773, 'eval_runtime': 9.8635, 'eval_samples_per_second': 101.384, 'eval_steps_per_second': 6.387, 'epoch': 0.52}
{'loss': 0.8838, 'grad_norm': 0.03967117890715599, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.136154890060425, 'eval_runtime': 9.8659, 'eval_samples_per_second': 101.359, 'eval_steps_per_second': 6.386, 'epoch': 0.56}
{'loss': 0.9155, 'grad_norm': 0.04024825617671013, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.1519575119018555, 'eval_runtime': 9.8964, 'eval_samples_per_second': 101.047, 'eval_steps_per_second': 6.366, 'epoch': 0.6}
{'loss': 0.8936, 'grad_norm': 0.04053281992673874, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.1490421295166016, 'eval_runtime': 9.8674, 'eval_samples_per_second': 101.344, 'eval_steps_per_second': 6.385, 'epoch': 0.64}
{'loss': 0.8589, 'grad_norm': 0.04151526466012001, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.1469926834106445, 'eval_runtime': 9.8733, 'eval_samples_per_second': 101.283, 'eval_steps_per_second': 6.381, 'epoch': 0.68}
{'loss': 0.8945, 'grad_norm': 0.03939994052052498, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.1930341720581055, 'eval_runtime': 9.8539, 'eval_samples_per_second': 101.483, 'eval_steps_per_second': 6.393, 'epoch': 0.72}
{'loss': 0.8866, 'grad_norm': 0.04286596551537514, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.1572816371917725, 'eval_runtime': 9.8491, 'eval_samples_per_second': 101.532, 'eval_steps_per_second': 6.397, 'epoch': 0.76}
{'loss': 0.8718, 'grad_norm': 0.04231532663106918, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.1621835231781006, 'eval_runtime': 9.8404, 'eval_samples_per_second': 101.622, 'eval_steps_per_second': 6.402, 'epoch': 0.8}
{'loss': 0.894, 'grad_norm': 0.04716877639293671, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.173288345336914, 'eval_runtime': 9.8456, 'eval_samples_per_second': 101.568, 'eval_steps_per_second': 6.399, 'epoch': 0.84}
{'loss': 0.8927, 'grad_norm': 0.041972190141677856, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.1755712032318115, 'eval_runtime': 9.8591, 'eval_samples_per_second': 101.429, 'eval_steps_per_second': 6.39, 'epoch': 0.88}
{'loss': 0.8656, 'grad_norm': 0.046907734125852585, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1882338523864746, 'eval_runtime': 9.8538, 'eval_samples_per_second': 101.483, 'eval_steps_per_second': 6.393, 'epoch': 0.92}
{'loss': 0.8632, 'grad_norm': 0.04684196785092354, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1818153858184814, 'eval_runtime': 9.8566, 'eval_samples_per_second': 101.455, 'eval_steps_per_second': 6.392, 'epoch': 0.96}
{'loss': 0.881, 'grad_norm': 0.046126894652843475, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.181793689727783, 'eval_runtime': 9.8542, 'eval_samples_per_second': 101.479, 'eval_steps_per_second': 6.393, 'epoch': 1.0}
{'train_runtime': 439.8683, 'train_samples_per_second': 22.732, 'train_steps_per_second': 1.421, 'train_loss': 1.0128923095703124, 'epoch': 1.0}
train_results:  {'eval_loss': [2.832519054412842, 2.2754268646240234, 2.0877113342285156, 1.9753234386444092, 2.0154173374176025, 2.034843683242798, 2.0379271507263184, 2.101329803466797, 2.099876880645752, 2.1079189777374268, 2.042649030685425, 2.1236133575439453, 2.1281957626342773, 2.136154890060425, 2.1519575119018555, 2.1490421295166016, 2.1469926834106445, 2.1930341720581055, 2.1572816371917725, 2.1621835231781006, 2.173288345336914, 2.1755712032318115, 2.1882338523864746, 2.1818153858184814, 2.181793689727783], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.832519054412842, 2.2754268646240234, 2.0877113342285156, 1.9753234386444092, 2.0154173374176025, 2.034843683242798, 2.0379271507263184, 2.101329803466797, 2.099876880645752, 2.1079189777374268, 2.042649030685425, 2.1236133575439453, 2.1281957626342773, 2.136154890060425, 2.1519575119018555, 2.1490421295166016, 2.1469926834106445, 2.1930341720581055, 2.1572816371917725, 2.1621835231781006, 2.173288345336914, 2.1755712032318115, 2.1882338523864746, 2.1818153858184814, 2.181793689727783]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.044987440109253
current iteration best possible eval_loss (full train run):  -2.181793689727783
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.044987440109253]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9874 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -1.0108782077898253
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -1.0108786016161437
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -1.0108494733537834
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -1.0108776982294598
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -0.980156910204155
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0581, dtype=torch.float64), 0, 0, tensor(0.9046, dtype=torch.float64), 0, tensor(0.0373, dtype=torch.float64), 0, 24, 0, 1, 1, 1, 1, 2, 8.721199550532644e-17, 20.743710849245453, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.7136e-17, dtype=torch.float64), tensor(0.0581, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9046, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0373, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7438, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(8.7212e-16, dtype=torch.float64), tensor(0.4322, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.058
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.905
  wikitext: 0
  mmlu: 0.037
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (8.721199550532644e-17,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (20.743710849245453,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  2
lora dropout:  8.721199550532644e-17
lora alpha:  20.743710849245453
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,899,968 || all params: 8,033,161,216 || trainable%: 0.0361
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4424, 'grad_norm': 6.952185153961182, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9783567190170288, 'eval_runtime': 10.1443, 'eval_samples_per_second': 98.578, 'eval_steps_per_second': 6.21, 'epoch': 0.04}
{'loss': 1.2946, 'grad_norm': 2.1961750984191895, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5157582759857178, 'eval_runtime': 10.2607, 'eval_samples_per_second': 97.459, 'eval_steps_per_second': 6.14, 'epoch': 0.08}
{'loss': 0.9802, 'grad_norm': 1.5753976106643677, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4037331342697144, 'eval_runtime': 10.2553, 'eval_samples_per_second': 97.511, 'eval_steps_per_second': 6.143, 'epoch': 0.12}
{'loss': 0.7845, 'grad_norm': 1.5536761283874512, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3776808977127075, 'eval_runtime': 10.251, 'eval_samples_per_second': 97.551, 'eval_steps_per_second': 6.146, 'epoch': 0.16}
{'loss': 0.7141, 'grad_norm': 1.662502408027649, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3561352491378784, 'eval_runtime': 10.2383, 'eval_samples_per_second': 97.673, 'eval_steps_per_second': 6.153, 'epoch': 0.2}
{'loss': 0.6461, 'grad_norm': 1.5511285066604614, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3486117124557495, 'eval_runtime': 10.2072, 'eval_samples_per_second': 97.97, 'eval_steps_per_second': 6.172, 'epoch': 0.24}
{'loss': 0.5583, 'grad_norm': 1.6310468912124634, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3574572801589966, 'eval_runtime': 10.2031, 'eval_samples_per_second': 98.009, 'eval_steps_per_second': 6.175, 'epoch': 0.28}
{'loss': 0.5578, 'grad_norm': 1.4678997993469238, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3371120691299438, 'eval_runtime': 10.1992, 'eval_samples_per_second': 98.047, 'eval_steps_per_second': 6.177, 'epoch': 0.32}
{'loss': 0.4865, 'grad_norm': 1.2722346782684326, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3486740589141846, 'eval_runtime': 10.2023, 'eval_samples_per_second': 98.017, 'eval_steps_per_second': 6.175, 'epoch': 0.36}
{'loss': 0.559, 'grad_norm': 1.2980111837387085, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3460332155227661, 'eval_runtime': 10.2276, 'eval_samples_per_second': 97.775, 'eval_steps_per_second': 6.16, 'epoch': 0.4}
{'loss': 0.4942, 'grad_norm': 1.038228988647461, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3340526819229126, 'eval_runtime': 10.2389, 'eval_samples_per_second': 97.667, 'eval_steps_per_second': 6.153, 'epoch': 0.44}
{'loss': 0.5394, 'grad_norm': 1.1026911735534668, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3302477598190308, 'eval_runtime': 10.2496, 'eval_samples_per_second': 97.565, 'eval_steps_per_second': 6.147, 'epoch': 0.48}
{'loss': 0.5406, 'grad_norm': 1.3524467945098877, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3314834833145142, 'eval_runtime': 10.2726, 'eval_samples_per_second': 97.347, 'eval_steps_per_second': 6.133, 'epoch': 0.52}
{'loss': 0.471, 'grad_norm': 1.0281351804733276, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3361543416976929, 'eval_runtime': 10.2966, 'eval_samples_per_second': 97.12, 'eval_steps_per_second': 6.119, 'epoch': 0.56}
{'loss': 0.5009, 'grad_norm': 1.0026044845581055, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3184813261032104, 'eval_runtime': 10.2916, 'eval_samples_per_second': 97.167, 'eval_steps_per_second': 6.122, 'epoch': 0.6}
{'loss': 0.5788, 'grad_norm': 0.9429407119750977, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.306331992149353, 'eval_runtime': 10.2687, 'eval_samples_per_second': 97.383, 'eval_steps_per_second': 6.135, 'epoch': 0.64}
{'loss': 0.5077, 'grad_norm': 0.728230357170105, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.320749044418335, 'eval_runtime': 10.3146, 'eval_samples_per_second': 96.95, 'eval_steps_per_second': 6.108, 'epoch': 0.68}
{'loss': 0.4502, 'grad_norm': 0.6696579456329346, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3027054071426392, 'eval_runtime': 10.3216, 'eval_samples_per_second': 96.884, 'eval_steps_per_second': 6.104, 'epoch': 0.72}
{'loss': 0.5169, 'grad_norm': 0.8739277124404907, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3082239627838135, 'eval_runtime': 10.2965, 'eval_samples_per_second': 97.12, 'eval_steps_per_second': 6.119, 'epoch': 0.76}
{'loss': 0.4833, 'grad_norm': 0.5366237759590149, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3099483251571655, 'eval_runtime': 10.293, 'eval_samples_per_second': 97.154, 'eval_steps_per_second': 6.121, 'epoch': 0.8}
{'loss': 0.4982, 'grad_norm': 0.5439663529396057, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3063912391662598, 'eval_runtime': 10.2677, 'eval_samples_per_second': 97.393, 'eval_steps_per_second': 6.136, 'epoch': 0.84}
{'loss': 0.5193, 'grad_norm': 0.7921960949897766, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3023850917816162, 'eval_runtime': 10.2961, 'eval_samples_per_second': 97.124, 'eval_steps_per_second': 6.119, 'epoch': 0.88}
{'loss': 0.5333, 'grad_norm': 0.759951651096344, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3021684885025024, 'eval_runtime': 10.2879, 'eval_samples_per_second': 97.202, 'eval_steps_per_second': 6.124, 'epoch': 0.92}
{'loss': 0.4552, 'grad_norm': 0.7185863852500916, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3034569025039673, 'eval_runtime': 10.3102, 'eval_samples_per_second': 96.991, 'eval_steps_per_second': 6.11, 'epoch': 0.96}
{'loss': 0.4401, 'grad_norm': 0.8669409155845642, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.302911400794983, 'eval_runtime': 10.2965, 'eval_samples_per_second': 97.12, 'eval_steps_per_second': 6.119, 'epoch': 1.0}
{'train_runtime': 421.0812, 'train_samples_per_second': 23.746, 'train_steps_per_second': 1.484, 'train_loss': 0.702110009765625, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9783567190170288, 1.5157582759857178, 1.4037331342697144, 1.3776808977127075, 1.3561352491378784, 1.3486117124557495, 1.3574572801589966, 1.3371120691299438, 1.3486740589141846, 1.3460332155227661, 1.3340526819229126, 1.3302477598190308, 1.3314834833145142, 1.3361543416976929, 1.3184813261032104, 1.306331992149353, 1.320749044418335, 1.3027054071426392, 1.3082239627838135, 1.3099483251571655, 1.3063912391662598, 1.3023850917816162, 1.3021684885025024, 1.3034569025039673, 1.302911400794983], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9783567190170288, 1.5157582759857178, 1.4037331342697144, 1.3776808977127075, 1.3561352491378784, 1.3486117124557495, 1.3574572801589966, 1.3371120691299438, 1.3486740589141846, 1.3460332155227661, 1.3340526819229126, 1.3302477598190308, 1.3314834833145142, 1.3361543416976929, 1.3184813261032104, 1.306331992149353, 1.320749044418335, 1.3027054071426392, 1.3082239627838135, 1.3099483251571655, 1.3063912391662598, 1.3023850917816162, 1.3021684885025024, 1.3034569025039673, 1.302911400794983]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.302911400794983
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.044987440109253, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3172 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -1.0016098993392144
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -1.0195268242736601
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -1.041818377060459
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -1.0195266589624998
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -1.028442387640085
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3245, dtype=torch.float64), 0, tensor(0.0928, dtype=torch.float64), tensor(0.2671, dtype=torch.float64), 0, 0, 0, 0, tensor(0.3156, dtype=torch.float64), 26, 0, 0, 1, 0, 1, 109, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.3245, dtype=torch.float64), tensor(1.3001e-18, dtype=torch.float64), tensor(0.0928, dtype=torch.float64), tensor(0.2671, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9714e-18, dtype=torch.float64), tensor(1.6500e-17, dtype=torch.float64), tensor(5.8155e-18, dtype=torch.float64), tensor(0.3156, dtype=torch.float64), tensor(0.8098, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8521, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.325
  gsm8k: 0
  rowan_hellaswag: 0.093
  sciq: 0.267
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.316

LoRA Parameters:
  lora_r: (109,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  109
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 104,472,576 || all params: 8,134,733,824 || trainable%: 1.2843
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1135, 'grad_norm': 0.7695331573486328, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.00812029838562, 'eval_runtime': 9.9663, 'eval_samples_per_second': 100.339, 'eval_steps_per_second': 6.321, 'epoch': 0.04}
{'loss': 1.3439, 'grad_norm': 0.2361031174659729, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9081698656082153, 'eval_runtime': 9.9933, 'eval_samples_per_second': 100.068, 'eval_steps_per_second': 6.304, 'epoch': 0.08}
{'loss': 1.1735, 'grad_norm': 0.2550217807292938, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8719996213912964, 'eval_runtime': 9.9689, 'eval_samples_per_second': 100.312, 'eval_steps_per_second': 6.32, 'epoch': 0.12}
{'loss': 1.0658, 'grad_norm': 0.2020031064748764, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8475068807601929, 'eval_runtime': 9.9911, 'eval_samples_per_second': 100.089, 'eval_steps_per_second': 6.306, 'epoch': 0.16}
{'loss': 1.068, 'grad_norm': 0.21504467725753784, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8108857870101929, 'eval_runtime': 10.0087, 'eval_samples_per_second': 99.913, 'eval_steps_per_second': 6.295, 'epoch': 0.2}
{'loss': 1.0245, 'grad_norm': 0.22906212508678436, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8327927589416504, 'eval_runtime': 10.0106, 'eval_samples_per_second': 99.894, 'eval_steps_per_second': 6.293, 'epoch': 0.24}
{'loss': 0.9941, 'grad_norm': 0.21598663926124573, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8480464220046997, 'eval_runtime': 10.0064, 'eval_samples_per_second': 99.936, 'eval_steps_per_second': 6.296, 'epoch': 0.28}
{'loss': 1.0182, 'grad_norm': 0.21896721422672272, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8613582849502563, 'eval_runtime': 10.022, 'eval_samples_per_second': 99.781, 'eval_steps_per_second': 6.286, 'epoch': 0.32}
{'loss': 0.9854, 'grad_norm': 0.22353579103946686, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8449538946151733, 'eval_runtime': 10.014, 'eval_samples_per_second': 99.86, 'eval_steps_per_second': 6.291, 'epoch': 0.36}
{'loss': 0.999, 'grad_norm': 0.2045876532793045, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9011379480361938, 'eval_runtime': 10.0139, 'eval_samples_per_second': 99.861, 'eval_steps_per_second': 6.291, 'epoch': 0.4}
{'loss': 0.8739, 'grad_norm': 0.24409283697605133, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9317373037338257, 'eval_runtime': 10.0183, 'eval_samples_per_second': 99.817, 'eval_steps_per_second': 6.288, 'epoch': 0.44}
{'loss': 0.9513, 'grad_norm': 0.22172468900680542, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9657150506973267, 'eval_runtime': 10.0189, 'eval_samples_per_second': 99.811, 'eval_steps_per_second': 6.288, 'epoch': 0.48}
{'loss': 0.9328, 'grad_norm': 0.25987109541893005, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9680733680725098, 'eval_runtime': 10.0715, 'eval_samples_per_second': 99.29, 'eval_steps_per_second': 6.255, 'epoch': 0.52}
{'loss': 0.8876, 'grad_norm': 0.2411578744649887, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.0263760089874268, 'eval_runtime': 10.0922, 'eval_samples_per_second': 99.087, 'eval_steps_per_second': 6.242, 'epoch': 0.56}
{'loss': 0.9175, 'grad_norm': 0.2612130045890808, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9481427669525146, 'eval_runtime': 10.0987, 'eval_samples_per_second': 99.023, 'eval_steps_per_second': 6.238, 'epoch': 0.6}
{'loss': 0.9371, 'grad_norm': 0.3560026288032532, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9970661401748657, 'eval_runtime': 10.1263, 'eval_samples_per_second': 98.753, 'eval_steps_per_second': 6.221, 'epoch': 0.64}
{'loss': 0.828, 'grad_norm': 0.2874782085418701, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9765061140060425, 'eval_runtime': 10.1434, 'eval_samples_per_second': 98.586, 'eval_steps_per_second': 6.211, 'epoch': 0.68}
{'loss': 0.948, 'grad_norm': 0.26892268657684326, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.956571102142334, 'eval_runtime': 10.1548, 'eval_samples_per_second': 98.475, 'eval_steps_per_second': 6.204, 'epoch': 0.72}
{'loss': 0.9169, 'grad_norm': 0.2839096784591675, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.002340078353882, 'eval_runtime': 10.1794, 'eval_samples_per_second': 98.238, 'eval_steps_per_second': 6.189, 'epoch': 0.76}
{'loss': 0.9213, 'grad_norm': 0.3231736421585083, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9872421026229858, 'eval_runtime': 10.1195, 'eval_samples_per_second': 98.819, 'eval_steps_per_second': 6.226, 'epoch': 0.8}
{'loss': 0.826, 'grad_norm': 0.311279296875, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0010969638824463, 'eval_runtime': 10.1056, 'eval_samples_per_second': 98.955, 'eval_steps_per_second': 6.234, 'epoch': 0.84}
{'loss': 0.8239, 'grad_norm': 0.344988614320755, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0373995304107666, 'eval_runtime': 10.1096, 'eval_samples_per_second': 98.916, 'eval_steps_per_second': 6.232, 'epoch': 0.88}
{'loss': 0.7902, 'grad_norm': 0.2923027575016022, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0237960815429688, 'eval_runtime': 10.0863, 'eval_samples_per_second': 99.144, 'eval_steps_per_second': 6.246, 'epoch': 0.92}
{'loss': 0.807, 'grad_norm': 0.32210519909858704, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0503058433532715, 'eval_runtime': 10.0839, 'eval_samples_per_second': 99.168, 'eval_steps_per_second': 6.248, 'epoch': 0.96}
{'loss': 0.8306, 'grad_norm': 0.2759450078010559, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.0475850105285645, 'eval_runtime': 10.0938, 'eval_samples_per_second': 99.071, 'eval_steps_per_second': 6.241, 'epoch': 1.0}
{'train_runtime': 437.7913, 'train_samples_per_second': 22.837, 'train_steps_per_second': 1.428, 'train_loss': 1.0391245422363282, 'epoch': 1.0}
train_results:  {'eval_loss': [2.00812029838562, 1.9081698656082153, 1.8719996213912964, 1.8475068807601929, 1.8108857870101929, 1.8327927589416504, 1.8480464220046997, 1.8613582849502563, 1.8449538946151733, 1.9011379480361938, 1.9317373037338257, 1.9657150506973267, 1.9680733680725098, 2.0263760089874268, 1.9481427669525146, 1.9970661401748657, 1.9765061140060425, 1.956571102142334, 2.002340078353882, 1.9872421026229858, 2.0010969638824463, 2.0373995304107666, 2.0237960815429688, 2.0503058433532715, 2.0475850105285645], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.00812029838562, 1.9081698656082153, 1.8719996213912964, 1.8475068807601929, 1.8108857870101929, 1.8327927589416504, 1.8480464220046997, 1.8613582849502563, 1.8449538946151733, 1.9011379480361938, 1.9317373037338257, 1.9657150506973267, 1.9680733680725098, 2.0263760089874268, 1.9481427669525146, 1.9970661401748657, 1.9765061140060425, 1.956571102142334, 2.002340078353882, 1.9872421026229858, 2.0010969638824463, 2.0373995304107666, 2.0237960815429688, 2.0503058433532715, 2.0475850105285645]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.0475850105285645
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.044987440109253, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.9360 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.0303067112978361
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.0303065997159557
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.0303066095027624
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.0303066017670797
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.030306981273262
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0343, dtype=torch.float64), tensor(0.1017, dtype=torch.float64), tensor(0.7373, dtype=torch.float64), tensor(0.0846, dtype=torch.float64), tensor(0.0421, dtype=torch.float64), 0, 0, 14, 0, 1, 1, 1, 1, 128, 0.0, 31.809795488232613, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(2.5952e-19, dtype=torch.float64), tensor(0.0343, dtype=torch.float64), tensor(0.1017, dtype=torch.float64), tensor(0.7373, dtype=torch.float64), tensor(0.0846, dtype=torch.float64), tensor(0.0421, dtype=torch.float64), tensor(1.1472e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4438, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6627, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.034
  sciq: 0.102
  triviaqa: 0.737
  truthfulqa_gen: 0.085
  wikitext: 0.042
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (31.809795488232613,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  31.809795488232613
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 108,265,472 || all params: 8,138,526,720 || trainable%: 1.3303
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9047, 'grad_norm': 0.973190426826477, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.092278242111206, 'eval_runtime': 9.4223, 'eval_samples_per_second': 106.131, 'eval_steps_per_second': 6.686, 'epoch': 0.04}
{'loss': 1.6149, 'grad_norm': 0.4171695113182068, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.99220609664917, 'eval_runtime': 9.4795, 'eval_samples_per_second': 105.491, 'eval_steps_per_second': 6.646, 'epoch': 0.08}
{'loss': 1.328, 'grad_norm': 0.2501351237297058, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0613133907318115, 'eval_runtime': 9.4916, 'eval_samples_per_second': 105.357, 'eval_steps_per_second': 6.637, 'epoch': 0.12}
{'loss': 1.2032, 'grad_norm': 0.24513162672519684, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0259785652160645, 'eval_runtime': 9.5237, 'eval_samples_per_second': 105.001, 'eval_steps_per_second': 6.615, 'epoch': 0.16}
{'loss': 1.1611, 'grad_norm': 0.24207067489624023, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0624122619628906, 'eval_runtime': 9.5581, 'eval_samples_per_second': 104.623, 'eval_steps_per_second': 6.591, 'epoch': 0.2}
{'loss': 1.1817, 'grad_norm': 0.24875299632549286, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.0766918659210205, 'eval_runtime': 9.5454, 'eval_samples_per_second': 104.763, 'eval_steps_per_second': 6.6, 'epoch': 0.24}
{'loss': 1.2171, 'grad_norm': 0.2503467798233032, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.100843906402588, 'eval_runtime': 9.5785, 'eval_samples_per_second': 104.401, 'eval_steps_per_second': 6.577, 'epoch': 0.28}
{'loss': 1.149, 'grad_norm': 0.20901475846767426, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.089817762374878, 'eval_runtime': 9.5906, 'eval_samples_per_second': 104.269, 'eval_steps_per_second': 6.569, 'epoch': 0.32}
{'loss': 1.151, 'grad_norm': 0.21310311555862427, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.094280242919922, 'eval_runtime': 9.578, 'eval_samples_per_second': 104.406, 'eval_steps_per_second': 6.578, 'epoch': 0.36}
{'loss': 1.152, 'grad_norm': 0.19600683450698853, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.0625391006469727, 'eval_runtime': 9.5712, 'eval_samples_per_second': 104.48, 'eval_steps_per_second': 6.582, 'epoch': 0.4}
{'loss': 1.225, 'grad_norm': 0.2535679042339325, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.115513801574707, 'eval_runtime': 9.5674, 'eval_samples_per_second': 104.522, 'eval_steps_per_second': 6.585, 'epoch': 0.44}
{'loss': 1.1304, 'grad_norm': 0.20974524319171906, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.0667965412139893, 'eval_runtime': 9.5759, 'eval_samples_per_second': 104.429, 'eval_steps_per_second': 6.579, 'epoch': 0.48}
{'loss': 1.0974, 'grad_norm': 0.2756505310535431, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.0891575813293457, 'eval_runtime': 9.5722, 'eval_samples_per_second': 104.469, 'eval_steps_per_second': 6.582, 'epoch': 0.52}
{'loss': 1.2019, 'grad_norm': 0.1970885992050171, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.095818281173706, 'eval_runtime': 9.5503, 'eval_samples_per_second': 104.709, 'eval_steps_per_second': 6.597, 'epoch': 0.56}
{'loss': 1.1227, 'grad_norm': 0.24366073310375214, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.1274452209472656, 'eval_runtime': 9.5449, 'eval_samples_per_second': 104.768, 'eval_steps_per_second': 6.6, 'epoch': 0.6}
{'loss': 1.1759, 'grad_norm': 0.23174789547920227, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.0992977619171143, 'eval_runtime': 9.5441, 'eval_samples_per_second': 104.777, 'eval_steps_per_second': 6.601, 'epoch': 0.64}
{'loss': 1.1878, 'grad_norm': 0.23497378826141357, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.0762202739715576, 'eval_runtime': 9.5398, 'eval_samples_per_second': 104.824, 'eval_steps_per_second': 6.604, 'epoch': 0.68}
{'loss': 1.1805, 'grad_norm': 0.21220965683460236, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.098112106323242, 'eval_runtime': 9.5731, 'eval_samples_per_second': 104.459, 'eval_steps_per_second': 6.581, 'epoch': 0.72}
{'loss': 1.106, 'grad_norm': 0.2597077190876007, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.1046063899993896, 'eval_runtime': 9.5588, 'eval_samples_per_second': 104.615, 'eval_steps_per_second': 6.591, 'epoch': 0.76}
{'loss': 1.137, 'grad_norm': 0.2886195480823517, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.097259283065796, 'eval_runtime': 9.5475, 'eval_samples_per_second': 104.739, 'eval_steps_per_second': 6.599, 'epoch': 0.8}
{'loss': 1.1315, 'grad_norm': 0.21318134665489197, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0716001987457275, 'eval_runtime': 9.5391, 'eval_samples_per_second': 104.832, 'eval_steps_per_second': 6.604, 'epoch': 0.84}
{'loss': 1.1581, 'grad_norm': 0.2484777569770813, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.061605930328369, 'eval_runtime': 9.5383, 'eval_samples_per_second': 104.84, 'eval_steps_per_second': 6.605, 'epoch': 0.88}
{'loss': 1.1506, 'grad_norm': 0.22505420446395874, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.084150791168213, 'eval_runtime': 9.5596, 'eval_samples_per_second': 104.607, 'eval_steps_per_second': 6.59, 'epoch': 0.92}
{'loss': 1.1333, 'grad_norm': 0.243069127202034, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0822088718414307, 'eval_runtime': 9.5853, 'eval_samples_per_second': 104.327, 'eval_steps_per_second': 6.573, 'epoch': 0.96}
{'loss': 1.1392, 'grad_norm': 0.25434616208076477, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.077538013458252, 'eval_runtime': 9.5768, 'eval_samples_per_second': 104.419, 'eval_steps_per_second': 6.578, 'epoch': 1.0}
{'train_runtime': 339.3046, 'train_samples_per_second': 29.463, 'train_steps_per_second': 1.842, 'train_loss': 1.2936046295166015, 'epoch': 1.0}
train_results:  {'eval_loss': [2.092278242111206, 1.99220609664917, 2.0613133907318115, 2.0259785652160645, 2.0624122619628906, 2.0766918659210205, 2.100843906402588, 2.089817762374878, 2.094280242919922, 2.0625391006469727, 2.115513801574707, 2.0667965412139893, 2.0891575813293457, 2.095818281173706, 2.1274452209472656, 2.0992977619171143, 2.0762202739715576, 2.098112106323242, 2.1046063899993896, 2.097259283065796, 2.0716001987457275, 2.061605930328369, 2.084150791168213, 2.0822088718414307, 2.077538013458252], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.092278242111206, 1.99220609664917, 2.0613133907318115, 2.0259785652160645, 2.0624122619628906, 2.0766918659210205, 2.100843906402588, 2.089817762374878, 2.094280242919922, 2.0625391006469727, 2.115513801574707, 2.0667965412139893, 2.0891575813293457, 2.095818281173706, 2.1274452209472656, 2.0992977619171143, 2.0762202739715576, 2.098112106323242, 2.1046063899993896, 2.097259283065796, 2.0716001987457275, 2.061605930328369, 2.084150791168213, 2.0822088718414307, 2.077538013458252]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.077538013458252
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.044987440109253, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.0603 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -1.0362514019564544
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -1.036261518752195
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -1.036657583679177
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -1.0362615187549937
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -1.036261518752195
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1666, dtype=torch.float64), tensor(0.0404, dtype=torch.float64), tensor(0.6109, dtype=torch.float64), tensor(0.1816, dtype=torch.float64), 0, 0, 0, 0, 32, 0, 1, 1, 1, 0, 79, 5.2054056750871866e-20, 36.8693598486112, 0]
normalized proposed parameters for next round by BO: [tensor(7.9528e-18, dtype=torch.float64), tensor(0.1666, dtype=torch.float64), tensor(0.0404, dtype=torch.float64), tensor(0.6109, dtype=torch.float64), tensor(0.1816, dtype=torch.float64), tensor(3.0954e-18, dtype=torch.float64), tensor(1.4340e-17, dtype=torch.float64), tensor(1.5059e-17, dtype=torch.float64), tensor(0.0005, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6172, dtype=torch.float64), tensor(5.2054e-19, dtype=torch.float64), tensor(0.7681, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.167
  rowan_hellaswag: 0.04
  sciq: 0.611
  triviaqa: 0.182
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (79,)
  lora_dropout: (5.2054056750871866e-20,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (36.8693598486112,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  79
lora dropout:  5.2054056750871866e-20
lora alpha:  36.8693598486112
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 106,135,552 || all params: 8,136,396,800 || trainable%: 1.3045
length of training data:  9993
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.812, 'grad_norm': 0.8551616668701172, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9262388944625854, 'eval_runtime': 10.6329, 'eval_samples_per_second': 94.047, 'eval_steps_per_second': 5.925, 'epoch': 0.04}
{'loss': 1.0959, 'grad_norm': 0.6515679955482483, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.8795818090438843, 'eval_runtime': 10.6316, 'eval_samples_per_second': 94.059, 'eval_steps_per_second': 5.926, 'epoch': 0.08}
{'loss': 0.9748, 'grad_norm': 0.3952619731426239, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8611987829208374, 'eval_runtime': 10.6621, 'eval_samples_per_second': 93.79, 'eval_steps_per_second': 5.909, 'epoch': 0.12}
{'loss': 0.9481, 'grad_norm': 0.32579270005226135, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9064650535583496, 'eval_runtime': 10.679, 'eval_samples_per_second': 93.642, 'eval_steps_per_second': 5.899, 'epoch': 0.16}
{'loss': 0.8984, 'grad_norm': 0.30522599816322327, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9185186624526978, 'eval_runtime': 10.6324, 'eval_samples_per_second': 94.052, 'eval_steps_per_second': 5.925, 'epoch': 0.2}
{'loss': 0.909, 'grad_norm': 0.2515595555305481, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.880360722541809, 'eval_runtime': 10.6382, 'eval_samples_per_second': 94.001, 'eval_steps_per_second': 5.922, 'epoch': 0.24}
{'loss': 0.8992, 'grad_norm': 0.3072986900806427, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8958677053451538, 'eval_runtime': 10.6248, 'eval_samples_per_second': 94.119, 'eval_steps_per_second': 5.93, 'epoch': 0.28}
{'loss': 0.9025, 'grad_norm': 0.31011515855789185, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8677129745483398, 'eval_runtime': 10.6322, 'eval_samples_per_second': 94.054, 'eval_steps_per_second': 5.925, 'epoch': 0.32}
{'loss': 0.8916, 'grad_norm': 0.3314281702041626, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.904220461845398, 'eval_runtime': 10.6398, 'eval_samples_per_second': 93.986, 'eval_steps_per_second': 5.921, 'epoch': 0.36}
{'loss': 0.9273, 'grad_norm': 0.4193141758441925, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8959606885910034, 'eval_runtime': 10.6845, 'eval_samples_per_second': 93.593, 'eval_steps_per_second': 5.896, 'epoch': 0.4}
{'loss': 0.856, 'grad_norm': 0.2743833065032959, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.90664541721344, 'eval_runtime': 10.6941, 'eval_samples_per_second': 93.51, 'eval_steps_per_second': 5.891, 'epoch': 0.44}
{'loss': 0.9339, 'grad_norm': 0.3374572992324829, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8847285509109497, 'eval_runtime': 10.7295, 'eval_samples_per_second': 93.201, 'eval_steps_per_second': 5.872, 'epoch': 0.48}
{'loss': 0.9446, 'grad_norm': 0.2749699652194977, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8941792249679565, 'eval_runtime': 10.7387, 'eval_samples_per_second': 93.121, 'eval_steps_per_second': 5.867, 'epoch': 0.52}
{'loss': 0.8125, 'grad_norm': 0.31355351209640503, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9280505180358887, 'eval_runtime': 10.7143, 'eval_samples_per_second': 93.333, 'eval_steps_per_second': 5.88, 'epoch': 0.56}
{'loss': 0.8861, 'grad_norm': 0.30175262689590454, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9255133867263794, 'eval_runtime': 10.7097, 'eval_samples_per_second': 93.374, 'eval_steps_per_second': 5.883, 'epoch': 0.6}
{'loss': 0.8996, 'grad_norm': 0.3681422472000122, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9213850498199463, 'eval_runtime': 10.6991, 'eval_samples_per_second': 93.466, 'eval_steps_per_second': 5.888, 'epoch': 0.64}
{'loss': 0.8791, 'grad_norm': 0.3410551846027374, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9154208898544312, 'eval_runtime': 10.6868, 'eval_samples_per_second': 93.573, 'eval_steps_per_second': 5.895, 'epoch': 0.68}
{'loss': 0.8437, 'grad_norm': 0.32372915744781494, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9270071983337402, 'eval_runtime': 10.7001, 'eval_samples_per_second': 93.457, 'eval_steps_per_second': 5.888, 'epoch': 0.72}
{'loss': 0.912, 'grad_norm': 0.2891490161418915, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.904389500617981, 'eval_runtime': 10.6868, 'eval_samples_per_second': 93.573, 'eval_steps_per_second': 5.895, 'epoch': 0.76}
{'loss': 0.8441, 'grad_norm': 0.26591941714286804, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.929019808769226, 'eval_runtime': 10.706, 'eval_samples_per_second': 93.405, 'eval_steps_per_second': 5.885, 'epoch': 0.8}
{'loss': 0.8683, 'grad_norm': 0.37551581859588623, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9094038009643555, 'eval_runtime': 10.7006, 'eval_samples_per_second': 93.452, 'eval_steps_per_second': 5.888, 'epoch': 0.84}
{'loss': 0.8578, 'grad_norm': 0.26730722188949585, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9075794219970703, 'eval_runtime': 10.6996, 'eval_samples_per_second': 93.461, 'eval_steps_per_second': 5.888, 'epoch': 0.88}
{'loss': 0.8523, 'grad_norm': 0.3992895185947418, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9141716957092285, 'eval_runtime': 10.6891, 'eval_samples_per_second': 93.553, 'eval_steps_per_second': 5.894, 'epoch': 0.92}
{'loss': 0.8435, 'grad_norm': 0.25759127736091614, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9167317152023315, 'eval_runtime': 10.6811, 'eval_samples_per_second': 93.624, 'eval_steps_per_second': 5.898, 'epoch': 0.96}
{'loss': 0.8639, 'grad_norm': 0.37230977416038513, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9168633222579956, 'eval_runtime': 10.6833, 'eval_samples_per_second': 93.604, 'eval_steps_per_second': 5.897, 'epoch': 1.0}
{'train_runtime': 502.7601, 'train_samples_per_second': 19.876, 'train_steps_per_second': 1.243, 'train_loss': 0.9742487274169922, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9262388944625854, 1.8795818090438843, 1.8611987829208374, 1.9064650535583496, 1.9185186624526978, 1.880360722541809, 1.8958677053451538, 1.8677129745483398, 1.904220461845398, 1.8959606885910034, 1.90664541721344, 1.8847285509109497, 1.8941792249679565, 1.9280505180358887, 1.9255133867263794, 1.9213850498199463, 1.9154208898544312, 1.9270071983337402, 1.904389500617981, 1.929019808769226, 1.9094038009643555, 1.9075794219970703, 1.9141716957092285, 1.9167317152023315, 1.9168633222579956], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9262388944625854, 1.8795818090438843, 1.8611987829208374, 1.9064650535583496, 1.9185186624526978, 1.880360722541809, 1.8958677053451538, 1.8677129745483398, 1.904220461845398, 1.8959606885910034, 1.90664541721344, 1.8847285509109497, 1.8941792249679565, 1.9280505180358887, 1.9255133867263794, 1.9213850498199463, 1.9154208898544312, 1.9270071983337402, 1.904389500617981, 1.929019808769226, 1.9094038009643555, 1.9075794219970703, 1.9141716957092285, 1.9167317152023315, 1.9168633222579956]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.9168633222579956
max eval_loss so far:  -1.2048003673553467
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.044987440109253, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.0126 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -1.0064877930107572
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -1.006292593994205
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -1.0064844112861229
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -1.0057148184110725
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -1.0064823040716875
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0531, dtype=torch.float64), 0, 0, tensor(0.1390, dtype=torch.float64), tensor(0.2806, dtype=torch.float64), tensor(0.0271, dtype=torch.float64), tensor(0.5003, dtype=torch.float64), 16, 0, 0, 1, 0, 1, 2, 0.0345514451094176, 27.149705834536086, 0]
normalized proposed parameters for next round by BO: [tensor(1.1029e-17, dtype=torch.float64), tensor(1.0274e-18, dtype=torch.float64), tensor(0.0531, dtype=torch.float64), tensor(1.4639e-19, dtype=torch.float64), tensor(6.8950e-18, dtype=torch.float64), tensor(0.1390, dtype=torch.float64), tensor(0.2806, dtype=torch.float64), tensor(0.0271, dtype=torch.float64), tensor(0.5003, dtype=torch.float64), tensor(0.4893, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.3455, dtype=torch.float64), tensor(0.5656, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-2.1192641258239746, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
mmlu
evaluation dataset:
data domain:  mmlu  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/mmlu/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.3161191096028816, 0.11426349731754912, 0.08683311140461922, 0.010648816854681904, 0.21071291050026175, 0.09848450659553602, 0.03252722732441375, 0.03858444462759559, 0.09182637577246108, 14, 1, 1, 1, 1, 0, 22, 0.04375408122258204, 13, 0]
Checking history sample input_X_between_0_1:  [0.3161191096028816, 0.11426349731754912, 0.08683311140461922, 0.010648816854681904, 0.21071291050026175, 0.09848450659553602, 0.03252722732441375, 0.03858444462759559, 0.09182637577246108, 0.4375, 1.0, 1.0, 1.0, 1.0, 0.0, 0.171875, 0.4375408122258204, 0.2708333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1066114902496338
Checking history sample input_X:  [0.17001858590456562, 0.06482605897458003, 0.0552231822789883, 0.2005042167465889, 0.1402610441334454, 0.22905697283425983, 0.05485942451788231, 0.033879744874935516, 0.05137076973475401, 21, 0, 1, 1, 1, 1, 71, 0.008194930202178574, 20, 0]
Checking history sample input_X_between_0_1:  [0.17001858590456562, 0.06482605897458003, 0.0552231822789883, 0.2005042167465889, 0.1402610441334454, 0.22905697283425983, 0.05485942451788231, 0.033879744874935516, 0.05137076973475401, 0.65625, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5546875, 0.08194930202178573, 0.4166666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9399064183235168
Checking history sample input_X:  [0.08457531624975921, 0.03444227766767646, 0.15033113821265595, 0.2888075957037953, 0.10483406652162627, 0.11125953049519431, 0.08585722091652723, 0.13154360559344933, 0.008349248639315885, 4, 0, 1, 1, 0, 0, 111, 0.08837128439087288, 42, 1]
Checking history sample input_X_between_0_1:  [0.08457531624975921, 0.03444227766767646, 0.15033113821265595, 0.2888075957037953, 0.10483406652162627, 0.11125953049519431, 0.08585722091652723, 0.13154360559344933, 0.008349248639315885, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8671875, 0.8837128439087288, 0.875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4136319160461426
Checking history sample input_X:  [0.16965502636986396, 0.1632541269761418, 0.06536948116802524, 0.22931460918771437, 0.07235389537017577, 0.07948837750247255, 0.09856044446849346, 0.10613959878213879, 0.015864440174974067, 2, 1, 0, 0, 0, 0, 52, 0.02440109885663705, 37, 1]
Checking history sample input_X_between_0_1:  [0.16965502636986396, 0.1632541269761418, 0.06536948116802524, 0.22931460918771437, 0.07235389537017577, 0.07948837750247255, 0.09856044446849346, 0.10613959878213879, 0.015864440174974067, 0.0625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.40625, 0.2440109885663705, 0.7708333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -2.0770633220672607
Checking history sample input_X:  [0.36131685118949053, 0.2612655996132517, 0.09728973792827665, 0.016352402241110658, 0.0637664750231403, 0.01680248270633843, 0.137283452814417, 0.01400964559956043, 0.03191335288441422, 16, 1, 1, 1, 0, 0, 70, 0.02082741206626623, 38, 1]
Checking history sample input_X_between_0_1:  [0.36131685118949053, 0.2612655996132517, 0.09728973792827665, 0.016352402241110658, 0.0637664750231403, 0.01680248270633843, 0.137283452814417, 0.01400964559956043, 0.03191335288441422, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.546875, 0.20827412066266227, 0.7916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0819246768951416
Checking history sample input_X:  [0.1471945258628963, 0.09048865243304925, 0.0028432699437709185, 0.03648812445816234, 0.15879745758736136, 0.08392207963907294, 0.3559804206469414, 0.1163518135768591, 0.007933655851886249, 30, 1, 1, 0, 1, 1, 27, 0.03341373220395105, 3, 1]
Checking history sample input_X_between_0_1:  [0.1471945258628963, 0.09048865243304925, 0.0028432699437709185, 0.03648812445816234, 0.15879745758736136, 0.08392207963907294, 0.3559804206469414, 0.1163518135768591, 0.007933655851886249, 0.9375, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2109375, 0.3341373220395105, 0.0625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2804617881774902
Checking history sample input_X:  [0.09708970671019405, 0.13689092369695835, 0.13726832030202735, 0.03956836294376928, 0.09044580719401238, 0.041564370215183784, 0.0018749764538878487, 0.24111683879487406, 0.21418069368909284, 25, 1, 0, 0, 1, 1, 72, 0.07793912935485114, 34, 0]
Checking history sample input_X_between_0_1:  [0.09708970671019405, 0.13689092369695835, 0.13726832030202735, 0.03956836294376928, 0.09044580719401238, 0.041564370215183784, 0.0018749764538878487, 0.24111683879487406, 0.21418069368909284, 0.78125, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5625, 0.7793912935485114, 0.7083333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0335614681243896
Checking history sample input_X:  [0.10072133130350767, 0.061195722672967294, 0.15043397081227575, 0.09909493084207656, 0.14517766942001123, 0.09911157330699087, 0.06709075752452866, 0.1444407486582126, 0.13273329545942936, 12, 1, 0, 0, 1, 0, 48, 0.029635134547486387, 5, 0]
Checking history sample input_X_between_0_1:  [0.10072133130350767, 0.061195722672967294, 0.15043397081227575, 0.09909493084207656, 0.14517766942001123, 0.09911157330699087, 0.06709075752452866, 0.1444407486582126, 0.13273329545942936, 0.375, 1.0, 0.0, 0.0, 1.0, 0.0, 0.375, 0.29635134547486386, 0.10416666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3930084705352783
Checking history sample input_X:  [0.0056408233326652405, 0.045454006708258686, 0.02419314968847306, 0.2621062198301061, 0.2543756705541004, 0.20901339392353954, 0.058849705392657874, 0.062447696140449545, 0.07791933442974946, 10, 0, 1, 0, 1, 1, 118, 0.004765499761682824, 9, 1]
Checking history sample input_X_between_0_1:  [0.0056408233326652405, 0.045454006708258686, 0.02419314968847306, 0.2621062198301061, 0.2543756705541004, 0.20901339392353954, 0.058849705392657874, 0.062447696140449545, 0.07791933442974946, 0.3125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.921875, 0.04765499761682824, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0495586395263672
Checking history sample input_X:  [0.03488693528172687, 0.35652639517245394, 0.07778500850664789, 0.10513445995831482, 0.36325923927607257, 0.0020496056495831597, 0.04514940236956389, 0.009292499052857398, 0.005916454732779405, 12, 1, 0, 1, 0, 0, 126, 0.042101384736383855, 9, 1]
Checking history sample input_X_between_0_1:  [0.03488693528172687, 0.35652639517245394, 0.07778500850664789, 0.10513445995831482, 0.36325923927607257, 0.0020496056495831597, 0.04514940236956389, 0.009292499052857398, 0.005916454732779405, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.984375, 0.4210138473638385, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.156050682067871
Checking history sample input_X:  [0.10948692372840407, 0.21669077939904466, 0.16290113684821536, 0.06632389273412838, 0.023331634670867053, 0.04511875862710639, 0.06417984157378358, 0.04013429279943936, 0.2718327396190111, 1, 1, 1, 1, 1, 0, 86, 0.0106663316581613, 38, 1]
Checking history sample input_X_between_0_1:  [0.10948692372840407, 0.21669077939904466, 0.16290113684821536, 0.06632389273412838, 0.023331634670867053, 0.04511875862710639, 0.06417984157378358, 0.04013429279943936, 0.2718327396190111, 0.03125, 1.0, 1.0, 1.0, 1.0, 0.0, 0.671875, 0.10666331658161299, 0.7916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.288783311843872
Checking history sample input_X:  [0.01438882831028789, 0.12997755569201166, 0.04075163354943151, 0.023595154255388905, 0.35477131795304295, 0.016630964685680923, 0.05509145809339371, 0.17242366325727926, 0.19236942420348319, 23, 1, 0, 1, 1, 0, 44, 0.0856132560172227, 24, 0]
Checking history sample input_X_between_0_1:  [0.01438882831028789, 0.12997755569201166, 0.04075163354943151, 0.023595154255388905, 0.35477131795304295, 0.016630964685680923, 0.05509145809339371, 0.17242366325727926, 0.19236942420348319, 0.71875, 1.0, 0.0, 1.0, 1.0, 0.0, 0.34375, 0.8561325601722269, 0.5, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9945324659347534
Checking history sample input_X:  [0.24449605892239556, 0.03207833927765533, 0.03240197712149855, 0.1790966164420265, 0.0030442733927761897, 0.05196332423567327, 0.23192639161149287, 0.20689610574355566, 0.018096913252926106, 27, 1, 0, 1, 0, 1, 88, 0.010689967975687609, 46, 0]
Checking history sample input_X_between_0_1:  [0.24449605892239556, 0.03207833927765533, 0.03240197712149855, 0.1790966164420265, 0.0030442733927761897, 0.05196332423567327, 0.23192639161149287, 0.20689610574355566, 0.018096913252926106, 0.84375, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6875, 0.10689967975687609, 0.9583333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0903468132019043
Checking history sample input_X:  [0.02102103063057685, 0.04371291849458268, 0.03642670439125302, 0.1177710527319927, 0.001407831158738137, 0.07877731789412545, 0.023108436790582124, 0.2712565713655495, 0.4065181365425995, 14, 0, 0, 1, 0, 1, 116, 0.0043609167679767745, 32, 0]
Checking history sample input_X_between_0_1:  [0.02102103063057685, 0.04371291849458268, 0.03642670439125302, 0.1177710527319927, 0.001407831158738137, 0.07877731789412545, 0.023108436790582124, 0.2712565713655495, 0.4065181365425995, 0.4375, 0.0, 0.0, 1.0, 0.0, 1.0, 0.90625, 0.04360916767976774, 0.6666666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.8854451179504395
Checking history sample input_X:  [0.07412507163006393, 0.2445504968671524, 0.13622879351830353, 0.04483166378469822, 0.22478374969436524, 0.03378463940701705, 0.23468787806930036, 0.0038162629362842922, 0.00319144409281491, 3, 1, 1, 0, 1, 1, 42, 0.09004868565085311, 14, 1]
Checking history sample input_X_between_0_1:  [0.07412507163006393, 0.2445504968671524, 0.13622879351830353, 0.04483166378469822, 0.22478374969436524, 0.03378463940701705, 0.23468787806930036, 0.0038162629362842922, 0.00319144409281491, 0.09375, 1.0, 1.0, 0.0, 1.0, 1.0, 0.328125, 0.9004868565085311, 0.2916666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.3376717567443848
Checking history sample input_X:  [0.13979331608413714, 0.22567075739867196, 0.047220640032135254, 0.04226304004460249, 0.20524041855401653, 0.1714340493110747, 0.08299650343653195, 0.04377008261845374, 0.04161119252037616, 32, 1, 1, 0, 1, 0, 29, 0.006821658518724117, 29, 0]
Checking history sample input_X_between_0_1:  [0.13979331608413714, 0.22567075739867196, 0.047220640032135254, 0.04226304004460249, 0.20524041855401653, 0.1714340493110747, 0.08299650343653195, 0.04377008261845374, 0.04161119252037616, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2265625, 0.06821658518724116, 0.6041666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9732747077941895
Checking history sample input_X:  [0.05744047965548223, 0.289973214141178, 0.20610036282289132, 0.008161921516160834, 0.05654145345652092, 0.0039259100145156494, 0.08013508882995081, 0.2050713532761911, 0.09265021628710923, 30, 1, 0, 1, 0, 1, 112, 0.003324070102424537, 36, 1]
Checking history sample input_X_between_0_1:  [0.05744047965548223, 0.289973214141178, 0.20610036282289132, 0.008161921516160834, 0.05654145345652092, 0.0039259100145156494, 0.08013508882995081, 0.2050713532761911, 0.09265021628710923, 0.9375, 1.0, 0.0, 1.0, 0.0, 1.0, 0.875, 0.03324070102424537, 0.75, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1381622552871704
Checking history sample input_X:  [0.011148142522340278, 0.2280230847756128, 0.289512358234605, 0.006770020691989324, 0.024707092605269564, 0.08731213423792487, 0.005804310855452206, 0.08464343941500369, 0.26207941666180234, 9, 1, 1, 1, 1, 0, 105, 0.09544186461572818, 12, 0]
Checking history sample input_X_between_0_1:  [0.011148142522340278, 0.2280230847756128, 0.289512358234605, 0.006770020691989324, 0.024707092605269564, 0.08731213423792487, 0.005804310855452206, 0.08464343941500369, 0.26207941666180234, 0.28125, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8203125, 0.9544186461572818, 0.25, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3062412738800049
Checking history sample input_X:  [0.018288746378659515, 0.11384303280049743, 0.03547167747709447, 0.12207096189867232, 0.06447946973899343, 0.18231445052447756, 0.1522399185681228, 0.13557467183201224, 0.17571707078147022, 4, 0, 1, 0, 1, 0, 53, 0.035644857853155076, 11, 0]
Checking history sample input_X_between_0_1:  [0.018288746378659515, 0.11384303280049743, 0.03547167747709447, 0.12207096189867232, 0.06447946973899343, 0.18231445052447756, 0.1522399185681228, 0.13557467183201224, 0.17571707078147022, 0.125, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4140625, 0.35644857853155076, 0.22916666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2854622602462769
Checking history sample input_X:  [0.511268635219867, 0.08759324792444484, 0.1034528140114814, 0.07300440359718659, 0.03242123832197673, 0.031167825877860832, 0.0622431292646515, 0.05784239394761446, 0.04100631183491673, 15, 0, 0, 1, 1, 0, 125, 0.022242287070241652, 48, 0]
Checking history sample input_X_between_0_1:  [0.511268635219867, 0.08759324792444484, 0.1034528140114814, 0.07300440359718659, 0.03242123832197673, 0.031167825877860832, 0.0622431292646515, 0.05784239394761446, 0.04100631183491673, 0.46875, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9765625, 0.2224228707024165, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0917696952819824
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9739 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.23116534948349, 0.6225166320800781, 0.7920008301734924, 0.5187347531318665, 0.9650238752365112, 0.19140440225601196, 0.8116955757141113, 0.7366132736206055, 0.26297563314437866, 0.5906185507774353, 0.5984089374542236, 0.6888700723648071, 0.4023132920265198, 0.26052409410476685, 0.36655211448669434, 0.7073982357978821, 0.7560136914253235, 0.4306621849536896, 0.6716703176498413]  ‚Üí  acq = -0.1802830134236164
X = [0.2264142632484436, 0.07403439283370972, 0.07219696044921875, 0.10861033201217651, 0.2566419839859009, 0.6019071340560913, 0.3223649859428406, 0.11172157526016235, 0.9948449730873108, 0.44289490580558777, 0.044420480728149414, 0.9455077648162842, 0.014932751655578613, 0.09747213125228882, 0.9369887113571167, 0.6387571692466736, 0.8574733734130859, 0.6355741024017334, 0.6718999147415161]  ‚Üí  acq = -0.18986574183712235
X = [0.02157503366470337, 0.6681593656539917, 0.1901683211326599, 0.7496262192726135, 0.6109984517097473, 0.4938375949859619, 0.4704396724700928, 0.3573959469795227, 0.5634814500808716, 0.7511853575706482, 0.7567681074142456, 0.4444613456726074, 0.7176378965377808, 0.620341420173645, 0.0801476240158081, 0.315092533826828, 0.40826380252838135, 0.6730163097381592, 0.5462942719459534]  ‚Üí  acq = -0.18119114202019193
X = [0.7039620280265808, 0.13568663597106934, 0.07833486795425415, 0.22286415100097656, 0.8236333727836609, 0.07582056522369385, 0.16812419891357422, 0.8353177309036255, 0.27740031480789185, 0.9245650768280029, 0.17562174797058105, 0.7579988837242126, 0.6997465491294861, 0.24228346347808838, 0.8386580944061279, 0.6914545893669128, 0.9157900810241699, 0.23076486587524414, 0.6642056107521057]  ‚Üí  acq = -0.16814016298850198
X = [0.7319151759147644, 0.9710763096809387, 0.5548725724220276, 0.39842361211776733, 0.7612097263336182, 0.6759727001190186, 0.173833429813385, 0.7195560932159424, 0.6185808181762695, 0.9764630794525146, 0.5892108082771301, 0.5290414094924927, 0.6469666361808777, 0.31331807374954224, 0.14616739749908447, 0.055544473230838776, 0.12637335062026978, 0.5483622550964355, 0.2759196162223816]  ‚Üí  acq = -0.1802865477531561
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.4450, dtype=torch.float64), 0, tensor(0.0300, dtype=torch.float64), 0, 0, tensor(0.0885, dtype=torch.float64), 0, 0, tensor(0.4366, dtype=torch.float64), 27, 1, 1, 1, 0, 1, 124, 0.012544744668879141, 48.0, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.4450, dtype=torch.float64), tensor(3.1863e-18, dtype=torch.float64), tensor(0.0300, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.2050e-18, dtype=torch.float64), tensor(0.0885, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9145e-18, dtype=torch.float64), tensor(0.4366, dtype=torch.float64), tensor(0.8359, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9660, dtype=torch.float64), tensor(0.1254, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.445
  gsm8k: 0
  rowan_hellaswag: 0.03
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.088
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.437

LoRA Parameters:
  lora_r: (124,)
  lora_dropout: (0.012544744668879141,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  124
lora dropout:  0.012544744668879141
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 167,989,248 || all params: 8,198,250,496 || trainable%: 2.0491
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7993, 'grad_norm': 0.45134854316711426, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0341360569000244, 'eval_runtime': 10.2536, 'eval_samples_per_second': 97.527, 'eval_steps_per_second': 6.144, 'epoch': 0.04}
{'loss': 1.2353, 'grad_norm': 0.2311703860759735, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9563812017440796, 'eval_runtime': 10.3364, 'eval_samples_per_second': 96.746, 'eval_steps_per_second': 6.095, 'epoch': 0.08}
{'loss': 1.0452, 'grad_norm': 0.23211030662059784, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9148300886154175, 'eval_runtime': 10.3415, 'eval_samples_per_second': 96.697, 'eval_steps_per_second': 6.092, 'epoch': 0.12}
{'loss': 0.9627, 'grad_norm': 0.21031560003757477, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8558993339538574, 'eval_runtime': 10.3364, 'eval_samples_per_second': 96.745, 'eval_steps_per_second': 6.095, 'epoch': 0.16}
{'loss': 0.8829, 'grad_norm': 0.2558412551879883, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9520536661148071, 'eval_runtime': 10.3502, 'eval_samples_per_second': 96.616, 'eval_steps_per_second': 6.087, 'epoch': 0.2}
{'loss': 0.8795, 'grad_norm': 0.24323438107967377, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9098085165023804, 'eval_runtime': 10.3531, 'eval_samples_per_second': 96.589, 'eval_steps_per_second': 6.085, 'epoch': 0.24}
{'loss': 0.8225, 'grad_norm': 0.2877662479877472, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9219740629196167, 'eval_runtime': 10.3587, 'eval_samples_per_second': 96.537, 'eval_steps_per_second': 6.082, 'epoch': 0.28}
{'loss': 0.8324, 'grad_norm': 0.25117573142051697, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9153547286987305, 'eval_runtime': 10.3507, 'eval_samples_per_second': 96.612, 'eval_steps_per_second': 6.087, 'epoch': 0.32}
{'loss': 0.777, 'grad_norm': 0.275419145822525, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.914566159248352, 'eval_runtime': 10.3542, 'eval_samples_per_second': 96.58, 'eval_steps_per_second': 6.085, 'epoch': 0.36}
{'loss': 0.7267, 'grad_norm': 0.323481947183609, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9453120231628418, 'eval_runtime': 10.3625, 'eval_samples_per_second': 96.502, 'eval_steps_per_second': 6.08, 'epoch': 0.4}
{'loss': 0.6983, 'grad_norm': 0.2777138352394104, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9450063705444336, 'eval_runtime': 10.3586, 'eval_samples_per_second': 96.538, 'eval_steps_per_second': 6.082, 'epoch': 0.44}
{'loss': 0.7372, 'grad_norm': 0.32434922456741333, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9614511728286743, 'eval_runtime': 10.3652, 'eval_samples_per_second': 96.476, 'eval_steps_per_second': 6.078, 'epoch': 0.48}
{'loss': 0.7435, 'grad_norm': 0.2868749499320984, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9974051713943481, 'eval_runtime': 10.363, 'eval_samples_per_second': 96.498, 'eval_steps_per_second': 6.079, 'epoch': 0.52}
{'loss': 0.7019, 'grad_norm': 0.2697165310382843, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.034618854522705, 'eval_runtime': 10.3938, 'eval_samples_per_second': 96.211, 'eval_steps_per_second': 6.061, 'epoch': 0.56}
{'loss': 0.7141, 'grad_norm': 0.34394484758377075, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.063586711883545, 'eval_runtime': 10.3841, 'eval_samples_per_second': 96.301, 'eval_steps_per_second': 6.067, 'epoch': 0.6}
{'loss': 0.6641, 'grad_norm': 0.2745512127876282, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.1125829219818115, 'eval_runtime': 10.3896, 'eval_samples_per_second': 96.25, 'eval_steps_per_second': 6.064, 'epoch': 0.64}
{'loss': 0.6341, 'grad_norm': 0.3038448393344879, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.010545253753662, 'eval_runtime': 10.3921, 'eval_samples_per_second': 96.227, 'eval_steps_per_second': 6.062, 'epoch': 0.68}
{'loss': 0.6116, 'grad_norm': 0.28550824522972107, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.0156774520874023, 'eval_runtime': 10.4016, 'eval_samples_per_second': 96.139, 'eval_steps_per_second': 6.057, 'epoch': 0.72}
{'loss': 0.6554, 'grad_norm': 0.39915406703948975, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.042233943939209, 'eval_runtime': 10.3981, 'eval_samples_per_second': 96.171, 'eval_steps_per_second': 6.059, 'epoch': 0.76}
{'loss': 0.5912, 'grad_norm': 0.3249760568141937, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.0525386333465576, 'eval_runtime': 10.3865, 'eval_samples_per_second': 96.278, 'eval_steps_per_second': 6.066, 'epoch': 0.8}
{'loss': 0.5755, 'grad_norm': 0.20481985807418823, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0096757411956787, 'eval_runtime': 10.3799, 'eval_samples_per_second': 96.34, 'eval_steps_per_second': 6.069, 'epoch': 0.84}
{'loss': 0.5943, 'grad_norm': 0.34295162558555603, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0739831924438477, 'eval_runtime': 10.3783, 'eval_samples_per_second': 96.355, 'eval_steps_per_second': 6.07, 'epoch': 0.88}
{'loss': 0.598, 'grad_norm': 0.4099556505680084, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.093055486679077, 'eval_runtime': 10.3868, 'eval_samples_per_second': 96.276, 'eval_steps_per_second': 6.065, 'epoch': 0.92}
{'loss': 0.5783, 'grad_norm': 0.2176796942949295, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0809483528137207, 'eval_runtime': 10.4126, 'eval_samples_per_second': 96.038, 'eval_steps_per_second': 6.05, 'epoch': 0.96}
{'loss': 0.5608, 'grad_norm': 0.23922863602638245, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.0815322399139404, 'eval_runtime': 10.4051, 'eval_samples_per_second': 96.107, 'eval_steps_per_second': 6.055, 'epoch': 1.0}
{'train_runtime': 430.9015, 'train_samples_per_second': 23.2, 'train_steps_per_second': 1.45, 'train_loss': 0.8248748916625976, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0341360569000244, 1.9563812017440796, 1.9148300886154175, 1.8558993339538574, 1.9520536661148071, 1.9098085165023804, 1.9219740629196167, 1.9153547286987305, 1.914566159248352, 1.9453120231628418, 1.9450063705444336, 1.9614511728286743, 1.9974051713943481, 2.034618854522705, 2.063586711883545, 2.1125829219818115, 2.010545253753662, 2.0156774520874023, 2.042233943939209, 2.0525386333465576, 2.0096757411956787, 2.0739831924438477, 2.093055486679077, 2.0809483528137207, 2.0815322399139404], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0341360569000244, 1.9563812017440796, 1.9148300886154175, 1.8558993339538574, 1.9520536661148071, 1.9098085165023804, 1.9219740629196167, 1.9153547286987305, 1.914566159248352, 1.9453120231628418, 1.9450063705444336, 1.9614511728286743, 1.9974051713943481, 2.034618854522705, 2.063586711883545, 2.1125829219818115, 2.010545253753662, 2.0156774520874023, 2.042233943939209, 2.0525386333465576, 2.0096757411956787, 2.0739831924438477, 2.093055486679077, 2.0809483528137207, 2.0815322399139404]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.0815322399139404
max eval_loss so far:  -2.0815322399139404
BO observations:  [-1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.2664 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.4174410390572746
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.4174287457344613
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.41742874573413047
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.4174287457341309
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.41749784901356113
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2476, dtype=torch.float64), 0, 0, 0, tensor(0.2152, dtype=torch.float64), 0, tensor(0.4728, dtype=torch.float64), tensor(0.0644, dtype=torch.float64), 16, 0, 1, 1, 1, 1, 128, 6.938893903907231e-19, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.4857e-17, dtype=torch.float64), tensor(0.2476, dtype=torch.float64), tensor(6.0413e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2152, dtype=torch.float64), tensor(1.5119e-18, dtype=torch.float64), tensor(0.4728, dtype=torch.float64), tensor(0.0644, dtype=torch.float64), tensor(0.5024, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(6.9389e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.248
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.215
  wikitext: 0
  mmlu: 0.473
  arc_challenge: 0.064

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (6.938893903907231e-19,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  6.938893903907231e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 123,731,968 || all params: 8,153,993,216 || trainable%: 1.5174
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4892, 'grad_norm': 0.7187880873680115, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.654153823852539, 'eval_runtime': 9.4805, 'eval_samples_per_second': 105.48, 'eval_steps_per_second': 6.645, 'epoch': 0.04}
{'loss': 1.2273, 'grad_norm': 0.8739983439445496, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4108295440673828, 'eval_runtime': 9.4872, 'eval_samples_per_second': 105.405, 'eval_steps_per_second': 6.641, 'epoch': 0.08}
{'loss': 1.1294, 'grad_norm': 0.29276084899902344, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3373929262161255, 'eval_runtime': 9.5128, 'eval_samples_per_second': 105.121, 'eval_steps_per_second': 6.623, 'epoch': 0.12}
{'loss': 1.1079, 'grad_norm': 0.42364436388015747, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3107693195343018, 'eval_runtime': 9.5344, 'eval_samples_per_second': 104.884, 'eval_steps_per_second': 6.608, 'epoch': 0.16}
{'loss': 1.1316, 'grad_norm': 0.28916212916374207, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2905327081680298, 'eval_runtime': 9.5644, 'eval_samples_per_second': 104.555, 'eval_steps_per_second': 6.587, 'epoch': 0.2}
{'loss': 1.0769, 'grad_norm': 0.2647084593772888, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2770273685455322, 'eval_runtime': 9.5579, 'eval_samples_per_second': 104.626, 'eval_steps_per_second': 6.591, 'epoch': 0.24}
{'loss': 1.038, 'grad_norm': 0.2825588583946228, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2730655670166016, 'eval_runtime': 9.5582, 'eval_samples_per_second': 104.622, 'eval_steps_per_second': 6.591, 'epoch': 0.28}
{'loss': 1.0769, 'grad_norm': 0.2165907770395279, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2647764682769775, 'eval_runtime': 9.5651, 'eval_samples_per_second': 104.547, 'eval_steps_per_second': 6.586, 'epoch': 0.32}
{'loss': 1.0102, 'grad_norm': 0.46484529972076416, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2598177194595337, 'eval_runtime': 9.5691, 'eval_samples_per_second': 104.503, 'eval_steps_per_second': 6.584, 'epoch': 0.36}
{'loss': 0.9926, 'grad_norm': 0.23707778751850128, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2599598169326782, 'eval_runtime': 9.5697, 'eval_samples_per_second': 104.496, 'eval_steps_per_second': 6.583, 'epoch': 0.4}
{'loss': 1.0395, 'grad_norm': 0.23910358548164368, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2574914693832397, 'eval_runtime': 9.583, 'eval_samples_per_second': 104.352, 'eval_steps_per_second': 6.574, 'epoch': 0.44}
{'loss': 1.0324, 'grad_norm': 0.2411545068025589, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2513900995254517, 'eval_runtime': 9.5911, 'eval_samples_per_second': 104.264, 'eval_steps_per_second': 6.569, 'epoch': 0.48}
{'loss': 1.0247, 'grad_norm': 0.26227545738220215, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2454673051834106, 'eval_runtime': 9.5756, 'eval_samples_per_second': 104.432, 'eval_steps_per_second': 6.579, 'epoch': 0.52}
{'loss': 1.0772, 'grad_norm': 2.206000804901123, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.239333987236023, 'eval_runtime': 9.572, 'eval_samples_per_second': 104.472, 'eval_steps_per_second': 6.582, 'epoch': 0.56}
{'loss': 0.9733, 'grad_norm': 0.22863972187042236, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2378848791122437, 'eval_runtime': 9.6001, 'eval_samples_per_second': 104.166, 'eval_steps_per_second': 6.562, 'epoch': 0.6}
{'loss': 1.0051, 'grad_norm': 0.24735593795776367, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2411394119262695, 'eval_runtime': 9.6466, 'eval_samples_per_second': 103.664, 'eval_steps_per_second': 6.531, 'epoch': 0.64}
{'loss': 0.9852, 'grad_norm': 0.2551860213279724, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2373698949813843, 'eval_runtime': 9.6691, 'eval_samples_per_second': 103.422, 'eval_steps_per_second': 6.516, 'epoch': 0.68}
{'loss': 1.0169, 'grad_norm': 0.4009983241558075, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.232361078262329, 'eval_runtime': 9.5999, 'eval_samples_per_second': 104.168, 'eval_steps_per_second': 6.563, 'epoch': 0.72}
{'loss': 0.9707, 'grad_norm': 0.25961562991142273, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.23171067237854, 'eval_runtime': 9.6264, 'eval_samples_per_second': 103.881, 'eval_steps_per_second': 6.544, 'epoch': 0.76}
{'loss': 0.9606, 'grad_norm': 0.23478533327579498, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2304939031600952, 'eval_runtime': 9.6132, 'eval_samples_per_second': 104.024, 'eval_steps_per_second': 6.553, 'epoch': 0.8}
{'loss': 0.9641, 'grad_norm': 0.25784367322921753, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.22787606716156, 'eval_runtime': 9.5482, 'eval_samples_per_second': 104.731, 'eval_steps_per_second': 6.598, 'epoch': 0.84}
{'loss': 0.9958, 'grad_norm': 0.2700810730457306, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.225317358970642, 'eval_runtime': 9.56, 'eval_samples_per_second': 104.603, 'eval_steps_per_second': 6.59, 'epoch': 0.88}
{'loss': 0.9731, 'grad_norm': 0.22412998974323273, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2224795818328857, 'eval_runtime': 9.5423, 'eval_samples_per_second': 104.797, 'eval_steps_per_second': 6.602, 'epoch': 0.92}
{'loss': 0.9694, 'grad_norm': 0.2664065361022949, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.221395492553711, 'eval_runtime': 9.55, 'eval_samples_per_second': 104.712, 'eval_steps_per_second': 6.597, 'epoch': 0.96}
{'loss': 1.0001, 'grad_norm': 0.7775081396102905, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2208707332611084, 'eval_runtime': 9.5483, 'eval_samples_per_second': 104.731, 'eval_steps_per_second': 6.598, 'epoch': 1.0}
{'train_runtime': 458.505, 'train_samples_per_second': 21.806, 'train_steps_per_second': 1.363, 'train_loss': 1.0907167755126954, 'epoch': 1.0}
train_results:  {'eval_loss': [1.654153823852539, 1.4108295440673828, 1.3373929262161255, 1.3107693195343018, 1.2905327081680298, 1.2770273685455322, 1.2730655670166016, 1.2647764682769775, 1.2598177194595337, 1.2599598169326782, 1.2574914693832397, 1.2513900995254517, 1.2454673051834106, 1.239333987236023, 1.2378848791122437, 1.2411394119262695, 1.2373698949813843, 1.232361078262329, 1.23171067237854, 1.2304939031600952, 1.22787606716156, 1.225317358970642, 1.2224795818328857, 1.221395492553711, 1.2208707332611084], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.654153823852539, 1.4108295440673828, 1.3373929262161255, 1.3107693195343018, 1.2905327081680298, 1.2770273685455322, 1.2730655670166016, 1.2647764682769775, 1.2598177194595337, 1.2599598169326782, 1.2574914693832397, 1.2513900995254517, 1.2454673051834106, 1.239333987236023, 1.2378848791122437, 1.2411394119262695, 1.2373698949813843, 1.232361078262329, 1.23171067237854, 1.2304939031600952, 1.22787606716156, 1.225317358970642, 1.2224795818328857, 1.221395492553711, 1.2208707332611084]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2208707332611084
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.9204 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.5115911714546821
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.5113503199869119
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.5113503199871404
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.5129333065329148
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.508226637281874
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3730, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6230, dtype=torch.float64), 15, 0, 0, 1, 1, 1, 128, 2.7640376422070524e-19, 17.0360875256447, 1]
normalized proposed parameters for next round by BO: [tensor(4.6596e-18, dtype=torch.float64), tensor(3.0603e-18, dtype=torch.float64), tensor(0.0040, dtype=torch.float64), tensor(0.3730, dtype=torch.float64), tensor(2.7615e-18, dtype=torch.float64), tensor(2.1444e-18, dtype=torch.float64), tensor(1.5349e-18, dtype=torch.float64), tensor(2.3681e-18, dtype=torch.float64), tensor(0.6230, dtype=torch.float64), tensor(0.4653, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.7640e-18, dtype=torch.float64), tensor(0.3549, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.373
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.623

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.7640376422070524e-19,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (17.0360875256447,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.7640376422070524e-19
lora alpha:  17.0360875256447
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 106,168,320 || all params: 8,136,429,568 || trainable%: 1.3049
length of training data:  9959
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3345, 'grad_norm': 1.4048213958740234, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9736436605453491, 'eval_runtime': 9.6214, 'eval_samples_per_second': 103.934, 'eval_steps_per_second': 6.548, 'epoch': 0.04}
{'loss': 1.2453, 'grad_norm': 0.3080766201019287, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9440858364105225, 'eval_runtime': 9.2939, 'eval_samples_per_second': 107.598, 'eval_steps_per_second': 6.779, 'epoch': 0.08}
{'loss': 0.9437, 'grad_norm': 0.19095580279827118, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.9094955921173096, 'eval_runtime': 9.3278, 'eval_samples_per_second': 107.206, 'eval_steps_per_second': 6.754, 'epoch': 0.12}
{'loss': 0.8972, 'grad_norm': 0.19034795463085175, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.924383282661438, 'eval_runtime': 9.3546, 'eval_samples_per_second': 106.9, 'eval_steps_per_second': 6.735, 'epoch': 0.16}
{'loss': 0.8748, 'grad_norm': 0.23683491349220276, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.9219735860824585, 'eval_runtime': 9.3836, 'eval_samples_per_second': 106.569, 'eval_steps_per_second': 6.714, 'epoch': 0.2}
{'loss': 0.8028, 'grad_norm': 0.15780885517597198, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.9149789810180664, 'eval_runtime': 9.4316, 'eval_samples_per_second': 106.026, 'eval_steps_per_second': 6.68, 'epoch': 0.24}
{'loss': 0.8088, 'grad_norm': 0.1784268021583557, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.8452799320220947, 'eval_runtime': 9.4425, 'eval_samples_per_second': 105.905, 'eval_steps_per_second': 6.672, 'epoch': 0.28}
{'loss': 0.7768, 'grad_norm': 0.16451020538806915, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.8971940279006958, 'eval_runtime': 9.4286, 'eval_samples_per_second': 106.061, 'eval_steps_per_second': 6.682, 'epoch': 0.32}
{'loss': 0.7659, 'grad_norm': 0.18280918896198273, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.870577335357666, 'eval_runtime': 9.4063, 'eval_samples_per_second': 106.312, 'eval_steps_per_second': 6.698, 'epoch': 0.36}
{'loss': 0.7491, 'grad_norm': 0.2203502058982849, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.9412264823913574, 'eval_runtime': 9.3866, 'eval_samples_per_second': 106.535, 'eval_steps_per_second': 6.712, 'epoch': 0.4}
{'loss': 0.7292, 'grad_norm': 0.19927239418029785, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.8769469261169434, 'eval_runtime': 9.382, 'eval_samples_per_second': 106.588, 'eval_steps_per_second': 6.715, 'epoch': 0.44}
{'loss': 0.7065, 'grad_norm': 0.2577991783618927, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.9337326288223267, 'eval_runtime': 9.3827, 'eval_samples_per_second': 106.579, 'eval_steps_per_second': 6.714, 'epoch': 0.48}
{'loss': 0.6993, 'grad_norm': 0.3563549816608429, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.943225622177124, 'eval_runtime': 9.3897, 'eval_samples_per_second': 106.499, 'eval_steps_per_second': 6.709, 'epoch': 0.52}
{'loss': 0.6736, 'grad_norm': 0.3249606192111969, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.948541522026062, 'eval_runtime': 9.3858, 'eval_samples_per_second': 106.543, 'eval_steps_per_second': 6.712, 'epoch': 0.56}
{'loss': 0.6538, 'grad_norm': 0.29026710987091064, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.9763749837875366, 'eval_runtime': 9.3933, 'eval_samples_per_second': 106.459, 'eval_steps_per_second': 6.707, 'epoch': 0.6}
{'loss': 0.6776, 'grad_norm': 0.4240340292453766, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 2.029430389404297, 'eval_runtime': 9.3886, 'eval_samples_per_second': 106.512, 'eval_steps_per_second': 6.71, 'epoch': 0.64}
{'loss': 0.6365, 'grad_norm': 0.39469021558761597, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 2.0184168815612793, 'eval_runtime': 9.3883, 'eval_samples_per_second': 106.516, 'eval_steps_per_second': 6.71, 'epoch': 0.68}
{'loss': 0.6056, 'grad_norm': 0.41509610414505005, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 2.0283303260803223, 'eval_runtime': 9.3882, 'eval_samples_per_second': 106.517, 'eval_steps_per_second': 6.711, 'epoch': 0.72}
{'loss': 0.5636, 'grad_norm': 0.3817235231399536, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 2.0794155597686768, 'eval_runtime': 9.4101, 'eval_samples_per_second': 106.268, 'eval_steps_per_second': 6.695, 'epoch': 0.76}
{'loss': 0.5652, 'grad_norm': 0.40184420347213745, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 2.090715169906616, 'eval_runtime': 9.4337, 'eval_samples_per_second': 106.003, 'eval_steps_per_second': 6.678, 'epoch': 0.8}
{'loss': 0.5509, 'grad_norm': 0.4561772346496582, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 2.0547447204589844, 'eval_runtime': 9.4336, 'eval_samples_per_second': 106.005, 'eval_steps_per_second': 6.678, 'epoch': 0.84}
{'loss': 0.5356, 'grad_norm': 0.47930729389190674, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 2.1006436347961426, 'eval_runtime': 9.4376, 'eval_samples_per_second': 105.959, 'eval_steps_per_second': 6.675, 'epoch': 0.88}
{'loss': 0.4981, 'grad_norm': 0.44438013434410095, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 2.1345884799957275, 'eval_runtime': 9.5113, 'eval_samples_per_second': 105.138, 'eval_steps_per_second': 6.624, 'epoch': 0.92}
{'loss': 0.5162, 'grad_norm': 0.5416380167007446, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 2.1648600101470947, 'eval_runtime': 9.5048, 'eval_samples_per_second': 105.21, 'eval_steps_per_second': 6.628, 'epoch': 0.96}
{'train_runtime': 373.3991, 'train_samples_per_second': 26.671, 'train_steps_per_second': 1.668, 'train_loss': 0.812175063413372, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9736436605453491, 1.9440858364105225, 1.9094955921173096, 1.924383282661438, 1.9219735860824585, 1.9149789810180664, 1.8452799320220947, 1.8971940279006958, 1.870577335357666, 1.9412264823913574, 1.8769469261169434, 1.9337326288223267, 1.943225622177124, 1.948541522026062, 1.9763749837875366, 2.029430389404297, 2.0184168815612793, 2.0283303260803223, 2.0794155597686768, 2.090715169906616, 2.0547447204589844, 2.1006436347961426, 2.1345884799957275, 2.1648600101470947], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9736436605453491, 1.9440858364105225, 1.9094955921173096, 1.924383282661438, 1.9219735860824585, 1.9149789810180664, 1.8452799320220947, 1.8971940279006958, 1.870577335357666, 1.9412264823913574, 1.8769469261169434, 1.9337326288223267, 1.943225622177124, 1.948541522026062, 1.9763749837875366, 2.029430389404297, 2.0184168815612793, 2.0283303260803223, 2.0794155597686768, 2.090715169906616, 2.0547447204589844, 2.1006436347961426, 2.1345884799957275, 2.1648600101470947]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.1648600101470947
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.7609 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.04070591926574707, 0.2670907974243164, 0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.11440835893154144, 0.8343492150306702, 0.6285832524299622, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5520853400230408, 0.8785960674285889, 0.7897642850875854, 0.10385686159133911]  ‚Üí  acq = -0.5897670590994496
X = [0.1669445037841797, 0.07206535339355469, 0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.29802340269088745, 0.02810537815093994, 0.37732040882110596, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4432501196861267, 0.8628382086753845, 0.2149072289466858, 0.16291123628616333]  ‚Üí  acq = -0.5897670590994496
X = [0.5642975568771362, 0.34905433654785156, 0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.45446842908859253, 0.9301089644432068, 0.8571122884750366, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9834365248680115, 0.5003925561904907, 0.9843506813049316, 0.11054939031600952]  ‚Üí  acq = -0.5897670590994496
X = [0.16956406831741333, 0.7232905030250549, 0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.11824709177017212, 0.728330671787262, 0.602367103099823, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4289284646511078, 0.0390055775642395, 0.09640960395336151, 0.9996876120567322]  ‚Üí  acq = -0.5897670591350773
X = [0.7488081455230713, 0.8432244062423706, 0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.5128886699676514, 0.7522366642951965, 0.6226845979690552, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.46897920966148376, 0.9671209454536438, 0.12299968302249908, 0.006412029266357422]  ‚Üí  acq = -0.58976705909945
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2344, dtype=torch.float64), tensor(0.0780, dtype=torch.float64), tensor(0.0681, dtype=torch.float64), tensor(0.0445, dtype=torch.float64), tensor(0.2167, dtype=torch.float64), 0, tensor(0.3584, dtype=torch.float64), 0, 27, 0, 0, 1, 0, 1, 2, 8.673617379884043e-20, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2344, dtype=torch.float64), tensor(0.0780, dtype=torch.float64), tensor(0.0681, dtype=torch.float64), tensor(0.0445, dtype=torch.float64), tensor(0.2167, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3584, dtype=torch.float64), tensor(2.8768e-19, dtype=torch.float64), tensor(0.8452, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(8.6736e-19, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.234
  rowan_hellaswag: 0.078
  sciq: 0.068
  triviaqa: 0.044
  truthfulqa_gen: 0.217
  wikitext: 0
  mmlu: 0.358
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (8.673617379884043e-20,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  2
lora dropout:  8.673617379884043e-20
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,990,656 || all params: 8,032,251,904 || trainable%: 0.0248
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.579, 'grad_norm': 2.8405773639678955, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6945451498031616, 'eval_runtime': 9.5324, 'eval_samples_per_second': 104.906, 'eval_steps_per_second': 6.609, 'epoch': 0.04}
{'loss': 1.4271, 'grad_norm': 1.7302099466323853, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.439468264579773, 'eval_runtime': 9.5671, 'eval_samples_per_second': 104.525, 'eval_steps_per_second': 6.585, 'epoch': 0.08}
{'loss': 1.2633, 'grad_norm': 1.24779212474823, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3185746669769287, 'eval_runtime': 9.6106, 'eval_samples_per_second': 104.052, 'eval_steps_per_second': 6.555, 'epoch': 0.12}
{'loss': 1.1963, 'grad_norm': 1.1161826848983765, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2884100675582886, 'eval_runtime': 9.6222, 'eval_samples_per_second': 103.927, 'eval_steps_per_second': 6.547, 'epoch': 0.16}
{'loss': 1.1796, 'grad_norm': 1.1054441928863525, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2769651412963867, 'eval_runtime': 9.6395, 'eval_samples_per_second': 103.74, 'eval_steps_per_second': 6.536, 'epoch': 0.2}
{'loss': 1.1377, 'grad_norm': 1.1516486406326294, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2731672525405884, 'eval_runtime': 9.6599, 'eval_samples_per_second': 103.521, 'eval_steps_per_second': 6.522, 'epoch': 0.24}
{'loss': 1.0974, 'grad_norm': 2.45592999458313, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2739506959915161, 'eval_runtime': 9.6723, 'eval_samples_per_second': 103.388, 'eval_steps_per_second': 6.513, 'epoch': 0.28}
{'loss': 1.1361, 'grad_norm': 1.0823873281478882, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2720566987991333, 'eval_runtime': 9.6505, 'eval_samples_per_second': 103.622, 'eval_steps_per_second': 6.528, 'epoch': 0.32}
{'loss': 1.1048, 'grad_norm': 1.2558279037475586, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2665104866027832, 'eval_runtime': 9.6489, 'eval_samples_per_second': 103.639, 'eval_steps_per_second': 6.529, 'epoch': 0.36}
{'loss': 1.0458, 'grad_norm': 1.0985832214355469, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.267712116241455, 'eval_runtime': 9.6545, 'eval_samples_per_second': 103.579, 'eval_steps_per_second': 6.525, 'epoch': 0.4}
{'loss': 1.1522, 'grad_norm': 1.274189829826355, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2629985809326172, 'eval_runtime': 9.6419, 'eval_samples_per_second': 103.714, 'eval_steps_per_second': 6.534, 'epoch': 0.44}
{'loss': 1.1213, 'grad_norm': 1.470007061958313, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2551791667938232, 'eval_runtime': 9.6446, 'eval_samples_per_second': 103.685, 'eval_steps_per_second': 6.532, 'epoch': 0.48}
{'loss': 1.1015, 'grad_norm': 1.1741527318954468, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2539246082305908, 'eval_runtime': 9.6171, 'eval_samples_per_second': 103.981, 'eval_steps_per_second': 6.551, 'epoch': 0.52}
{'loss': 1.1033, 'grad_norm': 1.5477032661437988, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2519824504852295, 'eval_runtime': 9.5891, 'eval_samples_per_second': 104.285, 'eval_steps_per_second': 6.57, 'epoch': 0.56}
{'loss': 1.0394, 'grad_norm': 1.6131017208099365, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2456448078155518, 'eval_runtime': 9.5962, 'eval_samples_per_second': 104.208, 'eval_steps_per_second': 6.565, 'epoch': 0.6}
{'loss': 1.0601, 'grad_norm': 1.5459692478179932, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2430797815322876, 'eval_runtime': 9.5922, 'eval_samples_per_second': 104.251, 'eval_steps_per_second': 6.568, 'epoch': 0.64}
{'loss': 1.0816, 'grad_norm': 1.8492145538330078, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2404342889785767, 'eval_runtime': 9.6094, 'eval_samples_per_second': 104.065, 'eval_steps_per_second': 6.556, 'epoch': 0.68}
{'loss': 1.0818, 'grad_norm': 1.1586016416549683, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2404181957244873, 'eval_runtime': 9.6302, 'eval_samples_per_second': 103.84, 'eval_steps_per_second': 6.542, 'epoch': 0.72}
{'loss': 1.0365, 'grad_norm': 1.2504414319992065, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2381501197814941, 'eval_runtime': 9.6647, 'eval_samples_per_second': 103.469, 'eval_steps_per_second': 6.519, 'epoch': 0.76}
{'loss': 1.0519, 'grad_norm': 1.3604326248168945, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2369242906570435, 'eval_runtime': 9.6804, 'eval_samples_per_second': 103.302, 'eval_steps_per_second': 6.508, 'epoch': 0.8}
{'loss': 1.0988, 'grad_norm': 1.6504290103912354, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2376370429992676, 'eval_runtime': 9.7192, 'eval_samples_per_second': 102.889, 'eval_steps_per_second': 6.482, 'epoch': 0.84}
{'loss': 1.0538, 'grad_norm': 1.2491132020950317, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2348710298538208, 'eval_runtime': 9.7293, 'eval_samples_per_second': 102.782, 'eval_steps_per_second': 6.475, 'epoch': 0.88}
{'loss': 1.0877, 'grad_norm': 2.235518455505371, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2354552745819092, 'eval_runtime': 9.7346, 'eval_samples_per_second': 102.726, 'eval_steps_per_second': 6.472, 'epoch': 0.92}
{'loss': 1.0209, 'grad_norm': 1.1651307344436646, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2342172861099243, 'eval_runtime': 9.7317, 'eval_samples_per_second': 102.757, 'eval_steps_per_second': 6.474, 'epoch': 0.96}
{'loss': 1.0328, 'grad_norm': 1.4740360975265503, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.233554720878601, 'eval_runtime': 9.7347, 'eval_samples_per_second': 102.725, 'eval_steps_per_second': 6.472, 'epoch': 1.0}
{'train_runtime': 441.0911, 'train_samples_per_second': 22.662, 'train_steps_per_second': 1.417, 'train_loss': 1.1716309509277343, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6945451498031616, 1.439468264579773, 1.3185746669769287, 1.2884100675582886, 1.2769651412963867, 1.2731672525405884, 1.2739506959915161, 1.2720566987991333, 1.2665104866027832, 1.267712116241455, 1.2629985809326172, 1.2551791667938232, 1.2539246082305908, 1.2519824504852295, 1.2456448078155518, 1.2430797815322876, 1.2404342889785767, 1.2404181957244873, 1.2381501197814941, 1.2369242906570435, 1.2376370429992676, 1.2348710298538208, 1.2354552745819092, 1.2342172861099243, 1.233554720878601], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6945451498031616, 1.439468264579773, 1.3185746669769287, 1.2884100675582886, 1.2769651412963867, 1.2731672525405884, 1.2739506959915161, 1.2720566987991333, 1.2665104866027832, 1.267712116241455, 1.2629985809326172, 1.2551791667938232, 1.2539246082305908, 1.2519824504852295, 1.2456448078155518, 1.2430797815322876, 1.2404342889785767, 1.2404181957244873, 1.2381501197814941, 1.2369242906570435, 1.2376370429992676, 1.2348710298538208, 1.2354552745819092, 1.2342172861099243, 1.233554720878601]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.233554720878601
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.0425 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.6397866595110896
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.6360157286122274
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.6360156288479331
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.6362260513034493
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.6360196061755198
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.5201, dtype=torch.float64), 0, tensor(0.0325, dtype=torch.float64), tensor(0.0788, dtype=torch.float64), 0, 0, tensor(0.0242, dtype=torch.float64), tensor(0.2512, dtype=torch.float64), tensor(0.0914, dtype=torch.float64), 18, 0, 0, 1, 0, 1, 128, 3.80733994358938e-19, 11.577462822391881, 0]
normalized proposed parameters for next round by BO: [tensor(0.5201, dtype=torch.float64), tensor(2.7488e-18, dtype=torch.float64), tensor(0.0325, dtype=torch.float64), tensor(0.0788, dtype=torch.float64), tensor(1.1140e-18, dtype=torch.float64), tensor(0.0018, dtype=torch.float64), tensor(0.0242, dtype=torch.float64), tensor(0.2512, dtype=torch.float64), tensor(0.0914, dtype=torch.float64), tensor(0.5476, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.8073e-18, dtype=torch.float64), tensor(0.2412, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.52
  gsm8k: 0
  rowan_hellaswag: 0.033
  sciq: 0.079
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.024
  mmlu: 0.251
  arc_challenge: 0.091

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.80733994358938e-19,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (11.577462822391881,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  128
lora dropout:  3.80733994358938e-19
lora alpha:  11.577462822391881
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 84,934,656 || all params: 8,115,195,904 || trainable%: 1.0466
length of training data:  9980
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6887, 'grad_norm': 0.30606552958488464, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.100508689880371, 'eval_runtime': 9.1244, 'eval_samples_per_second': 109.596, 'eval_steps_per_second': 6.905, 'epoch': 0.04}
{'loss': 1.8225, 'grad_norm': 0.1734231412410736, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6375823020935059, 'eval_runtime': 9.1968, 'eval_samples_per_second': 108.734, 'eval_steps_per_second': 6.85, 'epoch': 0.08}
{'loss': 1.4518, 'grad_norm': 0.09025423973798752, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.547664999961853, 'eval_runtime': 9.2489, 'eval_samples_per_second': 108.121, 'eval_steps_per_second': 6.812, 'epoch': 0.12}
{'loss': 1.3198, 'grad_norm': 0.09341587871313095, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.4910932779312134, 'eval_runtime': 9.2644, 'eval_samples_per_second': 107.94, 'eval_steps_per_second': 6.8, 'epoch': 0.16}
{'loss': 1.3103, 'grad_norm': 0.12815812230110168, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.4343186616897583, 'eval_runtime': 9.2867, 'eval_samples_per_second': 107.681, 'eval_steps_per_second': 6.784, 'epoch': 0.2}
{'loss': 1.2041, 'grad_norm': 0.11706485599279404, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 1.3711904287338257, 'eval_runtime': 9.2669, 'eval_samples_per_second': 107.911, 'eval_steps_per_second': 6.798, 'epoch': 0.24}
{'loss': 1.1293, 'grad_norm': 0.09467841684818268, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 1.3295814990997314, 'eval_runtime': 9.2844, 'eval_samples_per_second': 107.708, 'eval_steps_per_second': 6.786, 'epoch': 0.28}
{'loss': 1.1343, 'grad_norm': 0.08311329036951065, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 1.3210152387619019, 'eval_runtime': 9.2793, 'eval_samples_per_second': 107.767, 'eval_steps_per_second': 6.789, 'epoch': 0.32}
{'loss': 1.1683, 'grad_norm': 0.09552539885044098, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 1.320314645767212, 'eval_runtime': 9.2639, 'eval_samples_per_second': 107.946, 'eval_steps_per_second': 6.801, 'epoch': 0.36}
{'loss': 1.1005, 'grad_norm': 0.11267086863517761, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 1.3196544647216797, 'eval_runtime': 9.2468, 'eval_samples_per_second': 108.146, 'eval_steps_per_second': 6.813, 'epoch': 0.4}
{'loss': 1.1157, 'grad_norm': 0.09406741708517075, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 1.318228006362915, 'eval_runtime': 9.2246, 'eval_samples_per_second': 108.405, 'eval_steps_per_second': 6.83, 'epoch': 0.44}
{'loss': 1.1112, 'grad_norm': 0.09235575050115585, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 1.316900372505188, 'eval_runtime': 9.2332, 'eval_samples_per_second': 108.304, 'eval_steps_per_second': 6.823, 'epoch': 0.48}
{'loss': 1.0751, 'grad_norm': 0.10712981224060059, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 1.3121854066848755, 'eval_runtime': 9.2402, 'eval_samples_per_second': 108.223, 'eval_steps_per_second': 6.818, 'epoch': 0.52}
{'loss': 1.0861, 'grad_norm': 0.11735349893569946, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 1.3063650131225586, 'eval_runtime': 9.2276, 'eval_samples_per_second': 108.371, 'eval_steps_per_second': 6.827, 'epoch': 0.56}
{'loss': 1.0798, 'grad_norm': 0.10059607774019241, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 1.3055869340896606, 'eval_runtime': 9.2264, 'eval_samples_per_second': 108.385, 'eval_steps_per_second': 6.828, 'epoch': 0.6}
{'loss': 1.1149, 'grad_norm': 0.09883366525173187, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 1.3072025775909424, 'eval_runtime': 9.235, 'eval_samples_per_second': 108.284, 'eval_steps_per_second': 6.822, 'epoch': 0.64}
{'loss': 1.0671, 'grad_norm': 0.10736013203859329, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 1.3013248443603516, 'eval_runtime': 9.2281, 'eval_samples_per_second': 108.365, 'eval_steps_per_second': 6.827, 'epoch': 0.68}
{'loss': 1.1011, 'grad_norm': 0.11046232283115387, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 1.2997126579284668, 'eval_runtime': 9.2361, 'eval_samples_per_second': 108.271, 'eval_steps_per_second': 6.821, 'epoch': 0.72}
{'loss': 1.039, 'grad_norm': 0.13190299272537231, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 1.2995742559432983, 'eval_runtime': 9.2283, 'eval_samples_per_second': 108.362, 'eval_steps_per_second': 6.827, 'epoch': 0.76}
{'loss': 1.0378, 'grad_norm': 0.11151203513145447, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 1.299715280532837, 'eval_runtime': 9.2342, 'eval_samples_per_second': 108.294, 'eval_steps_per_second': 6.822, 'epoch': 0.8}
{'loss': 1.0748, 'grad_norm': 0.09736242145299911, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 1.2979835271835327, 'eval_runtime': 9.2681, 'eval_samples_per_second': 107.897, 'eval_steps_per_second': 6.797, 'epoch': 0.84}
{'loss': 1.0265, 'grad_norm': 0.10306394100189209, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 1.2965493202209473, 'eval_runtime': 9.2863, 'eval_samples_per_second': 107.686, 'eval_steps_per_second': 6.784, 'epoch': 0.88}
{'loss': 1.0822, 'grad_norm': 0.11056791245937347, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 1.295849084854126, 'eval_runtime': 9.3138, 'eval_samples_per_second': 107.367, 'eval_steps_per_second': 6.764, 'epoch': 0.92}
{'loss': 1.0508, 'grad_norm': 0.11421246081590652, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 1.2947897911071777, 'eval_runtime': 9.3538, 'eval_samples_per_second': 106.908, 'eval_steps_per_second': 6.735, 'epoch': 0.96}
{'train_runtime': 365.1449, 'train_samples_per_second': 27.332, 'train_steps_per_second': 1.709, 'train_loss': 1.2581940919925005, 'epoch': 1.0}
train_results:  {'eval_loss': [2.100508689880371, 1.6375823020935059, 1.547664999961853, 1.4910932779312134, 1.4343186616897583, 1.3711904287338257, 1.3295814990997314, 1.3210152387619019, 1.320314645767212, 1.3196544647216797, 1.318228006362915, 1.316900372505188, 1.3121854066848755, 1.3063650131225586, 1.3055869340896606, 1.3072025775909424, 1.3013248443603516, 1.2997126579284668, 1.2995742559432983, 1.299715280532837, 1.2979835271835327, 1.2965493202209473, 1.295849084854126, 1.2947897911071777], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.100508689880371, 1.6375823020935059, 1.547664999961853, 1.4910932779312134, 1.4343186616897583, 1.3711904287338257, 1.3295814990997314, 1.3210152387619019, 1.320314645767212, 1.3196544647216797, 1.318228006362915, 1.316900372505188, 1.3121854066848755, 1.3063650131225586, 1.3055869340896606, 1.3072025775909424, 1.3013248443603516, 1.2997126579284668, 1.2995742559432983, 1.299715280532837, 1.2979835271835327, 1.2965493202209473, 1.295849084854126, 1.2947897911071777]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2947897911071777
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.4195 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6827057003974915, 0.18881899118423462, 0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.22865861654281616, 0.6389873623847961, 0.8768579363822937, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.41306138038635254, 0.5377413034439087, 0.5487606525421143, 0.6661252975463867]  ‚Üí  acq = -0.6376005166103127
X = [0.31545931100845337, 0.2939772605895996, 0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.06392225623130798, 0.6660334467887878, 0.2530719041824341, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.3691119849681854, 0.5324565768241882, 0.4317083954811096, 0.24413615465164185]  ‚Üí  acq = -0.6359690886413273
X = [0.67851322889328, 0.8478764891624451, 0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6715726852416992, 0.0001609325408935547, 0.40142709016799927, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9496906995773315, 0.7664200663566589, 0.20589514076709747, 0.764849066734314]  ‚Üí  acq = -0.6182546726801784
X = [0.23112940788269043, 0.6169077157974243, 0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.33536380529403687, 0.6516563892364502, 0.9490533471107483, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5683628916740417, 0.7479527592658997, 0.7264589071273804, 0.12401306629180908]  ‚Üí  acq = -0.636141052640983
X = [0.9198684096336365, 0.3799903988838196, 0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.5606127381324768, 0.6152615547180176, 0.8877220153808594, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7248244285583496, 0.8123634457588196, 0.8578460216522217, 0.20193207263946533]  ‚Üí  acq = -0.6359690886413275
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0226, dtype=torch.float64), tensor(0.0233, dtype=torch.float64), tensor(0.0483, dtype=torch.float64), tensor(0.3056, dtype=torch.float64), tensor(0.2628, dtype=torch.float64), 0, 0, tensor(0.3374, dtype=torch.float64), 19, 1, 0, 1, 0, 1, 124, 0.0, 26.349023302934995, 0]
normalized proposed parameters for next round by BO: [tensor(8.5650e-18, dtype=torch.float64), tensor(0.0226, dtype=torch.float64), tensor(0.0233, dtype=torch.float64), tensor(0.0483, dtype=torch.float64), tensor(0.3056, dtype=torch.float64), tensor(0.2628, dtype=torch.float64), tensor(1.7732e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3374, dtype=torch.float64), tensor(0.5970, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9701, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5489, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.023
  rowan_hellaswag: 0.023
  sciq: 0.048
  triviaqa: 0.306
  truthfulqa_gen: 0.263
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.337

LoRA Parameters:
  lora_r: (124,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (26.349023302934995,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  124
lora dropout:  0.0
lora alpha:  26.349023302934995
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 106,151,936 || all params: 8,136,413,184 || trainable%: 1.3047
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.454, 'grad_norm': 0.4163941740989685, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.042656421661377, 'eval_runtime': 9.6757, 'eval_samples_per_second': 103.352, 'eval_steps_per_second': 6.511, 'epoch': 0.04}
{'loss': 1.5226, 'grad_norm': 0.1668262928724289, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9036494493484497, 'eval_runtime': 9.7208, 'eval_samples_per_second': 102.872, 'eval_steps_per_second': 6.481, 'epoch': 0.08}
{'loss': 1.2499, 'grad_norm': 0.1714528650045395, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9086086750030518, 'eval_runtime': 9.7106, 'eval_samples_per_second': 102.98, 'eval_steps_per_second': 6.488, 'epoch': 0.12}
{'loss': 1.1128, 'grad_norm': 0.20657892525196075, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8761781454086304, 'eval_runtime': 9.7327, 'eval_samples_per_second': 102.746, 'eval_steps_per_second': 6.473, 'epoch': 0.16}
{'loss': 0.9901, 'grad_norm': 0.16241595149040222, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.780273675918579, 'eval_runtime': 9.7451, 'eval_samples_per_second': 102.616, 'eval_steps_per_second': 6.465, 'epoch': 0.2}
{'loss': 0.9503, 'grad_norm': 0.15463966131210327, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8020973205566406, 'eval_runtime': 9.7364, 'eval_samples_per_second': 102.708, 'eval_steps_per_second': 6.471, 'epoch': 0.24}
{'loss': 0.9022, 'grad_norm': 0.20053809881210327, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.864013671875, 'eval_runtime': 9.7493, 'eval_samples_per_second': 102.571, 'eval_steps_per_second': 6.462, 'epoch': 0.28}
{'loss': 0.8968, 'grad_norm': 0.17077261209487915, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8298276662826538, 'eval_runtime': 9.7531, 'eval_samples_per_second': 102.531, 'eval_steps_per_second': 6.459, 'epoch': 0.32}
{'loss': 0.8853, 'grad_norm': 0.18406139314174652, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8133050203323364, 'eval_runtime': 9.7385, 'eval_samples_per_second': 102.686, 'eval_steps_per_second': 6.469, 'epoch': 0.36}
{'loss': 0.8772, 'grad_norm': 0.20265349745750427, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8692538738250732, 'eval_runtime': 9.7309, 'eval_samples_per_second': 102.765, 'eval_steps_per_second': 6.474, 'epoch': 0.4}
{'loss': 0.899, 'grad_norm': 0.23722539842128754, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8822673559188843, 'eval_runtime': 9.739, 'eval_samples_per_second': 102.68, 'eval_steps_per_second': 6.469, 'epoch': 0.44}
{'loss': 0.8397, 'grad_norm': 0.2342773675918579, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.881629228591919, 'eval_runtime': 9.7375, 'eval_samples_per_second': 102.695, 'eval_steps_per_second': 6.47, 'epoch': 0.48}
{'loss': 0.8361, 'grad_norm': 0.23713673651218414, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8724353313446045, 'eval_runtime': 9.7202, 'eval_samples_per_second': 102.879, 'eval_steps_per_second': 6.481, 'epoch': 0.52}
{'loss': 0.7925, 'grad_norm': 0.19565188884735107, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.874996304512024, 'eval_runtime': 9.7004, 'eval_samples_per_second': 103.088, 'eval_steps_per_second': 6.495, 'epoch': 0.56}
{'loss': 0.7476, 'grad_norm': 0.2844647467136383, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.883105993270874, 'eval_runtime': 9.7149, 'eval_samples_per_second': 102.935, 'eval_steps_per_second': 6.485, 'epoch': 0.6}
{'loss': 0.8014, 'grad_norm': 0.23890544474124908, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8684771060943604, 'eval_runtime': 9.7274, 'eval_samples_per_second': 102.803, 'eval_steps_per_second': 6.477, 'epoch': 0.64}
{'loss': 0.7482, 'grad_norm': 0.2182292491197586, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9454238414764404, 'eval_runtime': 9.7307, 'eval_samples_per_second': 102.767, 'eval_steps_per_second': 6.474, 'epoch': 0.68}
{'loss': 0.7506, 'grad_norm': 0.3257903754711151, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9426220655441284, 'eval_runtime': 9.7289, 'eval_samples_per_second': 102.787, 'eval_steps_per_second': 6.476, 'epoch': 0.72}
{'loss': 0.7276, 'grad_norm': 0.2859170436859131, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9691500663757324, 'eval_runtime': 9.7141, 'eval_samples_per_second': 102.943, 'eval_steps_per_second': 6.485, 'epoch': 0.76}
{'loss': 0.7226, 'grad_norm': 0.298442006111145, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9520927667617798, 'eval_runtime': 9.7287, 'eval_samples_per_second': 102.789, 'eval_steps_per_second': 6.476, 'epoch': 0.8}
{'loss': 0.7114, 'grad_norm': 0.3260800540447235, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9889215230941772, 'eval_runtime': 9.7271, 'eval_samples_per_second': 102.806, 'eval_steps_per_second': 6.477, 'epoch': 0.84}
{'loss': 0.7362, 'grad_norm': 0.3982299864292145, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9677844047546387, 'eval_runtime': 9.7147, 'eval_samples_per_second': 102.937, 'eval_steps_per_second': 6.485, 'epoch': 0.88}
{'loss': 0.7232, 'grad_norm': 0.33835431933403015, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9619613885879517, 'eval_runtime': 9.7422, 'eval_samples_per_second': 102.646, 'eval_steps_per_second': 6.467, 'epoch': 0.92}
{'loss': 0.6841, 'grad_norm': 0.33719077706336975, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.000854969024658, 'eval_runtime': 9.7462, 'eval_samples_per_second': 102.604, 'eval_steps_per_second': 6.464, 'epoch': 0.96}
{'loss': 0.6864, 'grad_norm': 0.32486090064048767, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9931309223175049, 'eval_runtime': 9.8108, 'eval_samples_per_second': 101.928, 'eval_steps_per_second': 6.421, 'epoch': 1.0}
{'train_runtime': 380.5319, 'train_samples_per_second': 26.271, 'train_steps_per_second': 1.642, 'train_loss': 0.9699093536376953, 'epoch': 1.0}
train_results:  {'eval_loss': [2.042656421661377, 1.9036494493484497, 1.9086086750030518, 1.8761781454086304, 1.780273675918579, 1.8020973205566406, 1.864013671875, 1.8298276662826538, 1.8133050203323364, 1.8692538738250732, 1.8822673559188843, 1.881629228591919, 1.8724353313446045, 1.874996304512024, 1.883105993270874, 1.8684771060943604, 1.9454238414764404, 1.9426220655441284, 1.9691500663757324, 1.9520927667617798, 1.9889215230941772, 1.9677844047546387, 1.9619613885879517, 2.000854969024658, 1.9931309223175049], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.042656421661377, 1.9036494493484497, 1.9086086750030518, 1.8761781454086304, 1.780273675918579, 1.8020973205566406, 1.864013671875, 1.8298276662826538, 1.8133050203323364, 1.8692538738250732, 1.8822673559188843, 1.881629228591919, 1.8724353313446045, 1.874996304512024, 1.883105993270874, 1.8684771060943604, 1.9454238414764404, 1.9426220655441284, 1.9691500663757324, 1.9520927667617798, 1.9889215230941772, 1.9677844047546387, 1.9619613885879517, 2.000854969024658, 1.9931309223175049]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.9931309223175049
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.3109 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.7146548332000475
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.714654833200046
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.7146607150281306
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.7146542926916019
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.714654833200046
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1822, dtype=torch.float64), tensor(0.2528, dtype=torch.float64), tensor(0.0934, dtype=torch.float64), tensor(0.0271, dtype=torch.float64), tensor(0.0192, dtype=torch.float64), tensor(0.0848, dtype=torch.float64), tensor(0.0808, dtype=torch.float64), 0, tensor(0.2597, dtype=torch.float64), 29, 1, 1, 1, 1, 0, 87, 0.028296481034245105, 36.37327416317987, 1]
normalized proposed parameters for next round by BO: [tensor(0.1822, dtype=torch.float64), tensor(0.2528, dtype=torch.float64), tensor(0.0934, dtype=torch.float64), tensor(0.0271, dtype=torch.float64), tensor(0.0192, dtype=torch.float64), tensor(0.0848, dtype=torch.float64), tensor(0.0808, dtype=torch.float64), tensor(2.9941e-18, dtype=torch.float64), tensor(0.2597, dtype=torch.float64), tensor(0.8928, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6834, dtype=torch.float64), tensor(0.2830, dtype=torch.float64), tensor(0.7578, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.182
  gsm8k: 0.253
  rowan_hellaswag: 0.093
  sciq: 0.027
  triviaqa: 0.019
  truthfulqa_gen: 0.085
  wikitext: 0.081
  mmlu: 0
  arc_challenge: 0.26

LoRA Parameters:
  lora_r: (87,)
  lora_dropout: (0.028296481034245105,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (36.37327416317987,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  87
lora dropout:  0.028296481034245105
lora alpha:  36.37327416317987
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 126,594,048 || all params: 8,156,855,296 || trainable%: 1.5520
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6014, 'grad_norm': 0.6593329310417175, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9185651540756226, 'eval_runtime': 10.9554, 'eval_samples_per_second': 91.279, 'eval_steps_per_second': 5.751, 'epoch': 0.04}
{'loss': 1.339, 'grad_norm': 0.5581902861595154, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7969982624053955, 'eval_runtime': 10.8757, 'eval_samples_per_second': 91.948, 'eval_steps_per_second': 5.793, 'epoch': 0.08}
{'loss': 1.1615, 'grad_norm': 0.3836503326892853, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8116302490234375, 'eval_runtime': 10.8828, 'eval_samples_per_second': 91.888, 'eval_steps_per_second': 5.789, 'epoch': 0.12}
{'loss': 1.1706, 'grad_norm': 0.322064608335495, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.769126296043396, 'eval_runtime': 10.9113, 'eval_samples_per_second': 91.648, 'eval_steps_per_second': 5.774, 'epoch': 0.16}
{'loss': 1.0836, 'grad_norm': 0.2575012445449829, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7877463102340698, 'eval_runtime': 10.9159, 'eval_samples_per_second': 91.609, 'eval_steps_per_second': 5.771, 'epoch': 0.2}
{'loss': 1.0496, 'grad_norm': 0.2640528082847595, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7932783365249634, 'eval_runtime': 10.901, 'eval_samples_per_second': 91.734, 'eval_steps_per_second': 5.779, 'epoch': 0.24}
{'loss': 1.0991, 'grad_norm': 0.2530287802219391, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.777895212173462, 'eval_runtime': 10.902, 'eval_samples_per_second': 91.726, 'eval_steps_per_second': 5.779, 'epoch': 0.28}
{'loss': 1.1158, 'grad_norm': 0.236158549785614, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8480788469314575, 'eval_runtime': 10.9146, 'eval_samples_per_second': 91.62, 'eval_steps_per_second': 5.772, 'epoch': 0.32}
{'loss': 1.0012, 'grad_norm': 0.23870939016342163, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7877141237258911, 'eval_runtime': 10.9048, 'eval_samples_per_second': 91.703, 'eval_steps_per_second': 5.777, 'epoch': 0.36}
{'loss': 1.0851, 'grad_norm': 0.26271191239356995, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.804002285003662, 'eval_runtime': 10.9062, 'eval_samples_per_second': 91.691, 'eval_steps_per_second': 5.777, 'epoch': 0.4}
{'loss': 0.9839, 'grad_norm': 0.23804259300231934, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8267314434051514, 'eval_runtime': 10.881, 'eval_samples_per_second': 91.903, 'eval_steps_per_second': 5.79, 'epoch': 0.44}
{'loss': 1.0611, 'grad_norm': 0.22371523082256317, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7690479755401611, 'eval_runtime': 10.8603, 'eval_samples_per_second': 92.079, 'eval_steps_per_second': 5.801, 'epoch': 0.48}
{'loss': 0.9838, 'grad_norm': 0.25428634881973267, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8189997673034668, 'eval_runtime': 10.8645, 'eval_samples_per_second': 92.043, 'eval_steps_per_second': 5.799, 'epoch': 0.52}
{'loss': 0.9849, 'grad_norm': 0.2661426067352295, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.803497552871704, 'eval_runtime': 10.843, 'eval_samples_per_second': 92.225, 'eval_steps_per_second': 5.81, 'epoch': 0.56}
{'loss': 0.9635, 'grad_norm': 0.25534138083457947, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8419981002807617, 'eval_runtime': 10.8396, 'eval_samples_per_second': 92.254, 'eval_steps_per_second': 5.812, 'epoch': 0.6}
{'loss': 1.0439, 'grad_norm': 0.2477140724658966, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8358981609344482, 'eval_runtime': 10.8392, 'eval_samples_per_second': 92.258, 'eval_steps_per_second': 5.812, 'epoch': 0.64}
{'loss': 0.9842, 'grad_norm': 0.30835092067718506, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8227976560592651, 'eval_runtime': 10.8489, 'eval_samples_per_second': 92.175, 'eval_steps_per_second': 5.807, 'epoch': 0.68}
{'loss': 1.0268, 'grad_norm': 0.2647823095321655, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8366178274154663, 'eval_runtime': 10.8612, 'eval_samples_per_second': 92.071, 'eval_steps_per_second': 5.8, 'epoch': 0.72}
{'loss': 0.9537, 'grad_norm': 0.28903645277023315, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.848148226737976, 'eval_runtime': 10.9167, 'eval_samples_per_second': 91.603, 'eval_steps_per_second': 5.771, 'epoch': 0.76}
{'loss': 1.0028, 'grad_norm': 0.28523436188697815, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8303042650222778, 'eval_runtime': 10.9111, 'eval_samples_per_second': 91.65, 'eval_steps_per_second': 5.774, 'epoch': 0.8}
{'loss': 0.9565, 'grad_norm': 0.3131949305534363, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8691097497940063, 'eval_runtime': 10.9235, 'eval_samples_per_second': 91.546, 'eval_steps_per_second': 5.767, 'epoch': 0.84}
{'loss': 0.9795, 'grad_norm': 0.30463463068008423, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8709157705307007, 'eval_runtime': 10.9164, 'eval_samples_per_second': 91.605, 'eval_steps_per_second': 5.771, 'epoch': 0.88}
{'loss': 1.0115, 'grad_norm': 0.3116276264190674, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8586136102676392, 'eval_runtime': 10.9082, 'eval_samples_per_second': 91.674, 'eval_steps_per_second': 5.775, 'epoch': 0.92}
{'loss': 0.8634, 'grad_norm': 0.2673158347606659, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8968673944473267, 'eval_runtime': 10.8914, 'eval_samples_per_second': 91.816, 'eval_steps_per_second': 5.784, 'epoch': 0.96}
{'loss': 0.8868, 'grad_norm': 0.3627464175224304, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8890843391418457, 'eval_runtime': 10.9501, 'eval_samples_per_second': 91.324, 'eval_steps_per_second': 5.753, 'epoch': 1.0}
{'train_runtime': 530.8751, 'train_samples_per_second': 18.827, 'train_steps_per_second': 1.177, 'train_loss': 1.0957319641113281, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9185651540756226, 1.7969982624053955, 1.8116302490234375, 1.769126296043396, 1.7877463102340698, 1.7932783365249634, 1.777895212173462, 1.8480788469314575, 1.7877141237258911, 1.804002285003662, 1.8267314434051514, 1.7690479755401611, 1.8189997673034668, 1.803497552871704, 1.8419981002807617, 1.8358981609344482, 1.8227976560592651, 1.8366178274154663, 1.848148226737976, 1.8303042650222778, 1.8691097497940063, 1.8709157705307007, 1.8586136102676392, 1.8968673944473267, 1.8890843391418457], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9185651540756226, 1.7969982624053955, 1.8116302490234375, 1.769126296043396, 1.7877463102340698, 1.7932783365249634, 1.777895212173462, 1.8480788469314575, 1.7877141237258911, 1.804002285003662, 1.8267314434051514, 1.7690479755401611, 1.8189997673034668, 1.803497552871704, 1.8419981002807617, 1.8358981609344482, 1.8227976560592651, 1.8366178274154663, 1.848148226737976, 1.8303042650222778, 1.8691097497940063, 1.8709157705307007, 1.8586136102676392, 1.8968673944473267, 1.8890843391418457]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.8890843391418457
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 26.6888 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.7333828382715669
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.7262900501459307
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.7262900501459307
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.726290050145931
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.7262900513735232
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0413, dtype=torch.float64), tensor(0.0511, dtype=torch.float64), tensor(0.1473, dtype=torch.float64), 0, 0, tensor(0.0637, dtype=torch.float64), tensor(0.2654, dtype=torch.float64), tensor(0.4201, dtype=torch.float64), 18, 0, 1, 0, 1, 1, 90, 1.2542689694135411e-20, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0087, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0.0511, dtype=torch.float64), tensor(0.1473, dtype=torch.float64), tensor(1.4207e-18, dtype=torch.float64), tensor(0.0024, dtype=torch.float64), tensor(0.0637, dtype=torch.float64), tensor(0.2654, dtype=torch.float64), tensor(0.4201, dtype=torch.float64), tensor(0.5685, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7066, dtype=torch.float64), tensor(1.2543e-19, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.041
  rowan_hellaswag: 0.051
  sciq: 0.147
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.064
  mmlu: 0.265
  arc_challenge: 0.42

LoRA Parameters:
  lora_r: (90,)
  lora_dropout: (1.2542689694135411e-20,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  90
lora dropout:  1.2542689694135411e-20
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 68,014,080 || all params: 8,098,275,328 || trainable%: 0.8399
length of training data:  9884
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8076, 'grad_norm': 0.8435684442520142, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6814993619918823, 'eval_runtime': 9.5523, 'eval_samples_per_second': 104.687, 'eval_steps_per_second': 6.595, 'epoch': 0.04}
{'loss': 1.4269, 'grad_norm': 0.5796507000923157, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4776203632354736, 'eval_runtime': 9.4762, 'eval_samples_per_second': 105.527, 'eval_steps_per_second': 6.648, 'epoch': 0.08}
{'loss': 1.2606, 'grad_norm': 0.4288865327835083, 'learning_rate': 0.0002873239436619718, 'epoch': 0.12}
{'eval_loss': 1.3694723844528198, 'eval_runtime': 9.4745, 'eval_samples_per_second': 105.546, 'eval_steps_per_second': 6.649, 'epoch': 0.12}
{'loss': 1.1389, 'grad_norm': 0.46189600229263306, 'learning_rate': 0.0002741197183098591, 'epoch': 0.16}
{'eval_loss': 1.333624005317688, 'eval_runtime': 9.5014, 'eval_samples_per_second': 105.248, 'eval_steps_per_second': 6.631, 'epoch': 0.16}
{'loss': 1.1661, 'grad_norm': 0.3909499943256378, 'learning_rate': 0.0002609154929577464, 'epoch': 0.2}
{'eval_loss': 1.308813214302063, 'eval_runtime': 9.5176, 'eval_samples_per_second': 105.068, 'eval_steps_per_second': 6.619, 'epoch': 0.2}
{'loss': 1.071, 'grad_norm': 0.3519481420516968, 'learning_rate': 0.0002477112676056338, 'epoch': 0.24}
{'eval_loss': 1.310483694076538, 'eval_runtime': 9.5316, 'eval_samples_per_second': 104.914, 'eval_steps_per_second': 6.61, 'epoch': 0.24}
{'loss': 1.1251, 'grad_norm': 0.3276318907737732, 'learning_rate': 0.00023450704225352109, 'epoch': 0.28}
{'eval_loss': 1.3054388761520386, 'eval_runtime': 9.5438, 'eval_samples_per_second': 104.78, 'eval_steps_per_second': 6.601, 'epoch': 0.28}
{'loss': 1.0756, 'grad_norm': 0.4003286063671112, 'learning_rate': 0.00022130281690140843, 'epoch': 0.32}
{'eval_loss': 1.2840286493301392, 'eval_runtime': 9.5408, 'eval_samples_per_second': 104.813, 'eval_steps_per_second': 6.603, 'epoch': 0.32}
{'loss': 1.079, 'grad_norm': 0.34756097197532654, 'learning_rate': 0.00020809859154929575, 'epoch': 0.36}
{'eval_loss': 1.2925564050674438, 'eval_runtime': 9.5461, 'eval_samples_per_second': 104.754, 'eval_steps_per_second': 6.6, 'epoch': 0.36}
{'loss': 1.0833, 'grad_norm': 0.42100954055786133, 'learning_rate': 0.00019489436619718307, 'epoch': 0.4}
{'eval_loss': 1.2830696105957031, 'eval_runtime': 9.548, 'eval_samples_per_second': 104.734, 'eval_steps_per_second': 6.598, 'epoch': 0.4}
{'loss': 1.0929, 'grad_norm': 0.3503957688808441, 'learning_rate': 0.0001816901408450704, 'epoch': 0.44}
{'eval_loss': 1.2800098657608032, 'eval_runtime': 9.5329, 'eval_samples_per_second': 104.9, 'eval_steps_per_second': 6.609, 'epoch': 0.44}
{'loss': 1.0274, 'grad_norm': 0.4718826711177826, 'learning_rate': 0.0001684859154929577, 'epoch': 0.49}
{'eval_loss': 1.27936589717865, 'eval_runtime': 9.5249, 'eval_samples_per_second': 104.987, 'eval_steps_per_second': 6.614, 'epoch': 0.49}
{'loss': 1.0112, 'grad_norm': 0.3836912214756012, 'learning_rate': 0.00015528169014084506, 'epoch': 0.53}
{'eval_loss': 1.27631413936615, 'eval_runtime': 9.5015, 'eval_samples_per_second': 105.246, 'eval_steps_per_second': 6.63, 'epoch': 0.53}
{'loss': 1.0819, 'grad_norm': 0.4258478879928589, 'learning_rate': 0.00014207746478873238, 'epoch': 0.57}
{'eval_loss': 1.270402431488037, 'eval_runtime': 9.4792, 'eval_samples_per_second': 105.494, 'eval_steps_per_second': 6.646, 'epoch': 0.57}
{'loss': 0.9243, 'grad_norm': 0.4632588028907776, 'learning_rate': 0.0001288732394366197, 'epoch': 0.61}
{'eval_loss': 1.2704421281814575, 'eval_runtime': 9.4747, 'eval_samples_per_second': 105.545, 'eval_steps_per_second': 6.649, 'epoch': 0.61}
{'loss': 0.9808, 'grad_norm': 0.579961895942688, 'learning_rate': 0.00011566901408450703, 'epoch': 0.65}
{'eval_loss': 1.2650150060653687, 'eval_runtime': 9.4859, 'eval_samples_per_second': 105.419, 'eval_steps_per_second': 6.641, 'epoch': 0.65}
{'loss': 0.9605, 'grad_norm': 0.40252697467803955, 'learning_rate': 0.00010246478873239435, 'epoch': 0.69}
{'eval_loss': 1.2674663066864014, 'eval_runtime': 9.4768, 'eval_samples_per_second': 105.521, 'eval_steps_per_second': 6.648, 'epoch': 0.69}
{'loss': 1.0103, 'grad_norm': 0.4851559102535248, 'learning_rate': 8.926056338028169e-05, 'epoch': 0.73}
{'eval_loss': 1.265439510345459, 'eval_runtime': 9.5219, 'eval_samples_per_second': 105.021, 'eval_steps_per_second': 6.616, 'epoch': 0.73}
{'loss': 1.0098, 'grad_norm': 0.4321829378604889, 'learning_rate': 7.6056338028169e-05, 'epoch': 0.77}
{'eval_loss': 1.2604728937149048, 'eval_runtime': 9.475, 'eval_samples_per_second': 105.541, 'eval_steps_per_second': 6.649, 'epoch': 0.77}
{'loss': 0.9326, 'grad_norm': 0.42081764340400696, 'learning_rate': 6.285211267605634e-05, 'epoch': 0.81}
{'eval_loss': 1.2602061033248901, 'eval_runtime': 9.4815, 'eval_samples_per_second': 105.469, 'eval_steps_per_second': 6.645, 'epoch': 0.81}
{'loss': 0.9665, 'grad_norm': 0.5551823973655701, 'learning_rate': 4.964788732394366e-05, 'epoch': 0.85}
{'eval_loss': 1.2556142807006836, 'eval_runtime': 9.4746, 'eval_samples_per_second': 105.545, 'eval_steps_per_second': 6.649, 'epoch': 0.85}
{'loss': 0.9948, 'grad_norm': 0.48615264892578125, 'learning_rate': 3.6443661971830985e-05, 'epoch': 0.89}
{'eval_loss': 1.2522755861282349, 'eval_runtime': 9.4637, 'eval_samples_per_second': 105.667, 'eval_steps_per_second': 6.657, 'epoch': 0.89}
{'loss': 0.9159, 'grad_norm': 0.47621864080429077, 'learning_rate': 2.3239436619718305e-05, 'epoch': 0.93}
{'eval_loss': 1.2530659437179565, 'eval_runtime': 9.4666, 'eval_samples_per_second': 105.634, 'eval_steps_per_second': 6.655, 'epoch': 0.93}
{'loss': 0.8454, 'grad_norm': 0.5336098670959473, 'learning_rate': 1.0035211267605631e-05, 'epoch': 0.97}
{'eval_loss': 1.2530133724212646, 'eval_runtime': 9.4664, 'eval_samples_per_second': 105.637, 'eval_steps_per_second': 6.655, 'epoch': 0.97}
{'train_runtime': 435.6901, 'train_samples_per_second': 22.686, 'train_steps_per_second': 1.418, 'train_loss': 1.1177619730384605, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6814993619918823, 1.4776203632354736, 1.3694723844528198, 1.333624005317688, 1.308813214302063, 1.310483694076538, 1.3054388761520386, 1.2840286493301392, 1.2925564050674438, 1.2830696105957031, 1.2800098657608032, 1.27936589717865, 1.27631413936615, 1.270402431488037, 1.2704421281814575, 1.2650150060653687, 1.2674663066864014, 1.265439510345459, 1.2604728937149048, 1.2602061033248901, 1.2556142807006836, 1.2522755861282349, 1.2530659437179565, 1.2530133724212646], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.6814993619918823, 1.4776203632354736, 1.3694723844528198, 1.333624005317688, 1.308813214302063, 1.310483694076538, 1.3054388761520386, 1.2840286493301392, 1.2925564050674438, 1.2830696105957031, 1.2800098657608032, 1.27936589717865, 1.27631413936615, 1.270402431488037, 1.2704421281814575, 1.2650150060653687, 1.2674663066864014, 1.265439510345459, 1.2604728937149048, 1.2602061033248901, 1.2556142807006836, 1.2522755861282349, 1.2530659437179565, 1.2530133724212646]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2530133724212646
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.9099 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1560913324356079, 0.29771536588668823, 0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.4007338583469391, 0.8963236808776855, 0.5591384768486023, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.42323532700538635, 0.003319978713989258, 0.666995644569397, 0.6627236008644104]  ‚Üí  acq = -0.728635816562611
X = [0.6140679717063904, 0.7597888708114624, 0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.5511543154716492, 0.10114860534667969, 0.46051692962646484, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6117270588874817, 0.06090492010116577, 0.29019129276275635, 0.544792890548706]  ‚Üí  acq = -0.728635816562611
X = [0.4161165952682495, 0.6534545421600342, 0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9124553799629211, 0.5554662346839905, 0.09747803211212158, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9407435655593872, 0.9420399069786072, 0.717787504196167, 0.5669505000114441]  ‚Üí  acq = -0.728635816562611
X = [0.11413401365280151, 0.7658460736274719, 0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.20504699647426605, 0.40315020084381104, 0.9066953659057617, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7133153080940247, 0.3376033306121826, 0.9497083425521851, 0.04203289747238159]  ‚Üí  acq = -0.7286358166038356
X = [0.7649766802787781, 0.17325276136398315, 0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7676264643669128, 0.04699522256851196, 0.9299392104148865, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.02683671936392784, 0.7503892779350281, 0.20525000989437103, 0.7006362080574036]  ‚Üí  acq = -0.728635816562611
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0916, dtype=torch.float64), tensor(0.0185, dtype=torch.float64), tensor(0.3673, dtype=torch.float64), tensor(0.0774, dtype=torch.float64), tensor(0.1323, dtype=torch.float64), tensor(0.0430, dtype=torch.float64), tensor(0.1363, dtype=torch.float64), tensor(0.1337, dtype=torch.float64), 20, 0, 0, 1, 0, 1, 65, 0.024659687825963585, 25.132416168030776, 0]
normalized proposed parameters for next round by BO: [tensor(6.3569e-19, dtype=torch.float64), tensor(0.0916, dtype=torch.float64), tensor(0.0185, dtype=torch.float64), tensor(0.3673, dtype=torch.float64), tensor(0.0774, dtype=torch.float64), tensor(0.1323, dtype=torch.float64), tensor(0.0430, dtype=torch.float64), tensor(0.1363, dtype=torch.float64), tensor(0.1337, dtype=torch.float64), tensor(0.6123, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5083, dtype=torch.float64), tensor(0.2466, dtype=torch.float64), tensor(0.5236, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.092
  rowan_hellaswag: 0.019
  sciq: 0.367
  triviaqa: 0.077
  truthfulqa_gen: 0.132
  wikitext: 0.043
  mmlu: 0.136
  arc_challenge: 0.134

LoRA Parameters:
  lora_r: (65,)
  lora_dropout: (0.024659687825963585,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (25.132416168030776,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  65
lora dropout:  0.024659687825963585
lora alpha:  25.132416168030776
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 47,923,200 || all params: 8,078,184,448 || trainable%: 0.5932
length of training data:  9994
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1716, 'grad_norm': 0.5191206336021423, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9594064950942993, 'eval_runtime': 9.5128, 'eval_samples_per_second': 105.122, 'eval_steps_per_second': 6.623, 'epoch': 0.04}
{'loss': 1.5661, 'grad_norm': 0.23363423347473145, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6192973852157593, 'eval_runtime': 9.5789, 'eval_samples_per_second': 104.396, 'eval_steps_per_second': 6.577, 'epoch': 0.08}
{'loss': 1.3186, 'grad_norm': 0.25329628586769104, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5207128524780273, 'eval_runtime': 9.6094, 'eval_samples_per_second': 104.065, 'eval_steps_per_second': 6.556, 'epoch': 0.12}
{'loss': 1.218, 'grad_norm': 0.3370545208454132, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3966772556304932, 'eval_runtime': 9.7181, 'eval_samples_per_second': 102.9, 'eval_steps_per_second': 6.483, 'epoch': 0.16}
{'loss': 1.1796, 'grad_norm': 0.27896881103515625, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3371821641921997, 'eval_runtime': 9.728, 'eval_samples_per_second': 102.796, 'eval_steps_per_second': 6.476, 'epoch': 0.2}
{'loss': 1.056, 'grad_norm': 0.25075793266296387, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3329577445983887, 'eval_runtime': 9.7168, 'eval_samples_per_second': 102.914, 'eval_steps_per_second': 6.484, 'epoch': 0.24}
{'loss': 1.0898, 'grad_norm': 0.22317110002040863, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3338099718093872, 'eval_runtime': 9.7322, 'eval_samples_per_second': 102.752, 'eval_steps_per_second': 6.473, 'epoch': 0.28}
{'loss': 1.0706, 'grad_norm': 0.19102469086647034, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.321582317352295, 'eval_runtime': 9.7413, 'eval_samples_per_second': 102.656, 'eval_steps_per_second': 6.467, 'epoch': 0.32}
{'loss': 1.0822, 'grad_norm': 0.2094336748123169, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.322900414466858, 'eval_runtime': 9.859, 'eval_samples_per_second': 101.43, 'eval_steps_per_second': 6.39, 'epoch': 0.36}
{'loss': 1.0057, 'grad_norm': 0.2096634954214096, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3237311840057373, 'eval_runtime': 9.9451, 'eval_samples_per_second': 100.552, 'eval_steps_per_second': 6.335, 'epoch': 0.4}
{'loss': 1.0478, 'grad_norm': 0.22225084900856018, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3098397254943848, 'eval_runtime': 9.9345, 'eval_samples_per_second': 100.659, 'eval_steps_per_second': 6.342, 'epoch': 0.44}
{'loss': 0.9554, 'grad_norm': 0.21113570034503937, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3130064010620117, 'eval_runtime': 9.9369, 'eval_samples_per_second': 100.635, 'eval_steps_per_second': 6.34, 'epoch': 0.48}
{'loss': 1.0491, 'grad_norm': 0.240218847990036, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3104115724563599, 'eval_runtime': 9.9309, 'eval_samples_per_second': 100.696, 'eval_steps_per_second': 6.344, 'epoch': 0.52}
{'loss': 0.9926, 'grad_norm': 0.20660638809204102, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.30586576461792, 'eval_runtime': 9.9233, 'eval_samples_per_second': 100.773, 'eval_steps_per_second': 6.349, 'epoch': 0.56}
{'loss': 1.0127, 'grad_norm': 0.2134377807378769, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3059778213500977, 'eval_runtime': 10.0164, 'eval_samples_per_second': 99.837, 'eval_steps_per_second': 6.29, 'epoch': 0.6}
{'loss': 1.0429, 'grad_norm': 0.2244999259710312, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3010687828063965, 'eval_runtime': 9.8977, 'eval_samples_per_second': 101.034, 'eval_steps_per_second': 6.365, 'epoch': 0.64}
{'loss': 1.004, 'grad_norm': 0.24238377809524536, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3014217615127563, 'eval_runtime': 9.8561, 'eval_samples_per_second': 101.46, 'eval_steps_per_second': 6.392, 'epoch': 0.68}
{'loss': 0.9631, 'grad_norm': 0.2158689945936203, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3012648820877075, 'eval_runtime': 9.735, 'eval_samples_per_second': 102.722, 'eval_steps_per_second': 6.471, 'epoch': 0.72}
{'loss': 0.9785, 'grad_norm': 0.22664299607276917, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2973322868347168, 'eval_runtime': 10.0563, 'eval_samples_per_second': 99.44, 'eval_steps_per_second': 6.265, 'epoch': 0.76}
{'loss': 0.9698, 'grad_norm': 0.24517489969730377, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2991948127746582, 'eval_runtime': 11.4796, 'eval_samples_per_second': 87.111, 'eval_steps_per_second': 5.488, 'epoch': 0.8}
{'loss': 0.9409, 'grad_norm': 0.26424524188041687, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2945442199707031, 'eval_runtime': 11.2351, 'eval_samples_per_second': 89.007, 'eval_steps_per_second': 5.607, 'epoch': 0.84}
{'loss': 0.9744, 'grad_norm': 0.2620941400527954, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2939057350158691, 'eval_runtime': 11.5036, 'eval_samples_per_second': 86.929, 'eval_steps_per_second': 5.477, 'epoch': 0.88}
{'loss': 1.0072, 'grad_norm': 0.213899627327919, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2944166660308838, 'eval_runtime': 11.9217, 'eval_samples_per_second': 83.88, 'eval_steps_per_second': 5.284, 'epoch': 0.92}
{'loss': 0.9487, 'grad_norm': 0.21050536632537842, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2931115627288818, 'eval_runtime': 11.6365, 'eval_samples_per_second': 85.937, 'eval_steps_per_second': 5.414, 'epoch': 0.96}
{'loss': 0.9948, 'grad_norm': 0.3371274173259735, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2924851179122925, 'eval_runtime': 11.2182, 'eval_samples_per_second': 89.141, 'eval_steps_per_second': 5.616, 'epoch': 1.0}
{'train_runtime': 435.7555, 'train_samples_per_second': 22.935, 'train_steps_per_second': 1.434, 'train_loss': 1.1455977569580078, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9594064950942993, 1.6192973852157593, 1.5207128524780273, 1.3966772556304932, 1.3371821641921997, 1.3329577445983887, 1.3338099718093872, 1.321582317352295, 1.322900414466858, 1.3237311840057373, 1.3098397254943848, 1.3130064010620117, 1.3104115724563599, 1.30586576461792, 1.3059778213500977, 1.3010687828063965, 1.3014217615127563, 1.3012648820877075, 1.2973322868347168, 1.2991948127746582, 1.2945442199707031, 1.2939057350158691, 1.2944166660308838, 1.2931115627288818, 1.2924851179122925], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9594064950942993, 1.6192973852157593, 1.5207128524780273, 1.3966772556304932, 1.3371821641921997, 1.3329577445983887, 1.3338099718093872, 1.321582317352295, 1.322900414466858, 1.3237311840057373, 1.3098397254943848, 1.3130064010620117, 1.3104115724563599, 1.30586576461792, 1.3059778213500977, 1.3010687828063965, 1.3014217615127563, 1.3012648820877075, 1.2973322868347168, 1.2991948127746582, 1.2945442199707031, 1.2939057350158691, 1.2944166660308838, 1.2931115627288818, 1.2924851179122925]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2924851179122925
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.5521 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.7551622851779985
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.7469338141199277
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.7552517025410634
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.7551598603263827
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.7552900715920345
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1787, dtype=torch.float64), 0, tensor(0.0666, dtype=torch.float64), tensor(0.0572, dtype=torch.float64), tensor(0.4003, dtype=torch.float64), tensor(0.2590, dtype=torch.float64), tensor(0.0382, dtype=torch.float64), 0, 0, 28, 0, 0, 1, 0, 1, 10, 0.041174130864611844, 1.4800000190734866, 0]
normalized proposed parameters for next round by BO: [tensor(0.1787, dtype=torch.float64), tensor(7.9469e-18, dtype=torch.float64), tensor(0.0666, dtype=torch.float64), tensor(0.0572, dtype=torch.float64), tensor(0.4003, dtype=torch.float64), tensor(0.2590, dtype=torch.float64), tensor(0.0382, dtype=torch.float64), tensor(1.3635e-18, dtype=torch.float64), tensor(6.5996e-18, dtype=torch.float64), tensor(0.8612, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0786, dtype=torch.float64), tensor(0.4117, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.179
  gsm8k: 0
  rowan_hellaswag: 0.067
  sciq: 0.057
  triviaqa: 0.4
  truthfulqa_gen: 0.259
  wikitext: 0.038
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (10,)
  lora_dropout: (0.041174130864611844,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  10
lora dropout:  0.041174130864611844
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 10,321,920 || all params: 8,040,583,168 || trainable%: 0.1284
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.8401, 'grad_norm': 0.5505985021591187, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.8239569664001465, 'eval_runtime': 9.5594, 'eval_samples_per_second': 104.609, 'eval_steps_per_second': 6.59, 'epoch': 0.04}
{'loss': 2.701, 'grad_norm': 0.2519027292728424, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.1505258083343506, 'eval_runtime': 9.5596, 'eval_samples_per_second': 104.607, 'eval_steps_per_second': 6.59, 'epoch': 0.08}
{'loss': 1.8312, 'grad_norm': 0.40903130173683167, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1254091262817383, 'eval_runtime': 9.6293, 'eval_samples_per_second': 103.849, 'eval_steps_per_second': 6.543, 'epoch': 0.12}
{'loss': 1.622, 'grad_norm': 0.2065063863992691, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0764448642730713, 'eval_runtime': 9.6615, 'eval_samples_per_second': 103.503, 'eval_steps_per_second': 6.521, 'epoch': 0.16}
{'loss': 1.4641, 'grad_norm': 0.18418395519256592, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.089270830154419, 'eval_runtime': 9.7296, 'eval_samples_per_second': 102.779, 'eval_steps_per_second': 6.475, 'epoch': 0.2}
{'loss': 1.438, 'grad_norm': 0.14658822119235992, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1272618770599365, 'eval_runtime': 9.7265, 'eval_samples_per_second': 102.812, 'eval_steps_per_second': 6.477, 'epoch': 0.24}
{'loss': 1.4047, 'grad_norm': 0.18326331675052643, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.0994746685028076, 'eval_runtime': 9.7175, 'eval_samples_per_second': 102.907, 'eval_steps_per_second': 6.483, 'epoch': 0.28}
{'loss': 1.3718, 'grad_norm': 0.1922353059053421, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.0913026332855225, 'eval_runtime': 9.734, 'eval_samples_per_second': 102.733, 'eval_steps_per_second': 6.472, 'epoch': 0.32}
{'loss': 1.3429, 'grad_norm': 0.21884670853614807, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.0854909420013428, 'eval_runtime': 9.7331, 'eval_samples_per_second': 102.743, 'eval_steps_per_second': 6.473, 'epoch': 0.36}
{'loss': 1.2796, 'grad_norm': 0.3608710467815399, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.072129249572754, 'eval_runtime': 9.7238, 'eval_samples_per_second': 102.841, 'eval_steps_per_second': 6.479, 'epoch': 0.4}
{'loss': 1.2097, 'grad_norm': 0.22680355608463287, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0624258518218994, 'eval_runtime': 9.7119, 'eval_samples_per_second': 102.967, 'eval_steps_per_second': 6.487, 'epoch': 0.44}
{'loss': 1.2337, 'grad_norm': 0.19684024155139923, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.026500701904297, 'eval_runtime': 9.7074, 'eval_samples_per_second': 103.014, 'eval_steps_per_second': 6.49, 'epoch': 0.48}
{'loss': 1.1923, 'grad_norm': 0.18219642341136932, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.0374066829681396, 'eval_runtime': 9.7151, 'eval_samples_per_second': 102.933, 'eval_steps_per_second': 6.485, 'epoch': 0.52}
{'loss': 1.154, 'grad_norm': 0.15483339130878448, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.00614595413208, 'eval_runtime': 9.7039, 'eval_samples_per_second': 103.052, 'eval_steps_per_second': 6.492, 'epoch': 0.56}
{'loss': 1.1375, 'grad_norm': 0.1816493272781372, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.053009033203125, 'eval_runtime': 9.7108, 'eval_samples_per_second': 102.978, 'eval_steps_per_second': 6.488, 'epoch': 0.6}
{'loss': 1.1585, 'grad_norm': 0.17765653133392334, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.0497615337371826, 'eval_runtime': 9.7315, 'eval_samples_per_second': 102.759, 'eval_steps_per_second': 6.474, 'epoch': 0.64}
{'loss': 1.1343, 'grad_norm': 0.218537375330925, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.0603270530700684, 'eval_runtime': 9.7255, 'eval_samples_per_second': 102.823, 'eval_steps_per_second': 6.478, 'epoch': 0.68}
{'loss': 1.1239, 'grad_norm': 0.2218484729528427, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.056605577468872, 'eval_runtime': 9.7252, 'eval_samples_per_second': 102.826, 'eval_steps_per_second': 6.478, 'epoch': 0.72}
{'loss': 1.1298, 'grad_norm': 0.2132778912782669, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.0648281574249268, 'eval_runtime': 9.7165, 'eval_samples_per_second': 102.918, 'eval_steps_per_second': 6.484, 'epoch': 0.76}
{'loss': 1.0644, 'grad_norm': 0.19707131385803223, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.0778286457061768, 'eval_runtime': 9.7097, 'eval_samples_per_second': 102.99, 'eval_steps_per_second': 6.488, 'epoch': 0.8}
{'loss': 1.0778, 'grad_norm': 0.2264937311410904, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.06740665435791, 'eval_runtime': 9.7038, 'eval_samples_per_second': 103.052, 'eval_steps_per_second': 6.492, 'epoch': 0.84}
{'loss': 0.9995, 'grad_norm': 0.21929119527339935, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0690910816192627, 'eval_runtime': 9.7014, 'eval_samples_per_second': 103.078, 'eval_steps_per_second': 6.494, 'epoch': 0.88}
{'loss': 1.1555, 'grad_norm': 0.30018991231918335, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0815107822418213, 'eval_runtime': 9.7235, 'eval_samples_per_second': 102.843, 'eval_steps_per_second': 6.479, 'epoch': 0.92}
{'loss': 1.0834, 'grad_norm': 0.20699186623096466, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0887579917907715, 'eval_runtime': 9.69, 'eval_samples_per_second': 103.199, 'eval_steps_per_second': 6.502, 'epoch': 0.96}
{'loss': 1.1633, 'grad_norm': 0.1943923383951187, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.0904347896575928, 'eval_runtime': 9.7241, 'eval_samples_per_second': 102.837, 'eval_steps_per_second': 6.479, 'epoch': 1.0}
{'train_runtime': 402.6824, 'train_samples_per_second': 24.826, 'train_steps_per_second': 1.552, 'train_loss': 1.4525281097412108, 'epoch': 1.0}
train_results:  {'eval_loss': [2.8239569664001465, 2.1505258083343506, 2.1254091262817383, 2.0764448642730713, 2.089270830154419, 2.1272618770599365, 2.0994746685028076, 2.0913026332855225, 2.0854909420013428, 2.072129249572754, 2.0624258518218994, 2.026500701904297, 2.0374066829681396, 2.00614595413208, 2.053009033203125, 2.0497615337371826, 2.0603270530700684, 2.056605577468872, 2.0648281574249268, 2.0778286457061768, 2.06740665435791, 2.0690910816192627, 2.0815107822418213, 2.0887579917907715, 2.0904347896575928], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.8239569664001465, 2.1505258083343506, 2.1254091262817383, 2.0764448642730713, 2.089270830154419, 2.1272618770599365, 2.0994746685028076, 2.0913026332855225, 2.0854909420013428, 2.072129249572754, 2.0624258518218994, 2.026500701904297, 2.0374066829681396, 2.00614595413208, 2.053009033203125, 2.0497615337371826, 2.0603270530700684, 2.056605577468872, 2.0648281574249268, 2.0778286457061768, 2.06740665435791, 2.0690910816192627, 2.0815107822418213, 2.0887579917907715, 2.0904347896575928]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.0904347896575928
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9336 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.8237841604720763
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.82590925379613
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.8242818962677205
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.8237040459382394
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.8237040011326913
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0963, dtype=torch.float64), tensor(0.2036, dtype=torch.float64), tensor(0.0879, dtype=torch.float64), 0, tensor(0.2573, dtype=torch.float64), tensor(0.0242, dtype=torch.float64), tensor(0.0789, dtype=torch.float64), tensor(0.1284, dtype=torch.float64), tensor(0.1235, dtype=torch.float64), 24, 1, 1, 0, 1, 0, 41, 0.07837922769276573, 20.50951351735384, 1]
normalized proposed parameters for next round by BO: [tensor(0.0963, dtype=torch.float64), tensor(0.2036, dtype=torch.float64), tensor(0.0879, dtype=torch.float64), tensor(1.5695e-18, dtype=torch.float64), tensor(0.2573, dtype=torch.float64), tensor(0.0242, dtype=torch.float64), tensor(0.0789, dtype=torch.float64), tensor(0.1284, dtype=torch.float64), tensor(0.1235, dtype=torch.float64), tensor(0.7472, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3170, dtype=torch.float64), tensor(0.7838, dtype=torch.float64), tensor(0.4273, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.096
  gsm8k: 0.204
  rowan_hellaswag: 0.088
  sciq: 0
  triviaqa: 0.257
  truthfulqa_gen: 0.024
  wikitext: 0.079
  mmlu: 0.128
  arc_challenge: 0.123

LoRA Parameters:
  lora_r: (41,)
  lora_dropout: (0.07837922769276573,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (20.50951351735384,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  41
lora dropout:  0.07837922769276573
lora alpha:  20.50951351735384
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 31,236,096 || all params: 8,061,497,344 || trainable%: 0.3875
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0754, 'grad_norm': 1.6671065092086792, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9159296751022339, 'eval_runtime': 9.7478, 'eval_samples_per_second': 102.587, 'eval_steps_per_second': 6.463, 'epoch': 0.04}
{'loss': 1.6169, 'grad_norm': 1.0668553113937378, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.57780122756958, 'eval_runtime': 9.7253, 'eval_samples_per_second': 102.824, 'eval_steps_per_second': 6.478, 'epoch': 0.08}
{'loss': 1.2982, 'grad_norm': 0.4392218291759491, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5024794340133667, 'eval_runtime': 9.7269, 'eval_samples_per_second': 102.807, 'eval_steps_per_second': 6.477, 'epoch': 0.12}
{'loss': 1.3242, 'grad_norm': 0.40517228841781616, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4533525705337524, 'eval_runtime': 9.7804, 'eval_samples_per_second': 102.245, 'eval_steps_per_second': 6.441, 'epoch': 0.16}
{'loss': 1.2385, 'grad_norm': 0.3146551847457886, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3959587812423706, 'eval_runtime': 9.7992, 'eval_samples_per_second': 102.05, 'eval_steps_per_second': 6.429, 'epoch': 0.2}
{'loss': 1.1993, 'grad_norm': 0.48770061135292053, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3540561199188232, 'eval_runtime': 9.8259, 'eval_samples_per_second': 101.772, 'eval_steps_per_second': 6.412, 'epoch': 0.24}
{'loss': 1.2614, 'grad_norm': 0.3944004476070404, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3388056755065918, 'eval_runtime': 9.7722, 'eval_samples_per_second': 102.331, 'eval_steps_per_second': 6.447, 'epoch': 0.28}
{'loss': 1.1853, 'grad_norm': 0.37078049778938293, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.324151635169983, 'eval_runtime': 9.8092, 'eval_samples_per_second': 101.945, 'eval_steps_per_second': 6.423, 'epoch': 0.32}
{'loss': 1.145, 'grad_norm': 0.355307400226593, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3116828203201294, 'eval_runtime': 9.8142, 'eval_samples_per_second': 101.893, 'eval_steps_per_second': 6.419, 'epoch': 0.36}
{'loss': 1.1691, 'grad_norm': 0.314841091632843, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3010766506195068, 'eval_runtime': 9.8669, 'eval_samples_per_second': 101.349, 'eval_steps_per_second': 6.385, 'epoch': 0.4}
{'loss': 1.2046, 'grad_norm': 0.3157694935798645, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.294797658920288, 'eval_runtime': 9.8592, 'eval_samples_per_second': 101.428, 'eval_steps_per_second': 6.39, 'epoch': 0.44}
{'loss': 1.1463, 'grad_norm': 0.25208255648612976, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2899365425109863, 'eval_runtime': 9.8275, 'eval_samples_per_second': 101.755, 'eval_steps_per_second': 6.411, 'epoch': 0.48}
{'loss': 1.173, 'grad_norm': 0.3271847367286682, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.285940170288086, 'eval_runtime': 9.8099, 'eval_samples_per_second': 101.938, 'eval_steps_per_second': 6.422, 'epoch': 0.52}
{'loss': 1.1344, 'grad_norm': 0.3303543031215668, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2896912097930908, 'eval_runtime': 9.8086, 'eval_samples_per_second': 101.952, 'eval_steps_per_second': 6.423, 'epoch': 0.56}
{'loss': 1.1619, 'grad_norm': 0.3101882040500641, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2821497917175293, 'eval_runtime': 9.8022, 'eval_samples_per_second': 102.018, 'eval_steps_per_second': 6.427, 'epoch': 0.6}
{'loss': 1.1387, 'grad_norm': 0.24189525842666626, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2794030904769897, 'eval_runtime': 9.8185, 'eval_samples_per_second': 101.848, 'eval_steps_per_second': 6.416, 'epoch': 0.64}
{'loss': 1.2039, 'grad_norm': 0.2736104428768158, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2792915105819702, 'eval_runtime': 9.8343, 'eval_samples_per_second': 101.685, 'eval_steps_per_second': 6.406, 'epoch': 0.68}
{'loss': 1.132, 'grad_norm': 0.27202656865119934, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2792363166809082, 'eval_runtime': 9.8102, 'eval_samples_per_second': 101.934, 'eval_steps_per_second': 6.422, 'epoch': 0.72}
{'loss': 1.1173, 'grad_norm': 0.31693148612976074, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2754319906234741, 'eval_runtime': 9.8014, 'eval_samples_per_second': 102.026, 'eval_steps_per_second': 6.428, 'epoch': 0.76}
{'loss': 1.0975, 'grad_norm': 0.26791101694107056, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2720893621444702, 'eval_runtime': 9.803, 'eval_samples_per_second': 102.009, 'eval_steps_per_second': 6.427, 'epoch': 0.8}
{'loss': 1.1681, 'grad_norm': 0.2297118902206421, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2719253301620483, 'eval_runtime': 9.8038, 'eval_samples_per_second': 102.002, 'eval_steps_per_second': 6.426, 'epoch': 0.84}
{'loss': 1.0691, 'grad_norm': 0.256509929895401, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.271176815032959, 'eval_runtime': 9.8114, 'eval_samples_per_second': 101.922, 'eval_steps_per_second': 6.421, 'epoch': 0.88}
{'loss': 1.1224, 'grad_norm': 0.3706169128417969, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2714797258377075, 'eval_runtime': 9.8107, 'eval_samples_per_second': 101.929, 'eval_steps_per_second': 6.422, 'epoch': 0.92}
{'loss': 1.1772, 'grad_norm': 0.2679681181907654, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.269514799118042, 'eval_runtime': 9.8173, 'eval_samples_per_second': 101.861, 'eval_steps_per_second': 6.417, 'epoch': 0.96}
{'loss': 1.0865, 'grad_norm': 0.3251514136791229, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2689417600631714, 'eval_runtime': 9.8195, 'eval_samples_per_second': 101.838, 'eval_steps_per_second': 6.416, 'epoch': 1.0}
{'train_runtime': 482.4971, 'train_samples_per_second': 20.715, 'train_steps_per_second': 1.295, 'train_loss': 1.265854037475586, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9159296751022339, 1.57780122756958, 1.5024794340133667, 1.4533525705337524, 1.3959587812423706, 1.3540561199188232, 1.3388056755065918, 1.324151635169983, 1.3116828203201294, 1.3010766506195068, 1.294797658920288, 1.2899365425109863, 1.285940170288086, 1.2896912097930908, 1.2821497917175293, 1.2794030904769897, 1.2792915105819702, 1.2792363166809082, 1.2754319906234741, 1.2720893621444702, 1.2719253301620483, 1.271176815032959, 1.2714797258377075, 1.269514799118042, 1.2689417600631714], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9159296751022339, 1.57780122756958, 1.5024794340133667, 1.4533525705337524, 1.3959587812423706, 1.3540561199188232, 1.3388056755065918, 1.324151635169983, 1.3116828203201294, 1.3010766506195068, 1.294797658920288, 1.2899365425109863, 1.285940170288086, 1.2896912097930908, 1.2821497917175293, 1.2794030904769897, 1.2792915105819702, 1.2792363166809082, 1.2754319906234741, 1.2720893621444702, 1.2719253301620483, 1.271176815032959, 1.2714797258377075, 1.269514799118042, 1.2689417600631714]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2689417600631714
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.8458 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.8159607185538729
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.8159619844022313
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.815943874699669
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.8174357300872794
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.8159607185538729
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0236, dtype=torch.float64), tensor(0.0347, dtype=torch.float64), tensor(0.0785, dtype=torch.float64), 0, tensor(0.1670, dtype=torch.float64), tensor(0.0274, dtype=torch.float64), tensor(0.2928, dtype=torch.float64), tensor(0.3677, dtype=torch.float64), 14, 0, 1, 0, 0, 1, 127, 0.0008728393867451921, 28.81200762677992, 0]
normalized proposed parameters for next round by BO: [tensor(0.0082, dtype=torch.float64), tensor(0.0236, dtype=torch.float64), tensor(0.0347, dtype=torch.float64), tensor(0.0785, dtype=torch.float64), tensor(3.9367e-19, dtype=torch.float64), tensor(0.1670, dtype=torch.float64), tensor(0.0274, dtype=torch.float64), tensor(0.2928, dtype=torch.float64), tensor(0.3677, dtype=torch.float64), tensor(0.4237, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9936, dtype=torch.float64), tensor(0.0087, dtype=torch.float64), tensor(0.6003, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.024
  rowan_hellaswag: 0.035
  sciq: 0.079
  triviaqa: 0
  truthfulqa_gen: 0.167
  wikitext: 0.027
  mmlu: 0.293
  arc_challenge: 0.368

LoRA Parameters:
  lora_r: (127,)
  lora_dropout: (0.0008728393867451921,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (28.81200762677992,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  127
lora dropout:  0.0008728393867451921
lora alpha:  28.81200762677992
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 41,875,456 || all params: 8,072,136,704 || trainable%: 0.5188
length of training data:  9915
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4177, 'grad_norm': 0.3702463209629059, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0768184661865234, 'eval_runtime': 9.0698, 'eval_samples_per_second': 110.255, 'eval_steps_per_second': 6.946, 'epoch': 0.04}
{'loss': 1.7404, 'grad_norm': 0.17037491500377655, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.665874719619751, 'eval_runtime': 9.105, 'eval_samples_per_second': 109.83, 'eval_steps_per_second': 6.919, 'epoch': 0.08}
{'loss': 1.5023, 'grad_norm': 0.1677631139755249, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.562005877494812, 'eval_runtime': 9.1415, 'eval_samples_per_second': 109.391, 'eval_steps_per_second': 6.892, 'epoch': 0.12}
{'loss': 1.3354, 'grad_norm': 0.19764137268066406, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.4953346252441406, 'eval_runtime': 9.1683, 'eval_samples_per_second': 109.072, 'eval_steps_per_second': 6.872, 'epoch': 0.16}
{'loss': 1.3013, 'grad_norm': 0.18689179420471191, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 1.4193940162658691, 'eval_runtime': 9.2695, 'eval_samples_per_second': 107.881, 'eval_steps_per_second': 6.797, 'epoch': 0.2}
{'loss': 1.2011, 'grad_norm': 0.17346620559692383, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 1.3752309083938599, 'eval_runtime': 9.2668, 'eval_samples_per_second': 107.912, 'eval_steps_per_second': 6.798, 'epoch': 0.24}
{'loss': 1.1951, 'grad_norm': 0.1597597897052765, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 1.3506304025650024, 'eval_runtime': 9.2921, 'eval_samples_per_second': 107.618, 'eval_steps_per_second': 6.78, 'epoch': 0.28}
{'loss': 1.1522, 'grad_norm': 0.1650085598230362, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 1.346181869506836, 'eval_runtime': 9.2933, 'eval_samples_per_second': 107.604, 'eval_steps_per_second': 6.779, 'epoch': 0.32}
{'loss': 1.1377, 'grad_norm': 0.14713868498802185, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 1.3472965955734253, 'eval_runtime': 9.3141, 'eval_samples_per_second': 107.365, 'eval_steps_per_second': 6.764, 'epoch': 0.36}
{'loss': 1.1537, 'grad_norm': 0.15654009580612183, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 1.3418307304382324, 'eval_runtime': 9.3003, 'eval_samples_per_second': 107.524, 'eval_steps_per_second': 6.774, 'epoch': 0.4}
{'loss': 1.0768, 'grad_norm': 0.1637449860572815, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 1.3410691022872925, 'eval_runtime': 9.2868, 'eval_samples_per_second': 107.68, 'eval_steps_per_second': 6.784, 'epoch': 0.44}
{'loss': 1.123, 'grad_norm': 0.18060564994812012, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 1.3399806022644043, 'eval_runtime': 9.2893, 'eval_samples_per_second': 107.651, 'eval_steps_per_second': 6.782, 'epoch': 0.48}
{'loss': 1.1458, 'grad_norm': 0.16125421226024628, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 1.3356943130493164, 'eval_runtime': 9.2872, 'eval_samples_per_second': 107.675, 'eval_steps_per_second': 6.784, 'epoch': 0.52}
{'loss': 1.089, 'grad_norm': 0.1897459626197815, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 1.330187201499939, 'eval_runtime': 9.2921, 'eval_samples_per_second': 107.619, 'eval_steps_per_second': 6.78, 'epoch': 0.56}
{'loss': 1.0917, 'grad_norm': 0.2078542560338974, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 1.3317272663116455, 'eval_runtime': 9.3039, 'eval_samples_per_second': 107.481, 'eval_steps_per_second': 6.771, 'epoch': 0.6}
{'loss': 1.0511, 'grad_norm': 0.19773182272911072, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 1.333871603012085, 'eval_runtime': 9.2838, 'eval_samples_per_second': 107.715, 'eval_steps_per_second': 6.786, 'epoch': 0.65}
{'loss': 1.1023, 'grad_norm': 0.19088420271873474, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 1.3288218975067139, 'eval_runtime': 9.2958, 'eval_samples_per_second': 107.575, 'eval_steps_per_second': 6.777, 'epoch': 0.69}
{'loss': 1.0986, 'grad_norm': 0.20651444792747498, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 1.3303806781768799, 'eval_runtime': 9.2944, 'eval_samples_per_second': 107.592, 'eval_steps_per_second': 6.778, 'epoch': 0.73}
{'loss': 1.0799, 'grad_norm': 0.18539433181285858, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 1.3334026336669922, 'eval_runtime': 9.2755, 'eval_samples_per_second': 107.81, 'eval_steps_per_second': 6.792, 'epoch': 0.77}
{'loss': 1.0122, 'grad_norm': 0.22572895884513855, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 1.3297351598739624, 'eval_runtime': 9.2822, 'eval_samples_per_second': 107.733, 'eval_steps_per_second': 6.787, 'epoch': 0.81}
{'loss': 1.0584, 'grad_norm': 0.22530068457126617, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 1.329113245010376, 'eval_runtime': 9.2718, 'eval_samples_per_second': 107.854, 'eval_steps_per_second': 6.795, 'epoch': 0.85}
{'loss': 0.9832, 'grad_norm': 0.21852663159370422, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 1.3332699537277222, 'eval_runtime': 9.2744, 'eval_samples_per_second': 107.823, 'eval_steps_per_second': 6.793, 'epoch': 0.89}
{'loss': 1.0531, 'grad_norm': 0.2225964516401291, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 1.332345962524414, 'eval_runtime': 9.2424, 'eval_samples_per_second': 108.197, 'eval_steps_per_second': 6.816, 'epoch': 0.93}
{'loss': 1.0346, 'grad_norm': 0.21532492339611053, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 1.3306982517242432, 'eval_runtime': 9.256, 'eval_samples_per_second': 108.038, 'eval_steps_per_second': 6.806, 'epoch': 0.97}
{'train_runtime': 363.6511, 'train_samples_per_second': 27.265, 'train_steps_per_second': 1.705, 'train_loss': 1.2486449733857186, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0768184661865234, 1.665874719619751, 1.562005877494812, 1.4953346252441406, 1.4193940162658691, 1.3752309083938599, 1.3506304025650024, 1.346181869506836, 1.3472965955734253, 1.3418307304382324, 1.3410691022872925, 1.3399806022644043, 1.3356943130493164, 1.330187201499939, 1.3317272663116455, 1.333871603012085, 1.3288218975067139, 1.3303806781768799, 1.3334026336669922, 1.3297351598739624, 1.329113245010376, 1.3332699537277222, 1.332345962524414, 1.3306982517242432], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.0768184661865234, 1.665874719619751, 1.562005877494812, 1.4953346252441406, 1.4193940162658691, 1.3752309083938599, 1.3506304025650024, 1.346181869506836, 1.3472965955734253, 1.3418307304382324, 1.3410691022872925, 1.3399806022644043, 1.3356943130493164, 1.330187201499939, 1.3317272663116455, 1.333871603012085, 1.3288218975067139, 1.3303806781768799, 1.3334026336669922, 1.3297351598739624, 1.329113245010376, 1.3332699537277222, 1.332345962524414, 1.3306982517242432]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3306982517242432
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.3379 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1795889139175415, 0.5639668703079224, 0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.6092051267623901, 0.8282220363616943, 0.8343875408172607, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.22647850215435028, 0.034817516803741455, 0.2369377166032791, 0.8301550149917603]  ‚Üí  acq = -0.8573581290032544
X = [0.3627225160598755, 0.4275026321411133, 0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.20957553386688232, 0.29544466733932495, 0.45958811044692993, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7996509075164795, 0.16252559423446655, 0.23583632707595825, 0.46233898401260376]  ‚Üí  acq = -0.8573581315310033
X = [0.3497363328933716, 0.9076562523841858, 0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.451727032661438, 0.6221595406532288, 0.021270394325256348, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.990592360496521, 0.6705264449119568, 0.5292245149612427, 0.4145408272743225]  ‚Üí  acq = -0.8576073074789272
X = [0.8545095920562744, 0.7290428876876831, 0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.1896294802427292, 0.13708847761154175, 0.2295888066291809, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.42133286595344543, 0.9889391660690308, 0.27955883741378784, 0.647262454032898]  ‚Üí  acq = -0.8573581292601684
X = [0.25802141427993774, 0.15090978145599365, 0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.16085723042488098, 0.13615381717681885, 0.3176853060722351, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7902160882949829, 0.012897372245788574, 0.5332701206207275, 0.154333233833313]  ‚Üí  acq = -0.8573581290032544
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0408, dtype=torch.float64), 0, 0, tensor(0.3145, dtype=torch.float64), tensor(0.0601, dtype=torch.float64), tensor(0.3444, dtype=torch.float64), tensor(0.2344, dtype=torch.float64), 14, 1, 0, 1, 1, 1, 103, 1.7150155446158543e-21, 23.96712468222696, 1]
normalized proposed parameters for next round by BO: [tensor(9.6843e-19, dtype=torch.float64), tensor(5.8393e-18, dtype=torch.float64), tensor(0.0408, dtype=torch.float64), tensor(0.0059, dtype=torch.float64), tensor(3.8275e-20, dtype=torch.float64), tensor(0.3145, dtype=torch.float64), tensor(0.0601, dtype=torch.float64), tensor(0.3444, dtype=torch.float64), tensor(0.2344, dtype=torch.float64), tensor(0.4530, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8050, dtype=torch.float64), tensor(1.7150e-20, dtype=torch.float64), tensor(0.4993, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.041
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.314
  wikitext: 0.06
  mmlu: 0.344
  arc_challenge: 0.234

LoRA Parameters:
  lora_r: (103,)
  lora_dropout: (1.7150155446158543e-21,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (23.96712468222696,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  103
lora dropout:  1.7150155446158543e-21
lora alpha:  23.96712468222696
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 91,549,696 || all params: 8,121,810,944 || trainable%: 1.1272
length of training data:  9938
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2484, 'grad_norm': 0.8806277513504028, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8389911651611328, 'eval_runtime': 9.986, 'eval_samples_per_second': 100.14, 'eval_steps_per_second': 6.309, 'epoch': 0.04}
{'loss': 1.5901, 'grad_norm': 0.5352651476860046, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4945058822631836, 'eval_runtime': 9.9684, 'eval_samples_per_second': 100.317, 'eval_steps_per_second': 6.32, 'epoch': 0.08}
{'loss': 1.3439, 'grad_norm': 0.34034526348114014, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.4066252708435059, 'eval_runtime': 9.965, 'eval_samples_per_second': 100.351, 'eval_steps_per_second': 6.322, 'epoch': 0.12}
{'loss': 1.2387, 'grad_norm': 0.34351226687431335, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.3583699464797974, 'eval_runtime': 9.9968, 'eval_samples_per_second': 100.032, 'eval_steps_per_second': 6.302, 'epoch': 0.16}
{'loss': 1.2456, 'grad_norm': 0.24436365067958832, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.3364651203155518, 'eval_runtime': 10.0295, 'eval_samples_per_second': 99.706, 'eval_steps_per_second': 6.281, 'epoch': 0.2}
{'loss': 1.1881, 'grad_norm': 0.24243628978729248, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.3064675331115723, 'eval_runtime': 10.0161, 'eval_samples_per_second': 99.839, 'eval_steps_per_second': 6.29, 'epoch': 0.24}
{'loss': 1.2204, 'grad_norm': 0.2449663132429123, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 1.2971032857894897, 'eval_runtime': 10.029, 'eval_samples_per_second': 99.711, 'eval_steps_per_second': 6.282, 'epoch': 0.28}
{'loss': 1.1058, 'grad_norm': 0.24283631145954132, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 1.2890475988388062, 'eval_runtime': 9.9648, 'eval_samples_per_second': 100.353, 'eval_steps_per_second': 6.322, 'epoch': 0.32}
{'loss': 1.1742, 'grad_norm': 0.21119561791419983, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 1.2881535291671753, 'eval_runtime': 9.9641, 'eval_samples_per_second': 100.361, 'eval_steps_per_second': 6.323, 'epoch': 0.36}
{'loss': 1.1006, 'grad_norm': 0.20062687993049622, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 1.2852787971496582, 'eval_runtime': 9.9681, 'eval_samples_per_second': 100.32, 'eval_steps_per_second': 6.32, 'epoch': 0.4}
{'loss': 1.1712, 'grad_norm': 0.2421409785747528, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 1.2922977209091187, 'eval_runtime': 10.0011, 'eval_samples_per_second': 99.989, 'eval_steps_per_second': 6.299, 'epoch': 0.44}
{'loss': 1.098, 'grad_norm': 0.21185626089572906, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 1.2803400754928589, 'eval_runtime': 9.9901, 'eval_samples_per_second': 100.099, 'eval_steps_per_second': 6.306, 'epoch': 0.48}
{'loss': 1.1337, 'grad_norm': 0.2349410504102707, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 1.2735642194747925, 'eval_runtime': 9.9891, 'eval_samples_per_second': 100.11, 'eval_steps_per_second': 6.307, 'epoch': 0.52}
{'loss': 1.1694, 'grad_norm': 0.2179066687822342, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 1.267389178276062, 'eval_runtime': 9.9821, 'eval_samples_per_second': 100.179, 'eval_steps_per_second': 6.311, 'epoch': 0.56}
{'loss': 1.0523, 'grad_norm': 0.3141040503978729, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 1.2710224390029907, 'eval_runtime': 9.9953, 'eval_samples_per_second': 100.047, 'eval_steps_per_second': 6.303, 'epoch': 0.6}
{'loss': 1.0981, 'grad_norm': 0.24659450352191925, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 1.2667802572250366, 'eval_runtime': 9.9912, 'eval_samples_per_second': 100.088, 'eval_steps_per_second': 6.306, 'epoch': 0.64}
{'loss': 1.0995, 'grad_norm': 0.26151350140571594, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 1.264420747756958, 'eval_runtime': 9.9898, 'eval_samples_per_second': 100.102, 'eval_steps_per_second': 6.306, 'epoch': 0.68}
{'loss': 1.0682, 'grad_norm': 0.27397891879081726, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 1.2641911506652832, 'eval_runtime': 9.9879, 'eval_samples_per_second': 100.121, 'eval_steps_per_second': 6.308, 'epoch': 0.72}
{'loss': 1.0444, 'grad_norm': 0.2799450755119324, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 1.2600854635238647, 'eval_runtime': 9.9877, 'eval_samples_per_second': 100.123, 'eval_steps_per_second': 6.308, 'epoch': 0.76}
{'loss': 1.0897, 'grad_norm': 0.2189602255821228, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 1.2596712112426758, 'eval_runtime': 9.9906, 'eval_samples_per_second': 100.094, 'eval_steps_per_second': 6.306, 'epoch': 0.8}
{'loss': 1.0443, 'grad_norm': 0.39294594526290894, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 1.2564244270324707, 'eval_runtime': 10.0202, 'eval_samples_per_second': 99.798, 'eval_steps_per_second': 6.287, 'epoch': 0.84}
{'loss': 1.0949, 'grad_norm': 0.29582539200782776, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 1.2534172534942627, 'eval_runtime': 10.0037, 'eval_samples_per_second': 99.963, 'eval_steps_per_second': 6.298, 'epoch': 0.88}
{'loss': 1.0467, 'grad_norm': 0.24603299796581268, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 1.252992033958435, 'eval_runtime': 10.072, 'eval_samples_per_second': 99.285, 'eval_steps_per_second': 6.255, 'epoch': 0.92}
{'loss': 1.031, 'grad_norm': 0.3191891610622406, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 1.252777338027954, 'eval_runtime': 10.0473, 'eval_samples_per_second': 99.529, 'eval_steps_per_second': 6.27, 'epoch': 0.96}
{'train_runtime': 458.2554, 'train_samples_per_second': 21.687, 'train_steps_per_second': 1.357, 'train_loss': 1.2294742854078482, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8389911651611328, 1.4945058822631836, 1.4066252708435059, 1.3583699464797974, 1.3364651203155518, 1.3064675331115723, 1.2971032857894897, 1.2890475988388062, 1.2881535291671753, 1.2852787971496582, 1.2922977209091187, 1.2803400754928589, 1.2735642194747925, 1.267389178276062, 1.2710224390029907, 1.2667802572250366, 1.264420747756958, 1.2641911506652832, 1.2600854635238647, 1.2596712112426758, 1.2564244270324707, 1.2534172534942627, 1.252992033958435, 1.252777338027954], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.8389911651611328, 1.4945058822631836, 1.4066252708435059, 1.3583699464797974, 1.3364651203155518, 1.3064675331115723, 1.2971032857894897, 1.2890475988388062, 1.2881535291671753, 1.2852787971496582, 1.2922977209091187, 1.2803400754928589, 1.2735642194747925, 1.267389178276062, 1.2710224390029907, 1.2667802572250366, 1.264420747756958, 1.2641911506652832, 1.2600854635238647, 1.2596712112426758, 1.2564244270324707, 1.2534172534942627, 1.252992033958435, 1.252777338027954]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.252777338027954
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.2894 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9384865760803223, 0.07229626178741455, 0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6294164657592773, 0.5043690800666809, 0.12173128128051758, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.45333993434906006, 0.9395368695259094, 0.38172125816345215, 0.5809779763221741]  ‚Üí  acq = -0.7851826529419136
X = [0.25227582454681396, 0.03358107805252075, 0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.7713519334793091, 0.4265725612640381, 0.10601025819778442, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.3584800362586975, 0.16592669486999512, 0.6665055751800537, 0.5913098454475403]  ‚Üí  acq = -0.784500100050649
X = [0.019490361213684082, 0.33564668893814087, 0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.24613288044929504, 0.783804178237915, 0.05754297971725464, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6837789416313171, 0.5803513526916504, 0.10683952271938324, 0.5482228994369507]  ‚Üí  acq = -0.7851825134306192
X = [0.08164948225021362, 0.510372519493103, 0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3423933982849121, 0.566781759262085, 0.5619614124298096, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9544339776039124, 0.7185770869255066, 0.4849817454814911, 0.03939622640609741]  ‚Üí  acq = -0.7851824988822951
X = [0.9194858074188232, 0.7918861508369446, 0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5424157381057739, 0.4093313217163086, 0.6975671648979187, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.975213885307312, 0.7470415830612183, 0.2785099744796753, 0.9093899726867676]  ‚Üí  acq = -0.7852538653772484
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0768, dtype=torch.float64), tensor(0.1291, dtype=torch.float64), tensor(0.1639, dtype=torch.float64), tensor(0.4772, dtype=torch.float64), tensor(0.0140, dtype=torch.float64), 0, tensor(0.1130, dtype=torch.float64), tensor(0.0261, dtype=torch.float64), 24, 0, 1, 0, 1, 1, 89, 0.0841323339478148, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0768, dtype=torch.float64), tensor(0.1291, dtype=torch.float64), tensor(0.1639, dtype=torch.float64), tensor(0.4772, dtype=torch.float64), tensor(0.0140, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1130, dtype=torch.float64), tensor(0.0261, dtype=torch.float64), tensor(0.7614, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6917, dtype=torch.float64), tensor(0.8413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.077
  rowan_hellaswag: 0.129
  sciq: 0.164
  triviaqa: 0.477
  truthfulqa_gen: 0.014
  wikitext: 0
  mmlu: 0.113
  arc_challenge: 0.026

LoRA Parameters:
  lora_r: (89,)
  lora_dropout: (0.0841323339478148,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  89
lora dropout:  0.0841323339478148
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 89,677,824 || all params: 8,119,939,072 || trainable%: 1.1044
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0247, 'grad_norm': 0.7047815918922424, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.723657488822937, 'eval_runtime': 10.1863, 'eval_samples_per_second': 98.171, 'eval_steps_per_second': 6.185, 'epoch': 0.04}
{'loss': 1.5497, 'grad_norm': 0.3854135274887085, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4432909488677979, 'eval_runtime': 10.2147, 'eval_samples_per_second': 97.898, 'eval_steps_per_second': 6.168, 'epoch': 0.08}
{'loss': 1.2705, 'grad_norm': 0.35236671566963196, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3659228086471558, 'eval_runtime': 10.2202, 'eval_samples_per_second': 97.846, 'eval_steps_per_second': 6.164, 'epoch': 0.12}
{'loss': 1.2207, 'grad_norm': 0.30306166410446167, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3238470554351807, 'eval_runtime': 10.2359, 'eval_samples_per_second': 97.695, 'eval_steps_per_second': 6.155, 'epoch': 0.16}
{'loss': 1.25, 'grad_norm': 0.27582916617393494, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3129171133041382, 'eval_runtime': 10.2365, 'eval_samples_per_second': 97.689, 'eval_steps_per_second': 6.154, 'epoch': 0.2}
{'loss': 1.3046, 'grad_norm': 0.2886277437210083, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3117071390151978, 'eval_runtime': 10.2495, 'eval_samples_per_second': 97.566, 'eval_steps_per_second': 6.147, 'epoch': 0.24}
{'loss': 1.2337, 'grad_norm': 0.25328329205513, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.307470679283142, 'eval_runtime': 10.2441, 'eval_samples_per_second': 97.617, 'eval_steps_per_second': 6.15, 'epoch': 0.28}
{'loss': 1.2123, 'grad_norm': 0.2844138443470001, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.295986533164978, 'eval_runtime': 10.247, 'eval_samples_per_second': 97.589, 'eval_steps_per_second': 6.148, 'epoch': 0.32}
{'loss': 1.231, 'grad_norm': 0.27565741539001465, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2919386625289917, 'eval_runtime': 10.2634, 'eval_samples_per_second': 97.434, 'eval_steps_per_second': 6.138, 'epoch': 0.36}
{'loss': 1.2027, 'grad_norm': 0.27341657876968384, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2918223142623901, 'eval_runtime': 10.2697, 'eval_samples_per_second': 97.374, 'eval_steps_per_second': 6.135, 'epoch': 0.4}
{'loss': 1.2226, 'grad_norm': 0.33436572551727295, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2913029193878174, 'eval_runtime': 10.2705, 'eval_samples_per_second': 97.366, 'eval_steps_per_second': 6.134, 'epoch': 0.44}
{'loss': 1.2117, 'grad_norm': 0.30976414680480957, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.287877082824707, 'eval_runtime': 10.2452, 'eval_samples_per_second': 97.607, 'eval_steps_per_second': 6.149, 'epoch': 0.48}
{'loss': 1.2851, 'grad_norm': 0.26814204454421997, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.289650797843933, 'eval_runtime': 10.2398, 'eval_samples_per_second': 97.658, 'eval_steps_per_second': 6.152, 'epoch': 0.52}
{'loss': 1.1634, 'grad_norm': 0.30141714215278625, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2825226783752441, 'eval_runtime': 10.2537, 'eval_samples_per_second': 97.525, 'eval_steps_per_second': 6.144, 'epoch': 0.56}
{'loss': 1.2057, 'grad_norm': 0.28433844447135925, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2832705974578857, 'eval_runtime': 10.2396, 'eval_samples_per_second': 97.66, 'eval_steps_per_second': 6.153, 'epoch': 0.6}
{'loss': 1.2285, 'grad_norm': 0.274313747882843, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2767152786254883, 'eval_runtime': 10.2248, 'eval_samples_per_second': 97.802, 'eval_steps_per_second': 6.162, 'epoch': 0.64}
{'loss': 1.1866, 'grad_norm': 0.2506387233734131, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2780652046203613, 'eval_runtime': 10.2304, 'eval_samples_per_second': 97.748, 'eval_steps_per_second': 6.158, 'epoch': 0.68}
{'loss': 1.2138, 'grad_norm': 0.2652254104614258, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2780009508132935, 'eval_runtime': 10.2257, 'eval_samples_per_second': 97.793, 'eval_steps_per_second': 6.161, 'epoch': 0.72}
{'loss': 1.1947, 'grad_norm': 0.37845107913017273, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2710734605789185, 'eval_runtime': 10.233, 'eval_samples_per_second': 97.723, 'eval_steps_per_second': 6.157, 'epoch': 0.76}
{'loss': 1.1405, 'grad_norm': 0.24602995812892914, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.271777868270874, 'eval_runtime': 10.2363, 'eval_samples_per_second': 97.691, 'eval_steps_per_second': 6.155, 'epoch': 0.8}
{'loss': 1.2153, 'grad_norm': 0.2990504801273346, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.271215558052063, 'eval_runtime': 10.2306, 'eval_samples_per_second': 97.746, 'eval_steps_per_second': 6.158, 'epoch': 0.84}
{'loss': 1.1381, 'grad_norm': 0.2610016465187073, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2706350088119507, 'eval_runtime': 10.2427, 'eval_samples_per_second': 97.631, 'eval_steps_per_second': 6.151, 'epoch': 0.88}
{'loss': 1.1998, 'grad_norm': 0.2899833917617798, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2699135541915894, 'eval_runtime': 10.2387, 'eval_samples_per_second': 97.668, 'eval_steps_per_second': 6.153, 'epoch': 0.92}
{'loss': 1.2161, 'grad_norm': 0.25197938084602356, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2690834999084473, 'eval_runtime': 10.2356, 'eval_samples_per_second': 97.698, 'eval_steps_per_second': 6.155, 'epoch': 0.96}
{'loss': 1.2333, 'grad_norm': 0.3185853660106659, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2682859897613525, 'eval_runtime': 10.2411, 'eval_samples_per_second': 97.646, 'eval_steps_per_second': 6.152, 'epoch': 1.0}
{'train_runtime': 464.5494, 'train_samples_per_second': 21.518, 'train_steps_per_second': 1.345, 'train_loss': 1.3022181030273436, 'epoch': 1.0}
train_results:  {'eval_loss': [1.723657488822937, 1.4432909488677979, 1.3659228086471558, 1.3238470554351807, 1.3129171133041382, 1.3117071390151978, 1.307470679283142, 1.295986533164978, 1.2919386625289917, 1.2918223142623901, 1.2913029193878174, 1.287877082824707, 1.289650797843933, 1.2825226783752441, 1.2832705974578857, 1.2767152786254883, 1.2780652046203613, 1.2780009508132935, 1.2710734605789185, 1.271777868270874, 1.271215558052063, 1.2706350088119507, 1.2699135541915894, 1.2690834999084473, 1.2682859897613525], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.723657488822937, 1.4432909488677979, 1.3659228086471558, 1.3238470554351807, 1.3129171133041382, 1.3117071390151978, 1.307470679283142, 1.295986533164978, 1.2919386625289917, 1.2918223142623901, 1.2913029193878174, 1.287877082824707, 1.289650797843933, 1.2825226783752441, 1.2832705974578857, 1.2767152786254883, 1.2780652046203613, 1.2780009508132935, 1.2710734605789185, 1.271777868270874, 1.271215558052063, 1.2706350088119507, 1.2699135541915894, 1.2690834999084473, 1.2682859897613525]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2682859897613525
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.0546 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.8671446945074541
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.8671697016293494
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.8671457540214129
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.8675529104412281
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.8671446947469839
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0283, dtype=torch.float64), tensor(0.0796, dtype=torch.float64), 0, tensor(0.1162, dtype=torch.float64), 0, tensor(0.3041, dtype=torch.float64), tensor(0.4717, dtype=torch.float64), 14, 1, 1, 1, 0, 1, 128, 0.0014920802910958959, 12.388880899721157, 0]
normalized proposed parameters for next round by BO: [tensor(3.2409e-20, dtype=torch.float64), tensor(3.4124e-19, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.0796, dtype=torch.float64), tensor(2.0348e-18, dtype=torch.float64), tensor(0.1162, dtype=torch.float64), tensor(7.5277e-18, dtype=torch.float64), tensor(0.3041, dtype=torch.float64), tensor(0.4717, dtype=torch.float64), tensor(0.4391, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0149, dtype=torch.float64), tensor(0.2581, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.028
  sciq: 0.08
  triviaqa: 0
  truthfulqa_gen: 0.116
  wikitext: 0
  mmlu: 0.304
  arc_challenge: 0.472

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0014920802910958959,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (12.388880899721157,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.0014920802910958959
lora alpha:  12.388880899721157
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 89,915,392 || all params: 8,120,176,640 || trainable%: 1.1073
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4408, 'grad_norm': 0.3046451210975647, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0783655643463135, 'eval_runtime': 9.2473, 'eval_samples_per_second': 108.139, 'eval_steps_per_second': 6.813, 'epoch': 0.04}
{'loss': 1.6876, 'grad_norm': 0.11424426734447479, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.663474202156067, 'eval_runtime': 9.2913, 'eval_samples_per_second': 107.627, 'eval_steps_per_second': 6.781, 'epoch': 0.08}
{'loss': 1.3817, 'grad_norm': 0.10925047099590302, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5701476335525513, 'eval_runtime': 9.3116, 'eval_samples_per_second': 107.393, 'eval_steps_per_second': 6.766, 'epoch': 0.12}
{'loss': 1.3094, 'grad_norm': 0.12861937284469604, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.51019287109375, 'eval_runtime': 9.3428, 'eval_samples_per_second': 107.034, 'eval_steps_per_second': 6.743, 'epoch': 0.16}
{'loss': 1.2589, 'grad_norm': 0.10783399641513824, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.453998327255249, 'eval_runtime': 9.388, 'eval_samples_per_second': 106.519, 'eval_steps_per_second': 6.711, 'epoch': 0.2}
{'loss': 1.1801, 'grad_norm': 0.11850222945213318, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4127293825149536, 'eval_runtime': 9.4613, 'eval_samples_per_second': 105.694, 'eval_steps_per_second': 6.659, 'epoch': 0.24}
{'loss': 1.1032, 'grad_norm': 0.14149202406406403, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3903785943984985, 'eval_runtime': 9.4667, 'eval_samples_per_second': 105.633, 'eval_steps_per_second': 6.655, 'epoch': 0.28}
{'loss': 1.0656, 'grad_norm': 0.1422710418701172, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.357590675354004, 'eval_runtime': 9.4654, 'eval_samples_per_second': 105.648, 'eval_steps_per_second': 6.656, 'epoch': 0.32}
{'loss': 1.0703, 'grad_norm': 0.12951238453388214, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.353975772857666, 'eval_runtime': 9.4719, 'eval_samples_per_second': 105.575, 'eval_steps_per_second': 6.651, 'epoch': 0.36}
{'loss': 1.0743, 'grad_norm': 0.12654270231723785, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3480205535888672, 'eval_runtime': 9.4741, 'eval_samples_per_second': 105.551, 'eval_steps_per_second': 6.65, 'epoch': 0.4}
{'loss': 1.0587, 'grad_norm': 0.15300913155078888, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3440535068511963, 'eval_runtime': 9.4713, 'eval_samples_per_second': 105.582, 'eval_steps_per_second': 6.652, 'epoch': 0.44}
{'loss': 1.0317, 'grad_norm': 0.16012604534626007, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3402069807052612, 'eval_runtime': 9.4703, 'eval_samples_per_second': 105.594, 'eval_steps_per_second': 6.652, 'epoch': 0.48}
{'loss': 0.9822, 'grad_norm': 0.1838984191417694, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3406208753585815, 'eval_runtime': 9.4782, 'eval_samples_per_second': 105.506, 'eval_steps_per_second': 6.647, 'epoch': 0.52}
{'loss': 1.0059, 'grad_norm': 0.13691939413547516, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3418704271316528, 'eval_runtime': 9.4774, 'eval_samples_per_second': 105.515, 'eval_steps_per_second': 6.647, 'epoch': 0.56}
{'loss': 1.0338, 'grad_norm': 0.17773965001106262, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3349043130874634, 'eval_runtime': 9.4973, 'eval_samples_per_second': 105.293, 'eval_steps_per_second': 6.633, 'epoch': 0.6}
{'loss': 0.9812, 'grad_norm': 0.16851423680782318, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.332631230354309, 'eval_runtime': 9.4561, 'eval_samples_per_second': 105.751, 'eval_steps_per_second': 6.662, 'epoch': 0.64}
{'loss': 1.0029, 'grad_norm': 0.16598880290985107, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3353488445281982, 'eval_runtime': 9.4671, 'eval_samples_per_second': 105.629, 'eval_steps_per_second': 6.655, 'epoch': 0.68}
{'loss': 1.0059, 'grad_norm': 0.207063227891922, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.331329345703125, 'eval_runtime': 9.4881, 'eval_samples_per_second': 105.395, 'eval_steps_per_second': 6.64, 'epoch': 0.72}
{'loss': 0.9769, 'grad_norm': 0.1904287040233612, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3281691074371338, 'eval_runtime': 9.4742, 'eval_samples_per_second': 105.55, 'eval_steps_per_second': 6.65, 'epoch': 0.76}
{'loss': 0.9958, 'grad_norm': 0.19647493958473206, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3293640613555908, 'eval_runtime': 9.4677, 'eval_samples_per_second': 105.622, 'eval_steps_per_second': 6.654, 'epoch': 0.8}
{'loss': 0.9327, 'grad_norm': 0.1853313446044922, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3306115865707397, 'eval_runtime': 9.4712, 'eval_samples_per_second': 105.583, 'eval_steps_per_second': 6.652, 'epoch': 0.84}
{'loss': 0.9501, 'grad_norm': 0.21752065420150757, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3316162824630737, 'eval_runtime': 9.4705, 'eval_samples_per_second': 105.592, 'eval_steps_per_second': 6.652, 'epoch': 0.88}
{'loss': 0.9957, 'grad_norm': 0.2006717175245285, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3302456140518188, 'eval_runtime': 9.4787, 'eval_samples_per_second': 105.499, 'eval_steps_per_second': 6.646, 'epoch': 0.92}
{'loss': 0.9179, 'grad_norm': 0.20766916871070862, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.330288290977478, 'eval_runtime': 9.4899, 'eval_samples_per_second': 105.375, 'eval_steps_per_second': 6.639, 'epoch': 0.96}
{'loss': 0.9396, 'grad_norm': 0.2099868804216385, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3295392990112305, 'eval_runtime': 9.4844, 'eval_samples_per_second': 105.437, 'eval_steps_per_second': 6.643, 'epoch': 1.0}
{'train_runtime': 380.6536, 'train_samples_per_second': 26.265, 'train_steps_per_second': 1.642, 'train_loss': 1.175317510986328, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0783655643463135, 1.663474202156067, 1.5701476335525513, 1.51019287109375, 1.453998327255249, 1.4127293825149536, 1.3903785943984985, 1.357590675354004, 1.353975772857666, 1.3480205535888672, 1.3440535068511963, 1.3402069807052612, 1.3406208753585815, 1.3418704271316528, 1.3349043130874634, 1.332631230354309, 1.3353488445281982, 1.331329345703125, 1.3281691074371338, 1.3293640613555908, 1.3306115865707397, 1.3316162824630737, 1.3302456140518188, 1.330288290977478, 1.3295392990112305], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0783655643463135, 1.663474202156067, 1.5701476335525513, 1.51019287109375, 1.453998327255249, 1.4127293825149536, 1.3903785943984985, 1.357590675354004, 1.353975772857666, 1.3480205535888672, 1.3440535068511963, 1.3402069807052612, 1.3406208753585815, 1.3418704271316528, 1.3349043130874634, 1.332631230354309, 1.3353488445281982, 1.331329345703125, 1.3281691074371338, 1.3293640613555908, 1.3306115865707397, 1.3316162824630737, 1.3302456140518188, 1.330288290977478, 1.3295392990112305]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3295392990112305
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2596 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -0.8719711398517176
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.8720478139668402
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -0.87197113950611
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -0.8719711396729948
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -0.8733834726698628
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0115, dtype=torch.float64), tensor(0.0692, dtype=torch.float64), 0, 0, tensor(0.0659, dtype=torch.float64), tensor(0.0977, dtype=torch.float64), tensor(0.7557, dtype=torch.float64), 12, 0, 1, 1, 1, 1, 128, 3.7177577711031996e-21, 44.44664639798572, 0]
normalized proposed parameters for next round by BO: [tensor(1.1173e-18, dtype=torch.float64), tensor(9.0097e-19, dtype=torch.float64), tensor(0.0115, dtype=torch.float64), tensor(0.0692, dtype=torch.float64), tensor(2.3164e-18, dtype=torch.float64), tensor(4.5542e-19, dtype=torch.float64), tensor(0.0659, dtype=torch.float64), tensor(0.0977, dtype=torch.float64), tensor(0.7557, dtype=torch.float64), tensor(0.3686, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.7178e-20, dtype=torch.float64), tensor(0.9260, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.012
  sciq: 0.069
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.066
  mmlu: 0.098
  arc_challenge: 0.756

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.7177577711031996e-21,)
  num_layers_to_apply: (12,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (44.44664639798572,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  12
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  3.7177577711031996e-21
lora alpha:  44.44664639798572
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 92,798,976 || all params: 8,123,060,224 || trainable%: 1.1424
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8643, 'grad_norm': 0.6534630656242371, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8646953105926514, 'eval_runtime': 9.2844, 'eval_samples_per_second': 107.708, 'eval_steps_per_second': 6.786, 'epoch': 0.04}
{'loss': 1.1597, 'grad_norm': 0.29706695675849915, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5550034046173096, 'eval_runtime': 9.3259, 'eval_samples_per_second': 107.228, 'eval_steps_per_second': 6.755, 'epoch': 0.08}
{'loss': 1.1039, 'grad_norm': 0.1977972537279129, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4797947406768799, 'eval_runtime': 9.3777, 'eval_samples_per_second': 106.636, 'eval_steps_per_second': 6.718, 'epoch': 0.12}
{'loss': 0.9819, 'grad_norm': 0.22751615941524506, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4550373554229736, 'eval_runtime': 9.3993, 'eval_samples_per_second': 106.391, 'eval_steps_per_second': 6.703, 'epoch': 0.16}
{'loss': 0.9728, 'grad_norm': 0.23393425345420837, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4251199960708618, 'eval_runtime': 9.4528, 'eval_samples_per_second': 105.788, 'eval_steps_per_second': 6.665, 'epoch': 0.2}
{'loss': 0.9096, 'grad_norm': 0.2294284701347351, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4330828189849854, 'eval_runtime': 9.4368, 'eval_samples_per_second': 105.968, 'eval_steps_per_second': 6.676, 'epoch': 0.24}
{'loss': 0.8698, 'grad_norm': 0.2947900593280792, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4275773763656616, 'eval_runtime': 9.4319, 'eval_samples_per_second': 106.023, 'eval_steps_per_second': 6.679, 'epoch': 0.28}
{'loss': 0.8824, 'grad_norm': 0.29748308658599854, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4016565084457397, 'eval_runtime': 9.4455, 'eval_samples_per_second': 105.87, 'eval_steps_per_second': 6.67, 'epoch': 0.32}
{'loss': 0.8602, 'grad_norm': 0.3347042500972748, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3954250812530518, 'eval_runtime': 9.4265, 'eval_samples_per_second': 106.084, 'eval_steps_per_second': 6.683, 'epoch': 0.36}
{'loss': 0.8086, 'grad_norm': 0.31899788975715637, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.396619439125061, 'eval_runtime': 9.4244, 'eval_samples_per_second': 106.108, 'eval_steps_per_second': 6.685, 'epoch': 0.4}
{'loss': 0.7605, 'grad_norm': 0.28343889117240906, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.393218994140625, 'eval_runtime': 9.4013, 'eval_samples_per_second': 106.368, 'eval_steps_per_second': 6.701, 'epoch': 0.44}
{'loss': 0.7391, 'grad_norm': 0.33616578578948975, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.393904209136963, 'eval_runtime': 9.4095, 'eval_samples_per_second': 106.276, 'eval_steps_per_second': 6.695, 'epoch': 0.48}
{'loss': 0.6741, 'grad_norm': 0.3048747181892395, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3874411582946777, 'eval_runtime': 9.4136, 'eval_samples_per_second': 106.229, 'eval_steps_per_second': 6.692, 'epoch': 0.52}
{'loss': 0.6491, 'grad_norm': 0.32142767310142517, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3857309818267822, 'eval_runtime': 9.3936, 'eval_samples_per_second': 106.456, 'eval_steps_per_second': 6.707, 'epoch': 0.56}
{'loss': 0.584, 'grad_norm': 0.32406744360923767, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3907839059829712, 'eval_runtime': 9.3937, 'eval_samples_per_second': 106.454, 'eval_steps_per_second': 6.707, 'epoch': 0.6}
{'loss': 0.586, 'grad_norm': 0.3550384044647217, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3877507448196411, 'eval_runtime': 9.374, 'eval_samples_per_second': 106.678, 'eval_steps_per_second': 6.721, 'epoch': 0.64}
{'loss': 0.5708, 'grad_norm': 0.31221461296081543, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3787981271743774, 'eval_runtime': 9.3642, 'eval_samples_per_second': 106.79, 'eval_steps_per_second': 6.728, 'epoch': 0.68}
{'loss': 0.5021, 'grad_norm': 0.2923334538936615, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3821243047714233, 'eval_runtime': 9.3621, 'eval_samples_per_second': 106.814, 'eval_steps_per_second': 6.729, 'epoch': 0.72}
{'loss': 0.5107, 'grad_norm': 0.2973312735557556, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3774528503417969, 'eval_runtime': 9.3701, 'eval_samples_per_second': 106.722, 'eval_steps_per_second': 6.723, 'epoch': 0.76}
{'loss': 0.5235, 'grad_norm': 0.354727178812027, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3763099908828735, 'eval_runtime': 9.3933, 'eval_samples_per_second': 106.458, 'eval_steps_per_second': 6.707, 'epoch': 0.8}
{'loss': 0.4522, 'grad_norm': 0.25466933846473694, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3774311542510986, 'eval_runtime': 9.3854, 'eval_samples_per_second': 106.549, 'eval_steps_per_second': 6.713, 'epoch': 0.84}
{'loss': 0.4464, 'grad_norm': 0.2657810151576996, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3752589225769043, 'eval_runtime': 9.3845, 'eval_samples_per_second': 106.559, 'eval_steps_per_second': 6.713, 'epoch': 0.88}
{'loss': 0.4581, 'grad_norm': 0.2442675083875656, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3735800981521606, 'eval_runtime': 9.3714, 'eval_samples_per_second': 106.708, 'eval_steps_per_second': 6.723, 'epoch': 0.92}
{'loss': 0.4275, 'grad_norm': 0.43232911825180054, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3726890087127686, 'eval_runtime': 9.3649, 'eval_samples_per_second': 106.782, 'eval_steps_per_second': 6.727, 'epoch': 0.96}
{'loss': 0.4635, 'grad_norm': 0.31533554196357727, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3724277019500732, 'eval_runtime': 9.4098, 'eval_samples_per_second': 106.273, 'eval_steps_per_second': 6.695, 'epoch': 1.0}
{'train_runtime': 364.0998, 'train_samples_per_second': 27.46, 'train_steps_per_second': 1.717, 'train_loss': 0.790442707824707, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8646953105926514, 1.5550034046173096, 1.4797947406768799, 1.4550373554229736, 1.4251199960708618, 1.4330828189849854, 1.4275773763656616, 1.4016565084457397, 1.3954250812530518, 1.396619439125061, 1.393218994140625, 1.393904209136963, 1.3874411582946777, 1.3857309818267822, 1.3907839059829712, 1.3877507448196411, 1.3787981271743774, 1.3821243047714233, 1.3774528503417969, 1.3763099908828735, 1.3774311542510986, 1.3752589225769043, 1.3735800981521606, 1.3726890087127686, 1.3724277019500732], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8646953105926514, 1.5550034046173096, 1.4797947406768799, 1.4550373554229736, 1.4251199960708618, 1.4330828189849854, 1.4275773763656616, 1.4016565084457397, 1.3954250812530518, 1.396619439125061, 1.393218994140625, 1.393904209136963, 1.3874411582946777, 1.3857309818267822, 1.3907839059829712, 1.3877507448196411, 1.3787981271743774, 1.3821243047714233, 1.3774528503417969, 1.3763099908828735, 1.3774311542510986, 1.3752589225769043, 1.3735800981521606, 1.3726890087127686, 1.3724277019500732]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3724277019500732
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6813 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -0.9018014794395455
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -0.8978311596166229
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -0.9018294168961007
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.9021218608273479
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -0.9018033047107749
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2809, dtype=torch.float64), 0, tensor(0.0421, dtype=torch.float64), tensor(0.1141, dtype=torch.float64), 0, tensor(0.0897, dtype=torch.float64), 0, tensor(0.3221, dtype=torch.float64), tensor(0.1511, dtype=torch.float64), 24, 0, 0, 1, 0, 0, 67, 0.07254324450868337, 30.221597804569214, 1]
normalized proposed parameters for next round by BO: [tensor(0.2809, dtype=torch.float64), tensor(4.6787e-17, dtype=torch.float64), tensor(0.0421, dtype=torch.float64), tensor(0.1141, dtype=torch.float64), tensor(4.6780e-17, dtype=torch.float64), tensor(0.0897, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3221, dtype=torch.float64), tensor(0.1511, dtype=torch.float64), tensor(0.7416, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5201, dtype=torch.float64), tensor(0.7254, dtype=torch.float64), tensor(0.6296, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.281
  gsm8k: 0
  rowan_hellaswag: 0.042
  sciq: 0.114
  triviaqa: 0
  truthfulqa_gen: 0.09
  wikitext: 0
  mmlu: 0.322
  arc_challenge: 0.151

LoRA Parameters:
  lora_r: (67,)
  lora_dropout: (0.07254324450868337,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (30.221597804569214,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  67
lora dropout:  0.07254324450868337
lora alpha:  30.221597804569214
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4053, 'grad_norm': 0.4731626808643341, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9539350271224976, 'eval_runtime': 9.0832, 'eval_samples_per_second': 110.094, 'eval_steps_per_second': 6.936, 'epoch': 0.04}
{'loss': 1.6702, 'grad_norm': 0.3432822525501251, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.650481939315796, 'eval_runtime': 9.1285, 'eval_samples_per_second': 109.547, 'eval_steps_per_second': 6.901, 'epoch': 0.08}
{'loss': 1.4038, 'grad_norm': 0.1858775019645691, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4894094467163086, 'eval_runtime': 9.1365, 'eval_samples_per_second': 109.451, 'eval_steps_per_second': 6.895, 'epoch': 0.12}
{'loss': 1.3197, 'grad_norm': 0.2162284404039383, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4399069547653198, 'eval_runtime': 9.1646, 'eval_samples_per_second': 109.116, 'eval_steps_per_second': 6.874, 'epoch': 0.16}
{'loss': 1.2977, 'grad_norm': 0.24678683280944824, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3862661123275757, 'eval_runtime': 9.1711, 'eval_samples_per_second': 109.039, 'eval_steps_per_second': 6.869, 'epoch': 0.2}
{'loss': 1.1974, 'grad_norm': 0.19149573147296906, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3585959672927856, 'eval_runtime': 9.2063, 'eval_samples_per_second': 108.622, 'eval_steps_per_second': 6.843, 'epoch': 0.24}
{'loss': 1.243, 'grad_norm': 0.24257467687129974, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3434879779815674, 'eval_runtime': 9.2088, 'eval_samples_per_second': 108.591, 'eval_steps_per_second': 6.841, 'epoch': 0.28}
{'loss': 1.1824, 'grad_norm': 0.20289768278598785, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3397002220153809, 'eval_runtime': 9.2138, 'eval_samples_per_second': 108.532, 'eval_steps_per_second': 6.838, 'epoch': 0.32}
{'loss': 1.212, 'grad_norm': 0.24398082494735718, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.332466721534729, 'eval_runtime': 9.2151, 'eval_samples_per_second': 108.517, 'eval_steps_per_second': 6.837, 'epoch': 0.36}
{'loss': 1.2003, 'grad_norm': 0.19120822846889496, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.328053593635559, 'eval_runtime': 9.1986, 'eval_samples_per_second': 108.712, 'eval_steps_per_second': 6.849, 'epoch': 0.4}
{'loss': 1.1727, 'grad_norm': 0.2403942048549652, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3212307691574097, 'eval_runtime': 9.1978, 'eval_samples_per_second': 108.722, 'eval_steps_per_second': 6.849, 'epoch': 0.44}
{'loss': 1.1505, 'grad_norm': 0.194637730717659, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3183757066726685, 'eval_runtime': 9.1994, 'eval_samples_per_second': 108.703, 'eval_steps_per_second': 6.848, 'epoch': 0.48}
{'loss': 1.1425, 'grad_norm': 0.19950251281261444, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.316589593887329, 'eval_runtime': 9.3041, 'eval_samples_per_second': 107.479, 'eval_steps_per_second': 6.771, 'epoch': 0.52}
{'loss': 1.1247, 'grad_norm': 0.2051210254430771, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3113844394683838, 'eval_runtime': 9.2173, 'eval_samples_per_second': 108.491, 'eval_steps_per_second': 6.835, 'epoch': 0.56}
{'loss': 1.1579, 'grad_norm': 0.21449460089206696, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3073872327804565, 'eval_runtime': 9.218, 'eval_samples_per_second': 108.483, 'eval_steps_per_second': 6.834, 'epoch': 0.6}
{'loss': 1.0986, 'grad_norm': 0.2147621363401413, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3032431602478027, 'eval_runtime': 9.2996, 'eval_samples_per_second': 107.532, 'eval_steps_per_second': 6.775, 'epoch': 0.64}
{'loss': 1.178, 'grad_norm': 0.2060306817293167, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.297787070274353, 'eval_runtime': 9.3239, 'eval_samples_per_second': 107.251, 'eval_steps_per_second': 6.757, 'epoch': 0.68}
{'loss': 1.1413, 'grad_norm': 0.2148401290178299, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2946898937225342, 'eval_runtime': 9.2992, 'eval_samples_per_second': 107.536, 'eval_steps_per_second': 6.775, 'epoch': 0.72}
{'loss': 1.1278, 'grad_norm': 0.23468293249607086, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2922159433364868, 'eval_runtime': 9.3213, 'eval_samples_per_second': 107.281, 'eval_steps_per_second': 6.759, 'epoch': 0.76}
{'loss': 1.1364, 'grad_norm': 0.213149294257164, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2861754894256592, 'eval_runtime': 9.2941, 'eval_samples_per_second': 107.595, 'eval_steps_per_second': 6.778, 'epoch': 0.8}
{'loss': 1.1727, 'grad_norm': 0.18761371076107025, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2815980911254883, 'eval_runtime': 9.2886, 'eval_samples_per_second': 107.659, 'eval_steps_per_second': 6.783, 'epoch': 0.84}
{'loss': 1.1296, 'grad_norm': 0.22238188982009888, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.277398943901062, 'eval_runtime': 9.2871, 'eval_samples_per_second': 107.676, 'eval_steps_per_second': 6.784, 'epoch': 0.88}
{'loss': 1.1246, 'grad_norm': 0.2073410153388977, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2753835916519165, 'eval_runtime': 9.2488, 'eval_samples_per_second': 108.122, 'eval_steps_per_second': 6.812, 'epoch': 0.92}
{'loss': 1.1296, 'grad_norm': 0.22993355989456177, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.273016095161438, 'eval_runtime': 9.1983, 'eval_samples_per_second': 108.716, 'eval_steps_per_second': 6.849, 'epoch': 0.96}
{'loss': 1.0757, 'grad_norm': 0.2727946639060974, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2726505994796753, 'eval_runtime': 9.1614, 'eval_samples_per_second': 109.153, 'eval_steps_per_second': 6.877, 'epoch': 1.0}
{'train_runtime': 423.1507, 'train_samples_per_second': 23.625, 'train_steps_per_second': 1.477, 'train_loss': 1.287773553466797, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9539350271224976, 1.650481939315796, 1.4894094467163086, 1.4399069547653198, 1.3862661123275757, 1.3585959672927856, 1.3434879779815674, 1.3397002220153809, 1.332466721534729, 1.328053593635559, 1.3212307691574097, 1.3183757066726685, 1.316589593887329, 1.3113844394683838, 1.3073872327804565, 1.3032431602478027, 1.297787070274353, 1.2946898937225342, 1.2922159433364868, 1.2861754894256592, 1.2815980911254883, 1.277398943901062, 1.2753835916519165, 1.273016095161438, 1.2726505994796753], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9539350271224976, 1.650481939315796, 1.4894094467163086, 1.4399069547653198, 1.3862661123275757, 1.3585959672927856, 1.3434879779815674, 1.3397002220153809, 1.332466721534729, 1.328053593635559, 1.3212307691574097, 1.3183757066726685, 1.316589593887329, 1.3113844394683838, 1.3073872327804565, 1.3032431602478027, 1.297787070274353, 1.2946898937225342, 1.2922159433364868, 1.2861754894256592, 1.2815980911254883, 1.277398943901062, 1.2753835916519165, 1.273016095161438, 1.2726505994796753]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2726505994796753
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.4657 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -0.8946261257935209
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -0.8946261257934922
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -0.8946261257934949
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -0.8956028516056236
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -0.8946261257934918
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1733, dtype=torch.float64), tensor(0.1854, dtype=torch.float64), tensor(0.0277, dtype=torch.float64), 0, tensor(0.5826, dtype=torch.float64), 0, tensor(0.0231, dtype=torch.float64), 0, 0, 32, 0, 0, 1, 0, 1, 69, 0.021033115543661562, 44.87442857614467, 0]
normalized proposed parameters for next round by BO: [tensor(0.1733, dtype=torch.float64), tensor(0.1854, dtype=torch.float64), tensor(0.0277, dtype=torch.float64), tensor(5.9806e-19, dtype=torch.float64), tensor(0.5826, dtype=torch.float64), tensor(0.0079, dtype=torch.float64), tensor(0.0231, dtype=torch.float64), tensor(1.2920e-18, dtype=torch.float64), tensor(4.7092e-19, dtype=torch.float64), tensor(0.9897, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5395, dtype=torch.float64), tensor(0.2103, dtype=torch.float64), tensor(0.9349, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.173
  gsm8k: 0.185
  rowan_hellaswag: 0.028
  sciq: 0
  triviaqa: 0.583
  truthfulqa_gen: 0
  wikitext: 0.023
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (69,)
  lora_dropout: (0.021033115543661562,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (44.87442857614467,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  69
lora dropout:  0.021033115543661562
lora alpha:  44.87442857614467
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 81,395,712 || all params: 8,111,656,960 || trainable%: 1.0034
length of training data:  9919
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7664, 'grad_norm': 0.8024165034294128, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0506765842437744, 'eval_runtime': 10.0936, 'eval_samples_per_second': 99.073, 'eval_steps_per_second': 6.242, 'epoch': 0.04}
{'loss': 1.3819, 'grad_norm': 0.3548228442668915, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.217729091644287, 'eval_runtime': 10.1497, 'eval_samples_per_second': 98.525, 'eval_steps_per_second': 6.207, 'epoch': 0.08}
{'loss': 1.211, 'grad_norm': 0.3411566913127899, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 2.0489444732666016, 'eval_runtime': 10.2358, 'eval_samples_per_second': 97.697, 'eval_steps_per_second': 6.155, 'epoch': 0.12}
{'loss': 1.0532, 'grad_norm': 0.4922322630882263, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.951592206954956, 'eval_runtime': 10.2452, 'eval_samples_per_second': 97.606, 'eval_steps_per_second': 6.149, 'epoch': 0.16}
{'loss': 0.9816, 'grad_norm': 0.2297828495502472, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 1.9878031015396118, 'eval_runtime': 10.2552, 'eval_samples_per_second': 97.511, 'eval_steps_per_second': 6.143, 'epoch': 0.2}
{'loss': 1.04, 'grad_norm': 0.3313291668891907, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 1.9877642393112183, 'eval_runtime': 10.258, 'eval_samples_per_second': 97.485, 'eval_steps_per_second': 6.142, 'epoch': 0.24}
{'loss': 1.0037, 'grad_norm': 0.2881784439086914, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 1.9855455160140991, 'eval_runtime': 10.2653, 'eval_samples_per_second': 97.415, 'eval_steps_per_second': 6.137, 'epoch': 0.28}
{'loss': 0.9851, 'grad_norm': 0.20540612936019897, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 1.9875634908676147, 'eval_runtime': 10.2789, 'eval_samples_per_second': 97.286, 'eval_steps_per_second': 6.129, 'epoch': 0.32}
{'loss': 0.9895, 'grad_norm': 0.23819458484649658, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 1.9656234979629517, 'eval_runtime': 10.2665, 'eval_samples_per_second': 97.404, 'eval_steps_per_second': 6.136, 'epoch': 0.36}
{'loss': 1.0062, 'grad_norm': 0.2185809314250946, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 1.9435428380966187, 'eval_runtime': 10.1795, 'eval_samples_per_second': 98.237, 'eval_steps_per_second': 6.189, 'epoch': 0.4}
{'loss': 0.9926, 'grad_norm': 0.2567274272441864, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 1.9498242139816284, 'eval_runtime': 10.1768, 'eval_samples_per_second': 98.263, 'eval_steps_per_second': 6.191, 'epoch': 0.44}
{'loss': 1.0063, 'grad_norm': 0.25156643986701965, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 1.9476433992385864, 'eval_runtime': 10.1678, 'eval_samples_per_second': 98.35, 'eval_steps_per_second': 6.196, 'epoch': 0.48}
{'loss': 0.9994, 'grad_norm': 0.265168696641922, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 1.9498577117919922, 'eval_runtime': 10.2005, 'eval_samples_per_second': 98.035, 'eval_steps_per_second': 6.176, 'epoch': 0.52}
{'loss': 0.9314, 'grad_norm': 0.25205138325691223, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 1.9803674221038818, 'eval_runtime': 10.1953, 'eval_samples_per_second': 98.084, 'eval_steps_per_second': 6.179, 'epoch': 0.56}
{'loss': 0.9237, 'grad_norm': 0.31526702642440796, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 1.9460549354553223, 'eval_runtime': 10.2421, 'eval_samples_per_second': 97.636, 'eval_steps_per_second': 6.151, 'epoch': 0.6}
{'loss': 0.9781, 'grad_norm': 0.21657660603523254, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 1.948633074760437, 'eval_runtime': 10.2246, 'eval_samples_per_second': 97.803, 'eval_steps_per_second': 6.162, 'epoch': 0.65}
{'loss': 0.9773, 'grad_norm': 0.2671250104904175, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 1.9454078674316406, 'eval_runtime': 10.2282, 'eval_samples_per_second': 97.769, 'eval_steps_per_second': 6.159, 'epoch': 0.69}
{'loss': 0.9691, 'grad_norm': 0.3132033050060272, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 1.9666759967803955, 'eval_runtime': 10.2313, 'eval_samples_per_second': 97.739, 'eval_steps_per_second': 6.158, 'epoch': 0.73}
{'loss': 0.9386, 'grad_norm': 0.2818731665611267, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 1.9833014011383057, 'eval_runtime': 10.2305, 'eval_samples_per_second': 97.747, 'eval_steps_per_second': 6.158, 'epoch': 0.77}
{'loss': 0.926, 'grad_norm': 0.2697601020336151, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 1.98075270652771, 'eval_runtime': 10.2098, 'eval_samples_per_second': 97.945, 'eval_steps_per_second': 6.171, 'epoch': 0.81}
{'loss': 0.9439, 'grad_norm': 0.2830169200897217, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 1.9868979454040527, 'eval_runtime': 10.183, 'eval_samples_per_second': 98.203, 'eval_steps_per_second': 6.187, 'epoch': 0.85}
{'loss': 0.9838, 'grad_norm': 0.2417040765285492, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 1.980176568031311, 'eval_runtime': 10.1717, 'eval_samples_per_second': 98.312, 'eval_steps_per_second': 6.194, 'epoch': 0.89}
{'loss': 1.0399, 'grad_norm': 0.24372704327106476, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 1.9790288209915161, 'eval_runtime': 10.1673, 'eval_samples_per_second': 98.354, 'eval_steps_per_second': 6.196, 'epoch': 0.93}
{'loss': 0.9464, 'grad_norm': 0.1769019216299057, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 1.979783296585083, 'eval_runtime': 10.1328, 'eval_samples_per_second': 98.689, 'eval_steps_per_second': 6.217, 'epoch': 0.97}
{'train_runtime': 458.2075, 'train_samples_per_second': 21.647, 'train_steps_per_second': 1.353, 'train_loss': 1.0782546689433437, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0506765842437744, 2.217729091644287, 2.0489444732666016, 1.951592206954956, 1.9878031015396118, 1.9877642393112183, 1.9855455160140991, 1.9875634908676147, 1.9656234979629517, 1.9435428380966187, 1.9498242139816284, 1.9476433992385864, 1.9498577117919922, 1.9803674221038818, 1.9460549354553223, 1.948633074760437, 1.9454078674316406, 1.9666759967803955, 1.9833014011383057, 1.98075270652771, 1.9868979454040527, 1.980176568031311, 1.9790288209915161, 1.979783296585083], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.0506765842437744, 2.217729091644287, 2.0489444732666016, 1.951592206954956, 1.9878031015396118, 1.9877642393112183, 1.9855455160140991, 1.9875634908676147, 1.9656234979629517, 1.9435428380966187, 1.9498242139816284, 1.9476433992385864, 1.9498577117919922, 1.9803674221038818, 1.9460549354553223, 1.948633074760437, 1.9454078674316406, 1.9666759967803955, 1.9833014011383057, 1.98075270652771, 1.9868979454040527, 1.980176568031311, 1.9790288209915161, 1.979783296585083]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.979783296585083
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.8189 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9994975924491882, 0.07707124948501587, 0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5594547390937805, 0.1537771224975586, 0.817682683467865, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.14512693881988525, 0.883480966091156, 0.9173967838287354, 0.8848122358322144]  ‚Üí  acq = -0.8993522069936624
X = [0.9514877796173096, 0.9132158160209656, 0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.41754159331321716, 0.28531861305236816, 0.5411667227745056, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.23015734553337097, 0.9975385665893555, 0.2174660712480545, 0.13808739185333252]  ‚Üí  acq = -0.8993517028457203
X = [0.39850908517837524, 0.8170029520988464, 0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5720815062522888, 0.12135124206542969, 0.050965309143066406, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8528783321380615, 0.15156877040863037, 0.1581156700849533, 0.8321003913879395]  ‚Üí  acq = -0.8994777817786463
X = [0.28970110416412354, 0.6408820152282715, 0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7433760762214661, 0.9123662114143372, 0.8424274921417236, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5417225956916809, 0.7911179065704346, 0.9902287721633911, 0.020802080631256104]  ‚Üí  acq = -0.8993517028454081
X = [0.5538846254348755, 0.17085957527160645, 0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6919615864753723, 0.1586219072341919, 0.2204926609992981, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.45917418599128723, 0.6860421299934387, 0.8713036775588989, 0.5007997155189514]  ‚Üí  acq = -0.899351702850488
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4058, dtype=torch.float64), 0, tensor(0.0702, dtype=torch.float64), 0, tensor(0.4054, dtype=torch.float64), tensor(0.0637, dtype=torch.float64), tensor(0.0548, dtype=torch.float64), 0, 0, 23, 0, 1, 1, 1, 1, 28, 1.2349475663761566e-05, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.4058, dtype=torch.float64), tensor(0.0001, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4054, dtype=torch.float64), tensor(0.0637, dtype=torch.float64), tensor(0.0548, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7234, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2224, dtype=torch.float64), tensor(0.0001, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.406
  gsm8k: 0
  rowan_hellaswag: 0.07
  sciq: 0
  triviaqa: 0.405
  truthfulqa_gen: 0.064
  wikitext: 0.055
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (28,)
  lora_dropout: (1.2349475663761566e-05,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  28
lora dropout:  1.2349475663761566e-05
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 38,907,904 || all params: 8,069,169,152 || trainable%: 0.4822
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1864, 'grad_norm': 1.5363919734954834, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.988882303237915, 'eval_runtime': 10.045, 'eval_samples_per_second': 99.552, 'eval_steps_per_second': 6.272, 'epoch': 0.04}
{'loss': 1.3447, 'grad_norm': 0.7087365984916687, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.017096519470215, 'eval_runtime': 10.0658, 'eval_samples_per_second': 99.346, 'eval_steps_per_second': 6.259, 'epoch': 0.08}
{'loss': 1.1886, 'grad_norm': 0.48593735694885254, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0723187923431396, 'eval_runtime': 10.0924, 'eval_samples_per_second': 99.084, 'eval_steps_per_second': 6.242, 'epoch': 0.12}
{'loss': 1.1863, 'grad_norm': 0.5582363605499268, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.074822187423706, 'eval_runtime': 10.1185, 'eval_samples_per_second': 98.829, 'eval_steps_per_second': 6.226, 'epoch': 0.16}
{'loss': 1.1311, 'grad_norm': 0.47228825092315674, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.03052020072937, 'eval_runtime': 10.171, 'eval_samples_per_second': 98.319, 'eval_steps_per_second': 6.194, 'epoch': 0.2}
{'loss': 1.1942, 'grad_norm': 0.49978527426719666, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.0210678577423096, 'eval_runtime': 10.1616, 'eval_samples_per_second': 98.409, 'eval_steps_per_second': 6.2, 'epoch': 0.24}
{'loss': 1.0875, 'grad_norm': 0.8348314762115479, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.052954912185669, 'eval_runtime': 10.1498, 'eval_samples_per_second': 98.524, 'eval_steps_per_second': 6.207, 'epoch': 0.28}
{'loss': 1.1004, 'grad_norm': 0.49833303689956665, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.027724027633667, 'eval_runtime': 10.1402, 'eval_samples_per_second': 98.617, 'eval_steps_per_second': 6.213, 'epoch': 0.32}
{'loss': 1.1206, 'grad_norm': 0.5582322478294373, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9870353937149048, 'eval_runtime': 10.1445, 'eval_samples_per_second': 98.576, 'eval_steps_per_second': 6.21, 'epoch': 0.36}
{'loss': 1.1187, 'grad_norm': 0.4512357711791992, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.0370447635650635, 'eval_runtime': 10.1363, 'eval_samples_per_second': 98.656, 'eval_steps_per_second': 6.215, 'epoch': 0.4}
{'loss': 1.0965, 'grad_norm': 0.5437455177307129, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0450873374938965, 'eval_runtime': 10.1459, 'eval_samples_per_second': 98.562, 'eval_steps_per_second': 6.209, 'epoch': 0.44}
{'loss': 1.0954, 'grad_norm': 0.4855063855648041, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.0258047580718994, 'eval_runtime': 10.1394, 'eval_samples_per_second': 98.625, 'eval_steps_per_second': 6.213, 'epoch': 0.48}
{'loss': 1.0605, 'grad_norm': 0.5282154679298401, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.0342204570770264, 'eval_runtime': 10.1464, 'eval_samples_per_second': 98.557, 'eval_steps_per_second': 6.209, 'epoch': 0.52}
{'loss': 1.0618, 'grad_norm': 0.5456564426422119, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.016658306121826, 'eval_runtime': 10.1365, 'eval_samples_per_second': 98.654, 'eval_steps_per_second': 6.215, 'epoch': 0.56}
{'loss': 1.0245, 'grad_norm': 0.491140753030777, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.0364043712615967, 'eval_runtime': 10.1409, 'eval_samples_per_second': 98.611, 'eval_steps_per_second': 6.212, 'epoch': 0.6}
{'loss': 1.105, 'grad_norm': 0.48115408420562744, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.0262749195098877, 'eval_runtime': 10.1445, 'eval_samples_per_second': 98.575, 'eval_steps_per_second': 6.21, 'epoch': 0.64}
{'loss': 1.0964, 'grad_norm': 0.6206018924713135, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.045092821121216, 'eval_runtime': 10.1403, 'eval_samples_per_second': 98.617, 'eval_steps_per_second': 6.213, 'epoch': 0.68}
{'loss': 1.03, 'grad_norm': 0.5960275530815125, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.0235671997070312, 'eval_runtime': 10.1377, 'eval_samples_per_second': 98.641, 'eval_steps_per_second': 6.214, 'epoch': 0.72}
{'loss': 1.086, 'grad_norm': 0.5625431537628174, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.042154550552368, 'eval_runtime': 10.1767, 'eval_samples_per_second': 98.264, 'eval_steps_per_second': 6.191, 'epoch': 0.76}
{'loss': 1.0128, 'grad_norm': 0.5903043150901794, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.040598154067993, 'eval_runtime': 10.1721, 'eval_samples_per_second': 98.308, 'eval_steps_per_second': 6.193, 'epoch': 0.8}
{'loss': 1.0345, 'grad_norm': 0.47307899594306946, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0369067192077637, 'eval_runtime': 10.2103, 'eval_samples_per_second': 97.94, 'eval_steps_per_second': 6.17, 'epoch': 0.84}
{'loss': 1.0524, 'grad_norm': 0.4811990261077881, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.047316074371338, 'eval_runtime': 10.2023, 'eval_samples_per_second': 98.017, 'eval_steps_per_second': 6.175, 'epoch': 0.88}
{'loss': 1.0962, 'grad_norm': 0.5514163374900818, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.048626661300659, 'eval_runtime': 10.2008, 'eval_samples_per_second': 98.031, 'eval_steps_per_second': 6.176, 'epoch': 0.92}
{'loss': 1.0717, 'grad_norm': 0.5225269794464111, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0442564487457275, 'eval_runtime': 10.2028, 'eval_samples_per_second': 98.012, 'eval_steps_per_second': 6.175, 'epoch': 0.96}
{'loss': 1.1174, 'grad_norm': 0.6062633991241455, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.0456955432891846, 'eval_runtime': 10.2074, 'eval_samples_per_second': 97.968, 'eval_steps_per_second': 6.172, 'epoch': 1.0}
{'train_runtime': 424.4532, 'train_samples_per_second': 23.55, 'train_steps_per_second': 1.472, 'train_loss': 1.1879781524658204, 'epoch': 1.0}
train_results:  {'eval_loss': [1.988882303237915, 2.017096519470215, 2.0723187923431396, 2.074822187423706, 2.03052020072937, 2.0210678577423096, 2.052954912185669, 2.027724027633667, 1.9870353937149048, 2.0370447635650635, 2.0450873374938965, 2.0258047580718994, 2.0342204570770264, 2.016658306121826, 2.0364043712615967, 2.0262749195098877, 2.045092821121216, 2.0235671997070312, 2.042154550552368, 2.040598154067993, 2.0369067192077637, 2.047316074371338, 2.048626661300659, 2.0442564487457275, 2.0456955432891846], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.988882303237915, 2.017096519470215, 2.0723187923431396, 2.074822187423706, 2.03052020072937, 2.0210678577423096, 2.052954912185669, 2.027724027633667, 1.9870353937149048, 2.0370447635650635, 2.0450873374938965, 2.0258047580718994, 2.0342204570770264, 2.016658306121826, 2.0364043712615967, 2.0262749195098877, 2.045092821121216, 2.0235671997070312, 2.042154550552368, 2.040598154067993, 2.0369067192077637, 2.047316074371338, 2.048626661300659, 2.0442564487457275, 2.0456955432891846]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.0456955432891846
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0234 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.9005271663320675
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.9056516022703891
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.9078876307119179
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.9040692486895727
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -0.9152505344339832
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1774, dtype=torch.float64), tensor(0.0695, dtype=torch.float64), tensor(0.1999, dtype=torch.float64), 0, tensor(0.2468, dtype=torch.float64), tensor(0.0338, dtype=torch.float64), 0, tensor(0.2727, dtype=torch.float64), 21, 0, 1, 1, 1, 1, 128, 0.1, 46.09208539107979, 0]
normalized proposed parameters for next round by BO: [tensor(2.7277e-18, dtype=torch.float64), tensor(0.1774, dtype=torch.float64), tensor(0.0695, dtype=torch.float64), tensor(0.1999, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2468, dtype=torch.float64), tensor(0.0338, dtype=torch.float64), tensor(6.0611e-20, dtype=torch.float64), tensor(0.2727, dtype=torch.float64), tensor(0.6423, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9603, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.177
  rowan_hellaswag: 0.069
  sciq: 0.2
  triviaqa: 0
  truthfulqa_gen: 0.247
  wikitext: 0.034
  mmlu: 0
  arc_challenge: 0.273

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (46.09208539107979,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  46.09208539107979
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 162,398,208 || all params: 8,192,659,456 || trainable%: 1.9822
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6593, 'grad_norm': 0.6045188307762146, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8655833005905151, 'eval_runtime': 9.8681, 'eval_samples_per_second': 101.337, 'eval_steps_per_second': 6.384, 'epoch': 0.04}
{'loss': 1.2462, 'grad_norm': 0.32049715518951416, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7898350954055786, 'eval_runtime': 9.9145, 'eval_samples_per_second': 100.862, 'eval_steps_per_second': 6.354, 'epoch': 0.08}
{'loss': 1.0535, 'grad_norm': 0.24027177691459656, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8288049697875977, 'eval_runtime': 9.9379, 'eval_samples_per_second': 100.625, 'eval_steps_per_second': 6.339, 'epoch': 0.12}
{'loss': 0.99, 'grad_norm': 0.2235405147075653, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.847333550453186, 'eval_runtime': 9.9876, 'eval_samples_per_second': 100.124, 'eval_steps_per_second': 6.308, 'epoch': 0.16}
{'loss': 0.9647, 'grad_norm': 0.19510146975517273, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.831287145614624, 'eval_runtime': 9.9977, 'eval_samples_per_second': 100.023, 'eval_steps_per_second': 6.301, 'epoch': 0.2}
{'loss': 1.0081, 'grad_norm': 0.21365585923194885, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8539347648620605, 'eval_runtime': 9.9898, 'eval_samples_per_second': 100.102, 'eval_steps_per_second': 6.306, 'epoch': 0.24}
{'loss': 0.985, 'grad_norm': 0.18163219094276428, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.856502890586853, 'eval_runtime': 9.9444, 'eval_samples_per_second': 100.559, 'eval_steps_per_second': 6.335, 'epoch': 0.28}
{'loss': 0.9378, 'grad_norm': 0.2820757031440735, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8643735647201538, 'eval_runtime': 9.9425, 'eval_samples_per_second': 100.579, 'eval_steps_per_second': 6.336, 'epoch': 0.32}
{'loss': 0.99, 'grad_norm': 0.2641523480415344, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8946785926818848, 'eval_runtime': 9.9396, 'eval_samples_per_second': 100.608, 'eval_steps_per_second': 6.338, 'epoch': 0.36}
{'loss': 0.9621, 'grad_norm': 0.2684982120990753, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9045051336288452, 'eval_runtime': 9.9406, 'eval_samples_per_second': 100.597, 'eval_steps_per_second': 6.338, 'epoch': 0.4}
{'loss': 0.9298, 'grad_norm': 0.2844974398612976, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8703309297561646, 'eval_runtime': 9.9398, 'eval_samples_per_second': 100.605, 'eval_steps_per_second': 6.338, 'epoch': 0.44}
{'loss': 0.9113, 'grad_norm': 0.250902384519577, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8850430250167847, 'eval_runtime': 9.9897, 'eval_samples_per_second': 100.103, 'eval_steps_per_second': 6.307, 'epoch': 0.48}
{'loss': 0.9298, 'grad_norm': 0.21860307455062866, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9361896514892578, 'eval_runtime': 9.952, 'eval_samples_per_second': 100.482, 'eval_steps_per_second': 6.33, 'epoch': 0.52}
{'loss': 0.9254, 'grad_norm': 0.34285539388656616, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.898468017578125, 'eval_runtime': 9.9833, 'eval_samples_per_second': 100.167, 'eval_steps_per_second': 6.311, 'epoch': 0.56}
{'loss': 0.9009, 'grad_norm': 0.2766267657279968, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8986400365829468, 'eval_runtime': 9.9814, 'eval_samples_per_second': 100.186, 'eval_steps_per_second': 6.312, 'epoch': 0.6}
{'loss': 0.8946, 'grad_norm': 0.25927498936653137, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9518084526062012, 'eval_runtime': 9.9588, 'eval_samples_per_second': 100.414, 'eval_steps_per_second': 6.326, 'epoch': 0.64}
{'loss': 0.8393, 'grad_norm': 0.25578248500823975, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9260457754135132, 'eval_runtime': 9.9402, 'eval_samples_per_second': 100.602, 'eval_steps_per_second': 6.338, 'epoch': 0.68}
{'loss': 0.8115, 'grad_norm': 0.2986564636230469, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.0101423263549805, 'eval_runtime': 9.9489, 'eval_samples_per_second': 100.514, 'eval_steps_per_second': 6.332, 'epoch': 0.72}
{'loss': 0.7766, 'grad_norm': 0.4571317434310913, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.0225906372070312, 'eval_runtime': 9.9561, 'eval_samples_per_second': 100.441, 'eval_steps_per_second': 6.328, 'epoch': 0.76}
{'loss': 0.847, 'grad_norm': 0.2930940091609955, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9855303764343262, 'eval_runtime': 9.9532, 'eval_samples_per_second': 100.47, 'eval_steps_per_second': 6.33, 'epoch': 0.8}
{'loss': 0.786, 'grad_norm': 0.3631271421909332, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.984673261642456, 'eval_runtime': 9.9648, 'eval_samples_per_second': 100.354, 'eval_steps_per_second': 6.322, 'epoch': 0.84}
{'loss': 0.8474, 'grad_norm': 0.23323598504066467, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9801748991012573, 'eval_runtime': 9.9445, 'eval_samples_per_second': 100.558, 'eval_steps_per_second': 6.335, 'epoch': 0.88}
{'loss': 0.7974, 'grad_norm': 0.36738869547843933, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0028209686279297, 'eval_runtime': 9.9478, 'eval_samples_per_second': 100.525, 'eval_steps_per_second': 6.333, 'epoch': 0.92}
{'loss': 0.8127, 'grad_norm': 0.4016610085964203, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.000112295150757, 'eval_runtime': 9.9533, 'eval_samples_per_second': 100.47, 'eval_steps_per_second': 6.33, 'epoch': 0.96}
{'loss': 0.8064, 'grad_norm': 0.40226346254348755, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9936916828155518, 'eval_runtime': 9.9377, 'eval_samples_per_second': 100.627, 'eval_steps_per_second': 6.34, 'epoch': 1.0}
{'train_runtime': 438.835, 'train_samples_per_second': 22.778, 'train_steps_per_second': 1.424, 'train_loss': 0.9845127716064453, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8655833005905151, 1.7898350954055786, 1.8288049697875977, 1.847333550453186, 1.831287145614624, 1.8539347648620605, 1.856502890586853, 1.8643735647201538, 1.8946785926818848, 1.9045051336288452, 1.8703309297561646, 1.8850430250167847, 1.9361896514892578, 1.898468017578125, 1.8986400365829468, 1.9518084526062012, 1.9260457754135132, 2.0101423263549805, 2.0225906372070312, 1.9855303764343262, 1.984673261642456, 1.9801748991012573, 2.0028209686279297, 2.000112295150757, 1.9936916828155518], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8655833005905151, 1.7898350954055786, 1.8288049697875977, 1.847333550453186, 1.831287145614624, 1.8539347648620605, 1.856502890586853, 1.8643735647201538, 1.8946785926818848, 1.9045051336288452, 1.8703309297561646, 1.8850430250167847, 1.9361896514892578, 1.898468017578125, 1.8986400365829468, 1.9518084526062012, 1.9260457754135132, 2.0101423263549805, 2.0225906372070312, 1.9855303764343262, 1.984673261642456, 1.9801748991012573, 2.0028209686279297, 2.000112295150757, 1.9936916828155518]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.9936916828155518
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2957 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.8725078874526191
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -0.9035279940153241
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.859920802565445
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -0.9035280180766101
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -0.9108767207961302
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0796, dtype=torch.float64), tensor(0.0509, dtype=torch.float64), tensor(0.0781, dtype=torch.float64), tensor(0.0306, dtype=torch.float64), tensor(0.0662, dtype=torch.float64), tensor(0.0196, dtype=torch.float64), tensor(0.2782, dtype=torch.float64), tensor(0.3968, dtype=torch.float64), 24, 0, 0, 1, 1, 1, 119, 0.0038760596642227053, 30.825307989446735, 0]
normalized proposed parameters for next round by BO: [tensor(8.3567e-20, dtype=torch.float64), tensor(0.0796, dtype=torch.float64), tensor(0.0509, dtype=torch.float64), tensor(0.0781, dtype=torch.float64), tensor(0.0306, dtype=torch.float64), tensor(0.0662, dtype=torch.float64), tensor(0.0196, dtype=torch.float64), tensor(0.2782, dtype=torch.float64), tensor(0.3968, dtype=torch.float64), tensor(0.7397, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9331, dtype=torch.float64), tensor(0.0388, dtype=torch.float64), tensor(0.6422, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.08
  rowan_hellaswag: 0.051
  sciq: 0.078
  triviaqa: 0.031
  truthfulqa_gen: 0.066
  wikitext: 0.02
  mmlu: 0.278
  arc_challenge: 0.397

LoRA Parameters:
  lora_r: (119,)
  lora_dropout: (0.0038760596642227053,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (30.825307989446735,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  119
lora dropout:  0.0038760596642227053
lora alpha:  30.825307989446735
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 157,925,376 || all params: 8,188,186,624 || trainable%: 1.9287
length of training data:  9994
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7445, 'grad_norm': 0.5544185638427734, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6630405187606812, 'eval_runtime': 10.6937, 'eval_samples_per_second': 93.513, 'eval_steps_per_second': 5.891, 'epoch': 0.04}
{'loss': 1.351, 'grad_norm': 0.24105797708034515, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4039108753204346, 'eval_runtime': 10.7521, 'eval_samples_per_second': 93.005, 'eval_steps_per_second': 5.859, 'epoch': 0.08}
{'loss': 1.1933, 'grad_norm': 0.1967475861310959, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3233245611190796, 'eval_runtime': 10.748, 'eval_samples_per_second': 93.041, 'eval_steps_per_second': 5.862, 'epoch': 0.12}
{'loss': 1.1007, 'grad_norm': 0.17871050536632538, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3176532983779907, 'eval_runtime': 10.713, 'eval_samples_per_second': 93.344, 'eval_steps_per_second': 5.881, 'epoch': 0.16}
{'loss': 1.0652, 'grad_norm': 0.18287688493728638, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2957549095153809, 'eval_runtime': 10.7166, 'eval_samples_per_second': 93.313, 'eval_steps_per_second': 5.879, 'epoch': 0.2}
{'loss': 1.0985, 'grad_norm': 0.18790371716022491, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.28419828414917, 'eval_runtime': 10.721, 'eval_samples_per_second': 93.275, 'eval_steps_per_second': 5.876, 'epoch': 0.24}
{'loss': 1.1256, 'grad_norm': 0.17749610543251038, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2787724733352661, 'eval_runtime': 10.7286, 'eval_samples_per_second': 93.209, 'eval_steps_per_second': 5.872, 'epoch': 0.28}
{'loss': 1.0212, 'grad_norm': 0.17559124529361725, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2801094055175781, 'eval_runtime': 10.7939, 'eval_samples_per_second': 92.645, 'eval_steps_per_second': 5.837, 'epoch': 0.32}
{'loss': 1.0675, 'grad_norm': 0.1746576726436615, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2751948833465576, 'eval_runtime': 10.8031, 'eval_samples_per_second': 92.566, 'eval_steps_per_second': 5.832, 'epoch': 0.36}
{'loss': 1.0212, 'grad_norm': 0.22901028394699097, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2694196701049805, 'eval_runtime': 10.7897, 'eval_samples_per_second': 92.681, 'eval_steps_per_second': 5.839, 'epoch': 0.4}
{'loss': 1.033, 'grad_norm': 0.16539455950260162, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.271751046180725, 'eval_runtime': 10.7854, 'eval_samples_per_second': 92.718, 'eval_steps_per_second': 5.841, 'epoch': 0.44}
{'loss': 1.0392, 'grad_norm': 0.24600552022457123, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2670725584030151, 'eval_runtime': 10.8174, 'eval_samples_per_second': 92.444, 'eval_steps_per_second': 5.824, 'epoch': 0.48}
{'loss': 0.9864, 'grad_norm': 0.27101579308509827, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2675297260284424, 'eval_runtime': 10.805, 'eval_samples_per_second': 92.55, 'eval_steps_per_second': 5.831, 'epoch': 0.52}
{'loss': 0.9787, 'grad_norm': 0.2329193353652954, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2611513137817383, 'eval_runtime': 10.8458, 'eval_samples_per_second': 92.201, 'eval_steps_per_second': 5.809, 'epoch': 0.56}
{'loss': 1.0279, 'grad_norm': 0.21912898123264313, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2593495845794678, 'eval_runtime': 10.7945, 'eval_samples_per_second': 92.64, 'eval_steps_per_second': 5.836, 'epoch': 0.6}
{'loss': 0.9884, 'grad_norm': 0.234512597322464, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.258548378944397, 'eval_runtime': 10.743, 'eval_samples_per_second': 93.083, 'eval_steps_per_second': 5.864, 'epoch': 0.64}
{'loss': 0.9547, 'grad_norm': 0.20638206601142883, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.25506591796875, 'eval_runtime': 10.7506, 'eval_samples_per_second': 93.018, 'eval_steps_per_second': 5.86, 'epoch': 0.68}
{'loss': 0.9192, 'grad_norm': 0.22147418558597565, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2556846141815186, 'eval_runtime': 10.7422, 'eval_samples_per_second': 93.09, 'eval_steps_per_second': 5.865, 'epoch': 0.72}
{'loss': 0.8729, 'grad_norm': 0.22181336581707, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2519137859344482, 'eval_runtime': 10.7542, 'eval_samples_per_second': 92.987, 'eval_steps_per_second': 5.858, 'epoch': 0.76}
{'loss': 0.9105, 'grad_norm': 0.29844120144844055, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.249593734741211, 'eval_runtime': 10.7544, 'eval_samples_per_second': 92.985, 'eval_steps_per_second': 5.858, 'epoch': 0.8}
{'loss': 0.8644, 'grad_norm': 0.3047628700733185, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.246993899345398, 'eval_runtime': 10.7418, 'eval_samples_per_second': 93.094, 'eval_steps_per_second': 5.865, 'epoch': 0.84}
{'loss': 0.8355, 'grad_norm': 0.27369654178619385, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2457493543624878, 'eval_runtime': 10.7455, 'eval_samples_per_second': 93.062, 'eval_steps_per_second': 5.863, 'epoch': 0.88}
{'loss': 0.8857, 'grad_norm': 0.2223343402147293, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2442619800567627, 'eval_runtime': 10.7472, 'eval_samples_per_second': 93.048, 'eval_steps_per_second': 5.862, 'epoch': 0.92}
{'loss': 0.8647, 'grad_norm': 0.27187052369117737, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2419389486312866, 'eval_runtime': 10.7583, 'eval_samples_per_second': 92.952, 'eval_steps_per_second': 5.856, 'epoch': 0.96}
{'loss': 0.8671, 'grad_norm': 0.39724114537239075, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2415797710418701, 'eval_runtime': 10.7523, 'eval_samples_per_second': 93.003, 'eval_steps_per_second': 5.859, 'epoch': 1.0}
{'train_runtime': 486.4683, 'train_samples_per_second': 20.544, 'train_steps_per_second': 1.285, 'train_loss': 1.0726759796142578, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6630405187606812, 1.4039108753204346, 1.3233245611190796, 1.3176532983779907, 1.2957549095153809, 1.28419828414917, 1.2787724733352661, 1.2801094055175781, 1.2751948833465576, 1.2694196701049805, 1.271751046180725, 1.2670725584030151, 1.2675297260284424, 1.2611513137817383, 1.2593495845794678, 1.258548378944397, 1.25506591796875, 1.2556846141815186, 1.2519137859344482, 1.249593734741211, 1.246993899345398, 1.2457493543624878, 1.2442619800567627, 1.2419389486312866, 1.2415797710418701], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6630405187606812, 1.4039108753204346, 1.3233245611190796, 1.3176532983779907, 1.2957549095153809, 1.28419828414917, 1.2787724733352661, 1.2801094055175781, 1.2751948833465576, 1.2694196701049805, 1.271751046180725, 1.2670725584030151, 1.2675297260284424, 1.2611513137817383, 1.2593495845794678, 1.258548378944397, 1.25506591796875, 1.2556846141815186, 1.2519137859344482, 1.249593734741211, 1.246993899345398, 1.2457493543624878, 1.2442619800567627, 1.2419389486312866, 1.2415797710418701]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2415797710418701
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.8395 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -0.9078130287329309
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -0.9076101183482084
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -0.9075764360965752
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -0.9075760560470547
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.9081022687025505
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0363, dtype=torch.float64), tensor(0.0326, dtype=torch.float64), 0, tensor(0.0882, dtype=torch.float64), 0, tensor(0.0614, dtype=torch.float64), tensor(0.1304, dtype=torch.float64), tensor(0.2173, dtype=torch.float64), tensor(0.4244, dtype=torch.float64), 16, 0, 0, 1, 0, 1, 110, 0.00658251107660232, 34.897597979166015, 0]
normalized proposed parameters for next round by BO: [tensor(0.0363, dtype=torch.float64), tensor(0.0326, dtype=torch.float64), tensor(0.0094, dtype=torch.float64), tensor(0.0882, dtype=torch.float64), tensor(1.3833e-18, dtype=torch.float64), tensor(0.0614, dtype=torch.float64), tensor(0.1304, dtype=torch.float64), tensor(0.2173, dtype=torch.float64), tensor(0.4244, dtype=torch.float64), tensor(0.4881, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8608, dtype=torch.float64), tensor(0.0658, dtype=torch.float64), tensor(0.7270, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.036
  gsm8k: 0.033
  rowan_hellaswag: 0
  sciq: 0.088
  triviaqa: 0
  truthfulqa_gen: 0.061
  wikitext: 0.13
  mmlu: 0.217
  arc_challenge: 0.424

LoRA Parameters:
  lora_r: (110,)
  lora_dropout: (0.00658251107660232,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (34.897597979166015,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  110
lora dropout:  0.00658251107660232
lora alpha:  34.897597979166015
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 64,880,640 || all params: 8,095,141,888 || trainable%: 0.8015
length of training data:  9904
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1002, 'grad_norm': 0.43747320771217346, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9505847692489624, 'eval_runtime': 9.2757, 'eval_samples_per_second': 107.808, 'eval_steps_per_second': 6.792, 'epoch': 0.04}
{'loss': 1.5913, 'grad_norm': 0.19826769828796387, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5958300828933716, 'eval_runtime': 9.2653, 'eval_samples_per_second': 107.93, 'eval_steps_per_second': 6.8, 'epoch': 0.08}
{'loss': 1.3136, 'grad_norm': 0.19337120652198792, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 1.4931038618087769, 'eval_runtime': 9.2782, 'eval_samples_per_second': 107.779, 'eval_steps_per_second': 6.79, 'epoch': 0.12}
{'loss': 1.2972, 'grad_norm': 0.19197028875350952, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 1.4417847394943237, 'eval_runtime': 9.3131, 'eval_samples_per_second': 107.376, 'eval_steps_per_second': 6.765, 'epoch': 0.16}
{'loss': 1.1864, 'grad_norm': 0.18350885808467865, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 1.3773939609527588, 'eval_runtime': 9.3276, 'eval_samples_per_second': 107.209, 'eval_steps_per_second': 6.754, 'epoch': 0.2}
{'loss': 1.0959, 'grad_norm': 0.20802204310894012, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 1.3516770601272583, 'eval_runtime': 9.3513, 'eval_samples_per_second': 106.937, 'eval_steps_per_second': 6.737, 'epoch': 0.24}
{'loss': 1.1705, 'grad_norm': 0.2257758229970932, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 1.338302493095398, 'eval_runtime': 9.3802, 'eval_samples_per_second': 106.607, 'eval_steps_per_second': 6.716, 'epoch': 0.28}
{'loss': 1.0461, 'grad_norm': 0.245973140001297, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 1.3443034887313843, 'eval_runtime': 9.3593, 'eval_samples_per_second': 106.846, 'eval_steps_per_second': 6.731, 'epoch': 0.32}
{'loss': 1.0603, 'grad_norm': 0.23198984563350677, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 1.3341095447540283, 'eval_runtime': 9.4167, 'eval_samples_per_second': 106.194, 'eval_steps_per_second': 6.69, 'epoch': 0.36}
{'loss': 1.0704, 'grad_norm': 0.2186584174633026, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 1.3356999158859253, 'eval_runtime': 9.4324, 'eval_samples_per_second': 106.018, 'eval_steps_per_second': 6.679, 'epoch': 0.4}
{'loss': 1.002, 'grad_norm': 0.23341037333011627, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 1.3325316905975342, 'eval_runtime': 9.4508, 'eval_samples_per_second': 105.811, 'eval_steps_per_second': 6.666, 'epoch': 0.44}
{'loss': 0.9983, 'grad_norm': 0.2201969176530838, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 1.3309814929962158, 'eval_runtime': 9.424, 'eval_samples_per_second': 106.112, 'eval_steps_per_second': 6.685, 'epoch': 0.48}
{'loss': 1.0967, 'grad_norm': 0.2751298248767853, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 1.328325629234314, 'eval_runtime': 9.3755, 'eval_samples_per_second': 106.661, 'eval_steps_per_second': 6.72, 'epoch': 0.53}
{'loss': 0.987, 'grad_norm': 0.2705290913581848, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 1.3296501636505127, 'eval_runtime': 9.3609, 'eval_samples_per_second': 106.827, 'eval_steps_per_second': 6.73, 'epoch': 0.57}
{'loss': 1.0154, 'grad_norm': 0.3070533275604248, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 1.3245257139205933, 'eval_runtime': 9.3855, 'eval_samples_per_second': 106.547, 'eval_steps_per_second': 6.712, 'epoch': 0.61}
{'loss': 0.9844, 'grad_norm': 0.24391676485538483, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 1.326791763305664, 'eval_runtime': 9.4247, 'eval_samples_per_second': 106.105, 'eval_steps_per_second': 6.685, 'epoch': 0.65}
{'loss': 0.9605, 'grad_norm': 0.3797706663608551, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 1.325366735458374, 'eval_runtime': 9.3824, 'eval_samples_per_second': 106.582, 'eval_steps_per_second': 6.715, 'epoch': 0.69}
{'loss': 0.9774, 'grad_norm': 0.2901630997657776, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 1.324004888534546, 'eval_runtime': 9.3568, 'eval_samples_per_second': 106.874, 'eval_steps_per_second': 6.733, 'epoch': 0.73}
{'loss': 0.9266, 'grad_norm': 0.3169773817062378, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 1.3213539123535156, 'eval_runtime': 9.3541, 'eval_samples_per_second': 106.905, 'eval_steps_per_second': 6.735, 'epoch': 0.77}
{'loss': 0.9979, 'grad_norm': 0.29117122292518616, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 1.3202463388442993, 'eval_runtime': 9.3739, 'eval_samples_per_second': 106.68, 'eval_steps_per_second': 6.721, 'epoch': 0.81}
{'loss': 0.8999, 'grad_norm': 0.3302585184574127, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 1.3182129859924316, 'eval_runtime': 9.3395, 'eval_samples_per_second': 107.072, 'eval_steps_per_second': 6.746, 'epoch': 0.85}
{'loss': 0.936, 'grad_norm': 0.35082903504371643, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 1.3189380168914795, 'eval_runtime': 9.3494, 'eval_samples_per_second': 106.958, 'eval_steps_per_second': 6.738, 'epoch': 0.89}
{'loss': 0.9351, 'grad_norm': 0.24311348795890808, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 1.3174291849136353, 'eval_runtime': 9.3397, 'eval_samples_per_second': 107.07, 'eval_steps_per_second': 6.745, 'epoch': 0.93}
{'loss': 0.8599, 'grad_norm': 0.2804577350616455, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 1.3173495531082153, 'eval_runtime': 9.35, 'eval_samples_per_second': 106.952, 'eval_steps_per_second': 6.738, 'epoch': 0.97}
{'train_runtime': 365.4154, 'train_samples_per_second': 27.103, 'train_steps_per_second': 1.694, 'train_loss': 1.1380159381133097, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9505847692489624, 1.5958300828933716, 1.4931038618087769, 1.4417847394943237, 1.3773939609527588, 1.3516770601272583, 1.338302493095398, 1.3443034887313843, 1.3341095447540283, 1.3356999158859253, 1.3325316905975342, 1.3309814929962158, 1.328325629234314, 1.3296501636505127, 1.3245257139205933, 1.326791763305664, 1.325366735458374, 1.324004888534546, 1.3213539123535156, 1.3202463388442993, 1.3182129859924316, 1.3189380168914795, 1.3174291849136353, 1.3173495531082153], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9505847692489624, 1.5958300828933716, 1.4931038618087769, 1.4417847394943237, 1.3773939609527588, 1.3516770601272583, 1.338302493095398, 1.3443034887313843, 1.3341095447540283, 1.3356999158859253, 1.3325316905975342, 1.3309814929962158, 1.328325629234314, 1.3296501636505127, 1.3245257139205933, 1.326791763305664, 1.325366735458374, 1.324004888534546, 1.3213539123535156, 1.3202463388442993, 1.3182129859924316, 1.3189380168914795, 1.3174291849136353, 1.3173495531082153]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3173495531082153
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.0506 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1793595552444458, 0.3586342930793762, 0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.34960928559303284, 0.057854533195495605, 0.7710180282592773, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6546881198883057, 0.3577408194541931, 0.8840495347976685, 0.5400984883308411]  ‚Üí  acq = -0.9272890472096695
X = [0.7988576889038086, 0.1653779149055481, 0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9513463973999023, 0.8898367881774902, 0.2988312840461731, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.24033895134925842, 0.2050917148590088, 0.051226988434791565, 0.8115118741989136]  ‚Üí  acq = -0.9271792901808745
X = [0.22445803880691528, 0.07812052965164185, 0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9895481467247009, 0.6373879313468933, 0.9490465521812439, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6339337825775146, 0.44919973611831665, 0.9787859916687012, 0.45290279388427734]  ‚Üí  acq = -0.9271868928443077
X = [0.3654218316078186, 0.6524207592010498, 0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.8971459269523621, 0.9170647859573364, 0.6597838997840881, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.8601893186569214, 0.164650559425354, 0.14350587129592896, 0.7966952323913574]  ‚Üí  acq = -0.9271862680061931
X = [0.4475772976875305, 0.6951091885566711, 0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.06307335197925568, 0.7526708841323853, 0.20946532487869263, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.10499551892280579, 0.07692551612854004, 0.719855546951294, 0.04362046718597412]  ‚Üí  acq = -0.9286996040379007
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1274, dtype=torch.float64), tensor(0.1042, dtype=torch.float64), tensor(0.0262, dtype=torch.float64), tensor(0.1630, dtype=torch.float64), 0, tensor(0.1616, dtype=torch.float64), 0, tensor(0.2318, dtype=torch.float64), tensor(0.1859, dtype=torch.float64), 14, 0, 1, 0, 1, 0, 48, 0.005970830468771614, 28.779954952061352, 0]
normalized proposed parameters for next round by BO: [tensor(0.1274, dtype=torch.float64), tensor(0.1042, dtype=torch.float64), tensor(0.0262, dtype=torch.float64), tensor(0.1630, dtype=torch.float64), tensor(3.9713e-18, dtype=torch.float64), tensor(0.1616, dtype=torch.float64), tensor(1.8574e-17, dtype=torch.float64), tensor(0.2318, dtype=torch.float64), tensor(0.1859, dtype=torch.float64), tensor(0.4384, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3715, dtype=torch.float64), tensor(0.0597, dtype=torch.float64), tensor(0.5996, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.127
  gsm8k: 0.104
  rowan_hellaswag: 0.026
  sciq: 0.163
  triviaqa: 0
  truthfulqa_gen: 0.162
  wikitext: 0
  mmlu: 0.232
  arc_challenge: 0.186

LoRA Parameters:
  lora_r: (48,)
  lora_dropout: (0.005970830468771614,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (28.779954952061352,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  48
lora dropout:  0.005970830468771614
lora alpha:  28.779954952061352
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 15,826,944 || all params: 8,046,088,192 || trainable%: 0.1967
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4007, 'grad_norm': 1.7596441507339478, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0574212074279785, 'eval_runtime': 9.0008, 'eval_samples_per_second': 111.102, 'eval_steps_per_second': 6.999, 'epoch': 0.04}
{'loss': 1.5681, 'grad_norm': 0.5466635227203369, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.559507131576538, 'eval_runtime': 9.0329, 'eval_samples_per_second': 110.707, 'eval_steps_per_second': 6.975, 'epoch': 0.08}
{'loss': 1.2674, 'grad_norm': 0.3662336766719818, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4701164960861206, 'eval_runtime': 9.0929, 'eval_samples_per_second': 109.976, 'eval_steps_per_second': 6.928, 'epoch': 0.12}
{'loss': 1.1623, 'grad_norm': 0.28524404764175415, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4540884494781494, 'eval_runtime': 9.1259, 'eval_samples_per_second': 109.578, 'eval_steps_per_second': 6.903, 'epoch': 0.16}
{'loss': 1.1245, 'grad_norm': 0.3244531750679016, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4139026403427124, 'eval_runtime': 9.1236, 'eval_samples_per_second': 109.606, 'eval_steps_per_second': 6.905, 'epoch': 0.2}
{'loss': 1.1391, 'grad_norm': 0.294646292924881, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3934025764465332, 'eval_runtime': 9.1161, 'eval_samples_per_second': 109.696, 'eval_steps_per_second': 6.911, 'epoch': 0.24}
{'loss': 1.1538, 'grad_norm': 0.31022366881370544, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3726418018341064, 'eval_runtime': 9.1313, 'eval_samples_per_second': 109.514, 'eval_steps_per_second': 6.899, 'epoch': 0.28}
{'loss': 1.1276, 'grad_norm': 0.2445778250694275, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3542464971542358, 'eval_runtime': 9.1392, 'eval_samples_per_second': 109.419, 'eval_steps_per_second': 6.893, 'epoch': 0.32}
{'loss': 1.1304, 'grad_norm': 0.2769661843776703, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3495099544525146, 'eval_runtime': 9.1197, 'eval_samples_per_second': 109.653, 'eval_steps_per_second': 6.908, 'epoch': 0.36}
{'loss': 1.1105, 'grad_norm': 0.26280516386032104, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3503296375274658, 'eval_runtime': 9.1236, 'eval_samples_per_second': 109.606, 'eval_steps_per_second': 6.905, 'epoch': 0.4}
{'loss': 1.0634, 'grad_norm': 0.27567771077156067, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.340837836265564, 'eval_runtime': 9.1276, 'eval_samples_per_second': 109.557, 'eval_steps_per_second': 6.902, 'epoch': 0.44}
{'loss': 1.0956, 'grad_norm': 0.23577994108200073, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3422155380249023, 'eval_runtime': 9.1343, 'eval_samples_per_second': 109.478, 'eval_steps_per_second': 6.897, 'epoch': 0.48}
{'loss': 1.103, 'grad_norm': 0.27905863523483276, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3446650505065918, 'eval_runtime': 9.1305, 'eval_samples_per_second': 109.523, 'eval_steps_per_second': 6.9, 'epoch': 0.52}
{'loss': 1.0662, 'grad_norm': 0.2636580169200897, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3416049480438232, 'eval_runtime': 9.1487, 'eval_samples_per_second': 109.305, 'eval_steps_per_second': 6.886, 'epoch': 0.56}
{'loss': 1.0506, 'grad_norm': 0.28233665227890015, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3378559350967407, 'eval_runtime': 9.0786, 'eval_samples_per_second': 110.149, 'eval_steps_per_second': 6.939, 'epoch': 0.6}
{'loss': 1.1128, 'grad_norm': 0.29332348704338074, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3370198011398315, 'eval_runtime': 9.0751, 'eval_samples_per_second': 110.191, 'eval_steps_per_second': 6.942, 'epoch': 0.64}
{'loss': 1.0328, 'grad_norm': 0.2792801260948181, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3337042331695557, 'eval_runtime': 9.0681, 'eval_samples_per_second': 110.277, 'eval_steps_per_second': 6.947, 'epoch': 0.68}
{'loss': 1.0615, 'grad_norm': 0.26523080468177795, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3337510824203491, 'eval_runtime': 9.0774, 'eval_samples_per_second': 110.164, 'eval_steps_per_second': 6.94, 'epoch': 0.72}
{'loss': 1.0715, 'grad_norm': 0.2654317617416382, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.332517147064209, 'eval_runtime': 9.0624, 'eval_samples_per_second': 110.346, 'eval_steps_per_second': 6.952, 'epoch': 0.76}
{'loss': 1.076, 'grad_norm': 0.27601978182792664, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3282116651535034, 'eval_runtime': 9.0797, 'eval_samples_per_second': 110.135, 'eval_steps_per_second': 6.939, 'epoch': 0.8}
{'loss': 1.1181, 'grad_norm': 0.28115588426589966, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3290729522705078, 'eval_runtime': 9.0577, 'eval_samples_per_second': 110.403, 'eval_steps_per_second': 6.955, 'epoch': 0.84}
{'loss': 1.0601, 'grad_norm': 0.2886159420013428, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3264129161834717, 'eval_runtime': 9.0112, 'eval_samples_per_second': 110.974, 'eval_steps_per_second': 6.991, 'epoch': 0.88}
{'loss': 1.0723, 'grad_norm': 0.2870139479637146, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.32810378074646, 'eval_runtime': 9.0059, 'eval_samples_per_second': 111.038, 'eval_steps_per_second': 6.995, 'epoch': 0.92}
{'loss': 1.0491, 'grad_norm': 0.2872622311115265, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3270208835601807, 'eval_runtime': 8.9961, 'eval_samples_per_second': 111.159, 'eval_steps_per_second': 7.003, 'epoch': 0.96}
{'loss': 1.0653, 'grad_norm': 0.2678881287574768, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3270573616027832, 'eval_runtime': 9.0015, 'eval_samples_per_second': 111.093, 'eval_steps_per_second': 6.999, 'epoch': 1.0}
{'train_runtime': 370.2254, 'train_samples_per_second': 27.002, 'train_steps_per_second': 1.688, 'train_loss': 1.2112946960449218, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0574212074279785, 1.559507131576538, 1.4701164960861206, 1.4540884494781494, 1.4139026403427124, 1.3934025764465332, 1.3726418018341064, 1.3542464971542358, 1.3495099544525146, 1.3503296375274658, 1.340837836265564, 1.3422155380249023, 1.3446650505065918, 1.3416049480438232, 1.3378559350967407, 1.3370198011398315, 1.3337042331695557, 1.3337510824203491, 1.332517147064209, 1.3282116651535034, 1.3290729522705078, 1.3264129161834717, 1.32810378074646, 1.3270208835601807, 1.3270573616027832], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0574212074279785, 1.559507131576538, 1.4701164960861206, 1.4540884494781494, 1.4139026403427124, 1.3934025764465332, 1.3726418018341064, 1.3542464971542358, 1.3495099544525146, 1.3503296375274658, 1.340837836265564, 1.3422155380249023, 1.3446650505065918, 1.3416049480438232, 1.3378559350967407, 1.3370198011398315, 1.3337042331695557, 1.3337510824203491, 1.332517147064209, 1.3282116651535034, 1.3290729522705078, 1.3264129161834717, 1.32810378074646, 1.3270208835601807, 1.3270573616027832]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3270573616027832
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.2133 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -0.9220925936492532
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -0.9220207929020374
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -0.9220404506142599
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -0.9232015496708411
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -0.922584744229465
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0494, dtype=torch.float64), tensor(0.0466, dtype=torch.float64), tensor(0.1309, dtype=torch.float64), tensor(0.0247, dtype=torch.float64), tensor(0.0718, dtype=torch.float64), tensor(0.0186, dtype=torch.float64), tensor(0.2417, dtype=torch.float64), tensor(0.4158, dtype=torch.float64), 18, 0, 1, 1, 0, 1, 127, 3.017204541766461e-20, 33.487441432087834, 0]
normalized proposed parameters for next round by BO: [tensor(0.0004, dtype=torch.float64), tensor(0.0494, dtype=torch.float64), tensor(0.0466, dtype=torch.float64), tensor(0.1309, dtype=torch.float64), tensor(0.0247, dtype=torch.float64), tensor(0.0718, dtype=torch.float64), tensor(0.0186, dtype=torch.float64), tensor(0.2417, dtype=torch.float64), tensor(0.4158, dtype=torch.float64), tensor(0.5671, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9935, dtype=torch.float64), tensor(3.0172e-19, dtype=torch.float64), tensor(0.6977, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.049
  rowan_hellaswag: 0.047
  sciq: 0.131
  triviaqa: 0.025
  truthfulqa_gen: 0.072
  wikitext: 0.019
  mmlu: 0.242
  arc_challenge: 0.416

LoRA Parameters:
  lora_r: (127,)
  lora_dropout: (3.017204541766461e-20,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (33.487441432087834,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  127
lora dropout:  3.017204541766461e-20
lora alpha:  33.487441432087834
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 95,975,424 || all params: 8,126,236,672 || trainable%: 1.1811
length of training data:  9993
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0793, 'grad_norm': 0.48784932494163513, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.866782307624817, 'eval_runtime': 9.7248, 'eval_samples_per_second': 102.83, 'eval_steps_per_second': 6.478, 'epoch': 0.04}
{'loss': 1.55, 'grad_norm': 0.2401411086320877, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5871025323867798, 'eval_runtime': 9.7276, 'eval_samples_per_second': 102.8, 'eval_steps_per_second': 6.476, 'epoch': 0.08}
{'loss': 1.32, 'grad_norm': 0.20348690450191498, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.482720136642456, 'eval_runtime': 9.7363, 'eval_samples_per_second': 102.709, 'eval_steps_per_second': 6.471, 'epoch': 0.12}
{'loss': 1.2263, 'grad_norm': 0.20275422930717468, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3983803987503052, 'eval_runtime': 9.7667, 'eval_samples_per_second': 102.389, 'eval_steps_per_second': 6.451, 'epoch': 0.16}
{'loss': 1.1132, 'grad_norm': 0.2084166705608368, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3436733484268188, 'eval_runtime': 9.7838, 'eval_samples_per_second': 102.21, 'eval_steps_per_second': 6.439, 'epoch': 0.2}
{'loss': 1.1054, 'grad_norm': 0.18127182126045227, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3393476009368896, 'eval_runtime': 9.7849, 'eval_samples_per_second': 102.199, 'eval_steps_per_second': 6.439, 'epoch': 0.24}
{'loss': 1.0926, 'grad_norm': 0.19316934049129486, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3268767595291138, 'eval_runtime': 9.789, 'eval_samples_per_second': 102.155, 'eval_steps_per_second': 6.436, 'epoch': 0.28}
{'loss': 1.0494, 'grad_norm': 0.20436999201774597, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3244162797927856, 'eval_runtime': 9.7984, 'eval_samples_per_second': 102.058, 'eval_steps_per_second': 6.43, 'epoch': 0.32}
{'loss': 1.0793, 'grad_norm': 0.19138729572296143, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.314400315284729, 'eval_runtime': 9.8101, 'eval_samples_per_second': 101.936, 'eval_steps_per_second': 6.422, 'epoch': 0.36}
{'loss': 1.0257, 'grad_norm': 0.2307545393705368, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3115460872650146, 'eval_runtime': 9.8159, 'eval_samples_per_second': 101.876, 'eval_steps_per_second': 6.418, 'epoch': 0.4}
{'loss': 1.0281, 'grad_norm': 0.20972144603729248, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3136240243911743, 'eval_runtime': 9.8081, 'eval_samples_per_second': 101.956, 'eval_steps_per_second': 6.423, 'epoch': 0.44}
{'loss': 0.9787, 'grad_norm': 0.2503697872161865, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3099489212036133, 'eval_runtime': 9.8072, 'eval_samples_per_second': 101.966, 'eval_steps_per_second': 6.424, 'epoch': 0.48}
{'loss': 1.0236, 'grad_norm': 0.2577824294567108, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2991962432861328, 'eval_runtime': 9.8255, 'eval_samples_per_second': 101.776, 'eval_steps_per_second': 6.412, 'epoch': 0.52}
{'loss': 1.0061, 'grad_norm': 0.26087626814842224, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2939670085906982, 'eval_runtime': 9.8026, 'eval_samples_per_second': 102.014, 'eval_steps_per_second': 6.427, 'epoch': 0.56}
{'loss': 0.9649, 'grad_norm': 0.24374696612358093, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.299306869506836, 'eval_runtime': 9.8026, 'eval_samples_per_second': 102.014, 'eval_steps_per_second': 6.427, 'epoch': 0.6}
{'loss': 0.9746, 'grad_norm': 0.2249651551246643, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2903615236282349, 'eval_runtime': 9.8082, 'eval_samples_per_second': 101.956, 'eval_steps_per_second': 6.423, 'epoch': 0.64}
{'loss': 0.9615, 'grad_norm': 0.21081013977527618, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2900301218032837, 'eval_runtime': 9.7954, 'eval_samples_per_second': 102.089, 'eval_steps_per_second': 6.432, 'epoch': 0.68}
{'loss': 1.0171, 'grad_norm': 0.2236587405204773, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2893315553665161, 'eval_runtime': 9.8281, 'eval_samples_per_second': 101.749, 'eval_steps_per_second': 6.41, 'epoch': 0.72}
{'loss': 0.9581, 'grad_norm': 0.24530699849128723, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2860172986984253, 'eval_runtime': 9.8261, 'eval_samples_per_second': 101.77, 'eval_steps_per_second': 6.412, 'epoch': 0.76}
{'loss': 0.9317, 'grad_norm': 0.24586783349514008, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.286426067352295, 'eval_runtime': 9.9163, 'eval_samples_per_second': 100.845, 'eval_steps_per_second': 6.353, 'epoch': 0.8}
{'loss': 0.8522, 'grad_norm': 0.272936075925827, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.287264108657837, 'eval_runtime': 9.9326, 'eval_samples_per_second': 100.678, 'eval_steps_per_second': 6.343, 'epoch': 0.84}
{'loss': 0.8842, 'grad_norm': 0.2749636173248291, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.284942865371704, 'eval_runtime': 9.9355, 'eval_samples_per_second': 100.649, 'eval_steps_per_second': 6.341, 'epoch': 0.88}
{'loss': 0.8547, 'grad_norm': 0.29830801486968994, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2838369607925415, 'eval_runtime': 9.9322, 'eval_samples_per_second': 100.682, 'eval_steps_per_second': 6.343, 'epoch': 0.92}
{'loss': 0.9034, 'grad_norm': 0.28722572326660156, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2842190265655518, 'eval_runtime': 9.9228, 'eval_samples_per_second': 100.778, 'eval_steps_per_second': 6.349, 'epoch': 0.96}
{'loss': 0.8403, 'grad_norm': 0.38769423961639404, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2838715314865112, 'eval_runtime': 9.8976, 'eval_samples_per_second': 101.035, 'eval_steps_per_second': 6.365, 'epoch': 1.0}
{'train_runtime': 415.7161, 'train_samples_per_second': 24.038, 'train_steps_per_second': 1.503, 'train_loss': 1.1128074615478516, 'epoch': 1.0}
train_results:  {'eval_loss': [1.866782307624817, 1.5871025323867798, 1.482720136642456, 1.3983803987503052, 1.3436733484268188, 1.3393476009368896, 1.3268767595291138, 1.3244162797927856, 1.314400315284729, 1.3115460872650146, 1.3136240243911743, 1.3099489212036133, 1.2991962432861328, 1.2939670085906982, 1.299306869506836, 1.2903615236282349, 1.2900301218032837, 1.2893315553665161, 1.2860172986984253, 1.286426067352295, 1.287264108657837, 1.284942865371704, 1.2838369607925415, 1.2842190265655518, 1.2838715314865112], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.866782307624817, 1.5871025323867798, 1.482720136642456, 1.3983803987503052, 1.3436733484268188, 1.3393476009368896, 1.3268767595291138, 1.3244162797927856, 1.314400315284729, 1.3115460872650146, 1.3136240243911743, 1.3099489212036133, 1.2991962432861328, 1.2939670085906982, 1.299306869506836, 1.2903615236282349, 1.2900301218032837, 1.2893315553665161, 1.2860172986984253, 1.286426067352295, 1.287264108657837, 1.284942865371704, 1.2838369607925415, 1.2842190265655518, 1.2838715314865112]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2838715314865112
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.3352 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -0.9189758617616696
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -0.9185117872407195
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -0.9185117418845059
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -0.9282300447763698
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -0.9189284341594244
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0293, dtype=torch.float64), tensor(0.0917, dtype=torch.float64), tensor(0.1223, dtype=torch.float64), 0, tensor(0.0692, dtype=torch.float64), tensor(0.0280, dtype=torch.float64), tensor(0.2754, dtype=torch.float64), tensor(0.3731, dtype=torch.float64), 23, 0, 0, 1, 0, 1, 120, 0.005235151538746052, 34.28823135894563, 0]
normalized proposed parameters for next round by BO: [tensor(0.0042, dtype=torch.float64), tensor(0.0293, dtype=torch.float64), tensor(0.0917, dtype=torch.float64), tensor(0.1223, dtype=torch.float64), tensor(0.0067, dtype=torch.float64), tensor(0.0692, dtype=torch.float64), tensor(0.0280, dtype=torch.float64), tensor(0.2754, dtype=torch.float64), tensor(0.3731, dtype=torch.float64), tensor(0.7192, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9374, dtype=torch.float64), tensor(0.0524, dtype=torch.float64), tensor(0.7143, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.029
  rowan_hellaswag: 0.092
  sciq: 0.122
  triviaqa: 0
  truthfulqa_gen: 0.069
  wikitext: 0.028
  mmlu: 0.275
  arc_challenge: 0.373

LoRA Parameters:
  lora_r: (120,)
  lora_dropout: (0.005235151538746052,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (34.28823135894563,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  120
lora dropout:  0.005235151538746052
lora alpha:  34.28823135894563
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 101,744,640 || all params: 8,132,005,888 || trainable%: 1.2512
length of training data:  9887
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0204, 'grad_norm': 0.8216063976287842, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8393505811691284, 'eval_runtime': 9.3329, 'eval_samples_per_second': 107.147, 'eval_steps_per_second': 6.75, 'epoch': 0.04}
{'loss': 1.5753, 'grad_norm': 0.19995322823524475, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5551482439041138, 'eval_runtime': 9.3569, 'eval_samples_per_second': 106.873, 'eval_steps_per_second': 6.733, 'epoch': 0.08}
{'loss': 1.3586, 'grad_norm': 0.17506425082683563, 'learning_rate': 0.0002873239436619718, 'epoch': 0.12}
{'eval_loss': 1.4354586601257324, 'eval_runtime': 9.4037, 'eval_samples_per_second': 106.342, 'eval_steps_per_second': 6.7, 'epoch': 0.12}
{'loss': 1.3344, 'grad_norm': 0.17499354481697083, 'learning_rate': 0.0002741197183098591, 'epoch': 0.16}
{'eval_loss': 1.346065640449524, 'eval_runtime': 9.4163, 'eval_samples_per_second': 106.198, 'eval_steps_per_second': 6.69, 'epoch': 0.16}
{'loss': 1.1716, 'grad_norm': 0.18872295320034027, 'learning_rate': 0.0002609154929577464, 'epoch': 0.2}
{'eval_loss': 1.3001126050949097, 'eval_runtime': 9.4252, 'eval_samples_per_second': 106.099, 'eval_steps_per_second': 6.684, 'epoch': 0.2}
{'loss': 1.1491, 'grad_norm': 0.18073563277721405, 'learning_rate': 0.0002477112676056338, 'epoch': 0.24}
{'eval_loss': 1.2947986125946045, 'eval_runtime': 9.4606, 'eval_samples_per_second': 105.701, 'eval_steps_per_second': 6.659, 'epoch': 0.24}
{'loss': 1.1791, 'grad_norm': 0.20067952573299408, 'learning_rate': 0.00023450704225352109, 'epoch': 0.28}
{'eval_loss': 1.2919716835021973, 'eval_runtime': 9.4633, 'eval_samples_per_second': 105.672, 'eval_steps_per_second': 6.657, 'epoch': 0.28}
{'loss': 1.1509, 'grad_norm': 0.19273528456687927, 'learning_rate': 0.00022130281690140843, 'epoch': 0.32}
{'eval_loss': 1.2842756509780884, 'eval_runtime': 9.468, 'eval_samples_per_second': 105.619, 'eval_steps_per_second': 6.654, 'epoch': 0.32}
{'loss': 1.1491, 'grad_norm': 0.1860894113779068, 'learning_rate': 0.00020809859154929575, 'epoch': 0.36}
{'eval_loss': 1.2780195474624634, 'eval_runtime': 9.4673, 'eval_samples_per_second': 105.627, 'eval_steps_per_second': 6.654, 'epoch': 0.36}
{'loss': 1.0776, 'grad_norm': 0.16877757012844086, 'learning_rate': 0.00019489436619718307, 'epoch': 0.4}
{'eval_loss': 1.2796791791915894, 'eval_runtime': 9.468, 'eval_samples_per_second': 105.618, 'eval_steps_per_second': 6.654, 'epoch': 0.4}
{'loss': 1.1639, 'grad_norm': 0.18235625326633453, 'learning_rate': 0.0001816901408450704, 'epoch': 0.44}
{'eval_loss': 1.2724599838256836, 'eval_runtime': 9.4758, 'eval_samples_per_second': 105.532, 'eval_steps_per_second': 6.649, 'epoch': 0.44}
{'loss': 1.098, 'grad_norm': 0.23239856958389282, 'learning_rate': 0.0001684859154929577, 'epoch': 0.49}
{'eval_loss': 1.2715929746627808, 'eval_runtime': 9.4557, 'eval_samples_per_second': 105.756, 'eval_steps_per_second': 6.663, 'epoch': 0.49}
{'loss': 1.1132, 'grad_norm': 0.23266535997390747, 'learning_rate': 0.00015528169014084506, 'epoch': 0.53}
{'eval_loss': 1.2652605772018433, 'eval_runtime': 9.4453, 'eval_samples_per_second': 105.873, 'eval_steps_per_second': 6.67, 'epoch': 0.53}
{'loss': 1.029, 'grad_norm': 0.21163760125637054, 'learning_rate': 0.00014207746478873238, 'epoch': 0.57}
{'eval_loss': 1.2643210887908936, 'eval_runtime': 9.4501, 'eval_samples_per_second': 105.819, 'eval_steps_per_second': 6.667, 'epoch': 0.57}
{'loss': 0.9993, 'grad_norm': 0.2038116604089737, 'learning_rate': 0.0001288732394366197, 'epoch': 0.61}
{'eval_loss': 1.2627863883972168, 'eval_runtime': 9.4528, 'eval_samples_per_second': 105.788, 'eval_steps_per_second': 6.665, 'epoch': 0.61}
{'loss': 1.1031, 'grad_norm': 0.2373904436826706, 'learning_rate': 0.00011566901408450703, 'epoch': 0.65}
{'eval_loss': 1.259424090385437, 'eval_runtime': 9.4439, 'eval_samples_per_second': 105.888, 'eval_steps_per_second': 6.671, 'epoch': 0.65}
{'loss': 0.999, 'grad_norm': 0.2139984518289566, 'learning_rate': 0.00010246478873239435, 'epoch': 0.69}
{'eval_loss': 1.2582178115844727, 'eval_runtime': 9.44, 'eval_samples_per_second': 105.932, 'eval_steps_per_second': 6.674, 'epoch': 0.69}
{'loss': 1.0496, 'grad_norm': 0.25043201446533203, 'learning_rate': 8.926056338028169e-05, 'epoch': 0.73}
{'eval_loss': 1.2545289993286133, 'eval_runtime': 9.4228, 'eval_samples_per_second': 106.126, 'eval_steps_per_second': 6.686, 'epoch': 0.73}
{'loss': 1.0518, 'grad_norm': 0.21782617270946503, 'learning_rate': 7.6056338028169e-05, 'epoch': 0.77}
{'eval_loss': 1.2513315677642822, 'eval_runtime': 9.4315, 'eval_samples_per_second': 106.028, 'eval_steps_per_second': 6.68, 'epoch': 0.77}
{'loss': 1.052, 'grad_norm': 0.23622682690620422, 'learning_rate': 6.285211267605634e-05, 'epoch': 0.81}
{'eval_loss': 1.2489553689956665, 'eval_runtime': 9.4336, 'eval_samples_per_second': 106.004, 'eval_steps_per_second': 6.678, 'epoch': 0.81}
{'loss': 0.9494, 'grad_norm': 0.24595040082931519, 'learning_rate': 4.964788732394366e-05, 'epoch': 0.85}
{'eval_loss': 1.2503708600997925, 'eval_runtime': 9.4193, 'eval_samples_per_second': 106.165, 'eval_steps_per_second': 6.688, 'epoch': 0.85}
{'loss': 1.0112, 'grad_norm': 0.24968455731868744, 'learning_rate': 3.6443661971830985e-05, 'epoch': 0.89}
{'eval_loss': 1.2491776943206787, 'eval_runtime': 9.4403, 'eval_samples_per_second': 105.929, 'eval_steps_per_second': 6.674, 'epoch': 0.89}
{'loss': 0.9753, 'grad_norm': 0.27304428815841675, 'learning_rate': 2.3239436619718305e-05, 'epoch': 0.93}
{'eval_loss': 1.2484666109085083, 'eval_runtime': 9.4786, 'eval_samples_per_second': 105.501, 'eval_steps_per_second': 6.647, 'epoch': 0.93}
{'loss': 1.01, 'grad_norm': 0.2271573692560196, 'learning_rate': 1.0035211267605631e-05, 'epoch': 0.97}
{'eval_loss': 1.2493830919265747, 'eval_runtime': 9.4955, 'eval_samples_per_second': 105.313, 'eval_steps_per_second': 6.635, 'epoch': 0.97}
{'train_runtime': 403.4671, 'train_samples_per_second': 24.505, 'train_steps_per_second': 1.532, 'train_loss': 1.1901115275509535, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8393505811691284, 1.5551482439041138, 1.4354586601257324, 1.346065640449524, 1.3001126050949097, 1.2947986125946045, 1.2919716835021973, 1.2842756509780884, 1.2780195474624634, 1.2796791791915894, 1.2724599838256836, 1.2715929746627808, 1.2652605772018433, 1.2643210887908936, 1.2627863883972168, 1.259424090385437, 1.2582178115844727, 1.2545289993286133, 1.2513315677642822, 1.2489553689956665, 1.2503708600997925, 1.2491776943206787, 1.2484666109085083, 1.2493830919265747], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.8393505811691284, 1.5551482439041138, 1.4354586601257324, 1.346065640449524, 1.3001126050949097, 1.2947986125946045, 1.2919716835021973, 1.2842756509780884, 1.2780195474624634, 1.2796791791915894, 1.2724599838256836, 1.2715929746627808, 1.2652605772018433, 1.2643210887908936, 1.2627863883972168, 1.259424090385437, 1.2582178115844727, 1.2545289993286133, 1.2513315677642822, 1.2489553689956665, 1.2503708600997925, 1.2491776943206787, 1.2484666109085083, 1.2493830919265747]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2493830919265747
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8605 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.08295202255249023, 0.8953140377998352, 0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.36218807101249695, 0.6549691557884216, 0.8173236846923828, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3347404897212982, 0.355441689491272, 0.4367692470550537, 0.45111382007598877]  ‚Üí  acq = -0.9310905231474675
X = [0.35591208934783936, 0.098782479763031, 0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.36877870559692383, 0.8672244548797607, 0.5090312361717224, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9806126356124878, 0.5262150168418884, 0.617688775062561, 0.313107967376709]  ‚Üí  acq = -0.9310905230646958
X = [0.2322162389755249, 0.03715479373931885, 0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.40351834893226624, 0.9447525143623352, 0.40425533056259155, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9491793513298035, 0.21524584293365479, 0.22934073209762573, 0.15074169635772705]  ‚Üí  acq = -0.9310973287113964
X = [0.5550482869148254, 0.7731989026069641, 0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6225687265396118, 0.3806919455528259, 0.9421022534370422, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7150893807411194, 0.5437468886375427, 0.6237008571624756, 0.9093855619430542]  ‚Üí  acq = -0.9310905232656865
X = [0.7380771636962891, 0.9890440702438354, 0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.4263235926628113, 0.9630946516990662, 0.6111319065093994, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.834472119808197, 0.3034810423851013, 0.033137477934360504, 0.012760698795318604]  ‚Üí  acq = -0.9310906814256018
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2543, dtype=torch.float64), tensor(0.3776, dtype=torch.float64), tensor(0.0607, dtype=torch.float64), tensor(0.1560, dtype=torch.float64), 0, tensor(0.1143, dtype=torch.float64), tensor(0.0370, dtype=torch.float64), 0, 0, 24, 0, 0, 1, 0, 1, 51, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2543, dtype=torch.float64), tensor(0.3776, dtype=torch.float64), tensor(0.0607, dtype=torch.float64), tensor(0.1560, dtype=torch.float64), tensor(1.6790e-18, dtype=torch.float64), tensor(0.1143, dtype=torch.float64), tensor(0.0370, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.2665e-18, dtype=torch.float64), tensor(0.7556, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3964, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.254
  gsm8k: 0.378
  rowan_hellaswag: 0.061
  sciq: 0.156
  triviaqa: 0
  truthfulqa_gen: 0.114
  wikitext: 0.037
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (51,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  51
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 45,121,536 || all params: 8,075,382,784 || trainable%: 0.5588
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5268, 'grad_norm': 0.8223556876182556, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1508913040161133, 'eval_runtime': 9.6526, 'eval_samples_per_second': 103.599, 'eval_steps_per_second': 6.527, 'epoch': 0.04}
{'loss': 1.3857, 'grad_norm': 0.4054490923881531, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.148883819580078, 'eval_runtime': 9.6304, 'eval_samples_per_second': 103.838, 'eval_steps_per_second': 6.542, 'epoch': 0.08}
{'loss': 1.1303, 'grad_norm': 0.27764594554901123, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1417176723480225, 'eval_runtime': 9.7318, 'eval_samples_per_second': 102.755, 'eval_steps_per_second': 6.474, 'epoch': 0.12}
{'loss': 1.1614, 'grad_norm': 0.27871865034103394, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.075577735900879, 'eval_runtime': 9.7563, 'eval_samples_per_second': 102.498, 'eval_steps_per_second': 6.457, 'epoch': 0.16}
{'loss': 0.9954, 'grad_norm': 0.23115229606628418, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0680899620056152, 'eval_runtime': 9.7437, 'eval_samples_per_second': 102.63, 'eval_steps_per_second': 6.466, 'epoch': 0.2}
{'loss': 1.0082, 'grad_norm': 0.23680426180362701, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.0608396530151367, 'eval_runtime': 9.7614, 'eval_samples_per_second': 102.445, 'eval_steps_per_second': 6.454, 'epoch': 0.24}
{'loss': 0.9768, 'grad_norm': 0.24869893491268158, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.0660226345062256, 'eval_runtime': 9.7332, 'eval_samples_per_second': 102.741, 'eval_steps_per_second': 6.473, 'epoch': 0.28}
{'loss': 0.9445, 'grad_norm': 0.25647273659706116, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.015815019607544, 'eval_runtime': 9.729, 'eval_samples_per_second': 102.785, 'eval_steps_per_second': 6.475, 'epoch': 0.32}
{'loss': 1.0019, 'grad_norm': 0.26661235094070435, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.099622964859009, 'eval_runtime': 9.7215, 'eval_samples_per_second': 102.865, 'eval_steps_per_second': 6.481, 'epoch': 0.36}
{'loss': 0.9346, 'grad_norm': 0.2126971036195755, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.1001980304718018, 'eval_runtime': 9.7225, 'eval_samples_per_second': 102.854, 'eval_steps_per_second': 6.48, 'epoch': 0.4}
{'loss': 0.9895, 'grad_norm': 0.313388466835022, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0867557525634766, 'eval_runtime': 9.7274, 'eval_samples_per_second': 102.803, 'eval_steps_per_second': 6.477, 'epoch': 0.44}
{'loss': 0.9018, 'grad_norm': 0.255446195602417, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.073399305343628, 'eval_runtime': 9.7238, 'eval_samples_per_second': 102.84, 'eval_steps_per_second': 6.479, 'epoch': 0.48}
{'loss': 0.945, 'grad_norm': 0.2512470483779907, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.1031386852264404, 'eval_runtime': 9.7291, 'eval_samples_per_second': 102.784, 'eval_steps_per_second': 6.475, 'epoch': 0.52}
{'loss': 0.9755, 'grad_norm': 0.2417960911989212, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.07637882232666, 'eval_runtime': 9.7291, 'eval_samples_per_second': 102.785, 'eval_steps_per_second': 6.475, 'epoch': 0.56}
{'loss': 0.9043, 'grad_norm': 0.25538867712020874, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.10863995552063, 'eval_runtime': 9.7304, 'eval_samples_per_second': 102.77, 'eval_steps_per_second': 6.475, 'epoch': 0.6}
{'loss': 0.9272, 'grad_norm': 0.3079928159713745, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.101670026779175, 'eval_runtime': 9.7328, 'eval_samples_per_second': 102.746, 'eval_steps_per_second': 6.473, 'epoch': 0.64}
{'loss': 0.9603, 'grad_norm': 0.25192826986312866, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.138068437576294, 'eval_runtime': 9.7302, 'eval_samples_per_second': 102.773, 'eval_steps_per_second': 6.475, 'epoch': 0.68}
{'loss': 0.921, 'grad_norm': 0.2711765468120575, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.130178928375244, 'eval_runtime': 9.7183, 'eval_samples_per_second': 102.898, 'eval_steps_per_second': 6.483, 'epoch': 0.72}
{'loss': 0.8826, 'grad_norm': 0.29258665442466736, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.1327803134918213, 'eval_runtime': 9.7278, 'eval_samples_per_second': 102.798, 'eval_steps_per_second': 6.476, 'epoch': 0.76}
{'loss': 0.9799, 'grad_norm': 0.3031678795814514, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.110715866088867, 'eval_runtime': 9.7251, 'eval_samples_per_second': 102.827, 'eval_steps_per_second': 6.478, 'epoch': 0.8}
{'loss': 0.9652, 'grad_norm': 0.29517877101898193, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.10430908203125, 'eval_runtime': 9.7327, 'eval_samples_per_second': 102.746, 'eval_steps_per_second': 6.473, 'epoch': 0.84}
{'loss': 0.9894, 'grad_norm': 0.3208741545677185, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.1017558574676514, 'eval_runtime': 9.7192, 'eval_samples_per_second': 102.89, 'eval_steps_per_second': 6.482, 'epoch': 0.88}
{'loss': 0.9347, 'grad_norm': 0.3350062072277069, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1356112957000732, 'eval_runtime': 9.7319, 'eval_samples_per_second': 102.755, 'eval_steps_per_second': 6.474, 'epoch': 0.92}
{'loss': 0.8912, 'grad_norm': 0.27309152483940125, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1435039043426514, 'eval_runtime': 9.7406, 'eval_samples_per_second': 102.664, 'eval_steps_per_second': 6.468, 'epoch': 0.96}
{'loss': 0.9014, 'grad_norm': 0.31647324562072754, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.1461219787597656, 'eval_runtime': 9.7492, 'eval_samples_per_second': 102.573, 'eval_steps_per_second': 6.462, 'epoch': 1.0}
{'train_runtime': 436.8054, 'train_samples_per_second': 22.889, 'train_steps_per_second': 1.431, 'train_loss': 1.0453909606933594, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1508913040161133, 2.148883819580078, 2.1417176723480225, 2.075577735900879, 2.0680899620056152, 2.0608396530151367, 2.0660226345062256, 2.015815019607544, 2.099622964859009, 2.1001980304718018, 2.0867557525634766, 2.073399305343628, 2.1031386852264404, 2.07637882232666, 2.10863995552063, 2.101670026779175, 2.138068437576294, 2.130178928375244, 2.1327803134918213, 2.110715866088867, 2.10430908203125, 2.1017558574676514, 2.1356112957000732, 2.1435039043426514, 2.1461219787597656], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1508913040161133, 2.148883819580078, 2.1417176723480225, 2.075577735900879, 2.0680899620056152, 2.0608396530151367, 2.0660226345062256, 2.015815019607544, 2.099622964859009, 2.1001980304718018, 2.0867557525634766, 2.073399305343628, 2.1031386852264404, 2.07637882232666, 2.10863995552063, 2.101670026779175, 2.138068437576294, 2.130178928375244, 2.1327803134918213, 2.110715866088867, 2.10430908203125, 2.1017558574676514, 2.1356112957000732, 2.1435039043426514, 2.1461219787597656]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.1461219787597656
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.5225 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -0.9228556243314852
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -0.9228792886589909
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -0.9228316454749701
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -0.9228165107833719
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -0.9211582922506368
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1276, dtype=torch.float64), tensor(0.1345, dtype=torch.float64), tensor(0.0716, dtype=torch.float64), tensor(0.0519, dtype=torch.float64), tensor(0.0983, dtype=torch.float64), tensor(0.0656, dtype=torch.float64), tensor(0.0106, dtype=torch.float64), tensor(0.2811, dtype=torch.float64), tensor(0.1587, dtype=torch.float64), 26, 1, 0, 1, 0, 1, 83, 0.06440237816148787, 29.413398852538755, 0]
normalized proposed parameters for next round by BO: [tensor(0.1276, dtype=torch.float64), tensor(0.1345, dtype=torch.float64), tensor(0.0716, dtype=torch.float64), tensor(0.0519, dtype=torch.float64), tensor(0.0983, dtype=torch.float64), tensor(0.0656, dtype=torch.float64), tensor(0.0106, dtype=torch.float64), tensor(0.2811, dtype=torch.float64), tensor(0.1587, dtype=torch.float64), tensor(0.8165, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6465, dtype=torch.float64), tensor(0.6440, dtype=torch.float64), tensor(0.6128, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.128
  gsm8k: 0.134
  rowan_hellaswag: 0.072
  sciq: 0.052
  triviaqa: 0.098
  truthfulqa_gen: 0.066
  wikitext: 0.011
  mmlu: 0.281
  arc_challenge: 0.159

LoRA Parameters:
  lora_r: (83,)
  lora_dropout: (0.06440237816148787,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (29.413398852538755,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  83
lora dropout:  0.06440237816148787
lora alpha:  29.413398852538755
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 97,230,848 || all params: 8,127,492,096 || trainable%: 1.1963
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9923, 'grad_norm': 0.39331957697868347, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8415849208831787, 'eval_runtime': 10.2222, 'eval_samples_per_second': 97.826, 'eval_steps_per_second': 6.163, 'epoch': 0.04}
{'loss': 1.5947, 'grad_norm': 0.25623899698257446, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5318526029586792, 'eval_runtime': 10.2416, 'eval_samples_per_second': 97.641, 'eval_steps_per_second': 6.151, 'epoch': 0.08}
{'loss': 1.3744, 'grad_norm': 0.2456689178943634, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4308338165283203, 'eval_runtime': 10.2414, 'eval_samples_per_second': 97.643, 'eval_steps_per_second': 6.152, 'epoch': 0.12}
{'loss': 1.2734, 'grad_norm': 0.19427859783172607, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3507771492004395, 'eval_runtime': 10.2682, 'eval_samples_per_second': 97.388, 'eval_steps_per_second': 6.135, 'epoch': 0.16}
{'loss': 1.1749, 'grad_norm': 0.18160241842269897, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.296195149421692, 'eval_runtime': 10.3728, 'eval_samples_per_second': 96.406, 'eval_steps_per_second': 6.074, 'epoch': 0.2}
{'loss': 1.1762, 'grad_norm': 0.1622883826494217, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2753803730010986, 'eval_runtime': 10.3786, 'eval_samples_per_second': 96.352, 'eval_steps_per_second': 6.07, 'epoch': 0.24}
{'loss': 1.0884, 'grad_norm': 0.16224689781665802, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2728397846221924, 'eval_runtime': 10.3517, 'eval_samples_per_second': 96.603, 'eval_steps_per_second': 6.086, 'epoch': 0.28}
{'loss': 1.1282, 'grad_norm': 0.15003754198551178, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2712844610214233, 'eval_runtime': 10.3307, 'eval_samples_per_second': 96.799, 'eval_steps_per_second': 6.098, 'epoch': 0.32}
{'loss': 1.1363, 'grad_norm': 0.20915783941745758, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2649701833724976, 'eval_runtime': 10.3442, 'eval_samples_per_second': 96.672, 'eval_steps_per_second': 6.09, 'epoch': 0.36}
{'loss': 1.1621, 'grad_norm': 0.1763104349374771, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2633219957351685, 'eval_runtime': 10.3455, 'eval_samples_per_second': 96.66, 'eval_steps_per_second': 6.09, 'epoch': 0.4}
{'loss': 1.1786, 'grad_norm': 0.19078640639781952, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.261447787284851, 'eval_runtime': 10.3338, 'eval_samples_per_second': 96.77, 'eval_steps_per_second': 6.097, 'epoch': 0.44}
{'loss': 1.1259, 'grad_norm': 0.19642606377601624, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.260581612586975, 'eval_runtime': 10.3153, 'eval_samples_per_second': 96.944, 'eval_steps_per_second': 6.107, 'epoch': 0.48}
{'loss': 1.0844, 'grad_norm': 0.1681094467639923, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2585715055465698, 'eval_runtime': 10.3275, 'eval_samples_per_second': 96.829, 'eval_steps_per_second': 6.1, 'epoch': 0.52}
{'loss': 1.1378, 'grad_norm': 0.17503051459789276, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.253127932548523, 'eval_runtime': 10.3383, 'eval_samples_per_second': 96.728, 'eval_steps_per_second': 6.094, 'epoch': 0.56}
{'loss': 1.0593, 'grad_norm': 0.1697927713394165, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2510854005813599, 'eval_runtime': 10.3422, 'eval_samples_per_second': 96.691, 'eval_steps_per_second': 6.092, 'epoch': 0.6}
{'loss': 1.1188, 'grad_norm': 0.20844383537769318, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2492696046829224, 'eval_runtime': 10.358, 'eval_samples_per_second': 96.544, 'eval_steps_per_second': 6.082, 'epoch': 0.64}
{'loss': 1.0609, 'grad_norm': 0.21733711659908295, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2477195262908936, 'eval_runtime': 10.3548, 'eval_samples_per_second': 96.574, 'eval_steps_per_second': 6.084, 'epoch': 0.68}
{'loss': 1.1209, 'grad_norm': 0.19180867075920105, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2467113733291626, 'eval_runtime': 10.331, 'eval_samples_per_second': 96.796, 'eval_steps_per_second': 6.098, 'epoch': 0.72}
{'loss': 1.0656, 'grad_norm': 0.20768991112709045, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2428686618804932, 'eval_runtime': 10.2813, 'eval_samples_per_second': 97.264, 'eval_steps_per_second': 6.128, 'epoch': 0.76}
{'loss': 1.1048, 'grad_norm': 0.16957320272922516, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2416081428527832, 'eval_runtime': 10.2712, 'eval_samples_per_second': 97.359, 'eval_steps_per_second': 6.134, 'epoch': 0.8}
{'loss': 1.1061, 'grad_norm': 0.21260207891464233, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2403764724731445, 'eval_runtime': 10.2576, 'eval_samples_per_second': 97.488, 'eval_steps_per_second': 6.142, 'epoch': 0.84}
{'loss': 1.0542, 'grad_norm': 0.17549948394298553, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2394696474075317, 'eval_runtime': 10.2574, 'eval_samples_per_second': 97.49, 'eval_steps_per_second': 6.142, 'epoch': 0.88}
{'loss': 1.0535, 'grad_norm': 0.424251526594162, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.23819100856781, 'eval_runtime': 10.2502, 'eval_samples_per_second': 97.559, 'eval_steps_per_second': 6.146, 'epoch': 0.92}
{'loss': 1.0544, 'grad_norm': 0.1968047022819519, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2345390319824219, 'eval_runtime': 10.253, 'eval_samples_per_second': 97.532, 'eval_steps_per_second': 6.145, 'epoch': 0.96}
{'loss': 1.0798, 'grad_norm': 0.20698370039463043, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2344038486480713, 'eval_runtime': 10.2712, 'eval_samples_per_second': 97.36, 'eval_steps_per_second': 6.134, 'epoch': 1.0}
{'train_runtime': 473.4823, 'train_samples_per_second': 21.112, 'train_steps_per_second': 1.32, 'train_loss': 1.2202370086669923, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8415849208831787, 1.5318526029586792, 1.4308338165283203, 1.3507771492004395, 1.296195149421692, 1.2753803730010986, 1.2728397846221924, 1.2712844610214233, 1.2649701833724976, 1.2633219957351685, 1.261447787284851, 1.260581612586975, 1.2585715055465698, 1.253127932548523, 1.2510854005813599, 1.2492696046829224, 1.2477195262908936, 1.2467113733291626, 1.2428686618804932, 1.2416081428527832, 1.2403764724731445, 1.2394696474075317, 1.23819100856781, 1.2345390319824219, 1.2344038486480713], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8415849208831787, 1.5318526029586792, 1.4308338165283203, 1.3507771492004395, 1.296195149421692, 1.2753803730010986, 1.2728397846221924, 1.2712844610214233, 1.2649701833724976, 1.2633219957351685, 1.261447787284851, 1.260581612586975, 1.2585715055465698, 1.253127932548523, 1.2510854005813599, 1.2492696046829224, 1.2477195262908936, 1.2467113733291626, 1.2428686618804932, 1.2416081428527832, 1.2403764724731445, 1.2394696474075317, 1.23819100856781, 1.2345390319824219, 1.2344038486480713]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2344038486480713
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.3091 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -0.9202748713473785
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -0.9214387714046832
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -0.9270824134038298
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -0.921438771402695
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -0.9227605109368215
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0157, dtype=torch.float64), 0, tensor(0.0201, dtype=torch.float64), tensor(0.2055, dtype=torch.float64), tensor(0.0354, dtype=torch.float64), tensor(0.0825, dtype=torch.float64), 0, tensor(0.2698, dtype=torch.float64), tensor(0.3678, dtype=torch.float64), 14, 1, 0, 1, 1, 0, 128, 0.0047183635333877885, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0157, dtype=torch.float64), tensor(0.0032, dtype=torch.float64), tensor(0.0201, dtype=torch.float64), tensor(0.2055, dtype=torch.float64), tensor(0.0354, dtype=torch.float64), tensor(0.0825, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2698, dtype=torch.float64), tensor(0.3678, dtype=torch.float64), tensor(0.4239, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0472, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.016
  gsm8k: 0
  rowan_hellaswag: 0.02
  sciq: 0.205
  triviaqa: 0.035
  truthfulqa_gen: 0.083
  wikitext: 0
  mmlu: 0.27
  arc_challenge: 0.368

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0047183635333877885,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.0047183635333877885
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 80,740,352 || all params: 8,111,001,600 || trainable%: 0.9954
length of training data:  9964
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0655, 'grad_norm': 0.8198384642601013, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7913650274276733, 'eval_runtime': 9.1564, 'eval_samples_per_second': 109.213, 'eval_steps_per_second': 6.88, 'epoch': 0.04}
{'loss': 1.3846, 'grad_norm': 0.2890970706939697, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4769761562347412, 'eval_runtime': 9.2068, 'eval_samples_per_second': 108.615, 'eval_steps_per_second': 6.843, 'epoch': 0.08}
{'loss': 1.2077, 'grad_norm': 0.22059395909309387, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.410589337348938, 'eval_runtime': 9.2545, 'eval_samples_per_second': 108.056, 'eval_steps_per_second': 6.808, 'epoch': 0.12}
{'loss': 1.0687, 'grad_norm': 0.22170691192150116, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.3818286657333374, 'eval_runtime': 9.2597, 'eval_samples_per_second': 107.994, 'eval_steps_per_second': 6.804, 'epoch': 0.16}
{'loss': 1.0681, 'grad_norm': 0.204787477850914, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.3553098440170288, 'eval_runtime': 9.2827, 'eval_samples_per_second': 107.727, 'eval_steps_per_second': 6.787, 'epoch': 0.2}
{'loss': 1.0642, 'grad_norm': 0.20516060292720795, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.3445632457733154, 'eval_runtime': 9.3017, 'eval_samples_per_second': 107.508, 'eval_steps_per_second': 6.773, 'epoch': 0.24}
{'loss': 1.0755, 'grad_norm': 0.19084003567695618, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.3380656242370605, 'eval_runtime': 9.3205, 'eval_samples_per_second': 107.29, 'eval_steps_per_second': 6.759, 'epoch': 0.28}
{'loss': 1.0622, 'grad_norm': 0.22790683805942535, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.3353781700134277, 'eval_runtime': 9.3339, 'eval_samples_per_second': 107.136, 'eval_steps_per_second': 6.75, 'epoch': 0.32}
{'loss': 1.1082, 'grad_norm': 0.21082885563373566, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.3267403841018677, 'eval_runtime': 9.3573, 'eval_samples_per_second': 106.869, 'eval_steps_per_second': 6.733, 'epoch': 0.36}
{'loss': 1.0183, 'grad_norm': 0.2577016055583954, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.3304294347763062, 'eval_runtime': 9.3964, 'eval_samples_per_second': 106.423, 'eval_steps_per_second': 6.705, 'epoch': 0.4}
{'loss': 1.0529, 'grad_norm': 0.21739062666893005, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.320367693901062, 'eval_runtime': 9.4316, 'eval_samples_per_second': 106.027, 'eval_steps_per_second': 6.68, 'epoch': 0.44}
{'loss': 0.9836, 'grad_norm': 0.2373998612165451, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.3178805112838745, 'eval_runtime': 9.4268, 'eval_samples_per_second': 106.08, 'eval_steps_per_second': 6.683, 'epoch': 0.48}
{'loss': 1.0185, 'grad_norm': 0.23395629227161407, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.3176889419555664, 'eval_runtime': 9.4299, 'eval_samples_per_second': 106.045, 'eval_steps_per_second': 6.681, 'epoch': 0.52}
{'loss': 0.997, 'grad_norm': 0.2551223337650299, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.317056655883789, 'eval_runtime': 9.4389, 'eval_samples_per_second': 105.944, 'eval_steps_per_second': 6.674, 'epoch': 0.56}
{'loss': 1.0025, 'grad_norm': 0.23612256348133087, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.3156262636184692, 'eval_runtime': 9.4472, 'eval_samples_per_second': 105.851, 'eval_steps_per_second': 6.669, 'epoch': 0.6}
{'loss': 1.0207, 'grad_norm': 0.27159756422042847, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 1.3139475584030151, 'eval_runtime': 9.4364, 'eval_samples_per_second': 105.972, 'eval_steps_per_second': 6.676, 'epoch': 0.64}
{'loss': 0.9594, 'grad_norm': 0.2575465142726898, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 1.3106611967086792, 'eval_runtime': 9.4648, 'eval_samples_per_second': 105.655, 'eval_steps_per_second': 6.656, 'epoch': 0.68}
{'loss': 0.9527, 'grad_norm': 0.2890640199184418, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 1.3098671436309814, 'eval_runtime': 9.4726, 'eval_samples_per_second': 105.568, 'eval_steps_per_second': 6.651, 'epoch': 0.72}
{'loss': 0.9492, 'grad_norm': 0.3080586791038513, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 1.3079479932785034, 'eval_runtime': 9.4311, 'eval_samples_per_second': 106.033, 'eval_steps_per_second': 6.68, 'epoch': 0.76}
{'loss': 0.9673, 'grad_norm': 0.285508930683136, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 1.3086861371994019, 'eval_runtime': 9.4425, 'eval_samples_per_second': 105.905, 'eval_steps_per_second': 6.672, 'epoch': 0.8}
{'loss': 0.9251, 'grad_norm': 0.3518957197666168, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 1.3055124282836914, 'eval_runtime': 9.433, 'eval_samples_per_second': 106.011, 'eval_steps_per_second': 6.679, 'epoch': 0.84}
{'loss': 0.9884, 'grad_norm': 0.2806011736392975, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 1.3041279315948486, 'eval_runtime': 9.4412, 'eval_samples_per_second': 105.919, 'eval_steps_per_second': 6.673, 'epoch': 0.88}
{'loss': 0.9372, 'grad_norm': 0.30465489625930786, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 1.3025410175323486, 'eval_runtime': 9.4254, 'eval_samples_per_second': 106.096, 'eval_steps_per_second': 6.684, 'epoch': 0.92}
{'loss': 0.8845, 'grad_norm': 0.29218751192092896, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 1.3026082515716553, 'eval_runtime': 9.4323, 'eval_samples_per_second': 106.018, 'eval_steps_per_second': 6.679, 'epoch': 0.96}
{'train_runtime': 363.5998, 'train_samples_per_second': 27.404, 'train_steps_per_second': 1.713, 'train_loss': 1.1089966438746568, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7913650274276733, 1.4769761562347412, 1.410589337348938, 1.3818286657333374, 1.3553098440170288, 1.3445632457733154, 1.3380656242370605, 1.3353781700134277, 1.3267403841018677, 1.3304294347763062, 1.320367693901062, 1.3178805112838745, 1.3176889419555664, 1.317056655883789, 1.3156262636184692, 1.3139475584030151, 1.3106611967086792, 1.3098671436309814, 1.3079479932785034, 1.3086861371994019, 1.3055124282836914, 1.3041279315948486, 1.3025410175323486, 1.3026082515716553], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.7913650274276733, 1.4769761562347412, 1.410589337348938, 1.3818286657333374, 1.3553098440170288, 1.3445632457733154, 1.3380656242370605, 1.3353781700134277, 1.3267403841018677, 1.3304294347763062, 1.320367693901062, 1.3178805112838745, 1.3176889419555664, 1.317056655883789, 1.3156262636184692, 1.3139475584030151, 1.3106611967086792, 1.3098671436309814, 1.3079479932785034, 1.3086861371994019, 1.3055124282836914, 1.3041279315948486, 1.3025410175323486, 1.3026082515716553]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3026082515716553
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8541 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2780535817146301, 0.5095536708831787, 0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8690414428710938, 0.7770436406135559, 0.3981754779815674, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5308693051338196, 0.46824127435684204, 0.11125880479812622, 0.5334663987159729]  ‚Üí  acq = -0.9324951587649126
X = [0.13693922758102417, 0.3389108180999756, 0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.29940420389175415, 0.7294906973838806, 0.6888439059257507, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.24545004963874817, 0.027868032455444336, 0.553374171257019, 0.30727219581604004]  ‚Üí  acq = -0.9324951587649126
X = [0.5848507881164551, 0.3444458246231079, 0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9495751857757568, 0.8749518990516663, 0.5119346976280212, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9309940338134766, 0.7199150323867798, 0.07934227585792542, 0.6311293244361877]  ‚Üí  acq = -0.9324951587649133
X = [0.559325098991394, 0.008942186832427979, 0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7127609848976135, 0.6300967335700989, 0.4754225015640259, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7242633700370789, 0.3136094808578491, 0.8101937770843506, 0.6237255334854126]  ‚Üí  acq = -0.9327726174316829
X = [0.8355806469917297, 0.3124581575393677, 0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.39794930815696716, 0.06517422199249268, 0.27738410234451294, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7913540005683899, 0.0963255763053894, 0.29601335525512695, 0.6288021206855774]  ‚Üí  acq = -0.9354385669849381
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2273, dtype=torch.float64), 0, tensor(0.0352, dtype=torch.float64), tensor(0.2193, dtype=torch.float64), tensor(0.1175, dtype=torch.float64), tensor(0.2736, dtype=torch.float64), tensor(0.0639, dtype=torch.float64), 0, tensor(0.0533, dtype=torch.float64), 18, 0, 1, 1, 1, 1, 98, 3.36827138036549e-20, 25.926700957732898, 1]
normalized proposed parameters for next round by BO: [tensor(0.2273, dtype=torch.float64), tensor(0.0094, dtype=torch.float64), tensor(0.0352, dtype=torch.float64), tensor(0.2193, dtype=torch.float64), tensor(0.1175, dtype=torch.float64), tensor(0.2736, dtype=torch.float64), tensor(0.0639, dtype=torch.float64), tensor(0.0006, dtype=torch.float64), tensor(0.0533, dtype=torch.float64), tensor(0.5504, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7625, dtype=torch.float64), tensor(3.3683e-19, dtype=torch.float64), tensor(0.5401, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.227
  gsm8k: 0
  rowan_hellaswag: 0.035
  sciq: 0.219
  triviaqa: 0.118
  truthfulqa_gen: 0.274
  wikitext: 0.064
  mmlu: 0
  arc_challenge: 0.053

LoRA Parameters:
  lora_r: (98,)
  lora_dropout: (3.36827138036549e-20,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (25.926700957732898,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  98
lora dropout:  3.36827138036549e-20
lora alpha:  25.926700957732898
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 106,573,824 || all params: 8,136,835,072 || trainable%: 1.3098
length of training data:  9896
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4586, 'grad_norm': 1.0754830837249756, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9924579858779907, 'eval_runtime': 9.9074, 'eval_samples_per_second': 100.935, 'eval_steps_per_second': 6.359, 'epoch': 0.04}
{'loss': 1.4434, 'grad_norm': 0.839638352394104, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7566043138504028, 'eval_runtime': 9.8541, 'eval_samples_per_second': 101.481, 'eval_steps_per_second': 6.393, 'epoch': 0.08}
{'loss': 1.1972, 'grad_norm': 0.5123037099838257, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 1.7666041851043701, 'eval_runtime': 9.8414, 'eval_samples_per_second': 101.611, 'eval_steps_per_second': 6.402, 'epoch': 0.12}
{'loss': 1.1374, 'grad_norm': 0.32038411498069763, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 1.7563318014144897, 'eval_runtime': 9.8533, 'eval_samples_per_second': 101.489, 'eval_steps_per_second': 6.394, 'epoch': 0.16}
{'loss': 1.0939, 'grad_norm': 0.498383104801178, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 1.747294306755066, 'eval_runtime': 9.8695, 'eval_samples_per_second': 101.322, 'eval_steps_per_second': 6.383, 'epoch': 0.2}
{'loss': 1.0772, 'grad_norm': 0.30954188108444214, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 1.6741135120391846, 'eval_runtime': 9.8766, 'eval_samples_per_second': 101.249, 'eval_steps_per_second': 6.379, 'epoch': 0.24}
{'loss': 0.9871, 'grad_norm': 0.3401954472064972, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 1.7117127180099487, 'eval_runtime': 9.889, 'eval_samples_per_second': 101.122, 'eval_steps_per_second': 6.371, 'epoch': 0.28}
{'loss': 0.9627, 'grad_norm': 0.2796475291252136, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 1.7437443733215332, 'eval_runtime': 9.8841, 'eval_samples_per_second': 101.173, 'eval_steps_per_second': 6.374, 'epoch': 0.32}
{'loss': 0.9996, 'grad_norm': 0.25843581557273865, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 1.7341492176055908, 'eval_runtime': 9.903, 'eval_samples_per_second': 100.979, 'eval_steps_per_second': 6.362, 'epoch': 0.36}
{'loss': 0.9637, 'grad_norm': 0.25418612360954285, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 1.7959308624267578, 'eval_runtime': 9.9109, 'eval_samples_per_second': 100.899, 'eval_steps_per_second': 6.357, 'epoch': 0.4}
{'loss': 0.9222, 'grad_norm': 0.3662818968296051, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 1.7714391946792603, 'eval_runtime': 9.9156, 'eval_samples_per_second': 100.851, 'eval_steps_per_second': 6.354, 'epoch': 0.44}
{'loss': 0.9763, 'grad_norm': 0.2635687291622162, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 1.791364312171936, 'eval_runtime': 9.8955, 'eval_samples_per_second': 101.056, 'eval_steps_per_second': 6.367, 'epoch': 0.48}
{'loss': 0.9622, 'grad_norm': 0.21047443151474, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 1.8031253814697266, 'eval_runtime': 9.9084, 'eval_samples_per_second': 100.924, 'eval_steps_per_second': 6.358, 'epoch': 0.53}
{'loss': 0.9709, 'grad_norm': 0.28060242533683777, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 1.7674965858459473, 'eval_runtime': 9.8992, 'eval_samples_per_second': 101.018, 'eval_steps_per_second': 6.364, 'epoch': 0.57}
{'loss': 0.9506, 'grad_norm': 0.4040595293045044, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 1.7633142471313477, 'eval_runtime': 9.9091, 'eval_samples_per_second': 100.917, 'eval_steps_per_second': 6.358, 'epoch': 0.61}
{'loss': 1.0169, 'grad_norm': 0.3753279447555542, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 1.7501262426376343, 'eval_runtime': 9.9127, 'eval_samples_per_second': 100.881, 'eval_steps_per_second': 6.355, 'epoch': 0.65}
{'loss': 0.9785, 'grad_norm': 0.31310194730758667, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 1.7651855945587158, 'eval_runtime': 9.9021, 'eval_samples_per_second': 100.988, 'eval_steps_per_second': 6.362, 'epoch': 0.69}
{'loss': 0.9704, 'grad_norm': 0.3280108869075775, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 1.7632299661636353, 'eval_runtime': 9.9066, 'eval_samples_per_second': 100.942, 'eval_steps_per_second': 6.359, 'epoch': 0.73}
{'loss': 0.9471, 'grad_norm': 0.2909168004989624, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 1.7640233039855957, 'eval_runtime': 9.8924, 'eval_samples_per_second': 101.087, 'eval_steps_per_second': 6.368, 'epoch': 0.77}
{'loss': 0.9128, 'grad_norm': 0.2586263418197632, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 1.7655845880508423, 'eval_runtime': 9.8826, 'eval_samples_per_second': 101.188, 'eval_steps_per_second': 6.375, 'epoch': 0.81}
{'loss': 0.8923, 'grad_norm': 0.35443010926246643, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 1.7783925533294678, 'eval_runtime': 9.9521, 'eval_samples_per_second': 100.481, 'eval_steps_per_second': 6.33, 'epoch': 0.85}
{'loss': 0.9813, 'grad_norm': 0.6227446794509888, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 1.7925151586532593, 'eval_runtime': 9.9673, 'eval_samples_per_second': 100.328, 'eval_steps_per_second': 6.321, 'epoch': 0.89}
{'loss': 0.9423, 'grad_norm': 0.308454304933548, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 1.7948611974716187, 'eval_runtime': 10.0108, 'eval_samples_per_second': 99.892, 'eval_steps_per_second': 6.293, 'epoch': 0.93}
{'loss': 0.8848, 'grad_norm': 0.30600348114967346, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 1.7985069751739502, 'eval_runtime': 9.9891, 'eval_samples_per_second': 100.109, 'eval_steps_per_second': 6.307, 'epoch': 0.97}
{'train_runtime': 407.4837, 'train_samples_per_second': 24.286, 'train_steps_per_second': 1.519, 'train_loss': 1.1051789685867137, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9924579858779907, 1.7566043138504028, 1.7666041851043701, 1.7563318014144897, 1.747294306755066, 1.6741135120391846, 1.7117127180099487, 1.7437443733215332, 1.7341492176055908, 1.7959308624267578, 1.7714391946792603, 1.791364312171936, 1.8031253814697266, 1.7674965858459473, 1.7633142471313477, 1.7501262426376343, 1.7651855945587158, 1.7632299661636353, 1.7640233039855957, 1.7655845880508423, 1.7783925533294678, 1.7925151586532593, 1.7948611974716187, 1.7985069751739502], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9924579858779907, 1.7566043138504028, 1.7666041851043701, 1.7563318014144897, 1.747294306755066, 1.6741135120391846, 1.7117127180099487, 1.7437443733215332, 1.7341492176055908, 1.7959308624267578, 1.7714391946792603, 1.791364312171936, 1.8031253814697266, 1.7674965858459473, 1.7633142471313477, 1.7501262426376343, 1.7651855945587158, 1.7632299661636353, 1.7640233039855957, 1.7655845880508423, 1.7783925533294678, 1.7925151586532593, 1.7948611974716187, 1.7985069751739502]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.7985069751739502
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 20.1269 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.01858508586883545, 0.27087509632110596, 0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3516203463077545, 0.6742079257965088, 0.8425379991531372, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.3618133068084717, 0.971827507019043, 0.9077439308166504, 0.298198401927948]  ‚Üí  acq = -0.933931556626728
X = [0.8427011370658875, 0.498738169670105, 0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.949406087398529, 0.2025597095489502, 0.1727500557899475, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6172329187393188, 0.7515861392021179, 0.5611653327941895, 0.20585447549819946]  ‚Üí  acq = -0.9339290159131142
X = [0.2902684807777405, 0.8506918549537659, 0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.44211840629577637, 0.9807340502738953, 0.05173856019973755, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5744302272796631, 0.3193025588989258, 0.46983620524406433, 0.4887549877166748]  ‚Üí  acq = -0.9339290159131146
X = [0.4835529327392578, 0.9963791966438293, 0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.5757967829704285, 0.605756938457489, 0.38045990467071533, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.12970638275146484, 0.7689643502235413, 0.6867401599884033, 0.6047636270523071]  ‚Üí  acq = -0.9342002469498426
X = [0.500048816204071, 0.24933886528015137, 0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.07476793229579926, 0.3871777653694153, 0.952457845211029, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.24435395002365112, 0.10478633642196655, 0.9867004156112671, 0.0338512659072876]  ‚Üí  acq = -0.9828383653131805
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0612, dtype=torch.float64), tensor(0.0186, dtype=torch.float64), tensor(0.0988, dtype=torch.float64), tensor(0.0146, dtype=torch.float64), tensor(0.0856, dtype=torch.float64), tensor(0.0168, dtype=torch.float64), tensor(0.3148, dtype=torch.float64), tensor(0.3896, dtype=torch.float64), 14, 0, 0, 1, 1, 0, 112, 2.603692118484303e-20, 32.15603397258978, 0]
normalized proposed parameters for next round by BO: [tensor(4.8926e-19, dtype=torch.float64), tensor(0.0612, dtype=torch.float64), tensor(0.0186, dtype=torch.float64), tensor(0.0988, dtype=torch.float64), tensor(0.0146, dtype=torch.float64), tensor(0.0856, dtype=torch.float64), tensor(0.0168, dtype=torch.float64), tensor(0.3148, dtype=torch.float64), tensor(0.3896, dtype=torch.float64), tensor(0.4375, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8788, dtype=torch.float64), tensor(2.6037e-19, dtype=torch.float64), tensor(0.6699, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.061
  rowan_hellaswag: 0.019
  sciq: 0.099
  triviaqa: 0.015
  truthfulqa_gen: 0.086
  wikitext: 0.017
  mmlu: 0.315
  arc_challenge: 0.39

LoRA Parameters:
  lora_r: (112,)
  lora_dropout: (2.603692118484303e-20,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (32.15603397258978,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  112
lora dropout:  2.603692118484303e-20
lora alpha:  32.15603397258978
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 57,802,752 || all params: 8,088,064,000 || trainable%: 0.7147
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0533, 'grad_norm': 0.7180413603782654, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8520689010620117, 'eval_runtime': 9.1538, 'eval_samples_per_second': 109.245, 'eval_steps_per_second': 6.882, 'epoch': 0.04}
{'loss': 1.4282, 'grad_norm': 0.2762705385684967, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4878813028335571, 'eval_runtime': 9.1918, 'eval_samples_per_second': 108.793, 'eval_steps_per_second': 6.854, 'epoch': 0.08}
{'loss': 1.2084, 'grad_norm': 0.17180831730365753, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.408353328704834, 'eval_runtime': 9.2575, 'eval_samples_per_second': 108.02, 'eval_steps_per_second': 6.805, 'epoch': 0.12}
{'loss': 1.1393, 'grad_norm': 0.1824013739824295, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.374913215637207, 'eval_runtime': 9.2923, 'eval_samples_per_second': 107.616, 'eval_steps_per_second': 6.78, 'epoch': 0.16}
{'loss': 1.1165, 'grad_norm': 0.1851530373096466, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3529698848724365, 'eval_runtime': 9.279, 'eval_samples_per_second': 107.77, 'eval_steps_per_second': 6.79, 'epoch': 0.2}
{'loss': 1.1127, 'grad_norm': 0.17876207828521729, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3451060056686401, 'eval_runtime': 9.2833, 'eval_samples_per_second': 107.72, 'eval_steps_per_second': 6.786, 'epoch': 0.24}
{'loss': 1.1353, 'grad_norm': 0.1985710859298706, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.342508316040039, 'eval_runtime': 9.277, 'eval_samples_per_second': 107.794, 'eval_steps_per_second': 6.791, 'epoch': 0.28}
{'loss': 1.0904, 'grad_norm': 0.19326743483543396, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3324055671691895, 'eval_runtime': 9.2568, 'eval_samples_per_second': 108.029, 'eval_steps_per_second': 6.806, 'epoch': 0.32}
{'loss': 1.0953, 'grad_norm': 0.19234587252140045, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3317065238952637, 'eval_runtime': 9.2244, 'eval_samples_per_second': 108.408, 'eval_steps_per_second': 6.83, 'epoch': 0.36}
{'loss': 1.088, 'grad_norm': 0.18063119053840637, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3282614946365356, 'eval_runtime': 9.2396, 'eval_samples_per_second': 108.229, 'eval_steps_per_second': 6.818, 'epoch': 0.4}
{'loss': 1.0782, 'grad_norm': 0.1952529400587082, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3315805196762085, 'eval_runtime': 9.2399, 'eval_samples_per_second': 108.226, 'eval_steps_per_second': 6.818, 'epoch': 0.44}
{'loss': 1.0621, 'grad_norm': 0.19046665728092194, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3264758586883545, 'eval_runtime': 9.2361, 'eval_samples_per_second': 108.271, 'eval_steps_per_second': 6.821, 'epoch': 0.48}
{'loss': 1.0385, 'grad_norm': 0.2091875970363617, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.320245385169983, 'eval_runtime': 9.2383, 'eval_samples_per_second': 108.245, 'eval_steps_per_second': 6.819, 'epoch': 0.52}
{'loss': 1.0601, 'grad_norm': 0.19907131791114807, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3200806379318237, 'eval_runtime': 9.2254, 'eval_samples_per_second': 108.396, 'eval_steps_per_second': 6.829, 'epoch': 0.56}
{'loss': 1.0994, 'grad_norm': 0.2226753830909729, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3179351091384888, 'eval_runtime': 9.2206, 'eval_samples_per_second': 108.453, 'eval_steps_per_second': 6.833, 'epoch': 0.6}
{'loss': 1.0459, 'grad_norm': 0.20451219379901886, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3188483715057373, 'eval_runtime': 9.2044, 'eval_samples_per_second': 108.644, 'eval_steps_per_second': 6.845, 'epoch': 0.64}
{'loss': 1.0216, 'grad_norm': 0.2201986014842987, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.315946102142334, 'eval_runtime': 9.1878, 'eval_samples_per_second': 108.84, 'eval_steps_per_second': 6.857, 'epoch': 0.68}
{'loss': 1.0278, 'grad_norm': 0.2562157213687897, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3133409023284912, 'eval_runtime': 9.1752, 'eval_samples_per_second': 108.99, 'eval_steps_per_second': 6.866, 'epoch': 0.72}
{'loss': 1.0092, 'grad_norm': 0.22907549142837524, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3114792108535767, 'eval_runtime': 9.1931, 'eval_samples_per_second': 108.777, 'eval_steps_per_second': 6.853, 'epoch': 0.76}
{'loss': 0.9917, 'grad_norm': 0.2339053750038147, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3124558925628662, 'eval_runtime': 9.1764, 'eval_samples_per_second': 108.975, 'eval_steps_per_second': 6.865, 'epoch': 0.8}
{'loss': 0.9789, 'grad_norm': 0.27947551012039185, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3102524280548096, 'eval_runtime': 9.1726, 'eval_samples_per_second': 109.021, 'eval_steps_per_second': 6.868, 'epoch': 0.84}
{'loss': 1.0471, 'grad_norm': 0.3035615086555481, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3110251426696777, 'eval_runtime': 9.1726, 'eval_samples_per_second': 109.02, 'eval_steps_per_second': 6.868, 'epoch': 0.88}
{'loss': 0.9739, 'grad_norm': 0.2723221182823181, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3088152408599854, 'eval_runtime': 9.1392, 'eval_samples_per_second': 109.419, 'eval_steps_per_second': 6.893, 'epoch': 0.92}
{'loss': 0.9702, 'grad_norm': 0.2689482271671295, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3088934421539307, 'eval_runtime': 9.1636, 'eval_samples_per_second': 109.128, 'eval_steps_per_second': 6.875, 'epoch': 0.96}
{'loss': 0.9762, 'grad_norm': 0.2992071509361267, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.308030605316162, 'eval_runtime': 9.1555, 'eval_samples_per_second': 109.224, 'eval_steps_per_second': 6.881, 'epoch': 1.0}
{'train_runtime': 373.3558, 'train_samples_per_second': 26.776, 'train_steps_per_second': 1.674, 'train_loss': 1.153933984375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8520689010620117, 1.4878813028335571, 1.408353328704834, 1.374913215637207, 1.3529698848724365, 1.3451060056686401, 1.342508316040039, 1.3324055671691895, 1.3317065238952637, 1.3282614946365356, 1.3315805196762085, 1.3264758586883545, 1.320245385169983, 1.3200806379318237, 1.3179351091384888, 1.3188483715057373, 1.315946102142334, 1.3133409023284912, 1.3114792108535767, 1.3124558925628662, 1.3102524280548096, 1.3110251426696777, 1.3088152408599854, 1.3088934421539307, 1.308030605316162], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8520689010620117, 1.4878813028335571, 1.408353328704834, 1.374913215637207, 1.3529698848724365, 1.3451060056686401, 1.342508316040039, 1.3324055671691895, 1.3317065238952637, 1.3282614946365356, 1.3315805196762085, 1.3264758586883545, 1.320245385169983, 1.3200806379318237, 1.3179351091384888, 1.3188483715057373, 1.315946102142334, 1.3133409023284912, 1.3114792108535767, 1.3124558925628662, 1.3102524280548096, 1.3110251426696777, 1.3088152408599854, 1.3088934421539307, 1.308030605316162]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.308030605316162
max eval_loss so far:  -1.2208707332611084
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.9083 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -0.9367334836080641
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -0.9366649625597763
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -0.9386232077402233
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -0.968927281607514
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -0.9366491039162168
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0148, dtype=torch.float64), 0, tensor(0.0112, dtype=torch.float64), tensor(0.0881, dtype=torch.float64), 0, tensor(0.0722, dtype=torch.float64), 0, tensor(0.3323, dtype=torch.float64), tensor(0.4634, dtype=torch.float64), 14, 0, 0, 1, 0, 1, 119, 4.3368086899420186e-21, 32.31099931354841, 1]
normalized proposed parameters for next round by BO: [tensor(0.0148, dtype=torch.float64), tensor(0.0060, dtype=torch.float64), tensor(0.0112, dtype=torch.float64), tensor(0.0881, dtype=torch.float64), tensor(0.0070, dtype=torch.float64), tensor(0.0722, dtype=torch.float64), tensor(0.0050, dtype=torch.float64), tensor(0.3323, dtype=torch.float64), tensor(0.4634, dtype=torch.float64), tensor(0.4320, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9310, dtype=torch.float64), tensor(4.3368e-20, dtype=torch.float64), tensor(0.6731, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-2.0815322399139404, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
mmlu
evaluation dataset:
data domain:  mmlu  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/mmlu/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.3161191096028816, 0.11426349731754912, 0.08683311140461922, 0.010648816854681904, 0.21071291050026175, 0.09848450659553602, 0.03252722732441375, 0.03858444462759559, 0.09182637577246108, 14, 1, 1, 1, 1, 0, 22, 0.04375408122258204, 13, 0]
Checking history sample input_X_between_0_1:  [0.3161191096028816, 0.11426349731754912, 0.08683311140461922, 0.010648816854681904, 0.21071291050026175, 0.09848450659553602, 0.03252722732441375, 0.03858444462759559, 0.09182637577246108, 0.4375, 1.0, 1.0, 1.0, 1.0, 0.0, 0.171875, 0.4375408122258204, 0.2708333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1066114902496338
Checking history sample input_X:  [0.17001858590456562, 0.06482605897458003, 0.0552231822789883, 0.2005042167465889, 0.1402610441334454, 0.22905697283425983, 0.05485942451788231, 0.033879744874935516, 0.05137076973475401, 21, 0, 1, 1, 1, 1, 71, 0.008194930202178574, 20, 0]
Checking history sample input_X_between_0_1:  [0.17001858590456562, 0.06482605897458003, 0.0552231822789883, 0.2005042167465889, 0.1402610441334454, 0.22905697283425983, 0.05485942451788231, 0.033879744874935516, 0.05137076973475401, 0.65625, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5546875, 0.08194930202178573, 0.4166666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9399064183235168
Checking history sample input_X:  [0.08457531624975921, 0.03444227766767646, 0.15033113821265595, 0.2888075957037953, 0.10483406652162627, 0.11125953049519431, 0.08585722091652723, 0.13154360559344933, 0.008349248639315885, 4, 0, 1, 1, 0, 0, 111, 0.08837128439087288, 42, 1]
Checking history sample input_X_between_0_1:  [0.08457531624975921, 0.03444227766767646, 0.15033113821265595, 0.2888075957037953, 0.10483406652162627, 0.11125953049519431, 0.08585722091652723, 0.13154360559344933, 0.008349248639315885, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8671875, 0.8837128439087288, 0.875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4136319160461426
Checking history sample input_X:  [0.16965502636986396, 0.1632541269761418, 0.06536948116802524, 0.22931460918771437, 0.07235389537017577, 0.07948837750247255, 0.09856044446849346, 0.10613959878213879, 0.015864440174974067, 2, 1, 0, 0, 0, 0, 52, 0.02440109885663705, 37, 1]
Checking history sample input_X_between_0_1:  [0.16965502636986396, 0.1632541269761418, 0.06536948116802524, 0.22931460918771437, 0.07235389537017577, 0.07948837750247255, 0.09856044446849346, 0.10613959878213879, 0.015864440174974067, 0.0625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.40625, 0.2440109885663705, 0.7708333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -2.0770633220672607
Checking history sample input_X:  [0.36131685118949053, 0.2612655996132517, 0.09728973792827665, 0.016352402241110658, 0.0637664750231403, 0.01680248270633843, 0.137283452814417, 0.01400964559956043, 0.03191335288441422, 16, 1, 1, 1, 0, 0, 70, 0.02082741206626623, 38, 1]
Checking history sample input_X_between_0_1:  [0.36131685118949053, 0.2612655996132517, 0.09728973792827665, 0.016352402241110658, 0.0637664750231403, 0.01680248270633843, 0.137283452814417, 0.01400964559956043, 0.03191335288441422, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.546875, 0.20827412066266227, 0.7916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0819246768951416
Checking history sample input_X:  [0.1471945258628963, 0.09048865243304925, 0.0028432699437709185, 0.03648812445816234, 0.15879745758736136, 0.08392207963907294, 0.3559804206469414, 0.1163518135768591, 0.007933655851886249, 30, 1, 1, 0, 1, 1, 27, 0.03341373220395105, 3, 1]
Checking history sample input_X_between_0_1:  [0.1471945258628963, 0.09048865243304925, 0.0028432699437709185, 0.03648812445816234, 0.15879745758736136, 0.08392207963907294, 0.3559804206469414, 0.1163518135768591, 0.007933655851886249, 0.9375, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2109375, 0.3341373220395105, 0.0625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2804617881774902
Checking history sample input_X:  [0.09708970671019405, 0.13689092369695835, 0.13726832030202735, 0.03956836294376928, 0.09044580719401238, 0.041564370215183784, 0.0018749764538878487, 0.24111683879487406, 0.21418069368909284, 25, 1, 0, 0, 1, 1, 72, 0.07793912935485114, 34, 0]
Checking history sample input_X_between_0_1:  [0.09708970671019405, 0.13689092369695835, 0.13726832030202735, 0.03956836294376928, 0.09044580719401238, 0.041564370215183784, 0.0018749764538878487, 0.24111683879487406, 0.21418069368909284, 0.78125, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5625, 0.7793912935485114, 0.7083333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0335614681243896
Checking history sample input_X:  [0.10072133130350767, 0.061195722672967294, 0.15043397081227575, 0.09909493084207656, 0.14517766942001123, 0.09911157330699087, 0.06709075752452866, 0.1444407486582126, 0.13273329545942936, 12, 1, 0, 0, 1, 0, 48, 0.029635134547486387, 5, 0]
Checking history sample input_X_between_0_1:  [0.10072133130350767, 0.061195722672967294, 0.15043397081227575, 0.09909493084207656, 0.14517766942001123, 0.09911157330699087, 0.06709075752452866, 0.1444407486582126, 0.13273329545942936, 0.375, 1.0, 0.0, 0.0, 1.0, 0.0, 0.375, 0.29635134547486386, 0.10416666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3930084705352783
Checking history sample input_X:  [0.0056408233326652405, 0.045454006708258686, 0.02419314968847306, 0.2621062198301061, 0.2543756705541004, 0.20901339392353954, 0.058849705392657874, 0.062447696140449545, 0.07791933442974946, 10, 0, 1, 0, 1, 1, 118, 0.004765499761682824, 9, 1]
Checking history sample input_X_between_0_1:  [0.0056408233326652405, 0.045454006708258686, 0.02419314968847306, 0.2621062198301061, 0.2543756705541004, 0.20901339392353954, 0.058849705392657874, 0.062447696140449545, 0.07791933442974946, 0.3125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.921875, 0.04765499761682824, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0495586395263672
Checking history sample input_X:  [0.03488693528172687, 0.35652639517245394, 0.07778500850664789, 0.10513445995831482, 0.36325923927607257, 0.0020496056495831597, 0.04514940236956389, 0.009292499052857398, 0.005916454732779405, 12, 1, 0, 1, 0, 0, 126, 0.042101384736383855, 9, 1]
Checking history sample input_X_between_0_1:  [0.03488693528172687, 0.35652639517245394, 0.07778500850664789, 0.10513445995831482, 0.36325923927607257, 0.0020496056495831597, 0.04514940236956389, 0.009292499052857398, 0.005916454732779405, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.984375, 0.4210138473638385, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.156050682067871
Checking history sample input_X:  [0.10948692372840407, 0.21669077939904466, 0.16290113684821536, 0.06632389273412838, 0.023331634670867053, 0.04511875862710639, 0.06417984157378358, 0.04013429279943936, 0.2718327396190111, 1, 1, 1, 1, 1, 0, 86, 0.0106663316581613, 38, 1]
Checking history sample input_X_between_0_1:  [0.10948692372840407, 0.21669077939904466, 0.16290113684821536, 0.06632389273412838, 0.023331634670867053, 0.04511875862710639, 0.06417984157378358, 0.04013429279943936, 0.2718327396190111, 0.03125, 1.0, 1.0, 1.0, 1.0, 0.0, 0.671875, 0.10666331658161299, 0.7916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.288783311843872
Checking history sample input_X:  [0.01438882831028789, 0.12997755569201166, 0.04075163354943151, 0.023595154255388905, 0.35477131795304295, 0.016630964685680923, 0.05509145809339371, 0.17242366325727926, 0.19236942420348319, 23, 1, 0, 1, 1, 0, 44, 0.0856132560172227, 24, 0]
Checking history sample input_X_between_0_1:  [0.01438882831028789, 0.12997755569201166, 0.04075163354943151, 0.023595154255388905, 0.35477131795304295, 0.016630964685680923, 0.05509145809339371, 0.17242366325727926, 0.19236942420348319, 0.71875, 1.0, 0.0, 1.0, 1.0, 0.0, 0.34375, 0.8561325601722269, 0.5, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9945324659347534
Checking history sample input_X:  [0.24449605892239556, 0.03207833927765533, 0.03240197712149855, 0.1790966164420265, 0.0030442733927761897, 0.05196332423567327, 0.23192639161149287, 0.20689610574355566, 0.018096913252926106, 27, 1, 0, 1, 0, 1, 88, 0.010689967975687609, 46, 0]
Checking history sample input_X_between_0_1:  [0.24449605892239556, 0.03207833927765533, 0.03240197712149855, 0.1790966164420265, 0.0030442733927761897, 0.05196332423567327, 0.23192639161149287, 0.20689610574355566, 0.018096913252926106, 0.84375, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6875, 0.10689967975687609, 0.9583333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0903468132019043
Checking history sample input_X:  [0.02102103063057685, 0.04371291849458268, 0.03642670439125302, 0.1177710527319927, 0.001407831158738137, 0.07877731789412545, 0.023108436790582124, 0.2712565713655495, 0.4065181365425995, 14, 0, 0, 1, 0, 1, 116, 0.0043609167679767745, 32, 0]
Checking history sample input_X_between_0_1:  [0.02102103063057685, 0.04371291849458268, 0.03642670439125302, 0.1177710527319927, 0.001407831158738137, 0.07877731789412545, 0.023108436790582124, 0.2712565713655495, 0.4065181365425995, 0.4375, 0.0, 0.0, 1.0, 0.0, 1.0, 0.90625, 0.04360916767976774, 0.6666666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.8854451179504395
Checking history sample input_X:  [0.07412507163006393, 0.2445504968671524, 0.13622879351830353, 0.04483166378469822, 0.22478374969436524, 0.03378463940701705, 0.23468787806930036, 0.0038162629362842922, 0.00319144409281491, 3, 1, 1, 0, 1, 1, 42, 0.09004868565085311, 14, 1]
Checking history sample input_X_between_0_1:  [0.07412507163006393, 0.2445504968671524, 0.13622879351830353, 0.04483166378469822, 0.22478374969436524, 0.03378463940701705, 0.23468787806930036, 0.0038162629362842922, 0.00319144409281491, 0.09375, 1.0, 1.0, 0.0, 1.0, 1.0, 0.328125, 0.9004868565085311, 0.2916666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.3376717567443848
Checking history sample input_X:  [0.13979331608413714, 0.22567075739867196, 0.047220640032135254, 0.04226304004460249, 0.20524041855401653, 0.1714340493110747, 0.08299650343653195, 0.04377008261845374, 0.04161119252037616, 32, 1, 1, 0, 1, 0, 29, 0.006821658518724117, 29, 0]
Checking history sample input_X_between_0_1:  [0.13979331608413714, 0.22567075739867196, 0.047220640032135254, 0.04226304004460249, 0.20524041855401653, 0.1714340493110747, 0.08299650343653195, 0.04377008261845374, 0.04161119252037616, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2265625, 0.06821658518724116, 0.6041666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9732747077941895
Checking history sample input_X:  [0.05744047965548223, 0.289973214141178, 0.20610036282289132, 0.008161921516160834, 0.05654145345652092, 0.0039259100145156494, 0.08013508882995081, 0.2050713532761911, 0.09265021628710923, 30, 1, 0, 1, 0, 1, 112, 0.003324070102424537, 36, 1]
Checking history sample input_X_between_0_1:  [0.05744047965548223, 0.289973214141178, 0.20610036282289132, 0.008161921516160834, 0.05654145345652092, 0.0039259100145156494, 0.08013508882995081, 0.2050713532761911, 0.09265021628710923, 0.9375, 1.0, 0.0, 1.0, 0.0, 1.0, 0.875, 0.03324070102424537, 0.75, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1381622552871704
Checking history sample input_X:  [0.011148142522340278, 0.2280230847756128, 0.289512358234605, 0.006770020691989324, 0.024707092605269564, 0.08731213423792487, 0.005804310855452206, 0.08464343941500369, 0.26207941666180234, 9, 1, 1, 1, 1, 0, 105, 0.09544186461572818, 12, 0]
Checking history sample input_X_between_0_1:  [0.011148142522340278, 0.2280230847756128, 0.289512358234605, 0.006770020691989324, 0.024707092605269564, 0.08731213423792487, 0.005804310855452206, 0.08464343941500369, 0.26207941666180234, 0.28125, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8203125, 0.9544186461572818, 0.25, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3062412738800049
Checking history sample input_X:  [0.018288746378659515, 0.11384303280049743, 0.03547167747709447, 0.12207096189867232, 0.06447946973899343, 0.18231445052447756, 0.1522399185681228, 0.13557467183201224, 0.17571707078147022, 4, 0, 1, 0, 1, 0, 53, 0.035644857853155076, 11, 0]
Checking history sample input_X_between_0_1:  [0.018288746378659515, 0.11384303280049743, 0.03547167747709447, 0.12207096189867232, 0.06447946973899343, 0.18231445052447756, 0.1522399185681228, 0.13557467183201224, 0.17571707078147022, 0.125, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4140625, 0.35644857853155076, 0.22916666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2854622602462769
Checking history sample input_X:  [0.511268635219867, 0.08759324792444484, 0.1034528140114814, 0.07300440359718659, 0.03242123832197673, 0.031167825877860832, 0.0622431292646515, 0.05784239394761446, 0.04100631183491673, 15, 0, 0, 1, 1, 0, 125, 0.022242287070241652, 48, 0]
Checking history sample input_X_between_0_1:  [0.511268635219867, 0.08759324792444484, 0.1034528140114814, 0.07300440359718659, 0.03242123832197673, 0.031167825877860832, 0.0622431292646515, 0.05784239394761446, 0.04100631183491673, 0.46875, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9765625, 0.2224228707024165, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0917696952819824
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.4595 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6924781799316406, 0.9914748668670654, 0.46820950508117676, 0.14190673828125, 0.7638208866119385, 0.5338059067726135, 0.91374272108078, 0.7691851854324341, 0.3934860825538635, 0.7314881086349487, 0.5359349250793457, 0.5624076724052429, 0.30159080028533936, 0.013053536415100098, 0.8476026654243469, 0.6031768918037415, 0.21682864427566528, 0.15649300813674927, 0.30232417583465576]  ‚Üí  acq = -0.1802830134352471
X = [0.919795572757721, 0.353039026260376, 0.31309473514556885, 0.9690325856208801, 0.741429328918457, 0.08862662315368652, 0.8519266843795776, 0.39755064249038696, 0.6575251221656799, 0.20242971181869507, 0.34949588775634766, 0.22177106142044067, 0.6537419557571411, 0.5333794355392456, 0.9127693772315979, 0.7757470011711121, 0.5574998259544373, 0.15530815720558167, 0.5965682864189148]  ‚Üí  acq = -0.18028291024062315
X = [0.18142277002334595, 0.8060216903686523, 0.9821801781654358, 0.217138409614563, 0.7242514491081238, 0.04273468255996704, 0.23225504159927368, 0.4354449510574341, 0.8733453154563904, 0.38225600123405457, 0.8084838390350342, 0.07627373933792114, 0.6253472566604614, 0.7630738615989685, 0.6941807866096497, 0.7885355353355408, 0.37886643409729004, 0.955147385597229, 0.1228141188621521]  ‚Üí  acq = -0.18028301342361464
X = [0.446244478225708, 0.5306597948074341, 0.6137567162513733, 0.5560302138328552, 0.8858624696731567, 0.5024411082267761, 0.2697206139564514, 0.3054194450378418, 0.5546465516090393, 0.8616708517074585, 0.91047203540802, 0.9998549818992615, 0.12504887580871582, 0.70234215259552, 0.4380992650985718, 0.7238349914550781, 0.301927387714386, 0.2914554476737976, 0.37629246711730957]  ‚Üí  acq = -0.18028496753916112
X = [0.6603155136108398, 0.7565454244613647, 0.5336613655090332, 0.17799031734466553, 0.8127012848854065, 0.2872012257575989, 0.9108836054801941, 0.3954470753669739, 0.7753071188926697, 0.2436017096042633, 0.8140610456466675, 0.9655599594116211, 0.3219142556190491, 0.10273993015289307, 0.758270800113678, 0.3312584161758423, 0.8472918272018433, 0.06632013618946075, 0.3413085341453552]  ‚Üí  acq = -0.18028301338757302
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.4513, dtype=torch.float64), 0, tensor(0.0285, dtype=torch.float64), 0, 0, tensor(0.0928, dtype=torch.float64), 0, 0, tensor(0.4274, dtype=torch.float64), 27, 1, 1, 1, 0, 1, 128, 0.012024394532071167, 48.0, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.4513, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0285, dtype=torch.float64), tensor(2.6828e-17, dtype=torch.float64), tensor(2.6697e-17, dtype=torch.float64), tensor(0.0928, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4274, dtype=torch.float64), tensor(0.8319, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9979, dtype=torch.float64), tensor(0.1202, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.451
  gsm8k: 0
  rowan_hellaswag: 0.028
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.093
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.427

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.012024394532071167,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.012024394532071167
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 173,408,256 || all params: 8,203,669,504 || trainable%: 2.1138
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8166, 'grad_norm': 0.5087034106254578, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.017194986343384, 'eval_runtime': 10.0182, 'eval_samples_per_second': 99.818, 'eval_steps_per_second': 6.289, 'epoch': 0.04}
{'loss': 1.2361, 'grad_norm': 0.24429409205913544, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9365900754928589, 'eval_runtime': 10.0471, 'eval_samples_per_second': 99.532, 'eval_steps_per_second': 6.27, 'epoch': 0.08}
{'loss': 1.0169, 'grad_norm': 0.2223055213689804, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8832968473434448, 'eval_runtime': 10.1067, 'eval_samples_per_second': 98.944, 'eval_steps_per_second': 6.233, 'epoch': 0.12}
{'loss': 0.9764, 'grad_norm': 0.22156134247779846, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8537930250167847, 'eval_runtime': 10.1437, 'eval_samples_per_second': 98.584, 'eval_steps_per_second': 6.211, 'epoch': 0.16}
{'loss': 0.8753, 'grad_norm': 0.23177212476730347, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8707717657089233, 'eval_runtime': 10.1468, 'eval_samples_per_second': 98.553, 'eval_steps_per_second': 6.209, 'epoch': 0.2}
{'loss': 0.8799, 'grad_norm': 0.2353074848651886, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8514397144317627, 'eval_runtime': 10.1588, 'eval_samples_per_second': 98.437, 'eval_steps_per_second': 6.202, 'epoch': 0.24}
{'loss': 0.8509, 'grad_norm': 0.2165634036064148, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8721210956573486, 'eval_runtime': 10.1593, 'eval_samples_per_second': 98.432, 'eval_steps_per_second': 6.201, 'epoch': 0.28}
{'loss': 0.8267, 'grad_norm': 0.22637154161930084, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8782260417938232, 'eval_runtime': 10.1499, 'eval_samples_per_second': 98.523, 'eval_steps_per_second': 6.207, 'epoch': 0.32}
{'loss': 0.7814, 'grad_norm': 0.2536167502403259, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8930186033248901, 'eval_runtime': 10.1503, 'eval_samples_per_second': 98.519, 'eval_steps_per_second': 6.207, 'epoch': 0.36}
{'loss': 0.756, 'grad_norm': 0.2626000940799713, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8817778825759888, 'eval_runtime': 10.1733, 'eval_samples_per_second': 98.297, 'eval_steps_per_second': 6.193, 'epoch': 0.4}
{'loss': 0.7289, 'grad_norm': 0.2900078594684601, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.912124752998352, 'eval_runtime': 10.1746, 'eval_samples_per_second': 98.284, 'eval_steps_per_second': 6.192, 'epoch': 0.44}
{'loss': 0.7229, 'grad_norm': 0.2692983150482178, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9709956645965576, 'eval_runtime': 10.1569, 'eval_samples_per_second': 98.455, 'eval_steps_per_second': 6.203, 'epoch': 0.48}
{'loss': 0.7122, 'grad_norm': 0.26522594690322876, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9636694192886353, 'eval_runtime': 10.1287, 'eval_samples_per_second': 98.73, 'eval_steps_per_second': 6.22, 'epoch': 0.52}
{'loss': 0.7154, 'grad_norm': 0.2781792879104614, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9454536437988281, 'eval_runtime': 10.1212, 'eval_samples_per_second': 98.803, 'eval_steps_per_second': 6.225, 'epoch': 0.56}
{'loss': 0.706, 'grad_norm': 0.3295597434043884, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9641411304473877, 'eval_runtime': 10.1387, 'eval_samples_per_second': 98.632, 'eval_steps_per_second': 6.214, 'epoch': 0.6}
{'loss': 0.6776, 'grad_norm': 0.23536169528961182, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9537878036499023, 'eval_runtime': 10.1295, 'eval_samples_per_second': 98.721, 'eval_steps_per_second': 6.219, 'epoch': 0.64}
{'loss': 0.6754, 'grad_norm': 0.19242900609970093, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9656453132629395, 'eval_runtime': 10.1156, 'eval_samples_per_second': 98.857, 'eval_steps_per_second': 6.228, 'epoch': 0.68}
{'loss': 0.6483, 'grad_norm': 0.25207749009132385, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.024258852005005, 'eval_runtime': 10.1407, 'eval_samples_per_second': 98.612, 'eval_steps_per_second': 6.213, 'epoch': 0.72}
{'loss': 0.653, 'grad_norm': 0.40814727544784546, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.0402095317840576, 'eval_runtime': 10.1181, 'eval_samples_per_second': 98.833, 'eval_steps_per_second': 6.226, 'epoch': 0.76}
{'loss': 0.5876, 'grad_norm': 0.3665250539779663, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.0658135414123535, 'eval_runtime': 10.0863, 'eval_samples_per_second': 99.145, 'eval_steps_per_second': 6.246, 'epoch': 0.8}
{'loss': 0.5694, 'grad_norm': 0.3306707441806793, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0374338626861572, 'eval_runtime': 10.1043, 'eval_samples_per_second': 98.968, 'eval_steps_per_second': 6.235, 'epoch': 0.84}
{'loss': 0.6091, 'grad_norm': 0.2443430870771408, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0600154399871826, 'eval_runtime': 10.0905, 'eval_samples_per_second': 99.103, 'eval_steps_per_second': 6.243, 'epoch': 0.88}
{'loss': 0.5818, 'grad_norm': 0.2947952449321747, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1021811962127686, 'eval_runtime': 10.0878, 'eval_samples_per_second': 99.129, 'eval_steps_per_second': 6.245, 'epoch': 0.92}
{'loss': 0.5561, 'grad_norm': 0.2748168110847473, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1489994525909424, 'eval_runtime': 10.0779, 'eval_samples_per_second': 99.227, 'eval_steps_per_second': 6.251, 'epoch': 0.96}
{'loss': 0.5916, 'grad_norm': 0.28678715229034424, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.1364684104919434, 'eval_runtime': 10.0642, 'eval_samples_per_second': 99.363, 'eval_steps_per_second': 6.26, 'epoch': 1.0}
{'train_runtime': 418.0868, 'train_samples_per_second': 23.911, 'train_steps_per_second': 1.495, 'train_loss': 0.8300662658691407, 'epoch': 1.0}
train_results:  {'eval_loss': [2.017194986343384, 1.9365900754928589, 1.8832968473434448, 1.8537930250167847, 1.8707717657089233, 1.8514397144317627, 1.8721210956573486, 1.8782260417938232, 1.8930186033248901, 1.8817778825759888, 1.912124752998352, 1.9709956645965576, 1.9636694192886353, 1.9454536437988281, 1.9641411304473877, 1.9537878036499023, 1.9656453132629395, 2.024258852005005, 2.0402095317840576, 2.0658135414123535, 2.0374338626861572, 2.0600154399871826, 2.1021811962127686, 2.1489994525909424, 2.1364684104919434], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.017194986343384, 1.9365900754928589, 1.8832968473434448, 1.8537930250167847, 1.8707717657089233, 1.8514397144317627, 1.8721210956573486, 1.8782260417938232, 1.8930186033248901, 1.8817778825759888, 1.912124752998352, 1.9709956645965576, 1.9636694192886353, 1.9454536437988281, 1.9641411304473877, 1.9537878036499023, 1.9656453132629395, 2.024258852005005, 2.0402095317840576, 2.0658135414123535, 2.0374338626861572, 2.0600154399871826, 2.1021811962127686, 2.1489994525909424, 2.1364684104919434]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.1364684104919434
max eval_loss so far:  -2.1364684104919434
BO observations:  [-1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.2360 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.41261879326010376
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.4126068499777118
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.41260684997740005
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.4126068499774005
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.41266674326842834
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2542, dtype=torch.float64), 0, 0, 0, tensor(0.2100, dtype=torch.float64), 0, tensor(0.4596, dtype=torch.float64), tensor(0.0763, dtype=torch.float64), 16, 0, 1, 1, 1, 1, 128, 3.4694469519535913e-19, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2542, dtype=torch.float64), tensor(6.0312e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2100, dtype=torch.float64), tensor(1.0283e-18, dtype=torch.float64), tensor(0.4596, dtype=torch.float64), tensor(0.0763, dtype=torch.float64), tensor(0.5034, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.4694e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.254
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.21
  wikitext: 0
  mmlu: 0.46
  arc_challenge: 0.076

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.4694469519535913e-19,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  3.4694469519535913e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 123,731,968 || all params: 8,153,993,216 || trainable%: 1.5174
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4565, 'grad_norm': 0.6335557699203491, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6305292844772339, 'eval_runtime': 9.5392, 'eval_samples_per_second': 104.831, 'eval_steps_per_second': 6.604, 'epoch': 0.04}
{'loss': 1.2864, 'grad_norm': 0.5001861453056335, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3996391296386719, 'eval_runtime': 9.528, 'eval_samples_per_second': 104.953, 'eval_steps_per_second': 6.612, 'epoch': 0.08}
{'loss': 1.15, 'grad_norm': 0.357722669839859, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.341939926147461, 'eval_runtime': 9.5578, 'eval_samples_per_second': 104.627, 'eval_steps_per_second': 6.591, 'epoch': 0.12}
{'loss': 1.143, 'grad_norm': 0.37661197781562805, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3095381259918213, 'eval_runtime': 9.5758, 'eval_samples_per_second': 104.43, 'eval_steps_per_second': 6.579, 'epoch': 0.16}
{'loss': 1.0575, 'grad_norm': 0.27666935324668884, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3008205890655518, 'eval_runtime': 9.5875, 'eval_samples_per_second': 104.302, 'eval_steps_per_second': 6.571, 'epoch': 0.2}
{'loss': 1.0641, 'grad_norm': 0.2898523509502411, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2879455089569092, 'eval_runtime': 9.6007, 'eval_samples_per_second': 104.159, 'eval_steps_per_second': 6.562, 'epoch': 0.24}
{'loss': 1.0255, 'grad_norm': 0.2166307121515274, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.275900959968567, 'eval_runtime': 9.6061, 'eval_samples_per_second': 104.101, 'eval_steps_per_second': 6.558, 'epoch': 0.28}
{'loss': 1.0228, 'grad_norm': 0.23175843060016632, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.269924283027649, 'eval_runtime': 9.6355, 'eval_samples_per_second': 103.783, 'eval_steps_per_second': 6.538, 'epoch': 0.32}
{'loss': 1.0243, 'grad_norm': 0.3037758767604828, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.268859624862671, 'eval_runtime': 9.6097, 'eval_samples_per_second': 104.062, 'eval_steps_per_second': 6.556, 'epoch': 0.36}
{'loss': 1.0148, 'grad_norm': 0.213746577501297, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2649192810058594, 'eval_runtime': 9.6301, 'eval_samples_per_second': 103.841, 'eval_steps_per_second': 6.542, 'epoch': 0.4}
{'loss': 1.0212, 'grad_norm': 0.20650438964366913, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2625505924224854, 'eval_runtime': 9.6067, 'eval_samples_per_second': 104.094, 'eval_steps_per_second': 6.558, 'epoch': 0.44}
{'loss': 1.0093, 'grad_norm': 0.2423660159111023, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2619357109069824, 'eval_runtime': 9.5925, 'eval_samples_per_second': 104.248, 'eval_steps_per_second': 6.568, 'epoch': 0.48}
{'loss': 0.9846, 'grad_norm': 0.2641238272190094, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2552520036697388, 'eval_runtime': 9.589, 'eval_samples_per_second': 104.287, 'eval_steps_per_second': 6.57, 'epoch': 0.52}
{'loss': 1.0207, 'grad_norm': 0.2399895340204239, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2516391277313232, 'eval_runtime': 9.5909, 'eval_samples_per_second': 104.265, 'eval_steps_per_second': 6.569, 'epoch': 0.56}
{'loss': 0.9881, 'grad_norm': 0.37793323397636414, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2479194402694702, 'eval_runtime': 9.6029, 'eval_samples_per_second': 104.135, 'eval_steps_per_second': 6.561, 'epoch': 0.6}
{'loss': 0.975, 'grad_norm': 0.26520267128944397, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.248952031135559, 'eval_runtime': 9.6659, 'eval_samples_per_second': 103.457, 'eval_steps_per_second': 6.518, 'epoch': 0.64}
{'loss': 0.9817, 'grad_norm': 0.21818819642066956, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2427151203155518, 'eval_runtime': 9.6771, 'eval_samples_per_second': 103.337, 'eval_steps_per_second': 6.51, 'epoch': 0.68}
{'loss': 0.9791, 'grad_norm': 0.24342775344848633, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2401286363601685, 'eval_runtime': 9.7019, 'eval_samples_per_second': 103.073, 'eval_steps_per_second': 6.494, 'epoch': 0.72}
{'loss': 0.9715, 'grad_norm': 0.22611065208911896, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2391376495361328, 'eval_runtime': 9.7387, 'eval_samples_per_second': 102.683, 'eval_steps_per_second': 6.469, 'epoch': 0.76}
{'loss': 0.9463, 'grad_norm': 0.27839842438697815, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.237491488456726, 'eval_runtime': 9.7043, 'eval_samples_per_second': 103.047, 'eval_steps_per_second': 6.492, 'epoch': 0.8}
{'loss': 0.9953, 'grad_norm': 0.2651957869529724, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2338627576828003, 'eval_runtime': 9.674, 'eval_samples_per_second': 103.37, 'eval_steps_per_second': 6.512, 'epoch': 0.84}
{'loss': 0.9838, 'grad_norm': 0.26237863302230835, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2349623441696167, 'eval_runtime': 9.6723, 'eval_samples_per_second': 103.388, 'eval_steps_per_second': 6.513, 'epoch': 0.88}
{'loss': 0.9789, 'grad_norm': 0.29580315947532654, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2329682111740112, 'eval_runtime': 9.6821, 'eval_samples_per_second': 103.283, 'eval_steps_per_second': 6.507, 'epoch': 0.92}
{'loss': 0.9622, 'grad_norm': 0.2321953922510147, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.232421875, 'eval_runtime': 9.6763, 'eval_samples_per_second': 103.345, 'eval_steps_per_second': 6.511, 'epoch': 0.96}
{'loss': 0.9619, 'grad_norm': 0.3836207985877991, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2323100566864014, 'eval_runtime': 9.6408, 'eval_samples_per_second': 103.726, 'eval_steps_per_second': 6.535, 'epoch': 1.0}
{'train_runtime': 463.143, 'train_samples_per_second': 21.587, 'train_steps_per_second': 1.349, 'train_loss': 1.0801811218261719, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6305292844772339, 1.3996391296386719, 1.341939926147461, 1.3095381259918213, 1.3008205890655518, 1.2879455089569092, 1.275900959968567, 1.269924283027649, 1.268859624862671, 1.2649192810058594, 1.2625505924224854, 1.2619357109069824, 1.2552520036697388, 1.2516391277313232, 1.2479194402694702, 1.248952031135559, 1.2427151203155518, 1.2401286363601685, 1.2391376495361328, 1.237491488456726, 1.2338627576828003, 1.2349623441696167, 1.2329682111740112, 1.232421875, 1.2323100566864014], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6305292844772339, 1.3996391296386719, 1.341939926147461, 1.3095381259918213, 1.3008205890655518, 1.2879455089569092, 1.275900959968567, 1.269924283027649, 1.268859624862671, 1.2649192810058594, 1.2625505924224854, 1.2619357109069824, 1.2552520036697388, 1.2516391277313232, 1.2479194402694702, 1.248952031135559, 1.2427151203155518, 1.2401286363601685, 1.2391376495361328, 1.237491488456726, 1.2338627576828003, 1.2349623441696167, 1.2329682111740112, 1.232421875, 1.2323100566864014]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2323100566864014
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2863 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.6287689996481992
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.6286858344856412
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.6289290575358107
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.628668232130543
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.6286263620084899
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2568, dtype=torch.float64), tensor(0.0957, dtype=torch.float64), tensor(0.0390, dtype=torch.float64), tensor(0.0179, dtype=torch.float64), tensor(0.0190, dtype=torch.float64), tensor(0.2311, dtype=torch.float64), 0, tensor(0.0750, dtype=torch.float64), tensor(0.2654, dtype=torch.float64), 29, 0, 0, 1, 0, 1, 19, 0.05726998082505017, 37.44050969109808, 0]
normalized proposed parameters for next round by BO: [tensor(0.2568, dtype=torch.float64), tensor(0.0957, dtype=torch.float64), tensor(0.0390, dtype=torch.float64), tensor(0.0179, dtype=torch.float64), tensor(0.0190, dtype=torch.float64), tensor(0.2311, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0750, dtype=torch.float64), tensor(0.2654, dtype=torch.float64), tensor(0.9173, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1517, dtype=torch.float64), tensor(0.5727, dtype=torch.float64), tensor(0.7800, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.257
  gsm8k: 0.096
  rowan_hellaswag: 0.039
  sciq: 0.018
  triviaqa: 0.019
  truthfulqa_gen: 0.231
  wikitext: 0
  mmlu: 0.075
  arc_challenge: 0.265

LoRA Parameters:
  lora_r: (19,)
  lora_dropout: (0.05726998082505017,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (37.44050969109808,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  19
lora dropout:  0.05726998082505017
lora alpha:  37.44050969109808
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 20,312,064 || all params: 8,050,573,312 || trainable%: 0.2523
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9235, 'grad_norm': 1.3831312656402588, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8900076150894165, 'eval_runtime': 9.8668, 'eval_samples_per_second': 101.35, 'eval_steps_per_second': 6.385, 'epoch': 0.04}
{'loss': 1.3019, 'grad_norm': 0.4664829969406128, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5908414125442505, 'eval_runtime': 9.8836, 'eval_samples_per_second': 101.177, 'eval_steps_per_second': 6.374, 'epoch': 0.08}
{'loss': 1.1728, 'grad_norm': 0.46895888447761536, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4960196018218994, 'eval_runtime': 9.9164, 'eval_samples_per_second': 100.843, 'eval_steps_per_second': 6.353, 'epoch': 0.12}
{'loss': 1.047, 'grad_norm': 0.42910876870155334, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3740782737731934, 'eval_runtime': 9.9354, 'eval_samples_per_second': 100.651, 'eval_steps_per_second': 6.341, 'epoch': 0.16}
{'loss': 0.9627, 'grad_norm': 0.46571940183639526, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3671400547027588, 'eval_runtime': 9.9517, 'eval_samples_per_second': 100.485, 'eval_steps_per_second': 6.331, 'epoch': 0.2}
{'loss': 1.0017, 'grad_norm': 0.37370389699935913, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.33103609085083, 'eval_runtime': 9.953, 'eval_samples_per_second': 100.472, 'eval_steps_per_second': 6.33, 'epoch': 0.24}
{'loss': 0.9518, 'grad_norm': 0.4118301272392273, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3219678401947021, 'eval_runtime': 9.9392, 'eval_samples_per_second': 100.612, 'eval_steps_per_second': 6.339, 'epoch': 0.28}
{'loss': 0.9509, 'grad_norm': 0.41510239243507385, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3142399787902832, 'eval_runtime': 9.9492, 'eval_samples_per_second': 100.511, 'eval_steps_per_second': 6.332, 'epoch': 0.32}
{'loss': 0.9101, 'grad_norm': 0.4758511781692505, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3092353343963623, 'eval_runtime': 9.9286, 'eval_samples_per_second': 100.719, 'eval_steps_per_second': 6.345, 'epoch': 0.36}
{'loss': 0.9543, 'grad_norm': 0.6058410406112671, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3021936416625977, 'eval_runtime': 9.9622, 'eval_samples_per_second': 100.38, 'eval_steps_per_second': 6.324, 'epoch': 0.4}
{'loss': 0.9064, 'grad_norm': 0.4992564618587494, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.309783935546875, 'eval_runtime': 9.9656, 'eval_samples_per_second': 100.345, 'eval_steps_per_second': 6.322, 'epoch': 0.44}
{'loss': 0.9075, 'grad_norm': 0.5815911293029785, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3014628887176514, 'eval_runtime': 9.9522, 'eval_samples_per_second': 100.48, 'eval_steps_per_second': 6.33, 'epoch': 0.48}
{'loss': 0.9252, 'grad_norm': 0.4575534760951996, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3011375665664673, 'eval_runtime': 9.9467, 'eval_samples_per_second': 100.536, 'eval_steps_per_second': 6.334, 'epoch': 0.52}
{'loss': 0.8543, 'grad_norm': 0.5501767992973328, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3055778741836548, 'eval_runtime': 9.9505, 'eval_samples_per_second': 100.497, 'eval_steps_per_second': 6.331, 'epoch': 0.56}
{'loss': 0.8248, 'grad_norm': 0.5415235161781311, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3021225929260254, 'eval_runtime': 9.9758, 'eval_samples_per_second': 100.242, 'eval_steps_per_second': 6.315, 'epoch': 0.6}
{'loss': 0.8738, 'grad_norm': 0.5419549942016602, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2951815128326416, 'eval_runtime': 9.9631, 'eval_samples_per_second': 100.37, 'eval_steps_per_second': 6.323, 'epoch': 0.64}
{'loss': 0.8088, 'grad_norm': 0.6189743280410767, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.295670509338379, 'eval_runtime': 9.9672, 'eval_samples_per_second': 100.329, 'eval_steps_per_second': 6.321, 'epoch': 0.68}
{'loss': 0.8305, 'grad_norm': 0.46315068006515503, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.297979474067688, 'eval_runtime': 9.9579, 'eval_samples_per_second': 100.423, 'eval_steps_per_second': 6.327, 'epoch': 0.72}
{'loss': 0.8055, 'grad_norm': 0.5928473472595215, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2946165800094604, 'eval_runtime': 9.9703, 'eval_samples_per_second': 100.298, 'eval_steps_per_second': 6.319, 'epoch': 0.76}
{'loss': 0.7832, 'grad_norm': 0.6778614521026611, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2912282943725586, 'eval_runtime': 9.9613, 'eval_samples_per_second': 100.389, 'eval_steps_per_second': 6.325, 'epoch': 0.8}
{'loss': 0.8173, 'grad_norm': 0.5386195182800293, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2934426069259644, 'eval_runtime': 10.0036, 'eval_samples_per_second': 99.964, 'eval_steps_per_second': 6.298, 'epoch': 0.84}
{'loss': 0.766, 'grad_norm': 0.5174582004547119, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.291537880897522, 'eval_runtime': 9.9771, 'eval_samples_per_second': 100.229, 'eval_steps_per_second': 6.314, 'epoch': 0.88}
{'loss': 0.7794, 'grad_norm': 0.6331081986427307, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2932285070419312, 'eval_runtime': 9.9729, 'eval_samples_per_second': 100.272, 'eval_steps_per_second': 6.317, 'epoch': 0.92}
{'loss': 0.7734, 'grad_norm': 0.5327174067497253, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2895137071609497, 'eval_runtime': 9.9381, 'eval_samples_per_second': 100.623, 'eval_steps_per_second': 6.339, 'epoch': 0.96}
{'loss': 0.8337, 'grad_norm': 0.7365594506263733, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2891234159469604, 'eval_runtime': 9.9021, 'eval_samples_per_second': 100.989, 'eval_steps_per_second': 6.362, 'epoch': 1.0}
{'train_runtime': 442.9645, 'train_samples_per_second': 22.566, 'train_steps_per_second': 1.411, 'train_loss': 0.986669580078125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8900076150894165, 1.5908414125442505, 1.4960196018218994, 1.3740782737731934, 1.3671400547027588, 1.33103609085083, 1.3219678401947021, 1.3142399787902832, 1.3092353343963623, 1.3021936416625977, 1.309783935546875, 1.3014628887176514, 1.3011375665664673, 1.3055778741836548, 1.3021225929260254, 1.2951815128326416, 1.295670509338379, 1.297979474067688, 1.2946165800094604, 1.2912282943725586, 1.2934426069259644, 1.291537880897522, 1.2932285070419312, 1.2895137071609497, 1.2891234159469604], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8900076150894165, 1.5908414125442505, 1.4960196018218994, 1.3740782737731934, 1.3671400547027588, 1.33103609085083, 1.3219678401947021, 1.3142399787902832, 1.3092353343963623, 1.3021936416625977, 1.309783935546875, 1.3014628887176514, 1.3011375665664673, 1.3055778741836548, 1.3021225929260254, 1.2951815128326416, 1.295670509338379, 1.297979474067688, 1.2946165800094604, 1.2912282943725586, 1.2934426069259644, 1.291537880897522, 1.2932285070419312, 1.2895137071609497, 1.2891234159469604]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2891234159469604
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6773 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.649029572099859
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.7057079665904207
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.6972730974242434
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.7057136138914185
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.7057079767901511
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0937, dtype=torch.float64), tensor(0.0780, dtype=torch.float64), tensor(0.0541, dtype=torch.float64), tensor(0.1019, dtype=torch.float64), tensor(0.0979, dtype=torch.float64), tensor(0.0337, dtype=torch.float64), tensor(0.0187, dtype=torch.float64), tensor(0.1704, dtype=torch.float64), tensor(0.3516, dtype=torch.float64), 20, 0, 1, 0, 0, 0, 110, 0.020693833849474735, 33.59545163686889, 0]
normalized proposed parameters for next round by BO: [tensor(0.0937, dtype=torch.float64), tensor(0.0780, dtype=torch.float64), tensor(0.0541, dtype=torch.float64), tensor(0.1019, dtype=torch.float64), tensor(0.0979, dtype=torch.float64), tensor(0.0337, dtype=torch.float64), tensor(0.0187, dtype=torch.float64), tensor(0.1704, dtype=torch.float64), tensor(0.3516, dtype=torch.float64), tensor(0.6382, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8564, dtype=torch.float64), tensor(0.2069, dtype=torch.float64), tensor(0.6999, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.094
  gsm8k: 0.078
  rowan_hellaswag: 0.054
  sciq: 0.102
  triviaqa: 0.098
  truthfulqa_gen: 0.034
  wikitext: 0.019
  mmlu: 0.17
  arc_challenge: 0.352

LoRA Parameters:
  lora_r: (110,)
  lora_dropout: (0.020693833849474735,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (33.59545163686889,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  110
lora dropout:  0.020693833849474735
lora alpha:  33.59545163686889
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 11,264,000 || all params: 8,041,525,248 || trainable%: 0.1401
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6387, 'grad_norm': 0.9442320466041565, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.23370361328125, 'eval_runtime': 8.7383, 'eval_samples_per_second': 114.439, 'eval_steps_per_second': 7.21, 'epoch': 0.04}
{'loss': 1.8929, 'grad_norm': 0.36242738366127014, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6750404834747314, 'eval_runtime': 8.7633, 'eval_samples_per_second': 114.113, 'eval_steps_per_second': 7.189, 'epoch': 0.08}
{'loss': 1.5651, 'grad_norm': 0.2926464080810547, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6263054609298706, 'eval_runtime': 8.8022, 'eval_samples_per_second': 113.609, 'eval_steps_per_second': 7.157, 'epoch': 0.12}
{'loss': 1.3628, 'grad_norm': 0.26070454716682434, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5829825401306152, 'eval_runtime': 8.813, 'eval_samples_per_second': 113.468, 'eval_steps_per_second': 7.149, 'epoch': 0.16}
{'loss': 1.3393, 'grad_norm': 0.2784798741340637, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.571133017539978, 'eval_runtime': 8.856, 'eval_samples_per_second': 112.917, 'eval_steps_per_second': 7.114, 'epoch': 0.2}
{'loss': 1.295, 'grad_norm': 0.4256056845188141, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5331027507781982, 'eval_runtime': 8.8382, 'eval_samples_per_second': 113.145, 'eval_steps_per_second': 7.128, 'epoch': 0.24}
{'loss': 1.2531, 'grad_norm': 0.3367888629436493, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5161409378051758, 'eval_runtime': 8.861, 'eval_samples_per_second': 112.855, 'eval_steps_per_second': 7.11, 'epoch': 0.28}
{'loss': 1.2938, 'grad_norm': 0.2529045343399048, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4819403886795044, 'eval_runtime': 8.8478, 'eval_samples_per_second': 113.022, 'eval_steps_per_second': 7.12, 'epoch': 0.32}
{'loss': 1.2417, 'grad_norm': 0.29308581352233887, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.472832441329956, 'eval_runtime': 8.8522, 'eval_samples_per_second': 112.966, 'eval_steps_per_second': 7.117, 'epoch': 0.36}
{'loss': 1.2012, 'grad_norm': 0.3683425486087799, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4645251035690308, 'eval_runtime': 8.8492, 'eval_samples_per_second': 113.004, 'eval_steps_per_second': 7.119, 'epoch': 0.4}
{'loss': 1.2493, 'grad_norm': 0.3248206377029419, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4264353513717651, 'eval_runtime': 8.859, 'eval_samples_per_second': 112.88, 'eval_steps_per_second': 7.111, 'epoch': 0.44}
{'loss': 1.2011, 'grad_norm': 0.2901105284690857, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4188506603240967, 'eval_runtime': 8.8471, 'eval_samples_per_second': 113.032, 'eval_steps_per_second': 7.121, 'epoch': 0.48}
{'loss': 1.1933, 'grad_norm': 0.3759855329990387, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.389760971069336, 'eval_runtime': 8.8499, 'eval_samples_per_second': 112.995, 'eval_steps_per_second': 7.119, 'epoch': 0.52}
{'loss': 1.1245, 'grad_norm': 0.3638545870780945, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.383785605430603, 'eval_runtime': 8.8497, 'eval_samples_per_second': 112.999, 'eval_steps_per_second': 7.119, 'epoch': 0.56}
{'loss': 1.1685, 'grad_norm': 0.349602073431015, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3708608150482178, 'eval_runtime': 8.8393, 'eval_samples_per_second': 113.131, 'eval_steps_per_second': 7.127, 'epoch': 0.6}
{'loss': 1.097, 'grad_norm': 0.2925702929496765, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.364078164100647, 'eval_runtime': 8.8522, 'eval_samples_per_second': 112.966, 'eval_steps_per_second': 7.117, 'epoch': 0.64}
{'loss': 1.1131, 'grad_norm': 0.246650829911232, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3652101755142212, 'eval_runtime': 8.8418, 'eval_samples_per_second': 113.099, 'eval_steps_per_second': 7.125, 'epoch': 0.68}
{'loss': 1.1397, 'grad_norm': 0.2702866494655609, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3602049350738525, 'eval_runtime': 8.8512, 'eval_samples_per_second': 112.979, 'eval_steps_per_second': 7.118, 'epoch': 0.72}
{'loss': 1.0459, 'grad_norm': 0.2802594304084778, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3603416681289673, 'eval_runtime': 8.8508, 'eval_samples_per_second': 112.985, 'eval_steps_per_second': 7.118, 'epoch': 0.76}
{'loss': 1.0683, 'grad_norm': 0.3176601529121399, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.359540581703186, 'eval_runtime': 8.8551, 'eval_samples_per_second': 112.929, 'eval_steps_per_second': 7.115, 'epoch': 0.8}
{'loss': 1.0646, 'grad_norm': 0.3441210389137268, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.355929970741272, 'eval_runtime': 8.8582, 'eval_samples_per_second': 112.889, 'eval_steps_per_second': 7.112, 'epoch': 0.84}
{'loss': 1.1544, 'grad_norm': 0.3463118374347687, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3547505140304565, 'eval_runtime': 8.8526, 'eval_samples_per_second': 112.962, 'eval_steps_per_second': 7.117, 'epoch': 0.88}
{'loss': 1.1195, 'grad_norm': 0.2953733503818512, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3512659072875977, 'eval_runtime': 8.8516, 'eval_samples_per_second': 112.974, 'eval_steps_per_second': 7.117, 'epoch': 0.92}
{'loss': 1.0667, 'grad_norm': 0.30493661761283875, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3519688844680786, 'eval_runtime': 8.8709, 'eval_samples_per_second': 112.729, 'eval_steps_per_second': 7.102, 'epoch': 0.96}
{'loss': 1.1086, 'grad_norm': 0.2980954349040985, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.352358341217041, 'eval_runtime': 8.8475, 'eval_samples_per_second': 113.026, 'eval_steps_per_second': 7.121, 'epoch': 1.0}
{'train_runtime': 378.1432, 'train_samples_per_second': 26.432, 'train_steps_per_second': 1.653, 'train_loss': 1.3199222717285157, 'epoch': 1.0}
train_results:  {'eval_loss': [2.23370361328125, 1.6750404834747314, 1.6263054609298706, 1.5829825401306152, 1.571133017539978, 1.5331027507781982, 1.5161409378051758, 1.4819403886795044, 1.472832441329956, 1.4645251035690308, 1.4264353513717651, 1.4188506603240967, 1.389760971069336, 1.383785605430603, 1.3708608150482178, 1.364078164100647, 1.3652101755142212, 1.3602049350738525, 1.3603416681289673, 1.359540581703186, 1.355929970741272, 1.3547505140304565, 1.3512659072875977, 1.3519688844680786, 1.352358341217041], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.23370361328125, 1.6750404834747314, 1.6263054609298706, 1.5829825401306152, 1.571133017539978, 1.5331027507781982, 1.5161409378051758, 1.4819403886795044, 1.472832441329956, 1.4645251035690308, 1.4264353513717651, 1.4188506603240967, 1.389760971069336, 1.383785605430603, 1.3708608150482178, 1.364078164100647, 1.3652101755142212, 1.3602049350738525, 1.3603416681289673, 1.359540581703186, 1.355929970741272, 1.3547505140304565, 1.3512659072875977, 1.3519688844680786, 1.352358341217041]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.352358341217041
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2413 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.8002709591338328
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.7796859139226064
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.7796335872597814
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.7819786402950097
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.7798874707780747
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0448, dtype=torch.float64), 0, tensor(0.0537, dtype=torch.float64), tensor(0.1494, dtype=torch.float64), tensor(0.3912, dtype=torch.float64), 0, 0, 0, tensor(0.3608, dtype=torch.float64), 20, 0, 0, 1, 0, 0, 107, 1.5178830414797064e-19, 36.65335984963713, 0]
normalized proposed parameters for next round by BO: [tensor(0.0448, dtype=torch.float64), tensor(1.8822e-18, dtype=torch.float64), tensor(0.0537, dtype=torch.float64), tensor(0.1494, dtype=torch.float64), tensor(0.3912, dtype=torch.float64), tensor(1.6519e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.4601e-18, dtype=torch.float64), tensor(0.3608, dtype=torch.float64), tensor(0.6373, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8385, dtype=torch.float64), tensor(1.5179e-18, dtype=torch.float64), tensor(0.7636, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.045
  gsm8k: 0
  rowan_hellaswag: 0.054
  sciq: 0.149
  triviaqa: 0.391
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.361

LoRA Parameters:
  lora_r: (107,)
  lora_dropout: (1.5178830414797064e-19,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (36.65335984963713,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  107
lora dropout:  1.5178830414797064e-19
lora alpha:  36.65335984963713
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 39,444,480 || all params: 8,069,705,728 || trainable%: 0.4888
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6078, 'grad_norm': 0.6919269561767578, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.034275770187378, 'eval_runtime': 9.108, 'eval_samples_per_second': 109.793, 'eval_steps_per_second': 6.917, 'epoch': 0.04}
{'loss': 1.5996, 'grad_norm': 0.2450494021177292, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.8561387062072754, 'eval_runtime': 9.1723, 'eval_samples_per_second': 109.023, 'eval_steps_per_second': 6.868, 'epoch': 0.08}
{'loss': 1.3818, 'grad_norm': 0.23886366188526154, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9339848756790161, 'eval_runtime': 9.1951, 'eval_samples_per_second': 108.754, 'eval_steps_per_second': 6.851, 'epoch': 0.12}
{'loss': 1.2089, 'grad_norm': 0.2538019120693207, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8626874685287476, 'eval_runtime': 9.2156, 'eval_samples_per_second': 108.512, 'eval_steps_per_second': 6.836, 'epoch': 0.16}
{'loss': 1.1701, 'grad_norm': 0.20281744003295898, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8216285705566406, 'eval_runtime': 9.2082, 'eval_samples_per_second': 108.599, 'eval_steps_per_second': 6.842, 'epoch': 0.2}
{'loss': 1.0789, 'grad_norm': 0.24909980595111847, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8479262590408325, 'eval_runtime': 9.2183, 'eval_samples_per_second': 108.48, 'eval_steps_per_second': 6.834, 'epoch': 0.24}
{'loss': 1.1185, 'grad_norm': 0.2026548832654953, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7836552858352661, 'eval_runtime': 9.2268, 'eval_samples_per_second': 108.38, 'eval_steps_per_second': 6.828, 'epoch': 0.28}
{'loss': 0.9658, 'grad_norm': 0.270546555519104, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7907755374908447, 'eval_runtime': 9.2374, 'eval_samples_per_second': 108.256, 'eval_steps_per_second': 6.82, 'epoch': 0.32}
{'loss': 1.0501, 'grad_norm': 0.20795071125030518, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8136484622955322, 'eval_runtime': 9.2377, 'eval_samples_per_second': 108.253, 'eval_steps_per_second': 6.82, 'epoch': 0.36}
{'loss': 1.028, 'grad_norm': 0.3255373537540436, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8420186042785645, 'eval_runtime': 9.2441, 'eval_samples_per_second': 108.177, 'eval_steps_per_second': 6.815, 'epoch': 0.4}
{'loss': 0.9825, 'grad_norm': 0.22243422269821167, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.798467993736267, 'eval_runtime': 9.2327, 'eval_samples_per_second': 108.311, 'eval_steps_per_second': 6.824, 'epoch': 0.44}
{'loss': 0.971, 'grad_norm': 0.23757098615169525, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8303709030151367, 'eval_runtime': 9.2316, 'eval_samples_per_second': 108.324, 'eval_steps_per_second': 6.824, 'epoch': 0.48}
{'loss': 0.9756, 'grad_norm': 0.2808307111263275, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8316972255706787, 'eval_runtime': 9.2428, 'eval_samples_per_second': 108.192, 'eval_steps_per_second': 6.816, 'epoch': 0.52}
{'loss': 0.9287, 'grad_norm': 0.2646961212158203, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8427109718322754, 'eval_runtime': 9.2367, 'eval_samples_per_second': 108.263, 'eval_steps_per_second': 6.821, 'epoch': 0.56}
{'loss': 1.0156, 'grad_norm': 0.29458484053611755, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8799232244491577, 'eval_runtime': 9.2448, 'eval_samples_per_second': 108.169, 'eval_steps_per_second': 6.815, 'epoch': 0.6}
{'loss': 0.9929, 'grad_norm': 0.2616930603981018, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8706755638122559, 'eval_runtime': 9.2433, 'eval_samples_per_second': 108.186, 'eval_steps_per_second': 6.816, 'epoch': 0.64}
{'loss': 0.9246, 'grad_norm': 0.28243568539619446, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8700206279754639, 'eval_runtime': 9.2436, 'eval_samples_per_second': 108.183, 'eval_steps_per_second': 6.816, 'epoch': 0.68}
{'loss': 0.9117, 'grad_norm': 0.34753233194351196, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8613022565841675, 'eval_runtime': 9.2913, 'eval_samples_per_second': 107.627, 'eval_steps_per_second': 6.781, 'epoch': 0.72}
{'loss': 1.0067, 'grad_norm': 0.29901203513145447, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9018808603286743, 'eval_runtime': 9.3024, 'eval_samples_per_second': 107.499, 'eval_steps_per_second': 6.772, 'epoch': 0.76}
{'loss': 0.9021, 'grad_norm': 0.34073585271835327, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.907684564590454, 'eval_runtime': 9.2994, 'eval_samples_per_second': 107.534, 'eval_steps_per_second': 6.775, 'epoch': 0.8}
{'loss': 0.8727, 'grad_norm': 0.37603408098220825, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9323192834854126, 'eval_runtime': 9.2917, 'eval_samples_per_second': 107.623, 'eval_steps_per_second': 6.78, 'epoch': 0.84}
{'loss': 0.8551, 'grad_norm': 0.44012850522994995, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.949895977973938, 'eval_runtime': 9.2946, 'eval_samples_per_second': 107.589, 'eval_steps_per_second': 6.778, 'epoch': 0.88}
{'loss': 0.8554, 'grad_norm': 0.3184719979763031, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.92094087600708, 'eval_runtime': 9.2968, 'eval_samples_per_second': 107.564, 'eval_steps_per_second': 6.777, 'epoch': 0.92}
{'loss': 0.857, 'grad_norm': 0.3300763666629791, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.951525092124939, 'eval_runtime': 9.293, 'eval_samples_per_second': 107.608, 'eval_steps_per_second': 6.779, 'epoch': 0.96}
{'loss': 0.9111, 'grad_norm': 0.3906451463699341, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.948819875717163, 'eval_runtime': 9.2883, 'eval_samples_per_second': 107.662, 'eval_steps_per_second': 6.783, 'epoch': 1.0}
{'train_runtime': 365.8922, 'train_samples_per_second': 27.325, 'train_steps_per_second': 1.708, 'train_loss': 1.1269041625976564, 'epoch': 1.0}
train_results:  {'eval_loss': [2.034275770187378, 1.8561387062072754, 1.9339848756790161, 1.8626874685287476, 1.8216285705566406, 1.8479262590408325, 1.7836552858352661, 1.7907755374908447, 1.8136484622955322, 1.8420186042785645, 1.798467993736267, 1.8303709030151367, 1.8316972255706787, 1.8427109718322754, 1.8799232244491577, 1.8706755638122559, 1.8700206279754639, 1.8613022565841675, 1.9018808603286743, 1.907684564590454, 1.9323192834854126, 1.949895977973938, 1.92094087600708, 1.951525092124939, 1.948819875717163], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.034275770187378, 1.8561387062072754, 1.9339848756790161, 1.8626874685287476, 1.8216285705566406, 1.8479262590408325, 1.7836552858352661, 1.7907755374908447, 1.8136484622955322, 1.8420186042785645, 1.798467993736267, 1.8303709030151367, 1.8316972255706787, 1.8427109718322754, 1.8799232244491577, 1.8706755638122559, 1.8700206279754639, 1.8613022565841675, 1.9018808603286743, 1.907684564590454, 1.9323192834854126, 1.949895977973938, 1.92094087600708, 1.951525092124939, 1.948819875717163]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.948819875717163
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.2837 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.7842995459061908
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.7840352070261067
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.7840351644810708
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.7840659561205795
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.7840398889432502
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0319, dtype=torch.float64), tensor(0.1465, dtype=torch.float64), tensor(0.0610, dtype=torch.float64), tensor(0.1200, dtype=torch.float64), 0, 0, tensor(0.0267, dtype=torch.float64), tensor(0.2905, dtype=torch.float64), tensor(0.3170, dtype=torch.float64), 21, 0, 1, 0, 1, 0, 103, 0.05732655764013649, 29.080316769873598, 0]
normalized proposed parameters for next round by BO: [tensor(0.0319, dtype=torch.float64), tensor(0.1465, dtype=torch.float64), tensor(0.0610, dtype=torch.float64), tensor(0.1200, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0065, dtype=torch.float64), tensor(0.0267, dtype=torch.float64), tensor(0.2905, dtype=torch.float64), tensor(0.3170, dtype=torch.float64), tensor(0.6685, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8024, dtype=torch.float64), tensor(0.5733, dtype=torch.float64), tensor(0.6058, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.032
  gsm8k: 0.147
  rowan_hellaswag: 0.061
  sciq: 0.12
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.027
  mmlu: 0.29
  arc_challenge: 0.317

LoRA Parameters:
  lora_r: (103,)
  lora_dropout: (0.05732655764013649,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (29.080316769873598,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  103
lora dropout:  0.05732655764013649
lora alpha:  29.080316769873598
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 50,942,976 || all params: 8,081,204,224 || trainable%: 0.6304
length of training data:  9931
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0042, 'grad_norm': 0.8261402249336243, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8336551189422607, 'eval_runtime': 9.4985, 'eval_samples_per_second': 105.279, 'eval_steps_per_second': 6.633, 'epoch': 0.04}
{'loss': 1.5178, 'grad_norm': 0.33373865485191345, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4562448263168335, 'eval_runtime': 9.5177, 'eval_samples_per_second': 105.067, 'eval_steps_per_second': 6.619, 'epoch': 0.08}
{'loss': 1.2796, 'grad_norm': 0.20456674695014954, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.3854172229766846, 'eval_runtime': 9.5853, 'eval_samples_per_second': 104.327, 'eval_steps_per_second': 6.573, 'epoch': 0.12}
{'loss': 1.1644, 'grad_norm': 0.22937379777431488, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.3563334941864014, 'eval_runtime': 9.626, 'eval_samples_per_second': 103.886, 'eval_steps_per_second': 6.545, 'epoch': 0.16}
{'loss': 1.163, 'grad_norm': 0.22203058004379272, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.3426902294158936, 'eval_runtime': 9.6218, 'eval_samples_per_second': 103.93, 'eval_steps_per_second': 6.548, 'epoch': 0.2}
{'loss': 1.1321, 'grad_norm': 0.2028104066848755, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.3213942050933838, 'eval_runtime': 9.6196, 'eval_samples_per_second': 103.954, 'eval_steps_per_second': 6.549, 'epoch': 0.24}
{'loss': 1.1878, 'grad_norm': 0.24843904376029968, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.310214638710022, 'eval_runtime': 9.6337, 'eval_samples_per_second': 103.802, 'eval_steps_per_second': 6.54, 'epoch': 0.28}
{'loss': 1.1065, 'grad_norm': 0.1813814342021942, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.3037055730819702, 'eval_runtime': 9.6328, 'eval_samples_per_second': 103.812, 'eval_steps_per_second': 6.54, 'epoch': 0.32}
{'loss': 1.0792, 'grad_norm': 0.18270748853683472, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.304071068763733, 'eval_runtime': 9.6368, 'eval_samples_per_second': 103.769, 'eval_steps_per_second': 6.537, 'epoch': 0.36}
{'loss': 1.1351, 'grad_norm': 0.21619784832000732, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.3021847009658813, 'eval_runtime': 9.6322, 'eval_samples_per_second': 103.818, 'eval_steps_per_second': 6.541, 'epoch': 0.4}
{'loss': 1.0465, 'grad_norm': 0.20067870616912842, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.2995290756225586, 'eval_runtime': 9.6252, 'eval_samples_per_second': 103.894, 'eval_steps_per_second': 6.545, 'epoch': 0.44}
{'loss': 1.1691, 'grad_norm': 0.21070213615894318, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.2952075004577637, 'eval_runtime': 9.617, 'eval_samples_per_second': 103.982, 'eval_steps_per_second': 6.551, 'epoch': 0.48}
{'loss': 1.1195, 'grad_norm': 0.20023669302463531, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.2927734851837158, 'eval_runtime': 9.6098, 'eval_samples_per_second': 104.061, 'eval_steps_per_second': 6.556, 'epoch': 0.52}
{'loss': 1.0849, 'grad_norm': 0.211256742477417, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.2912911176681519, 'eval_runtime': 9.573, 'eval_samples_per_second': 104.46, 'eval_steps_per_second': 6.581, 'epoch': 0.56}
{'loss': 1.0869, 'grad_norm': 0.20010416209697723, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.2925859689712524, 'eval_runtime': 9.5927, 'eval_samples_per_second': 104.246, 'eval_steps_per_second': 6.567, 'epoch': 0.6}
{'loss': 1.0863, 'grad_norm': 0.21153923869132996, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.2904272079467773, 'eval_runtime': 9.5834, 'eval_samples_per_second': 104.347, 'eval_steps_per_second': 6.574, 'epoch': 0.64}
{'loss': 1.0543, 'grad_norm': 0.20942631363868713, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.288187861442566, 'eval_runtime': 9.5878, 'eval_samples_per_second': 104.299, 'eval_steps_per_second': 6.571, 'epoch': 0.68}
{'loss': 1.1413, 'grad_norm': 0.2086697667837143, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.2860490083694458, 'eval_runtime': 9.6229, 'eval_samples_per_second': 103.918, 'eval_steps_per_second': 6.547, 'epoch': 0.72}
{'loss': 1.108, 'grad_norm': 0.23423048853874207, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.2821156978607178, 'eval_runtime': 9.6198, 'eval_samples_per_second': 103.952, 'eval_steps_per_second': 6.549, 'epoch': 0.76}
{'loss': 1.1342, 'grad_norm': 0.2050170749425888, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.2831549644470215, 'eval_runtime': 9.6182, 'eval_samples_per_second': 103.97, 'eval_steps_per_second': 6.55, 'epoch': 0.81}
{'loss': 1.105, 'grad_norm': 0.2122778445482254, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.2842437028884888, 'eval_runtime': 9.6199, 'eval_samples_per_second': 103.952, 'eval_steps_per_second': 6.549, 'epoch': 0.85}
{'loss': 1.1033, 'grad_norm': 0.2517690658569336, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.2817732095718384, 'eval_runtime': 9.6261, 'eval_samples_per_second': 103.884, 'eval_steps_per_second': 6.545, 'epoch': 0.89}
{'loss': 1.1252, 'grad_norm': 0.1958894282579422, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.2804099321365356, 'eval_runtime': 9.6299, 'eval_samples_per_second': 103.843, 'eval_steps_per_second': 6.542, 'epoch': 0.93}
{'loss': 1.0608, 'grad_norm': 0.18819598853588104, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.2800376415252686, 'eval_runtime': 9.6201, 'eval_samples_per_second': 103.949, 'eval_steps_per_second': 6.549, 'epoch': 0.97}
{'train_runtime': 413.614, 'train_samples_per_second': 24.01, 'train_steps_per_second': 1.501, 'train_loss': 1.210911368402306, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8336551189422607, 1.4562448263168335, 1.3854172229766846, 1.3563334941864014, 1.3426902294158936, 1.3213942050933838, 1.310214638710022, 1.3037055730819702, 1.304071068763733, 1.3021847009658813, 1.2995290756225586, 1.2952075004577637, 1.2927734851837158, 1.2912911176681519, 1.2925859689712524, 1.2904272079467773, 1.288187861442566, 1.2860490083694458, 1.2821156978607178, 1.2831549644470215, 1.2842437028884888, 1.2817732095718384, 1.2804099321365356, 1.2800376415252686], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.8336551189422607, 1.4562448263168335, 1.3854172229766846, 1.3563334941864014, 1.3426902294158936, 1.3213942050933838, 1.310214638710022, 1.3037055730819702, 1.304071068763733, 1.3021847009658813, 1.2995290756225586, 1.2952075004577637, 1.2927734851837158, 1.2912911176681519, 1.2925859689712524, 1.2904272079467773, 1.288187861442566, 1.2860490083694458, 1.2821156978607178, 1.2831549644470215, 1.2842437028884888, 1.2817732095718384, 1.2804099321365356, 1.2800376415252686]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2800376415252686
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.4292 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.20075875520706177, 0.2661306858062744, 0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8751383423805237, 0.10287010669708252, 0.1595216989517212, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.02520020864903927, 0.987917423248291, 0.7536473274230957, 0.3050987720489502]  ‚Üí  acq = -0.6899190632645328
X = [0.37903499603271484, 0.49315524101257324, 0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.2505585253238678, 0.6847650408744812, 0.7922602295875549, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.23907232284545898, 0.9114632606506348, 0.9043515920639038, 0.9609596729278564]  ‚Üí  acq = -0.6899164190281456
X = [0.9641072154045105, 0.4331236481666565, 0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.4157118797302246, 0.0375140905380249, 0.23879104852676392, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.47058141231536865, 0.22714883089065552, 0.2083243578672409, 0.21395939588546753]  ‚Üí  acq = -0.6899164190396373
X = [0.8797377347946167, 0.2856326103210449, 0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2817014753818512, 0.0661017894744873, 0.16436314582824707, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5602686405181885, 0.5082452893257141, 0.49900704622268677, 0.5792016983032227]  ‚Üí  acq = -0.6899164472622933
X = [0.30917781591415405, 0.6994286775588989, 0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.7001289129257202, 0.6122615933418274, 0.49610579013824463, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8769228458404541, 0.1870233416557312, 0.19417396187782288, 0.6010990738868713]  ‚Üí  acq = -0.6899170792274106
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0754, dtype=torch.float64), tensor(0.2494, dtype=torch.float64), tensor(0.0335, dtype=torch.float64), tensor(0.1246, dtype=torch.float64), 0, tensor(0.0369, dtype=torch.float64), tensor(0.0866, dtype=torch.float64), tensor(0.1205, dtype=torch.float64), tensor(0.2731, dtype=torch.float64), 25, 0, 1, 0, 0, 1, 109, 7.702380480917157e-20, 26.750225304509414, 0]
normalized proposed parameters for next round by BO: [tensor(0.0754, dtype=torch.float64), tensor(0.2494, dtype=torch.float64), tensor(0.0335, dtype=torch.float64), tensor(0.1246, dtype=torch.float64), tensor(9.3015e-19, dtype=torch.float64), tensor(0.0369, dtype=torch.float64), tensor(0.0866, dtype=torch.float64), tensor(0.1205, dtype=torch.float64), tensor(0.2731, dtype=torch.float64), tensor(0.7833, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8495, dtype=torch.float64), tensor(7.7024e-19, dtype=torch.float64), tensor(0.5573, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.075
  gsm8k: 0.249
  rowan_hellaswag: 0.034
  sciq: 0.125
  triviaqa: 0
  truthfulqa_gen: 0.037
  wikitext: 0.087
  mmlu: 0.12
  arc_challenge: 0.273

LoRA Parameters:
  lora_r: (109,)
  lora_dropout: (7.702380480917157e-20,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (26.750225304509414,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  109
lora dropout:  7.702380480917157e-20
lora alpha:  26.750225304509414
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 64,179,200 || all params: 8,094,440,448 || trainable%: 0.7929
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9875, 'grad_norm': 0.42453235387802124, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.039130210876465, 'eval_runtime': 9.5219, 'eval_samples_per_second': 105.022, 'eval_steps_per_second': 6.616, 'epoch': 0.04}
{'loss': 1.4972, 'grad_norm': 0.20590649545192719, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.603322982788086, 'eval_runtime': 9.5262, 'eval_samples_per_second': 104.974, 'eval_steps_per_second': 6.613, 'epoch': 0.08}
{'loss': 1.3069, 'grad_norm': 0.1720293015241623, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5486482381820679, 'eval_runtime': 9.5508, 'eval_samples_per_second': 104.703, 'eval_steps_per_second': 6.596, 'epoch': 0.12}
{'loss': 1.2125, 'grad_norm': 0.17975397408008575, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.468582034111023, 'eval_runtime': 9.5849, 'eval_samples_per_second': 104.33, 'eval_steps_per_second': 6.573, 'epoch': 0.16}
{'loss': 1.1583, 'grad_norm': 0.2801699936389923, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4527357816696167, 'eval_runtime': 9.598, 'eval_samples_per_second': 104.189, 'eval_steps_per_second': 6.564, 'epoch': 0.2}
{'loss': 1.1174, 'grad_norm': 0.19138629734516144, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3753602504730225, 'eval_runtime': 9.5973, 'eval_samples_per_second': 104.196, 'eval_steps_per_second': 6.564, 'epoch': 0.24}
{'loss': 1.1221, 'grad_norm': 0.1525581330060959, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3292338848114014, 'eval_runtime': 9.6197, 'eval_samples_per_second': 103.953, 'eval_steps_per_second': 6.549, 'epoch': 0.28}
{'loss': 1.0215, 'grad_norm': 0.15989959239959717, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3213638067245483, 'eval_runtime': 9.613, 'eval_samples_per_second': 104.026, 'eval_steps_per_second': 6.554, 'epoch': 0.32}
{'loss': 1.0188, 'grad_norm': 0.17796815931797028, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3197075128555298, 'eval_runtime': 9.6325, 'eval_samples_per_second': 103.815, 'eval_steps_per_second': 6.54, 'epoch': 0.36}
{'loss': 1.051, 'grad_norm': 0.1733214110136032, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3152223825454712, 'eval_runtime': 9.6252, 'eval_samples_per_second': 103.894, 'eval_steps_per_second': 6.545, 'epoch': 0.4}
{'loss': 1.0241, 'grad_norm': 0.2125530242919922, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3127788305282593, 'eval_runtime': 9.6311, 'eval_samples_per_second': 103.83, 'eval_steps_per_second': 6.541, 'epoch': 0.44}
{'loss': 1.0912, 'grad_norm': 0.20136478543281555, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3108506202697754, 'eval_runtime': 9.6276, 'eval_samples_per_second': 103.868, 'eval_steps_per_second': 6.544, 'epoch': 0.48}
{'loss': 0.9958, 'grad_norm': 0.16786392033100128, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3099863529205322, 'eval_runtime': 9.6442, 'eval_samples_per_second': 103.689, 'eval_steps_per_second': 6.532, 'epoch': 0.52}
{'loss': 1.0482, 'grad_norm': 0.18737711012363434, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3105405569076538, 'eval_runtime': 9.6301, 'eval_samples_per_second': 103.841, 'eval_steps_per_second': 6.542, 'epoch': 0.56}
{'loss': 1.0746, 'grad_norm': 0.1702338606119156, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3029780387878418, 'eval_runtime': 9.6409, 'eval_samples_per_second': 103.724, 'eval_steps_per_second': 6.535, 'epoch': 0.6}
{'loss': 1.0266, 'grad_norm': 0.20912232995033264, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3026044368743896, 'eval_runtime': 9.6876, 'eval_samples_per_second': 103.225, 'eval_steps_per_second': 6.503, 'epoch': 0.64}
{'loss': 0.9996, 'grad_norm': 0.18967649340629578, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3030954599380493, 'eval_runtime': 9.6697, 'eval_samples_per_second': 103.416, 'eval_steps_per_second': 6.515, 'epoch': 0.68}
{'loss': 1.0226, 'grad_norm': 0.1784432977437973, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3026561737060547, 'eval_runtime': 9.6692, 'eval_samples_per_second': 103.421, 'eval_steps_per_second': 6.516, 'epoch': 0.72}
{'loss': 1.0015, 'grad_norm': 0.22154949605464935, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.300155758857727, 'eval_runtime': 9.6785, 'eval_samples_per_second': 103.322, 'eval_steps_per_second': 6.509, 'epoch': 0.76}
{'loss': 0.9582, 'grad_norm': 0.23663713037967682, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3000397682189941, 'eval_runtime': 9.6642, 'eval_samples_per_second': 103.474, 'eval_steps_per_second': 6.519, 'epoch': 0.8}
{'loss': 1.0028, 'grad_norm': 0.2025773823261261, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2991992235183716, 'eval_runtime': 9.6631, 'eval_samples_per_second': 103.486, 'eval_steps_per_second': 6.52, 'epoch': 0.84}
{'loss': 0.9607, 'grad_norm': 0.1823858767747879, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.298959732055664, 'eval_runtime': 9.6643, 'eval_samples_per_second': 103.474, 'eval_steps_per_second': 6.519, 'epoch': 0.88}
{'loss': 1.0193, 'grad_norm': 0.23592349886894226, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2972626686096191, 'eval_runtime': 9.6615, 'eval_samples_per_second': 103.503, 'eval_steps_per_second': 6.521, 'epoch': 0.92}
{'loss': 0.9784, 'grad_norm': 0.22742368280887604, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2955156564712524, 'eval_runtime': 9.6693, 'eval_samples_per_second': 103.42, 'eval_steps_per_second': 6.515, 'epoch': 0.96}
{'loss': 0.994, 'grad_norm': 0.23589280247688293, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.295063853263855, 'eval_runtime': 9.675, 'eval_samples_per_second': 103.36, 'eval_steps_per_second': 6.512, 'epoch': 1.0}
{'train_runtime': 432.1076, 'train_samples_per_second': 23.133, 'train_steps_per_second': 1.446, 'train_loss': 1.1476265014648437, 'epoch': 1.0}
train_results:  {'eval_loss': [2.039130210876465, 1.603322982788086, 1.5486482381820679, 1.468582034111023, 1.4527357816696167, 1.3753602504730225, 1.3292338848114014, 1.3213638067245483, 1.3197075128555298, 1.3152223825454712, 1.3127788305282593, 1.3108506202697754, 1.3099863529205322, 1.3105405569076538, 1.3029780387878418, 1.3026044368743896, 1.3030954599380493, 1.3026561737060547, 1.300155758857727, 1.3000397682189941, 1.2991992235183716, 1.298959732055664, 1.2972626686096191, 1.2955156564712524, 1.295063853263855], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.039130210876465, 1.603322982788086, 1.5486482381820679, 1.468582034111023, 1.4527357816696167, 1.3753602504730225, 1.3292338848114014, 1.3213638067245483, 1.3197075128555298, 1.3152223825454712, 1.3127788305282593, 1.3108506202697754, 1.3099863529205322, 1.3105405569076538, 1.3029780387878418, 1.3026044368743896, 1.3030954599380493, 1.3026561737060547, 1.300155758857727, 1.3000397682189941, 1.2991992235183716, 1.298959732055664, 1.2972626686096191, 1.2955156564712524, 1.295063853263855]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.295063853263855
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8140 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.8202752739745379
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.8200636222318014
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.8227231216069487
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.8210178298863168
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.8325143128917296
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.0845, dtype=torch.float64), 0, tensor(0.3876, dtype=torch.float64), 0, tensor(0.3691, dtype=torch.float64), tensor(0.1588, dtype=torch.float64), 28, 0, 1, 1, 0, 1, 86, 0.032751345507328285, 26.741034153719994, 0]
normalized proposed parameters for next round by BO: [tensor(7.1919e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.6288e-18, dtype=torch.float64), tensor(0.0845, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3876, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3691, dtype=torch.float64), tensor(0.1588, dtype=torch.float64), tensor(0.8751, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6702, dtype=torch.float64), tensor(0.3275, dtype=torch.float64), tensor(0.5571, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.085
  triviaqa: 0
  truthfulqa_gen: 0.388
  wikitext: 0
  mmlu: 0.369
  arc_challenge: 0.159

LoRA Parameters:
  lora_r: (86,)
  lora_dropout: (0.032751345507328285,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (26.741034153719994,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  86
lora dropout:  0.032751345507328285
lora alpha:  26.741034153719994
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 101,097,472 || all params: 8,131,358,720 || trainable%: 1.2433
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.043, 'grad_norm': 0.6482231616973877, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7571206092834473, 'eval_runtime': 10.0159, 'eval_samples_per_second': 99.842, 'eval_steps_per_second': 6.29, 'epoch': 0.04}
{'loss': 1.4195, 'grad_norm': 0.24568584561347961, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5157381296157837, 'eval_runtime': 10.0243, 'eval_samples_per_second': 99.757, 'eval_steps_per_second': 6.285, 'epoch': 0.08}
{'loss': 1.2199, 'grad_norm': 0.27233046293258667, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.394323468208313, 'eval_runtime': 10.0504, 'eval_samples_per_second': 99.498, 'eval_steps_per_second': 6.268, 'epoch': 0.12}
{'loss': 1.1517, 'grad_norm': 0.23613326251506805, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.347423791885376, 'eval_runtime': 10.0734, 'eval_samples_per_second': 99.271, 'eval_steps_per_second': 6.254, 'epoch': 0.16}
{'loss': 1.1148, 'grad_norm': 0.2559531033039093, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2988358736038208, 'eval_runtime': 10.0914, 'eval_samples_per_second': 99.095, 'eval_steps_per_second': 6.243, 'epoch': 0.2}
{'loss': 0.9842, 'grad_norm': 0.27239781618118286, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2838752269744873, 'eval_runtime': 10.1332, 'eval_samples_per_second': 98.685, 'eval_steps_per_second': 6.217, 'epoch': 0.24}
{'loss': 1.0048, 'grad_norm': 0.2708994448184967, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2778669595718384, 'eval_runtime': 10.1272, 'eval_samples_per_second': 98.744, 'eval_steps_per_second': 6.221, 'epoch': 0.28}
{'loss': 1.0057, 'grad_norm': 0.30492305755615234, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2728941440582275, 'eval_runtime': 10.1146, 'eval_samples_per_second': 98.867, 'eval_steps_per_second': 6.229, 'epoch': 0.32}
{'loss': 0.9878, 'grad_norm': 0.22782586514949799, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2676290273666382, 'eval_runtime': 10.1069, 'eval_samples_per_second': 98.942, 'eval_steps_per_second': 6.233, 'epoch': 0.36}
{'loss': 0.9244, 'grad_norm': 0.2475813627243042, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2657959461212158, 'eval_runtime': 10.1084, 'eval_samples_per_second': 98.928, 'eval_steps_per_second': 6.232, 'epoch': 0.4}
{'loss': 0.9455, 'grad_norm': 0.3616805076599121, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.258608102798462, 'eval_runtime': 10.1134, 'eval_samples_per_second': 98.879, 'eval_steps_per_second': 6.229, 'epoch': 0.44}
{'loss': 0.9264, 'grad_norm': 0.2953813970088959, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2569470405578613, 'eval_runtime': 10.1181, 'eval_samples_per_second': 98.833, 'eval_steps_per_second': 6.226, 'epoch': 0.48}
{'loss': 0.9009, 'grad_norm': 0.24386103451251984, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.254543662071228, 'eval_runtime': 10.1137, 'eval_samples_per_second': 98.875, 'eval_steps_per_second': 6.229, 'epoch': 0.52}
{'loss': 0.8628, 'grad_norm': 0.27774277329444885, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2522567510604858, 'eval_runtime': 10.1069, 'eval_samples_per_second': 98.942, 'eval_steps_per_second': 6.233, 'epoch': 0.56}
{'loss': 0.9123, 'grad_norm': 0.21950072050094604, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2480953931808472, 'eval_runtime': 10.1114, 'eval_samples_per_second': 98.898, 'eval_steps_per_second': 6.231, 'epoch': 0.6}
{'loss': 0.8674, 'grad_norm': 0.2776353657245636, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2453280687332153, 'eval_runtime': 10.1067, 'eval_samples_per_second': 98.945, 'eval_steps_per_second': 6.234, 'epoch': 0.64}
{'loss': 0.8675, 'grad_norm': 0.24790260195732117, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.243502140045166, 'eval_runtime': 10.11, 'eval_samples_per_second': 98.912, 'eval_steps_per_second': 6.231, 'epoch': 0.68}
{'loss': 0.8612, 'grad_norm': 0.2161744087934494, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.242432951927185, 'eval_runtime': 10.1102, 'eval_samples_per_second': 98.91, 'eval_steps_per_second': 6.231, 'epoch': 0.72}
{'loss': 0.8555, 'grad_norm': 0.33967703580856323, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.242446780204773, 'eval_runtime': 10.1149, 'eval_samples_per_second': 98.864, 'eval_steps_per_second': 6.228, 'epoch': 0.76}
{'loss': 0.88, 'grad_norm': 0.3475193977355957, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2369719743728638, 'eval_runtime': 10.1125, 'eval_samples_per_second': 98.888, 'eval_steps_per_second': 6.23, 'epoch': 0.8}
{'loss': 0.8512, 'grad_norm': 0.358778715133667, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2367995977401733, 'eval_runtime': 10.1247, 'eval_samples_per_second': 98.768, 'eval_steps_per_second': 6.222, 'epoch': 0.84}
{'loss': 0.825, 'grad_norm': 0.23826737701892853, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2350225448608398, 'eval_runtime': 10.1227, 'eval_samples_per_second': 98.788, 'eval_steps_per_second': 6.224, 'epoch': 0.88}
{'loss': 0.8806, 'grad_norm': 0.2735486328601837, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.235116720199585, 'eval_runtime': 10.1085, 'eval_samples_per_second': 98.927, 'eval_steps_per_second': 6.232, 'epoch': 0.92}
{'loss': 0.8928, 'grad_norm': 0.24331527948379517, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2333769798278809, 'eval_runtime': 10.1172, 'eval_samples_per_second': 98.841, 'eval_steps_per_second': 6.227, 'epoch': 0.96}
{'loss': 0.8334, 'grad_norm': 0.2855648994445801, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.232936978340149, 'eval_runtime': 10.1222, 'eval_samples_per_second': 98.793, 'eval_steps_per_second': 6.224, 'epoch': 1.0}
{'train_runtime': 445.9883, 'train_samples_per_second': 22.415, 'train_steps_per_second': 1.401, 'train_loss': 1.040740933227539, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7571206092834473, 1.5157381296157837, 1.394323468208313, 1.347423791885376, 1.2988358736038208, 1.2838752269744873, 1.2778669595718384, 1.2728941440582275, 1.2676290273666382, 1.2657959461212158, 1.258608102798462, 1.2569470405578613, 1.254543662071228, 1.2522567510604858, 1.2480953931808472, 1.2453280687332153, 1.243502140045166, 1.242432951927185, 1.242446780204773, 1.2369719743728638, 1.2367995977401733, 1.2350225448608398, 1.235116720199585, 1.2333769798278809, 1.232936978340149], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7571206092834473, 1.5157381296157837, 1.394323468208313, 1.347423791885376, 1.2988358736038208, 1.2838752269744873, 1.2778669595718384, 1.2728941440582275, 1.2676290273666382, 1.2657959461212158, 1.258608102798462, 1.2569470405578613, 1.254543662071228, 1.2522567510604858, 1.2480953931808472, 1.2453280687332153, 1.243502140045166, 1.242432951927185, 1.242446780204773, 1.2369719743728638, 1.2367995977401733, 1.2350225448608398, 1.235116720199585, 1.2333769798278809, 1.232936978340149]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.232936978340149
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.8968 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.7728173800527771
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.7728173802469398
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.772817380053048
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.7728173801387412
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.7728173800552656
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0630, dtype=torch.float64), 0, tensor(0.2717, dtype=torch.float64), 0, 0, tensor(0.1057, dtype=torch.float64), tensor(0.0758, dtype=torch.float64), tensor(0.4765, dtype=torch.float64), 14, 0, 1, 1, 0, 1, 128, 1.7347234759768077e-19, 20.427103638479345, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0630, dtype=torch.float64), tensor(2.0552e-17, dtype=torch.float64), tensor(0.2717, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0074, dtype=torch.float64), tensor(0.1057, dtype=torch.float64), tensor(0.0758, dtype=torch.float64), tensor(0.4765, dtype=torch.float64), tensor(0.4413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.7347e-18, dtype=torch.float64), tensor(0.4256, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.063
  rowan_hellaswag: 0
  sciq: 0.272
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.106
  mmlu: 0.076
  arc_challenge: 0.477

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.7347234759768077e-19,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (20.427103638479345,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  128
lora dropout:  1.7347234759768077e-19
lora alpha:  20.427103638479345
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,235,328 || all params: 8,105,496,576 || trainable%: 0.9282
length of training data:  9923
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2659, 'grad_norm': 0.35813698172569275, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1059365272521973, 'eval_runtime': 9.1956, 'eval_samples_per_second': 108.748, 'eval_steps_per_second': 6.851, 'epoch': 0.04}
{'loss': 1.5082, 'grad_norm': 0.1446315497159958, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.685479998588562, 'eval_runtime': 9.2006, 'eval_samples_per_second': 108.689, 'eval_steps_per_second': 6.847, 'epoch': 0.08}
{'loss': 1.3355, 'grad_norm': 0.13217736780643463, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.6418415307998657, 'eval_runtime': 9.2107, 'eval_samples_per_second': 108.569, 'eval_steps_per_second': 6.84, 'epoch': 0.12}
{'loss': 1.2062, 'grad_norm': 0.16874489188194275, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.5923900604248047, 'eval_runtime': 9.2366, 'eval_samples_per_second': 108.265, 'eval_steps_per_second': 6.821, 'epoch': 0.16}
{'loss': 1.2026, 'grad_norm': 0.14292265474796295, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.5255404710769653, 'eval_runtime': 9.2542, 'eval_samples_per_second': 108.059, 'eval_steps_per_second': 6.808, 'epoch': 0.2}
{'loss': 1.1149, 'grad_norm': 0.16005758941173553, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.4864660501480103, 'eval_runtime': 9.2711, 'eval_samples_per_second': 107.862, 'eval_steps_per_second': 6.795, 'epoch': 0.24}
{'loss': 1.0547, 'grad_norm': 0.17103803157806396, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.4518319368362427, 'eval_runtime': 9.2731, 'eval_samples_per_second': 107.839, 'eval_steps_per_second': 6.794, 'epoch': 0.28}
{'loss': 1.0038, 'grad_norm': 0.17788447439670563, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.4385548830032349, 'eval_runtime': 9.2807, 'eval_samples_per_second': 107.75, 'eval_steps_per_second': 6.788, 'epoch': 0.32}
{'loss': 1.0145, 'grad_norm': 0.17713089287281036, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.4294524192810059, 'eval_runtime': 9.2877, 'eval_samples_per_second': 107.669, 'eval_steps_per_second': 6.783, 'epoch': 0.36}
{'loss': 0.933, 'grad_norm': 0.19405773282051086, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.4269672632217407, 'eval_runtime': 9.2797, 'eval_samples_per_second': 107.763, 'eval_steps_per_second': 6.789, 'epoch': 0.4}
{'loss': 0.9219, 'grad_norm': 0.1973700076341629, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.4116183519363403, 'eval_runtime': 9.2815, 'eval_samples_per_second': 107.741, 'eval_steps_per_second': 6.788, 'epoch': 0.44}
{'loss': 0.9627, 'grad_norm': 0.18071693181991577, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.4108816385269165, 'eval_runtime': 9.2889, 'eval_samples_per_second': 107.656, 'eval_steps_per_second': 6.782, 'epoch': 0.48}
{'loss': 0.8488, 'grad_norm': 0.2120220810174942, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.4270788431167603, 'eval_runtime': 9.2866, 'eval_samples_per_second': 107.682, 'eval_steps_per_second': 6.784, 'epoch': 0.52}
{'loss': 0.9463, 'grad_norm': 0.19763115048408508, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.4100006818771362, 'eval_runtime': 9.3148, 'eval_samples_per_second': 107.356, 'eval_steps_per_second': 6.763, 'epoch': 0.56}
{'loss': 0.8777, 'grad_norm': 0.2055739015340805, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.4130957126617432, 'eval_runtime': 9.3426, 'eval_samples_per_second': 107.036, 'eval_steps_per_second': 6.743, 'epoch': 0.6}
{'loss': 0.935, 'grad_norm': 0.2271178513765335, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.398413062095642, 'eval_runtime': 9.3472, 'eval_samples_per_second': 106.984, 'eval_steps_per_second': 6.74, 'epoch': 0.64}
{'loss': 0.8945, 'grad_norm': 0.27557119727134705, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.3943158388137817, 'eval_runtime': 9.3457, 'eval_samples_per_second': 107.001, 'eval_steps_per_second': 6.741, 'epoch': 0.68}
{'loss': 0.8076, 'grad_norm': 0.272555947303772, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.400741696357727, 'eval_runtime': 9.3327, 'eval_samples_per_second': 107.15, 'eval_steps_per_second': 6.75, 'epoch': 0.72}
{'loss': 0.8949, 'grad_norm': 0.28795984387397766, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.3956249952316284, 'eval_runtime': 9.3276, 'eval_samples_per_second': 107.209, 'eval_steps_per_second': 6.754, 'epoch': 0.76}
{'loss': 0.9414, 'grad_norm': 0.2722417712211609, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.39228355884552, 'eval_runtime': 9.3158, 'eval_samples_per_second': 107.345, 'eval_steps_per_second': 6.763, 'epoch': 0.81}
{'loss': 0.8216, 'grad_norm': 0.28621014952659607, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.3946260213851929, 'eval_runtime': 9.3205, 'eval_samples_per_second': 107.291, 'eval_steps_per_second': 6.759, 'epoch': 0.85}
{'loss': 0.8622, 'grad_norm': 0.28719401359558105, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.391835331916809, 'eval_runtime': 9.3092, 'eval_samples_per_second': 107.421, 'eval_steps_per_second': 6.768, 'epoch': 0.89}
{'loss': 0.8871, 'grad_norm': 0.296652615070343, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.3929831981658936, 'eval_runtime': 9.2862, 'eval_samples_per_second': 107.687, 'eval_steps_per_second': 6.784, 'epoch': 0.93}
{'loss': 0.8821, 'grad_norm': 0.2937764525413513, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.3919250965118408, 'eval_runtime': 9.2607, 'eval_samples_per_second': 107.983, 'eval_steps_per_second': 6.803, 'epoch': 0.97}
{'train_runtime': 353.1809, 'train_samples_per_second': 28.096, 'train_steps_per_second': 1.758, 'train_loss': 1.0771474945756356, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1059365272521973, 1.685479998588562, 1.6418415307998657, 1.5923900604248047, 1.5255404710769653, 1.4864660501480103, 1.4518319368362427, 1.4385548830032349, 1.4294524192810059, 1.4269672632217407, 1.4116183519363403, 1.4108816385269165, 1.4270788431167603, 1.4100006818771362, 1.4130957126617432, 1.398413062095642, 1.3943158388137817, 1.400741696357727, 1.3956249952316284, 1.39228355884552, 1.3946260213851929, 1.391835331916809, 1.3929831981658936, 1.3919250965118408], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.1059365272521973, 1.685479998588562, 1.6418415307998657, 1.5923900604248047, 1.5255404710769653, 1.4864660501480103, 1.4518319368362427, 1.4385548830032349, 1.4294524192810059, 1.4269672632217407, 1.4116183519363403, 1.4108816385269165, 1.4270788431167603, 1.4100006818771362, 1.4130957126617432, 1.398413062095642, 1.3943158388137817, 1.400741696357727, 1.3956249952316284, 1.39228355884552, 1.3946260213851929, 1.391835331916809, 1.3929831981658936, 1.3919250965118408]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3919250965118408
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6581 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5173616409301758, 0.9290173649787903, 0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8086898326873779, 0.40152454376220703, 0.7644284963607788, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.11481685936450958, 0.6888900399208069, 0.67311692237854, 0.032737672328948975]  ‚Üí  acq = -0.8747780039529107
X = [0.24963438510894775, 0.8990642428398132, 0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8877483606338501, 0.9734378457069397, 0.9886022210121155, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.3387552499771118, 0.028142869472503662, 0.04686132073402405, 0.12386679649353027]  ‚Üí  acq = -0.8749999979663221
X = [0.8100742101669312, 0.591465413570404, 0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.45014575123786926, 0.044465720653533936, 0.6919609904289246, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.3413050174713135, 0.471097469329834, 0.6840248107910156, 0.8633646368980408]  ‚Üí  acq = -0.8750409860696251
X = [0.565035343170166, 0.14758515357971191, 0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8177289366722107, 0.8848571181297302, 0.35672861337661743, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.5902637243270874, 0.3791559934616089, 0.7795722484588623, 0.572867214679718]  ‚Üí  acq = -0.8750358404566144
X = [0.9166635870933533, 0.9858017563819885, 0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9887644648551941, 0.9247068166732788, 0.6096784472465515, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.931157112121582, 0.6287887096405029, 0.3411646783351898, 0.1318853497505188]  ‚Üí  acq = -0.8749903778769046
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1724, dtype=torch.float64), 0, tensor(0.2680, dtype=torch.float64), 0, tensor(0.0384, dtype=torch.float64), tensor(0.5212, dtype=torch.float64), 16, 0, 1, 1, 1, 1, 128, 0.1, 22.547760685979775, 1]
normalized proposed parameters for next round by BO: [tensor(3.7359e-18, dtype=torch.float64), tensor(4.6710e-19, dtype=torch.float64), tensor(3.4648e-18, dtype=torch.float64), tensor(0.1724, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2680, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0384, dtype=torch.float64), tensor(0.5212, dtype=torch.float64), tensor(0.5152, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4697, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.172
  triviaqa: 0
  truthfulqa_gen: 0.268
  wikitext: 0
  mmlu: 0.038
  arc_challenge: 0.521

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (22.547760685979775,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  22.547760685979775
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 123,731,968 || all params: 8,153,993,216 || trainable%: 1.5174
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1188, 'grad_norm': 1.2340989112854004, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8984330892562866, 'eval_runtime': 9.5781, 'eval_samples_per_second': 104.405, 'eval_steps_per_second': 6.577, 'epoch': 0.04}
{'loss': 1.2322, 'grad_norm': 0.7228997349739075, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.611452579498291, 'eval_runtime': 9.5569, 'eval_samples_per_second': 104.637, 'eval_steps_per_second': 6.592, 'epoch': 0.08}
{'loss': 0.9618, 'grad_norm': 0.28077492117881775, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5828111171722412, 'eval_runtime': 9.6372, 'eval_samples_per_second': 103.765, 'eval_steps_per_second': 6.537, 'epoch': 0.12}
{'loss': 0.9563, 'grad_norm': 0.20178158581256866, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5144823789596558, 'eval_runtime': 9.654, 'eval_samples_per_second': 103.584, 'eval_steps_per_second': 6.526, 'epoch': 0.16}
{'loss': 0.8895, 'grad_norm': 0.19416621327400208, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.519745111465454, 'eval_runtime': 9.6798, 'eval_samples_per_second': 103.308, 'eval_steps_per_second': 6.508, 'epoch': 0.2}
{'loss': 0.8449, 'grad_norm': 0.3454184830188751, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4758808612823486, 'eval_runtime': 9.6758, 'eval_samples_per_second': 103.35, 'eval_steps_per_second': 6.511, 'epoch': 0.24}
{'loss': 0.8109, 'grad_norm': 0.23411571979522705, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.493241310119629, 'eval_runtime': 9.6837, 'eval_samples_per_second': 103.267, 'eval_steps_per_second': 6.506, 'epoch': 0.28}
{'loss': 0.788, 'grad_norm': 0.2135317474603653, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4390674829483032, 'eval_runtime': 9.6765, 'eval_samples_per_second': 103.343, 'eval_steps_per_second': 6.511, 'epoch': 0.32}
{'loss': 0.7437, 'grad_norm': 0.25325411558151245, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4213778972625732, 'eval_runtime': 9.6907, 'eval_samples_per_second': 103.191, 'eval_steps_per_second': 6.501, 'epoch': 0.36}
{'loss': 0.7666, 'grad_norm': 0.25416892766952515, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.427679419517517, 'eval_runtime': 9.6828, 'eval_samples_per_second': 103.276, 'eval_steps_per_second': 6.506, 'epoch': 0.4}
{'loss': 0.7214, 'grad_norm': 0.2653336822986603, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4301854372024536, 'eval_runtime': 9.6984, 'eval_samples_per_second': 103.109, 'eval_steps_per_second': 6.496, 'epoch': 0.44}
{'loss': 0.742, 'grad_norm': 0.24581389129161835, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3896503448486328, 'eval_runtime': 9.7017, 'eval_samples_per_second': 103.075, 'eval_steps_per_second': 6.494, 'epoch': 0.48}
{'loss': 0.6941, 'grad_norm': 0.31221166253089905, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4008103609085083, 'eval_runtime': 9.7093, 'eval_samples_per_second': 102.994, 'eval_steps_per_second': 6.489, 'epoch': 0.52}
{'loss': 0.6948, 'grad_norm': 0.32836636900901794, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3795485496520996, 'eval_runtime': 9.7053, 'eval_samples_per_second': 103.036, 'eval_steps_per_second': 6.491, 'epoch': 0.56}
{'loss': 0.6471, 'grad_norm': 0.30968397855758667, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3709381818771362, 'eval_runtime': 9.7107, 'eval_samples_per_second': 102.979, 'eval_steps_per_second': 6.488, 'epoch': 0.6}
{'loss': 0.656, 'grad_norm': 0.3895137906074524, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3653398752212524, 'eval_runtime': 9.7118, 'eval_samples_per_second': 102.967, 'eval_steps_per_second': 6.487, 'epoch': 0.64}
{'loss': 0.6151, 'grad_norm': 0.3678932189941406, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3702189922332764, 'eval_runtime': 9.666, 'eval_samples_per_second': 103.455, 'eval_steps_per_second': 6.518, 'epoch': 0.68}
{'loss': 0.6012, 'grad_norm': 0.3491657078266144, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3625590801239014, 'eval_runtime': 9.6321, 'eval_samples_per_second': 103.819, 'eval_steps_per_second': 6.541, 'epoch': 0.72}
{'loss': 0.59, 'grad_norm': 0.4189768135547638, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3585295677185059, 'eval_runtime': 9.6545, 'eval_samples_per_second': 103.578, 'eval_steps_per_second': 6.525, 'epoch': 0.76}
{'loss': 0.5424, 'grad_norm': 0.41569679975509644, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3644686937332153, 'eval_runtime': 9.7173, 'eval_samples_per_second': 102.909, 'eval_steps_per_second': 6.483, 'epoch': 0.8}
{'loss': 0.5771, 'grad_norm': 0.4481308162212372, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.364526629447937, 'eval_runtime': 9.7184, 'eval_samples_per_second': 102.897, 'eval_steps_per_second': 6.483, 'epoch': 0.84}
{'loss': 0.5536, 'grad_norm': 0.38571545481681824, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3582110404968262, 'eval_runtime': 9.7211, 'eval_samples_per_second': 102.869, 'eval_steps_per_second': 6.481, 'epoch': 0.88}
{'loss': 0.5406, 'grad_norm': 0.48647043108940125, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3572582006454468, 'eval_runtime': 9.6996, 'eval_samples_per_second': 103.097, 'eval_steps_per_second': 6.495, 'epoch': 0.92}
{'loss': 0.4891, 'grad_norm': 0.3916969895362854, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.355359673500061, 'eval_runtime': 9.6943, 'eval_samples_per_second': 103.153, 'eval_steps_per_second': 6.499, 'epoch': 0.96}
{'loss': 0.4781, 'grad_norm': 0.6124297380447388, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3568898439407349, 'eval_runtime': 9.6867, 'eval_samples_per_second': 103.234, 'eval_steps_per_second': 6.504, 'epoch': 1.0}
{'train_runtime': 403.749, 'train_samples_per_second': 24.76, 'train_steps_per_second': 1.548, 'train_loss': 0.8102112518310547, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8984330892562866, 1.611452579498291, 1.5828111171722412, 1.5144823789596558, 1.519745111465454, 1.4758808612823486, 1.493241310119629, 1.4390674829483032, 1.4213778972625732, 1.427679419517517, 1.4301854372024536, 1.3896503448486328, 1.4008103609085083, 1.3795485496520996, 1.3709381818771362, 1.3653398752212524, 1.3702189922332764, 1.3625590801239014, 1.3585295677185059, 1.3644686937332153, 1.364526629447937, 1.3582110404968262, 1.3572582006454468, 1.355359673500061, 1.3568898439407349], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8984330892562866, 1.611452579498291, 1.5828111171722412, 1.5144823789596558, 1.519745111465454, 1.4758808612823486, 1.493241310119629, 1.4390674829483032, 1.4213778972625732, 1.427679419517517, 1.4301854372024536, 1.3896503448486328, 1.4008103609085083, 1.3795485496520996, 1.3709381818771362, 1.3653398752212524, 1.3702189922332764, 1.3625590801239014, 1.3585295677185059, 1.3644686937332153, 1.364526629447937, 1.3582110404968262, 1.3572582006454468, 1.355359673500061, 1.3568898439407349]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3568898439407349
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.5861 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.8405153237551237
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.8432998870778871
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.8413130868323712
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.8405120606426728
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.84051209011073
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0739, dtype=torch.float64), tensor(0.0349, dtype=torch.float64), tensor(0.0602, dtype=torch.float64), tensor(0.2125, dtype=torch.float64), tensor(0.1624, dtype=torch.float64), tensor(0.0200, dtype=torch.float64), tensor(0.2912, dtype=torch.float64), tensor(0.1448, dtype=torch.float64), 17, 1, 1, 0, 1, 0, 75, 0.011888729898562803, 11.34044673278546, 1]
normalized proposed parameters for next round by BO: [tensor(2.2096e-20, dtype=torch.float64), tensor(0.0739, dtype=torch.float64), tensor(0.0349, dtype=torch.float64), tensor(0.0602, dtype=torch.float64), tensor(0.2125, dtype=torch.float64), tensor(0.1624, dtype=torch.float64), tensor(0.0200, dtype=torch.float64), tensor(0.2912, dtype=torch.float64), tensor(0.1448, dtype=torch.float64), tensor(0.5388, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5862, dtype=torch.float64), tensor(0.1189, dtype=torch.float64), tensor(0.2363, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.074
  rowan_hellaswag: 0.035
  sciq: 0.06
  triviaqa: 0.213
  truthfulqa_gen: 0.162
  wikitext: 0.02
  mmlu: 0.291
  arc_challenge: 0.145

LoRA Parameters:
  lora_r: (75,)
  lora_dropout: (0.011888729898562803,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (11.34044673278546,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  75
lora dropout:  0.011888729898562803
lora alpha:  11.34044673278546
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 40,473,600 || all params: 8,070,734,848 || trainable%: 0.5015
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4653, 'grad_norm': 1.4657028913497925, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0750961303710938, 'eval_runtime': 9.506, 'eval_samples_per_second': 105.197, 'eval_steps_per_second': 6.627, 'epoch': 0.04}
{'loss': 1.8239, 'grad_norm': 0.34042903780937195, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.556065559387207, 'eval_runtime': 9.5172, 'eval_samples_per_second': 105.073, 'eval_steps_per_second': 6.62, 'epoch': 0.08}
{'loss': 1.4498, 'grad_norm': 0.45183268189430237, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4635398387908936, 'eval_runtime': 9.5338, 'eval_samples_per_second': 104.89, 'eval_steps_per_second': 6.608, 'epoch': 0.12}
{'loss': 1.3123, 'grad_norm': 0.22186769545078278, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.427103877067566, 'eval_runtime': 9.5316, 'eval_samples_per_second': 104.914, 'eval_steps_per_second': 6.61, 'epoch': 0.16}
{'loss': 1.2532, 'grad_norm': 0.30529090762138367, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.409484624862671, 'eval_runtime': 9.5647, 'eval_samples_per_second': 104.551, 'eval_steps_per_second': 6.587, 'epoch': 0.2}
{'loss': 1.2583, 'grad_norm': 0.49092161655426025, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3644919395446777, 'eval_runtime': 9.5767, 'eval_samples_per_second': 104.42, 'eval_steps_per_second': 6.578, 'epoch': 0.24}
{'loss': 1.2835, 'grad_norm': 0.22145216166973114, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.329099416732788, 'eval_runtime': 9.5768, 'eval_samples_per_second': 104.42, 'eval_steps_per_second': 6.578, 'epoch': 0.28}
{'loss': 1.1563, 'grad_norm': 0.24672941863536835, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3148225545883179, 'eval_runtime': 9.5692, 'eval_samples_per_second': 104.502, 'eval_steps_per_second': 6.584, 'epoch': 0.32}
{'loss': 1.1844, 'grad_norm': 0.2243354767560959, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3055435419082642, 'eval_runtime': 9.5659, 'eval_samples_per_second': 104.538, 'eval_steps_per_second': 6.586, 'epoch': 0.36}
{'loss': 1.1618, 'grad_norm': 0.3245104253292084, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3051916360855103, 'eval_runtime': 9.5781, 'eval_samples_per_second': 104.405, 'eval_steps_per_second': 6.578, 'epoch': 0.4}
{'loss': 1.1651, 'grad_norm': 0.20618131756782532, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2928481101989746, 'eval_runtime': 9.5551, 'eval_samples_per_second': 104.656, 'eval_steps_per_second': 6.593, 'epoch': 0.44}
{'loss': 1.1949, 'grad_norm': 0.26462507247924805, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2964587211608887, 'eval_runtime': 9.5275, 'eval_samples_per_second': 104.96, 'eval_steps_per_second': 6.612, 'epoch': 0.48}
{'loss': 1.1214, 'grad_norm': 0.22842937707901, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.292635202407837, 'eval_runtime': 9.5763, 'eval_samples_per_second': 104.425, 'eval_steps_per_second': 6.579, 'epoch': 0.52}
{'loss': 1.1063, 'grad_norm': 0.2598963677883148, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2862744331359863, 'eval_runtime': 9.5898, 'eval_samples_per_second': 104.277, 'eval_steps_per_second': 6.569, 'epoch': 0.56}
{'loss': 1.1739, 'grad_norm': 0.22710135579109192, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2845673561096191, 'eval_runtime': 9.5845, 'eval_samples_per_second': 104.335, 'eval_steps_per_second': 6.573, 'epoch': 0.6}
{'loss': 1.085, 'grad_norm': 0.1967398077249527, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2860255241394043, 'eval_runtime': 9.5951, 'eval_samples_per_second': 104.22, 'eval_steps_per_second': 6.566, 'epoch': 0.64}
{'loss': 1.0866, 'grad_norm': 0.27870455384254456, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2839607000350952, 'eval_runtime': 9.5759, 'eval_samples_per_second': 104.429, 'eval_steps_per_second': 6.579, 'epoch': 0.68}
{'loss': 1.1599, 'grad_norm': 0.20777542889118195, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.281121850013733, 'eval_runtime': 9.5669, 'eval_samples_per_second': 104.527, 'eval_steps_per_second': 6.585, 'epoch': 0.72}
{'loss': 1.115, 'grad_norm': 0.20776031911373138, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2806289196014404, 'eval_runtime': 9.5368, 'eval_samples_per_second': 104.857, 'eval_steps_per_second': 6.606, 'epoch': 0.76}
{'loss': 1.1463, 'grad_norm': 0.20439891517162323, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.278627634048462, 'eval_runtime': 9.5643, 'eval_samples_per_second': 104.556, 'eval_steps_per_second': 6.587, 'epoch': 0.8}
{'loss': 1.107, 'grad_norm': 0.26903456449508667, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.277137041091919, 'eval_runtime': 9.5327, 'eval_samples_per_second': 104.903, 'eval_steps_per_second': 6.609, 'epoch': 0.84}
{'loss': 1.1453, 'grad_norm': 0.20559906959533691, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2759042978286743, 'eval_runtime': 9.5105, 'eval_samples_per_second': 105.147, 'eval_steps_per_second': 6.624, 'epoch': 0.88}
{'loss': 1.1564, 'grad_norm': 0.22891201078891754, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2759836912155151, 'eval_runtime': 9.5133, 'eval_samples_per_second': 105.116, 'eval_steps_per_second': 6.622, 'epoch': 0.92}
{'loss': 1.09, 'grad_norm': 0.23603351414203644, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2761603593826294, 'eval_runtime': 9.5231, 'eval_samples_per_second': 105.008, 'eval_steps_per_second': 6.615, 'epoch': 0.96}
{'loss': 1.1384, 'grad_norm': 0.2197481095790863, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.276037335395813, 'eval_runtime': 9.5199, 'eval_samples_per_second': 105.043, 'eval_steps_per_second': 6.618, 'epoch': 1.0}
{'train_runtime': 447.4844, 'train_samples_per_second': 22.343, 'train_steps_per_second': 1.397, 'train_loss': 1.2936033966064453, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0750961303710938, 1.556065559387207, 1.4635398387908936, 1.427103877067566, 1.409484624862671, 1.3644919395446777, 1.329099416732788, 1.3148225545883179, 1.3055435419082642, 1.3051916360855103, 1.2928481101989746, 1.2964587211608887, 1.292635202407837, 1.2862744331359863, 1.2845673561096191, 1.2860255241394043, 1.2839607000350952, 1.281121850013733, 1.2806289196014404, 1.278627634048462, 1.277137041091919, 1.2759042978286743, 1.2759836912155151, 1.2761603593826294, 1.276037335395813], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0750961303710938, 1.556065559387207, 1.4635398387908936, 1.427103877067566, 1.409484624862671, 1.3644919395446777, 1.329099416732788, 1.3148225545883179, 1.3055435419082642, 1.3051916360855103, 1.2928481101989746, 1.2964587211608887, 1.292635202407837, 1.2862744331359863, 1.2845673561096191, 1.2860255241394043, 1.2839607000350952, 1.281121850013733, 1.2806289196014404, 1.278627634048462, 1.277137041091919, 1.2759042978286743, 1.2759836912155151, 1.2761603593826294, 1.276037335395813]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.276037335395813
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9375 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.8665699289871929
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.8665844232768625
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.8673463059180138
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.8681101630097764
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.8665640257600427
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0657, dtype=torch.float64), tensor(0.0236, dtype=torch.float64), tensor(0.0671, dtype=torch.float64), tensor(0.2076, dtype=torch.float64), tensor(0.1636, dtype=torch.float64), tensor(0.0265, dtype=torch.float64), tensor(0.2668, dtype=torch.float64), tensor(0.1791, dtype=torch.float64), 17, 0, 0, 0, 0, 1, 81, 0.0029132816856709976, 13.518872213893042, 1]
normalized proposed parameters for next round by BO: [tensor(6.5684e-19, dtype=torch.float64), tensor(0.0657, dtype=torch.float64), tensor(0.0236, dtype=torch.float64), tensor(0.0671, dtype=torch.float64), tensor(0.2076, dtype=torch.float64), tensor(0.1636, dtype=torch.float64), tensor(0.0265, dtype=torch.float64), tensor(0.2668, dtype=torch.float64), tensor(0.1791, dtype=torch.float64), tensor(0.5449, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6357, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.2816, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.066
  rowan_hellaswag: 0.024
  sciq: 0.067
  triviaqa: 0.208
  truthfulqa_gen: 0.164
  wikitext: 0.027
  mmlu: 0.267
  arc_challenge: 0.179

LoRA Parameters:
  lora_r: (81,)
  lora_dropout: (0.0029132816856709976,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (13.518872213893042,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  81
lora dropout:  0.0029132816856709976
lora alpha:  13.518872213893042
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 25,380,864 || all params: 8,055,642,112 || trainable%: 0.3151
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5963, 'grad_norm': 0.6480387449264526, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2909739017486572, 'eval_runtime': 9.0169, 'eval_samples_per_second': 110.902, 'eval_steps_per_second': 6.987, 'epoch': 0.04}
{'loss': 2.1188, 'grad_norm': 0.24514839053153992, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7489328384399414, 'eval_runtime': 9.0439, 'eval_samples_per_second': 110.572, 'eval_steps_per_second': 6.966, 'epoch': 0.08}
{'loss': 1.6171, 'grad_norm': 0.23821218311786652, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.647653579711914, 'eval_runtime': 9.0795, 'eval_samples_per_second': 110.138, 'eval_steps_per_second': 6.939, 'epoch': 0.12}
{'loss': 1.4695, 'grad_norm': 0.14118610322475433, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5452797412872314, 'eval_runtime': 9.0936, 'eval_samples_per_second': 109.967, 'eval_steps_per_second': 6.928, 'epoch': 0.16}
{'loss': 1.3993, 'grad_norm': 0.1639769822359085, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5177632570266724, 'eval_runtime': 9.1095, 'eval_samples_per_second': 109.775, 'eval_steps_per_second': 6.916, 'epoch': 0.2}
{'loss': 1.3821, 'grad_norm': 0.13557565212249756, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4623271226882935, 'eval_runtime': 9.1105, 'eval_samples_per_second': 109.763, 'eval_steps_per_second': 6.915, 'epoch': 0.24}
{'loss': 1.3344, 'grad_norm': 0.19686086475849152, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4228930473327637, 'eval_runtime': 9.1169, 'eval_samples_per_second': 109.686, 'eval_steps_per_second': 6.91, 'epoch': 0.28}
{'loss': 1.2091, 'grad_norm': 0.15753677487373352, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4044592380523682, 'eval_runtime': 9.1311, 'eval_samples_per_second': 109.516, 'eval_steps_per_second': 6.9, 'epoch': 0.32}
{'loss': 1.2771, 'grad_norm': 0.1574144959449768, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3823463916778564, 'eval_runtime': 9.1236, 'eval_samples_per_second': 109.606, 'eval_steps_per_second': 6.905, 'epoch': 0.36}
{'loss': 1.2028, 'grad_norm': 0.1628710925579071, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3737881183624268, 'eval_runtime': 9.1243, 'eval_samples_per_second': 109.598, 'eval_steps_per_second': 6.905, 'epoch': 0.4}
{'loss': 1.2447, 'grad_norm': 0.12682196497917175, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.367316722869873, 'eval_runtime': 9.1254, 'eval_samples_per_second': 109.584, 'eval_steps_per_second': 6.904, 'epoch': 0.44}
{'loss': 1.2276, 'grad_norm': 0.1554134041070938, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3626418113708496, 'eval_runtime': 9.1215, 'eval_samples_per_second': 109.631, 'eval_steps_per_second': 6.907, 'epoch': 0.48}
{'loss': 1.1896, 'grad_norm': 0.16929632425308228, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3594756126403809, 'eval_runtime': 9.1334, 'eval_samples_per_second': 109.488, 'eval_steps_per_second': 6.898, 'epoch': 0.52}
{'loss': 1.238, 'grad_norm': 0.1904505342245102, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3593522310256958, 'eval_runtime': 9.1349, 'eval_samples_per_second': 109.471, 'eval_steps_per_second': 6.897, 'epoch': 0.56}
{'loss': 1.196, 'grad_norm': 0.17449256777763367, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.354814887046814, 'eval_runtime': 9.132, 'eval_samples_per_second': 109.506, 'eval_steps_per_second': 6.899, 'epoch': 0.6}
{'loss': 1.2498, 'grad_norm': 0.1637413501739502, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.353097677230835, 'eval_runtime': 9.1162, 'eval_samples_per_second': 109.695, 'eval_steps_per_second': 6.911, 'epoch': 0.64}
{'loss': 1.2481, 'grad_norm': 0.13208092749118805, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3498486280441284, 'eval_runtime': 9.1117, 'eval_samples_per_second': 109.748, 'eval_steps_per_second': 6.914, 'epoch': 0.68}
{'loss': 1.125, 'grad_norm': 0.17093276977539062, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3478318452835083, 'eval_runtime': 9.1068, 'eval_samples_per_second': 109.809, 'eval_steps_per_second': 6.918, 'epoch': 0.72}
{'loss': 1.1653, 'grad_norm': 0.14353656768798828, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3472957611083984, 'eval_runtime': 9.1078, 'eval_samples_per_second': 109.796, 'eval_steps_per_second': 6.917, 'epoch': 0.76}
{'loss': 1.1602, 'grad_norm': 0.1378152072429657, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.344537377357483, 'eval_runtime': 9.098, 'eval_samples_per_second': 109.915, 'eval_steps_per_second': 6.925, 'epoch': 0.8}
{'loss': 1.1984, 'grad_norm': 0.20866858959197998, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.343614935874939, 'eval_runtime': 9.1019, 'eval_samples_per_second': 109.867, 'eval_steps_per_second': 6.922, 'epoch': 0.84}
{'loss': 1.1655, 'grad_norm': 0.16926664113998413, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3424766063690186, 'eval_runtime': 9.0624, 'eval_samples_per_second': 110.346, 'eval_steps_per_second': 6.952, 'epoch': 0.88}
{'loss': 1.2186, 'grad_norm': 0.16586866974830627, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3419277667999268, 'eval_runtime': 9.0497, 'eval_samples_per_second': 110.501, 'eval_steps_per_second': 6.962, 'epoch': 0.92}
{'loss': 1.1763, 'grad_norm': 0.13510537147521973, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3405195474624634, 'eval_runtime': 9.0493, 'eval_samples_per_second': 110.506, 'eval_steps_per_second': 6.962, 'epoch': 0.96}
{'loss': 1.1767, 'grad_norm': 0.1519182026386261, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3406656980514526, 'eval_runtime': 9.053, 'eval_samples_per_second': 110.46, 'eval_steps_per_second': 6.959, 'epoch': 1.0}
{'train_runtime': 415.5611, 'train_samples_per_second': 24.054, 'train_steps_per_second': 1.504, 'train_loss': 1.3834540557861328, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2909739017486572, 1.7489328384399414, 1.647653579711914, 1.5452797412872314, 1.5177632570266724, 1.4623271226882935, 1.4228930473327637, 1.4044592380523682, 1.3823463916778564, 1.3737881183624268, 1.367316722869873, 1.3626418113708496, 1.3594756126403809, 1.3593522310256958, 1.354814887046814, 1.353097677230835, 1.3498486280441284, 1.3478318452835083, 1.3472957611083984, 1.344537377357483, 1.343614935874939, 1.3424766063690186, 1.3419277667999268, 1.3405195474624634, 1.3406656980514526], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2909739017486572, 1.7489328384399414, 1.647653579711914, 1.5452797412872314, 1.5177632570266724, 1.4623271226882935, 1.4228930473327637, 1.4044592380523682, 1.3823463916778564, 1.3737881183624268, 1.367316722869873, 1.3626418113708496, 1.3594756126403809, 1.3593522310256958, 1.354814887046814, 1.353097677230835, 1.3498486280441284, 1.3478318452835083, 1.3472957611083984, 1.344537377357483, 1.343614935874939, 1.3424766063690186, 1.3419277667999268, 1.3405195474624634, 1.3406656980514526]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3406656980514526
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2232 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -0.9138424804992201
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -0.9101841079946102
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -0.9138382783115769
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -0.9156563522635628
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -0.9138524047847713
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0555, dtype=torch.float64), tensor(0.5285, dtype=torch.float64), tensor(0.0812, dtype=torch.float64), tensor(0.0216, dtype=torch.float64), tensor(0.3133, dtype=torch.float64), 0, 0, 0, 0, 27, 0, 0, 1, 1, 1, 34, 0.0, 27.365244235642333, 0]
normalized proposed parameters for next round by BO: [tensor(0.0555, dtype=torch.float64), tensor(0.5285, dtype=torch.float64), tensor(0.0812, dtype=torch.float64), tensor(0.0216, dtype=torch.float64), tensor(0.3133, dtype=torch.float64), tensor(2.9863e-05, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8327, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2642, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5701, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.055
  gsm8k: 0.528
  rowan_hellaswag: 0.081
  sciq: 0.022
  triviaqa: 0.313
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (34,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (27.365244235642333,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  34
lora dropout:  0.0
lora alpha:  27.365244235642333
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 50,761,728 || all params: 8,081,022,976 || trainable%: 0.6282
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.464, 'grad_norm': 0.9538792967796326, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.944295048713684, 'eval_runtime': 10.3149, 'eval_samples_per_second': 96.948, 'eval_steps_per_second': 6.108, 'epoch': 0.04}
{'loss': 1.1947, 'grad_norm': 0.4198777973651886, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9986716508865356, 'eval_runtime': 10.3378, 'eval_samples_per_second': 96.732, 'eval_steps_per_second': 6.094, 'epoch': 0.08}
{'loss': 1.0409, 'grad_norm': 0.2734529972076416, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9390212297439575, 'eval_runtime': 10.3584, 'eval_samples_per_second': 96.54, 'eval_steps_per_second': 6.082, 'epoch': 0.12}
{'loss': 1.0266, 'grad_norm': 0.25384747982025146, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9256349802017212, 'eval_runtime': 10.3767, 'eval_samples_per_second': 96.369, 'eval_steps_per_second': 6.071, 'epoch': 0.16}
{'loss': 1.0014, 'grad_norm': 0.2847802937030792, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9516388177871704, 'eval_runtime': 10.3631, 'eval_samples_per_second': 96.496, 'eval_steps_per_second': 6.079, 'epoch': 0.2}
{'loss': 0.9618, 'grad_norm': 0.25376173853874207, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9641327857971191, 'eval_runtime': 10.371, 'eval_samples_per_second': 96.423, 'eval_steps_per_second': 6.075, 'epoch': 0.24}
{'loss': 0.9895, 'grad_norm': 0.22325830161571503, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9301831722259521, 'eval_runtime': 10.3471, 'eval_samples_per_second': 96.645, 'eval_steps_per_second': 6.089, 'epoch': 0.28}
{'loss': 0.9933, 'grad_norm': 0.2723543643951416, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9335483312606812, 'eval_runtime': 10.3053, 'eval_samples_per_second': 97.038, 'eval_steps_per_second': 6.113, 'epoch': 0.32}
{'loss': 1.0086, 'grad_norm': 0.23355242609977722, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9441814422607422, 'eval_runtime': 10.3171, 'eval_samples_per_second': 96.927, 'eval_steps_per_second': 6.106, 'epoch': 0.36}
{'loss': 0.9729, 'grad_norm': 0.2689827084541321, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.935370922088623, 'eval_runtime': 10.3168, 'eval_samples_per_second': 96.929, 'eval_steps_per_second': 6.107, 'epoch': 0.4}
{'loss': 0.9422, 'grad_norm': 0.2449612021446228, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9085192680358887, 'eval_runtime': 10.3462, 'eval_samples_per_second': 96.654, 'eval_steps_per_second': 6.089, 'epoch': 0.44}
{'loss': 1.0049, 'grad_norm': 0.24449259042739868, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9377620220184326, 'eval_runtime': 10.3646, 'eval_samples_per_second': 96.482, 'eval_steps_per_second': 6.078, 'epoch': 0.48}
{'loss': 0.9469, 'grad_norm': 0.2507156729698181, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.937575101852417, 'eval_runtime': 10.3718, 'eval_samples_per_second': 96.415, 'eval_steps_per_second': 6.074, 'epoch': 0.52}
{'loss': 0.9495, 'grad_norm': 0.2670785188674927, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.943668246269226, 'eval_runtime': 10.3758, 'eval_samples_per_second': 96.378, 'eval_steps_per_second': 6.072, 'epoch': 0.56}
{'loss': 0.9383, 'grad_norm': 0.264639288187027, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9600226879119873, 'eval_runtime': 10.3723, 'eval_samples_per_second': 96.411, 'eval_steps_per_second': 6.074, 'epoch': 0.6}
{'loss': 0.9889, 'grad_norm': 0.23288767039775848, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.950475811958313, 'eval_runtime': 10.3985, 'eval_samples_per_second': 96.168, 'eval_steps_per_second': 6.059, 'epoch': 0.64}
{'loss': 0.9302, 'grad_norm': 0.2431870698928833, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9756383895874023, 'eval_runtime': 10.4031, 'eval_samples_per_second': 96.125, 'eval_steps_per_second': 6.056, 'epoch': 0.68}
{'loss': 0.9598, 'grad_norm': 0.29388827085494995, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.964080810546875, 'eval_runtime': 10.4006, 'eval_samples_per_second': 96.148, 'eval_steps_per_second': 6.057, 'epoch': 0.72}
{'loss': 0.8912, 'grad_norm': 0.24297752976417542, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9777536392211914, 'eval_runtime': 10.3963, 'eval_samples_per_second': 96.188, 'eval_steps_per_second': 6.06, 'epoch': 0.76}
{'loss': 0.9288, 'grad_norm': 0.27520161867141724, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9796409606933594, 'eval_runtime': 10.3712, 'eval_samples_per_second': 96.421, 'eval_steps_per_second': 6.075, 'epoch': 0.8}
{'loss': 0.9656, 'grad_norm': 0.2528991997241974, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9747852087020874, 'eval_runtime': 10.3769, 'eval_samples_per_second': 96.367, 'eval_steps_per_second': 6.071, 'epoch': 0.84}
{'loss': 0.9332, 'grad_norm': 0.2797669470310211, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9738662242889404, 'eval_runtime': 10.344, 'eval_samples_per_second': 96.675, 'eval_steps_per_second': 6.091, 'epoch': 0.88}
{'loss': 0.9187, 'grad_norm': 0.2556895315647125, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9789447784423828, 'eval_runtime': 10.3695, 'eval_samples_per_second': 96.437, 'eval_steps_per_second': 6.076, 'epoch': 0.92}
{'loss': 0.9424, 'grad_norm': 0.2861626446247101, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9752299785614014, 'eval_runtime': 10.3778, 'eval_samples_per_second': 96.36, 'eval_steps_per_second': 6.071, 'epoch': 0.96}
{'loss': 0.9095, 'grad_norm': 0.30332913994789124, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9749101400375366, 'eval_runtime': 10.3771, 'eval_samples_per_second': 96.366, 'eval_steps_per_second': 6.071, 'epoch': 1.0}
{'train_runtime': 476.4208, 'train_samples_per_second': 20.984, 'train_steps_per_second': 1.312, 'train_loss': 1.0321536651611327, 'epoch': 1.0}
train_results:  {'eval_loss': [1.944295048713684, 1.9986716508865356, 1.9390212297439575, 1.9256349802017212, 1.9516388177871704, 1.9641327857971191, 1.9301831722259521, 1.9335483312606812, 1.9441814422607422, 1.935370922088623, 1.9085192680358887, 1.9377620220184326, 1.937575101852417, 1.943668246269226, 1.9600226879119873, 1.950475811958313, 1.9756383895874023, 1.964080810546875, 1.9777536392211914, 1.9796409606933594, 1.9747852087020874, 1.9738662242889404, 1.9789447784423828, 1.9752299785614014, 1.9749101400375366], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.944295048713684, 1.9986716508865356, 1.9390212297439575, 1.9256349802017212, 1.9516388177871704, 1.9641327857971191, 1.9301831722259521, 1.9335483312606812, 1.9441814422607422, 1.935370922088623, 1.9085192680358887, 1.9377620220184326, 1.937575101852417, 1.943668246269226, 1.9600226879119873, 1.950475811958313, 1.9756383895874023, 1.964080810546875, 1.9777536392211914, 1.9796409606933594, 1.9747852087020874, 1.9738662242889404, 1.9789447784423828, 1.9752299785614014, 1.9749101400375366]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.9749101400375366
max eval_loss so far:  -1.2323100566864014
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.4486 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -0.9599866514497559
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -0.9556732392390561
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -0.9561578361106471
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.9590653407905214
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -0.9266453286502947
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0278, dtype=torch.float64), 0, 0, tensor(0.0693, dtype=torch.float64), tensor(0.1069, dtype=torch.float64), tensor(0.1524, dtype=torch.float64), 0, tensor(0.6435, dtype=torch.float64), 0, 29, 0, 1, 1, 1, 1, 80, 6.938893903907226e-19, 28.92542453675752, 0]
normalized proposed parameters for next round by BO: [tensor(0.0278, dtype=torch.float64), tensor(6.2735e-18, dtype=torch.float64), tensor(4.4078e-17, dtype=torch.float64), tensor(0.0693, dtype=torch.float64), tensor(0.1069, dtype=torch.float64), tensor(0.1524, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6435, dtype=torch.float64), tensor(3.3054e-19, dtype=torch.float64), tensor(0.9071, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6249, dtype=torch.float64), tensor(6.9389e-18, dtype=torch.float64), tensor(0.6026, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.028
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.069
  triviaqa: 0.107
  truthfulqa_gen: 0.152
  wikitext: 0
  mmlu: 0.644
  arc_challenge: 0

LoRA Parameters:
  lora_r: (80,)
  lora_dropout: (6.938893903907226e-19,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (28.92542453675752,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  80
lora dropout:  6.938893903907226e-19
lora alpha:  28.92542453675752
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 140,165,120 || all params: 8,170,426,368 || trainable%: 1.7155
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7955, 'grad_norm': 0.9656780958175659, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.549322485923767, 'eval_runtime': 10.4107, 'eval_samples_per_second': 96.055, 'eval_steps_per_second': 6.051, 'epoch': 0.04}
{'loss': 1.3706, 'grad_norm': 0.33750393986701965, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3078585863113403, 'eval_runtime': 10.3996, 'eval_samples_per_second': 96.157, 'eval_steps_per_second': 6.058, 'epoch': 0.08}
{'loss': 1.2011, 'grad_norm': 0.3059874176979065, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2815165519714355, 'eval_runtime': 10.4083, 'eval_samples_per_second': 96.077, 'eval_steps_per_second': 6.053, 'epoch': 0.12}
{'loss': 1.2754, 'grad_norm': 0.232054203748703, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2608113288879395, 'eval_runtime': 10.4146, 'eval_samples_per_second': 96.019, 'eval_steps_per_second': 6.049, 'epoch': 0.16}
{'loss': 1.1741, 'grad_norm': 0.22691307961940765, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2521352767944336, 'eval_runtime': 10.4275, 'eval_samples_per_second': 95.9, 'eval_steps_per_second': 6.042, 'epoch': 0.2}
{'loss': 1.1856, 'grad_norm': 0.42329803109169006, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2456955909729004, 'eval_runtime': 10.4262, 'eval_samples_per_second': 95.913, 'eval_steps_per_second': 6.042, 'epoch': 0.24}
{'loss': 1.1419, 'grad_norm': 0.26891815662384033, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2405940294265747, 'eval_runtime': 10.4386, 'eval_samples_per_second': 95.799, 'eval_steps_per_second': 6.035, 'epoch': 0.28}
{'loss': 1.1683, 'grad_norm': 0.250823438167572, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.236109733581543, 'eval_runtime': 10.4343, 'eval_samples_per_second': 95.838, 'eval_steps_per_second': 6.038, 'epoch': 0.32}
{'loss': 1.1235, 'grad_norm': 0.24809227883815765, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2323076725006104, 'eval_runtime': 10.4427, 'eval_samples_per_second': 95.76, 'eval_steps_per_second': 6.033, 'epoch': 0.36}
{'loss': 1.0897, 'grad_norm': 0.2334599792957306, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.233400583267212, 'eval_runtime': 10.4306, 'eval_samples_per_second': 95.872, 'eval_steps_per_second': 6.04, 'epoch': 0.4}
{'loss': 1.1534, 'grad_norm': 0.24291810393333435, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2247288227081299, 'eval_runtime': 10.4374, 'eval_samples_per_second': 95.81, 'eval_steps_per_second': 6.036, 'epoch': 0.44}
{'loss': 1.1212, 'grad_norm': 0.2698046565055847, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2197099924087524, 'eval_runtime': 10.4657, 'eval_samples_per_second': 95.55, 'eval_steps_per_second': 6.02, 'epoch': 0.48}
{'loss': 1.0959, 'grad_norm': 0.22733482718467712, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.216888666152954, 'eval_runtime': 10.4891, 'eval_samples_per_second': 95.337, 'eval_steps_per_second': 6.006, 'epoch': 0.52}
{'loss': 1.0548, 'grad_norm': 0.25859832763671875, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.212585687637329, 'eval_runtime': 10.4909, 'eval_samples_per_second': 95.321, 'eval_steps_per_second': 6.005, 'epoch': 0.56}
{'loss': 1.1074, 'grad_norm': 0.268026202917099, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2092386484146118, 'eval_runtime': 10.485, 'eval_samples_per_second': 95.375, 'eval_steps_per_second': 6.009, 'epoch': 0.6}
{'loss': 1.0935, 'grad_norm': 0.27937260270118713, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2067573070526123, 'eval_runtime': 10.4868, 'eval_samples_per_second': 95.358, 'eval_steps_per_second': 6.008, 'epoch': 0.64}
{'loss': 1.0731, 'grad_norm': 0.25215575098991394, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2046581506729126, 'eval_runtime': 10.4877, 'eval_samples_per_second': 95.35, 'eval_steps_per_second': 6.007, 'epoch': 0.68}
{'loss': 1.0845, 'grad_norm': 0.2624049484729767, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2013075351715088, 'eval_runtime': 10.4813, 'eval_samples_per_second': 95.408, 'eval_steps_per_second': 6.011, 'epoch': 0.72}
{'loss': 1.0256, 'grad_norm': 0.3929210305213928, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.199204683303833, 'eval_runtime': 10.4858, 'eval_samples_per_second': 95.367, 'eval_steps_per_second': 6.008, 'epoch': 0.76}
{'loss': 1.0681, 'grad_norm': 0.30457547307014465, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1940889358520508, 'eval_runtime': 10.4855, 'eval_samples_per_second': 95.37, 'eval_steps_per_second': 6.008, 'epoch': 0.8}
{'loss': 1.0925, 'grad_norm': 0.3135170042514801, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1912360191345215, 'eval_runtime': 10.4885, 'eval_samples_per_second': 95.343, 'eval_steps_per_second': 6.007, 'epoch': 0.84}
{'loss': 1.0483, 'grad_norm': 0.25104543566703796, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1891953945159912, 'eval_runtime': 10.4872, 'eval_samples_per_second': 95.355, 'eval_steps_per_second': 6.007, 'epoch': 0.88}
{'loss': 1.0526, 'grad_norm': 0.2858445942401886, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1896991729736328, 'eval_runtime': 10.4634, 'eval_samples_per_second': 95.571, 'eval_steps_per_second': 6.021, 'epoch': 0.92}
{'loss': 0.9947, 'grad_norm': 0.2675575315952301, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1886711120605469, 'eval_runtime': 10.4373, 'eval_samples_per_second': 95.81, 'eval_steps_per_second': 6.036, 'epoch': 0.96}
{'loss': 1.0349, 'grad_norm': 0.28339144587516785, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.188298225402832, 'eval_runtime': 10.441, 'eval_samples_per_second': 95.776, 'eval_steps_per_second': 6.034, 'epoch': 1.0}
{'train_runtime': 485.4785, 'train_samples_per_second': 20.594, 'train_steps_per_second': 1.287, 'train_loss': 1.1850545349121093, 'epoch': 1.0}
train_results:  {'eval_loss': [1.549322485923767, 1.3078585863113403, 1.2815165519714355, 1.2608113288879395, 1.2521352767944336, 1.2456955909729004, 1.2405940294265747, 1.236109733581543, 1.2323076725006104, 1.233400583267212, 1.2247288227081299, 1.2197099924087524, 1.216888666152954, 1.212585687637329, 1.2092386484146118, 1.2067573070526123, 1.2046581506729126, 1.2013075351715088, 1.199204683303833, 1.1940889358520508, 1.1912360191345215, 1.1891953945159912, 1.1896991729736328, 1.1886711120605469, 1.188298225402832], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.549322485923767, 1.3078585863113403, 1.2815165519714355, 1.2608113288879395, 1.2521352767944336, 1.2456955909729004, 1.2405940294265747, 1.236109733581543, 1.2323076725006104, 1.233400583267212, 1.2247288227081299, 1.2197099924087524, 1.216888666152954, 1.212585687637329, 1.2092386484146118, 1.2067573070526123, 1.2046581506729126, 1.2013075351715088, 1.199204683303833, 1.1940889358520508, 1.1912360191345215, 1.1891953945159912, 1.1896991729736328, 1.1886711120605469, 1.188298225402832]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.188298225402832
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1273 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.9571017807920963
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.9571067600862537
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.9543786026011765
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.957380313321027
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.95574472127088
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3269, dtype=torch.float64), 0, tensor(0.1416, dtype=torch.float64), tensor(0.1040, dtype=torch.float64), tensor(0.0442, dtype=torch.float64), 0, tensor(0.1050, dtype=torch.float64), tensor(0.0602, dtype=torch.float64), tensor(0.2181, dtype=torch.float64), 26, 0, 0, 1, 0, 1, 44, 8.6736173798840345e-19, 16.314432532153482, 0]
normalized proposed parameters for next round by BO: [tensor(0.3269, dtype=torch.float64), tensor(1.3187e-18, dtype=torch.float64), tensor(0.1416, dtype=torch.float64), tensor(0.1040, dtype=torch.float64), tensor(0.0442, dtype=torch.float64), tensor(2.7458e-18, dtype=torch.float64), tensor(0.1050, dtype=torch.float64), tensor(0.0602, dtype=torch.float64), tensor(0.2181, dtype=torch.float64), tensor(0.8260, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3469, dtype=torch.float64), tensor(8.6736e-18, dtype=torch.float64), tensor(0.3399, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.327
  gsm8k: 0
  rowan_hellaswag: 0.142
  sciq: 0.104
  triviaqa: 0.044
  truthfulqa_gen: 0
  wikitext: 0.105
  mmlu: 0.06
  arc_challenge: 0.218

LoRA Parameters:
  lora_r: (44,)
  lora_dropout: (8.6736173798840345e-19,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (16.314432532153482,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  44
lora dropout:  8.6736173798840345e-19
lora alpha:  16.314432532153482
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 42,172,416 || all params: 8,072,433,664 || trainable%: 0.5224
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5556, 'grad_norm': 0.6094592809677124, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.061042547225952, 'eval_runtime': 9.6332, 'eval_samples_per_second': 103.807, 'eval_steps_per_second': 6.54, 'epoch': 0.04}
{'loss': 1.7918, 'grad_norm': 0.2629058063030243, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.659989595413208, 'eval_runtime': 9.7623, 'eval_samples_per_second': 102.435, 'eval_steps_per_second': 6.453, 'epoch': 0.08}
{'loss': 1.5232, 'grad_norm': 0.2192593514919281, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5721454620361328, 'eval_runtime': 9.8081, 'eval_samples_per_second': 101.957, 'eval_steps_per_second': 6.423, 'epoch': 0.12}
{'loss': 1.4628, 'grad_norm': 0.20641012489795685, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5376811027526855, 'eval_runtime': 9.8165, 'eval_samples_per_second': 101.87, 'eval_steps_per_second': 6.418, 'epoch': 0.16}
{'loss': 1.3766, 'grad_norm': 0.2247868776321411, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5012761354446411, 'eval_runtime': 9.8165, 'eval_samples_per_second': 101.87, 'eval_steps_per_second': 6.418, 'epoch': 0.2}
{'loss': 1.3921, 'grad_norm': 0.1946784108877182, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4206796884536743, 'eval_runtime': 9.7868, 'eval_samples_per_second': 102.178, 'eval_steps_per_second': 6.437, 'epoch': 0.24}
{'loss': 1.2564, 'grad_norm': 0.20788930356502533, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.38694167137146, 'eval_runtime': 9.7838, 'eval_samples_per_second': 102.21, 'eval_steps_per_second': 6.439, 'epoch': 0.28}
{'loss': 1.2578, 'grad_norm': 0.20478573441505432, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3710100650787354, 'eval_runtime': 9.7927, 'eval_samples_per_second': 102.117, 'eval_steps_per_second': 6.433, 'epoch': 0.32}
{'loss': 1.2569, 'grad_norm': 0.19438984990119934, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3487114906311035, 'eval_runtime': 9.7746, 'eval_samples_per_second': 102.306, 'eval_steps_per_second': 6.445, 'epoch': 0.36}
{'loss': 1.3087, 'grad_norm': 0.20392189919948578, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3365986347198486, 'eval_runtime': 9.7581, 'eval_samples_per_second': 102.479, 'eval_steps_per_second': 6.456, 'epoch': 0.4}
{'loss': 1.1943, 'grad_norm': 0.23956167697906494, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3249517679214478, 'eval_runtime': 9.7608, 'eval_samples_per_second': 102.451, 'eval_steps_per_second': 6.454, 'epoch': 0.44}
{'loss': 1.2198, 'grad_norm': 0.18818873167037964, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3231536149978638, 'eval_runtime': 9.7814, 'eval_samples_per_second': 102.235, 'eval_steps_per_second': 6.441, 'epoch': 0.48}
{'loss': 1.2148, 'grad_norm': 0.18709953129291534, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.32387375831604, 'eval_runtime': 9.7584, 'eval_samples_per_second': 102.476, 'eval_steps_per_second': 6.456, 'epoch': 0.52}
{'loss': 1.1857, 'grad_norm': 0.24786581099033356, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.330471158027649, 'eval_runtime': 9.7558, 'eval_samples_per_second': 102.503, 'eval_steps_per_second': 6.458, 'epoch': 0.56}
{'loss': 1.145, 'grad_norm': 0.2237897366285324, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3217260837554932, 'eval_runtime': 9.7495, 'eval_samples_per_second': 102.569, 'eval_steps_per_second': 6.462, 'epoch': 0.6}
{'loss': 1.2123, 'grad_norm': 0.21056894958019257, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3157793283462524, 'eval_runtime': 9.7558, 'eval_samples_per_second': 102.504, 'eval_steps_per_second': 6.458, 'epoch': 0.64}
{'loss': 1.1999, 'grad_norm': 0.23476973176002502, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.317306399345398, 'eval_runtime': 9.7756, 'eval_samples_per_second': 102.295, 'eval_steps_per_second': 6.445, 'epoch': 0.68}
{'loss': 1.1832, 'grad_norm': 0.2516939342021942, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3123993873596191, 'eval_runtime': 9.7782, 'eval_samples_per_second': 102.268, 'eval_steps_per_second': 6.443, 'epoch': 0.72}
{'loss': 1.1895, 'grad_norm': 0.24815820157527924, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3151801824569702, 'eval_runtime': 9.7605, 'eval_samples_per_second': 102.454, 'eval_steps_per_second': 6.455, 'epoch': 0.76}
{'loss': 1.1204, 'grad_norm': 0.23125430941581726, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3103796243667603, 'eval_runtime': 9.7805, 'eval_samples_per_second': 102.244, 'eval_steps_per_second': 6.441, 'epoch': 0.8}
{'loss': 1.25, 'grad_norm': 0.20542998611927032, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3123114109039307, 'eval_runtime': 9.7782, 'eval_samples_per_second': 102.268, 'eval_steps_per_second': 6.443, 'epoch': 0.84}
{'loss': 1.164, 'grad_norm': 0.2264191061258316, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3106101751327515, 'eval_runtime': 9.7677, 'eval_samples_per_second': 102.379, 'eval_steps_per_second': 6.45, 'epoch': 0.88}
{'loss': 1.1626, 'grad_norm': 0.2277505099773407, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3111226558685303, 'eval_runtime': 9.789, 'eval_samples_per_second': 102.155, 'eval_steps_per_second': 6.436, 'epoch': 0.92}
{'loss': 1.1773, 'grad_norm': 0.23180793225765228, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3102116584777832, 'eval_runtime': 9.785, 'eval_samples_per_second': 102.197, 'eval_steps_per_second': 6.438, 'epoch': 0.96}
{'loss': 1.1806, 'grad_norm': 0.2619883120059967, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3104159832000732, 'eval_runtime': 9.755, 'eval_samples_per_second': 102.512, 'eval_steps_per_second': 6.458, 'epoch': 1.0}
{'train_runtime': 433.8309, 'train_samples_per_second': 23.041, 'train_steps_per_second': 1.441, 'train_loss': 1.3592582916259766, 'epoch': 1.0}
train_results:  {'eval_loss': [2.061042547225952, 1.659989595413208, 1.5721454620361328, 1.5376811027526855, 1.5012761354446411, 1.4206796884536743, 1.38694167137146, 1.3710100650787354, 1.3487114906311035, 1.3365986347198486, 1.3249517679214478, 1.3231536149978638, 1.32387375831604, 1.330471158027649, 1.3217260837554932, 1.3157793283462524, 1.317306399345398, 1.3123993873596191, 1.3151801824569702, 1.3103796243667603, 1.3123114109039307, 1.3106101751327515, 1.3111226558685303, 1.3102116584777832, 1.3104159832000732], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.061042547225952, 1.659989595413208, 1.5721454620361328, 1.5376811027526855, 1.5012761354446411, 1.4206796884536743, 1.38694167137146, 1.3710100650787354, 1.3487114906311035, 1.3365986347198486, 1.3249517679214478, 1.3231536149978638, 1.32387375831604, 1.330471158027649, 1.3217260837554932, 1.3157793283462524, 1.317306399345398, 1.3123993873596191, 1.3151801824569702, 1.3103796243667603, 1.3123114109039307, 1.3106101751327515, 1.3111226558685303, 1.3102116584777832, 1.3104159832000732]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.3104159832000732
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5943 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -0.9646138115274814
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.9437753916641501
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -0.9501124348713206
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -0.9572853778739789
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -0.9804416985812766
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0130, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.0903, dtype=torch.float64), tensor(0.3687, dtype=torch.float64), tensor(0.5280, dtype=torch.float64), 7, 0, 0, 1, 0, 1, 128, 0.0, 28.725442576076304, 0]
normalized proposed parameters for next round by BO: [tensor(0.0130, dtype=torch.float64), tensor(4.5002e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.1533e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0903, dtype=torch.float64), tensor(0.3687, dtype=torch.float64), tensor(0.5280, dtype=torch.float64), tensor(0.2179, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5984, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.013
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.09
  mmlu: 0.369
  arc_challenge: 0.528

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (7,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (28.725442576076304,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  7
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  28.725442576076304
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 33,030,144 || all params: 8,063,291,392 || trainable%: 0.4096
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3091, 'grad_norm': 0.48438820242881775, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1520302295684814, 'eval_runtime': 8.8666, 'eval_samples_per_second': 112.783, 'eval_steps_per_second': 7.105, 'epoch': 0.04}
{'loss': 1.7416, 'grad_norm': 0.14247533679008484, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6691877841949463, 'eval_runtime': 8.9164, 'eval_samples_per_second': 112.153, 'eval_steps_per_second': 7.066, 'epoch': 0.08}
{'loss': 1.4661, 'grad_norm': 0.13756558299064636, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5894559621810913, 'eval_runtime': 8.9348, 'eval_samples_per_second': 111.922, 'eval_steps_per_second': 7.051, 'epoch': 0.12}
{'loss': 1.3105, 'grad_norm': 0.1562184989452362, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5391494035720825, 'eval_runtime': 8.9612, 'eval_samples_per_second': 111.592, 'eval_steps_per_second': 7.03, 'epoch': 0.16}
{'loss': 1.2732, 'grad_norm': 0.19419421255588531, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4667816162109375, 'eval_runtime': 8.9876, 'eval_samples_per_second': 111.265, 'eval_steps_per_second': 7.01, 'epoch': 0.2}
{'loss': 1.255, 'grad_norm': 0.14683300256729126, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.410968542098999, 'eval_runtime': 8.9958, 'eval_samples_per_second': 111.163, 'eval_steps_per_second': 7.003, 'epoch': 0.24}
{'loss': 1.2066, 'grad_norm': 0.18978336453437805, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3796160221099854, 'eval_runtime': 8.9958, 'eval_samples_per_second': 111.163, 'eval_steps_per_second': 7.003, 'epoch': 0.28}
{'loss': 1.1692, 'grad_norm': 0.15501493215560913, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3717658519744873, 'eval_runtime': 9.0102, 'eval_samples_per_second': 110.985, 'eval_steps_per_second': 6.992, 'epoch': 0.32}
{'loss': 1.1669, 'grad_norm': 0.17560800909996033, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.373818278312683, 'eval_runtime': 8.9733, 'eval_samples_per_second': 111.442, 'eval_steps_per_second': 7.021, 'epoch': 0.36}
{'loss': 1.1531, 'grad_norm': 0.15149852633476257, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3623510599136353, 'eval_runtime': 8.9545, 'eval_samples_per_second': 111.676, 'eval_steps_per_second': 7.036, 'epoch': 0.4}
{'loss': 1.1015, 'grad_norm': 0.221708282828331, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3651491403579712, 'eval_runtime': 8.9474, 'eval_samples_per_second': 111.764, 'eval_steps_per_second': 7.041, 'epoch': 0.44}
{'loss': 1.1081, 'grad_norm': 0.2165212631225586, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.363647222518921, 'eval_runtime': 8.9435, 'eval_samples_per_second': 111.813, 'eval_steps_per_second': 7.044, 'epoch': 0.48}
{'loss': 1.0831, 'grad_norm': 0.1982739269733429, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.361034631729126, 'eval_runtime': 8.9299, 'eval_samples_per_second': 111.983, 'eval_steps_per_second': 7.055, 'epoch': 0.52}
{'loss': 1.0637, 'grad_norm': 0.20434927940368652, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3619824647903442, 'eval_runtime': 8.9507, 'eval_samples_per_second': 111.723, 'eval_steps_per_second': 7.039, 'epoch': 0.56}
{'loss': 1.0599, 'grad_norm': 0.22753167152404785, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3618029356002808, 'eval_runtime': 8.9339, 'eval_samples_per_second': 111.933, 'eval_steps_per_second': 7.052, 'epoch': 0.6}
{'loss': 1.1105, 'grad_norm': 0.23325271904468536, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3551996946334839, 'eval_runtime': 8.9269, 'eval_samples_per_second': 112.021, 'eval_steps_per_second': 7.057, 'epoch': 0.64}
{'loss': 1.0996, 'grad_norm': 0.24491627514362335, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3546736240386963, 'eval_runtime': 8.9412, 'eval_samples_per_second': 111.842, 'eval_steps_per_second': 7.046, 'epoch': 0.68}
{'loss': 0.9831, 'grad_norm': 0.23630639910697937, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3551175594329834, 'eval_runtime': 8.9392, 'eval_samples_per_second': 111.867, 'eval_steps_per_second': 7.048, 'epoch': 0.72}
{'loss': 1.0716, 'grad_norm': 0.24979384243488312, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.355062484741211, 'eval_runtime': 8.9291, 'eval_samples_per_second': 111.993, 'eval_steps_per_second': 7.056, 'epoch': 0.76}
{'loss': 1.0794, 'grad_norm': 0.21947400271892548, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3504791259765625, 'eval_runtime': 8.9501, 'eval_samples_per_second': 111.73, 'eval_steps_per_second': 7.039, 'epoch': 0.8}
{'loss': 1.0246, 'grad_norm': 0.20345045626163483, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3532150983810425, 'eval_runtime': 8.9447, 'eval_samples_per_second': 111.798, 'eval_steps_per_second': 7.043, 'epoch': 0.84}
{'loss': 0.9904, 'grad_norm': 0.25501200556755066, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3520488739013672, 'eval_runtime': 8.9369, 'eval_samples_per_second': 111.896, 'eval_steps_per_second': 7.049, 'epoch': 0.88}
{'loss': 1.0141, 'grad_norm': 0.23068049550056458, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3514809608459473, 'eval_runtime': 8.9566, 'eval_samples_per_second': 111.649, 'eval_steps_per_second': 7.034, 'epoch': 0.92}
{'loss': 1.0358, 'grad_norm': 0.29847919940948486, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3509109020233154, 'eval_runtime': 8.9321, 'eval_samples_per_second': 111.956, 'eval_steps_per_second': 7.053, 'epoch': 0.96}
{'loss': 0.9979, 'grad_norm': 0.28970766067504883, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.350144386291504, 'eval_runtime': 8.9551, 'eval_samples_per_second': 111.669, 'eval_steps_per_second': 7.035, 'epoch': 1.0}
{'train_runtime': 332.4452, 'train_samples_per_second': 30.074, 'train_steps_per_second': 1.88, 'train_loss': 1.2349934417724608, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1520302295684814, 1.6691877841949463, 1.5894559621810913, 1.5391494035720825, 1.4667816162109375, 1.410968542098999, 1.3796160221099854, 1.3717658519744873, 1.373818278312683, 1.3623510599136353, 1.3651491403579712, 1.363647222518921, 1.361034631729126, 1.3619824647903442, 1.3618029356002808, 1.3551996946334839, 1.3546736240386963, 1.3551175594329834, 1.355062484741211, 1.3504791259765625, 1.3532150983810425, 1.3520488739013672, 1.3514809608459473, 1.3509109020233154, 1.350144386291504], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1520302295684814, 1.6691877841949463, 1.5894559621810913, 1.5391494035720825, 1.4667816162109375, 1.410968542098999, 1.3796160221099854, 1.3717658519744873, 1.373818278312683, 1.3623510599136353, 1.3651491403579712, 1.363647222518921, 1.361034631729126, 1.3619824647903442, 1.3618029356002808, 1.3551996946334839, 1.3546736240386963, 1.3551175594329834, 1.355062484741211, 1.3504791259765625, 1.3532150983810425, 1.3520488739013672, 1.3514809608459473, 1.3509109020233154, 1.350144386291504]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.350144386291504
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.8120 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -0.9278120189755046
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -0.9212676059481124
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -0.9278379787417506
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.9352761863577759
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -0.9278949542201864
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0204, dtype=torch.float64), tensor(0.0716, dtype=torch.float64), tensor(0.1794, dtype=torch.float64), tensor(0.1563, dtype=torch.float64), tensor(0.0204, dtype=torch.float64), tensor(0.3824, dtype=torch.float64), tensor(0.1680, dtype=torch.float64), 17, 0, 0, 1, 0, 1, 78, 0.020429312308463978, 19.683460321773087, 1]
normalized proposed parameters for next round by BO: [tensor(0.0015, dtype=torch.float64), tensor(1.0166e-18, dtype=torch.float64), tensor(0.0204, dtype=torch.float64), tensor(0.0716, dtype=torch.float64), tensor(0.1794, dtype=torch.float64), tensor(0.1563, dtype=torch.float64), tensor(0.0204, dtype=torch.float64), tensor(0.3824, dtype=torch.float64), tensor(0.1680, dtype=torch.float64), tensor(0.5466, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6096, dtype=torch.float64), tensor(0.2043, dtype=torch.float64), tensor(0.4101, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.02
  sciq: 0.072
  triviaqa: 0.179
  truthfulqa_gen: 0.156
  wikitext: 0.02
  mmlu: 0.382
  arc_challenge: 0.168

LoRA Parameters:
  lora_r: (78,)
  lora_dropout: (0.020429312308463978,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (19.683460321773087,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  78
lora dropout:  0.020429312308463978
lora alpha:  19.683460321773087
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 48,881,664 || all params: 8,079,142,912 || trainable%: 0.6050
length of training data:  9982
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4057, 'grad_norm': 0.5092615485191345, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9591577053070068, 'eval_runtime': 9.3185, 'eval_samples_per_second': 107.313, 'eval_steps_per_second': 6.761, 'epoch': 0.04}
{'loss': 1.8558, 'grad_norm': 0.2710516154766083, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6401923894882202, 'eval_runtime': 9.3098, 'eval_samples_per_second': 107.413, 'eval_steps_per_second': 6.767, 'epoch': 0.08}
{'loss': 1.4587, 'grad_norm': 0.21174900233745575, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.4855263233184814, 'eval_runtime': 9.3363, 'eval_samples_per_second': 107.109, 'eval_steps_per_second': 6.748, 'epoch': 0.12}
{'loss': 1.4115, 'grad_norm': 0.17126266658306122, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.442806363105774, 'eval_runtime': 9.3616, 'eval_samples_per_second': 106.819, 'eval_steps_per_second': 6.73, 'epoch': 0.16}
{'loss': 1.3387, 'grad_norm': 0.2757593095302582, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.3799916505813599, 'eval_runtime': 9.3725, 'eval_samples_per_second': 106.695, 'eval_steps_per_second': 6.722, 'epoch': 0.2}
{'loss': 1.2769, 'grad_norm': 0.18381071090698242, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 1.3668458461761475, 'eval_runtime': 9.3937, 'eval_samples_per_second': 106.454, 'eval_steps_per_second': 6.707, 'epoch': 0.24}
{'loss': 1.2254, 'grad_norm': 0.21092340350151062, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 1.3514823913574219, 'eval_runtime': 9.3849, 'eval_samples_per_second': 106.554, 'eval_steps_per_second': 6.713, 'epoch': 0.28}
{'loss': 1.2185, 'grad_norm': 0.12976977229118347, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 1.3499364852905273, 'eval_runtime': 9.3995, 'eval_samples_per_second': 106.389, 'eval_steps_per_second': 6.702, 'epoch': 0.32}
{'loss': 1.2394, 'grad_norm': 0.2456713169813156, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 1.3470370769500732, 'eval_runtime': 9.3988, 'eval_samples_per_second': 106.396, 'eval_steps_per_second': 6.703, 'epoch': 0.36}
{'loss': 1.2505, 'grad_norm': 0.17378784716129303, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 1.337350845336914, 'eval_runtime': 9.3924, 'eval_samples_per_second': 106.469, 'eval_steps_per_second': 6.708, 'epoch': 0.4}
{'loss': 1.2237, 'grad_norm': 0.15263241529464722, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 1.3329143524169922, 'eval_runtime': 9.3889, 'eval_samples_per_second': 106.509, 'eval_steps_per_second': 6.71, 'epoch': 0.44}
{'loss': 1.2537, 'grad_norm': 0.2033655345439911, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 1.3303322792053223, 'eval_runtime': 9.3879, 'eval_samples_per_second': 106.52, 'eval_steps_per_second': 6.711, 'epoch': 0.48}
{'loss': 1.2125, 'grad_norm': 0.17981041967868805, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 1.323390007019043, 'eval_runtime': 9.3812, 'eval_samples_per_second': 106.597, 'eval_steps_per_second': 6.716, 'epoch': 0.52}
{'loss': 1.1356, 'grad_norm': 0.17916734516620636, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 1.3174231052398682, 'eval_runtime': 9.374, 'eval_samples_per_second': 106.678, 'eval_steps_per_second': 6.721, 'epoch': 0.56}
{'loss': 1.1986, 'grad_norm': 0.1761225461959839, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 1.3143059015274048, 'eval_runtime': 9.3626, 'eval_samples_per_second': 106.808, 'eval_steps_per_second': 6.729, 'epoch': 0.6}
{'loss': 1.1767, 'grad_norm': 0.16209404170513153, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 1.3024176359176636, 'eval_runtime': 9.33, 'eval_samples_per_second': 107.181, 'eval_steps_per_second': 6.752, 'epoch': 0.64}
{'loss': 1.1347, 'grad_norm': 0.3224277198314667, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 1.2957262992858887, 'eval_runtime': 9.3244, 'eval_samples_per_second': 107.246, 'eval_steps_per_second': 6.756, 'epoch': 0.68}
{'loss': 1.1542, 'grad_norm': 0.17630261182785034, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 1.2872990369796753, 'eval_runtime': 9.387, 'eval_samples_per_second': 106.53, 'eval_steps_per_second': 6.711, 'epoch': 0.72}
{'loss': 1.0949, 'grad_norm': 0.18239545822143555, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 1.277429461479187, 'eval_runtime': 9.3799, 'eval_samples_per_second': 106.611, 'eval_steps_per_second': 6.716, 'epoch': 0.76}
{'loss': 1.1146, 'grad_norm': 0.3055531978607178, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 1.273043155670166, 'eval_runtime': 9.3667, 'eval_samples_per_second': 106.761, 'eval_steps_per_second': 6.726, 'epoch': 0.8}
{'loss': 1.1054, 'grad_norm': 0.2058052122592926, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 1.2694889307022095, 'eval_runtime': 9.3831, 'eval_samples_per_second': 106.575, 'eval_steps_per_second': 6.714, 'epoch': 0.84}
{'loss': 1.1495, 'grad_norm': 0.22549374401569366, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 1.2700614929199219, 'eval_runtime': 9.3719, 'eval_samples_per_second': 106.702, 'eval_steps_per_second': 6.722, 'epoch': 0.88}
{'loss': 1.1685, 'grad_norm': 0.18926402926445007, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 1.264485239982605, 'eval_runtime': 9.3764, 'eval_samples_per_second': 106.65, 'eval_steps_per_second': 6.719, 'epoch': 0.92}
{'loss': 1.121, 'grad_norm': 0.2281097173690796, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 1.2641600370407104, 'eval_runtime': 9.3686, 'eval_samples_per_second': 106.74, 'eval_steps_per_second': 6.725, 'epoch': 0.96}
{'train_runtime': 412.5532, 'train_samples_per_second': 24.196, 'train_steps_per_second': 1.513, 'train_loss': 1.3234255772370558, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9591577053070068, 1.6401923894882202, 1.4855263233184814, 1.442806363105774, 1.3799916505813599, 1.3668458461761475, 1.3514823913574219, 1.3499364852905273, 1.3470370769500732, 1.337350845336914, 1.3329143524169922, 1.3303322792053223, 1.323390007019043, 1.3174231052398682, 1.3143059015274048, 1.3024176359176636, 1.2957262992858887, 1.2872990369796753, 1.277429461479187, 1.273043155670166, 1.2694889307022095, 1.2700614929199219, 1.264485239982605, 1.2641600370407104], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9591577053070068, 1.6401923894882202, 1.4855263233184814, 1.442806363105774, 1.3799916505813599, 1.3668458461761475, 1.3514823913574219, 1.3499364852905273, 1.3470370769500732, 1.337350845336914, 1.3329143524169922, 1.3303322792053223, 1.323390007019043, 1.3174231052398682, 1.3143059015274048, 1.3024176359176636, 1.2957262992858887, 1.2872990369796753, 1.277429461479187, 1.273043155670166, 1.2694889307022095, 1.2700614929199219, 1.264485239982605, 1.2641600370407104]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2641600370407104
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9976 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.35491812229156494, 0.0927686095237732, 0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7262229919433594, 0.8165992498397827, 0.8520699143409729, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9559857845306396, 0.3273009657859802, 0.9828505516052246, 0.5352497100830078]  ‚Üí  acq = -0.9575892412945182
X = [0.8189860582351685, 0.7869956493377686, 0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8973821401596069, 0.7314273715019226, 0.9184417128562927, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9420246481895447, 0.9331071972846985, 0.31236356496810913, 0.36077266931533813]  ‚Üí  acq = -0.9564434203373938
X = [0.7738390564918518, 0.5021045207977295, 0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5741828680038452, 0.5517953038215637, 0.7892404794692993, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9668935537338257, 0.5031551122665405, 0.7251023054122925, 0.2269490361213684]  ‚Üí  acq = -0.9529799036344528
X = [0.20103693008422852, 0.27278316020965576, 0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.14547844231128693, 0.0291365385055542, 0.3460528254508972, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.32679685950279236, 0.4884251356124878, 0.9665257930755615, 0.2274898886680603]  ‚Üí  acq = -0.9578505645656562
X = [0.6374526023864746, 0.36883002519607544, 0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3681538701057434, 0.6981962323188782, 0.7622081637382507, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.5123441219329834, 0.3907546401023865, 0.21269100904464722, 0.09688013792037964]  ‚Üí  acq = -0.9604063913891103
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0137, dtype=torch.float64), 0, 0, tensor(0.2502, dtype=torch.float64), 0, tensor(0.0800, dtype=torch.float64), tensor(0.0204, dtype=torch.float64), tensor(0.1940, dtype=torch.float64), tensor(0.4416, dtype=torch.float64), 24, 0, 0, 1, 0, 1, 87, 3.469446951953616e-18, 11.78877862546179, 0]
normalized proposed parameters for next round by BO: [tensor(0.0137, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.0776e-17, dtype=torch.float64), tensor(0.2502, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0800, dtype=torch.float64), tensor(0.0204, dtype=torch.float64), tensor(0.1940, dtype=torch.float64), tensor(0.4416, dtype=torch.float64), tensor(0.7363, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6773, dtype=torch.float64), tensor(3.4694e-17, dtype=torch.float64), tensor(0.2456, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.014
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.25
  triviaqa: 0
  truthfulqa_gen: 0.08
  wikitext: 0.02
  mmlu: 0.194
  arc_challenge: 0.442

LoRA Parameters:
  lora_r: (87,)
  lora_dropout: (3.469446951953616e-18,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (11.78877862546179,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  87
lora dropout:  3.469446951953616e-18
lora alpha:  11.78877862546179
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 76,972,032 || all params: 8,107,233,280 || trainable%: 0.9494
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3244, 'grad_norm': 0.30060121417045593, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0009844303131104, 'eval_runtime': 9.8936, 'eval_samples_per_second': 101.076, 'eval_steps_per_second': 6.368, 'epoch': 0.04}
{'loss': 1.5527, 'grad_norm': 0.1758771389722824, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6149792671203613, 'eval_runtime': 9.9179, 'eval_samples_per_second': 100.828, 'eval_steps_per_second': 6.352, 'epoch': 0.08}
{'loss': 1.2203, 'grad_norm': 0.10236203670501709, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.547508955001831, 'eval_runtime': 9.9544, 'eval_samples_per_second': 100.458, 'eval_steps_per_second': 6.329, 'epoch': 0.12}
{'loss': 1.2019, 'grad_norm': 0.1277845799922943, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4651005268096924, 'eval_runtime': 9.9639, 'eval_samples_per_second': 100.362, 'eval_steps_per_second': 6.323, 'epoch': 0.16}
{'loss': 1.1284, 'grad_norm': 0.19912783801555634, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3835880756378174, 'eval_runtime': 9.9764, 'eval_samples_per_second': 100.237, 'eval_steps_per_second': 6.315, 'epoch': 0.2}
{'loss': 1.0473, 'grad_norm': 0.15490837395191193, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3309868574142456, 'eval_runtime': 9.9588, 'eval_samples_per_second': 100.414, 'eval_steps_per_second': 6.326, 'epoch': 0.24}
{'loss': 0.9714, 'grad_norm': 0.16025753319263458, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3155639171600342, 'eval_runtime': 9.9576, 'eval_samples_per_second': 100.426, 'eval_steps_per_second': 6.327, 'epoch': 0.28}
{'loss': 0.9396, 'grad_norm': 0.14627192914485931, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3075201511383057, 'eval_runtime': 9.9666, 'eval_samples_per_second': 100.335, 'eval_steps_per_second': 6.321, 'epoch': 0.32}
{'loss': 0.8918, 'grad_norm': 0.15409094095230103, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3081457614898682, 'eval_runtime': 9.981, 'eval_samples_per_second': 100.19, 'eval_steps_per_second': 6.312, 'epoch': 0.36}
{'loss': 0.9202, 'grad_norm': 0.1712150275707245, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3027387857437134, 'eval_runtime': 9.9775, 'eval_samples_per_second': 100.226, 'eval_steps_per_second': 6.314, 'epoch': 0.4}
{'loss': 0.8948, 'grad_norm': 0.14329737424850464, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3064148426055908, 'eval_runtime': 9.9748, 'eval_samples_per_second': 100.253, 'eval_steps_per_second': 6.316, 'epoch': 0.44}
{'loss': 0.8921, 'grad_norm': 0.17503991723060608, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3052289485931396, 'eval_runtime': 9.9991, 'eval_samples_per_second': 100.009, 'eval_steps_per_second': 6.301, 'epoch': 0.48}
{'loss': 0.8798, 'grad_norm': 0.19316469132900238, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.298166275024414, 'eval_runtime': 10.005, 'eval_samples_per_second': 99.95, 'eval_steps_per_second': 6.297, 'epoch': 0.52}
{'loss': 0.8769, 'grad_norm': 0.19600215554237366, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2966594696044922, 'eval_runtime': 10.0018, 'eval_samples_per_second': 99.982, 'eval_steps_per_second': 6.299, 'epoch': 0.56}
{'loss': 0.8606, 'grad_norm': 0.225082129240036, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2935056686401367, 'eval_runtime': 10.0088, 'eval_samples_per_second': 99.912, 'eval_steps_per_second': 6.294, 'epoch': 0.6}
{'loss': 0.7906, 'grad_norm': 0.2195063680410385, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.297422170639038, 'eval_runtime': 9.9812, 'eval_samples_per_second': 100.189, 'eval_steps_per_second': 6.312, 'epoch': 0.64}
{'loss': 0.8549, 'grad_norm': 0.22058378159999847, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2935363054275513, 'eval_runtime': 9.9766, 'eval_samples_per_second': 100.234, 'eval_steps_per_second': 6.315, 'epoch': 0.68}
{'loss': 0.8445, 'grad_norm': 0.2478397786617279, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2947322130203247, 'eval_runtime': 9.9797, 'eval_samples_per_second': 100.203, 'eval_steps_per_second': 6.313, 'epoch': 0.72}
{'loss': 0.8842, 'grad_norm': 0.24220554530620575, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2938880920410156, 'eval_runtime': 9.985, 'eval_samples_per_second': 100.15, 'eval_steps_per_second': 6.309, 'epoch': 0.76}
{'loss': 0.7618, 'grad_norm': 0.2639577090740204, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2923436164855957, 'eval_runtime': 9.9804, 'eval_samples_per_second': 100.196, 'eval_steps_per_second': 6.312, 'epoch': 0.8}
{'loss': 0.7794, 'grad_norm': 0.31254667043685913, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2946358919143677, 'eval_runtime': 9.9442, 'eval_samples_per_second': 100.561, 'eval_steps_per_second': 6.335, 'epoch': 0.84}
{'loss': 0.7986, 'grad_norm': 0.3042997121810913, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2913762331008911, 'eval_runtime': 9.9598, 'eval_samples_per_second': 100.403, 'eval_steps_per_second': 6.325, 'epoch': 0.88}
{'loss': 0.75, 'grad_norm': 0.28279542922973633, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2914236783981323, 'eval_runtime': 9.95, 'eval_samples_per_second': 100.503, 'eval_steps_per_second': 6.332, 'epoch': 0.92}
{'loss': 0.8224, 'grad_norm': 0.2823416590690613, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2907984256744385, 'eval_runtime': 9.9498, 'eval_samples_per_second': 100.505, 'eval_steps_per_second': 6.332, 'epoch': 0.96}
{'loss': 0.8267, 'grad_norm': 0.3045988082885742, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2898694276809692, 'eval_runtime': 9.9649, 'eval_samples_per_second': 100.352, 'eval_steps_per_second': 6.322, 'epoch': 1.0}
{'train_runtime': 412.0571, 'train_samples_per_second': 24.261, 'train_steps_per_second': 1.517, 'train_loss': 1.0286085662841797, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0009844303131104, 1.6149792671203613, 1.547508955001831, 1.4651005268096924, 1.3835880756378174, 1.3309868574142456, 1.3155639171600342, 1.3075201511383057, 1.3081457614898682, 1.3027387857437134, 1.3064148426055908, 1.3052289485931396, 1.298166275024414, 1.2966594696044922, 1.2935056686401367, 1.297422170639038, 1.2935363054275513, 1.2947322130203247, 1.2938880920410156, 1.2923436164855957, 1.2946358919143677, 1.2913762331008911, 1.2914236783981323, 1.2907984256744385, 1.2898694276809692], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0009844303131104, 1.6149792671203613, 1.547508955001831, 1.4651005268096924, 1.3835880756378174, 1.3309868574142456, 1.3155639171600342, 1.3075201511383057, 1.3081457614898682, 1.3027387857437134, 1.3064148426055908, 1.3052289485931396, 1.298166275024414, 1.2966594696044922, 1.2935056686401367, 1.297422170639038, 1.2935363054275513, 1.2947322130203247, 1.2938880920410156, 1.2923436164855957, 1.2946358919143677, 1.2913762331008911, 1.2914236783981323, 1.2907984256744385, 1.2898694276809692]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2898694276809692
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6747 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -0.9635084831825049
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.9805657197440165
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -0.9660517459481128
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -0.9645972023029195
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -0.9689045104770536
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.6808, dtype=torch.float64), tensor(0.2627, dtype=torch.float64), 0, tensor(0.0565, dtype=torch.float64), 0, 0, 0, 0, 0, 27, 1, 0, 1, 0, 1, 39, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.6808, dtype=torch.float64), tensor(0.2627, dtype=torch.float64), tensor(1.9658e-17, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), tensor(1.7090e-17, dtype=torch.float64), tensor(2.4664e-17, dtype=torch.float64), tensor(1.1693e-17, dtype=torch.float64), tensor(8.0507e-18, dtype=torch.float64), tensor(4.9679e-18, dtype=torch.float64), tensor(0.8400, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3036, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.681
  gsm8k: 0.263
  rowan_hellaswag: 0
  sciq: 0.056
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (39,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  39
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 47,443,968 || all params: 8,077,705,216 || trainable%: 0.5873
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5752, 'grad_norm': 0.5069500803947449, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.332165479660034, 'eval_runtime': 10.167, 'eval_samples_per_second': 98.357, 'eval_steps_per_second': 6.197, 'epoch': 0.04}
{'loss': 1.1944, 'grad_norm': 0.2859593629837036, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.2401678562164307, 'eval_runtime': 10.176, 'eval_samples_per_second': 98.27, 'eval_steps_per_second': 6.191, 'epoch': 0.08}
{'loss': 1.0279, 'grad_norm': 0.3350730240345001, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.173220157623291, 'eval_runtime': 10.2044, 'eval_samples_per_second': 97.997, 'eval_steps_per_second': 6.174, 'epoch': 0.12}
{'loss': 0.9708, 'grad_norm': 0.27531781792640686, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1711392402648926, 'eval_runtime': 10.2259, 'eval_samples_per_second': 97.791, 'eval_steps_per_second': 6.161, 'epoch': 0.16}
{'loss': 0.938, 'grad_norm': 0.22727571427822113, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.1408257484436035, 'eval_runtime': 10.2367, 'eval_samples_per_second': 97.688, 'eval_steps_per_second': 6.154, 'epoch': 0.2}
{'loss': 0.9314, 'grad_norm': 0.27686870098114014, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.125185012817383, 'eval_runtime': 10.2438, 'eval_samples_per_second': 97.62, 'eval_steps_per_second': 6.15, 'epoch': 0.24}
{'loss': 0.9017, 'grad_norm': 0.2647014856338501, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.1560256481170654, 'eval_runtime': 10.2506, 'eval_samples_per_second': 97.555, 'eval_steps_per_second': 6.146, 'epoch': 0.28}
{'loss': 0.8904, 'grad_norm': 0.299137145280838, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.1508688926696777, 'eval_runtime': 10.2737, 'eval_samples_per_second': 97.336, 'eval_steps_per_second': 6.132, 'epoch': 0.32}
{'loss': 0.8369, 'grad_norm': 0.30140990018844604, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.1248552799224854, 'eval_runtime': 10.3003, 'eval_samples_per_second': 97.084, 'eval_steps_per_second': 6.116, 'epoch': 0.36}
{'loss': 0.8248, 'grad_norm': 0.25465184450149536, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.108273983001709, 'eval_runtime': 10.2949, 'eval_samples_per_second': 97.135, 'eval_steps_per_second': 6.12, 'epoch': 0.4}
{'loss': 0.8284, 'grad_norm': 0.2853870391845703, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.098513603210449, 'eval_runtime': 10.3103, 'eval_samples_per_second': 96.99, 'eval_steps_per_second': 6.11, 'epoch': 0.44}
{'loss': 0.8311, 'grad_norm': 0.26950839161872864, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.0745108127593994, 'eval_runtime': 10.2941, 'eval_samples_per_second': 97.143, 'eval_steps_per_second': 6.12, 'epoch': 0.48}
{'loss': 0.8262, 'grad_norm': 0.28446558117866516, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.149545431137085, 'eval_runtime': 10.2581, 'eval_samples_per_second': 97.484, 'eval_steps_per_second': 6.142, 'epoch': 0.52}
{'loss': 0.8225, 'grad_norm': 0.2619171142578125, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.170921802520752, 'eval_runtime': 10.2566, 'eval_samples_per_second': 97.498, 'eval_steps_per_second': 6.142, 'epoch': 0.56}
{'loss': 0.8082, 'grad_norm': 0.2941332757472992, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.1380789279937744, 'eval_runtime': 10.2598, 'eval_samples_per_second': 97.468, 'eval_steps_per_second': 6.14, 'epoch': 0.6}
{'loss': 0.7934, 'grad_norm': 0.2923542559146881, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.1435930728912354, 'eval_runtime': 10.2745, 'eval_samples_per_second': 97.328, 'eval_steps_per_second': 6.132, 'epoch': 0.64}
{'loss': 0.8086, 'grad_norm': 0.28389692306518555, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2024102210998535, 'eval_runtime': 10.2636, 'eval_samples_per_second': 97.432, 'eval_steps_per_second': 6.138, 'epoch': 0.68}
{'loss': 0.7823, 'grad_norm': 0.3717970848083496, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.1743433475494385, 'eval_runtime': 10.2709, 'eval_samples_per_second': 97.362, 'eval_steps_per_second': 6.134, 'epoch': 0.72}
{'loss': 0.7992, 'grad_norm': 0.3183193802833557, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.157043695449829, 'eval_runtime': 10.2693, 'eval_samples_per_second': 97.378, 'eval_steps_per_second': 6.135, 'epoch': 0.76}
{'loss': 0.7902, 'grad_norm': 0.31532222032546997, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.1648447513580322, 'eval_runtime': 10.2506, 'eval_samples_per_second': 97.555, 'eval_steps_per_second': 6.146, 'epoch': 0.8}
{'loss': 0.7819, 'grad_norm': 0.3248165547847748, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.1488819122314453, 'eval_runtime': 10.2621, 'eval_samples_per_second': 97.446, 'eval_steps_per_second': 6.139, 'epoch': 0.84}
{'loss': 0.7737, 'grad_norm': 0.3070811927318573, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.1640679836273193, 'eval_runtime': 10.2579, 'eval_samples_per_second': 97.486, 'eval_steps_per_second': 6.142, 'epoch': 0.88}
{'loss': 0.7869, 'grad_norm': 0.32945922017097473, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1771609783172607, 'eval_runtime': 10.2564, 'eval_samples_per_second': 97.5, 'eval_steps_per_second': 6.143, 'epoch': 0.92}
{'loss': 0.7868, 'grad_norm': 0.3744654357433319, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.177342176437378, 'eval_runtime': 10.2562, 'eval_samples_per_second': 97.502, 'eval_steps_per_second': 6.143, 'epoch': 0.96}
{'loss': 0.7787, 'grad_norm': 0.3731018006801605, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.1777353286743164, 'eval_runtime': 10.261, 'eval_samples_per_second': 97.457, 'eval_steps_per_second': 6.14, 'epoch': 1.0}
{'train_runtime': 474.8453, 'train_samples_per_second': 21.057, 'train_steps_per_second': 1.316, 'train_loss': 0.9235936004638672, 'epoch': 1.0}
train_results:  {'eval_loss': [2.332165479660034, 2.2401678562164307, 2.173220157623291, 2.1711392402648926, 2.1408257484436035, 2.125185012817383, 2.1560256481170654, 2.1508688926696777, 2.1248552799224854, 2.108273983001709, 2.098513603210449, 2.0745108127593994, 2.149545431137085, 2.170921802520752, 2.1380789279937744, 2.1435930728912354, 2.2024102210998535, 2.1743433475494385, 2.157043695449829, 2.1648447513580322, 2.1488819122314453, 2.1640679836273193, 2.1771609783172607, 2.177342176437378, 2.1777353286743164], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.332165479660034, 2.2401678562164307, 2.173220157623291, 2.1711392402648926, 2.1408257484436035, 2.125185012817383, 2.1560256481170654, 2.1508688926696777, 2.1248552799224854, 2.108273983001709, 2.098513603210449, 2.0745108127593994, 2.149545431137085, 2.170921802520752, 2.1380789279937744, 2.1435930728912354, 2.2024102210998535, 2.1743433475494385, 2.157043695449829, 2.1648447513580322, 2.1488819122314453, 2.1640679836273193, 2.1771609783172607, 2.177342176437378, 2.1777353286743164]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.1777353286743164
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1733 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.9644683553005854
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.9775544973217667
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.9779751067146267
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.9775666084748451
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -0.966322456243608
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0451, dtype=torch.float64), 0, 0, 0, tensor(0.0601, dtype=torch.float64), 0, tensor(0.2749, dtype=torch.float64), tensor(0.6199, dtype=torch.float64), 10, 1, 1, 1, 1, 1, 128, 2.7755575615629192e-18, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0451, dtype=torch.float64), tensor(8.9038e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0601, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2749, dtype=torch.float64), tensor(0.6199, dtype=torch.float64), tensor(0.3177, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.7756e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.045
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.06
  wikitext: 0
  mmlu: 0.275
  arc_challenge: 0.62

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.7755575615629192e-18,)
  num_layers_to_apply: (10,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  10
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  2.7755575615629192e-18
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 87,818,240 || all params: 8,118,079,488 || trainable%: 1.0818
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.814, 'grad_norm': 0.7819940447807312, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7612195014953613, 'eval_runtime': 9.3196, 'eval_samples_per_second': 107.3, 'eval_steps_per_second': 6.76, 'epoch': 0.04}
{'loss': 1.2512, 'grad_norm': 0.3070319890975952, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.479220986366272, 'eval_runtime': 9.3294, 'eval_samples_per_second': 107.188, 'eval_steps_per_second': 6.753, 'epoch': 0.08}
{'loss': 1.059, 'grad_norm': 0.19479899108409882, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.410338282585144, 'eval_runtime': 9.3429, 'eval_samples_per_second': 107.033, 'eval_steps_per_second': 6.743, 'epoch': 0.12}
{'loss': 1.0641, 'grad_norm': 0.2049540877342224, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3861701488494873, 'eval_runtime': 9.3815, 'eval_samples_per_second': 106.593, 'eval_steps_per_second': 6.715, 'epoch': 0.16}
{'loss': 1.001, 'grad_norm': 0.22677454352378845, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.371279239654541, 'eval_runtime': 9.3823, 'eval_samples_per_second': 106.584, 'eval_steps_per_second': 6.715, 'epoch': 0.2}
{'loss': 0.996, 'grad_norm': 0.20436972379684448, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3663474321365356, 'eval_runtime': 9.3905, 'eval_samples_per_second': 106.491, 'eval_steps_per_second': 6.709, 'epoch': 0.24}
{'loss': 0.9382, 'grad_norm': 0.2286672443151474, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3580738306045532, 'eval_runtime': 9.4181, 'eval_samples_per_second': 106.178, 'eval_steps_per_second': 6.689, 'epoch': 0.28}
{'loss': 0.9812, 'grad_norm': 0.2453555017709732, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.354093313217163, 'eval_runtime': 9.4217, 'eval_samples_per_second': 106.138, 'eval_steps_per_second': 6.687, 'epoch': 0.32}
{'loss': 0.9104, 'grad_norm': 0.23061110079288483, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3499681949615479, 'eval_runtime': 9.4394, 'eval_samples_per_second': 105.939, 'eval_steps_per_second': 6.674, 'epoch': 0.36}
{'loss': 0.8995, 'grad_norm': 0.2464759349822998, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3452497720718384, 'eval_runtime': 9.4755, 'eval_samples_per_second': 105.535, 'eval_steps_per_second': 6.649, 'epoch': 0.4}
{'loss': 0.867, 'grad_norm': 0.2475271224975586, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.346274733543396, 'eval_runtime': 9.4831, 'eval_samples_per_second': 105.451, 'eval_steps_per_second': 6.643, 'epoch': 0.44}
{'loss': 0.8874, 'grad_norm': 0.2614901661872864, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3470823764801025, 'eval_runtime': 9.4842, 'eval_samples_per_second': 105.438, 'eval_steps_per_second': 6.643, 'epoch': 0.48}
{'loss': 0.8174, 'grad_norm': 0.3031778633594513, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.344299077987671, 'eval_runtime': 9.4899, 'eval_samples_per_second': 105.375, 'eval_steps_per_second': 6.639, 'epoch': 0.52}
{'loss': 0.7907, 'grad_norm': 0.3151096701622009, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3423998355865479, 'eval_runtime': 9.4854, 'eval_samples_per_second': 105.425, 'eval_steps_per_second': 6.642, 'epoch': 0.56}
{'loss': 0.8386, 'grad_norm': 0.31946679949760437, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3410470485687256, 'eval_runtime': 9.4924, 'eval_samples_per_second': 105.347, 'eval_steps_per_second': 6.637, 'epoch': 0.6}
{'loss': 0.747, 'grad_norm': 0.328107088804245, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3374078273773193, 'eval_runtime': 9.4859, 'eval_samples_per_second': 105.42, 'eval_steps_per_second': 6.641, 'epoch': 0.64}
{'loss': 0.7287, 'grad_norm': 0.3247348964214325, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3365825414657593, 'eval_runtime': 9.4908, 'eval_samples_per_second': 105.365, 'eval_steps_per_second': 6.638, 'epoch': 0.68}
{'loss': 0.7016, 'grad_norm': 0.29928892850875854, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3354748487472534, 'eval_runtime': 9.4545, 'eval_samples_per_second': 105.77, 'eval_steps_per_second': 6.663, 'epoch': 0.72}
{'loss': 0.7107, 'grad_norm': 0.34209102392196655, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3362263441085815, 'eval_runtime': 9.4341, 'eval_samples_per_second': 105.999, 'eval_steps_per_second': 6.678, 'epoch': 0.76}
{'loss': 0.7203, 'grad_norm': 0.2855518162250519, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3324978351593018, 'eval_runtime': 9.47, 'eval_samples_per_second': 105.596, 'eval_steps_per_second': 6.653, 'epoch': 0.8}
{'loss': 0.6497, 'grad_norm': 0.2507916986942291, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3347431421279907, 'eval_runtime': 9.5107, 'eval_samples_per_second': 105.145, 'eval_steps_per_second': 6.624, 'epoch': 0.84}
{'loss': 0.6593, 'grad_norm': 0.35317492485046387, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3315322399139404, 'eval_runtime': 9.4795, 'eval_samples_per_second': 105.49, 'eval_steps_per_second': 6.646, 'epoch': 0.88}
{'loss': 0.6269, 'grad_norm': 0.3010770082473755, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3312655687332153, 'eval_runtime': 9.5075, 'eval_samples_per_second': 105.18, 'eval_steps_per_second': 6.626, 'epoch': 0.92}
{'loss': 0.6659, 'grad_norm': 0.2811602056026459, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3305412530899048, 'eval_runtime': 9.5092, 'eval_samples_per_second': 105.162, 'eval_steps_per_second': 6.625, 'epoch': 0.96}
{'loss': 0.7204, 'grad_norm': 0.25561532378196716, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.329271674156189, 'eval_runtime': 9.5064, 'eval_samples_per_second': 105.192, 'eval_steps_per_second': 6.627, 'epoch': 1.0}
{'train_runtime': 369.0358, 'train_samples_per_second': 27.092, 'train_steps_per_second': 1.694, 'train_loss': 0.9218509216308594, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7612195014953613, 1.479220986366272, 1.410338282585144, 1.3861701488494873, 1.371279239654541, 1.3663474321365356, 1.3580738306045532, 1.354093313217163, 1.3499681949615479, 1.3452497720718384, 1.346274733543396, 1.3470823764801025, 1.344299077987671, 1.3423998355865479, 1.3410470485687256, 1.3374078273773193, 1.3365825414657593, 1.3354748487472534, 1.3362263441085815, 1.3324978351593018, 1.3347431421279907, 1.3315322399139404, 1.3312655687332153, 1.3305412530899048, 1.329271674156189], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7612195014953613, 1.479220986366272, 1.410338282585144, 1.3861701488494873, 1.371279239654541, 1.3663474321365356, 1.3580738306045532, 1.354093313217163, 1.3499681949615479, 1.3452497720718384, 1.346274733543396, 1.3470823764801025, 1.344299077987671, 1.3423998355865479, 1.3410470485687256, 1.3374078273773193, 1.3365825414657593, 1.3354748487472534, 1.3362263441085815, 1.3324978351593018, 1.3347431421279907, 1.3315322399139404, 1.3312655687332153, 1.3305412530899048, 1.329271674156189]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.329271674156189
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9657 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.9379566456329231
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -0.9866113808136896
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.9866520367427947
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -0.9921981739100048
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -0.964875961116169
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1610, dtype=torch.float64), 0, tensor(0.0420, dtype=torch.float64), 0, tensor(0.7970, dtype=torch.float64), 0, 12, 0, 0, 1, 1, 1, 128, 0.0, 2.8431045386042193, 0]
normalized proposed parameters for next round by BO: [tensor(1.1390e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1610, dtype=torch.float64), tensor(2.4215e-17, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(3.1147e-17, dtype=torch.float64), tensor(0.7970, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3815, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0592, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.161
  triviaqa: 0
  truthfulqa_gen: 0.042
  wikitext: 0
  mmlu: 0.797
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (12,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (2.8431045386042193,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  12
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  2.8431045386042193
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 84,934,656 || all params: 8,115,195,904 || trainable%: 1.0466
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.668, 'grad_norm': 0.21151649951934814, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.5787858963012695, 'eval_runtime': 9.2805, 'eval_samples_per_second': 107.753, 'eval_steps_per_second': 6.788, 'epoch': 0.04}
{'loss': 2.058, 'grad_norm': 0.19017831981182098, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.628284215927124, 'eval_runtime': 9.3168, 'eval_samples_per_second': 107.334, 'eval_steps_per_second': 6.762, 'epoch': 0.08}
{'loss': 1.5379, 'grad_norm': 0.08653992414474487, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4526296854019165, 'eval_runtime': 9.3457, 'eval_samples_per_second': 107.001, 'eval_steps_per_second': 6.741, 'epoch': 0.12}
{'loss': 1.3863, 'grad_norm': 0.0552554726600647, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3973983526229858, 'eval_runtime': 9.3773, 'eval_samples_per_second': 106.641, 'eval_steps_per_second': 6.718, 'epoch': 0.16}
{'loss': 1.3372, 'grad_norm': 0.04783142730593681, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3667292594909668, 'eval_runtime': 9.3662, 'eval_samples_per_second': 106.767, 'eval_steps_per_second': 6.726, 'epoch': 0.2}
{'loss': 1.3021, 'grad_norm': 0.05376807600259781, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3504055738449097, 'eval_runtime': 9.3585, 'eval_samples_per_second': 106.854, 'eval_steps_per_second': 6.732, 'epoch': 0.24}
{'loss': 1.2649, 'grad_norm': 0.047732189297676086, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.342891812324524, 'eval_runtime': 9.3712, 'eval_samples_per_second': 106.71, 'eval_steps_per_second': 6.723, 'epoch': 0.28}
{'loss': 1.2929, 'grad_norm': 0.047682713717222214, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3373643159866333, 'eval_runtime': 9.3762, 'eval_samples_per_second': 106.653, 'eval_steps_per_second': 6.719, 'epoch': 0.32}
{'loss': 1.2881, 'grad_norm': 0.04975837469100952, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.332528829574585, 'eval_runtime': 9.3831, 'eval_samples_per_second': 106.574, 'eval_steps_per_second': 6.714, 'epoch': 0.36}
{'loss': 1.278, 'grad_norm': 0.0475030280649662, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3297748565673828, 'eval_runtime': 9.3962, 'eval_samples_per_second': 106.426, 'eval_steps_per_second': 6.705, 'epoch': 0.4}
{'loss': 1.3084, 'grad_norm': 0.048446446657180786, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3247445821762085, 'eval_runtime': 9.4334, 'eval_samples_per_second': 106.006, 'eval_steps_per_second': 6.678, 'epoch': 0.44}
{'loss': 1.2708, 'grad_norm': 0.0563013032078743, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3213847875595093, 'eval_runtime': 9.4597, 'eval_samples_per_second': 105.711, 'eval_steps_per_second': 6.66, 'epoch': 0.48}
{'loss': 1.2883, 'grad_norm': 0.06134912371635437, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3191965818405151, 'eval_runtime': 9.4521, 'eval_samples_per_second': 105.796, 'eval_steps_per_second': 6.665, 'epoch': 0.52}
{'loss': 1.2921, 'grad_norm': 0.051588091999292374, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3162599802017212, 'eval_runtime': 9.4548, 'eval_samples_per_second': 105.767, 'eval_steps_per_second': 6.663, 'epoch': 0.56}
{'loss': 1.2161, 'grad_norm': 0.056403350085020065, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3130755424499512, 'eval_runtime': 9.4968, 'eval_samples_per_second': 105.299, 'eval_steps_per_second': 6.634, 'epoch': 0.6}
{'loss': 1.2789, 'grad_norm': 0.05043159797787666, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3131417036056519, 'eval_runtime': 9.4489, 'eval_samples_per_second': 105.833, 'eval_steps_per_second': 6.667, 'epoch': 0.64}
{'loss': 1.2532, 'grad_norm': 0.05423261225223541, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3105063438415527, 'eval_runtime': 9.4564, 'eval_samples_per_second': 105.748, 'eval_steps_per_second': 6.662, 'epoch': 0.68}
{'loss': 1.2863, 'grad_norm': 0.04905717074871063, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3078616857528687, 'eval_runtime': 9.4578, 'eval_samples_per_second': 105.733, 'eval_steps_per_second': 6.661, 'epoch': 0.72}
{'loss': 1.2487, 'grad_norm': 0.061935536563396454, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3063621520996094, 'eval_runtime': 9.4445, 'eval_samples_per_second': 105.882, 'eval_steps_per_second': 6.671, 'epoch': 0.76}
{'loss': 1.2651, 'grad_norm': 0.06080232933163643, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3058431148529053, 'eval_runtime': 9.4427, 'eval_samples_per_second': 105.902, 'eval_steps_per_second': 6.672, 'epoch': 0.8}
{'loss': 1.2287, 'grad_norm': 0.06210440769791603, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3048571348190308, 'eval_runtime': 9.4357, 'eval_samples_per_second': 105.981, 'eval_steps_per_second': 6.677, 'epoch': 0.84}
{'loss': 1.2417, 'grad_norm': 0.056525275111198425, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.303851842880249, 'eval_runtime': 9.4122, 'eval_samples_per_second': 106.245, 'eval_steps_per_second': 6.693, 'epoch': 0.88}
{'loss': 1.2463, 'grad_norm': 0.06195954978466034, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3034530878067017, 'eval_runtime': 9.3958, 'eval_samples_per_second': 106.43, 'eval_steps_per_second': 6.705, 'epoch': 0.92}
{'loss': 1.23, 'grad_norm': 0.0556856244802475, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3027244806289673, 'eval_runtime': 9.4441, 'eval_samples_per_second': 105.886, 'eval_steps_per_second': 6.671, 'epoch': 0.96}
{'loss': 1.2695, 'grad_norm': 0.06806148588657379, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3024567365646362, 'eval_runtime': 9.4436, 'eval_samples_per_second': 105.892, 'eval_steps_per_second': 6.671, 'epoch': 1.0}
{'train_runtime': 376.5044, 'train_samples_per_second': 26.555, 'train_steps_per_second': 1.66, 'train_loss': 1.4135082672119141, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5787858963012695, 1.628284215927124, 1.4526296854019165, 1.3973983526229858, 1.3667292594909668, 1.3504055738449097, 1.342891812324524, 1.3373643159866333, 1.332528829574585, 1.3297748565673828, 1.3247445821762085, 1.3213847875595093, 1.3191965818405151, 1.3162599802017212, 1.3130755424499512, 1.3131417036056519, 1.3105063438415527, 1.3078616857528687, 1.3063621520996094, 1.3058431148529053, 1.3048571348190308, 1.303851842880249, 1.3034530878067017, 1.3027244806289673, 1.3024567365646362], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.5787858963012695, 1.628284215927124, 1.4526296854019165, 1.3973983526229858, 1.3667292594909668, 1.3504055738449097, 1.342891812324524, 1.3373643159866333, 1.332528829574585, 1.3297748565673828, 1.3247445821762085, 1.3213847875595093, 1.3191965818405151, 1.3162599802017212, 1.3130755424499512, 1.3131417036056519, 1.3105063438415527, 1.3078616857528687, 1.3063621520996094, 1.3058431148529053, 1.3048571348190308, 1.303851842880249, 1.3034530878067017, 1.3027244806289673, 1.3024567365646362]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0963753461837769
current iteration best possible eval_loss (full train run):  -1.3024567365646362
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6489 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -0.971286567775548
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -0.9778857220834001
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -0.9782055388446129
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -0.9777977156571194
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.97727457812789
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1859, dtype=torch.float64), tensor(0.0139, dtype=torch.float64), tensor(0.0724, dtype=torch.float64), tensor(0.1445, dtype=torch.float64), tensor(0.5832, dtype=torch.float64), 0, 0, 0, 0, 27, 1, 1, 1, 0, 0, 7, 1.3010426069826058e-19, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.1859, dtype=torch.float64), tensor(0.0139, dtype=torch.float64), tensor(0.0724, dtype=torch.float64), tensor(0.1445, dtype=torch.float64), tensor(0.5832, dtype=torch.float64), tensor(5.8041e-18, dtype=torch.float64), tensor(5.4826e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.7507e-18, dtype=torch.float64), tensor(0.8569, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0526, dtype=torch.float64), tensor(1.3010e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.186
  gsm8k: 0.014
  rowan_hellaswag: 0.072
  sciq: 0.145
  triviaqa: 0.583
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (7,)
  lora_dropout: (1.3010426069826058e-19,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  7
lora dropout:  1.3010426069826058e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 5,999,616 || all params: 8,036,260,864 || trainable%: 0.0747
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3898, 'grad_norm': 5.735043048858643, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.061530590057373, 'eval_runtime': 9.7452, 'eval_samples_per_second': 102.615, 'eval_steps_per_second': 6.465, 'epoch': 0.04}
{'loss': 1.5261, 'grad_norm': 1.4338438510894775, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.086082935333252, 'eval_runtime': 9.8064, 'eval_samples_per_second': 101.974, 'eval_steps_per_second': 6.424, 'epoch': 0.08}
{'loss': 1.3432, 'grad_norm': 1.408449649810791, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.045147180557251, 'eval_runtime': 9.8223, 'eval_samples_per_second': 101.809, 'eval_steps_per_second': 6.414, 'epoch': 0.12}
{'loss': 1.2044, 'grad_norm': 0.9952613115310669, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9939428567886353, 'eval_runtime': 9.8216, 'eval_samples_per_second': 101.816, 'eval_steps_per_second': 6.414, 'epoch': 0.16}
{'loss': 1.0873, 'grad_norm': 1.4784756898880005, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.983831763267517, 'eval_runtime': 9.807, 'eval_samples_per_second': 101.968, 'eval_steps_per_second': 6.424, 'epoch': 0.2}
{'loss': 1.1033, 'grad_norm': 1.0472699403762817, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.0026025772094727, 'eval_runtime': 9.821, 'eval_samples_per_second': 101.822, 'eval_steps_per_second': 6.415, 'epoch': 0.24}
{'loss': 1.1237, 'grad_norm': 0.8992383480072021, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.906082272529602, 'eval_runtime': 9.8248, 'eval_samples_per_second': 101.783, 'eval_steps_per_second': 6.412, 'epoch': 0.28}
{'loss': 1.1431, 'grad_norm': 0.9466607570648193, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.91640043258667, 'eval_runtime': 9.824, 'eval_samples_per_second': 101.792, 'eval_steps_per_second': 6.413, 'epoch': 0.32}
{'loss': 1.0529, 'grad_norm': 0.9980488419532776, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9570915699005127, 'eval_runtime': 9.8417, 'eval_samples_per_second': 101.608, 'eval_steps_per_second': 6.401, 'epoch': 0.36}
{'loss': 1.1106, 'grad_norm': 0.9809343218803406, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9266321659088135, 'eval_runtime': 9.8399, 'eval_samples_per_second': 101.628, 'eval_steps_per_second': 6.403, 'epoch': 0.4}
{'loss': 1.0615, 'grad_norm': 1.0290528535842896, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9399805068969727, 'eval_runtime': 9.8495, 'eval_samples_per_second': 101.528, 'eval_steps_per_second': 6.396, 'epoch': 0.44}
{'loss': 1.1336, 'grad_norm': 0.8699468374252319, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9379521608352661, 'eval_runtime': 9.8544, 'eval_samples_per_second': 101.477, 'eval_steps_per_second': 6.393, 'epoch': 0.48}
{'loss': 1.1183, 'grad_norm': 0.8659077882766724, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.95363450050354, 'eval_runtime': 9.8511, 'eval_samples_per_second': 101.512, 'eval_steps_per_second': 6.395, 'epoch': 0.52}
{'loss': 1.0735, 'grad_norm': 0.9843188524246216, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9546754360198975, 'eval_runtime': 9.8476, 'eval_samples_per_second': 101.547, 'eval_steps_per_second': 6.397, 'epoch': 0.56}
{'loss': 1.0742, 'grad_norm': 0.8454841375350952, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9357411861419678, 'eval_runtime': 9.8382, 'eval_samples_per_second': 101.645, 'eval_steps_per_second': 6.404, 'epoch': 0.6}
{'loss': 1.1421, 'grad_norm': 0.8151661157608032, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.931215763092041, 'eval_runtime': 9.8526, 'eval_samples_per_second': 101.496, 'eval_steps_per_second': 6.394, 'epoch': 0.64}
{'loss': 1.1107, 'grad_norm': 0.855757474899292, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9425393342971802, 'eval_runtime': 9.8418, 'eval_samples_per_second': 101.607, 'eval_steps_per_second': 6.401, 'epoch': 0.68}
{'loss': 1.0308, 'grad_norm': 0.9176294207572937, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9622831344604492, 'eval_runtime': 9.8258, 'eval_samples_per_second': 101.773, 'eval_steps_per_second': 6.412, 'epoch': 0.72}
{'loss': 1.0962, 'grad_norm': 1.0705273151397705, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9629957675933838, 'eval_runtime': 9.817, 'eval_samples_per_second': 101.864, 'eval_steps_per_second': 6.417, 'epoch': 0.76}
{'loss': 1.0361, 'grad_norm': 1.1352952718734741, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9536572694778442, 'eval_runtime': 9.8122, 'eval_samples_per_second': 101.914, 'eval_steps_per_second': 6.421, 'epoch': 0.8}
{'loss': 1.0562, 'grad_norm': 1.0188487768173218, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9556525945663452, 'eval_runtime': 9.8246, 'eval_samples_per_second': 101.785, 'eval_steps_per_second': 6.412, 'epoch': 0.84}
{'loss': 0.9715, 'grad_norm': 1.0296497344970703, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9481741189956665, 'eval_runtime': 9.8028, 'eval_samples_per_second': 102.012, 'eval_steps_per_second': 6.427, 'epoch': 0.88}
{'loss': 1.0564, 'grad_norm': 0.8141758441925049, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9494696855545044, 'eval_runtime': 9.8166, 'eval_samples_per_second': 101.869, 'eval_steps_per_second': 6.418, 'epoch': 0.92}
{'loss': 1.0468, 'grad_norm': 0.8231950402259827, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9488532543182373, 'eval_runtime': 9.8173, 'eval_samples_per_second': 101.861, 'eval_steps_per_second': 6.417, 'epoch': 0.96}
{'loss': 1.0925, 'grad_norm': 0.8067804574966431, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9505414962768555, 'eval_runtime': 9.8093, 'eval_samples_per_second': 101.944, 'eval_steps_per_second': 6.422, 'epoch': 1.0}
{'train_runtime': 417.7294, 'train_samples_per_second': 23.932, 'train_steps_per_second': 1.496, 'train_loss': 1.2073850402832031, 'epoch': 1.0}
train_results:  {'eval_loss': [2.061530590057373, 2.086082935333252, 2.045147180557251, 1.9939428567886353, 1.983831763267517, 2.0026025772094727, 1.906082272529602, 1.91640043258667, 1.9570915699005127, 1.9266321659088135, 1.9399805068969727, 1.9379521608352661, 1.95363450050354, 1.9546754360198975, 1.9357411861419678, 1.931215763092041, 1.9425393342971802, 1.9622831344604492, 1.9629957675933838, 1.9536572694778442, 1.9556525945663452, 1.9481741189956665, 1.9494696855545044, 1.9488532543182373, 1.9505414962768555], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.061530590057373, 2.086082935333252, 2.045147180557251, 1.9939428567886353, 1.983831763267517, 2.0026025772094727, 1.906082272529602, 1.91640043258667, 1.9570915699005127, 1.9266321659088135, 1.9399805068969727, 1.9379521608352661, 1.95363450050354, 1.9546754360198975, 1.9357411861419678, 1.931215763092041, 1.9425393342971802, 1.9622831344604492, 1.9629957675933838, 1.9536572694778442, 1.9556525945663452, 1.9481741189956665, 1.9494696855545044, 1.9488532543182373, 1.9505414962768555]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.9505414962768555
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2405 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -0.9062496025794378
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -0.906249602579438
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -0.9062496025794373
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -0.9062496025794378
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -0.906249602579196
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, tensor(0.0440, dtype=torch.float64), tensor(0.9560, dtype=torch.float64), 13, 0, 0, 1, 0, 1, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.0252e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.6860e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0440, dtype=torch.float64), tensor(0.9560, dtype=torch.float64), tensor(0.4041, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.044
  arc_challenge: 0.956

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 61,341,696 || all params: 8,091,602,944 || trainable%: 0.7581
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9077, 'grad_norm': 0.14317962527275085, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.9355080127716064, 'eval_runtime': 9.0349, 'eval_samples_per_second': 110.682, 'eval_steps_per_second': 6.973, 'epoch': 0.04}
{'loss': 2.3403, 'grad_norm': 0.13342678546905518, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9772002696990967, 'eval_runtime': 9.0752, 'eval_samples_per_second': 110.191, 'eval_steps_per_second': 6.942, 'epoch': 0.08}
{'loss': 1.3408, 'grad_norm': 0.03521853685379028, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8352502584457397, 'eval_runtime': 9.1057, 'eval_samples_per_second': 109.821, 'eval_steps_per_second': 6.919, 'epoch': 0.12}
{'loss': 1.228, 'grad_norm': 0.06936449557542801, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7965588569641113, 'eval_runtime': 9.1601, 'eval_samples_per_second': 109.169, 'eval_steps_per_second': 6.878, 'epoch': 0.16}
{'loss': 1.2128, 'grad_norm': 0.0357455313205719, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7803696393966675, 'eval_runtime': 9.1886, 'eval_samples_per_second': 108.83, 'eval_steps_per_second': 6.856, 'epoch': 0.2}
{'loss': 1.1846, 'grad_norm': 0.04515540972352028, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.721379041671753, 'eval_runtime': 9.1675, 'eval_samples_per_second': 109.081, 'eval_steps_per_second': 6.872, 'epoch': 0.24}
{'loss': 1.1279, 'grad_norm': 0.04397339001297951, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7156044244766235, 'eval_runtime': 9.1629, 'eval_samples_per_second': 109.136, 'eval_steps_per_second': 6.876, 'epoch': 0.28}
{'loss': 1.0721, 'grad_norm': 0.04513494670391083, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.693249225616455, 'eval_runtime': 9.1907, 'eval_samples_per_second': 108.806, 'eval_steps_per_second': 6.855, 'epoch': 0.32}
{'loss': 1.0509, 'grad_norm': 0.046601638197898865, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.653228998184204, 'eval_runtime': 9.1432, 'eval_samples_per_second': 109.37, 'eval_steps_per_second': 6.89, 'epoch': 0.36}
{'loss': 1.0395, 'grad_norm': 0.0336759015917778, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6420527696609497, 'eval_runtime': 9.1454, 'eval_samples_per_second': 109.344, 'eval_steps_per_second': 6.889, 'epoch': 0.4}
{'loss': 1.0332, 'grad_norm': 0.051907990127801895, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6325876712799072, 'eval_runtime': 9.1224, 'eval_samples_per_second': 109.62, 'eval_steps_per_second': 6.906, 'epoch': 0.44}
{'loss': 1.0094, 'grad_norm': 0.04100453108549118, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6139150857925415, 'eval_runtime': 9.0947, 'eval_samples_per_second': 109.954, 'eval_steps_per_second': 6.927, 'epoch': 0.48}
{'loss': 1.0101, 'grad_norm': 0.04390710964798927, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.622427225112915, 'eval_runtime': 9.1267, 'eval_samples_per_second': 109.569, 'eval_steps_per_second': 6.903, 'epoch': 0.52}
{'loss': 1.0268, 'grad_norm': 0.046090804040431976, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.59597909450531, 'eval_runtime': 9.1405, 'eval_samples_per_second': 109.403, 'eval_steps_per_second': 6.892, 'epoch': 0.56}
{'loss': 0.9912, 'grad_norm': 0.04602481424808502, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5971481800079346, 'eval_runtime': 9.127, 'eval_samples_per_second': 109.565, 'eval_steps_per_second': 6.903, 'epoch': 0.6}
{'loss': 0.9895, 'grad_norm': 0.04908805713057518, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5888690948486328, 'eval_runtime': 9.2098, 'eval_samples_per_second': 108.58, 'eval_steps_per_second': 6.841, 'epoch': 0.64}
{'loss': 0.9795, 'grad_norm': 0.04985491558909416, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.575953722000122, 'eval_runtime': 9.1936, 'eval_samples_per_second': 108.771, 'eval_steps_per_second': 6.853, 'epoch': 0.68}
{'loss': 0.9447, 'grad_norm': 0.05719665810465813, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5754977464675903, 'eval_runtime': 9.1743, 'eval_samples_per_second': 109.0, 'eval_steps_per_second': 6.867, 'epoch': 0.72}
{'loss': 0.9622, 'grad_norm': 0.05683817341923714, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5534217357635498, 'eval_runtime': 9.1684, 'eval_samples_per_second': 109.07, 'eval_steps_per_second': 6.871, 'epoch': 0.76}
{'loss': 0.9265, 'grad_norm': 0.054247524589300156, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5388576984405518, 'eval_runtime': 9.2049, 'eval_samples_per_second': 108.638, 'eval_steps_per_second': 6.844, 'epoch': 0.8}
{'loss': 0.9363, 'grad_norm': 0.06319255381822586, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.52911376953125, 'eval_runtime': 9.1726, 'eval_samples_per_second': 109.021, 'eval_steps_per_second': 6.868, 'epoch': 0.84}
{'loss': 0.8943, 'grad_norm': 0.055446136742830276, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5186702013015747, 'eval_runtime': 9.1719, 'eval_samples_per_second': 109.028, 'eval_steps_per_second': 6.869, 'epoch': 0.88}
{'loss': 0.9163, 'grad_norm': 0.06057462841272354, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.515724539756775, 'eval_runtime': 9.1872, 'eval_samples_per_second': 108.847, 'eval_steps_per_second': 6.857, 'epoch': 0.92}
{'loss': 0.896, 'grad_norm': 0.05808655545115471, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5185688734054565, 'eval_runtime': 9.1745, 'eval_samples_per_second': 108.998, 'eval_steps_per_second': 6.867, 'epoch': 0.96}
{'loss': 0.9041, 'grad_norm': 0.06591165065765381, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.517771601676941, 'eval_runtime': 9.1713, 'eval_samples_per_second': 109.036, 'eval_steps_per_second': 6.869, 'epoch': 1.0}
{'train_runtime': 382.5173, 'train_samples_per_second': 26.14, 'train_steps_per_second': 1.634, 'train_loss': 1.1969894439697266, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9355080127716064, 1.9772002696990967, 1.8352502584457397, 1.7965588569641113, 1.7803696393966675, 1.721379041671753, 1.7156044244766235, 1.693249225616455, 1.653228998184204, 1.6420527696609497, 1.6325876712799072, 1.6139150857925415, 1.622427225112915, 1.59597909450531, 1.5971481800079346, 1.5888690948486328, 1.575953722000122, 1.5754977464675903, 1.5534217357635498, 1.5388576984405518, 1.52911376953125, 1.5186702013015747, 1.515724539756775, 1.5185688734054565, 1.517771601676941], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.9355080127716064, 1.9772002696990967, 1.8352502584457397, 1.7965588569641113, 1.7803696393966675, 1.721379041671753, 1.7156044244766235, 1.693249225616455, 1.653228998184204, 1.6420527696609497, 1.6325876712799072, 1.6139150857925415, 1.622427225112915, 1.59597909450531, 1.5971481800079346, 1.5888690948486328, 1.575953722000122, 1.5754977464675903, 1.5534217357635498, 1.5388576984405518, 1.52911376953125, 1.5186702013015747, 1.515724539756775, 1.5185688734054565, 1.517771601676941]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0929954051971436
current iteration best possible eval_loss (full train run):  -1.517771601676941
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769, -1.0282820463180542, -1.0929954051971436]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.4021 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -1.0139525383534407
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -1.0139368722136919
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -1.0139550871264458
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -1.0135107131923056
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -1.0139323351394107
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1164, dtype=torch.float64), tensor(0.0140, dtype=torch.float64), tensor(0.1006, dtype=torch.float64), tensor(0.3007, dtype=torch.float64), 0, 0, tensor(0.4682, dtype=torch.float64), 0, 32, 1, 1, 1, 0, 0, 128, 0.08080808031454562, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(2.5511e-17, dtype=torch.float64), tensor(0.1164, dtype=torch.float64), tensor(0.0140, dtype=torch.float64), tensor(0.1006, dtype=torch.float64), tensor(0.3007, dtype=torch.float64), tensor(8.2170e-20, dtype=torch.float64), tensor(2.3850e-17, dtype=torch.float64), tensor(0.4682, dtype=torch.float64), tensor(2.6040e-19, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8081, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.116
  rowan_hellaswag: 0.014
  sciq: 0.101
  triviaqa: 0.301
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.468
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.08080808031454562,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.08080808031454562
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6015, 'grad_norm': 0.748537540435791, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6055190563201904, 'eval_runtime': 9.659, 'eval_samples_per_second': 103.531, 'eval_steps_per_second': 6.522, 'epoch': 0.04}
{'loss': 1.4741, 'grad_norm': 0.372670978307724, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4366681575775146, 'eval_runtime': 9.6525, 'eval_samples_per_second': 103.6, 'eval_steps_per_second': 6.527, 'epoch': 0.08}
{'loss': 1.2773, 'grad_norm': 0.31474366784095764, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3559054136276245, 'eval_runtime': 9.7072, 'eval_samples_per_second': 103.016, 'eval_steps_per_second': 6.49, 'epoch': 0.12}
{'loss': 1.229, 'grad_norm': 0.2654440999031067, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2950226068496704, 'eval_runtime': 9.7513, 'eval_samples_per_second': 102.55, 'eval_steps_per_second': 6.461, 'epoch': 0.16}
{'loss': 1.1626, 'grad_norm': 0.2681942284107208, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.268341064453125, 'eval_runtime': 9.7658, 'eval_samples_per_second': 102.398, 'eval_steps_per_second': 6.451, 'epoch': 0.2}
{'loss': 1.1995, 'grad_norm': 0.24377307295799255, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2541199922561646, 'eval_runtime': 9.8099, 'eval_samples_per_second': 101.938, 'eval_steps_per_second': 6.422, 'epoch': 0.24}
{'loss': 1.1318, 'grad_norm': 0.2992299795150757, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2471609115600586, 'eval_runtime': 9.7978, 'eval_samples_per_second': 102.064, 'eval_steps_per_second': 6.43, 'epoch': 0.28}
{'loss': 1.1074, 'grad_norm': 0.24403345584869385, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2404354810714722, 'eval_runtime': 9.7599, 'eval_samples_per_second': 102.46, 'eval_steps_per_second': 6.455, 'epoch': 0.32}
{'loss': 1.0842, 'grad_norm': 0.29184380173683167, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2408263683319092, 'eval_runtime': 9.7515, 'eval_samples_per_second': 102.548, 'eval_steps_per_second': 6.461, 'epoch': 0.36}
{'loss': 1.1318, 'grad_norm': 0.22698767483234406, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2340784072875977, 'eval_runtime': 9.7335, 'eval_samples_per_second': 102.738, 'eval_steps_per_second': 6.473, 'epoch': 0.4}
{'loss': 1.1326, 'grad_norm': 0.21414987742900848, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2327998876571655, 'eval_runtime': 9.7661, 'eval_samples_per_second': 102.395, 'eval_steps_per_second': 6.451, 'epoch': 0.44}
{'loss': 1.1101, 'grad_norm': 0.22551020979881287, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2305047512054443, 'eval_runtime': 9.7379, 'eval_samples_per_second': 102.691, 'eval_steps_per_second': 6.47, 'epoch': 0.48}
{'loss': 1.121, 'grad_norm': 0.2700178623199463, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2267011404037476, 'eval_runtime': 9.697, 'eval_samples_per_second': 103.124, 'eval_steps_per_second': 6.497, 'epoch': 0.52}
{'loss': 1.109, 'grad_norm': 0.23610453307628632, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2209768295288086, 'eval_runtime': 9.7149, 'eval_samples_per_second': 102.935, 'eval_steps_per_second': 6.485, 'epoch': 0.56}
{'loss': 1.061, 'grad_norm': 0.22492273151874542, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2197957038879395, 'eval_runtime': 9.7147, 'eval_samples_per_second': 102.936, 'eval_steps_per_second': 6.485, 'epoch': 0.6}
{'loss': 1.0627, 'grad_norm': 0.20969869196414948, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2187896966934204, 'eval_runtime': 9.7042, 'eval_samples_per_second': 103.049, 'eval_steps_per_second': 6.492, 'epoch': 0.64}
{'loss': 1.1345, 'grad_norm': 0.235558420419693, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2137775421142578, 'eval_runtime': 9.7103, 'eval_samples_per_second': 102.984, 'eval_steps_per_second': 6.488, 'epoch': 0.68}
{'loss': 1.0865, 'grad_norm': 0.26970720291137695, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2128052711486816, 'eval_runtime': 9.7429, 'eval_samples_per_second': 102.639, 'eval_steps_per_second': 6.466, 'epoch': 0.72}
{'loss': 1.0823, 'grad_norm': 0.20954807102680206, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2110257148742676, 'eval_runtime': 9.7386, 'eval_samples_per_second': 102.684, 'eval_steps_per_second': 6.469, 'epoch': 0.76}
{'loss': 1.082, 'grad_norm': 0.2360939234495163, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.20682954788208, 'eval_runtime': 9.7325, 'eval_samples_per_second': 102.748, 'eval_steps_per_second': 6.473, 'epoch': 0.8}
{'loss': 1.0225, 'grad_norm': 0.32678377628326416, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2074459791183472, 'eval_runtime': 9.7115, 'eval_samples_per_second': 102.971, 'eval_steps_per_second': 6.487, 'epoch': 0.84}
{'loss': 1.0844, 'grad_norm': 0.38409870862960815, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2041791677474976, 'eval_runtime': 9.7091, 'eval_samples_per_second': 102.996, 'eval_steps_per_second': 6.489, 'epoch': 0.88}
{'loss': 1.0683, 'grad_norm': 0.23931315541267395, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2039562463760376, 'eval_runtime': 9.7045, 'eval_samples_per_second': 103.045, 'eval_steps_per_second': 6.492, 'epoch': 0.92}
{'loss': 1.062, 'grad_norm': 0.22575432062149048, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.202932357788086, 'eval_runtime': 9.6885, 'eval_samples_per_second': 103.215, 'eval_steps_per_second': 6.503, 'epoch': 0.96}
{'loss': 1.0592, 'grad_norm': 0.2510281503200531, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2030500173568726, 'eval_runtime': 9.696, 'eval_samples_per_second': 103.135, 'eval_steps_per_second': 6.498, 'epoch': 1.0}
{'train_runtime': 460.0309, 'train_samples_per_second': 21.733, 'train_steps_per_second': 1.359, 'train_loss': 1.1870958129882812, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6055190563201904, 1.4366681575775146, 1.3559054136276245, 1.2950226068496704, 1.268341064453125, 1.2541199922561646, 1.2471609115600586, 1.2404354810714722, 1.2408263683319092, 1.2340784072875977, 1.2327998876571655, 1.2305047512054443, 1.2267011404037476, 1.2209768295288086, 1.2197957038879395, 1.2187896966934204, 1.2137775421142578, 1.2128052711486816, 1.2110257148742676, 1.20682954788208, 1.2074459791183472, 1.2041791677474976, 1.2039562463760376, 1.202932357788086, 1.2030500173568726], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6055190563201904, 1.4366681575775146, 1.3559054136276245, 1.2950226068496704, 1.268341064453125, 1.2541199922561646, 1.2471609115600586, 1.2404354810714722, 1.2408263683319092, 1.2340784072875977, 1.2327998876571655, 1.2305047512054443, 1.2267011404037476, 1.2209768295288086, 1.2197957038879395, 1.2187896966934204, 1.2137775421142578, 1.2128052711486816, 1.2110257148742676, 1.20682954788208, 1.2074459791183472, 1.2041791677474976, 1.2039562463760376, 1.202932357788086, 1.2030500173568726]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2030500173568726
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769, -1.0282820463180542, -1.0929954051971436, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8672 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -1.003534791536613
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -0.9874562113188308
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -0.9868480125378523
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -0.9868505990166648
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -0.9873914763188434
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1599, dtype=torch.float64), tensor(0.1296, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7105, dtype=torch.float64), 10, 0, 1, 1, 0, 1, 128, 6.938893903907231e-19, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(1.0932e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1599, dtype=torch.float64), tensor(0.1296, dtype=torch.float64), tensor(1.0106e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3296e-17, dtype=torch.float64), tensor(0.7105, dtype=torch.float64), tensor(0.3216, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(6.9389e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.16
  sciq: 0.13
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.711

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (6.938893903907231e-19,)
  num_layers_to_apply: (10,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  10
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  128
lora dropout:  6.938893903907231e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,739,520 || all params: 8,084,000,768 || trainable%: 0.6648
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2606, 'grad_norm': 0.6050525903701782, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1148130893707275, 'eval_runtime': 9.0096, 'eval_samples_per_second': 110.992, 'eval_steps_per_second': 6.993, 'epoch': 0.04}
{'loss': 1.5739, 'grad_norm': 0.23000410199165344, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.962153434753418, 'eval_runtime': 9.0442, 'eval_samples_per_second': 110.568, 'eval_steps_per_second': 6.966, 'epoch': 0.08}
{'loss': 1.3972, 'grad_norm': 0.18310189247131348, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9661482572555542, 'eval_runtime': 9.0583, 'eval_samples_per_second': 110.396, 'eval_steps_per_second': 6.955, 'epoch': 0.12}
{'loss': 1.2187, 'grad_norm': 0.2018260806798935, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8957363367080688, 'eval_runtime': 9.0741, 'eval_samples_per_second': 110.204, 'eval_steps_per_second': 6.943, 'epoch': 0.16}
{'loss': 1.1565, 'grad_norm': 0.2140156328678131, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8931703567504883, 'eval_runtime': 9.0812, 'eval_samples_per_second': 110.118, 'eval_steps_per_second': 6.937, 'epoch': 0.2}
{'loss': 1.0948, 'grad_norm': 0.2297610342502594, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9024113416671753, 'eval_runtime': 9.144, 'eval_samples_per_second': 109.362, 'eval_steps_per_second': 6.89, 'epoch': 0.24}
{'loss': 1.1478, 'grad_norm': 0.2477595955133438, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8657623529434204, 'eval_runtime': 9.1627, 'eval_samples_per_second': 109.138, 'eval_steps_per_second': 6.876, 'epoch': 0.28}
{'loss': 1.0771, 'grad_norm': 0.273282915353775, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.93104887008667, 'eval_runtime': 9.165, 'eval_samples_per_second': 109.111, 'eval_steps_per_second': 6.874, 'epoch': 0.32}
{'loss': 1.0663, 'grad_norm': 0.30571597814559937, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9429962635040283, 'eval_runtime': 9.1842, 'eval_samples_per_second': 108.883, 'eval_steps_per_second': 6.86, 'epoch': 0.36}
{'loss': 1.0513, 'grad_norm': 0.25011947751045227, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9500499963760376, 'eval_runtime': 9.1737, 'eval_samples_per_second': 109.007, 'eval_steps_per_second': 6.867, 'epoch': 0.4}
{'loss': 0.9925, 'grad_norm': 0.27716362476348877, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9450066089630127, 'eval_runtime': 9.1704, 'eval_samples_per_second': 109.046, 'eval_steps_per_second': 6.87, 'epoch': 0.44}
{'loss': 1.0173, 'grad_norm': 0.35936060547828674, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.031468391418457, 'eval_runtime': 9.1965, 'eval_samples_per_second': 108.737, 'eval_steps_per_second': 6.85, 'epoch': 0.48}
{'loss': 0.9713, 'grad_norm': 0.32683849334716797, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9758328199386597, 'eval_runtime': 9.1712, 'eval_samples_per_second': 109.037, 'eval_steps_per_second': 6.869, 'epoch': 0.52}
{'loss': 0.8612, 'grad_norm': 0.3184051811695099, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9927270412445068, 'eval_runtime': 9.1715, 'eval_samples_per_second': 109.034, 'eval_steps_per_second': 6.869, 'epoch': 0.56}
{'loss': 0.9545, 'grad_norm': 0.2882821261882782, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.0206515789031982, 'eval_runtime': 9.166, 'eval_samples_per_second': 109.099, 'eval_steps_per_second': 6.873, 'epoch': 0.6}
{'loss': 0.8976, 'grad_norm': 0.3224862515926361, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.031918525695801, 'eval_runtime': 9.1687, 'eval_samples_per_second': 109.067, 'eval_steps_per_second': 6.871, 'epoch': 0.64}
{'loss': 0.9139, 'grad_norm': 0.31671783328056335, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.0425398349761963, 'eval_runtime': 9.1608, 'eval_samples_per_second': 109.161, 'eval_steps_per_second': 6.877, 'epoch': 0.68}
{'loss': 0.8375, 'grad_norm': 0.4578997492790222, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.076939582824707, 'eval_runtime': 9.1567, 'eval_samples_per_second': 109.209, 'eval_steps_per_second': 6.88, 'epoch': 0.72}
{'loss': 0.8279, 'grad_norm': 0.31067410111427307, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.0837149620056152, 'eval_runtime': 9.1206, 'eval_samples_per_second': 109.642, 'eval_steps_per_second': 6.907, 'epoch': 0.76}
{'loss': 0.8731, 'grad_norm': 0.3562275171279907, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.0735151767730713, 'eval_runtime': 9.1047, 'eval_samples_per_second': 109.833, 'eval_steps_per_second': 6.919, 'epoch': 0.8}
{'loss': 0.7721, 'grad_norm': 0.3542591631412506, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0794031620025635, 'eval_runtime': 9.1152, 'eval_samples_per_second': 109.707, 'eval_steps_per_second': 6.912, 'epoch': 0.84}
{'loss': 0.7424, 'grad_norm': 0.4015447199344635, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.1133506298065186, 'eval_runtime': 9.1273, 'eval_samples_per_second': 109.561, 'eval_steps_per_second': 6.902, 'epoch': 0.88}
{'loss': 0.7665, 'grad_norm': 0.3695240020751953, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1234521865844727, 'eval_runtime': 9.1179, 'eval_samples_per_second': 109.675, 'eval_steps_per_second': 6.91, 'epoch': 0.92}
{'loss': 0.7603, 'grad_norm': 0.3772733807563782, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1382060050964355, 'eval_runtime': 9.1175, 'eval_samples_per_second': 109.679, 'eval_steps_per_second': 6.91, 'epoch': 0.96}
{'loss': 0.7293, 'grad_norm': 0.32904115319252014, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.132551908493042, 'eval_runtime': 9.1434, 'eval_samples_per_second': 109.368, 'eval_steps_per_second': 6.89, 'epoch': 1.0}
{'train_runtime': 358.8638, 'train_samples_per_second': 27.86, 'train_steps_per_second': 1.742, 'train_loss': 1.0784681762695312, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1148130893707275, 1.962153434753418, 1.9661482572555542, 1.8957363367080688, 1.8931703567504883, 1.9024113416671753, 1.8657623529434204, 1.93104887008667, 1.9429962635040283, 1.9500499963760376, 1.9450066089630127, 2.031468391418457, 1.9758328199386597, 1.9927270412445068, 2.0206515789031982, 2.031918525695801, 2.0425398349761963, 2.076939582824707, 2.0837149620056152, 2.0735151767730713, 2.0794031620025635, 2.1133506298065186, 2.1234521865844727, 2.1382060050964355, 2.132551908493042], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1148130893707275, 1.962153434753418, 1.9661482572555542, 1.8957363367080688, 1.8931703567504883, 1.9024113416671753, 1.8657623529434204, 1.93104887008667, 1.9429962635040283, 1.9500499963760376, 1.9450066089630127, 2.031468391418457, 1.9758328199386597, 1.9927270412445068, 2.0206515789031982, 2.031918525695801, 2.0425398349761963, 2.076939582824707, 2.0837149620056152, 2.0735151767730713, 2.0794031620025635, 2.1133506298065186, 2.1234521865844727, 2.1382060050964355, 2.132551908493042]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.132551908493042
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769, -1.0282820463180542, -1.0929954051971436, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.0644 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -1.0349169231386295
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -1.035246659223548
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -1.0420331453798182
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -1.0349167349452242
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -1.0349170046983032
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1031, dtype=torch.float64), tensor(0.0792, dtype=torch.float64), tensor(0.1711, dtype=torch.float64), 0, tensor(0.4591, dtype=torch.float64), 0, tensor(0.1876, dtype=torch.float64), 0, 23, 0, 0, 1, 1, 1, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(4.9308e-18, dtype=torch.float64), tensor(0.1031, dtype=torch.float64), tensor(0.0792, dtype=torch.float64), tensor(0.1711, dtype=torch.float64), tensor(1.5777e-19, dtype=torch.float64), tensor(0.4591, dtype=torch.float64), tensor(1.5406e-19, dtype=torch.float64), tensor(0.1876, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7102, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.103
  rowan_hellaswag: 0.079
  sciq: 0.171
  triviaqa: 0
  truthfulqa_gen: 0.459
  wikitext: 0
  mmlu: 0.188
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,543,616 || all params: 8,032,804,864 || trainable%: 0.0317
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8737, 'grad_norm': 7.625176906585693, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5748740434646606, 'eval_runtime': 9.9946, 'eval_samples_per_second': 100.054, 'eval_steps_per_second': 6.303, 'epoch': 0.04}
{'loss': 1.3084, 'grad_norm': 2.635596752166748, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3927080631256104, 'eval_runtime': 10.0187, 'eval_samples_per_second': 99.813, 'eval_steps_per_second': 6.288, 'epoch': 0.08}
{'loss': 1.1517, 'grad_norm': 1.7972702980041504, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3372390270233154, 'eval_runtime': 10.0541, 'eval_samples_per_second': 99.462, 'eval_steps_per_second': 6.266, 'epoch': 0.12}
{'loss': 1.0778, 'grad_norm': 2.074371576309204, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3345823287963867, 'eval_runtime': 10.0719, 'eval_samples_per_second': 99.287, 'eval_steps_per_second': 6.255, 'epoch': 0.16}
{'loss': 1.1293, 'grad_norm': 1.8290385007858276, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3177131414413452, 'eval_runtime': 10.078, 'eval_samples_per_second': 99.226, 'eval_steps_per_second': 6.251, 'epoch': 0.2}
{'loss': 1.0717, 'grad_norm': 1.7165409326553345, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3147523403167725, 'eval_runtime': 10.0741, 'eval_samples_per_second': 99.264, 'eval_steps_per_second': 6.254, 'epoch': 0.24}
{'loss': 1.0321, 'grad_norm': 1.650650978088379, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3081728219985962, 'eval_runtime': 10.0824, 'eval_samples_per_second': 99.182, 'eval_steps_per_second': 6.248, 'epoch': 0.28}
{'loss': 1.0465, 'grad_norm': 1.5722534656524658, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.312187910079956, 'eval_runtime': 10.0894, 'eval_samples_per_second': 99.114, 'eval_steps_per_second': 6.244, 'epoch': 0.32}
{'loss': 0.9917, 'grad_norm': 1.7101179361343384, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3096939325332642, 'eval_runtime': 10.0797, 'eval_samples_per_second': 99.209, 'eval_steps_per_second': 6.25, 'epoch': 0.36}
{'loss': 1.0079, 'grad_norm': 1.685259222984314, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3068894147872925, 'eval_runtime': 10.0598, 'eval_samples_per_second': 99.405, 'eval_steps_per_second': 6.263, 'epoch': 0.4}
{'loss': 0.9801, 'grad_norm': 1.8300551176071167, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3063355684280396, 'eval_runtime': 10.0588, 'eval_samples_per_second': 99.415, 'eval_steps_per_second': 6.263, 'epoch': 0.44}
{'loss': 0.9702, 'grad_norm': 1.3577567338943481, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3065032958984375, 'eval_runtime': 10.0574, 'eval_samples_per_second': 99.43, 'eval_steps_per_second': 6.264, 'epoch': 0.48}
{'loss': 0.9896, 'grad_norm': 2.9581916332244873, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2996032238006592, 'eval_runtime': 10.0561, 'eval_samples_per_second': 99.442, 'eval_steps_per_second': 6.265, 'epoch': 0.52}
{'loss': 0.9637, 'grad_norm': 1.6772055625915527, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.293465256690979, 'eval_runtime': 10.0646, 'eval_samples_per_second': 99.359, 'eval_steps_per_second': 6.26, 'epoch': 0.56}
{'loss': 0.9815, 'grad_norm': 1.4673840999603271, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.295554280281067, 'eval_runtime': 10.0602, 'eval_samples_per_second': 99.402, 'eval_steps_per_second': 6.262, 'epoch': 0.6}
{'loss': 0.9201, 'grad_norm': 1.3016859292984009, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2942042350769043, 'eval_runtime': 10.0624, 'eval_samples_per_second': 99.38, 'eval_steps_per_second': 6.261, 'epoch': 0.64}
{'loss': 0.928, 'grad_norm': 1.8873964548110962, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.29237699508667, 'eval_runtime': 10.0547, 'eval_samples_per_second': 99.456, 'eval_steps_per_second': 6.266, 'epoch': 0.68}
{'loss': 0.883, 'grad_norm': 1.759594202041626, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2901664972305298, 'eval_runtime': 10.0563, 'eval_samples_per_second': 99.441, 'eval_steps_per_second': 6.265, 'epoch': 0.72}
{'loss': 0.9148, 'grad_norm': 2.050232172012329, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2898505926132202, 'eval_runtime': 10.0938, 'eval_samples_per_second': 99.071, 'eval_steps_per_second': 6.241, 'epoch': 0.76}
{'loss': 0.9372, 'grad_norm': 1.4485794305801392, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.286439061164856, 'eval_runtime': 10.1128, 'eval_samples_per_second': 98.885, 'eval_steps_per_second': 6.23, 'epoch': 0.8}
{'loss': 0.9377, 'grad_norm': 1.45448899269104, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2864811420440674, 'eval_runtime': 10.1182, 'eval_samples_per_second': 98.832, 'eval_steps_per_second': 6.226, 'epoch': 0.84}
{'loss': 0.947, 'grad_norm': 1.5177855491638184, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2841196060180664, 'eval_runtime': 10.12, 'eval_samples_per_second': 98.814, 'eval_steps_per_second': 6.225, 'epoch': 0.88}
{'loss': 0.8877, 'grad_norm': 1.4206006526947021, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.283664345741272, 'eval_runtime': 10.123, 'eval_samples_per_second': 98.785, 'eval_steps_per_second': 6.223, 'epoch': 0.92}
{'loss': 0.9214, 'grad_norm': 1.6524842977523804, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.282500982284546, 'eval_runtime': 10.1237, 'eval_samples_per_second': 98.778, 'eval_steps_per_second': 6.223, 'epoch': 0.96}
{'loss': 0.8612, 'grad_norm': 1.5763474702835083, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2824980020523071, 'eval_runtime': 10.1137, 'eval_samples_per_second': 98.876, 'eval_steps_per_second': 6.229, 'epoch': 1.0}
{'train_runtime': 440.0846, 'train_samples_per_second': 22.716, 'train_steps_per_second': 1.42, 'train_loss': 1.0685565307617189, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5748740434646606, 1.3927080631256104, 1.3372390270233154, 1.3345823287963867, 1.3177131414413452, 1.3147523403167725, 1.3081728219985962, 1.312187910079956, 1.3096939325332642, 1.3068894147872925, 1.3063355684280396, 1.3065032958984375, 1.2996032238006592, 1.293465256690979, 1.295554280281067, 1.2942042350769043, 1.29237699508667, 1.2901664972305298, 1.2898505926132202, 1.286439061164856, 1.2864811420440674, 1.2841196060180664, 1.283664345741272, 1.282500982284546, 1.2824980020523071], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5748740434646606, 1.3927080631256104, 1.3372390270233154, 1.3345823287963867, 1.3177131414413452, 1.3147523403167725, 1.3081728219985962, 1.312187910079956, 1.3096939325332642, 1.3068894147872925, 1.3063355684280396, 1.3065032958984375, 1.2996032238006592, 1.293465256690979, 1.295554280281067, 1.2942042350769043, 1.29237699508667, 1.2901664972305298, 1.2898505926132202, 1.286439061164856, 1.2864811420440674, 1.2841196060180664, 1.283664345741272, 1.282500982284546, 1.2824980020523071]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.2824980020523071
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769, -1.0282820463180542, -1.0929954051971436, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.3682 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -1.0432965685416695
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -1.0433414312387215
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -1.0433121231084928
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -1.0433006986327253
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -1.0382067502115677
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0424, dtype=torch.float64), tensor(0.0571, dtype=torch.float64), tensor(0.3121, dtype=torch.float64), tensor(0.5884, dtype=torch.float64), 0, 0, 0, 27, 1, 1, 1, 0, 0, 63, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.3155e-18, dtype=torch.float64), tensor(2.2775e-18, dtype=torch.float64), tensor(0.0424, dtype=torch.float64), tensor(0.0571, dtype=torch.float64), tensor(0.3121, dtype=torch.float64), tensor(0.5884, dtype=torch.float64), tensor(6.8173e-18, dtype=torch.float64), tensor(1.1992e-17, dtype=torch.float64), tensor(8.0144e-18, dtype=torch.float64), tensor(0.8435, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4941, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.042
  sciq: 0.057
  triviaqa: 0.312
  truthfulqa_gen: 0.588
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (63,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  63
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,996,544 || all params: 8,084,257,792 || trainable%: 0.6679
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3662, 'grad_norm': 1.0003429651260376, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1086275577545166, 'eval_runtime': 9.8272, 'eval_samples_per_second': 101.758, 'eval_steps_per_second': 6.411, 'epoch': 0.04}
{'loss': 1.393, 'grad_norm': 0.4945772886276245, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.0787391662597656, 'eval_runtime': 9.8619, 'eval_samples_per_second': 101.4, 'eval_steps_per_second': 6.388, 'epoch': 0.08}
{'loss': 1.1729, 'grad_norm': 0.5128788352012634, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9634127616882324, 'eval_runtime': 9.9154, 'eval_samples_per_second': 100.853, 'eval_steps_per_second': 6.354, 'epoch': 0.12}
{'loss': 1.0361, 'grad_norm': 0.4532555043697357, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0275015830993652, 'eval_runtime': 9.9703, 'eval_samples_per_second': 100.297, 'eval_steps_per_second': 6.319, 'epoch': 0.16}
{'loss': 1.0117, 'grad_norm': 0.5861672759056091, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9962953329086304, 'eval_runtime': 9.9346, 'eval_samples_per_second': 100.658, 'eval_steps_per_second': 6.341, 'epoch': 0.2}
{'loss': 0.9303, 'grad_norm': 0.5205962061882019, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.051736354827881, 'eval_runtime': 9.9022, 'eval_samples_per_second': 100.987, 'eval_steps_per_second': 6.362, 'epoch': 0.24}
{'loss': 0.8746, 'grad_norm': 0.5293453931808472, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.0367558002471924, 'eval_runtime': 9.9077, 'eval_samples_per_second': 100.932, 'eval_steps_per_second': 6.359, 'epoch': 0.28}
{'loss': 0.804, 'grad_norm': 0.5062317252159119, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9735888242721558, 'eval_runtime': 9.9248, 'eval_samples_per_second': 100.757, 'eval_steps_per_second': 6.348, 'epoch': 0.32}
{'loss': 0.7828, 'grad_norm': 0.5154126286506653, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.006693124771118, 'eval_runtime': 9.914, 'eval_samples_per_second': 100.868, 'eval_steps_per_second': 6.355, 'epoch': 0.36}
{'loss': 0.7161, 'grad_norm': 0.44234949350357056, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9716013669967651, 'eval_runtime': 9.8904, 'eval_samples_per_second': 101.108, 'eval_steps_per_second': 6.37, 'epoch': 0.4}
{'loss': 0.7608, 'grad_norm': 0.5240257382392883, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9556227922439575, 'eval_runtime': 9.886, 'eval_samples_per_second': 101.153, 'eval_steps_per_second': 6.373, 'epoch': 0.44}
{'loss': 0.7579, 'grad_norm': 0.3666550815105438, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9019230604171753, 'eval_runtime': 9.8291, 'eval_samples_per_second': 101.739, 'eval_steps_per_second': 6.41, 'epoch': 0.48}
{'loss': 0.7254, 'grad_norm': 0.57447350025177, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.919574499130249, 'eval_runtime': 9.829, 'eval_samples_per_second': 101.74, 'eval_steps_per_second': 6.41, 'epoch': 0.52}
{'loss': 0.6419, 'grad_norm': 0.5788194537162781, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9522970914840698, 'eval_runtime': 9.8231, 'eval_samples_per_second': 101.801, 'eval_steps_per_second': 6.413, 'epoch': 0.56}
{'loss': 0.7183, 'grad_norm': 0.39912012219429016, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.953052282333374, 'eval_runtime': 9.8342, 'eval_samples_per_second': 101.686, 'eval_steps_per_second': 6.406, 'epoch': 0.6}
{'loss': 0.594, 'grad_norm': 0.5138525366783142, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9465585947036743, 'eval_runtime': 9.8213, 'eval_samples_per_second': 101.819, 'eval_steps_per_second': 6.415, 'epoch': 0.64}
{'loss': 0.7405, 'grad_norm': 0.3913916051387787, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9385582208633423, 'eval_runtime': 9.8226, 'eval_samples_per_second': 101.806, 'eval_steps_per_second': 6.414, 'epoch': 0.68}
{'loss': 0.611, 'grad_norm': 0.38980090618133545, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9638723134994507, 'eval_runtime': 9.8421, 'eval_samples_per_second': 101.604, 'eval_steps_per_second': 6.401, 'epoch': 0.72}
{'loss': 0.6519, 'grad_norm': 0.4590440094470978, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.981287956237793, 'eval_runtime': 9.8846, 'eval_samples_per_second': 101.167, 'eval_steps_per_second': 6.374, 'epoch': 0.76}
{'loss': 0.7564, 'grad_norm': 0.40146347880363464, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9796713590621948, 'eval_runtime': 9.8982, 'eval_samples_per_second': 101.029, 'eval_steps_per_second': 6.365, 'epoch': 0.8}
{'loss': 0.5843, 'grad_norm': 0.45500099658966064, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9626878499984741, 'eval_runtime': 9.8978, 'eval_samples_per_second': 101.033, 'eval_steps_per_second': 6.365, 'epoch': 0.84}
{'loss': 0.59, 'grad_norm': 0.5440663695335388, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.972057819366455, 'eval_runtime': 9.8899, 'eval_samples_per_second': 101.113, 'eval_steps_per_second': 6.37, 'epoch': 0.88}
{'loss': 0.6803, 'grad_norm': 0.44801464676856995, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9827287197113037, 'eval_runtime': 9.9001, 'eval_samples_per_second': 101.009, 'eval_steps_per_second': 6.364, 'epoch': 0.92}
{'loss': 0.5497, 'grad_norm': 0.549889326095581, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9740750789642334, 'eval_runtime': 9.8935, 'eval_samples_per_second': 101.077, 'eval_steps_per_second': 6.368, 'epoch': 0.96}
{'loss': 0.6741, 'grad_norm': 0.37353068590164185, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9764209985733032, 'eval_runtime': 9.8885, 'eval_samples_per_second': 101.128, 'eval_steps_per_second': 6.371, 'epoch': 1.0}
{'train_runtime': 392.6531, 'train_samples_per_second': 25.463, 'train_steps_per_second': 1.592, 'train_loss': 0.8849673599243164, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1086275577545166, 2.0787391662597656, 1.9634127616882324, 2.0275015830993652, 1.9962953329086304, 2.051736354827881, 2.0367558002471924, 1.9735888242721558, 2.006693124771118, 1.9716013669967651, 1.9556227922439575, 1.9019230604171753, 1.919574499130249, 1.9522970914840698, 1.953052282333374, 1.9465585947036743, 1.9385582208633423, 1.9638723134994507, 1.981287956237793, 1.9796713590621948, 1.9626878499984741, 1.972057819366455, 1.9827287197113037, 1.9740750789642334, 1.9764209985733032], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1086275577545166, 2.0787391662597656, 1.9634127616882324, 2.0275015830993652, 1.9962953329086304, 2.051736354827881, 2.0367558002471924, 1.9735888242721558, 2.006693124771118, 1.9716013669967651, 1.9556227922439575, 1.9019230604171753, 1.919574499130249, 1.9522970914840698, 1.953052282333374, 1.9465585947036743, 1.9385582208633423, 1.9638723134994507, 1.981287956237793, 1.9796713590621948, 1.9626878499984741, 1.972057819366455, 1.9827287197113037, 1.9740750789642334, 1.9764209985733032]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.9764209985733032
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769, -1.0282820463180542, -1.0929954051971436, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.7721 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -1.0443217337993693
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -1.0470106438673383
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -1.046693531183918
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -1.0470106437333127
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -1.0473215779857616
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4989, dtype=torch.float64), tensor(0.0360, dtype=torch.float64), tensor(0.1413, dtype=torch.float64), tensor(0.2757, dtype=torch.float64), tensor(0.0481, dtype=torch.float64), 0, 0, 0, 14, 0, 0, 1, 1, 1, 123, 0.0, 22.44647940040776, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.4989, dtype=torch.float64), tensor(0.0360, dtype=torch.float64), tensor(0.1413, dtype=torch.float64), tensor(0.2757, dtype=torch.float64), tensor(0.0481, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1037e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4230, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9631, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4676, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.499
  rowan_hellaswag: 0.036
  sciq: 0.141
  triviaqa: 0.276
  truthfulqa_gen: 0.048
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (123,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (22.44647940040776,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  123
lora dropout:  0.0
lora alpha:  22.44647940040776
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 95,219,712 || all params: 8,125,480,960 || trainable%: 1.1719
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6575, 'grad_norm': 0.5240201354026794, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1640546321868896, 'eval_runtime': 9.8552, 'eval_samples_per_second': 101.47, 'eval_steps_per_second': 6.393, 'epoch': 0.04}
{'loss': 1.2949, 'grad_norm': 0.19405488669872284, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9509055614471436, 'eval_runtime': 9.8861, 'eval_samples_per_second': 101.153, 'eval_steps_per_second': 6.373, 'epoch': 0.08}
{'loss': 1.0455, 'grad_norm': 0.12274973839521408, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9931195974349976, 'eval_runtime': 9.922, 'eval_samples_per_second': 100.787, 'eval_steps_per_second': 6.35, 'epoch': 0.12}
{'loss': 1.0117, 'grad_norm': 0.12073905766010284, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9638806581497192, 'eval_runtime': 9.9545, 'eval_samples_per_second': 100.457, 'eval_steps_per_second': 6.329, 'epoch': 0.16}
{'loss': 1.0007, 'grad_norm': 0.11081762611865997, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0170211791992188, 'eval_runtime': 9.977, 'eval_samples_per_second': 100.231, 'eval_steps_per_second': 6.315, 'epoch': 0.2}
{'loss': 1.0143, 'grad_norm': 0.09777548909187317, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.0264337062835693, 'eval_runtime': 9.9932, 'eval_samples_per_second': 100.068, 'eval_steps_per_second': 6.304, 'epoch': 0.24}
{'loss': 0.9896, 'grad_norm': 0.13477544486522675, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9735145568847656, 'eval_runtime': 9.9888, 'eval_samples_per_second': 100.112, 'eval_steps_per_second': 6.307, 'epoch': 0.28}
{'loss': 0.9733, 'grad_norm': 0.11508161574602127, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9929296970367432, 'eval_runtime': 9.9768, 'eval_samples_per_second': 100.233, 'eval_steps_per_second': 6.315, 'epoch': 0.32}
{'loss': 0.9813, 'grad_norm': 0.1222088411450386, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.030999183654785, 'eval_runtime': 9.9528, 'eval_samples_per_second': 100.474, 'eval_steps_per_second': 6.33, 'epoch': 0.36}
{'loss': 0.9261, 'grad_norm': 0.11404912918806076, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.051506757736206, 'eval_runtime': 9.9307, 'eval_samples_per_second': 100.698, 'eval_steps_per_second': 6.344, 'epoch': 0.4}
{'loss': 0.9857, 'grad_norm': 0.1306418627500534, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0473570823669434, 'eval_runtime': 9.9291, 'eval_samples_per_second': 100.715, 'eval_steps_per_second': 6.345, 'epoch': 0.44}
{'loss': 1.0019, 'grad_norm': 0.12207813560962677, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.0162715911865234, 'eval_runtime': 9.9287, 'eval_samples_per_second': 100.718, 'eval_steps_per_second': 6.345, 'epoch': 0.48}
{'loss': 0.9547, 'grad_norm': 0.11069086939096451, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.009258270263672, 'eval_runtime': 9.9359, 'eval_samples_per_second': 100.645, 'eval_steps_per_second': 6.341, 'epoch': 0.52}
{'loss': 0.9702, 'grad_norm': 0.12099111825227737, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.0132181644439697, 'eval_runtime': 9.9332, 'eval_samples_per_second': 100.673, 'eval_steps_per_second': 6.342, 'epoch': 0.56}
{'loss': 0.9589, 'grad_norm': 0.14576707780361176, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.05855655670166, 'eval_runtime': 9.9394, 'eval_samples_per_second': 100.609, 'eval_steps_per_second': 6.338, 'epoch': 0.6}
{'loss': 0.9227, 'grad_norm': 0.12540367245674133, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.0795412063598633, 'eval_runtime': 9.9584, 'eval_samples_per_second': 100.417, 'eval_steps_per_second': 6.326, 'epoch': 0.64}
{'loss': 0.9368, 'grad_norm': 0.1284729540348053, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.0724735260009766, 'eval_runtime': 10.0295, 'eval_samples_per_second': 99.706, 'eval_steps_per_second': 6.281, 'epoch': 0.68}
{'loss': 0.9754, 'grad_norm': 0.11738836020231247, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.068512439727783, 'eval_runtime': 10.02, 'eval_samples_per_second': 99.801, 'eval_steps_per_second': 6.287, 'epoch': 0.72}
{'loss': 0.9274, 'grad_norm': 0.12183667719364166, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.080728530883789, 'eval_runtime': 10.0285, 'eval_samples_per_second': 99.716, 'eval_steps_per_second': 6.282, 'epoch': 0.76}
{'loss': 0.9226, 'grad_norm': 0.11638036370277405, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.0800225734710693, 'eval_runtime': 10.0169, 'eval_samples_per_second': 99.831, 'eval_steps_per_second': 6.289, 'epoch': 0.8}
{'loss': 0.9064, 'grad_norm': 0.11196991801261902, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0789756774902344, 'eval_runtime': 10.0203, 'eval_samples_per_second': 99.798, 'eval_steps_per_second': 6.287, 'epoch': 0.84}
{'loss': 0.9715, 'grad_norm': 0.14603203535079956, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0803909301757812, 'eval_runtime': 10.016, 'eval_samples_per_second': 99.84, 'eval_steps_per_second': 6.29, 'epoch': 0.88}
{'loss': 0.989, 'grad_norm': 0.14718618988990784, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.081381320953369, 'eval_runtime': 10.0134, 'eval_samples_per_second': 99.867, 'eval_steps_per_second': 6.292, 'epoch': 0.92}
{'loss': 0.9672, 'grad_norm': 0.11252588033676147, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0859882831573486, 'eval_runtime': 10.0071, 'eval_samples_per_second': 99.929, 'eval_steps_per_second': 6.296, 'epoch': 0.96}
{'loss': 0.9054, 'grad_norm': 0.13894735276699066, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.0889034271240234, 'eval_runtime': 10.0047, 'eval_samples_per_second': 99.953, 'eval_steps_per_second': 6.297, 'epoch': 1.0}
{'train_runtime': 414.9594, 'train_samples_per_second': 24.094, 'train_steps_per_second': 1.506, 'train_loss': 1.0476155670166016, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1640546321868896, 1.9509055614471436, 1.9931195974349976, 1.9638806581497192, 2.0170211791992188, 2.0264337062835693, 1.9735145568847656, 1.9929296970367432, 2.030999183654785, 2.051506757736206, 2.0473570823669434, 2.0162715911865234, 2.009258270263672, 2.0132181644439697, 2.05855655670166, 2.0795412063598633, 2.0724735260009766, 2.068512439727783, 2.080728530883789, 2.0800225734710693, 2.0789756774902344, 2.0803909301757812, 2.081381320953369, 2.0859882831573486, 2.0889034271240234], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1640546321868896, 1.9509055614471436, 1.9931195974349976, 1.9638806581497192, 2.0170211791992188, 2.0264337062835693, 1.9735145568847656, 1.9929296970367432, 2.030999183654785, 2.051506757736206, 2.0473570823669434, 2.0162715911865234, 2.009258270263672, 2.0132181644439697, 2.05855655670166, 2.0795412063598633, 2.0724735260009766, 2.068512439727783, 2.080728530883789, 2.0800225734710693, 2.0789756774902344, 2.0803909301757812, 2.081381320953369, 2.0859882831573486, 2.0889034271240234]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.0889034271240234
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769, -1.0282820463180542, -1.0929954051971436, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2635 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -0.9939636354235418
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -0.9939013177960516
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -0.9939306274005102
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -0.9939008564391232
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -0.9939807043816649
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0786, dtype=torch.float64), 0, tensor(0.0620, dtype=torch.float64), tensor(0.0683, dtype=torch.float64), tensor(0.1198, dtype=torch.float64), tensor(0.1723, dtype=torch.float64), tensor(0.0330, dtype=torch.float64), tensor(0.0897, dtype=torch.float64), tensor(0.3762, dtype=torch.float64), 18, 0, 0, 0, 0, 1, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0786, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0620, dtype=torch.float64), tensor(0.0683, dtype=torch.float64), tensor(0.1198, dtype=torch.float64), tensor(0.1723, dtype=torch.float64), tensor(0.0330, dtype=torch.float64), tensor(0.0897, dtype=torch.float64), tensor(0.3762, dtype=torch.float64), tensor(0.5557, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.079
  gsm8k: 0
  rowan_hellaswag: 0.062
  sciq: 0.068
  triviaqa: 0.12
  truthfulqa_gen: 0.172
  wikitext: 0.033
  mmlu: 0.09
  arc_challenge: 0.376

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 42,467,328 || all params: 8,072,728,576 || trainable%: 0.5261
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5888, 'grad_norm': 0.32208046317100525, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0779812335968018, 'eval_runtime': 8.9389, 'eval_samples_per_second': 111.87, 'eval_steps_per_second': 7.048, 'epoch': 0.04}
{'loss': 1.688, 'grad_norm': 0.32835787534713745, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6636486053466797, 'eval_runtime': 8.966, 'eval_samples_per_second': 111.533, 'eval_steps_per_second': 7.027, 'epoch': 0.08}
{'loss': 1.3833, 'grad_norm': 0.18895497918128967, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5938408374786377, 'eval_runtime': 9.014, 'eval_samples_per_second': 110.938, 'eval_steps_per_second': 6.989, 'epoch': 0.12}
{'loss': 1.2723, 'grad_norm': 0.1848699301481247, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4877221584320068, 'eval_runtime': 9.0393, 'eval_samples_per_second': 110.628, 'eval_steps_per_second': 6.97, 'epoch': 0.16}
{'loss': 1.181, 'grad_norm': 0.2072315812110901, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4214006662368774, 'eval_runtime': 9.0375, 'eval_samples_per_second': 110.651, 'eval_steps_per_second': 6.971, 'epoch': 0.2}
{'loss': 1.1217, 'grad_norm': 0.16551904380321503, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4232478141784668, 'eval_runtime': 9.0178, 'eval_samples_per_second': 110.891, 'eval_steps_per_second': 6.986, 'epoch': 0.24}
{'loss': 1.1103, 'grad_norm': 0.21950967609882355, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4201689958572388, 'eval_runtime': 9.0039, 'eval_samples_per_second': 111.063, 'eval_steps_per_second': 6.997, 'epoch': 0.28}
{'loss': 1.0803, 'grad_norm': 0.2210337370634079, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4222843647003174, 'eval_runtime': 9.0121, 'eval_samples_per_second': 110.962, 'eval_steps_per_second': 6.991, 'epoch': 0.32}
{'loss': 1.0864, 'grad_norm': 0.22591376304626465, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3893349170684814, 'eval_runtime': 9.0111, 'eval_samples_per_second': 110.974, 'eval_steps_per_second': 6.991, 'epoch': 0.36}
{'loss': 1.0864, 'grad_norm': 0.19266416132450104, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3772391080856323, 'eval_runtime': 9.0399, 'eval_samples_per_second': 110.621, 'eval_steps_per_second': 6.969, 'epoch': 0.4}
{'loss': 1.062, 'grad_norm': 0.22540733218193054, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3692704439163208, 'eval_runtime': 9.0879, 'eval_samples_per_second': 110.037, 'eval_steps_per_second': 6.932, 'epoch': 0.44}
{'loss': 1.0185, 'grad_norm': 0.2343546599149704, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3687753677368164, 'eval_runtime': 9.1211, 'eval_samples_per_second': 109.636, 'eval_steps_per_second': 6.907, 'epoch': 0.48}
{'loss': 1.0373, 'grad_norm': 0.22932006418704987, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3731083869934082, 'eval_runtime': 9.0773, 'eval_samples_per_second': 110.164, 'eval_steps_per_second': 6.94, 'epoch': 0.52}
{'loss': 1.0854, 'grad_norm': 0.29358208179473877, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3643498420715332, 'eval_runtime': 9.0675, 'eval_samples_per_second': 110.284, 'eval_steps_per_second': 6.948, 'epoch': 0.56}
{'loss': 1.0297, 'grad_norm': 0.26756444573402405, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3583518266677856, 'eval_runtime': 9.0676, 'eval_samples_per_second': 110.282, 'eval_steps_per_second': 6.948, 'epoch': 0.6}
{'loss': 0.995, 'grad_norm': 0.2472914308309555, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3587239980697632, 'eval_runtime': 9.076, 'eval_samples_per_second': 110.181, 'eval_steps_per_second': 6.941, 'epoch': 0.64}
{'loss': 0.9792, 'grad_norm': 0.35711073875427246, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3568674325942993, 'eval_runtime': 9.0814, 'eval_samples_per_second': 110.116, 'eval_steps_per_second': 6.937, 'epoch': 0.68}
{'loss': 1.0747, 'grad_norm': 0.2782072126865387, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3572871685028076, 'eval_runtime': 9.0696, 'eval_samples_per_second': 110.258, 'eval_steps_per_second': 6.946, 'epoch': 0.72}
{'loss': 1.0026, 'grad_norm': 0.39771661162376404, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3582663536071777, 'eval_runtime': 9.0821, 'eval_samples_per_second': 110.107, 'eval_steps_per_second': 6.937, 'epoch': 0.76}
{'loss': 0.9694, 'grad_norm': 0.2878273129463196, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3587725162506104, 'eval_runtime': 9.0641, 'eval_samples_per_second': 110.326, 'eval_steps_per_second': 6.951, 'epoch': 0.8}
{'loss': 0.9508, 'grad_norm': 0.2870236933231354, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3559894561767578, 'eval_runtime': 9.0583, 'eval_samples_per_second': 110.396, 'eval_steps_per_second': 6.955, 'epoch': 0.84}
{'loss': 0.9168, 'grad_norm': 0.49955132603645325, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3537770509719849, 'eval_runtime': 9.0979, 'eval_samples_per_second': 109.915, 'eval_steps_per_second': 6.925, 'epoch': 0.88}
{'loss': 0.9894, 'grad_norm': 0.4396030306816101, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3533345460891724, 'eval_runtime': 9.0754, 'eval_samples_per_second': 110.188, 'eval_steps_per_second': 6.942, 'epoch': 0.92}
{'loss': 0.9474, 'grad_norm': 0.3300691545009613, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3521679639816284, 'eval_runtime': 9.0733, 'eval_samples_per_second': 110.213, 'eval_steps_per_second': 6.943, 'epoch': 0.96}
{'loss': 0.9967, 'grad_norm': 0.46878671646118164, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.352811336517334, 'eval_runtime': 9.084, 'eval_samples_per_second': 110.083, 'eval_steps_per_second': 6.935, 'epoch': 1.0}
{'train_runtime': 362.3834, 'train_samples_per_second': 27.584, 'train_steps_per_second': 1.725, 'train_loss': 1.1861371856689453, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0779812335968018, 1.6636486053466797, 1.5938408374786377, 1.4877221584320068, 1.4214006662368774, 1.4232478141784668, 1.4201689958572388, 1.4222843647003174, 1.3893349170684814, 1.3772391080856323, 1.3692704439163208, 1.3687753677368164, 1.3731083869934082, 1.3643498420715332, 1.3583518266677856, 1.3587239980697632, 1.3568674325942993, 1.3572871685028076, 1.3582663536071777, 1.3587725162506104, 1.3559894561767578, 1.3537770509719849, 1.3533345460891724, 1.3521679639816284, 1.352811336517334], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0779812335968018, 1.6636486053466797, 1.5938408374786377, 1.4877221584320068, 1.4214006662368774, 1.4232478141784668, 1.4201689958572388, 1.4222843647003174, 1.3893349170684814, 1.3772391080856323, 1.3692704439163208, 1.3687753677368164, 1.3731083869934082, 1.3643498420715332, 1.3583518266677856, 1.3587239980697632, 1.3568674325942993, 1.3572871685028076, 1.3582663536071777, 1.3587725162506104, 1.3559894561767578, 1.3537770509719849, 1.3533345460891724, 1.3521679639816284, 1.352811336517334]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -1.352811336517334
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769, -1.0282820463180542, -1.0929954051971436, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.0556 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -1.0330076785209954
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -1.02959508813217
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -1.0316353192550158
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -1.029599376531051
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -1.0297418415704744
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3014, dtype=torch.float64), 0, 0, tensor(0.0394, dtype=torch.float64), tensor(0.5683, dtype=torch.float64), tensor(0.0143, dtype=torch.float64), tensor(0.0722, dtype=torch.float64), 0, 0, 25, 1, 0, 1, 0, 1, 120, 0.09048139958173868, 46.94033913880201, 0]
normalized proposed parameters for next round by BO: [tensor(0.3014, dtype=torch.float64), tensor(1.8083e-18, dtype=torch.float64), tensor(0.0020, dtype=torch.float64), tensor(0.0394, dtype=torch.float64), tensor(0.5683, dtype=torch.float64), tensor(0.0143, dtype=torch.float64), tensor(0.0722, dtype=torch.float64), tensor(1.6948e-18, dtype=torch.float64), tensor(0.0024, dtype=torch.float64), tensor(0.7912, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9339, dtype=torch.float64), tensor(0.9048, dtype=torch.float64), tensor(0.9779, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.301
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.039
  triviaqa: 0.568
  truthfulqa_gen: 0.014
  wikitext: 0.072
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (120,)
  lora_dropout: (0.09048139958173868,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (46.94033913880201,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  120
lora dropout:  0.09048139958173868
lora alpha:  46.94033913880201
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 135,168,000 || all params: 8,165,429,248 || trainable%: 1.6554
length of training data:  9953
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3073, 'grad_norm': 0.7572374939918518, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.219360828399658, 'eval_runtime': 9.7331, 'eval_samples_per_second': 102.743, 'eval_steps_per_second': 6.473, 'epoch': 0.04}
{'loss': 1.4631, 'grad_norm': 0.2684940695762634, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.2639224529266357, 'eval_runtime': 9.8001, 'eval_samples_per_second': 102.04, 'eval_steps_per_second': 6.429, 'epoch': 0.08}
{'loss': 1.2839, 'grad_norm': 0.29310986399650574, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 2.1654601097106934, 'eval_runtime': 9.8064, 'eval_samples_per_second': 101.974, 'eval_steps_per_second': 6.424, 'epoch': 0.12}
{'loss': 1.1156, 'grad_norm': 0.24282528460025787, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 2.1048433780670166, 'eval_runtime': 9.8096, 'eval_samples_per_second': 101.941, 'eval_steps_per_second': 6.422, 'epoch': 0.16}
{'loss': 1.0245, 'grad_norm': 0.2234918177127838, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 2.0593068599700928, 'eval_runtime': 9.8268, 'eval_samples_per_second': 101.763, 'eval_steps_per_second': 6.411, 'epoch': 0.2}
{'loss': 1.102, 'grad_norm': 0.2234775424003601, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 2.0465657711029053, 'eval_runtime': 9.8134, 'eval_samples_per_second': 101.902, 'eval_steps_per_second': 6.42, 'epoch': 0.24}
{'loss': 1.0376, 'grad_norm': 0.21786510944366455, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 2.0765702724456787, 'eval_runtime': 9.8274, 'eval_samples_per_second': 101.756, 'eval_steps_per_second': 6.411, 'epoch': 0.28}
{'loss': 1.0338, 'grad_norm': 0.2338828444480896, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 2.048180103302002, 'eval_runtime': 9.8275, 'eval_samples_per_second': 101.756, 'eval_steps_per_second': 6.411, 'epoch': 0.32}
{'loss': 1.0296, 'grad_norm': 0.223208948969841, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 2.0644049644470215, 'eval_runtime': 9.8476, 'eval_samples_per_second': 101.548, 'eval_steps_per_second': 6.398, 'epoch': 0.36}
{'loss': 1.0167, 'grad_norm': 0.2013515681028366, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 2.07574462890625, 'eval_runtime': 9.8175, 'eval_samples_per_second': 101.858, 'eval_steps_per_second': 6.417, 'epoch': 0.4}
{'loss': 0.969, 'grad_norm': 0.2375510185956955, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 2.0303869247436523, 'eval_runtime': 9.8275, 'eval_samples_per_second': 101.755, 'eval_steps_per_second': 6.411, 'epoch': 0.44}
{'loss': 1.0467, 'grad_norm': 0.25974974036216736, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 2.046420097351074, 'eval_runtime': 9.8306, 'eval_samples_per_second': 101.723, 'eval_steps_per_second': 6.409, 'epoch': 0.48}
{'loss': 0.9834, 'grad_norm': 0.19554030895233154, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 2.0458552837371826, 'eval_runtime': 9.8086, 'eval_samples_per_second': 101.952, 'eval_steps_per_second': 6.423, 'epoch': 0.52}
{'loss': 1.0124, 'grad_norm': 0.2730502784252167, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 2.070263385772705, 'eval_runtime': 9.814, 'eval_samples_per_second': 101.895, 'eval_steps_per_second': 6.419, 'epoch': 0.56}
{'loss': 0.9744, 'grad_norm': 0.23705445230007172, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 2.0765953063964844, 'eval_runtime': 9.8345, 'eval_samples_per_second': 101.683, 'eval_steps_per_second': 6.406, 'epoch': 0.6}
{'loss': 1.0322, 'grad_norm': 0.23106466233730316, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 2.0480196475982666, 'eval_runtime': 9.8489, 'eval_samples_per_second': 101.534, 'eval_steps_per_second': 6.397, 'epoch': 0.64}
{'loss': 0.977, 'grad_norm': 0.22188353538513184, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 2.0669381618499756, 'eval_runtime': 9.8436, 'eval_samples_per_second': 101.589, 'eval_steps_per_second': 6.4, 'epoch': 0.68}
{'loss': 0.972, 'grad_norm': 0.2129436731338501, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 2.068629503250122, 'eval_runtime': 9.8652, 'eval_samples_per_second': 101.366, 'eval_steps_per_second': 6.386, 'epoch': 0.72}
{'loss': 0.9588, 'grad_norm': 0.1984858363866806, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 2.0808985233306885, 'eval_runtime': 9.8423, 'eval_samples_per_second': 101.602, 'eval_steps_per_second': 6.401, 'epoch': 0.76}
{'loss': 0.9635, 'grad_norm': 0.26949530839920044, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 2.074070930480957, 'eval_runtime': 9.8292, 'eval_samples_per_second': 101.737, 'eval_steps_per_second': 6.409, 'epoch': 0.8}
{'loss': 0.9943, 'grad_norm': 0.2602362036705017, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 2.0676286220550537, 'eval_runtime': 9.826, 'eval_samples_per_second': 101.771, 'eval_steps_per_second': 6.412, 'epoch': 0.84}
{'loss': 0.99, 'grad_norm': 0.25599589943885803, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 2.074191093444824, 'eval_runtime': 9.8524, 'eval_samples_per_second': 101.498, 'eval_steps_per_second': 6.394, 'epoch': 0.88}
{'loss': 1.009, 'grad_norm': 0.2268032282590866, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 2.078859329223633, 'eval_runtime': 9.8504, 'eval_samples_per_second': 101.518, 'eval_steps_per_second': 6.396, 'epoch': 0.92}
{'loss': 1.0358, 'grad_norm': 0.23387154936790466, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 2.0746893882751465, 'eval_runtime': 9.8494, 'eval_samples_per_second': 101.529, 'eval_steps_per_second': 6.396, 'epoch': 0.96}
{'train_runtime': 352.9, 'train_samples_per_second': 28.203, 'train_steps_per_second': 1.765, 'train_loss': 1.1366115527206593, 'epoch': 1.0}
train_results:  {'eval_loss': [2.219360828399658, 2.2639224529266357, 2.1654601097106934, 2.1048433780670166, 2.0593068599700928, 2.0465657711029053, 2.0765702724456787, 2.048180103302002, 2.0644049644470215, 2.07574462890625, 2.0303869247436523, 2.046420097351074, 2.0458552837371826, 2.070263385772705, 2.0765953063964844, 2.0480196475982666, 2.0669381618499756, 2.068629503250122, 2.0808985233306885, 2.074070930480957, 2.0676286220550537, 2.074191093444824, 2.078859329223633, 2.0746893882751465], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.219360828399658, 2.2639224529266357, 2.1654601097106934, 2.1048433780670166, 2.0593068599700928, 2.0465657711029053, 2.0765702724456787, 2.048180103302002, 2.0644049644470215, 2.07574462890625, 2.0303869247436523, 2.046420097351074, 2.0458552837371826, 2.070263385772705, 2.0765953063964844, 2.0480196475982666, 2.0669381618499756, 2.068629503250122, 2.0808985233306885, 2.074070930480957, 2.0676286220550537, 2.074191093444824, 2.078859329223633, 2.0746893882751465]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0282820463180542
current iteration best possible eval_loss (full train run):  -2.0746893882751465
max eval_loss so far:  -1.188298225402832
BO observations:  [-1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0963753461837769, -1.0282820463180542, -1.0929954051971436, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542, -1.0282820463180542]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.9144 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3357044458389282, 0.4876623749732971, 0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7566351294517517, 0.44088155031204224, 0.21595293283462524, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7294671535491943, 0.9934723973274231, 0.20285475254058838, 0.606965959072113]  ‚Üí  acq = -1.0118102517064365
X = [0.7454677820205688, 0.4424137473106384, 0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8704531192779541, 0.009343624114990234, 0.477742075920105, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7311053276062012, 0.7334456443786621, 0.34133514761924744, 0.4159647822380066]  ‚Üí  acq = -1.0165728009984494
X = [0.6070147752761841, 0.050814270973205566, 0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.06286248564720154, 0.38677525520324707, 0.3574179410934448, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5247937440872192, 0.7738797068595886, 0.6983144283294678, 0.9456675052642822]  ‚Üí  acq = -1.011820800868845
X = [0.05165296792984009, 0.43891578912734985, 0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.34409016370773315, 0.6168457269668579, 0.23861968517303467, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.4990995526313782, 0.5673343539237976, 0.3437628448009491, 0.6652377843856812]  ‚Üí  acq = -1.014971694022079
X = [0.05172693729400635, 0.2110685110092163, 0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.34519943594932556, 0.5801001787185669, 0.783306360244751, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9125829935073853, 0.5967274308204651, 0.9407432079315186, 0.06500035524368286]  ‚Üí  acq = -1.0233945618652012
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0682, dtype=torch.float64), 0, tensor(0.0670, dtype=torch.float64), tensor(0.0828, dtype=torch.float64), tensor(0.0204, dtype=torch.float64), tensor(0.1950, dtype=torch.float64), 0, tensor(0.1594, dtype=torch.float64), tensor(0.4049, dtype=torch.float64), 18, 1, 0, 0, 1, 1, 128, 0.0026084972719489924, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0682, dtype=torch.float64), tensor(2.7916e-19, dtype=torch.float64), tensor(0.0670, dtype=torch.float64), tensor(0.0828, dtype=torch.float64), tensor(0.0204, dtype=torch.float64), tensor(0.1950, dtype=torch.float64), tensor(0.0023, dtype=torch.float64), tensor(0.1594, dtype=torch.float64), tensor(0.4049, dtype=torch.float64), tensor(0.5483, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0261, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-2.1364684104919434, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'mmlu', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_mmlu_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['mmlu'], 'weight': [1.0], 'random': [[-2.1192641258239746, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467, -1.2048003673553467], [-2.0815322399139404, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084, -1.2208707332611084], [-2.1364684104919434, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.2323100566864014, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832, -1.188298225402832]], 'random_full_inputs': [[[0.46362886646416485, 0, 0.029629844144068874, 0, 0, 0.07506604643258129, 0, 0, 0.43167524295918513, 27, 1, 1, 1, 0, 1, 107, 0.008828946339714285, 47.99999999999999, 0], [0, 0.23756081949887417, 0, 0, 0, 0.1917563083703989, 0, 0.520062071291662, 0.05062080083906498, 16, 0, 1, 1, 1, 1, 128, 0.0, 48.0, 1], [0, 0, 0, 0.3629110350363794, 0, 0, 0, 0, 0.6339573891496822, 15, 0, 0, 1, 1, 1, 128, 0.0, 16.88194075791525, 1], [0, 0.24762938030783302, 0.0792526766653296, 0.03390502869376776, 0.045421748275761765, 0.24743200577151217, 0, 0.3463591602857957, 0, 27, 0, 0, 1, 0, 1, 7, 6.573674024565671e-20, 48.0, 0], [0.4924449113802735, 0, 0.03363174821566881, 0.09327122994350581, 0, 0, 0.022817192202879154, 0.2664302678524601, 0.09140465040521271, 18, 0, 0, 1, 0, 1, 128, 5.118389433593509e-20, 9.85882197002225, 0], [0, 0.016861249877429624, 0.020741813435926848, 0.05548462241594322, 0.30526291492347396, 0.2878183146155139, 0, 0, 0.3138310847317125, 19, 1, 0, 1, 0, 1, 128, 0.0, 26.163130094488803, 0], [0.16187773235304165, 0.2434925565475308, 0.09165923520077951, 0.04763504930564121, 0.02191937833045607, 0.04964005037871641, 0.08121407527821789, 0, 0.3025619226056166, 28, 1, 1, 1, 1, 0, 101, 0.021786671198701354, 35.69135507938533, 1], [0, 0.04852885301151239, 0.0509485667543197, 0.1373195506128805, 0, 0.02166806719881462, 0.0577670737431018, 0.24440985522965825, 0.4393580334497127, 18, 0, 1, 0, 1, 1, 72, 0.0, 48.0, 1], [0, 0.12228359313577986, 0.02082536522362706, 0.3410105725574465, 0.07209302083679862, 0.13572251313600525, 0.027185060954767436, 0.10442340472561565, 0.17645646942995963, 21, 0, 0, 1, 0, 1, 83, 0.029164550120442296, 22.156323452673107, 0], [0.3033791214259497, 0, 0.07222873866668925, 0, 0.348419223005997, 0.23514518550154642, 0.04082773139981773, 0, 0, 27, 1, 0, 1, 0, 1, 2, 0.06607897402843078, 9.100479302100386, 0], [0, 0, 0.018375638098257218, 0.13696068307817566, 0.08549776733527685, 0.09778696012140609, 0.039164870954547494, 0.2895485512687208, 0.32597144008301276, 14, 1, 1, 0, 0, 0, 81, 5.737496960664978e-20, 6.765812343970218, 1], [0, 0, 0, 0.13983665768087766, 0.06173534006861341, 0.07450241084964715, 0.04198288301295028, 0.2870334082774733, 0.38343482857500066, 14, 0, 0, 0, 0, 1, 87, 3.4694469519536134e-19, 8.072574667720746, 1], [0, 0, 0, 0.21852646515818916, 0, 0.24996173188908793, 0, 0.05894258769644881, 0.4725692152562742, 24, 0, 1, 0, 1, 0, 112, 0.06807334599599184, 14.94814645888734, 0], [0.01738477251026241, 0.18437297592160332, 0, 0.10729658012153681, 0, 0.030205021304849623, 0.015094609065939355, 0, 0.6456460410758084, 26, 0, 0, 1, 1, 1, 53, 0.0, 26.28572216499061, 0], [0.024798398811089285, 0, 0, 0.14646914862309302, 0.0377957599757528, 0.03126944678885254, 0.028846277387185468, 0.28118650756496516, 0.44319631391399666, 14, 0, 1, 0, 1, 1, 96, 0.0, 8.154815143544639, 0], [0, 0, 0, 0.029480648429063023, 0, 0.19108335624559047, 0.1572772543889154, 0, 0.622158740936431, 11, 1, 1, 0, 0, 0, 86, 1.3010426069826058e-19, 48.0, 1], [0, 0, 0, 0, 0, 0, 0, 0.8850449279942971, 0.11495507200570286, 15, 0, 0, 1, 1, 0, 128, 0.01965596956562212, 48.0, 0], [0, 0, 0, 0, 0, 0, 0.19474740745100425, 0.3950838191107472, 0.41016877343824854, 16, 0, 0, 1, 0, 1, 128, 0.0, 48.0, 0], [0, 0.14326132021770702, 0, 0.08136907696132971, 0, 0.013886227139863545, 0, 0.06533350519151834, 0.6961498704895815, 16, 0, 0, 1, 0, 1, 119, 0.09458529616067521, 48.0, 0], [0, 0, 0, 0.18985904249458121, 0, 0.2392076483184658, 0.1682022890942011, 0.26703228861864037, 0.13569873147411157, 17, 0, 1, 1, 1, 0, 128, 0.0, 48.0, 0], [0, 0.6119646878892526, 0, 0, 0, 0, 0, 0.38803531211074827, 0, 15, 0, 0, 1, 1, 1, 128, 0.0, 47.999999999999986, 0], [0, 0, 0, 0.18799767073997423, 0, 0, 0, 0.5384141929462039, 0.2735881363138219, 19, 1, 1, 1, 0, 1, 120, 0.0, 30.793208021204926, 0], [0.023155474552923487, 0, 0.03549318971200149, 0.06712430493170593, 0.6359121987498824, 0.23831483205348677, 0, 0, 0, 25, 1, 1, 0, 0, 0, 49, 0.034983514628449536, 48.0, 0], [0, 0.03850318859263001, 0, 0.08649004279876342, 0, 0.8750067686086067, 0, 0, 0, 18, 0, 1, 1, 1, 1, 84, 0.0, 1.4800000190734863, 1], [0, 0, 0.037331456458519374, 0, 0, 0.9626685435414807, 0, 0, 0, 14, 0, 1, 0, 0, 1, 128, 0.046733813955840336, 31.253608184671947, 0], [0, 0.9460753998800342, 0.05392460011996582, 0, 0, 0, 0, 0, 0, 21, 1, 1, 1, 1, 0, 128, 0.0, 1.4800000190734885, 0], [0, 0, 0.058113549793358456, 0, 0, 0.9046069067794462, 0, 0.037279543427195284, 0, 24, 0, 1, 1, 1, 1, 2, 8.721199550532644e-17, 20.743710849245453, 0], [0.3245260813792206, 0, 0.09279896212450994, 0.2670644972626349, 0, 0, 0, 0, 0.31561045923363457, 26, 0, 0, 1, 0, 1, 109, 0.1, 48.0, 0], [0, 0, 0.03429821861207504, 0.10168200238548296, 0.7373244940347592, 0.08455613559715858, 0.04213914937052441, 0, 0, 14, 0, 1, 1, 1, 1, 128, 0.0, 31.809795488232613, 0], [0, 0.16655362406066662, 0.040384823366566056, 0.610942550806438, 0.18161625277211124, 0, 0, 0, 0, 32, 0, 1, 1, 1, 0, 79, 5.2054056750871866e-20, 36.8693598486112, 0]], [[0.4449990747626971, 0, 0.029957318112662026, 0, 0, 0.08846048499270585, 0, 0, 0.436583122131935, 27, 1, 1, 1, 0, 1, 124, 0.012544744668879141, 48.0, 0], [0, 0.24755477986884275, 0, 0, 0, 0.21524771047580887, 0, 0.47280945302097577, 0.06438805663437273, 16, 0, 1, 1, 1, 1, 128, 6.938893903907231e-19, 48.0, 1], [0, 0, 0, 0.3729979104128747, 0, 0, 0, 0, 0.6230426697227535, 15, 0, 0, 1, 1, 1, 128, 2.7640376422070524e-19, 17.0360875256447, 1], [0, 0.23438980443455365, 0.07796009582762385, 0.06812838086995449, 0.044466057711501986, 0.21666923531163979, 0, 0.35838642584472635, 0, 27, 0, 0, 1, 0, 1, 2, 8.673617379884043e-20, 48.0, 0], [0.5200741123920395, 0, 0.03252586503362014, 0.07880364842425724, 0, 0, 0.02419493572609561, 0.25120827658209594, 0.09140820683534853, 18, 0, 0, 1, 0, 1, 128, 3.80733994358938e-19, 11.577462822391881, 0], [0, 0.022571716233978002, 0.02332826660601914, 0.048288735981051094, 0.30559914658839477, 0.2628066241659033, 0, 0, 0.33740551042465355, 19, 1, 0, 1, 0, 1, 124, 0.0, 26.349023302934995, 0], [0.18224818098931472, 0.25279305462747953, 0.09340996267633789, 0.02708711361886818, 0.019207393560179285, 0.08477191993534212, 0.08079041722073232, 0, 0.25969195737174605, 29, 1, 1, 1, 1, 0, 87, 0.028296481034245105, 36.37327416317987, 1], [0, 0.04127129627528273, 0.05109168687455694, 0.14729052905188555, 0, 0, 0.06374097244538743, 0.26537791285431556, 0.42005092351958195, 18, 0, 1, 0, 1, 1, 90, 1.2542689694135411e-20, 48.0, 1], [0, 0.09156867624112755, 0.018541688922751116, 0.3672723695390102, 0.07739630208649698, 0.13227298607751603, 0.042998582522471164, 0.1362815504558574, 0.13366784415476954, 20, 0, 0, 1, 0, 1, 65, 0.024659687825963585, 25.132416168030776, 0], [0.1786882405252458, 0, 0.06660870040224927, 0.057161088057376755, 0.40032370179821714, 0.2590337086832248, 0.038184560533686226, 0, 0, 28, 0, 0, 1, 0, 1, 10, 0.041174130864611844, 1.4800000190734866, 0], [0.09626959590345105, 0.203634268766369, 0.08791861532492984, 0, 0.25727511245878804, 0.02417948813149199, 0.07886136071393937, 0.12839049194509777, 0.12347106675593299, 24, 1, 1, 0, 1, 0, 41, 0.07837922769276573, 20.50951351735384, 1], [0, 0.023597620183207765, 0.03473563226670559, 0.07854615055622477, 0, 0.16700175003114684, 0.027428436615949656, 0.2928483018346263, 0.36769133295097484, 14, 0, 1, 0, 0, 1, 127, 0.0008728393867451921, 28.81200762677992, 0], [0, 0, 0.04077149038890335, 0, 0, 0.31446525909743894, 0.06009863932050394, 0.3444009732514154, 0.23435291716660064, 14, 1, 0, 1, 1, 1, 103, 1.7150155446158543e-21, 23.96712468222696, 1], [0, 0.07677089840088613, 0.12912757351166995, 0.1639464525791232, 0.477154834789526, 0.013953088208722132, 0, 0.11296078943979672, 0.026086363070275797, 24, 0, 1, 0, 1, 1, 89, 0.0841323339478148, 48.0, 0], [0, 0, 0.028326620327523252, 0.07958008823681877, 0, 0.11622117598425216, 0, 0.3041276803357937, 0.47174443511561226, 14, 1, 1, 1, 0, 1, 128, 0.0014920802910958959, 12.388880899721157, 0], [0, 0, 0.0115126938493298, 0.06924731248317757, 0, 0, 0.06588019283155819, 0.09765548245998999, 0.7557043183759445, 12, 0, 1, 1, 1, 1, 128, 3.7177577711031996e-21, 44.44664639798572, 0], [0.2809327651596025, 0, 0.042085750308026956, 0.1140597103443612, 0, 0.08972471642014196, 0, 0.32207873186824487, 0.15111832589962254, 24, 0, 0, 1, 0, 0, 67, 0.07254324450868337, 30.221597804569214, 1], [0.1732737576114604, 0.18543647602828633, 0.027710209771592244, 0, 0.5826070321626845, 0, 0.023093292698095888, 0, 0, 32, 0, 0, 1, 0, 1, 69, 0.021033115543661562, 44.87442857614467, 0], [0.40576571045125037, 0, 0.07022769535322546, 0, 0.4053787160007338, 0.06372614739471226, 0.0547857898368717, 0, 0, 23, 0, 1, 1, 1, 1, 28, 1.2349475663761566e-05, 48.0, 0], [0, 0.17735609700036578, 0.06948846964389929, 0.19991607819351326, 0, 0.24676750231825384, 0.03378099382622119, 0, 0.2726908590177465, 21, 0, 1, 1, 1, 1, 128, 0.1, 46.09208539107979, 0], [0, 0.07964226458499041, 0.05089675420353973, 0.07809705639664606, 0.030573501346274085, 0.06619080315557914, 0.019582640661525798, 0.278196430146609, 0.39682054950483575, 24, 0, 0, 1, 1, 1, 119, 0.0038760596642227053, 30.825307989446735, 0], [0.03632512292474749, 0.03255516871507373, 0, 0.08822647852944673, 0, 0.06141287696875396, 0.13041013965574327, 0.21729358862324727, 0.42440927475757334, 16, 0, 0, 1, 0, 1, 110, 0.00658251107660232, 34.897597979166015, 0], [0.12741618521489437, 0.10417490777363282, 0.02616731178737251, 0.16301096854874803, 0, 0.16155326215712257, 0, 0.23175812485695813, 0.1859192396612716, 14, 0, 1, 0, 1, 0, 48, 0.005970830468771614, 28.779954952061352, 0], [0, 0.04936985413303782, 0.046607842094900584, 0.13090240077321674, 0.024672057252717483, 0.07181000244248147, 0.018622201998151133, 0.2417399828291878, 0.41582812077536746, 18, 0, 1, 1, 0, 1, 127, 3.017204541766461e-20, 33.487441432087834, 0], [0, 0.029336499425144488, 0.09169036197381006, 0.12228049127961262, 0, 0.06920236616588378, 0.02799558669750484, 0.27543246301466645, 0.3731240778673751, 23, 0, 0, 1, 0, 1, 120, 0.005235151538746052, 34.28823135894563, 0], [0.25434204035302876, 0.37762001992353517, 0.06071164389465655, 0.15597051942889534, 0, 0.11432937879797832, 0.03702639760190587, 0, 0, 24, 0, 0, 1, 0, 1, 51, 0.1, 48.0, 0], [0.1276308494711097, 0.13449546402367057, 0.07161709801888337, 0.051871406789420045, 0.0983249835994307, 0.06562860271100326, 0.010593653381074826, 0.2811290121481868, 0.15870892985722077, 26, 1, 0, 1, 0, 1, 83, 0.06440237816148787, 29.413398852538755, 0], [0.01570478850124305, 0, 0.02006908301833397, 0.20547510985261652, 0.03537580828127034, 0.08254508469551484, 0, 0.2698381911432664, 0.36779772918709663, 14, 1, 0, 1, 1, 0, 128, 0.0047183635333877885, 48.0, 0], [0.22731262152986234, 0, 0.03519333155388658, 0.21927077769050604, 0.11750476048810148, 0.27356832122376173, 0.0638894837725837, 0, 0.05325177096983508, 18, 0, 1, 1, 1, 1, 98, 3.36827138036549e-20, 25.926700957732898, 1], [0, 0.061202870627227436, 0.018625281672686053, 0.09881317955952018, 0.014563149308021113, 0.08555189970928061, 0.016781715820469605, 0.31481648003045004, 0.389645423272345, 14, 0, 0, 1, 1, 0, 112, 2.603692118484303e-20, 32.15603397258978, 0]], [[0.451288658470524, 0, 0.028464999551661277, 0, 0, 0.0927982467570475, 0, 0, 0.4274480952207672, 27, 1, 1, 1, 0, 1, 128, 0.012024394532071167, 48.0, 0], [0, 0.25416289401017705, 0, 0, 0, 0.20995573816959862, 0, 0.45955449331163967, 0.07632687450858457, 16, 0, 1, 1, 1, 1, 128, 3.4694469519535913e-19, 48.0, 1], [0.2567566671276864, 0.09574335490448369, 0.038996343651286215, 0.017932419473796812, 0.019028858197792715, 0.23110670959147736, 0, 0.07504641427026186, 0.265389232783215, 29, 0, 0, 1, 0, 1, 19, 0.05726998082505017, 37.44050969109808, 0], [0.09365112550484579, 0.07797785299773326, 0.0540766497071094, 0.1019394529393953, 0.09792911499310016, 0.03369468604637628, 0.018662692918956798, 0.17043274710138445, 0.35163567779109856, 20, 0, 1, 0, 0, 0, 110, 0.020693833849474735, 33.59545163686889, 0], [0.04484757340827854, 0, 0.05372704652637709, 0.14940469197782405, 0.3912349361669898, 0, 0, 0, 0.36078575192053053, 20, 0, 0, 1, 0, 0, 107, 1.5178830414797064e-19, 36.65335984963713, 0], [0.03185189689697858, 0.14653251013801952, 0.061044225527391305, 0.11996690358151585, 0, 0, 0.02667169333771917, 0.2904916138570845, 0.3169609214975348, 21, 0, 1, 0, 1, 0, 103, 0.05732655764013649, 29.080316769873598, 0], [0.07543173533992635, 0.24943832492833773, 0.03351152072938269, 0.12461842747012825, 0, 0.036878067213225035, 0.08656650333554329, 0.12048888866776017, 0.2730665323156965, 25, 0, 1, 0, 0, 1, 109, 7.702380480917157e-20, 26.750225304509414, 0], [0, 0, 0, 0.08451651520800386, 0, 0.3875997441770561, 0, 0.36908770140304986, 0.1587960392118902, 28, 0, 1, 1, 0, 1, 86, 0.032751345507328285, 26.741034153719994, 0], [0, 0.06298088847652958, 0, 0.27166236669040555, 0, 0, 0.10569281702122812, 0.07578485576407859, 0.47651202398678044, 14, 0, 1, 1, 0, 1, 128, 1.7347234759768077e-19, 20.427103638479345, 0], [0, 0, 0, 0.1723828833826962, 0, 0.2680365069368517, 0, 0.03839512005819139, 0.5211854896222607, 16, 0, 1, 1, 1, 1, 128, 0.1, 22.547760685979775, 1], [0, 0.07391691027499545, 0.034903565023713654, 0.060228142915115135, 0.21250042907619837, 0.1624220433425817, 0.01997431172136129, 0.2912345061951807, 0.14482009145085373, 17, 1, 1, 0, 1, 0, 75, 0.011888729898562803, 11.34044673278546, 1], [0, 0.06568006148506697, 0.023627173224064962, 0.06713032473623899, 0.2075614679353732, 0.16357725816284266, 0.026540978138059942, 0.2667517761806203, 0.179130960137733, 17, 0, 0, 0, 0, 1, 81, 0.0029132816856709976, 13.518872213893042, 1], [0.055464475140961686, 0.5284979591494283, 0.08115018797050591, 0.021601750205095062, 0.3132557645222069, 0, 0, 0, 0, 27, 0, 0, 1, 1, 1, 34, 0.0, 27.365244235642333, 0], [0.027849933862558386, 0, 0, 0.06931851229023102, 0.1069309937896723, 0.1523662722062324, 0, 0.643534287851306, 0, 29, 0, 1, 1, 1, 1, 80, 6.938893903907226e-19, 28.92542453675752, 0], [0.326931303696114, 0, 0.1416291297438361, 0.10399590136384734, 0.04416617344262541, 0, 0.10499111572265576, 0.06017765123349211, 0.21810872479742938, 26, 0, 0, 1, 0, 1, 44, 8.6736173798840345e-19, 16.314432532153482, 0], [0.01303186337252332, 0, 0, 0, 0, 0, 0.09028777707789346, 0.3686661362564431, 0.5280142232931402, 7, 0, 0, 1, 0, 1, 128, 0.0, 28.725442576076304, 0], [0, 0, 0.02035312207058768, 0.07164313453482496, 0.17941436270701863, 0.15631083468777318, 0.02035227577368067, 0.3824236983926709, 0.16799714832767004, 17, 0, 0, 1, 0, 1, 78, 0.020429312308463978, 19.683460321773087, 1], [0.01371451245571765, 0, 0, 0.25024408442973356, 0, 0.0799955038723689, 0.020435842825407145, 0.19403014037988753, 0.44157991603688534, 24, 0, 0, 1, 0, 1, 87, 3.469446951953616e-18, 11.78877862546179, 0], [0.6808220159573912, 0.262724309138866, 0, 0.05645367490374295, 0, 0, 0, 0, 0, 27, 1, 0, 1, 0, 1, 39, 0.0, 48.0, 1], [0, 0.04510459424333166, 0, 0, 0, 0.06009140457936087, 0, 0.27490536732666027, 0.6198986338506473, 10, 1, 1, 1, 1, 1, 128, 2.7755575615629192e-18, 47.99999999999999, 0], [0, 0, 0, 0.16104262679005532, 0, 0.0419928842344843, 0, 0.7969644889754604, 0, 12, 0, 0, 1, 1, 1, 128, 0.0, 2.8431045386042193, 0], [0.1859431344290531, 0.01393502305250267, 0.07239535501831527, 0.1445299332871095, 0.5831965542130195, 0, 0, 0, 0, 27, 1, 1, 1, 0, 0, 7, 1.3010426069826058e-19, 48.0, 0], [0, 0, 0, 0, 0, 0, 0, 0.04397221643856289, 0.9560277835614371, 13, 0, 0, 1, 0, 1, 128, 0.1, 1.4800000190734863, 1], [0, 0.11641886839622208, 0.01402484338855804, 0.1005995366597655, 0.30074648616243504, 0, 0, 0.46821026539301935, 0, 32, 1, 1, 1, 0, 0, 128, 0.08080808031454562, 48.0, 0], [0, 0, 0.15988215570135056, 0.12959861757508864, 0, 0, 0, 0, 0.7105192267235609, 10, 0, 1, 1, 0, 1, 128, 6.938893903907231e-19, 48.0, 0], [0, 0.10312086329964763, 0.07916198148615415, 0.17106156051710206, 0, 0.45906567292195566, 0, 0.1875899217751407, 0, 23, 0, 0, 1, 1, 1, 2, 0.0, 48.0, 0], [0, 0, 0.042428866192213296, 0.05705388000367265, 0.3121224012817116, 0.5883948525224025, 0, 0, 0, 27, 1, 1, 1, 0, 0, 63, 0.0, 48.0, 1], [0, 0.49892865225497296, 0.03601129591956397, 0.14126619605090862, 0.27570764048794943, 0.04808621528660521, 0, 0, 0, 14, 0, 0, 1, 1, 1, 123, 0.0, 22.44647940040776, 0], [0.0786061771824369, 0, 0.06199196835377677, 0.06826314005230665, 0.11979357130471652, 0.17234588618487373, 0.03304403517768386, 0.08972550943131245, 0.3762297123128932, 18, 0, 0, 0, 0, 1, 128, 0.0, 48.0, 0], [0.3013725922487084, 0, 0, 0.03938334433133113, 0.5682822703943906, 0.014324135190354793, 0.07223589291224324, 0, 0, 25, 1, 0, 1, 0, 1, 120, 0.09048139958173868, 46.94033913880201, 0]]], 'random_full_train_performance': [-2.1364684104919434, -1.2323100566864014, -1.2891234159469604, -1.352358341217041, -1.948819875717163, -1.2800376415252686, -1.295063853263855, -1.232936978340149, -1.3919250965118408, -1.3568898439407349, -1.276037335395813, -1.3406656980514526, -1.9749101400375366, -1.188298225402832, -1.3104159832000732, -1.350144386291504, -1.2641600370407104, -1.2898694276809692, -2.1777353286743164, -1.329271674156189, -1.3024567365646362, -1.9505414962768555, -1.517771601676941, -1.2030500173568726, -2.132551908493042, -1.2824980020523071, -1.9764209985733032, -2.0889034271240234, -1.352811336517334, -2.0746893882751465]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.055 MB of 0.055 MB uploadedwandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.091 MB of 0.097 MB uploaded (0.006 MB deduped)wandb: - 0.091 MB of 1.296 MB uploaded (0.006 MB deduped)wandb: \ 0.745 MB of 1.296 MB uploaded (0.006 MB deduped)wandb: | 1.296 MB of 1.296 MB uploaded (0.006 MB deduped)wandb: / 1.296 MB of 1.296 MB uploaded (0.006 MB deduped)wandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÅ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÇ
wandb:            eval/runtime ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ
wandb: eval/samples_per_second ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá
wandb:   eval/steps_per_second ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá
wandb:             train/epoch ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñá
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñá
wandb:         train/grad_norm ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÑ‚ñÖ‚ñÑ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñá‚ñÉ‚ñà‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñà‚ñÉ‚ñá‚ñÇ‚ñà‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÅ
wandb:              train/loss ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñá‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                eval/loss 2.07469
wandb:             eval/runtime 9.8494
wandb:  eval/samples_per_second 101.529
wandb:    eval/steps_per_second 6.396
wandb:               total_flos 6.180370454013542e+16
wandb:              train/epoch 1.0
wandb:        train/global_step 623
wandb:          train/grad_norm 0.23387
wandb:      train/learning_rate 1e-05
wandb:               train/loss 1.0358
wandb:               train_loss 1.13661
wandb:            train_runtime 352.9
wandb: train_samples_per_second 28.203
wandb:   train_steps_per_second 1.765
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/ff9wxrri
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260102_055425-ff9wxrri/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

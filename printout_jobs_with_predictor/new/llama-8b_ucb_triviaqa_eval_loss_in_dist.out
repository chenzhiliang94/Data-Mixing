2026-01-01 05:33:56.243113: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-01 05:33:56.270763: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-01 05:33:56.270820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-01 05:33:56.271874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-01 05:33:56.276826: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-01 05:33:57.224626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'triviaqa', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_triviaqa_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['triviaqa']
evaluation tasks and weights:  {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
triviaqa
evaluation dataset:
data domain:  triviaqa  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/triviaqa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 2, 0, 0, 0, 1, 0, 29, 0.05396168787624587, 37, 1]
Checking history sample input_X_between_0_1:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2265625, 0.5396168787624587, 0.7708333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1686161756515503
Checking history sample input_X:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 7, 0, 0, 1, 0, 0, 101, 0.003930648435578799, 29, 1]
Checking history sample input_X_between_0_1:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7890625, 0.039306484355787985, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.066257357597351
Checking history sample input_X:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 22, 1, 0, 0, 1, 1, 3, 0.04742737743265794, 11, 1]
Checking history sample input_X_between_0_1:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 0.6875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0234375, 0.47427377432657936, 0.22916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.7734289765357971
Checking history sample input_X:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 13, 1, 0, 1, 1, 1, 123, 0.09371289547650785, 48, 0]
Checking history sample input_X_between_0_1:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 0.40625, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9609375, 0.9371289547650785, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0014729499816895
Checking history sample input_X:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 13, 0, 0, 1, 1, 0, 127, 0.09560839072699708, 9, 1]
Checking history sample input_X_between_0_1:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 0.40625, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9921875, 0.9560839072699707, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2857557535171509
Checking history sample input_X:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 9, 0, 0, 1, 1, 1, 16, 0.08052765798611784, 41, 1]
Checking history sample input_X_between_0_1:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 0.28125, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.8052765798611784, 0.8541666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8996233940124512
Checking history sample input_X:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 5, 0, 1, 0, 1, 0, 59, 0.05350476033435025, 24, 1]
Checking history sample input_X_between_0_1:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 0.15625, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4609375, 0.5350476033435024, 0.5, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1949825286865234
Checking history sample input_X:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 2, 1, 0, 0, 1, 0, 52, 0.0964290393747826, 9, 0]
Checking history sample input_X_between_0_1:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 0.0625, 1.0, 0.0, 0.0, 1.0, 0.0, 0.40625, 0.964290393747826, 0.1875, 0.0]
Checking history sample eval_loss at 625 steps:  -1.385130763053894
Checking history sample input_X:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 7, 1, 1, 0, 0, 1, 121, 0.029877603091235272, 27, 0]
Checking history sample input_X_between_0_1:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 0.21875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9453125, 0.2987760309123527, 0.5625, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9837757349014282
Checking history sample input_X:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 5, 1, 1, 0, 0, 0, 99, 0.012004498902397865, 15, 0]
Checking history sample input_X_between_0_1:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 0.15625, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7734375, 0.12004498902397864, 0.3125, 0.0]
Checking history sample eval_loss at 625 steps:  -1.6018083095550537
Checking history sample input_X:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 24, 1, 0, 1, 1, 1, 39, 0.002627655727844236, 41, 0]
Checking history sample input_X_between_0_1:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 0.75, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3046875, 0.02627655727844236, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.8874176144599915
Checking history sample input_X:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 24, 0, 0, 0, 0, 1, 93, 0.03163409179138251, 48, 0]
Checking history sample input_X_between_0_1:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 0.75, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7265625, 0.31634091791382507, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.058224081993103
Checking history sample input_X:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 28, 1, 0, 0, 0, 0, 89, 0.0740409362882843, 48, 0]
Checking history sample input_X_between_0_1:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 0.875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6953125, 0.740409362882843, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.431011438369751
Checking history sample input_X:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 28, 1, 0, 1, 1, 1, 2, 0.052382422456560135, 28, 1]
Checking history sample input_X_between_0_1:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 0.875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.015625, 0.5238242245656013, 0.5833333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9802380204200745
Checking history sample input_X:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 11, 0, 0, 0, 1, 1, 54, 0.06588137610749685, 20, 1]
Checking history sample input_X_between_0_1:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 0.34375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.421875, 0.6588137610749685, 0.4166666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -0.7846323251724243
Checking history sample input_X:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 27, 0, 0, 1, 1, 1, 76, 0.04383658958637696, 17, 0]
Checking history sample input_X_between_0_1:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 0.84375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.59375, 0.4383658958637696, 0.3541666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2182475328445435
Checking history sample input_X:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 29, 0, 1, 0, 0, 1, 82, 0.07326489367366754, 40, 1]
Checking history sample input_X_between_0_1:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 0.90625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.640625, 0.7326489367366753, 0.8333333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.7634263634681702
Checking history sample input_X:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 1, 0, 0, 0, 1, 0, 44, 0.09959850306478138, 19, 0]
Checking history sample input_X_between_0_1:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.34375, 0.9959850306478137, 0.3958333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.4772111177444458
Checking history sample input_X:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 26, 1, 1, 1, 0, 1, 67, 0.08108306564701856, 9, 1]
Checking history sample input_X_between_0_1:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 0.8125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5234375, 0.8108306564701856, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4388036727905273
Checking history sample input_X:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 29, 0, 0, 0, 1, 1, 29, 0.08410329802499458, 20, 0]
Checking history sample input_X_between_0_1:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 0.90625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2265625, 0.8410329802499458, 0.4166666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0373977422714233
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1770 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9257982969284058, 0.12132638692855835, 0.5172797441482544, 0.9784384369850159, 0.06273287534713745, 0.7016603946685791, 0.8933642506599426, 0.327457070350647, 0.2625097632408142, 0.3718041181564331, 0.29203659296035767, 0.2168428897857666, 0.40497177839279175, 0.9630089402198792, 0.07094216346740723, 0.6317062973976135, 0.37243539094924927, 0.6121106147766113, 0.552446186542511]  ‚Üí  acq = -0.4930561245002757
X = [0.35622352361679077, 0.9103285074234009, 0.10913622379302979, 0.7634136080741882, 0.7787624001502991, 0.1142457127571106, 0.8752115964889526, 0.7531395554542542, 0.3170129060745239, 0.6259313225746155, 0.24328261613845825, 0.03195786476135254, 0.974888265132904, 0.40299391746520996, 0.07547533512115479, 0.4216298460960388, 0.2525884509086609, 0.3136347830295563, 0.12894809246063232]  ‚Üí  acq = -0.4930641973081885
X = [0.5474957823753357, 0.8120182752609253, 0.4867405295372009, 0.1920188069343567, 0.11172300577163696, 0.591119110584259, 0.5620378851890564, 0.9039405584335327, 0.4800574779510498, 0.9106730818748474, 0.40568768978118896, 0.43510788679122925, 0.8959982991218567, 0.8990333676338196, 0.49801361560821533, 0.3459853529930115, 0.9576355218887329, 0.9072767496109009, 0.46402013301849365]  ‚Üí  acq = -0.4933433841999749
X = [0.08041691780090332, 0.5374331474304199, 0.5847190022468567, 0.11733436584472656, 0.21908700466156006, 0.7221159934997559, 0.9368019104003906, 0.8132724165916443, 0.6625928282737732, 0.6770657896995544, 0.7025123834609985, 0.9850216507911682, 0.19324404001235962, 0.8823720216751099, 0.9637840986251831, 0.9721358418464661, 0.5789272785186768, 0.8384073972702026, 0.6085694432258606]  ‚Üí  acq = -0.49305491692054737
X = [0.3787965774536133, 0.23119664192199707, 0.8863762021064758, 0.6097474694252014, 0.484377384185791, 0.5244515538215637, 0.40153175592422485, 0.32985562086105347, 0.16521400213241577, 0.6584050059318542, 0.9959684610366821, 0.11974716186523438, 0.17489075660705566, 0.3012251853942871, 0.7658048272132874, 0.22958268225193024, 0.019263088703155518, 0.21802935004234314, 0.4069908857345581]  ‚Üí  acq = -0.4931103500573961
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.4334, dtype=torch.float64), 0, tensor(0.3811, dtype=torch.float64), tensor(0.0852, dtype=torch.float64), 0, 0, tensor(0.1002, dtype=torch.float64), 0, 20, 0, 1, 0, 1, 1, 2, 0.016001276169592498, 48.0, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(2.0070e-17, dtype=torch.float64), tensor(0.4334, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3811, dtype=torch.float64), tensor(0.0852, dtype=torch.float64), tensor(6.9833e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1002, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6210, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.1600, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.433
  rowan_hellaswag: 0
  sciq: 0.381
  triviaqa: 0.085
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.1
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.016001276169592498,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  2
lora dropout:  0.016001276169592498
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,679,360 || all params: 8,031,940,608 || trainable%: 0.0209
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260101_054216-hoxe9otw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/hoxe9otw
{'loss': 2.3285, 'grad_norm': 4.695982933044434, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.806451439857483, 'eval_runtime': 3.3143, 'eval_samples_per_second': 301.724, 'eval_steps_per_second': 19.009, 'epoch': 0.04}
{'loss': 1.0917, 'grad_norm': 3.1782121658325195, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3000566959381104, 'eval_runtime': 3.1812, 'eval_samples_per_second': 314.343, 'eval_steps_per_second': 19.804, 'epoch': 0.08}
{'loss': 0.9851, 'grad_norm': 2.0582852363586426, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1453584432601929, 'eval_runtime': 3.1454, 'eval_samples_per_second': 317.929, 'eval_steps_per_second': 20.03, 'epoch': 0.12}
{'loss': 0.9203, 'grad_norm': 2.215282917022705, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1082359552383423, 'eval_runtime': 3.1335, 'eval_samples_per_second': 319.129, 'eval_steps_per_second': 20.105, 'epoch': 0.16}
{'loss': 0.9314, 'grad_norm': 1.837616205215454, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1059836149215698, 'eval_runtime': 3.1447, 'eval_samples_per_second': 318.0, 'eval_steps_per_second': 20.034, 'epoch': 0.2}
{'loss': 0.9262, 'grad_norm': 7.846348285675049, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1133406162261963, 'eval_runtime': 3.1351, 'eval_samples_per_second': 318.968, 'eval_steps_per_second': 20.095, 'epoch': 0.24}
{'loss': 0.8972, 'grad_norm': 1.9319982528686523, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0811423063278198, 'eval_runtime': 3.1427, 'eval_samples_per_second': 318.203, 'eval_steps_per_second': 20.047, 'epoch': 0.28}
{'loss': 0.9069, 'grad_norm': 1.5020965337753296, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.084820032119751, 'eval_runtime': 3.1354, 'eval_samples_per_second': 318.94, 'eval_steps_per_second': 20.093, 'epoch': 0.32}
{'loss': 0.8996, 'grad_norm': 1.6464548110961914, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0788276195526123, 'eval_runtime': 3.1384, 'eval_samples_per_second': 318.637, 'eval_steps_per_second': 20.074, 'epoch': 0.36}
{'loss': 0.8461, 'grad_norm': 1.5514158010482788, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0846314430236816, 'eval_runtime': 3.1391, 'eval_samples_per_second': 318.562, 'eval_steps_per_second': 20.069, 'epoch': 0.4}
{'loss': 0.9097, 'grad_norm': 1.5674526691436768, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0839965343475342, 'eval_runtime': 3.1404, 'eval_samples_per_second': 318.427, 'eval_steps_per_second': 20.061, 'epoch': 0.44}
{'loss': 0.8759, 'grad_norm': 1.6826039552688599, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0696907043457031, 'eval_runtime': 3.1362, 'eval_samples_per_second': 318.854, 'eval_steps_per_second': 20.088, 'epoch': 0.48}
{'loss': 0.8743, 'grad_norm': 1.4170918464660645, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0778332948684692, 'eval_runtime': 3.1749, 'eval_samples_per_second': 314.967, 'eval_steps_per_second': 19.843, 'epoch': 0.52}
{'loss': 0.8735, 'grad_norm': 1.579177737236023, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0802912712097168, 'eval_runtime': 3.1677, 'eval_samples_per_second': 315.69, 'eval_steps_per_second': 19.888, 'epoch': 0.56}
{'loss': 0.8808, 'grad_norm': 1.7964062690734863, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0774394273757935, 'eval_runtime': 3.1512, 'eval_samples_per_second': 317.344, 'eval_steps_per_second': 19.993, 'epoch': 0.6}
{'loss': 0.8706, 'grad_norm': 1.2371351718902588, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0748471021652222, 'eval_runtime': 3.137, 'eval_samples_per_second': 318.779, 'eval_steps_per_second': 20.083, 'epoch': 0.64}
{'loss': 0.8587, 'grad_norm': 1.5199471712112427, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0755535364151, 'eval_runtime': 3.1393, 'eval_samples_per_second': 318.546, 'eval_steps_per_second': 20.068, 'epoch': 0.68}
{'loss': 0.8476, 'grad_norm': 1.3442190885543823, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.077581524848938, 'eval_runtime': 3.1328, 'eval_samples_per_second': 319.208, 'eval_steps_per_second': 20.11, 'epoch': 0.72}
{'loss': 0.8559, 'grad_norm': 1.4491071701049805, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.053438663482666, 'eval_runtime': 3.1479, 'eval_samples_per_second': 317.674, 'eval_steps_per_second': 20.013, 'epoch': 0.76}
{'loss': 0.8198, 'grad_norm': 1.5697996616363525, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0666359663009644, 'eval_runtime': 3.1315, 'eval_samples_per_second': 319.336, 'eval_steps_per_second': 20.118, 'epoch': 0.8}
{'loss': 0.8307, 'grad_norm': 1.5705727338790894, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0584567785263062, 'eval_runtime': 3.1533, 'eval_samples_per_second': 317.126, 'eval_steps_per_second': 19.979, 'epoch': 0.84}
{'loss': 0.8517, 'grad_norm': 1.888883113861084, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.056117296218872, 'eval_runtime': 3.1376, 'eval_samples_per_second': 318.712, 'eval_steps_per_second': 20.079, 'epoch': 0.88}
{'loss': 0.8195, 'grad_norm': 1.2119107246398926, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.053888201713562, 'eval_runtime': 3.1367, 'eval_samples_per_second': 318.801, 'eval_steps_per_second': 20.084, 'epoch': 0.92}
{'loss': 0.8598, 'grad_norm': 1.413287878036499, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.054424524307251, 'eval_runtime': 3.1302, 'eval_samples_per_second': 319.464, 'eval_steps_per_second': 20.126, 'epoch': 0.96}
{'loss': 0.8332, 'grad_norm': 1.4270821809768677, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0554845333099365, 'eval_runtime': 3.1348, 'eval_samples_per_second': 318.998, 'eval_steps_per_second': 20.097, 'epoch': 1.0}
{'train_runtime': 303.215, 'train_samples_per_second': 32.977, 'train_steps_per_second': 2.061, 'train_loss': 0.9437786163330079, 'epoch': 1.0}
train_results:  {'eval_loss': [1.806451439857483, 1.3000566959381104, 1.1453584432601929, 1.1082359552383423, 1.1059836149215698, 1.1133406162261963, 1.0811423063278198, 1.084820032119751, 1.0788276195526123, 1.0846314430236816, 1.0839965343475342, 1.0696907043457031, 1.0778332948684692, 1.0802912712097168, 1.0774394273757935, 1.0748471021652222, 1.0755535364151, 1.077581524848938, 1.053438663482666, 1.0666359663009644, 1.0584567785263062, 1.056117296218872, 1.053888201713562, 1.054424524307251, 1.0554845333099365], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.806451439857483, 1.3000566959381104, 1.1453584432601929, 1.1082359552383423, 1.1059836149215698, 1.1133406162261963, 1.0811423063278198, 1.084820032119751, 1.0788276195526123, 1.0846314430236816, 1.0839965343475342, 1.0696907043457031, 1.0778332948684692, 1.0802912712097168, 1.0774394273757935, 1.0748471021652222, 1.0755535364151, 1.077581524848938, 1.053438663482666, 1.0666359663009644, 1.0584567785263062, 1.056117296218872, 1.053888201713562, 1.054424524307251, 1.0554845333099365]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.8673405647277832
current iteration best possible eval_loss (full train run):  -1.0554845333099365
max eval_loss so far:  -1.0554845333099365
BO observations:  [-1.8673405647277832]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8546 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.6962704969160382
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.6938342094164854
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.6938342003457797
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.694515197601315
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.6939301485371081
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0108, dtype=torch.float64), tensor(0.1267, dtype=torch.float64), tensor(0.0242, dtype=torch.float64), tensor(0.1133, dtype=torch.float64), 0, 0, 0, tensor(0.2230, dtype=torch.float64), tensor(0.5020, dtype=torch.float64), 1, 1, 0, 1, 0, 1, 128, 1.5088409119918436e-18, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0108, dtype=torch.float64), tensor(0.1267, dtype=torch.float64), tensor(0.0242, dtype=torch.float64), tensor(0.1133, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0560e-19, dtype=torch.float64), tensor(3.9715e-17, dtype=torch.float64), tensor(0.2230, dtype=torch.float64), tensor(0.5020, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.5088e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.011
  gsm8k: 0.127
  rowan_hellaswag: 0.024
  sciq: 0.113
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.223
  arc_challenge: 0.502

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.5088409119918436e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  1.5088409119918436e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 5,767,168 || all params: 8,036,028,416 || trainable%: 0.0718
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5931, 'grad_norm': 0.8906211256980896, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.965165853500366, 'eval_runtime': 2.8102, 'eval_samples_per_second': 355.844, 'eval_steps_per_second': 22.418, 'epoch': 0.04}
{'loss': 2.4659, 'grad_norm': 1.0032113790512085, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.429476022720337, 'eval_runtime': 2.7778, 'eval_samples_per_second': 359.991, 'eval_steps_per_second': 22.679, 'epoch': 0.08}
{'loss': 1.8253, 'grad_norm': 0.6227658987045288, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1965010166168213, 'eval_runtime': 2.7856, 'eval_samples_per_second': 358.989, 'eval_steps_per_second': 22.616, 'epoch': 0.12}
{'loss': 1.5131, 'grad_norm': 0.5477952361106873, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.137983798980713, 'eval_runtime': 2.7838, 'eval_samples_per_second': 359.223, 'eval_steps_per_second': 22.631, 'epoch': 0.16}
{'loss': 1.4941, 'grad_norm': 0.45101749897003174, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0801119804382324, 'eval_runtime': 2.7921, 'eval_samples_per_second': 358.153, 'eval_steps_per_second': 22.564, 'epoch': 0.2}
{'loss': 1.4493, 'grad_norm': 0.6267485022544861, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1030921936035156, 'eval_runtime': 2.7923, 'eval_samples_per_second': 358.124, 'eval_steps_per_second': 22.562, 'epoch': 0.24}
{'loss': 1.415, 'grad_norm': 0.8715690970420837, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.082550287246704, 'eval_runtime': 2.794, 'eval_samples_per_second': 357.91, 'eval_steps_per_second': 22.548, 'epoch': 0.28}
{'loss': 1.331, 'grad_norm': 0.48130175471305847, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9278082847595215, 'eval_runtime': 2.7989, 'eval_samples_per_second': 357.286, 'eval_steps_per_second': 22.509, 'epoch': 0.32}
{'loss': 1.2602, 'grad_norm': 0.3272976875305176, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9325337409973145, 'eval_runtime': 2.7945, 'eval_samples_per_second': 357.845, 'eval_steps_per_second': 22.544, 'epoch': 0.36}
{'loss': 1.2606, 'grad_norm': 0.6299185752868652, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9540749788284302, 'eval_runtime': 2.7961, 'eval_samples_per_second': 357.644, 'eval_steps_per_second': 22.532, 'epoch': 0.4}
{'loss': 1.2283, 'grad_norm': 0.3693488836288452, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8377668857574463, 'eval_runtime': 2.785, 'eval_samples_per_second': 359.072, 'eval_steps_per_second': 22.622, 'epoch': 0.44}
{'loss': 1.1525, 'grad_norm': 0.3831201195716858, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7664473056793213, 'eval_runtime': 2.7913, 'eval_samples_per_second': 358.262, 'eval_steps_per_second': 22.57, 'epoch': 0.48}
{'loss': 1.2063, 'grad_norm': 0.307591050863266, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7724970579147339, 'eval_runtime': 2.7997, 'eval_samples_per_second': 357.185, 'eval_steps_per_second': 22.503, 'epoch': 0.52}
{'loss': 1.178, 'grad_norm': 0.34158024191856384, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6890373229980469, 'eval_runtime': 2.791, 'eval_samples_per_second': 358.289, 'eval_steps_per_second': 22.572, 'epoch': 0.56}
{'loss': 1.156, 'grad_norm': 0.26512575149536133, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7370786666870117, 'eval_runtime': 2.7898, 'eval_samples_per_second': 358.453, 'eval_steps_per_second': 22.583, 'epoch': 0.6}
{'loss': 1.1339, 'grad_norm': 0.34282511472702026, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7690314054489136, 'eval_runtime': 2.7929, 'eval_samples_per_second': 358.057, 'eval_steps_per_second': 22.558, 'epoch': 0.64}
{'loss': 1.1678, 'grad_norm': 0.4984075427055359, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7304226160049438, 'eval_runtime': 2.7929, 'eval_samples_per_second': 358.047, 'eval_steps_per_second': 22.557, 'epoch': 0.68}
{'loss': 1.1507, 'grad_norm': 0.36544325947761536, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7090444564819336, 'eval_runtime': 2.7916, 'eval_samples_per_second': 358.217, 'eval_steps_per_second': 22.568, 'epoch': 0.72}
{'loss': 1.1423, 'grad_norm': 0.48582732677459717, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7011891603469849, 'eval_runtime': 2.7921, 'eval_samples_per_second': 358.152, 'eval_steps_per_second': 22.564, 'epoch': 0.76}
{'loss': 1.1411, 'grad_norm': 0.4200856387615204, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7393085956573486, 'eval_runtime': 2.7957, 'eval_samples_per_second': 357.689, 'eval_steps_per_second': 22.534, 'epoch': 0.8}
{'loss': 1.132, 'grad_norm': 0.40342599153518677, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.755873680114746, 'eval_runtime': 2.7894, 'eval_samples_per_second': 358.501, 'eval_steps_per_second': 22.586, 'epoch': 0.84}
{'loss': 1.1257, 'grad_norm': 0.33958911895751953, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7568000555038452, 'eval_runtime': 2.7929, 'eval_samples_per_second': 358.047, 'eval_steps_per_second': 22.557, 'epoch': 0.88}
{'loss': 1.1714, 'grad_norm': 0.36512088775634766, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7496370077133179, 'eval_runtime': 2.7919, 'eval_samples_per_second': 358.18, 'eval_steps_per_second': 22.565, 'epoch': 0.92}
{'loss': 1.0854, 'grad_norm': 0.32480448484420776, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7341371774673462, 'eval_runtime': 2.7916, 'eval_samples_per_second': 358.212, 'eval_steps_per_second': 22.567, 'epoch': 0.96}
{'loss': 1.1334, 'grad_norm': 0.38610684871673584, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.743719220161438, 'eval_runtime': 2.7868, 'eval_samples_per_second': 358.839, 'eval_steps_per_second': 22.607, 'epoch': 1.0}
{'train_runtime': 250.4195, 'train_samples_per_second': 39.921, 'train_steps_per_second': 2.496, 'train_loss': 1.396483462524414, 'epoch': 1.0}
train_results:  {'eval_loss': [3.965165853500366, 2.429476022720337, 2.1965010166168213, 2.137983798980713, 2.0801119804382324, 2.1030921936035156, 2.082550287246704, 1.9278082847595215, 1.9325337409973145, 1.9540749788284302, 1.8377668857574463, 1.7664473056793213, 1.7724970579147339, 1.6890373229980469, 1.7370786666870117, 1.7690314054489136, 1.7304226160049438, 1.7090444564819336, 1.7011891603469849, 1.7393085956573486, 1.755873680114746, 1.7568000555038452, 1.7496370077133179, 1.7341371774673462, 1.743719220161438], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.965165853500366, 2.429476022720337, 2.1965010166168213, 2.137983798980713, 2.0801119804382324, 2.1030921936035156, 2.082550287246704, 1.9278082847595215, 1.9325337409973145, 1.9540749788284302, 1.8377668857574463, 1.7664473056793213, 1.7724970579147339, 1.6890373229980469, 1.7370786666870117, 1.7690314054489136, 1.7304226160049438, 1.7090444564819336, 1.7011891603469849, 1.7393085956573486, 1.755873680114746, 1.7568000555038452, 1.7496370077133179, 1.7341371774673462, 1.743719220161438]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.6618080139160156
current iteration best possible eval_loss (full train run):  -1.743719220161438
max eval_loss so far:  -1.0554845333099365
BO observations:  [-1.8673405647277832, -3.6618080139160156]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3574 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.5629869708405022
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.6507534248902009
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.6484570735974069
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.5590761143771843
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.34774729194441045
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.1089, dtype=torch.float64), tensor(0.3129, dtype=torch.float64), 0, tensor(0.4467, dtype=torch.float64), tensor(0.1314, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 7.070722313504211e-18, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.3710e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1089, dtype=torch.float64), tensor(0.3129, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4467, dtype=torch.float64), tensor(0.1314, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(7.0707e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.109
  truthfulqa_gen: 0.313
  wikitext: 0
  mmlu: 0.447
  arc_challenge: 0.131

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (7.070722313504211e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  7.070722313504211e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6234, 'grad_norm': 0.5893052816390991, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.529489278793335, 'eval_runtime': 3.5636, 'eval_samples_per_second': 280.614, 'eval_steps_per_second': 17.679, 'epoch': 0.04}
{'loss': 1.2495, 'grad_norm': 0.5544722676277161, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1412495374679565, 'eval_runtime': 3.5624, 'eval_samples_per_second': 280.709, 'eval_steps_per_second': 17.685, 'epoch': 0.08}
{'loss': 1.1032, 'grad_norm': 0.23725926876068115, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.098586082458496, 'eval_runtime': 3.5525, 'eval_samples_per_second': 281.493, 'eval_steps_per_second': 17.734, 'epoch': 0.12}
{'loss': 1.0886, 'grad_norm': 0.2497556060552597, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1178432703018188, 'eval_runtime': 3.5547, 'eval_samples_per_second': 281.321, 'eval_steps_per_second': 17.723, 'epoch': 0.16}
{'loss': 1.0415, 'grad_norm': 0.2636071443557739, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0835754871368408, 'eval_runtime': 3.5518, 'eval_samples_per_second': 281.548, 'eval_steps_per_second': 17.738, 'epoch': 0.2}
{'loss': 1.0047, 'grad_norm': 0.30634456872940063, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0921738147735596, 'eval_runtime': 3.5633, 'eval_samples_per_second': 280.639, 'eval_steps_per_second': 17.68, 'epoch': 0.24}
{'loss': 1.0347, 'grad_norm': 0.279204785823822, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0546005964279175, 'eval_runtime': 3.5526, 'eval_samples_per_second': 281.487, 'eval_steps_per_second': 17.734, 'epoch': 0.28}
{'loss': 0.9803, 'grad_norm': 0.2591247856616974, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0668323040008545, 'eval_runtime': 3.5569, 'eval_samples_per_second': 281.141, 'eval_steps_per_second': 17.712, 'epoch': 0.32}
{'loss': 1.004, 'grad_norm': 0.27653858065605164, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0597466230392456, 'eval_runtime': 3.5694, 'eval_samples_per_second': 280.161, 'eval_steps_per_second': 17.65, 'epoch': 0.36}
{'loss': 1.0011, 'grad_norm': 0.2770194113254547, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.062023639678955, 'eval_runtime': 3.5644, 'eval_samples_per_second': 280.549, 'eval_steps_per_second': 17.675, 'epoch': 0.4}
{'loss': 0.9558, 'grad_norm': 0.2801157236099243, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0561884641647339, 'eval_runtime': 3.5579, 'eval_samples_per_second': 281.062, 'eval_steps_per_second': 17.707, 'epoch': 0.44}
{'loss': 0.9753, 'grad_norm': 0.3073069453239441, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0410696268081665, 'eval_runtime': 3.5648, 'eval_samples_per_second': 280.521, 'eval_steps_per_second': 17.673, 'epoch': 0.48}
{'loss': 0.9651, 'grad_norm': 0.2565792202949524, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.035770297050476, 'eval_runtime': 3.5634, 'eval_samples_per_second': 280.632, 'eval_steps_per_second': 17.68, 'epoch': 0.52}
{'loss': 0.9429, 'grad_norm': 0.36808961629867554, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.045420527458191, 'eval_runtime': 3.5645, 'eval_samples_per_second': 280.545, 'eval_steps_per_second': 17.674, 'epoch': 0.56}
{'loss': 0.905, 'grad_norm': 0.2564461827278137, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0353740453720093, 'eval_runtime': 3.5599, 'eval_samples_per_second': 280.906, 'eval_steps_per_second': 17.697, 'epoch': 0.6}
{'loss': 0.8533, 'grad_norm': 0.2308669537305832, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0200139284133911, 'eval_runtime': 3.5611, 'eval_samples_per_second': 280.815, 'eval_steps_per_second': 17.691, 'epoch': 0.64}
{'loss': 0.9462, 'grad_norm': 0.2786344885826111, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0305359363555908, 'eval_runtime': 3.5621, 'eval_samples_per_second': 280.734, 'eval_steps_per_second': 17.686, 'epoch': 0.68}
{'loss': 0.8709, 'grad_norm': 0.2861844301223755, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.030073642730713, 'eval_runtime': 3.5632, 'eval_samples_per_second': 280.643, 'eval_steps_per_second': 17.681, 'epoch': 0.72}
{'loss': 0.8766, 'grad_norm': 0.2849361300468445, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0278785228729248, 'eval_runtime': 3.561, 'eval_samples_per_second': 280.816, 'eval_steps_per_second': 17.691, 'epoch': 0.76}
{'loss': 0.8955, 'grad_norm': 0.2734728455543518, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.021525502204895, 'eval_runtime': 3.5605, 'eval_samples_per_second': 280.861, 'eval_steps_per_second': 17.694, 'epoch': 0.8}
{'loss': 0.9271, 'grad_norm': 0.346699059009552, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0283093452453613, 'eval_runtime': 3.558, 'eval_samples_per_second': 281.055, 'eval_steps_per_second': 17.706, 'epoch': 0.84}
{'loss': 0.8905, 'grad_norm': 0.2820320725440979, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0240951776504517, 'eval_runtime': 3.5935, 'eval_samples_per_second': 278.284, 'eval_steps_per_second': 17.532, 'epoch': 0.88}
{'loss': 0.898, 'grad_norm': 0.2677997350692749, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0222119092941284, 'eval_runtime': 3.6129, 'eval_samples_per_second': 276.785, 'eval_steps_per_second': 17.437, 'epoch': 0.92}
{'loss': 0.8969, 'grad_norm': 0.26513436436653137, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0226134061813354, 'eval_runtime': 3.5862, 'eval_samples_per_second': 278.846, 'eval_steps_per_second': 17.567, 'epoch': 0.96}
{'loss': 0.8806, 'grad_norm': 0.2850003242492676, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.022034764289856, 'eval_runtime': 3.5744, 'eval_samples_per_second': 279.769, 'eval_steps_per_second': 17.625, 'epoch': 1.0}
{'train_runtime': 318.3053, 'train_samples_per_second': 31.41, 'train_steps_per_second': 1.964, 'train_loss': 1.032422509765625, 'epoch': 1.0}
train_results:  {'eval_loss': [1.529489278793335, 1.1412495374679565, 1.098586082458496, 1.1178432703018188, 1.0835754871368408, 1.0921738147735596, 1.0546005964279175, 1.0668323040008545, 1.0597466230392456, 1.062023639678955, 1.0561884641647339, 1.0410696268081665, 1.035770297050476, 1.045420527458191, 1.0353740453720093, 1.0200139284133911, 1.0305359363555908, 1.030073642730713, 1.0278785228729248, 1.021525502204895, 1.0283093452453613, 1.0240951776504517, 1.0222119092941284, 1.0226134061813354, 1.022034764289856], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.529489278793335, 1.1412495374679565, 1.098586082458496, 1.1178432703018188, 1.0835754871368408, 1.0921738147735596, 1.0546005964279175, 1.0668323040008545, 1.0597466230392456, 1.062023639678955, 1.0561884641647339, 1.0410696268081665, 1.035770297050476, 1.045420527458191, 1.0353740453720093, 1.0200139284133911, 1.0305359363555908, 1.030073642730713, 1.0278785228729248, 1.021525502204895, 1.0283093452453613, 1.0240951776504517, 1.0222119092941284, 1.0226134061813354, 1.022034764289856]
current iteration observed (possibly low-fid or predicted) eval_loss:  -4.5206804275512695
current iteration best possible eval_loss (full train run):  -1.022034764289856
max eval_loss so far:  -1.022034764289856
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.2478 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = 0.38179969486137777
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = 0.34872478891944114
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = 0.30271459036442483
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = 0.5019290232993898
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = 0.29816872895754054
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1134, dtype=torch.float64), 0, 0, tensor(0.4008, dtype=torch.float64), tensor(0.3576, dtype=torch.float64), 0, tensor(0.1281, dtype=torch.float64), 0, 23, 0, 1, 0, 0, 0, 12, 0.011290650405569095, 44.94784649994607, 1]
normalized proposed parameters for next round by BO: [tensor(1.3311e-16, dtype=torch.float64), tensor(0.1134, dtype=torch.float64), tensor(1.3644e-16, dtype=torch.float64), tensor(1.6658e-16, dtype=torch.float64), tensor(0.4008, dtype=torch.float64), tensor(0.3576, dtype=torch.float64), tensor(2.2491e-16, dtype=torch.float64), tensor(0.1281, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7122, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0947, dtype=torch.float64), tensor(0.1129, dtype=torch.float64), tensor(0.9364, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.113
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.401
  truthfulqa_gen: 0.358
  wikitext: 0
  mmlu: 0.128
  arc_challenge: 0

LoRA Parameters:
  lora_r: (12,)
  lora_dropout: (0.011290650405569095,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (44.94784649994607,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  12
lora dropout:  0.011290650405569095
lora alpha:  44.94784649994607
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,413,120 || all params: 8,031,674,368 || trainable%: 0.0176
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4538, 'grad_norm': 4.112270355224609, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.366922616958618, 'eval_runtime': 2.8324, 'eval_samples_per_second': 353.058, 'eval_steps_per_second': 22.243, 'epoch': 0.04}
{'loss': 1.6243, 'grad_norm': 1.3058041334152222, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5200096368789673, 'eval_runtime': 2.836, 'eval_samples_per_second': 352.613, 'eval_steps_per_second': 22.215, 'epoch': 0.08}
{'loss': 1.3189, 'grad_norm': 1.4821538925170898, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4032588005065918, 'eval_runtime': 2.8328, 'eval_samples_per_second': 353.007, 'eval_steps_per_second': 22.239, 'epoch': 0.12}
{'loss': 1.2337, 'grad_norm': 1.4658775329589844, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2880975008010864, 'eval_runtime': 2.8392, 'eval_samples_per_second': 352.216, 'eval_steps_per_second': 22.19, 'epoch': 0.16}
{'loss': 1.1672, 'grad_norm': 0.9909777641296387, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.248133897781372, 'eval_runtime': 2.8441, 'eval_samples_per_second': 351.61, 'eval_steps_per_second': 22.151, 'epoch': 0.2}
{'loss': 1.0831, 'grad_norm': 1.2483510971069336, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.238122820854187, 'eval_runtime': 2.8414, 'eval_samples_per_second': 351.936, 'eval_steps_per_second': 22.172, 'epoch': 0.24}
{'loss': 1.0925, 'grad_norm': 1.0612643957138062, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.224761724472046, 'eval_runtime': 2.828, 'eval_samples_per_second': 353.607, 'eval_steps_per_second': 22.277, 'epoch': 0.28}
{'loss': 1.0895, 'grad_norm': 1.1176127195358276, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2107499837875366, 'eval_runtime': 2.8321, 'eval_samples_per_second': 353.092, 'eval_steps_per_second': 22.245, 'epoch': 0.32}
{'loss': 1.0338, 'grad_norm': 1.0975984334945679, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2058290243148804, 'eval_runtime': 2.8501, 'eval_samples_per_second': 350.869, 'eval_steps_per_second': 22.105, 'epoch': 0.36}
{'loss': 1.0568, 'grad_norm': 1.5606554746627808, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1786059141159058, 'eval_runtime': 2.8387, 'eval_samples_per_second': 352.28, 'eval_steps_per_second': 22.194, 'epoch': 0.4}
{'loss': 1.1015, 'grad_norm': 1.163306713104248, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1553337574005127, 'eval_runtime': 2.8288, 'eval_samples_per_second': 353.508, 'eval_steps_per_second': 22.271, 'epoch': 0.44}
{'loss': 1.0316, 'grad_norm': 1.0048730373382568, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1272616386413574, 'eval_runtime': 2.8305, 'eval_samples_per_second': 353.299, 'eval_steps_per_second': 22.258, 'epoch': 0.48}
{'loss': 0.9937, 'grad_norm': 1.6476384401321411, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0805175304412842, 'eval_runtime': 2.8276, 'eval_samples_per_second': 353.66, 'eval_steps_per_second': 22.281, 'epoch': 0.52}
{'loss': 0.9531, 'grad_norm': 1.618759036064148, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.069198489189148, 'eval_runtime': 2.8429, 'eval_samples_per_second': 351.748, 'eval_steps_per_second': 22.16, 'epoch': 0.56}
{'loss': 0.9525, 'grad_norm': 1.3774394989013672, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.06352961063385, 'eval_runtime': 2.8338, 'eval_samples_per_second': 352.889, 'eval_steps_per_second': 22.232, 'epoch': 0.6}
{'loss': 1.0038, 'grad_norm': 1.2299565076828003, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0486643314361572, 'eval_runtime': 2.8265, 'eval_samples_per_second': 353.794, 'eval_steps_per_second': 22.289, 'epoch': 0.64}
{'loss': 0.9764, 'grad_norm': 1.0791165828704834, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0478368997573853, 'eval_runtime': 2.8389, 'eval_samples_per_second': 352.255, 'eval_steps_per_second': 22.192, 'epoch': 0.68}
{'loss': 0.975, 'grad_norm': 1.6091965436935425, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0471190214157104, 'eval_runtime': 2.8343, 'eval_samples_per_second': 352.815, 'eval_steps_per_second': 22.227, 'epoch': 0.72}
{'loss': 0.9139, 'grad_norm': 1.344170093536377, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0427581071853638, 'eval_runtime': 2.869, 'eval_samples_per_second': 348.549, 'eval_steps_per_second': 21.959, 'epoch': 0.76}
{'loss': 0.9628, 'grad_norm': 1.302656888961792, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.039781093597412, 'eval_runtime': 2.8688, 'eval_samples_per_second': 348.579, 'eval_steps_per_second': 21.961, 'epoch': 0.8}
{'loss': 0.9473, 'grad_norm': 1.4091157913208008, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0406500101089478, 'eval_runtime': 2.8688, 'eval_samples_per_second': 348.574, 'eval_steps_per_second': 21.96, 'epoch': 0.84}
{'loss': 0.9637, 'grad_norm': 1.1803585290908813, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0381664037704468, 'eval_runtime': 2.8658, 'eval_samples_per_second': 348.944, 'eval_steps_per_second': 21.984, 'epoch': 0.88}
{'loss': 0.933, 'grad_norm': 1.1364556550979614, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0380580425262451, 'eval_runtime': 2.8597, 'eval_samples_per_second': 349.689, 'eval_steps_per_second': 22.03, 'epoch': 0.92}
{'loss': 0.9551, 'grad_norm': 1.116694688796997, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.035894751548767, 'eval_runtime': 2.8488, 'eval_samples_per_second': 351.022, 'eval_steps_per_second': 22.114, 'epoch': 0.96}
{'loss': 0.949, 'grad_norm': 1.1736280918121338, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0345104932785034, 'eval_runtime': 2.8427, 'eval_samples_per_second': 351.774, 'eval_steps_per_second': 22.162, 'epoch': 1.0}
{'train_runtime': 240.5111, 'train_samples_per_second': 41.57, 'train_steps_per_second': 2.599, 'train_loss': 1.150635711669922, 'epoch': 1.0}
train_results:  {'eval_loss': [2.366922616958618, 1.5200096368789673, 1.4032588005065918, 1.2880975008010864, 1.248133897781372, 1.238122820854187, 1.224761724472046, 1.2107499837875366, 1.2058290243148804, 1.1786059141159058, 1.1553337574005127, 1.1272616386413574, 1.0805175304412842, 1.069198489189148, 1.06352961063385, 1.0486643314361572, 1.0478368997573853, 1.0471190214157104, 1.0427581071853638, 1.039781093597412, 1.0406500101089478, 1.0381664037704468, 1.0380580425262451, 1.035894751548767, 1.0345104932785034], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.366922616958618, 1.5200096368789673, 1.4032588005065918, 1.2880975008010864, 1.248133897781372, 1.238122820854187, 1.224761724472046, 1.2107499837875366, 1.2058290243148804, 1.1786059141159058, 1.1553337574005127, 1.1272616386413574, 1.0805175304412842, 1.069198489189148, 1.06352961063385, 1.0486643314361572, 1.0478368997573853, 1.0471190214157104, 1.0427581071853638, 1.039781093597412, 1.0406500101089478, 1.0381664037704468, 1.0380580425262451, 1.035894751548767, 1.0345104932785034]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.182877540588379
current iteration best possible eval_loss (full train run):  -1.0345104932785034
max eval_loss so far:  -1.022034764289856
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3759 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.2640728195528723
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.2397124543666327
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.2964936569092389
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.2813031320903181
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.2827403921510383
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0336, dtype=torch.float64), 0, 0, tensor(0.1450, dtype=torch.float64), 0, tensor(0.5464, dtype=torch.float64), 0, 0, tensor(0.2750, dtype=torch.float64), 3, 0, 0, 1, 1, 1, 47, 0.1, 1.4800000190734883, 1]
normalized proposed parameters for next round by BO: [tensor(0.0336, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1450, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5464, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.4214e-16, dtype=torch.float64), tensor(0.2750, dtype=torch.float64), tensor(0.0818, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3637, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.034
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.145
  triviaqa: 0
  truthfulqa_gen: 0.546
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.275

LoRA Parameters:
  lora_r: (47,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (3,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734883,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  3
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  47
lora dropout:  0.1
lora alpha:  1.4800000190734883
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 7,796,736 || all params: 8,038,057,984 || trainable%: 0.0970
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.8558, 'grad_norm': 1.022089958190918, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 4.818484306335449, 'eval_runtime': 2.8392, 'eval_samples_per_second': 352.212, 'eval_steps_per_second': 22.189, 'epoch': 0.04}
{'loss': 3.5167, 'grad_norm': 1.324476957321167, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.7485225200653076, 'eval_runtime': 2.8447, 'eval_samples_per_second': 351.535, 'eval_steps_per_second': 22.147, 'epoch': 0.08}
{'loss': 2.1522, 'grad_norm': 0.6501592397689819, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.2011377811431885, 'eval_runtime': 2.8991, 'eval_samples_per_second': 344.938, 'eval_steps_per_second': 21.731, 'epoch': 0.12}
{'loss': 1.4949, 'grad_norm': 0.4785655438899994, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1285204887390137, 'eval_runtime': 2.8686, 'eval_samples_per_second': 348.608, 'eval_steps_per_second': 21.962, 'epoch': 0.16}
{'loss': 1.31, 'grad_norm': 0.6133965253829956, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9783382415771484, 'eval_runtime': 2.8595, 'eval_samples_per_second': 349.707, 'eval_steps_per_second': 22.032, 'epoch': 0.2}
{'loss': 1.1997, 'grad_norm': 0.3327556848526001, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9480102062225342, 'eval_runtime': 2.8658, 'eval_samples_per_second': 348.948, 'eval_steps_per_second': 21.984, 'epoch': 0.24}
{'loss': 1.1471, 'grad_norm': 0.3043181300163269, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9805808067321777, 'eval_runtime': 2.8662, 'eval_samples_per_second': 348.889, 'eval_steps_per_second': 21.98, 'epoch': 0.28}
{'loss': 1.0961, 'grad_norm': 0.36695489287376404, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.833240270614624, 'eval_runtime': 2.8654, 'eval_samples_per_second': 348.995, 'eval_steps_per_second': 21.987, 'epoch': 0.32}
{'loss': 1.0632, 'grad_norm': 0.39300045371055603, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7682491540908813, 'eval_runtime': 2.8704, 'eval_samples_per_second': 348.383, 'eval_steps_per_second': 21.948, 'epoch': 0.36}
{'loss': 1.0421, 'grad_norm': 0.5080037117004395, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.85115385055542, 'eval_runtime': 2.8568, 'eval_samples_per_second': 350.043, 'eval_steps_per_second': 22.053, 'epoch': 0.4}
{'loss': 1.0125, 'grad_norm': 0.1834886521100998, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7931345701217651, 'eval_runtime': 2.8574, 'eval_samples_per_second': 349.968, 'eval_steps_per_second': 22.048, 'epoch': 0.44}
{'loss': 0.9583, 'grad_norm': 0.2980690002441406, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8238896131515503, 'eval_runtime': 2.8536, 'eval_samples_per_second': 350.429, 'eval_steps_per_second': 22.077, 'epoch': 0.48}
{'loss': 0.9845, 'grad_norm': 0.26588955521583557, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7857084274291992, 'eval_runtime': 2.8552, 'eval_samples_per_second': 350.242, 'eval_steps_per_second': 22.065, 'epoch': 0.52}
{'loss': 0.9352, 'grad_norm': 0.2865285873413086, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7323349714279175, 'eval_runtime': 2.8527, 'eval_samples_per_second': 350.55, 'eval_steps_per_second': 22.085, 'epoch': 0.56}
{'loss': 0.9329, 'grad_norm': 0.1721760481595993, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8056755065917969, 'eval_runtime': 2.8555, 'eval_samples_per_second': 350.196, 'eval_steps_per_second': 22.062, 'epoch': 0.6}
{'loss': 0.9226, 'grad_norm': 0.37338986992836, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.651796579360962, 'eval_runtime': 2.8603, 'eval_samples_per_second': 349.617, 'eval_steps_per_second': 22.026, 'epoch': 0.64}
{'loss': 0.9253, 'grad_norm': 0.3306542932987213, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8534914255142212, 'eval_runtime': 2.8515, 'eval_samples_per_second': 350.691, 'eval_steps_per_second': 22.094, 'epoch': 0.68}
{'loss': 0.9028, 'grad_norm': 0.19499360024929047, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.653757095336914, 'eval_runtime': 2.8744, 'eval_samples_per_second': 347.904, 'eval_steps_per_second': 21.918, 'epoch': 0.72}
{'loss': 0.886, 'grad_norm': 0.23789876699447632, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6983332633972168, 'eval_runtime': 2.8662, 'eval_samples_per_second': 348.89, 'eval_steps_per_second': 21.98, 'epoch': 0.76}
{'loss': 0.9045, 'grad_norm': 0.2263725847005844, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7440258264541626, 'eval_runtime': 2.8577, 'eval_samples_per_second': 349.931, 'eval_steps_per_second': 22.046, 'epoch': 0.8}
{'loss': 0.9336, 'grad_norm': 0.1897331178188324, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6984621286392212, 'eval_runtime': 2.8523, 'eval_samples_per_second': 350.593, 'eval_steps_per_second': 22.087, 'epoch': 0.84}
{'loss': 0.911, 'grad_norm': 0.2394753247499466, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7313621044158936, 'eval_runtime': 2.8562, 'eval_samples_per_second': 350.117, 'eval_steps_per_second': 22.057, 'epoch': 0.88}
{'loss': 0.9208, 'grad_norm': 0.16805332899093628, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7965381145477295, 'eval_runtime': 2.853, 'eval_samples_per_second': 350.504, 'eval_steps_per_second': 22.082, 'epoch': 0.92}
{'loss': 0.8906, 'grad_norm': 0.21900784969329834, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7101584672927856, 'eval_runtime': 2.8532, 'eval_samples_per_second': 350.486, 'eval_steps_per_second': 22.081, 'epoch': 0.96}
{'loss': 0.8978, 'grad_norm': 0.23477578163146973, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.754929542541504, 'eval_runtime': 2.8534, 'eval_samples_per_second': 350.462, 'eval_steps_per_second': 22.079, 'epoch': 1.0}
{'train_runtime': 195.1937, 'train_samples_per_second': 51.221, 'train_steps_per_second': 3.202, 'train_loss': 1.311849429321289, 'epoch': 1.0}
train_results:  {'eval_loss': [4.818484306335449, 2.7485225200653076, 2.2011377811431885, 2.1285204887390137, 1.9783382415771484, 1.9480102062225342, 1.9805808067321777, 1.833240270614624, 1.7682491540908813, 1.85115385055542, 1.7931345701217651, 1.8238896131515503, 1.7857084274291992, 1.7323349714279175, 1.8056755065917969, 1.651796579360962, 1.8534914255142212, 1.653757095336914, 1.6983332633972168, 1.7440258264541626, 1.6984621286392212, 1.7313621044158936, 1.7965381145477295, 1.7101584672927856, 1.754929542541504], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [4.818484306335449, 2.7485225200653076, 2.2011377811431885, 2.1285204887390137, 1.9783382415771484, 1.9480102062225342, 1.9805808067321777, 1.833240270614624, 1.7682491540908813, 1.85115385055542, 1.7931345701217651, 1.8238896131515503, 1.7857084274291992, 1.7323349714279175, 1.8056755065917969, 1.651796579360962, 1.8534914255142212, 1.653757095336914, 1.6983332633972168, 1.7440258264541626, 1.6984621286392212, 1.7313621044158936, 1.7965381145477295, 1.7101584672927856, 1.754929542541504]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.265839099884033
current iteration best possible eval_loss (full train run):  -1.754929542541504
max eval_loss so far:  -1.022034764289856
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9089 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -1.05090816872783
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -1.089942751009184
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -1.0300072043347184
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -1.4903273421278853
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -1.0896763524834419
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1548, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6138, dtype=torch.float64), 0, tensor(0.1310, dtype=torch.float64), tensor(0.1004, dtype=torch.float64), 32, 0, 1, 0, 0, 1, 128, 0.05150083061893645, 26.744626045815792, 1]
normalized proposed parameters for next round by BO: [tensor(0.1548, dtype=torch.float64), tensor(3.0647e-17, dtype=torch.float64), tensor(2.7697e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6138, dtype=torch.float64), tensor(3.2034e-18, dtype=torch.float64), tensor(0.1310, dtype=torch.float64), tensor(0.1004, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5150, dtype=torch.float64), tensor(0.5572, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.155
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.614
  wikitext: 0
  mmlu: 0.131
  arc_challenge: 0.1

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05150083061893645,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (26.744626045815792,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  128
lora dropout:  0.05150083061893645
lora alpha:  26.744626045815792
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5026, 'grad_norm': 0.4293174147605896, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4102094173431396, 'eval_runtime': 3.0728, 'eval_samples_per_second': 325.432, 'eval_steps_per_second': 20.502, 'epoch': 0.04}
{'loss': 1.4186, 'grad_norm': 0.20814445614814758, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.8229353427886963, 'eval_runtime': 3.057, 'eval_samples_per_second': 327.117, 'eval_steps_per_second': 20.608, 'epoch': 0.08}
{'loss': 1.1434, 'grad_norm': 0.2303924858570099, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8492788076400757, 'eval_runtime': 3.0585, 'eval_samples_per_second': 326.954, 'eval_steps_per_second': 20.598, 'epoch': 0.12}
{'loss': 0.9819, 'grad_norm': 0.25047335028648376, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7403523921966553, 'eval_runtime': 3.0646, 'eval_samples_per_second': 326.309, 'eval_steps_per_second': 20.557, 'epoch': 0.16}
{'loss': 0.9071, 'grad_norm': 0.22721141576766968, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.570349097251892, 'eval_runtime': 3.0708, 'eval_samples_per_second': 325.645, 'eval_steps_per_second': 20.516, 'epoch': 0.2}
{'loss': 0.8248, 'grad_norm': 0.21735267341136932, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6428478956222534, 'eval_runtime': 3.0708, 'eval_samples_per_second': 325.649, 'eval_steps_per_second': 20.516, 'epoch': 0.24}
{'loss': 0.7799, 'grad_norm': 0.2477341741323471, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.752493977546692, 'eval_runtime': 3.0765, 'eval_samples_per_second': 325.04, 'eval_steps_per_second': 20.478, 'epoch': 0.28}
{'loss': 0.7587, 'grad_norm': 0.2045372724533081, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8111157417297363, 'eval_runtime': 3.0716, 'eval_samples_per_second': 325.564, 'eval_steps_per_second': 20.511, 'epoch': 0.32}
{'loss': 0.7475, 'grad_norm': 0.22108367085456848, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8061259984970093, 'eval_runtime': 3.0726, 'eval_samples_per_second': 325.453, 'eval_steps_per_second': 20.504, 'epoch': 0.36}
{'loss': 0.7213, 'grad_norm': 0.22989396750926971, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6798652410507202, 'eval_runtime': 3.0683, 'eval_samples_per_second': 325.916, 'eval_steps_per_second': 20.533, 'epoch': 0.4}
{'loss': 0.7002, 'grad_norm': 0.2541745603084564, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6411027908325195, 'eval_runtime': 3.06, 'eval_samples_per_second': 326.801, 'eval_steps_per_second': 20.588, 'epoch': 0.44}
{'loss': 0.7141, 'grad_norm': 0.18842613697052002, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.642499566078186, 'eval_runtime': 3.0708, 'eval_samples_per_second': 325.648, 'eval_steps_per_second': 20.516, 'epoch': 0.48}
{'loss': 0.6671, 'grad_norm': 0.23177511990070343, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7319059371948242, 'eval_runtime': 3.0605, 'eval_samples_per_second': 326.743, 'eval_steps_per_second': 20.585, 'epoch': 0.52}
{'loss': 0.6319, 'grad_norm': 0.22719848155975342, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6626698970794678, 'eval_runtime': 3.064, 'eval_samples_per_second': 326.375, 'eval_steps_per_second': 20.562, 'epoch': 0.56}
{'loss': 0.6701, 'grad_norm': 0.20536404848098755, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.756676435470581, 'eval_runtime': 3.0647, 'eval_samples_per_second': 326.299, 'eval_steps_per_second': 20.557, 'epoch': 0.6}
{'loss': 0.6333, 'grad_norm': 0.21082845330238342, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7198667526245117, 'eval_runtime': 3.0594, 'eval_samples_per_second': 326.857, 'eval_steps_per_second': 20.592, 'epoch': 0.64}
{'loss': 0.6672, 'grad_norm': 0.20778723061084747, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7755520343780518, 'eval_runtime': 3.0661, 'eval_samples_per_second': 326.151, 'eval_steps_per_second': 20.548, 'epoch': 0.68}
{'loss': 0.6471, 'grad_norm': 0.18128274381160736, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8072619438171387, 'eval_runtime': 3.0654, 'eval_samples_per_second': 326.226, 'eval_steps_per_second': 20.552, 'epoch': 0.72}
{'loss': 0.6108, 'grad_norm': 0.16089005768299103, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.817003607749939, 'eval_runtime': 3.0632, 'eval_samples_per_second': 326.451, 'eval_steps_per_second': 20.566, 'epoch': 0.76}
{'loss': 0.6316, 'grad_norm': 0.19782909750938416, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7985992431640625, 'eval_runtime': 3.0638, 'eval_samples_per_second': 326.389, 'eval_steps_per_second': 20.562, 'epoch': 0.8}
{'loss': 0.6605, 'grad_norm': 0.24385491013526917, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8074597120285034, 'eval_runtime': 3.0667, 'eval_samples_per_second': 326.084, 'eval_steps_per_second': 20.543, 'epoch': 0.84}
{'loss': 0.5783, 'grad_norm': 0.20428681373596191, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8083000183105469, 'eval_runtime': 3.0694, 'eval_samples_per_second': 325.797, 'eval_steps_per_second': 20.525, 'epoch': 0.88}
{'loss': 0.6159, 'grad_norm': 0.17258372902870178, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8225970268249512, 'eval_runtime': 3.1173, 'eval_samples_per_second': 320.786, 'eval_steps_per_second': 20.21, 'epoch': 0.92}
{'loss': 0.6067, 'grad_norm': 0.17368102073669434, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8199409246444702, 'eval_runtime': 3.0899, 'eval_samples_per_second': 323.632, 'eval_steps_per_second': 20.389, 'epoch': 0.96}
{'loss': 0.6079, 'grad_norm': 0.18377411365509033, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8194409608840942, 'eval_runtime': 3.1104, 'eval_samples_per_second': 321.5, 'eval_steps_per_second': 20.255, 'epoch': 1.0}
{'train_runtime': 228.5658, 'train_samples_per_second': 43.742, 'train_steps_per_second': 2.734, 'train_loss': 0.8571415054321289, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4102094173431396, 1.8229353427886963, 1.8492788076400757, 1.7403523921966553, 1.570349097251892, 1.6428478956222534, 1.752493977546692, 1.8111157417297363, 1.8061259984970093, 1.6798652410507202, 1.6411027908325195, 1.642499566078186, 1.7319059371948242, 1.6626698970794678, 1.756676435470581, 1.7198667526245117, 1.7755520343780518, 1.8072619438171387, 1.817003607749939, 1.7985992431640625, 1.8074597120285034, 1.8083000183105469, 1.8225970268249512, 1.8199409246444702, 1.8194409608840942], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.4102094173431396, 1.8229353427886963, 1.8492788076400757, 1.7403523921966553, 1.570349097251892, 1.6428478956222534, 1.752493977546692, 1.8111157417297363, 1.8061259984970093, 1.6798652410507202, 1.6411027908325195, 1.642499566078186, 1.7319059371948242, 1.6626698970794678, 1.756676435470581, 1.7198667526245117, 1.7755520343780518, 1.8072619438171387, 1.817003607749939, 1.7985992431640625, 1.8074597120285034, 1.8083000183105469, 1.8225970268249512, 1.8199409246444702, 1.8194409608840942]
current iteration observed (possibly low-fid or predicted) eval_loss:  -4.54395055770874
current iteration best possible eval_loss (full train run):  -1.8194409608840942
max eval_loss so far:  -1.022034764289856
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5607 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.15775145618816744
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = 0.1413696320320046
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = 0.41711241867627447
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = 0.15067134954733086
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.14943901403958693
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3916, dtype=torch.float64), tensor(0.0506, dtype=torch.float64), tensor(0.3487, dtype=torch.float64), tensor(0.2091, dtype=torch.float64), 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 107, 0.08725557331767518, 18.62627501178458, 1]
normalized proposed parameters for next round by BO: [tensor(1.9575e-17, dtype=torch.float64), tensor(0.3916, dtype=torch.float64), tensor(0.0506, dtype=torch.float64), tensor(0.3487, dtype=torch.float64), tensor(0.2091, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.6621e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0052e-16, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8398, dtype=torch.float64), tensor(0.8726, dtype=torch.float64), tensor(0.3880, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.392
  rowan_hellaswag: 0.051
  sciq: 0.349
  triviaqa: 0.209
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (107,)
  lora_dropout: (0.08725557331767518,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (18.62627501178458,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  107
lora dropout:  0.08725557331767518
lora alpha:  18.62627501178458
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,972,224 || all params: 8,032,233,472 || trainable%: 0.0246
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4923, 'grad_norm': 0.6229221820831299, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 4.63380765914917, 'eval_runtime': 2.7836, 'eval_samples_per_second': 359.246, 'eval_steps_per_second': 22.633, 'epoch': 0.04}
{'loss': 2.6349, 'grad_norm': 0.6476947665214539, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.584496259689331, 'eval_runtime': 2.7651, 'eval_samples_per_second': 361.652, 'eval_steps_per_second': 22.784, 'epoch': 0.08}
{'loss': 1.8279, 'grad_norm': 1.5622309446334839, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1410653591156006, 'eval_runtime': 2.7784, 'eval_samples_per_second': 359.914, 'eval_steps_per_second': 22.675, 'epoch': 0.12}
{'loss': 1.6393, 'grad_norm': 0.5614281892776489, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0035269260406494, 'eval_runtime': 2.7812, 'eval_samples_per_second': 359.552, 'eval_steps_per_second': 22.652, 'epoch': 0.16}
{'loss': 1.5858, 'grad_norm': 1.011629581451416, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.033130645751953, 'eval_runtime': 2.789, 'eval_samples_per_second': 358.548, 'eval_steps_per_second': 22.589, 'epoch': 0.2}
{'loss': 1.4894, 'grad_norm': 1.6187812089920044, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9370033740997314, 'eval_runtime': 2.7895, 'eval_samples_per_second': 358.485, 'eval_steps_per_second': 22.585, 'epoch': 0.24}
{'loss': 1.4713, 'grad_norm': 1.1895145177841187, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9531832933425903, 'eval_runtime': 2.7856, 'eval_samples_per_second': 358.99, 'eval_steps_per_second': 22.616, 'epoch': 0.28}
{'loss': 1.4849, 'grad_norm': 0.7835990786552429, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8645564317703247, 'eval_runtime': 2.7886, 'eval_samples_per_second': 358.608, 'eval_steps_per_second': 22.592, 'epoch': 0.32}
{'loss': 1.4515, 'grad_norm': 1.137560486793518, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.871406078338623, 'eval_runtime': 2.798, 'eval_samples_per_second': 357.392, 'eval_steps_per_second': 22.516, 'epoch': 0.36}
{'loss': 1.3909, 'grad_norm': 0.564287006855011, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.872286319732666, 'eval_runtime': 2.7906, 'eval_samples_per_second': 358.349, 'eval_steps_per_second': 22.576, 'epoch': 0.4}
{'loss': 1.3619, 'grad_norm': 0.8406528830528259, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8004223108291626, 'eval_runtime': 2.7944, 'eval_samples_per_second': 357.862, 'eval_steps_per_second': 22.545, 'epoch': 0.44}
{'loss': 1.3486, 'grad_norm': 0.6626374125480652, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.748702883720398, 'eval_runtime': 2.7935, 'eval_samples_per_second': 357.978, 'eval_steps_per_second': 22.553, 'epoch': 0.48}
{'loss': 1.3475, 'grad_norm': 0.8364750146865845, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7054911851882935, 'eval_runtime': 2.7961, 'eval_samples_per_second': 357.639, 'eval_steps_per_second': 22.531, 'epoch': 0.52}
{'loss': 1.2957, 'grad_norm': 0.8106578588485718, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6485909223556519, 'eval_runtime': 2.7947, 'eval_samples_per_second': 357.824, 'eval_steps_per_second': 22.543, 'epoch': 0.56}
{'loss': 1.2622, 'grad_norm': 1.0602085590362549, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6184308528900146, 'eval_runtime': 2.7972, 'eval_samples_per_second': 357.497, 'eval_steps_per_second': 22.522, 'epoch': 0.6}
{'loss': 1.3046, 'grad_norm': 1.1238608360290527, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6015888452529907, 'eval_runtime': 2.802, 'eval_samples_per_second': 356.89, 'eval_steps_per_second': 22.484, 'epoch': 0.64}
{'loss': 1.2377, 'grad_norm': 0.9693857431411743, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.578365683555603, 'eval_runtime': 2.801, 'eval_samples_per_second': 357.015, 'eval_steps_per_second': 22.492, 'epoch': 0.68}
{'loss': 1.2757, 'grad_norm': 0.817275881767273, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5813316106796265, 'eval_runtime': 2.7926, 'eval_samples_per_second': 358.084, 'eval_steps_per_second': 22.559, 'epoch': 0.72}
{'loss': 1.2247, 'grad_norm': 0.5046952962875366, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5704621076583862, 'eval_runtime': 2.7943, 'eval_samples_per_second': 357.871, 'eval_steps_per_second': 22.546, 'epoch': 0.76}
{'loss': 1.2232, 'grad_norm': 0.7917333841323853, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5538058280944824, 'eval_runtime': 2.8012, 'eval_samples_per_second': 356.989, 'eval_steps_per_second': 22.49, 'epoch': 0.8}
{'loss': 1.2185, 'grad_norm': 0.6963204741477966, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5436240434646606, 'eval_runtime': 2.7952, 'eval_samples_per_second': 357.76, 'eval_steps_per_second': 22.539, 'epoch': 0.84}
{'loss': 1.2249, 'grad_norm': 1.1092418432235718, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.542605996131897, 'eval_runtime': 2.7966, 'eval_samples_per_second': 357.574, 'eval_steps_per_second': 22.527, 'epoch': 0.88}
{'loss': 1.1852, 'grad_norm': 0.7193018794059753, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.531614065170288, 'eval_runtime': 2.7926, 'eval_samples_per_second': 358.091, 'eval_steps_per_second': 22.56, 'epoch': 0.92}
{'loss': 1.2334, 'grad_norm': 0.64895099401474, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5319113731384277, 'eval_runtime': 2.7957, 'eval_samples_per_second': 357.699, 'eval_steps_per_second': 22.535, 'epoch': 0.96}
{'loss': 1.2144, 'grad_norm': 0.7099972367286682, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5204275846481323, 'eval_runtime': 2.7934, 'eval_samples_per_second': 357.988, 'eval_steps_per_second': 22.553, 'epoch': 1.0}
{'train_runtime': 256.377, 'train_samples_per_second': 38.997, 'train_steps_per_second': 2.438, 'train_loss': 1.4970402191162109, 'epoch': 1.0}
train_results:  {'eval_loss': [4.63380765914917, 2.584496259689331, 2.1410653591156006, 2.0035269260406494, 2.033130645751953, 1.9370033740997314, 1.9531832933425903, 1.8645564317703247, 1.871406078338623, 1.872286319732666, 1.8004223108291626, 1.748702883720398, 1.7054911851882935, 1.6485909223556519, 1.6184308528900146, 1.6015888452529907, 1.578365683555603, 1.5813316106796265, 1.5704621076583862, 1.5538058280944824, 1.5436240434646606, 1.542605996131897, 1.531614065170288, 1.5319113731384277, 1.5204275846481323], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [4.63380765914917, 2.584496259689331, 2.1410653591156006, 2.0035269260406494, 2.033130645751953, 1.9370033740997314, 1.9531832933425903, 1.8645564317703247, 1.871406078338623, 1.872286319732666, 1.8004223108291626, 1.748702883720398, 1.7054911851882935, 1.6485909223556519, 1.6184308528900146, 1.6015888452529907, 1.578365683555603, 1.5813316106796265, 1.5704621076583862, 1.5538058280944824, 1.5436240434646606, 1.542605996131897, 1.531614065170288, 1.5319113731384277, 1.5204275846481323]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.3427348136901855
current iteration best possible eval_loss (full train run):  -1.5204275846481323
max eval_loss so far:  -1.022034764289856
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.7587 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.6099416615887154
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.6099802507777854
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.6099125870970026
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.6099510165438145
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.609378956193269
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0696, dtype=torch.float64), tensor(0.2926, dtype=torch.float64), 0, 0, tensor(0.2265, dtype=torch.float64), tensor(0.0867, dtype=torch.float64), 0, 0, tensor(0.3246, dtype=torch.float64), 18, 1, 1, 1, 0, 0, 54, 0.09993012740129961, 31.225536622143487, 1]
normalized proposed parameters for next round by BO: [tensor(0.0696, dtype=torch.float64), tensor(0.2926, dtype=torch.float64), tensor(1.0813e-18, dtype=torch.float64), tensor(2.1656e-17, dtype=torch.float64), tensor(0.2265, dtype=torch.float64), tensor(0.0867, dtype=torch.float64), tensor(4.9288e-18, dtype=torch.float64), tensor(3.2261e-18, dtype=torch.float64), tensor(0.3246, dtype=torch.float64), tensor(0.5567, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4181, dtype=torch.float64), tensor(0.9993, dtype=torch.float64), tensor(0.6505, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.07
  gsm8k: 0.293
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.227
  truthfulqa_gen: 0.087
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.325

LoRA Parameters:
  lora_r: (54,)
  lora_dropout: (0.09993012740129961,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (31.225536622143487,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  54
lora dropout:  0.09993012740129961
lora alpha:  31.225536622143487
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 30,855,168 || all params: 8,061,116,416 || trainable%: 0.3828
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7655, 'grad_norm': 0.6071689128875732, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.239901304244995, 'eval_runtime': 3.0229, 'eval_samples_per_second': 330.81, 'eval_steps_per_second': 20.841, 'epoch': 0.04}
{'loss': 1.3434, 'grad_norm': 0.4361253082752228, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5552852153778076, 'eval_runtime': 3.0185, 'eval_samples_per_second': 331.285, 'eval_steps_per_second': 20.871, 'epoch': 0.08}
{'loss': 1.0997, 'grad_norm': 0.33096739649772644, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4913476705551147, 'eval_runtime': 3.028, 'eval_samples_per_second': 330.255, 'eval_steps_per_second': 20.806, 'epoch': 0.12}
{'loss': 1.0577, 'grad_norm': 0.3268115520477295, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3259479999542236, 'eval_runtime': 3.0129, 'eval_samples_per_second': 331.904, 'eval_steps_per_second': 20.91, 'epoch': 0.16}
{'loss': 0.9804, 'grad_norm': 0.2862660884857178, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2579303979873657, 'eval_runtime': 3.0185, 'eval_samples_per_second': 331.288, 'eval_steps_per_second': 20.871, 'epoch': 0.2}
{'loss': 0.9876, 'grad_norm': 0.2952493727207184, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2541378736495972, 'eval_runtime': 3.02, 'eval_samples_per_second': 331.128, 'eval_steps_per_second': 20.861, 'epoch': 0.24}
{'loss': 0.9362, 'grad_norm': 0.2872461974620819, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2306134700775146, 'eval_runtime': 3.0443, 'eval_samples_per_second': 328.488, 'eval_steps_per_second': 20.695, 'epoch': 0.28}
{'loss': 0.949, 'grad_norm': 0.28197088837623596, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2307292222976685, 'eval_runtime': 3.0492, 'eval_samples_per_second': 327.959, 'eval_steps_per_second': 20.661, 'epoch': 0.32}
{'loss': 0.9416, 'grad_norm': 0.3104518949985504, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2189241647720337, 'eval_runtime': 3.0417, 'eval_samples_per_second': 328.765, 'eval_steps_per_second': 20.712, 'epoch': 0.36}
{'loss': 0.9249, 'grad_norm': 0.28782182931900024, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1985002756118774, 'eval_runtime': 3.0305, 'eval_samples_per_second': 329.979, 'eval_steps_per_second': 20.789, 'epoch': 0.4}
{'loss': 0.9357, 'grad_norm': 0.2741386890411377, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.19455885887146, 'eval_runtime': 3.0514, 'eval_samples_per_second': 327.718, 'eval_steps_per_second': 20.646, 'epoch': 0.44}
{'loss': 0.9041, 'grad_norm': 0.2863636910915375, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1785056591033936, 'eval_runtime': 3.0502, 'eval_samples_per_second': 327.851, 'eval_steps_per_second': 20.655, 'epoch': 0.48}
{'loss': 0.9126, 'grad_norm': 0.2935854494571686, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1701403856277466, 'eval_runtime': 3.0482, 'eval_samples_per_second': 328.06, 'eval_steps_per_second': 20.668, 'epoch': 0.52}
{'loss': 0.8845, 'grad_norm': 0.2973792850971222, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1502902507781982, 'eval_runtime': 3.0471, 'eval_samples_per_second': 328.176, 'eval_steps_per_second': 20.675, 'epoch': 0.56}
{'loss': 0.8718, 'grad_norm': 0.38807228207588196, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1276569366455078, 'eval_runtime': 3.0474, 'eval_samples_per_second': 328.146, 'eval_steps_per_second': 20.673, 'epoch': 0.6}
{'loss': 0.8632, 'grad_norm': 0.28432711958885193, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1005817651748657, 'eval_runtime': 3.0338, 'eval_samples_per_second': 329.614, 'eval_steps_per_second': 20.766, 'epoch': 0.64}
{'loss': 0.846, 'grad_norm': 0.3313962519168854, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.057402491569519, 'eval_runtime': 3.0348, 'eval_samples_per_second': 329.509, 'eval_steps_per_second': 20.759, 'epoch': 0.68}
{'loss': 0.8344, 'grad_norm': 0.3991529047489166, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0430244207382202, 'eval_runtime': 3.0238, 'eval_samples_per_second': 330.707, 'eval_steps_per_second': 20.835, 'epoch': 0.72}
{'loss': 0.8158, 'grad_norm': 0.34607118368148804, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0360690355300903, 'eval_runtime': 3.0258, 'eval_samples_per_second': 330.487, 'eval_steps_per_second': 20.821, 'epoch': 0.76}
{'loss': 0.802, 'grad_norm': 0.37062111496925354, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0390466451644897, 'eval_runtime': 3.0195, 'eval_samples_per_second': 331.181, 'eval_steps_per_second': 20.864, 'epoch': 0.8}
{'loss': 0.8222, 'grad_norm': 0.32763853669166565, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0353803634643555, 'eval_runtime': 3.0243, 'eval_samples_per_second': 330.66, 'eval_steps_per_second': 20.832, 'epoch': 0.84}
{'loss': 0.8022, 'grad_norm': 0.4526720345020294, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0335693359375, 'eval_runtime': 3.0215, 'eval_samples_per_second': 330.958, 'eval_steps_per_second': 20.85, 'epoch': 0.88}
{'loss': 0.8172, 'grad_norm': 0.32114988565444946, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0310925245285034, 'eval_runtime': 3.0228, 'eval_samples_per_second': 330.824, 'eval_steps_per_second': 20.842, 'epoch': 0.92}
{'loss': 0.7977, 'grad_norm': 0.3630322217941284, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0301978588104248, 'eval_runtime': 3.0277, 'eval_samples_per_second': 330.282, 'eval_steps_per_second': 20.808, 'epoch': 0.96}
{'loss': 0.798, 'grad_norm': 0.3739788830280304, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0274817943572998, 'eval_runtime': 3.0197, 'eval_samples_per_second': 331.159, 'eval_steps_per_second': 20.863, 'epoch': 1.0}
{'train_runtime': 278.6703, 'train_samples_per_second': 35.874, 'train_steps_per_second': 2.243, 'train_loss': 0.9877373962402344, 'epoch': 1.0}
train_results:  {'eval_loss': [2.239901304244995, 1.5552852153778076, 1.4913476705551147, 1.3259479999542236, 1.2579303979873657, 1.2541378736495972, 1.2306134700775146, 1.2307292222976685, 1.2189241647720337, 1.1985002756118774, 1.19455885887146, 1.1785056591033936, 1.1701403856277466, 1.1502902507781982, 1.1276569366455078, 1.1005817651748657, 1.057402491569519, 1.0430244207382202, 1.0360690355300903, 1.0390466451644897, 1.0353803634643555, 1.0335693359375, 1.0310925245285034, 1.0301978588104248, 1.0274817943572998], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.239901304244995, 1.5552852153778076, 1.4913476705551147, 1.3259479999542236, 1.2579303979873657, 1.2541378736495972, 1.2306134700775146, 1.2307292222976685, 1.2189241647720337, 1.1985002756118774, 1.19455885887146, 1.1785056591033936, 1.1701403856277466, 1.1502902507781982, 1.1276569366455078, 1.1005817651748657, 1.057402491569519, 1.0430244207382202, 1.0360690355300903, 1.0390466451644897, 1.0353803634643555, 1.0335693359375, 1.0310925245285034, 1.0301978588104248, 1.0274817943572998]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.8175196647644043
current iteration best possible eval_loss (full train run):  -1.0274817943572998
max eval_loss so far:  -1.022034764289856
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.3309 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.6710082996887259
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.4596620591694358
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.6705228734942374
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.8078606473340373
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.7473171229872677
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3238, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2766, dtype=torch.float64), tensor(0.1763, dtype=torch.float64), tensor(0.1709, dtype=torch.float64), tensor(0.0524, dtype=torch.float64), 8, 0, 0, 0, 1, 1, 2, 0.09435120808013689, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.3238, dtype=torch.float64), tensor(1.3877e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2766, dtype=torch.float64), tensor(0.1763, dtype=torch.float64), tensor(0.1709, dtype=torch.float64), tensor(0.0524, dtype=torch.float64), tensor(0.2400, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.9435, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.324
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.277
  wikitext: 0.176
  mmlu: 0.171
  arc_challenge: 0.052

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.09435120808013689,)
  num_layers_to_apply: (8,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  8
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.09435120808013689
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 589,824 || all params: 8,030,851,072 || trainable%: 0.0073
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.289, 'grad_norm': 3.069119930267334, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 4.581455230712891, 'eval_runtime': 2.8759, 'eval_samples_per_second': 347.721, 'eval_steps_per_second': 21.906, 'epoch': 0.04}
{'loss': 3.0883, 'grad_norm': 2.9225914478302, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.4966483116149902, 'eval_runtime': 2.8907, 'eval_samples_per_second': 345.932, 'eval_steps_per_second': 21.794, 'epoch': 0.08}
{'loss': 2.1156, 'grad_norm': 3.404071569442749, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.2117936611175537, 'eval_runtime': 2.879, 'eval_samples_per_second': 347.341, 'eval_steps_per_second': 21.882, 'epoch': 0.12}
{'loss': 1.6848, 'grad_norm': 2.174804925918579, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0622177124023438, 'eval_runtime': 2.888, 'eval_samples_per_second': 346.265, 'eval_steps_per_second': 21.815, 'epoch': 0.16}
{'loss': 1.5743, 'grad_norm': 1.1742264032363892, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9624406099319458, 'eval_runtime': 2.8808, 'eval_samples_per_second': 347.127, 'eval_steps_per_second': 21.869, 'epoch': 0.2}
{'loss': 1.5337, 'grad_norm': 1.5964497327804565, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8990497589111328, 'eval_runtime': 2.8801, 'eval_samples_per_second': 347.214, 'eval_steps_per_second': 21.874, 'epoch': 0.24}
{'loss': 1.3593, 'grad_norm': 0.8535717129707336, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9348284006118774, 'eval_runtime': 2.8814, 'eval_samples_per_second': 347.055, 'eval_steps_per_second': 21.864, 'epoch': 0.28}
{'loss': 1.3229, 'grad_norm': 0.6108412742614746, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9276189804077148, 'eval_runtime': 2.8888, 'eval_samples_per_second': 346.166, 'eval_steps_per_second': 21.808, 'epoch': 0.32}
{'loss': 1.3092, 'grad_norm': 0.6852779984474182, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7154196500778198, 'eval_runtime': 2.8873, 'eval_samples_per_second': 346.344, 'eval_steps_per_second': 21.82, 'epoch': 0.36}
{'loss': 1.3083, 'grad_norm': 0.7791613936424255, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.695605993270874, 'eval_runtime': 2.8899, 'eval_samples_per_second': 346.029, 'eval_steps_per_second': 21.8, 'epoch': 0.4}
{'loss': 1.2689, 'grad_norm': 0.8196290731430054, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7279645204544067, 'eval_runtime': 2.8961, 'eval_samples_per_second': 345.291, 'eval_steps_per_second': 21.753, 'epoch': 0.44}
{'loss': 1.2204, 'grad_norm': 0.6181392073631287, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6445133686065674, 'eval_runtime': 2.8942, 'eval_samples_per_second': 345.523, 'eval_steps_per_second': 21.768, 'epoch': 0.48}
{'loss': 1.1694, 'grad_norm': 0.7909053564071655, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8157069683074951, 'eval_runtime': 2.893, 'eval_samples_per_second': 345.657, 'eval_steps_per_second': 21.776, 'epoch': 0.52}
{'loss': 1.2702, 'grad_norm': 0.8857551217079163, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.750993251800537, 'eval_runtime': 2.8898, 'eval_samples_per_second': 346.045, 'eval_steps_per_second': 21.801, 'epoch': 0.56}
{'loss': 1.1792, 'grad_norm': 2.441774368286133, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.650971531867981, 'eval_runtime': 2.8921, 'eval_samples_per_second': 345.775, 'eval_steps_per_second': 21.784, 'epoch': 0.6}
{'loss': 1.1949, 'grad_norm': 0.6496096849441528, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6212000846862793, 'eval_runtime': 2.8963, 'eval_samples_per_second': 345.267, 'eval_steps_per_second': 21.752, 'epoch': 0.64}
{'loss': 1.2189, 'grad_norm': 0.7199251055717468, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7402153015136719, 'eval_runtime': 2.8935, 'eval_samples_per_second': 345.606, 'eval_steps_per_second': 21.773, 'epoch': 0.68}
{'loss': 1.2084, 'grad_norm': 0.6627723574638367, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7395139932632446, 'eval_runtime': 2.8962, 'eval_samples_per_second': 345.275, 'eval_steps_per_second': 21.752, 'epoch': 0.72}
{'loss': 1.199, 'grad_norm': 0.6053101420402527, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6530852317810059, 'eval_runtime': 2.884, 'eval_samples_per_second': 346.736, 'eval_steps_per_second': 21.844, 'epoch': 0.76}
{'loss': 1.165, 'grad_norm': 0.6844164729118347, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6393821239471436, 'eval_runtime': 2.8839, 'eval_samples_per_second': 346.751, 'eval_steps_per_second': 21.845, 'epoch': 0.8}
{'loss': 1.2263, 'grad_norm': 0.670195996761322, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.689300537109375, 'eval_runtime': 2.8752, 'eval_samples_per_second': 347.796, 'eval_steps_per_second': 21.911, 'epoch': 0.84}
{'loss': 1.2649, 'grad_norm': 0.9522021412849426, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7275408506393433, 'eval_runtime': 2.88, 'eval_samples_per_second': 347.222, 'eval_steps_per_second': 21.875, 'epoch': 0.88}
{'loss': 1.2426, 'grad_norm': 0.6600098013877869, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7365857362747192, 'eval_runtime': 2.8961, 'eval_samples_per_second': 345.296, 'eval_steps_per_second': 21.754, 'epoch': 0.92}
{'loss': 1.2019, 'grad_norm': 0.6390385627746582, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6772412061691284, 'eval_runtime': 2.8951, 'eval_samples_per_second': 345.41, 'eval_steps_per_second': 21.761, 'epoch': 0.96}
{'loss': 1.1879, 'grad_norm': 0.7493190169334412, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.695017695426941, 'eval_runtime': 2.8922, 'eval_samples_per_second': 345.761, 'eval_steps_per_second': 21.783, 'epoch': 1.0}
{'train_runtime': 233.9623, 'train_samples_per_second': 42.733, 'train_steps_per_second': 2.671, 'train_loss': 1.5121251037597656, 'epoch': 1.0}
train_results:  {'eval_loss': [4.581455230712891, 2.4966483116149902, 2.2117936611175537, 2.0622177124023438, 1.9624406099319458, 1.8990497589111328, 1.9348284006118774, 1.9276189804077148, 1.7154196500778198, 1.695605993270874, 1.7279645204544067, 1.6445133686065674, 1.8157069683074951, 1.750993251800537, 1.650971531867981, 1.6212000846862793, 1.7402153015136719, 1.7395139932632446, 1.6530852317810059, 1.6393821239471436, 1.689300537109375, 1.7275408506393433, 1.7365857362747192, 1.6772412061691284, 1.695017695426941], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [4.581455230712891, 2.4966483116149902, 2.2117936611175537, 2.0622177124023438, 1.9624406099319458, 1.8990497589111328, 1.9348284006118774, 1.9276189804077148, 1.7154196500778198, 1.695605993270874, 1.7279645204544067, 1.6445133686065674, 1.8157069683074951, 1.750993251800537, 1.650971531867981, 1.6212000846862793, 1.7402153015136719, 1.7395139932632446, 1.6530852317810059, 1.6393821239471436, 1.689300537109375, 1.7275408506393433, 1.7365857362747192, 1.6772412061691284, 1.695017695426941]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.676899790763855
current iteration best possible eval_loss (full train run):  -1.695017695426941
max eval_loss so far:  -1.022034764289856
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.2423 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.37791884601390224
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.3775131916726786
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.3721833177892191
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.3775133322862849
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.37751321870200294
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1925, dtype=torch.float64), 0, 0, tensor(0.2472, dtype=torch.float64), tensor(0.2341, dtype=torch.float64), 0, 0, tensor(0.3261, dtype=torch.float64), 29, 0, 0, 0, 1, 1, 21, 0.05887531104273486, 13.126751468842766, 0]
normalized proposed parameters for next round by BO: [tensor(3.0727e-18, dtype=torch.float64), tensor(0.1925, dtype=torch.float64), tensor(2.0213e-17, dtype=torch.float64), tensor(4.0276e-17, dtype=torch.float64), tensor(0.2472, dtype=torch.float64), tensor(0.2341, dtype=torch.float64), tensor(2.7164e-18, dtype=torch.float64), tensor(3.4142e-17, dtype=torch.float64), tensor(0.3261, dtype=torch.float64), tensor(0.9064, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1613, dtype=torch.float64), tensor(0.5888, dtype=torch.float64), tensor(0.2735, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.193
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.247
  truthfulqa_gen: 0.234
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.326

LoRA Parameters:
  lora_r: (21,)
  lora_dropout: (0.05887531104273486,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (13.126751468842766,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  21
lora dropout:  0.05887531104273486
lora alpha:  13.126751468842766
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 22,450,176 || all params: 8,052,711,424 || trainable%: 0.2788
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2113, 'grad_norm': 1.293020248413086, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.271014928817749, 'eval_runtime': 3.2652, 'eval_samples_per_second': 306.259, 'eval_steps_per_second': 19.294, 'epoch': 0.04}
{'loss': 1.31, 'grad_norm': 0.4099476933479309, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1641745567321777, 'eval_runtime': 3.2817, 'eval_samples_per_second': 304.717, 'eval_steps_per_second': 19.197, 'epoch': 0.08}
{'loss': 0.9263, 'grad_norm': 0.3318142890930176, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0863761901855469, 'eval_runtime': 3.2792, 'eval_samples_per_second': 304.951, 'eval_steps_per_second': 19.212, 'epoch': 0.12}
{'loss': 0.8695, 'grad_norm': 0.28411343693733215, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0632978677749634, 'eval_runtime': 3.2704, 'eval_samples_per_second': 305.771, 'eval_steps_per_second': 19.264, 'epoch': 0.16}
{'loss': 0.8816, 'grad_norm': 0.26104944944381714, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0524406433105469, 'eval_runtime': 3.2755, 'eval_samples_per_second': 305.302, 'eval_steps_per_second': 19.234, 'epoch': 0.2}
{'loss': 0.8704, 'grad_norm': 0.2984328269958496, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0443624258041382, 'eval_runtime': 3.2723, 'eval_samples_per_second': 305.596, 'eval_steps_per_second': 19.253, 'epoch': 0.24}
{'loss': 0.8415, 'grad_norm': 0.25249597430229187, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0434064865112305, 'eval_runtime': 3.2734, 'eval_samples_per_second': 305.497, 'eval_steps_per_second': 19.246, 'epoch': 0.28}
{'loss': 0.8583, 'grad_norm': 0.24094052612781525, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0403320789337158, 'eval_runtime': 3.2799, 'eval_samples_per_second': 304.884, 'eval_steps_per_second': 19.208, 'epoch': 0.32}
{'loss': 0.8043, 'grad_norm': 0.2449176162481308, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0359134674072266, 'eval_runtime': 3.2813, 'eval_samples_per_second': 304.753, 'eval_steps_per_second': 19.199, 'epoch': 0.36}
{'loss': 0.8508, 'grad_norm': 0.3120760917663574, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0253193378448486, 'eval_runtime': 3.3331, 'eval_samples_per_second': 300.02, 'eval_steps_per_second': 18.901, 'epoch': 0.4}
{'loss': 0.8145, 'grad_norm': 0.31582993268966675, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0223411321640015, 'eval_runtime': 3.2822, 'eval_samples_per_second': 304.671, 'eval_steps_per_second': 19.194, 'epoch': 0.44}
{'loss': 0.8312, 'grad_norm': 0.2640315294265747, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0288803577423096, 'eval_runtime': 3.2763, 'eval_samples_per_second': 305.218, 'eval_steps_per_second': 19.229, 'epoch': 0.48}
{'loss': 0.8117, 'grad_norm': 0.2816678583621979, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0286850929260254, 'eval_runtime': 3.2743, 'eval_samples_per_second': 305.41, 'eval_steps_per_second': 19.241, 'epoch': 0.52}
{'loss': 0.7875, 'grad_norm': 0.35917261242866516, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0295562744140625, 'eval_runtime': 3.2686, 'eval_samples_per_second': 305.941, 'eval_steps_per_second': 19.274, 'epoch': 0.56}
{'loss': 0.7707, 'grad_norm': 0.3702360987663269, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0183762311935425, 'eval_runtime': 3.2715, 'eval_samples_per_second': 305.67, 'eval_steps_per_second': 19.257, 'epoch': 0.6}
{'loss': 0.7422, 'grad_norm': 0.4519125819206238, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0205074548721313, 'eval_runtime': 3.2603, 'eval_samples_per_second': 306.716, 'eval_steps_per_second': 19.323, 'epoch': 0.64}
{'loss': 0.7501, 'grad_norm': 0.5081537961959839, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.03180730342865, 'eval_runtime': 3.2619, 'eval_samples_per_second': 306.566, 'eval_steps_per_second': 19.314, 'epoch': 0.68}
{'loss': 0.7678, 'grad_norm': 0.5027196407318115, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0169780254364014, 'eval_runtime': 3.2765, 'eval_samples_per_second': 305.199, 'eval_steps_per_second': 19.228, 'epoch': 0.72}
{'loss': 0.7415, 'grad_norm': 0.3547567129135132, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0239835977554321, 'eval_runtime': 3.2668, 'eval_samples_per_second': 306.108, 'eval_steps_per_second': 19.285, 'epoch': 0.76}
{'loss': 0.7297, 'grad_norm': 0.41031748056411743, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0173441171646118, 'eval_runtime': 3.263, 'eval_samples_per_second': 306.471, 'eval_steps_per_second': 19.308, 'epoch': 0.8}
{'loss': 0.7253, 'grad_norm': 0.410995215177536, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.020458459854126, 'eval_runtime': 3.2631, 'eval_samples_per_second': 306.461, 'eval_steps_per_second': 19.307, 'epoch': 0.84}
{'loss': 0.686, 'grad_norm': 0.5508363842964172, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.017324686050415, 'eval_runtime': 3.2657, 'eval_samples_per_second': 306.211, 'eval_steps_per_second': 19.291, 'epoch': 0.88}
{'loss': 0.7187, 'grad_norm': 0.4983022212982178, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0178921222686768, 'eval_runtime': 3.2616, 'eval_samples_per_second': 306.598, 'eval_steps_per_second': 19.316, 'epoch': 0.92}
{'loss': 0.6954, 'grad_norm': 0.4413491189479828, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0169202089309692, 'eval_runtime': 3.2613, 'eval_samples_per_second': 306.622, 'eval_steps_per_second': 19.317, 'epoch': 0.96}
{'loss': 0.7378, 'grad_norm': 0.5082137584686279, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0161330699920654, 'eval_runtime': 3.2685, 'eval_samples_per_second': 305.951, 'eval_steps_per_second': 19.275, 'epoch': 1.0}
{'train_runtime': 284.4899, 'train_samples_per_second': 35.144, 'train_steps_per_second': 2.197, 'train_loss': 0.9093585052490234, 'epoch': 1.0}
train_results:  {'eval_loss': [2.271014928817749, 1.1641745567321777, 1.0863761901855469, 1.0632978677749634, 1.0524406433105469, 1.0443624258041382, 1.0434064865112305, 1.0403320789337158, 1.0359134674072266, 1.0253193378448486, 1.0223411321640015, 1.0288803577423096, 1.0286850929260254, 1.0295562744140625, 1.0183762311935425, 1.0205074548721313, 1.03180730342865, 1.0169780254364014, 1.0239835977554321, 1.0173441171646118, 1.020458459854126, 1.017324686050415, 1.0178921222686768, 1.0169202089309692, 1.0161330699920654], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.271014928817749, 1.1641745567321777, 1.0863761901855469, 1.0632978677749634, 1.0524406433105469, 1.0443624258041382, 1.0434064865112305, 1.0403320789337158, 1.0359134674072266, 1.0253193378448486, 1.0223411321640015, 1.0288803577423096, 1.0286850929260254, 1.0295562744140625, 1.0183762311935425, 1.0205074548721313, 1.03180730342865, 1.0169780254364014, 1.0239835977554321, 1.0173441171646118, 1.020458459854126, 1.017324686050415, 1.0178921222686768, 1.0169202089309692, 1.0161330699920654]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.5180721282958984
current iteration best possible eval_loss (full train run):  -1.0161330699920654
max eval_loss so far:  -1.0161330699920654
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.6229 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.9279251048913628
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.9607856829950228
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.9223140672991543
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.9284140855539151
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.928243232613871
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1691, dtype=torch.float64), tensor(0.3543, dtype=torch.float64), tensor(0.2133, dtype=torch.float64), 0, tensor(0.1544, dtype=torch.float64), tensor(0.1090, dtype=torch.float64), 26, 0, 1, 0, 1, 1, 2, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(2.6563e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1691, dtype=torch.float64), tensor(0.3543, dtype=torch.float64), tensor(0.2133, dtype=torch.float64), tensor(1.2795e-16, dtype=torch.float64), tensor(0.1544, dtype=torch.float64), tensor(0.1090, dtype=torch.float64), tensor(0.8005, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.169
  triviaqa: 0.354
  truthfulqa_gen: 0.213
  wikitext: 0
  mmlu: 0.154
  arc_challenge: 0.109

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,183,168 || all params: 8,032,444,416 || trainable%: 0.0272
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0395, 'grad_norm': 6.931735515594482, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4448127746582031, 'eval_runtime': 3.2257, 'eval_samples_per_second': 310.01, 'eval_steps_per_second': 19.531, 'epoch': 0.04}
{'loss': 1.1344, 'grad_norm': 3.2758538722991943, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0986088514328003, 'eval_runtime': 3.2309, 'eval_samples_per_second': 309.512, 'eval_steps_per_second': 19.499, 'epoch': 0.08}
{'loss': 1.0674, 'grad_norm': 1.9181851148605347, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0677357912063599, 'eval_runtime': 3.2425, 'eval_samples_per_second': 308.408, 'eval_steps_per_second': 19.43, 'epoch': 0.12}
{'loss': 0.9836, 'grad_norm': 2.0207924842834473, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0388381481170654, 'eval_runtime': 3.256, 'eval_samples_per_second': 307.126, 'eval_steps_per_second': 19.349, 'epoch': 0.16}
{'loss': 1.032, 'grad_norm': 1.651245355606079, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.036566138267517, 'eval_runtime': 3.239, 'eval_samples_per_second': 308.741, 'eval_steps_per_second': 19.451, 'epoch': 0.2}
{'loss': 0.9683, 'grad_norm': 1.7869373559951782, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0450232028961182, 'eval_runtime': 3.2427, 'eval_samples_per_second': 308.384, 'eval_steps_per_second': 19.428, 'epoch': 0.24}
{'loss': 0.9294, 'grad_norm': 2.0233843326568604, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0341267585754395, 'eval_runtime': 3.2363, 'eval_samples_per_second': 308.992, 'eval_steps_per_second': 19.467, 'epoch': 0.28}
{'loss': 0.9571, 'grad_norm': 1.716729998588562, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0215332508087158, 'eval_runtime': 3.2547, 'eval_samples_per_second': 307.25, 'eval_steps_per_second': 19.357, 'epoch': 0.32}
{'loss': 0.9849, 'grad_norm': 1.66787588596344, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.032531499862671, 'eval_runtime': 3.2412, 'eval_samples_per_second': 308.53, 'eval_steps_per_second': 19.437, 'epoch': 0.36}
{'loss': 0.9522, 'grad_norm': 1.6553131341934204, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.021405577659607, 'eval_runtime': 3.2438, 'eval_samples_per_second': 308.276, 'eval_steps_per_second': 19.421, 'epoch': 0.4}
{'loss': 0.9505, 'grad_norm': 1.8712832927703857, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0272223949432373, 'eval_runtime': 3.2336, 'eval_samples_per_second': 309.254, 'eval_steps_per_second': 19.483, 'epoch': 0.44}
{'loss': 0.9322, 'grad_norm': 1.819579839706421, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0228873491287231, 'eval_runtime': 3.2397, 'eval_samples_per_second': 308.671, 'eval_steps_per_second': 19.446, 'epoch': 0.48}
{'loss': 0.9571, 'grad_norm': 1.8120384216308594, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.018190622329712, 'eval_runtime': 3.2355, 'eval_samples_per_second': 309.073, 'eval_steps_per_second': 19.472, 'epoch': 0.52}
{'loss': 0.9994, 'grad_norm': 1.9703842401504517, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0207589864730835, 'eval_runtime': 3.2352, 'eval_samples_per_second': 309.103, 'eval_steps_per_second': 19.473, 'epoch': 0.56}
{'loss': 0.9546, 'grad_norm': 1.8557835817337036, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0139377117156982, 'eval_runtime': 3.2347, 'eval_samples_per_second': 309.144, 'eval_steps_per_second': 19.476, 'epoch': 0.6}
{'loss': 0.8528, 'grad_norm': 1.7833888530731201, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0175418853759766, 'eval_runtime': 3.2347, 'eval_samples_per_second': 309.144, 'eval_steps_per_second': 19.476, 'epoch': 0.64}
{'loss': 0.904, 'grad_norm': 1.8792003393173218, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.01454758644104, 'eval_runtime': 3.2331, 'eval_samples_per_second': 309.305, 'eval_steps_per_second': 19.486, 'epoch': 0.68}
{'loss': 0.891, 'grad_norm': 1.9673473834991455, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0095553398132324, 'eval_runtime': 3.2347, 'eval_samples_per_second': 309.146, 'eval_steps_per_second': 19.476, 'epoch': 0.72}
{'loss': 0.9009, 'grad_norm': 1.8919711112976074, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.006581425666809, 'eval_runtime': 3.2363, 'eval_samples_per_second': 308.994, 'eval_steps_per_second': 19.467, 'epoch': 0.76}
{'loss': 0.9119, 'grad_norm': 1.94394850730896, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0088248252868652, 'eval_runtime': 3.2389, 'eval_samples_per_second': 308.751, 'eval_steps_per_second': 19.451, 'epoch': 0.8}
{'loss': 0.8867, 'grad_norm': 2.107145071029663, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0085065364837646, 'eval_runtime': 3.2543, 'eval_samples_per_second': 307.289, 'eval_steps_per_second': 19.359, 'epoch': 0.84}
{'loss': 0.8891, 'grad_norm': 1.6797232627868652, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0097522735595703, 'eval_runtime': 3.2523, 'eval_samples_per_second': 307.474, 'eval_steps_per_second': 19.371, 'epoch': 0.88}
{'loss': 0.8648, 'grad_norm': 1.6320511102676392, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.004093885421753, 'eval_runtime': 3.238, 'eval_samples_per_second': 308.83, 'eval_steps_per_second': 19.456, 'epoch': 0.92}
{'loss': 0.8453, 'grad_norm': 2.010272741317749, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0035287141799927, 'eval_runtime': 3.2417, 'eval_samples_per_second': 308.48, 'eval_steps_per_second': 19.434, 'epoch': 0.96}
{'loss': 0.8868, 'grad_norm': 2.2603204250335693, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.002569556236267, 'eval_runtime': 3.2425, 'eval_samples_per_second': 308.405, 'eval_steps_per_second': 19.43, 'epoch': 1.0}
{'train_runtime': 235.1989, 'train_samples_per_second': 42.504, 'train_steps_per_second': 2.657, 'train_loss': 1.027049252319336, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4448127746582031, 1.0986088514328003, 1.0677357912063599, 1.0388381481170654, 1.036566138267517, 1.0450232028961182, 1.0341267585754395, 1.0215332508087158, 1.032531499862671, 1.021405577659607, 1.0272223949432373, 1.0228873491287231, 1.018190622329712, 1.0207589864730835, 1.0139377117156982, 1.0175418853759766, 1.01454758644104, 1.0095553398132324, 1.006581425666809, 1.0088248252868652, 1.0085065364837646, 1.0097522735595703, 1.004093885421753, 1.0035287141799927, 1.002569556236267], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4448127746582031, 1.0986088514328003, 1.0677357912063599, 1.0388381481170654, 1.036566138267517, 1.0450232028961182, 1.0341267585754395, 1.0215332508087158, 1.032531499862671, 1.021405577659607, 1.0272223949432373, 1.0228873491287231, 1.018190622329712, 1.0207589864730835, 1.0139377117156982, 1.0175418853759766, 1.01454758644104, 1.0095553398132324, 1.006581425666809, 1.0088248252868652, 1.0085065364837646, 1.0097522735595703, 1.004093885421753, 1.0035287141799927, 1.002569556236267]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.034248113632202
current iteration best possible eval_loss (full train run):  -1.002569556236267
max eval_loss so far:  -1.002569556236267
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.4805 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -1.00686783692568
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -1.0257927871805772
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -1.0075861795728112
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -1.007591669298931
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -1.0310293202934355
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0617, dtype=torch.float64), tensor(0.0377, dtype=torch.float64), 0, 0, tensor(0.1280, dtype=torch.float64), tensor(0.2808, dtype=torch.float64), tensor(0.3448, dtype=torch.float64), tensor(0.1464, dtype=torch.float64), 0, 16, 0, 0, 0, 1, 1, 2, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0617, dtype=torch.float64), tensor(0.0377, dtype=torch.float64), tensor(1.5532e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1280, dtype=torch.float64), tensor(0.2808, dtype=torch.float64), tensor(0.3448, dtype=torch.float64), tensor(0.1464, dtype=torch.float64), tensor(0.0007, dtype=torch.float64), tensor(0.5146, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.062
  gsm8k: 0.038
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.128
  truthfulqa_gen: 0.281
  wikitext: 0.345
  mmlu: 0.146
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,179,648 || all params: 8,031,440,896 || trainable%: 0.0147
length of training data:  9990
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4635, 'grad_norm': 3.7910237312316895, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0551393032073975, 'eval_runtime': 2.9876, 'eval_samples_per_second': 334.718, 'eval_steps_per_second': 21.087, 'epoch': 0.04}
{'loss': 1.8146, 'grad_norm': 2.3240444660186768, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2830935716629028, 'eval_runtime': 2.997, 'eval_samples_per_second': 333.666, 'eval_steps_per_second': 21.021, 'epoch': 0.08}
{'loss': 1.5425, 'grad_norm': 2.1533424854278564, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1857726573944092, 'eval_runtime': 2.9882, 'eval_samples_per_second': 334.653, 'eval_steps_per_second': 21.083, 'epoch': 0.12}
{'loss': 1.3673, 'grad_norm': 1.9811512231826782, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1924045085906982, 'eval_runtime': 2.9997, 'eval_samples_per_second': 333.366, 'eval_steps_per_second': 21.002, 'epoch': 0.16}
{'loss': 1.3767, 'grad_norm': 2.2391130924224854, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1747190952301025, 'eval_runtime': 3.0009, 'eval_samples_per_second': 333.236, 'eval_steps_per_second': 20.994, 'epoch': 0.2}
{'loss': 1.4359, 'grad_norm': 1.8382939100265503, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1639306545257568, 'eval_runtime': 3.0118, 'eval_samples_per_second': 332.025, 'eval_steps_per_second': 20.918, 'epoch': 0.24}
{'loss': 1.3705, 'grad_norm': 2.1933798789978027, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.150692343711853, 'eval_runtime': 3.007, 'eval_samples_per_second': 332.557, 'eval_steps_per_second': 20.951, 'epoch': 0.28}
{'loss': 1.3888, 'grad_norm': 2.068789482116699, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1647921800613403, 'eval_runtime': 3.0133, 'eval_samples_per_second': 331.859, 'eval_steps_per_second': 20.907, 'epoch': 0.32}
{'loss': 1.3278, 'grad_norm': 2.062023162841797, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1377058029174805, 'eval_runtime': 3.01, 'eval_samples_per_second': 332.229, 'eval_steps_per_second': 20.93, 'epoch': 0.36}
{'loss': 1.4055, 'grad_norm': 1.6028876304626465, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1517800092697144, 'eval_runtime': 3.0114, 'eval_samples_per_second': 332.067, 'eval_steps_per_second': 20.92, 'epoch': 0.4}
{'loss': 1.3104, 'grad_norm': 1.521259069442749, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1472902297973633, 'eval_runtime': 3.0119, 'eval_samples_per_second': 332.011, 'eval_steps_per_second': 20.917, 'epoch': 0.44}
{'loss': 1.3688, 'grad_norm': 1.790242314338684, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1389954090118408, 'eval_runtime': 3.0153, 'eval_samples_per_second': 331.645, 'eval_steps_per_second': 20.894, 'epoch': 0.48}
{'loss': 1.3404, 'grad_norm': 2.1193220615386963, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.13760244846344, 'eval_runtime': 3.0126, 'eval_samples_per_second': 331.94, 'eval_steps_per_second': 20.912, 'epoch': 0.52}
{'loss': 1.244, 'grad_norm': 2.151860237121582, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.12655770778656, 'eval_runtime': 3.0226, 'eval_samples_per_second': 330.844, 'eval_steps_per_second': 20.843, 'epoch': 0.56}
{'loss': 1.3072, 'grad_norm': 2.1351757049560547, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1127477884292603, 'eval_runtime': 3.0292, 'eval_samples_per_second': 330.116, 'eval_steps_per_second': 20.797, 'epoch': 0.6}
{'loss': 1.2884, 'grad_norm': 2.1767613887786865, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1285958290100098, 'eval_runtime': 3.0319, 'eval_samples_per_second': 329.828, 'eval_steps_per_second': 20.779, 'epoch': 0.64}
{'loss': 1.3472, 'grad_norm': 1.7966766357421875, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1204280853271484, 'eval_runtime': 3.0191, 'eval_samples_per_second': 331.229, 'eval_steps_per_second': 20.867, 'epoch': 0.68}
{'loss': 1.3841, 'grad_norm': 1.8094233274459839, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1256595849990845, 'eval_runtime': 3.0179, 'eval_samples_per_second': 331.355, 'eval_steps_per_second': 20.875, 'epoch': 0.72}
{'loss': 1.3006, 'grad_norm': 1.818125605583191, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1197223663330078, 'eval_runtime': 3.0206, 'eval_samples_per_second': 331.064, 'eval_steps_per_second': 20.857, 'epoch': 0.76}
{'loss': 1.3032, 'grad_norm': 2.074965715408325, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1201943159103394, 'eval_runtime': 3.0111, 'eval_samples_per_second': 332.103, 'eval_steps_per_second': 20.922, 'epoch': 0.8}
{'loss': 1.3215, 'grad_norm': 1.9346575736999512, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1173468828201294, 'eval_runtime': 3.0054, 'eval_samples_per_second': 332.737, 'eval_steps_per_second': 20.962, 'epoch': 0.84}
{'loss': 1.253, 'grad_norm': 1.5779047012329102, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1162497997283936, 'eval_runtime': 3.0017, 'eval_samples_per_second': 333.139, 'eval_steps_per_second': 20.988, 'epoch': 0.88}
{'loss': 1.2754, 'grad_norm': 1.9893583059310913, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1179473400115967, 'eval_runtime': 3.006, 'eval_samples_per_second': 332.665, 'eval_steps_per_second': 20.958, 'epoch': 0.92}
{'loss': 1.2367, 'grad_norm': 1.6564795970916748, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1166809797286987, 'eval_runtime': 3.0021, 'eval_samples_per_second': 333.1, 'eval_steps_per_second': 20.985, 'epoch': 0.96}
{'loss': 1.1524, 'grad_norm': 6.046329021453857, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1149094104766846, 'eval_runtime': 2.998, 'eval_samples_per_second': 333.553, 'eval_steps_per_second': 21.014, 'epoch': 1.0}
{'train_runtime': 216.9949, 'train_samples_per_second': 46.038, 'train_steps_per_second': 2.88, 'train_loss': 1.4370455200195313, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0551393032073975, 1.2830935716629028, 1.1857726573944092, 1.1924045085906982, 1.1747190952301025, 1.1639306545257568, 1.150692343711853, 1.1647921800613403, 1.1377058029174805, 1.1517800092697144, 1.1472902297973633, 1.1389954090118408, 1.13760244846344, 1.12655770778656, 1.1127477884292603, 1.1285958290100098, 1.1204280853271484, 1.1256595849990845, 1.1197223663330078, 1.1201943159103394, 1.1173468828201294, 1.1162497997283936, 1.1179473400115967, 1.1166809797286987, 1.1149094104766846], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0551393032073975, 1.2830935716629028, 1.1857726573944092, 1.1924045085906982, 1.1747190952301025, 1.1639306545257568, 1.150692343711853, 1.1647921800613403, 1.1377058029174805, 1.1517800092697144, 1.1472902297973633, 1.1389954090118408, 1.13760244846344, 1.12655770778656, 1.1127477884292603, 1.1285958290100098, 1.1204280853271484, 1.1256595849990845, 1.1197223663330078, 1.1201943159103394, 1.1173468828201294, 1.1162497997283936, 1.1179473400115967, 1.1166809797286987, 1.1149094104766846]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.8127351999282837
current iteration best possible eval_loss (full train run):  -1.1149094104766846
max eval_loss so far:  -1.002569556236267
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.2932 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -1.4233148633990624
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -1.413407299949102
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -1.4232410306818304
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -1.2431243159284349
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -1.4203478429376002
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1787, dtype=torch.float64), tensor(0.1100, dtype=torch.float64), 0, tensor(0.1534, dtype=torch.float64), tensor(0.1289, dtype=torch.float64), tensor(0.1844, dtype=torch.float64), tensor(0.0139, dtype=torch.float64), tensor(0.1462, dtype=torch.float64), tensor(0.0762, dtype=torch.float64), 12, 0, 1, 0, 0, 1, 58, 0.0701835485681922, 33.126831708357024, 1]
normalized proposed parameters for next round by BO: [tensor(0.1787, dtype=torch.float64), tensor(0.1100, dtype=torch.float64), tensor(0.0083, dtype=torch.float64), tensor(0.1534, dtype=torch.float64), tensor(0.1289, dtype=torch.float64), tensor(0.1844, dtype=torch.float64), tensor(0.0139, dtype=torch.float64), tensor(0.1462, dtype=torch.float64), tensor(0.0762, dtype=torch.float64), tensor(0.3619, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4555, dtype=torch.float64), tensor(0.7018, dtype=torch.float64), tensor(0.6901, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.179
  gsm8k: 0.11
  rowan_hellaswag: 0
  sciq: 0.153
  triviaqa: 0.129
  truthfulqa_gen: 0.184
  wikitext: 0.014
  mmlu: 0.146
  arc_challenge: 0.076

LoRA Parameters:
  lora_r: (58,)
  lora_dropout: (0.0701835485681922,)
  num_layers_to_apply: (12,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (33.126831708357024,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  12
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  58
lora dropout:  0.0701835485681922
lora alpha:  33.126831708357024
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,392,192 || all params: 8,046,653,440 || trainable%: 0.2037
length of training data:  9913
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3594, 'grad_norm': 0.8782684803009033, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.414834499359131, 'eval_runtime': 2.887, 'eval_samples_per_second': 346.376, 'eval_steps_per_second': 21.822, 'epoch': 0.04}
{'loss': 1.7509, 'grad_norm': 0.6313668489456177, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9455418586730957, 'eval_runtime': 2.8912, 'eval_samples_per_second': 345.876, 'eval_steps_per_second': 21.79, 'epoch': 0.08}
{'loss': 1.3645, 'grad_norm': 0.47644901275634766, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.5611906051635742, 'eval_runtime': 2.9016, 'eval_samples_per_second': 344.64, 'eval_steps_per_second': 21.712, 'epoch': 0.12}
{'loss': 1.2654, 'grad_norm': 0.5064691305160522, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.4023127555847168, 'eval_runtime': 2.9141, 'eval_samples_per_second': 343.159, 'eval_steps_per_second': 21.619, 'epoch': 0.16}
{'loss': 1.1977, 'grad_norm': 0.4335840046405792, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 1.382169246673584, 'eval_runtime': 2.9177, 'eval_samples_per_second': 342.736, 'eval_steps_per_second': 21.592, 'epoch': 0.2}
{'loss': 1.155, 'grad_norm': 0.3283255696296692, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 1.2918448448181152, 'eval_runtime': 2.9177, 'eval_samples_per_second': 342.738, 'eval_steps_per_second': 21.593, 'epoch': 0.24}
{'loss': 1.1541, 'grad_norm': 0.3459140658378601, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 1.2832895517349243, 'eval_runtime': 2.943, 'eval_samples_per_second': 339.791, 'eval_steps_per_second': 21.407, 'epoch': 0.28}
{'loss': 1.1383, 'grad_norm': 0.32045724987983704, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 1.2646845579147339, 'eval_runtime': 2.9338, 'eval_samples_per_second': 340.854, 'eval_steps_per_second': 21.474, 'epoch': 0.32}
{'loss': 1.0937, 'grad_norm': 0.350070983171463, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 1.2726926803588867, 'eval_runtime': 2.9172, 'eval_samples_per_second': 342.794, 'eval_steps_per_second': 21.596, 'epoch': 0.36}
{'loss': 1.1477, 'grad_norm': 0.296421080827713, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 1.2438658475875854, 'eval_runtime': 2.9266, 'eval_samples_per_second': 341.699, 'eval_steps_per_second': 21.527, 'epoch': 0.4}
{'loss': 1.0564, 'grad_norm': 0.3299334645271301, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 1.2433542013168335, 'eval_runtime': 2.9088, 'eval_samples_per_second': 343.79, 'eval_steps_per_second': 21.659, 'epoch': 0.44}
{'loss': 1.0781, 'grad_norm': 0.29241296648979187, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 1.228083848953247, 'eval_runtime': 2.9027, 'eval_samples_per_second': 344.512, 'eval_steps_per_second': 21.704, 'epoch': 0.48}
{'loss': 1.0462, 'grad_norm': 0.2610441744327545, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 1.2128480672836304, 'eval_runtime': 2.9054, 'eval_samples_per_second': 344.185, 'eval_steps_per_second': 21.684, 'epoch': 0.52}
{'loss': 1.0256, 'grad_norm': 0.3805153965950012, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 1.1967880725860596, 'eval_runtime': 2.8929, 'eval_samples_per_second': 345.676, 'eval_steps_per_second': 21.778, 'epoch': 0.56}
{'loss': 1.0677, 'grad_norm': 0.31425562500953674, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 1.196596384048462, 'eval_runtime': 2.8942, 'eval_samples_per_second': 345.516, 'eval_steps_per_second': 21.768, 'epoch': 0.6}
{'loss': 1.0372, 'grad_norm': 0.2889786660671234, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 1.1780925989151, 'eval_runtime': 2.8998, 'eval_samples_per_second': 344.856, 'eval_steps_per_second': 21.726, 'epoch': 0.65}
{'loss': 1.0216, 'grad_norm': 0.3088981807231903, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 1.1703815460205078, 'eval_runtime': 2.9013, 'eval_samples_per_second': 344.669, 'eval_steps_per_second': 21.714, 'epoch': 0.69}
{'loss': 1.0055, 'grad_norm': 0.38471999764442444, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 1.151304841041565, 'eval_runtime': 2.8932, 'eval_samples_per_second': 345.635, 'eval_steps_per_second': 21.775, 'epoch': 0.73}
{'loss': 0.9757, 'grad_norm': 0.34047064185142517, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 1.1333152055740356, 'eval_runtime': 2.8915, 'eval_samples_per_second': 345.838, 'eval_steps_per_second': 21.788, 'epoch': 0.77}
{'loss': 0.9935, 'grad_norm': 0.34828200936317444, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 1.1184194087982178, 'eval_runtime': 2.8902, 'eval_samples_per_second': 345.993, 'eval_steps_per_second': 21.798, 'epoch': 0.81}
{'loss': 0.9903, 'grad_norm': 0.36869126558303833, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 1.0861601829528809, 'eval_runtime': 2.8943, 'eval_samples_per_second': 345.504, 'eval_steps_per_second': 21.767, 'epoch': 0.85}
{'loss': 0.9734, 'grad_norm': 0.3311987817287445, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 1.0834789276123047, 'eval_runtime': 2.9007, 'eval_samples_per_second': 344.746, 'eval_steps_per_second': 21.719, 'epoch': 0.89}
{'loss': 0.9985, 'grad_norm': 0.3377158045768738, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 1.0718623399734497, 'eval_runtime': 2.8855, 'eval_samples_per_second': 346.561, 'eval_steps_per_second': 21.833, 'epoch': 0.93}
{'loss': 0.962, 'grad_norm': 0.33414068818092346, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 1.0728083848953247, 'eval_runtime': 2.8841, 'eval_samples_per_second': 346.725, 'eval_steps_per_second': 21.844, 'epoch': 0.97}
{'train_runtime': 247.9605, 'train_samples_per_second': 39.978, 'train_steps_per_second': 2.5, 'train_loss': 1.1955913359119046, 'epoch': 1.0}
train_results:  {'eval_loss': [2.414834499359131, 1.9455418586730957, 1.5611906051635742, 1.4023127555847168, 1.382169246673584, 1.2918448448181152, 1.2832895517349243, 1.2646845579147339, 1.2726926803588867, 1.2438658475875854, 1.2433542013168335, 1.228083848953247, 1.2128480672836304, 1.1967880725860596, 1.196596384048462, 1.1780925989151, 1.1703815460205078, 1.151304841041565, 1.1333152055740356, 1.1184194087982178, 1.0861601829528809, 1.0834789276123047, 1.0718623399734497, 1.0728083848953247], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.414834499359131, 1.9455418586730957, 1.5611906051635742, 1.4023127555847168, 1.382169246673584, 1.2918448448181152, 1.2832895517349243, 1.2646845579147339, 1.2726926803588867, 1.2438658475875854, 1.2433542013168335, 1.228083848953247, 1.2128480672836304, 1.1967880725860596, 1.196596384048462, 1.1780925989151, 1.1703815460205078, 1.151304841041565, 1.1333152055740356, 1.1184194087982178, 1.0861601829528809, 1.0834789276123047, 1.0718623399734497, 1.0728083848953247]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.698012351989746
current iteration best possible eval_loss (full train run):  -1.0728083848953247
max eval_loss so far:  -1.002569556236267
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.7050 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9384865760803223, 0.07229626178741455, 0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6294164657592773, 0.5043690800666809, 0.12173128128051758, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.45333993434906006, 0.9395368695259094, 0.38172125816345215, 0.5809779763221741]  ‚Üí  acq = -0.7304988460547042
X = [0.25227582454681396, 0.03358107805252075, 0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.7713519334793091, 0.4265725612640381, 0.10601025819778442, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.3584800362586975, 0.16592669486999512, 0.6665055751800537, 0.5913098454475403]  ‚Üí  acq = -0.729022284556269
X = [0.019490361213684082, 0.33564668893814087, 0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.24613288044929504, 0.783804178237915, 0.05754297971725464, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6837789416313171, 0.5803513526916504, 0.10683952271938324, 0.5482228994369507]  ‚Üí  acq = -0.6906038904649381
X = [0.08164948225021362, 0.510372519493103, 0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3423933982849121, 0.566781759262085, 0.5619614124298096, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9544339776039124, 0.7185770869255066, 0.4849817454814911, 0.03939622640609741]  ‚Üí  acq = -0.7324559667465607
X = [0.9194858074188232, 0.7918861508369446, 0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5424157381057739, 0.4093313217163086, 0.6975671648979187, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.975213885307312, 0.7470415830612183, 0.2785099744796753, 0.9093899726867676]  ‚Üí  acq = -0.7307461292838118
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0758, dtype=torch.float64), tensor(0.2295, dtype=torch.float64), tensor(0.0904, dtype=torch.float64), 0, tensor(0.0960, dtype=torch.float64), tensor(0.3157, dtype=torch.float64), tensor(0.0806, dtype=torch.float64), tensor(0.1119, dtype=torch.float64), 0, 20, 0, 0, 1, 1, 1, 75, 0.03659911297533354, 13.596966742888617, 1]
normalized proposed parameters for next round by BO: [tensor(0.0758, dtype=torch.float64), tensor(0.2295, dtype=torch.float64), tensor(0.0904, dtype=torch.float64), tensor(8.8837e-18, dtype=torch.float64), tensor(0.0960, dtype=torch.float64), tensor(0.3157, dtype=torch.float64), tensor(0.0806, dtype=torch.float64), tensor(0.1119, dtype=torch.float64), tensor(4.7084e-18, dtype=torch.float64), tensor(0.6348, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5833, dtype=torch.float64), tensor(0.3660, dtype=torch.float64), tensor(0.2833, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.076
  gsm8k: 0.23
  rowan_hellaswag: 0.09
  sciq: 0
  triviaqa: 0.096
  truthfulqa_gen: 0.316
  wikitext: 0.081
  mmlu: 0.112
  arc_challenge: 0

LoRA Parameters:
  lora_r: (75,)
  lora_dropout: (0.03659911297533354,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (13.596966742888617,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  75
lora dropout:  0.03659911297533354
lora alpha:  13.596966742888617
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 82,944,000 || all params: 8,113,205,248 || trainable%: 1.0223
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1937, 'grad_norm': 0.991080105304718, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2046310901641846, 'eval_runtime': 3.3309, 'eval_samples_per_second': 300.217, 'eval_steps_per_second': 18.914, 'epoch': 0.04}
{'loss': 1.7297, 'grad_norm': 0.4202425479888916, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5139563083648682, 'eval_runtime': 3.3359, 'eval_samples_per_second': 299.773, 'eval_steps_per_second': 18.886, 'epoch': 0.08}
{'loss': 1.3644, 'grad_norm': 0.21413055062294006, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.321727991104126, 'eval_runtime': 3.3326, 'eval_samples_per_second': 300.063, 'eval_steps_per_second': 18.904, 'epoch': 0.12}
{'loss': 1.2469, 'grad_norm': 0.20651447772979736, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2527598142623901, 'eval_runtime': 3.3429, 'eval_samples_per_second': 299.141, 'eval_steps_per_second': 18.846, 'epoch': 0.16}
{'loss': 1.1954, 'grad_norm': 0.22190286219120026, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2419836521148682, 'eval_runtime': 3.3468, 'eval_samples_per_second': 298.791, 'eval_steps_per_second': 18.824, 'epoch': 0.2}
{'loss': 1.1872, 'grad_norm': 0.2491321712732315, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1754323244094849, 'eval_runtime': 3.3502, 'eval_samples_per_second': 298.487, 'eval_steps_per_second': 18.805, 'epoch': 0.24}
{'loss': 1.1951, 'grad_norm': 0.23493923246860504, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1416242122650146, 'eval_runtime': 3.3499, 'eval_samples_per_second': 298.517, 'eval_steps_per_second': 18.807, 'epoch': 0.28}
{'loss': 1.1238, 'grad_norm': 0.2830999195575714, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1081202030181885, 'eval_runtime': 3.353, 'eval_samples_per_second': 298.236, 'eval_steps_per_second': 18.789, 'epoch': 0.32}
{'loss': 1.1375, 'grad_norm': 0.19802311062812805, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1015191078186035, 'eval_runtime': 3.3483, 'eval_samples_per_second': 298.656, 'eval_steps_per_second': 18.815, 'epoch': 0.36}
{'loss': 1.1544, 'grad_norm': 0.19340936839580536, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0860358476638794, 'eval_runtime': 3.3621, 'eval_samples_per_second': 297.433, 'eval_steps_per_second': 18.738, 'epoch': 0.4}
{'loss': 1.104, 'grad_norm': 0.2567680776119232, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0986487865447998, 'eval_runtime': 3.3658, 'eval_samples_per_second': 297.103, 'eval_steps_per_second': 18.717, 'epoch': 0.44}
{'loss': 1.0455, 'grad_norm': 0.2624160051345825, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0958421230316162, 'eval_runtime': 3.3743, 'eval_samples_per_second': 296.361, 'eval_steps_per_second': 18.671, 'epoch': 0.48}
{'loss': 1.1169, 'grad_norm': 0.2074027806520462, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0875216722488403, 'eval_runtime': 3.3874, 'eval_samples_per_second': 295.212, 'eval_steps_per_second': 18.598, 'epoch': 0.52}
{'loss': 1.1102, 'grad_norm': 0.2199447602033615, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.076367974281311, 'eval_runtime': 3.3522, 'eval_samples_per_second': 298.312, 'eval_steps_per_second': 18.794, 'epoch': 0.56}
{'loss': 1.0464, 'grad_norm': 0.2453470677137375, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.066414713859558, 'eval_runtime': 3.3594, 'eval_samples_per_second': 297.668, 'eval_steps_per_second': 18.753, 'epoch': 0.6}
{'loss': 1.0468, 'grad_norm': 0.21865606307983398, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0743685960769653, 'eval_runtime': 3.355, 'eval_samples_per_second': 298.062, 'eval_steps_per_second': 18.778, 'epoch': 0.64}
{'loss': 1.1008, 'grad_norm': 0.1756950318813324, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0642743110656738, 'eval_runtime': 3.3545, 'eval_samples_per_second': 298.11, 'eval_steps_per_second': 18.781, 'epoch': 0.68}
{'loss': 1.1426, 'grad_norm': 0.242290660738945, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.066208004951477, 'eval_runtime': 3.3549, 'eval_samples_per_second': 298.07, 'eval_steps_per_second': 18.778, 'epoch': 0.72}
{'loss': 1.1486, 'grad_norm': 0.2264900803565979, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0617340803146362, 'eval_runtime': 3.3496, 'eval_samples_per_second': 298.545, 'eval_steps_per_second': 18.808, 'epoch': 0.76}
{'loss': 1.0684, 'grad_norm': 0.20508144795894623, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0616620779037476, 'eval_runtime': 3.3477, 'eval_samples_per_second': 298.714, 'eval_steps_per_second': 18.819, 'epoch': 0.8}
{'loss': 0.9922, 'grad_norm': 0.23370596766471863, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0658186674118042, 'eval_runtime': 3.3446, 'eval_samples_per_second': 298.986, 'eval_steps_per_second': 18.836, 'epoch': 0.84}
{'loss': 1.0201, 'grad_norm': 0.22597604990005493, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0535166263580322, 'eval_runtime': 3.3393, 'eval_samples_per_second': 299.465, 'eval_steps_per_second': 18.866, 'epoch': 0.88}
{'loss': 1.0213, 'grad_norm': 0.2249840497970581, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0507087707519531, 'eval_runtime': 3.3416, 'eval_samples_per_second': 299.257, 'eval_steps_per_second': 18.853, 'epoch': 0.92}
{'loss': 1.0492, 'grad_norm': 0.20164521038532257, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0518852472305298, 'eval_runtime': 3.342, 'eval_samples_per_second': 299.224, 'eval_steps_per_second': 18.851, 'epoch': 0.96}
{'loss': 1.0572, 'grad_norm': 0.20914945006370544, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0505154132843018, 'eval_runtime': 3.3425, 'eval_samples_per_second': 299.181, 'eval_steps_per_second': 18.848, 'epoch': 1.0}
{'train_runtime': 316.9947, 'train_samples_per_second': 31.537, 'train_steps_per_second': 1.972, 'train_loss': 1.2239254638671875, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2046310901641846, 1.5139563083648682, 1.321727991104126, 1.2527598142623901, 1.2419836521148682, 1.1754323244094849, 1.1416242122650146, 1.1081202030181885, 1.1015191078186035, 1.0860358476638794, 1.0986487865447998, 1.0958421230316162, 1.0875216722488403, 1.076367974281311, 1.066414713859558, 1.0743685960769653, 1.0642743110656738, 1.066208004951477, 1.0617340803146362, 1.0616620779037476, 1.0658186674118042, 1.0535166263580322, 1.0507087707519531, 1.0518852472305298, 1.0505154132843018], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2046310901641846, 1.5139563083648682, 1.321727991104126, 1.2527598142623901, 1.2419836521148682, 1.1754323244094849, 1.1416242122650146, 1.1081202030181885, 1.1015191078186035, 1.0860358476638794, 1.0986487865447998, 1.0958421230316162, 1.0875216722488403, 1.076367974281311, 1.066414713859558, 1.0743685960769653, 1.0642743110656738, 1.066208004951477, 1.0617340803146362, 1.0616620779037476, 1.0658186674118042, 1.0535166263580322, 1.0507087707519531, 1.0518852472305298, 1.0505154132843018]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.184635639190674
current iteration best possible eval_loss (full train run):  -1.0505154132843018
max eval_loss so far:  -1.002569556236267
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.7543 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.6079421002773306
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.6063547296899381
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.6076107107719753
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.6071942791630467
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.607186018592911
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2954, dtype=torch.float64), tensor(0.1171, dtype=torch.float64), 0, tensor(0.2534, dtype=torch.float64), 0, tensor(0.0838, dtype=torch.float64), tensor(0.1854, dtype=torch.float64), tensor(0.0649, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 6, 0.1, 47.99999999999997, 1]
normalized proposed parameters for next round by BO: [tensor(1.9735e-15, dtype=torch.float64), tensor(0.2954, dtype=torch.float64), tensor(0.1171, dtype=torch.float64), tensor(5.5140e-16, dtype=torch.float64), tensor(0.2534, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0838, dtype=torch.float64), tensor(0.1854, dtype=torch.float64), tensor(0.0649, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0430, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.295
  rowan_hellaswag: 0.117
  sciq: 0
  triviaqa: 0.253
  truthfulqa_gen: 0
  wikitext: 0.084
  mmlu: 0.185
  arc_challenge: 0.065

LoRA Parameters:
  lora_r: (6,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (47.99999999999997,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  6
lora dropout:  0.1
lora alpha:  47.99999999999997
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 8,060,928 || all params: 8,038,322,176 || trainable%: 0.1003
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4836, 'grad_norm': 1.711199164390564, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.486728310585022, 'eval_runtime': 3.331, 'eval_samples_per_second': 300.207, 'eval_steps_per_second': 18.913, 'epoch': 0.04}
{'loss': 1.37, 'grad_norm': 1.5479744672775269, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0625770092010498, 'eval_runtime': 3.3344, 'eval_samples_per_second': 299.901, 'eval_steps_per_second': 18.894, 'epoch': 0.08}
{'loss': 1.2993, 'grad_norm': 1.1534461975097656, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.053669810295105, 'eval_runtime': 3.3396, 'eval_samples_per_second': 299.439, 'eval_steps_per_second': 18.865, 'epoch': 0.12}
{'loss': 1.2226, 'grad_norm': 4.414500713348389, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0287532806396484, 'eval_runtime': 3.3375, 'eval_samples_per_second': 299.624, 'eval_steps_per_second': 18.876, 'epoch': 0.16}
{'loss': 1.1872, 'grad_norm': 0.8921651244163513, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.029419183731079, 'eval_runtime': 3.3546, 'eval_samples_per_second': 298.102, 'eval_steps_per_second': 18.78, 'epoch': 0.2}
{'loss': 1.183, 'grad_norm': 1.0893751382827759, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.019047498703003, 'eval_runtime': 3.3566, 'eval_samples_per_second': 297.917, 'eval_steps_per_second': 18.769, 'epoch': 0.24}
{'loss': 1.1705, 'grad_norm': 1.3449031114578247, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0169812440872192, 'eval_runtime': 3.3414, 'eval_samples_per_second': 299.279, 'eval_steps_per_second': 18.855, 'epoch': 0.28}
{'loss': 1.1361, 'grad_norm': 2.4800775051116943, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.014606237411499, 'eval_runtime': 3.3435, 'eval_samples_per_second': 299.09, 'eval_steps_per_second': 18.843, 'epoch': 0.32}
{'loss': 1.2291, 'grad_norm': 1.0364149808883667, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0207843780517578, 'eval_runtime': 3.3423, 'eval_samples_per_second': 299.195, 'eval_steps_per_second': 18.849, 'epoch': 0.36}
{'loss': 1.217, 'grad_norm': 1.283486247062683, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0028563737869263, 'eval_runtime': 3.3403, 'eval_samples_per_second': 299.37, 'eval_steps_per_second': 18.86, 'epoch': 0.4}
{'loss': 1.2381, 'grad_norm': 1.0806068181991577, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0085437297821045, 'eval_runtime': 3.3429, 'eval_samples_per_second': 299.139, 'eval_steps_per_second': 18.846, 'epoch': 0.44}
{'loss': 1.1646, 'grad_norm': 1.313010334968567, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0054486989974976, 'eval_runtime': 3.3482, 'eval_samples_per_second': 298.669, 'eval_steps_per_second': 18.816, 'epoch': 0.48}
{'loss': 1.155, 'grad_norm': 0.9694932103157043, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0177117586135864, 'eval_runtime': 3.3411, 'eval_samples_per_second': 299.306, 'eval_steps_per_second': 18.856, 'epoch': 0.52}
{'loss': 1.0969, 'grad_norm': 0.9962888956069946, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.004639983177185, 'eval_runtime': 3.3411, 'eval_samples_per_second': 299.299, 'eval_steps_per_second': 18.856, 'epoch': 0.56}
{'loss': 1.1228, 'grad_norm': 1.1369023323059082, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0044316053390503, 'eval_runtime': 3.3386, 'eval_samples_per_second': 299.53, 'eval_steps_per_second': 18.87, 'epoch': 0.6}
{'loss': 1.1341, 'grad_norm': 0.8954516649246216, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9991140365600586, 'eval_runtime': 3.3424, 'eval_samples_per_second': 299.184, 'eval_steps_per_second': 18.849, 'epoch': 0.64}
{'loss': 1.1593, 'grad_norm': 1.0051007270812988, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0002433061599731, 'eval_runtime': 3.3448, 'eval_samples_per_second': 298.973, 'eval_steps_per_second': 18.835, 'epoch': 0.68}
{'loss': 1.1451, 'grad_norm': 1.3126566410064697, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.006112813949585, 'eval_runtime': 3.347, 'eval_samples_per_second': 298.776, 'eval_steps_per_second': 18.823, 'epoch': 0.72}
{'loss': 1.1576, 'grad_norm': 1.1348763704299927, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9942687749862671, 'eval_runtime': 3.3463, 'eval_samples_per_second': 298.839, 'eval_steps_per_second': 18.827, 'epoch': 0.76}
{'loss': 1.1649, 'grad_norm': 1.0767066478729248, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9994319081306458, 'eval_runtime': 3.3452, 'eval_samples_per_second': 298.939, 'eval_steps_per_second': 18.833, 'epoch': 0.8}
{'loss': 1.1673, 'grad_norm': 0.880447506904602, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9936777353286743, 'eval_runtime': 3.3464, 'eval_samples_per_second': 298.828, 'eval_steps_per_second': 18.826, 'epoch': 0.84}
{'loss': 1.1209, 'grad_norm': 0.9201483726501465, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9931076765060425, 'eval_runtime': 3.3447, 'eval_samples_per_second': 298.981, 'eval_steps_per_second': 18.836, 'epoch': 0.88}
{'loss': 1.1112, 'grad_norm': 0.8922858238220215, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9931253790855408, 'eval_runtime': 3.3425, 'eval_samples_per_second': 299.174, 'eval_steps_per_second': 18.848, 'epoch': 0.92}
{'loss': 1.0911, 'grad_norm': 2.0151584148406982, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.991613507270813, 'eval_runtime': 3.3429, 'eval_samples_per_second': 299.145, 'eval_steps_per_second': 18.846, 'epoch': 0.96}
{'loss': 1.1076, 'grad_norm': 1.2002607583999634, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9920860528945923, 'eval_runtime': 3.346, 'eval_samples_per_second': 298.862, 'eval_steps_per_second': 18.828, 'epoch': 1.0}
{'train_runtime': 322.5937, 'train_samples_per_second': 30.986, 'train_steps_per_second': 1.937, 'train_loss': 1.2253992919921874, 'epoch': 1.0}
train_results:  {'eval_loss': [1.486728310585022, 1.0625770092010498, 1.053669810295105, 1.0287532806396484, 1.029419183731079, 1.019047498703003, 1.0169812440872192, 1.014606237411499, 1.0207843780517578, 1.0028563737869263, 1.0085437297821045, 1.0054486989974976, 1.0177117586135864, 1.004639983177185, 1.0044316053390503, 0.9991140365600586, 1.0002433061599731, 1.006112813949585, 0.9942687749862671, 0.9994319081306458, 0.9936777353286743, 0.9931076765060425, 0.9931253790855408, 0.991613507270813, 0.9920860528945923], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.486728310585022, 1.0625770092010498, 1.053669810295105, 1.0287532806396484, 1.029419183731079, 1.019047498703003, 1.0169812440872192, 1.014606237411499, 1.0207843780517578, 1.0028563737869263, 1.0085437297821045, 1.0054486989974976, 1.0177117586135864, 1.004639983177185, 1.0044316053390503, 0.9991140365600586, 1.0002433061599731, 1.006112813949585, 0.9942687749862671, 0.9994319081306458, 0.9936777353286743, 0.9931076765060425, 0.9931253790855408, 0.991613507270813, 0.9920860528945923]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.2610106468200684
current iteration best possible eval_loss (full train run):  -0.9920860528945923
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.6023 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -0.4997246692248598
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.4997246692248598
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -0.4997246692248598
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -0.4997246692248598
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -0.4997246692248598
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0277, dtype=torch.float64), tensor(0.5701, dtype=torch.float64), tensor(0.0342, dtype=torch.float64), tensor(0.1452, dtype=torch.float64), tensor(0.1061, dtype=torch.float64), 0, tensor(0.0326, dtype=torch.float64), 0, tensor(0.0841, dtype=torch.float64), 1, 0, 0, 1, 1, 1, 11, 0.1, 41.00748952346063, 1]
normalized proposed parameters for next round by BO: [tensor(0.0277, dtype=torch.float64), tensor(0.5701, dtype=torch.float64), tensor(0.0342, dtype=torch.float64), tensor(0.1452, dtype=torch.float64), tensor(0.1061, dtype=torch.float64), tensor(2.2316e-17, dtype=torch.float64), tensor(0.0326, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0841, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0860, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8543, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.028
  gsm8k: 0.57
  rowan_hellaswag: 0.034
  sciq: 0.145
  triviaqa: 0.106
  truthfulqa_gen: 0
  wikitext: 0.033
  mmlu: 0
  arc_challenge: 0.084

LoRA Parameters:
  lora_r: (11,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (41.00748952346063,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  11
lora dropout:  0.1
lora alpha:  41.00748952346063
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 608,256 || all params: 8,030,869,504 || trainable%: 0.0076
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9634, 'grad_norm': 2.91731333732605, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.755378484725952, 'eval_runtime': 2.8051, 'eval_samples_per_second': 356.49, 'eval_steps_per_second': 22.459, 'epoch': 0.04}
{'loss': 1.9985, 'grad_norm': 3.067164421081543, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.426539897918701, 'eval_runtime': 2.8096, 'eval_samples_per_second': 355.924, 'eval_steps_per_second': 22.423, 'epoch': 0.08}
{'loss': 1.5978, 'grad_norm': 3.1001901626586914, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.040435552597046, 'eval_runtime': 2.7967, 'eval_samples_per_second': 357.559, 'eval_steps_per_second': 22.526, 'epoch': 0.12}
{'loss': 1.4262, 'grad_norm': 1.9180339574813843, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9685810804367065, 'eval_runtime': 2.7882, 'eval_samples_per_second': 358.658, 'eval_steps_per_second': 22.595, 'epoch': 0.16}
{'loss': 1.3629, 'grad_norm': 1.7860379219055176, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8862353563308716, 'eval_runtime': 2.7891, 'eval_samples_per_second': 358.543, 'eval_steps_per_second': 22.588, 'epoch': 0.2}
{'loss': 1.2864, 'grad_norm': 2.1993911266326904, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7037761211395264, 'eval_runtime': 2.7949, 'eval_samples_per_second': 357.793, 'eval_steps_per_second': 22.541, 'epoch': 0.24}
{'loss': 1.2011, 'grad_norm': 1.7217276096343994, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7064052820205688, 'eval_runtime': 2.8005, 'eval_samples_per_second': 357.08, 'eval_steps_per_second': 22.496, 'epoch': 0.28}
{'loss': 1.1892, 'grad_norm': 1.4019293785095215, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5901004076004028, 'eval_runtime': 2.8007, 'eval_samples_per_second': 357.051, 'eval_steps_per_second': 22.494, 'epoch': 0.32}
{'loss': 1.1842, 'grad_norm': 1.4161341190338135, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4923666715621948, 'eval_runtime': 2.7929, 'eval_samples_per_second': 358.045, 'eval_steps_per_second': 22.557, 'epoch': 0.36}
{'loss': 1.1026, 'grad_norm': 1.8460997343063354, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4530166387557983, 'eval_runtime': 2.7946, 'eval_samples_per_second': 357.837, 'eval_steps_per_second': 22.544, 'epoch': 0.4}
{'loss': 1.1346, 'grad_norm': 1.5431346893310547, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.407141089439392, 'eval_runtime': 2.7934, 'eval_samples_per_second': 357.987, 'eval_steps_per_second': 22.553, 'epoch': 0.44}
{'loss': 1.1246, 'grad_norm': 1.8348939418792725, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4208993911743164, 'eval_runtime': 2.8124, 'eval_samples_per_second': 355.572, 'eval_steps_per_second': 22.401, 'epoch': 0.48}
{'loss': 1.1032, 'grad_norm': 1.2991169691085815, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4210667610168457, 'eval_runtime': 2.8362, 'eval_samples_per_second': 352.585, 'eval_steps_per_second': 22.213, 'epoch': 0.52}
{'loss': 1.0697, 'grad_norm': 1.6072404384613037, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3888781070709229, 'eval_runtime': 2.8274, 'eval_samples_per_second': 353.676, 'eval_steps_per_second': 22.282, 'epoch': 0.56}
{'loss': 1.1058, 'grad_norm': 1.8275758028030396, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3795850276947021, 'eval_runtime': 2.8146, 'eval_samples_per_second': 355.285, 'eval_steps_per_second': 22.383, 'epoch': 0.6}
{'loss': 1.1079, 'grad_norm': 1.33248770236969, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3852356672286987, 'eval_runtime': 2.8105, 'eval_samples_per_second': 355.811, 'eval_steps_per_second': 22.416, 'epoch': 0.64}
{'loss': 1.0868, 'grad_norm': 1.1486307382583618, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3635027408599854, 'eval_runtime': 2.812, 'eval_samples_per_second': 355.615, 'eval_steps_per_second': 22.404, 'epoch': 0.68}
{'loss': 1.0933, 'grad_norm': 1.166827917098999, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3293981552124023, 'eval_runtime': 2.8117, 'eval_samples_per_second': 355.66, 'eval_steps_per_second': 22.407, 'epoch': 0.72}
{'loss': 1.0299, 'grad_norm': 1.32958984375, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3257259130477905, 'eval_runtime': 2.8108, 'eval_samples_per_second': 355.768, 'eval_steps_per_second': 22.413, 'epoch': 0.76}
{'loss': 1.0714, 'grad_norm': 1.3661143779754639, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3276548385620117, 'eval_runtime': 2.8134, 'eval_samples_per_second': 355.443, 'eval_steps_per_second': 22.393, 'epoch': 0.8}
{'loss': 1.0614, 'grad_norm': 1.6818610429763794, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3185092210769653, 'eval_runtime': 2.8091, 'eval_samples_per_second': 355.986, 'eval_steps_per_second': 22.427, 'epoch': 0.84}
{'loss': 1.0613, 'grad_norm': 1.1732200384140015, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3093754053115845, 'eval_runtime': 2.8116, 'eval_samples_per_second': 355.665, 'eval_steps_per_second': 22.407, 'epoch': 0.88}
{'loss': 1.0237, 'grad_norm': 1.1690754890441895, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3176718950271606, 'eval_runtime': 2.8085, 'eval_samples_per_second': 356.058, 'eval_steps_per_second': 22.432, 'epoch': 0.92}
{'loss': 1.0738, 'grad_norm': 0.8789709210395813, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3040690422058105, 'eval_runtime': 2.8043, 'eval_samples_per_second': 356.591, 'eval_steps_per_second': 22.465, 'epoch': 0.96}
{'loss': 1.0188, 'grad_norm': 1.104516625404358, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3141698837280273, 'eval_runtime': 2.8045, 'eval_samples_per_second': 356.573, 'eval_steps_per_second': 22.464, 'epoch': 1.0}
{'train_runtime': 260.8554, 'train_samples_per_second': 38.324, 'train_steps_per_second': 2.396, 'train_loss': 1.25915029296875, 'epoch': 1.0}
train_results:  {'eval_loss': [3.755378484725952, 2.426539897918701, 2.040435552597046, 1.9685810804367065, 1.8862353563308716, 1.7037761211395264, 1.7064052820205688, 1.5901004076004028, 1.4923666715621948, 1.4530166387557983, 1.407141089439392, 1.4208993911743164, 1.4210667610168457, 1.3888781070709229, 1.3795850276947021, 1.3852356672286987, 1.3635027408599854, 1.3293981552124023, 1.3257259130477905, 1.3276548385620117, 1.3185092210769653, 1.3093754053115845, 1.3176718950271606, 1.3040690422058105, 1.3141698837280273], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.755378484725952, 2.426539897918701, 2.040435552597046, 1.9685810804367065, 1.8862353563308716, 1.7037761211395264, 1.7064052820205688, 1.5901004076004028, 1.4923666715621948, 1.4530166387557983, 1.407141089439392, 1.4208993911743164, 1.4210667610168457, 1.3888781070709229, 1.3795850276947021, 1.3852356672286987, 1.3635027408599854, 1.3293981552124023, 1.3257259130477905, 1.3276548385620117, 1.3185092210769653, 1.3093754053115845, 1.3176718950271606, 1.3040690422058105, 1.3141698837280273]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.5942556858062744
current iteration best possible eval_loss (full train run):  -1.3141698837280273
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.6192 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -0.6055238310754782
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -0.6055238310754782
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -0.6055238310754782
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.6055238310754782
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -0.6055238310754782
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3798, dtype=torch.float64), 0, 0, tensor(0.2551, dtype=torch.float64), tensor(0.0759, dtype=torch.float64), 0, 0, tensor(0.2892, dtype=torch.float64), 32, 0, 1, 0, 0, 1, 128, 1.4476073581606425e-18, 1.4800000190734894, 1]
normalized proposed parameters for next round by BO: [tensor(8.4048e-16, dtype=torch.float64), tensor(0.3798, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.6022e-15, dtype=torch.float64), tensor(0.2551, dtype=torch.float64), tensor(0.0759, dtype=torch.float64), tensor(6.7226e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2892, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.4476e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.38
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.255
  truthfulqa_gen: 0.076
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.289

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.4476073581606425e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (1.4800000190734894,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  128
lora dropout:  1.4476073581606425e-18
lora alpha:  1.4800000190734894
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3138, 'grad_norm': 0.17338554561138153, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 4.142961025238037, 'eval_runtime': 3.0565, 'eval_samples_per_second': 327.166, 'eval_steps_per_second': 20.611, 'epoch': 0.04}
{'loss': 2.0625, 'grad_norm': 0.08069804310798645, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.3461294174194336, 'eval_runtime': 3.0501, 'eval_samples_per_second': 327.863, 'eval_steps_per_second': 20.655, 'epoch': 0.08}
{'loss': 1.411, 'grad_norm': 0.04335782676935196, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.800021767616272, 'eval_runtime': 3.0507, 'eval_samples_per_second': 327.798, 'eval_steps_per_second': 20.651, 'epoch': 0.12}
{'loss': 1.181, 'grad_norm': 0.04775569215416908, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.580957293510437, 'eval_runtime': 3.0556, 'eval_samples_per_second': 327.267, 'eval_steps_per_second': 20.618, 'epoch': 0.16}
{'loss': 1.1306, 'grad_norm': 0.04969177022576332, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.516739845275879, 'eval_runtime': 3.0557, 'eval_samples_per_second': 327.253, 'eval_steps_per_second': 20.617, 'epoch': 0.2}
{'loss': 1.0743, 'grad_norm': 0.0500701367855072, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4614558219909668, 'eval_runtime': 3.0627, 'eval_samples_per_second': 326.507, 'eval_steps_per_second': 20.57, 'epoch': 0.24}
{'loss': 1.0655, 'grad_norm': 0.048548560589551926, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4354891777038574, 'eval_runtime': 3.0618, 'eval_samples_per_second': 326.603, 'eval_steps_per_second': 20.576, 'epoch': 0.28}
{'loss': 1.0499, 'grad_norm': 0.03852079436182976, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.409617304801941, 'eval_runtime': 3.072, 'eval_samples_per_second': 325.526, 'eval_steps_per_second': 20.508, 'epoch': 0.32}
{'loss': 1.0628, 'grad_norm': 0.049639008939266205, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.375976324081421, 'eval_runtime': 3.0765, 'eval_samples_per_second': 325.042, 'eval_steps_per_second': 20.478, 'epoch': 0.36}
{'loss': 1.0247, 'grad_norm': 0.04375263303518295, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3447396755218506, 'eval_runtime': 3.0834, 'eval_samples_per_second': 324.314, 'eval_steps_per_second': 20.432, 'epoch': 0.4}
{'loss': 0.9941, 'grad_norm': 0.045678336173295975, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2602128982543945, 'eval_runtime': 3.096, 'eval_samples_per_second': 322.996, 'eval_steps_per_second': 20.349, 'epoch': 0.44}
{'loss': 0.9514, 'grad_norm': 0.04073670133948326, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1798946857452393, 'eval_runtime': 3.0832, 'eval_samples_per_second': 324.34, 'eval_steps_per_second': 20.433, 'epoch': 0.48}
{'loss': 0.9272, 'grad_norm': 0.04391152411699295, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1249655485153198, 'eval_runtime': 3.0778, 'eval_samples_per_second': 324.909, 'eval_steps_per_second': 20.469, 'epoch': 0.52}
{'loss': 0.8972, 'grad_norm': 0.046551574021577835, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0898622274398804, 'eval_runtime': 3.0811, 'eval_samples_per_second': 324.562, 'eval_steps_per_second': 20.447, 'epoch': 0.56}
{'loss': 0.8918, 'grad_norm': 0.044102586805820465, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0567690134048462, 'eval_runtime': 3.078, 'eval_samples_per_second': 324.889, 'eval_steps_per_second': 20.468, 'epoch': 0.6}
{'loss': 0.8747, 'grad_norm': 0.05658547580242157, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0483388900756836, 'eval_runtime': 3.0804, 'eval_samples_per_second': 324.631, 'eval_steps_per_second': 20.452, 'epoch': 0.64}
{'loss': 0.8777, 'grad_norm': 0.05122339725494385, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.044579029083252, 'eval_runtime': 3.0798, 'eval_samples_per_second': 324.698, 'eval_steps_per_second': 20.456, 'epoch': 0.68}
{'loss': 0.8532, 'grad_norm': 0.046090297400951385, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0418486595153809, 'eval_runtime': 3.0758, 'eval_samples_per_second': 325.117, 'eval_steps_per_second': 20.482, 'epoch': 0.72}
{'loss': 0.8638, 'grad_norm': 0.043888889253139496, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0351183414459229, 'eval_runtime': 3.083, 'eval_samples_per_second': 324.363, 'eval_steps_per_second': 20.435, 'epoch': 0.76}
{'loss': 0.8464, 'grad_norm': 0.04278746247291565, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.040067434310913, 'eval_runtime': 3.0909, 'eval_samples_per_second': 323.535, 'eval_steps_per_second': 20.383, 'epoch': 0.8}
{'loss': 0.8633, 'grad_norm': 0.04265164956450462, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0335317850112915, 'eval_runtime': 3.0932, 'eval_samples_per_second': 323.294, 'eval_steps_per_second': 20.368, 'epoch': 0.84}
{'loss': 0.8612, 'grad_norm': 0.05079679936170578, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0391994714736938, 'eval_runtime': 3.0817, 'eval_samples_per_second': 324.501, 'eval_steps_per_second': 20.444, 'epoch': 0.88}
{'loss': 0.8821, 'grad_norm': 0.041050877422094345, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.036800503730774, 'eval_runtime': 3.082, 'eval_samples_per_second': 324.46, 'eval_steps_per_second': 20.441, 'epoch': 0.92}
{'loss': 0.8689, 'grad_norm': 0.05009832605719566, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0366688966751099, 'eval_runtime': 3.0721, 'eval_samples_per_second': 325.506, 'eval_steps_per_second': 20.507, 'epoch': 0.96}
{'loss': 0.8379, 'grad_norm': 0.04477715864777565, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0364645719528198, 'eval_runtime': 3.0803, 'eval_samples_per_second': 324.645, 'eval_steps_per_second': 20.453, 'epoch': 1.0}
{'train_runtime': 283.2716, 'train_samples_per_second': 35.295, 'train_steps_per_second': 2.206, 'train_loss': 1.1066893035888672, 'epoch': 1.0}
train_results:  {'eval_loss': [4.142961025238037, 2.3461294174194336, 1.800021767616272, 1.580957293510437, 1.516739845275879, 1.4614558219909668, 1.4354891777038574, 1.409617304801941, 1.375976324081421, 1.3447396755218506, 1.2602128982543945, 1.1798946857452393, 1.1249655485153198, 1.0898622274398804, 1.0567690134048462, 1.0483388900756836, 1.044579029083252, 1.0418486595153809, 1.0351183414459229, 1.040067434310913, 1.0335317850112915, 1.0391994714736938, 1.036800503730774, 1.0366688966751099, 1.0364645719528198], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [4.142961025238037, 2.3461294174194336, 1.800021767616272, 1.580957293510437, 1.516739845275879, 1.4614558219909668, 1.4354891777038574, 1.409617304801941, 1.375976324081421, 1.3447396755218506, 1.2602128982543945, 1.1798946857452393, 1.1249655485153198, 1.0898622274398804, 1.0567690134048462, 1.0483388900756836, 1.044579029083252, 1.0418486595153809, 1.0351183414459229, 1.040067434310913, 1.0335317850112915, 1.0391994714736938, 1.036800503730774, 1.0366688966751099, 1.0364645719528198]
current iteration observed (possibly low-fid or predicted) eval_loss:  -4.6048903465271
current iteration best possible eval_loss (full train run):  -1.0364645719528198
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.0670 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -0.5613653538791601
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -0.5613653538791601
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -0.5613653538791601
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -0.5613653538791601
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -0.5613653538791601
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1179, dtype=torch.float64), tensor(0.1729, dtype=torch.float64), tensor(0.0859, dtype=torch.float64), tensor(0.2020, dtype=torch.float64), tensor(0.0162, dtype=torch.float64), 0, tensor(0.2712, dtype=torch.float64), tensor(0.0193, dtype=torch.float64), tensor(0.1147, dtype=torch.float64), 14, 1, 0, 1, 1, 0, 72, 0.0727561254406501, 26.541880994595605, 1]
normalized proposed parameters for next round by BO: [tensor(0.1179, dtype=torch.float64), tensor(0.1729, dtype=torch.float64), tensor(0.0859, dtype=torch.float64), tensor(0.2020, dtype=torch.float64), tensor(0.0162, dtype=torch.float64), tensor(2.2969e-17, dtype=torch.float64), tensor(0.2712, dtype=torch.float64), tensor(0.0193, dtype=torch.float64), tensor(0.1147, dtype=torch.float64), tensor(0.4487, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5601, dtype=torch.float64), tensor(0.7276, dtype=torch.float64), tensor(0.5530, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.118
  gsm8k: 0.173
  rowan_hellaswag: 0.086
  sciq: 0.202
  triviaqa: 0.016
  truthfulqa_gen: 0
  wikitext: 0.271
  mmlu: 0.019
  arc_challenge: 0.115

LoRA Parameters:
  lora_r: (72,)
  lora_dropout: (0.0727561254406501,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (26.541880994595605,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  72
lora dropout:  0.0727561254406501
lora alpha:  26.541880994595605
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 45,416,448 || all params: 8,075,677,696 || trainable%: 0.5624
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1192, 'grad_norm': 0.8933281898498535, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.106265068054199, 'eval_runtime': 3.033, 'eval_samples_per_second': 329.711, 'eval_steps_per_second': 20.772, 'epoch': 0.04}
{'loss': 1.7838, 'grad_norm': 0.6290796995162964, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6881814002990723, 'eval_runtime': 2.994, 'eval_samples_per_second': 334.004, 'eval_steps_per_second': 21.042, 'epoch': 0.08}
{'loss': 1.4547, 'grad_norm': 0.3230971395969391, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4583256244659424, 'eval_runtime': 2.9961, 'eval_samples_per_second': 333.764, 'eval_steps_per_second': 21.027, 'epoch': 0.12}
{'loss': 1.3424, 'grad_norm': 0.46493208408355713, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4366142749786377, 'eval_runtime': 3.0029, 'eval_samples_per_second': 333.008, 'eval_steps_per_second': 20.98, 'epoch': 0.16}
{'loss': 1.3112, 'grad_norm': 0.28672096133232117, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2934192419052124, 'eval_runtime': 3.0001, 'eval_samples_per_second': 333.318, 'eval_steps_per_second': 20.999, 'epoch': 0.2}
{'loss': 1.3143, 'grad_norm': 0.5613663196563721, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2194428443908691, 'eval_runtime': 3.0088, 'eval_samples_per_second': 332.353, 'eval_steps_per_second': 20.938, 'epoch': 0.24}
{'loss': 1.2872, 'grad_norm': 0.26775655150413513, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2141714096069336, 'eval_runtime': 3.0065, 'eval_samples_per_second': 332.612, 'eval_steps_per_second': 20.955, 'epoch': 0.28}
{'loss': 1.2233, 'grad_norm': 0.2302531898021698, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.225269079208374, 'eval_runtime': 3.0063, 'eval_samples_per_second': 332.637, 'eval_steps_per_second': 20.956, 'epoch': 0.32}
{'loss': 1.2387, 'grad_norm': 0.3215884268283844, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1777883768081665, 'eval_runtime': 3.0169, 'eval_samples_per_second': 331.465, 'eval_steps_per_second': 20.882, 'epoch': 0.36}
{'loss': 1.2184, 'grad_norm': 0.24921007454395294, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1377264261245728, 'eval_runtime': 3.0076, 'eval_samples_per_second': 332.488, 'eval_steps_per_second': 20.947, 'epoch': 0.4}
{'loss': 1.322, 'grad_norm': 0.4032819867134094, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1299026012420654, 'eval_runtime': 3.0124, 'eval_samples_per_second': 331.957, 'eval_steps_per_second': 20.913, 'epoch': 0.44}
{'loss': 1.3439, 'grad_norm': 0.29387831687927246, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1395535469055176, 'eval_runtime': 3.021, 'eval_samples_per_second': 331.012, 'eval_steps_per_second': 20.854, 'epoch': 0.48}
{'loss': 1.2228, 'grad_norm': 0.28225553035736084, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1716034412384033, 'eval_runtime': 3.0181, 'eval_samples_per_second': 331.339, 'eval_steps_per_second': 20.874, 'epoch': 0.52}
{'loss': 1.1689, 'grad_norm': 0.34865081310272217, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1559807062149048, 'eval_runtime': 3.0142, 'eval_samples_per_second': 331.765, 'eval_steps_per_second': 20.901, 'epoch': 0.56}
{'loss': 1.2341, 'grad_norm': 0.2778500020503998, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1326687335968018, 'eval_runtime': 3.0185, 'eval_samples_per_second': 331.292, 'eval_steps_per_second': 20.871, 'epoch': 0.6}
{'loss': 1.2752, 'grad_norm': 0.2917834520339966, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1345324516296387, 'eval_runtime': 3.013, 'eval_samples_per_second': 331.9, 'eval_steps_per_second': 20.91, 'epoch': 0.64}
{'loss': 1.1928, 'grad_norm': 0.24664081633090973, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1268929243087769, 'eval_runtime': 3.0137, 'eval_samples_per_second': 331.822, 'eval_steps_per_second': 20.905, 'epoch': 0.68}
{'loss': 1.2656, 'grad_norm': 0.2047991156578064, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.13350248336792, 'eval_runtime': 3.0157, 'eval_samples_per_second': 331.593, 'eval_steps_per_second': 20.89, 'epoch': 0.72}
{'loss': 1.3248, 'grad_norm': 0.3704321086406708, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1237963438034058, 'eval_runtime': 3.0134, 'eval_samples_per_second': 331.85, 'eval_steps_per_second': 20.907, 'epoch': 0.76}
{'loss': 1.2432, 'grad_norm': 0.2936367690563202, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1431809663772583, 'eval_runtime': 3.022, 'eval_samples_per_second': 330.902, 'eval_steps_per_second': 20.847, 'epoch': 0.8}
{'loss': 1.2451, 'grad_norm': 0.23125848174095154, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1234050989151, 'eval_runtime': 3.015, 'eval_samples_per_second': 331.673, 'eval_steps_per_second': 20.895, 'epoch': 0.84}
{'loss': 1.2615, 'grad_norm': 0.2726864814758301, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1071215867996216, 'eval_runtime': 3.0146, 'eval_samples_per_second': 331.714, 'eval_steps_per_second': 20.898, 'epoch': 0.88}
{'loss': 1.2319, 'grad_norm': 0.2411409169435501, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1065552234649658, 'eval_runtime': 3.0195, 'eval_samples_per_second': 331.183, 'eval_steps_per_second': 20.865, 'epoch': 0.92}
{'loss': 1.2362, 'grad_norm': 0.2408955991268158, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1104648113250732, 'eval_runtime': 3.0132, 'eval_samples_per_second': 331.877, 'eval_steps_per_second': 20.908, 'epoch': 0.96}
{'loss': 1.2266, 'grad_norm': 0.3102467954158783, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1113262176513672, 'eval_runtime': 3.0195, 'eval_samples_per_second': 331.182, 'eval_steps_per_second': 20.864, 'epoch': 1.0}
{'train_runtime': 280.5688, 'train_samples_per_second': 35.628, 'train_steps_per_second': 2.228, 'train_loss': 1.3635174621582031, 'epoch': 1.0}
train_results:  {'eval_loss': [2.106265068054199, 1.6881814002990723, 1.4583256244659424, 1.4366142749786377, 1.2934192419052124, 1.2194428443908691, 1.2141714096069336, 1.225269079208374, 1.1777883768081665, 1.1377264261245728, 1.1299026012420654, 1.1395535469055176, 1.1716034412384033, 1.1559807062149048, 1.1326687335968018, 1.1345324516296387, 1.1268929243087769, 1.13350248336792, 1.1237963438034058, 1.1431809663772583, 1.1234050989151, 1.1071215867996216, 1.1065552234649658, 1.1104648113250732, 1.1113262176513672], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.106265068054199, 1.6881814002990723, 1.4583256244659424, 1.4366142749786377, 1.2934192419052124, 1.2194428443908691, 1.2141714096069336, 1.225269079208374, 1.1777883768081665, 1.1377264261245728, 1.1299026012420654, 1.1395535469055176, 1.1716034412384033, 1.1559807062149048, 1.1326687335968018, 1.1345324516296387, 1.1268929243087769, 1.13350248336792, 1.1237963438034058, 1.1431809663772583, 1.1234050989151, 1.1071215867996216, 1.1065552234649658, 1.1104648113250732, 1.1113262176513672]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.0058774948120117
current iteration best possible eval_loss (full train run):  -1.1113262176513672
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.3102 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -0.4081677975166915
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.4081677975120397
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -0.40816779751818566
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -0.408167797518463
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -0.4081677975184552
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0980, dtype=torch.float64), tensor(0.1787, dtype=torch.float64), tensor(0.0963, dtype=torch.float64), tensor(0.3151, dtype=torch.float64), tensor(0.0578, dtype=torch.float64), tensor(0.1336, dtype=torch.float64), tensor(0.0757, dtype=torch.float64), tensor(0.0448, dtype=torch.float64), 0, 21, 1, 0, 1, 1, 1, 57, 0.0873667913653274, 38.98837176613616, 0]
normalized proposed parameters for next round by BO: [tensor(0.0980, dtype=torch.float64), tensor(0.1787, dtype=torch.float64), tensor(0.0963, dtype=torch.float64), tensor(0.3151, dtype=torch.float64), tensor(0.0578, dtype=torch.float64), tensor(0.1336, dtype=torch.float64), tensor(0.0757, dtype=torch.float64), tensor(0.0448, dtype=torch.float64), tensor(6.0657e-19, dtype=torch.float64), tensor(0.6641, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4418, dtype=torch.float64), tensor(0.8737, dtype=torch.float64), tensor(0.8123, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.098
  gsm8k: 0.179
  rowan_hellaswag: 0.096
  sciq: 0.315
  triviaqa: 0.058
  truthfulqa_gen: 0.134
  wikitext: 0.076
  mmlu: 0.045
  arc_challenge: 0

LoRA Parameters:
  lora_r: (57,)
  lora_dropout: (0.0873667913653274,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (38.98837176613616,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  57
lora dropout:  0.0873667913653274
lora alpha:  38.98837176613616
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,995,136 || all params: 8,106,256,384 || trainable%: 0.9375
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9526, 'grad_norm': 0.9304580092430115, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7396059036254883, 'eval_runtime': 3.468, 'eval_samples_per_second': 288.35, 'eval_steps_per_second': 18.166, 'epoch': 0.04}
{'loss': 1.3971, 'grad_norm': 0.40335988998413086, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2565712928771973, 'eval_runtime': 3.4682, 'eval_samples_per_second': 288.336, 'eval_steps_per_second': 18.165, 'epoch': 0.08}
{'loss': 1.1941, 'grad_norm': 0.2948372960090637, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2207715511322021, 'eval_runtime': 3.4678, 'eval_samples_per_second': 288.367, 'eval_steps_per_second': 18.167, 'epoch': 0.12}
{'loss': 1.1569, 'grad_norm': 0.24836845695972443, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1867018938064575, 'eval_runtime': 3.4634, 'eval_samples_per_second': 288.731, 'eval_steps_per_second': 18.19, 'epoch': 0.16}
{'loss': 1.1573, 'grad_norm': 0.3075033724308014, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1878238916397095, 'eval_runtime': 3.4669, 'eval_samples_per_second': 288.443, 'eval_steps_per_second': 18.172, 'epoch': 0.2}
{'loss': 1.1443, 'grad_norm': 0.311802476644516, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1642307043075562, 'eval_runtime': 3.4668, 'eval_samples_per_second': 288.453, 'eval_steps_per_second': 18.173, 'epoch': 0.24}
{'loss': 1.1677, 'grad_norm': 0.21990720927715302, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1229280233383179, 'eval_runtime': 3.4641, 'eval_samples_per_second': 288.676, 'eval_steps_per_second': 18.187, 'epoch': 0.28}
{'loss': 1.112, 'grad_norm': 0.30669987201690674, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1248208284378052, 'eval_runtime': 3.4782, 'eval_samples_per_second': 287.504, 'eval_steps_per_second': 18.113, 'epoch': 0.32}
{'loss': 1.1158, 'grad_norm': 0.2774609923362732, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.118830919265747, 'eval_runtime': 3.5123, 'eval_samples_per_second': 284.714, 'eval_steps_per_second': 17.937, 'epoch': 0.36}
{'loss': 1.1632, 'grad_norm': 0.3234669268131256, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1188647747039795, 'eval_runtime': 3.4998, 'eval_samples_per_second': 285.728, 'eval_steps_per_second': 18.001, 'epoch': 0.4}
{'loss': 1.1514, 'grad_norm': 0.30642613768577576, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1106289625167847, 'eval_runtime': 3.4851, 'eval_samples_per_second': 286.935, 'eval_steps_per_second': 18.077, 'epoch': 0.44}
{'loss': 1.0588, 'grad_norm': 0.2935009300708771, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0973559617996216, 'eval_runtime': 3.5042, 'eval_samples_per_second': 285.369, 'eval_steps_per_second': 17.978, 'epoch': 0.48}
{'loss': 1.0843, 'grad_norm': 0.25526514649391174, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0927760601043701, 'eval_runtime': 3.4998, 'eval_samples_per_second': 285.733, 'eval_steps_per_second': 18.001, 'epoch': 0.52}
{'loss': 1.1168, 'grad_norm': 0.2895531952381134, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1081050634384155, 'eval_runtime': 3.4959, 'eval_samples_per_second': 286.053, 'eval_steps_per_second': 18.021, 'epoch': 0.56}
{'loss': 1.0898, 'grad_norm': 0.3196842074394226, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1064386367797852, 'eval_runtime': 3.5252, 'eval_samples_per_second': 283.673, 'eval_steps_per_second': 17.871, 'epoch': 0.6}
{'loss': 1.0751, 'grad_norm': 0.2505490481853485, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0850259065628052, 'eval_runtime': 3.4708, 'eval_samples_per_second': 288.121, 'eval_steps_per_second': 18.152, 'epoch': 0.64}
{'loss': 1.0433, 'grad_norm': 0.2857782542705536, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1029682159423828, 'eval_runtime': 3.4682, 'eval_samples_per_second': 288.336, 'eval_steps_per_second': 18.165, 'epoch': 0.68}
{'loss': 1.1695, 'grad_norm': 0.27343547344207764, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0767836570739746, 'eval_runtime': 3.4695, 'eval_samples_per_second': 288.224, 'eval_steps_per_second': 18.158, 'epoch': 0.72}
{'loss': 1.0965, 'grad_norm': 0.31813833117485046, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0888327360153198, 'eval_runtime': 3.4656, 'eval_samples_per_second': 288.547, 'eval_steps_per_second': 18.178, 'epoch': 0.76}
{'loss': 1.0797, 'grad_norm': 0.2847418785095215, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0843240022659302, 'eval_runtime': 3.4681, 'eval_samples_per_second': 288.344, 'eval_steps_per_second': 18.166, 'epoch': 0.8}
{'loss': 1.1096, 'grad_norm': 0.2655421495437622, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.080228328704834, 'eval_runtime': 3.4556, 'eval_samples_per_second': 289.381, 'eval_steps_per_second': 18.231, 'epoch': 0.84}
{'loss': 1.1231, 'grad_norm': 0.28823229670524597, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0818893909454346, 'eval_runtime': 3.4733, 'eval_samples_per_second': 287.914, 'eval_steps_per_second': 18.139, 'epoch': 0.88}
{'loss': 1.0879, 'grad_norm': 0.28752201795578003, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.081669807434082, 'eval_runtime': 3.4614, 'eval_samples_per_second': 288.898, 'eval_steps_per_second': 18.201, 'epoch': 0.92}
{'loss': 1.1342, 'grad_norm': 0.2788434326648712, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0827205181121826, 'eval_runtime': 3.4522, 'eval_samples_per_second': 289.667, 'eval_steps_per_second': 18.249, 'epoch': 0.96}
{'loss': 1.0438, 'grad_norm': 0.28641974925994873, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0843584537506104, 'eval_runtime': 3.4559, 'eval_samples_per_second': 289.358, 'eval_steps_per_second': 18.23, 'epoch': 1.0}
{'train_runtime': 296.2082, 'train_samples_per_second': 33.75, 'train_steps_per_second': 2.11, 'train_loss': 1.2009882934570313, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7396059036254883, 1.2565712928771973, 1.2207715511322021, 1.1867018938064575, 1.1878238916397095, 1.1642307043075562, 1.1229280233383179, 1.1248208284378052, 1.118830919265747, 1.1188647747039795, 1.1106289625167847, 1.0973559617996216, 1.0927760601043701, 1.1081050634384155, 1.1064386367797852, 1.0850259065628052, 1.1029682159423828, 1.0767836570739746, 1.0888327360153198, 1.0843240022659302, 1.080228328704834, 1.0818893909454346, 1.081669807434082, 1.0827205181121826, 1.0843584537506104], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7396059036254883, 1.2565712928771973, 1.2207715511322021, 1.1867018938064575, 1.1878238916397095, 1.1642307043075562, 1.1229280233383179, 1.1248208284378052, 1.118830919265747, 1.1188647747039795, 1.1106289625167847, 1.0973559617996216, 1.0927760601043701, 1.1081050634384155, 1.1064386367797852, 1.0850259065628052, 1.1029682159423828, 1.0767836570739746, 1.0888327360153198, 1.0843240022659302, 1.080228328704834, 1.0818893909454346, 1.081669807434082, 1.0827205181121826, 1.0843584537506104]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.929731845855713
current iteration best possible eval_loss (full train run):  -1.0843584537506104
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.8839 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.41669760823507795
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.43859154938332723
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.46063911610854746
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.4578529705879686
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -0.6267998601581337
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0103, dtype=torch.float64), tensor(0.1630, dtype=torch.float64), tensor(0.1924, dtype=torch.float64), tensor(0.1743, dtype=torch.float64), tensor(0.0359, dtype=torch.float64), tensor(0.0423, dtype=torch.float64), tensor(0.1418, dtype=torch.float64), tensor(0.1469, dtype=torch.float64), tensor(0.0932, dtype=torch.float64), 30, 0, 1, 0, 0, 1, 69, 0.055593431613738954, 36.50005180216975, 0]
normalized proposed parameters for next round by BO: [tensor(0.0103, dtype=torch.float64), tensor(0.1630, dtype=torch.float64), tensor(0.1924, dtype=torch.float64), tensor(0.1743, dtype=torch.float64), tensor(0.0359, dtype=torch.float64), tensor(0.0423, dtype=torch.float64), tensor(0.1418, dtype=torch.float64), tensor(0.1469, dtype=torch.float64), tensor(0.0932, dtype=torch.float64), tensor(0.9321, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5360, dtype=torch.float64), tensor(0.5559, dtype=torch.float64), tensor(0.7604, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.01
  gsm8k: 0.163
  rowan_hellaswag: 0.192
  sciq: 0.174
  triviaqa: 0.036
  truthfulqa_gen: 0.042
  wikitext: 0.142
  mmlu: 0.147
  arc_challenge: 0.093

LoRA Parameters:
  lora_r: (69,)
  lora_dropout: (0.055593431613738954,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (36.50005180216975,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  69
lora dropout:  0.055593431613738954
lora alpha:  36.50005180216975
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 48,752,640 || all params: 8,079,013,888 || trainable%: 0.6034
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0799, 'grad_norm': 0.5301010608673096, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.3252665996551514, 'eval_runtime': 3.1422, 'eval_samples_per_second': 318.248, 'eval_steps_per_second': 20.05, 'epoch': 0.04}
{'loss': 1.7974, 'grad_norm': 0.5884020924568176, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6781760454177856, 'eval_runtime': 3.1488, 'eval_samples_per_second': 317.583, 'eval_steps_per_second': 20.008, 'epoch': 0.08}
{'loss': 1.558, 'grad_norm': 0.342512845993042, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5044503211975098, 'eval_runtime': 3.1518, 'eval_samples_per_second': 317.276, 'eval_steps_per_second': 19.988, 'epoch': 0.12}
{'loss': 1.4123, 'grad_norm': 0.310926616191864, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3701775074005127, 'eval_runtime': 3.1579, 'eval_samples_per_second': 316.665, 'eval_steps_per_second': 19.95, 'epoch': 0.16}
{'loss': 1.4134, 'grad_norm': 0.23753216862678528, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2795294523239136, 'eval_runtime': 3.1839, 'eval_samples_per_second': 314.084, 'eval_steps_per_second': 19.787, 'epoch': 0.2}
{'loss': 1.4057, 'grad_norm': 0.2762084901332855, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.151113748550415, 'eval_runtime': 3.1644, 'eval_samples_per_second': 316.011, 'eval_steps_per_second': 19.909, 'epoch': 0.24}
{'loss': 1.3214, 'grad_norm': 0.25409793853759766, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.148952603340149, 'eval_runtime': 3.165, 'eval_samples_per_second': 315.961, 'eval_steps_per_second': 19.906, 'epoch': 0.28}
{'loss': 1.2938, 'grad_norm': 0.27283987402915955, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1176865100860596, 'eval_runtime': 3.155, 'eval_samples_per_second': 316.961, 'eval_steps_per_second': 19.969, 'epoch': 0.32}
{'loss': 1.2998, 'grad_norm': 0.2739924192428589, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1204502582550049, 'eval_runtime': 3.1496, 'eval_samples_per_second': 317.5, 'eval_steps_per_second': 20.002, 'epoch': 0.36}
{'loss': 1.3849, 'grad_norm': 0.24619053304195404, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1273772716522217, 'eval_runtime': 3.1517, 'eval_samples_per_second': 317.287, 'eval_steps_per_second': 19.989, 'epoch': 0.4}
{'loss': 1.2985, 'grad_norm': 0.2583446204662323, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1202778816223145, 'eval_runtime': 3.1485, 'eval_samples_per_second': 317.612, 'eval_steps_per_second': 20.01, 'epoch': 0.44}
{'loss': 1.3196, 'grad_norm': 0.22375917434692383, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.099600076675415, 'eval_runtime': 3.1485, 'eval_samples_per_second': 317.612, 'eval_steps_per_second': 20.01, 'epoch': 0.48}
{'loss': 1.3636, 'grad_norm': 0.254121869802475, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1131683588027954, 'eval_runtime': 3.146, 'eval_samples_per_second': 317.862, 'eval_steps_per_second': 20.025, 'epoch': 0.52}
{'loss': 1.2355, 'grad_norm': 0.2281663715839386, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1044094562530518, 'eval_runtime': 3.1462, 'eval_samples_per_second': 317.843, 'eval_steps_per_second': 20.024, 'epoch': 0.56}
{'loss': 1.2499, 'grad_norm': 0.2585026025772095, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1051872968673706, 'eval_runtime': 3.1464, 'eval_samples_per_second': 317.826, 'eval_steps_per_second': 20.023, 'epoch': 0.6}
{'loss': 1.3344, 'grad_norm': 0.25195348262786865, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0936884880065918, 'eval_runtime': 3.1466, 'eval_samples_per_second': 317.8, 'eval_steps_per_second': 20.021, 'epoch': 0.64}
{'loss': 1.2825, 'grad_norm': 0.22506295144557953, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0994009971618652, 'eval_runtime': 3.1499, 'eval_samples_per_second': 317.472, 'eval_steps_per_second': 20.001, 'epoch': 0.68}
{'loss': 1.2976, 'grad_norm': 0.21265901625156403, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0803769826889038, 'eval_runtime': 3.1625, 'eval_samples_per_second': 316.206, 'eval_steps_per_second': 19.921, 'epoch': 0.72}
{'loss': 1.2867, 'grad_norm': 0.30147939920425415, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0824822187423706, 'eval_runtime': 3.1831, 'eval_samples_per_second': 314.163, 'eval_steps_per_second': 19.792, 'epoch': 0.76}
{'loss': 1.213, 'grad_norm': 0.23352542519569397, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0841145515441895, 'eval_runtime': 3.1796, 'eval_samples_per_second': 314.509, 'eval_steps_per_second': 19.814, 'epoch': 0.8}
{'loss': 1.2975, 'grad_norm': 0.21496881544589996, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0818281173706055, 'eval_runtime': 3.1973, 'eval_samples_per_second': 312.764, 'eval_steps_per_second': 19.704, 'epoch': 0.84}
{'loss': 1.2646, 'grad_norm': 0.26695913076400757, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.085362195968628, 'eval_runtime': 3.2029, 'eval_samples_per_second': 312.221, 'eval_steps_per_second': 19.67, 'epoch': 0.88}
{'loss': 1.2988, 'grad_norm': 0.2663862109184265, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0850390195846558, 'eval_runtime': 3.1984, 'eval_samples_per_second': 312.658, 'eval_steps_per_second': 19.697, 'epoch': 0.92}
{'loss': 1.3414, 'grad_norm': 0.23162119090557098, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.080017328262329, 'eval_runtime': 3.2038, 'eval_samples_per_second': 312.133, 'eval_steps_per_second': 19.664, 'epoch': 0.96}
{'loss': 1.2797, 'grad_norm': 0.29741525650024414, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0774646997451782, 'eval_runtime': 3.2093, 'eval_samples_per_second': 311.593, 'eval_steps_per_second': 19.63, 'epoch': 1.0}
{'train_runtime': 293.7613, 'train_samples_per_second': 34.024, 'train_steps_per_second': 2.128, 'train_loss': 1.4131941680908202, 'epoch': 1.0}
train_results:  {'eval_loss': [2.3252665996551514, 1.6781760454177856, 1.5044503211975098, 1.3701775074005127, 1.2795294523239136, 1.151113748550415, 1.148952603340149, 1.1176865100860596, 1.1204502582550049, 1.1273772716522217, 1.1202778816223145, 1.099600076675415, 1.1131683588027954, 1.1044094562530518, 1.1051872968673706, 1.0936884880065918, 1.0994009971618652, 1.0803769826889038, 1.0824822187423706, 1.0841145515441895, 1.0818281173706055, 1.085362195968628, 1.0850390195846558, 1.080017328262329, 1.0774646997451782], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.3252665996551514, 1.6781760454177856, 1.5044503211975098, 1.3701775074005127, 1.2795294523239136, 1.151113748550415, 1.148952603340149, 1.1176865100860596, 1.1204502582550049, 1.1273772716522217, 1.1202778816223145, 1.099600076675415, 1.1131683588027954, 1.1044094562530518, 1.1051872968673706, 1.0936884880065918, 1.0994009971618652, 1.0803769826889038, 1.0824822187423706, 1.0841145515441895, 1.0818281173706055, 1.085362195968628, 1.0850390195846558, 1.080017328262329, 1.0774646997451782]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.436774253845215
current iteration best possible eval_loss (full train run):  -1.0774646997451782
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.4060 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.3003156109803158
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -0.4513354360170052
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.45539655800458156
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -0.4513296328454852
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -0.44723171715089616
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2841, dtype=torch.float64), 0, tensor(0.0213, dtype=torch.float64), tensor(0.2137, dtype=torch.float64), 0, tensor(0.1462, dtype=torch.float64), tensor(0.1653, dtype=torch.float64), 0, tensor(0.1693, dtype=torch.float64), 23, 1, 0, 1, 1, 1, 3, 1.9071165025763359e-19, 1.4800000190734899, 1]
normalized proposed parameters for next round by BO: [tensor(0.2841, dtype=torch.float64), tensor(2.5851e-17, dtype=torch.float64), tensor(0.0213, dtype=torch.float64), tensor(0.2137, dtype=torch.float64), tensor(1.3889e-17, dtype=torch.float64), tensor(0.1462, dtype=torch.float64), tensor(0.1653, dtype=torch.float64), tensor(5.1990e-18, dtype=torch.float64), tensor(0.1693, dtype=torch.float64), tensor(0.7320, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0201, dtype=torch.float64), tensor(1.9071e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.284
  gsm8k: 0
  rowan_hellaswag: 0.021
  sciq: 0.214
  triviaqa: 0
  truthfulqa_gen: 0.146
  wikitext: 0.165
  mmlu: 0
  arc_challenge: 0.169

LoRA Parameters:
  lora_r: (3,)
  lora_dropout: (1.9071165025763359e-19,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734899,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  3
lora dropout:  1.9071165025763359e-19
lora alpha:  1.4800000190734899
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 4,380,672 || all params: 8,034,641,920 || trainable%: 0.0545
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.3499, 'grad_norm': 2.052748918533325, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.4881598949432373, 'eval_runtime': 3.4714, 'eval_samples_per_second': 288.071, 'eval_steps_per_second': 18.148, 'epoch': 0.04}
{'loss': 2.3694, 'grad_norm': 3.470262050628662, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.4850106239318848, 'eval_runtime': 3.4563, 'eval_samples_per_second': 289.33, 'eval_steps_per_second': 18.228, 'epoch': 0.08}
{'loss': 1.4903, 'grad_norm': 0.9848312139511108, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1307859420776367, 'eval_runtime': 3.4469, 'eval_samples_per_second': 290.114, 'eval_steps_per_second': 18.277, 'epoch': 0.12}
{'loss': 1.4087, 'grad_norm': 0.6214703321456909, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0449650287628174, 'eval_runtime': 3.4418, 'eval_samples_per_second': 290.543, 'eval_steps_per_second': 18.304, 'epoch': 0.16}
{'loss': 1.3201, 'grad_norm': 0.33156734704971313, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8707420825958252, 'eval_runtime': 3.444, 'eval_samples_per_second': 290.361, 'eval_steps_per_second': 18.293, 'epoch': 0.2}
{'loss': 1.2727, 'grad_norm': 0.3247515857219696, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8255231380462646, 'eval_runtime': 3.4505, 'eval_samples_per_second': 289.814, 'eval_steps_per_second': 18.258, 'epoch': 0.24}
{'loss': 1.1887, 'grad_norm': 0.3261284828186035, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7085165977478027, 'eval_runtime': 3.4407, 'eval_samples_per_second': 290.64, 'eval_steps_per_second': 18.31, 'epoch': 0.28}
{'loss': 1.0808, 'grad_norm': 0.285758376121521, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7493749856948853, 'eval_runtime': 3.441, 'eval_samples_per_second': 290.612, 'eval_steps_per_second': 18.309, 'epoch': 0.32}
{'loss': 1.1094, 'grad_norm': 0.36175259947776794, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7352190017700195, 'eval_runtime': 3.4401, 'eval_samples_per_second': 290.693, 'eval_steps_per_second': 18.314, 'epoch': 0.36}
{'loss': 1.0331, 'grad_norm': 0.3391532003879547, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.737219214439392, 'eval_runtime': 3.4446, 'eval_samples_per_second': 290.308, 'eval_steps_per_second': 18.289, 'epoch': 0.4}
{'loss': 1.0883, 'grad_norm': 0.3884931802749634, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7349966764450073, 'eval_runtime': 3.4393, 'eval_samples_per_second': 290.754, 'eval_steps_per_second': 18.317, 'epoch': 0.44}
{'loss': 1.1033, 'grad_norm': 0.3539668619632721, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8430938720703125, 'eval_runtime': 3.4402, 'eval_samples_per_second': 290.678, 'eval_steps_per_second': 18.313, 'epoch': 0.48}
{'loss': 1.0049, 'grad_norm': 0.3305274248123169, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7595727443695068, 'eval_runtime': 3.4477, 'eval_samples_per_second': 290.045, 'eval_steps_per_second': 18.273, 'epoch': 0.52}
{'loss': 1.0383, 'grad_norm': 0.43196722865104675, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8185259103775024, 'eval_runtime': 3.4427, 'eval_samples_per_second': 290.471, 'eval_steps_per_second': 18.3, 'epoch': 0.56}
{'loss': 1.115, 'grad_norm': 0.47276103496551514, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.815988540649414, 'eval_runtime': 3.4405, 'eval_samples_per_second': 290.659, 'eval_steps_per_second': 18.312, 'epoch': 0.6}
{'loss': 0.9892, 'grad_norm': 0.4886683523654938, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8022226095199585, 'eval_runtime': 3.4458, 'eval_samples_per_second': 290.21, 'eval_steps_per_second': 18.283, 'epoch': 0.64}
{'loss': 1.046, 'grad_norm': 0.3598528504371643, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8243974447250366, 'eval_runtime': 3.4433, 'eval_samples_per_second': 290.418, 'eval_steps_per_second': 18.296, 'epoch': 0.68}
{'loss': 1.0771, 'grad_norm': 0.3512265086174011, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8231545686721802, 'eval_runtime': 3.4426, 'eval_samples_per_second': 290.474, 'eval_steps_per_second': 18.3, 'epoch': 0.72}
{'loss': 1.0374, 'grad_norm': 0.4597998261451721, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9219064712524414, 'eval_runtime': 3.4507, 'eval_samples_per_second': 289.797, 'eval_steps_per_second': 18.257, 'epoch': 0.76}
{'loss': 1.054, 'grad_norm': 0.4647006094455719, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8755888938903809, 'eval_runtime': 3.4392, 'eval_samples_per_second': 290.763, 'eval_steps_per_second': 18.318, 'epoch': 0.8}
{'loss': 1.0057, 'grad_norm': 0.35645541548728943, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8345556259155273, 'eval_runtime': 3.4445, 'eval_samples_per_second': 290.322, 'eval_steps_per_second': 18.29, 'epoch': 0.84}
{'loss': 1.1201, 'grad_norm': 0.33148375153541565, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8704355955123901, 'eval_runtime': 3.4409, 'eval_samples_per_second': 290.619, 'eval_steps_per_second': 18.309, 'epoch': 0.88}
{'loss': 1.0609, 'grad_norm': 0.4443317651748657, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8650342226028442, 'eval_runtime': 3.4419, 'eval_samples_per_second': 290.537, 'eval_steps_per_second': 18.304, 'epoch': 0.92}
{'loss': 1.0665, 'grad_norm': 0.43437811732292175, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8698726892471313, 'eval_runtime': 3.4405, 'eval_samples_per_second': 290.653, 'eval_steps_per_second': 18.311, 'epoch': 0.96}
{'loss': 1.0638, 'grad_norm': 0.4832053482532501, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8766571283340454, 'eval_runtime': 3.4425, 'eval_samples_per_second': 290.489, 'eval_steps_per_second': 18.301, 'epoch': 1.0}
{'train_runtime': 277.9858, 'train_samples_per_second': 35.966, 'train_steps_per_second': 2.248, 'train_loss': 1.2997441772460938, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4881598949432373, 2.4850106239318848, 2.1307859420776367, 2.0449650287628174, 1.8707420825958252, 1.8255231380462646, 1.7085165977478027, 1.7493749856948853, 1.7352190017700195, 1.737219214439392, 1.7349966764450073, 1.8430938720703125, 1.7595727443695068, 1.8185259103775024, 1.815988540649414, 1.8022226095199585, 1.8243974447250366, 1.8231545686721802, 1.9219064712524414, 1.8755888938903809, 1.8345556259155273, 1.8704355955123901, 1.8650342226028442, 1.8698726892471313, 1.8766571283340454], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.4881598949432373, 2.4850106239318848, 2.1307859420776367, 2.0449650287628174, 1.8707420825958252, 1.8255231380462646, 1.7085165977478027, 1.7493749856948853, 1.7352190017700195, 1.737219214439392, 1.7349966764450073, 1.8430938720703125, 1.7595727443695068, 1.8185259103775024, 1.815988540649414, 1.8022226095199585, 1.8243974447250366, 1.8231545686721802, 1.9219064712524414, 1.8755888938903809, 1.8345556259155273, 1.8704355955123901, 1.8650342226028442, 1.8698726892471313, 1.8766571283340454]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.138716697692871
current iteration best possible eval_loss (full train run):  -1.8766571283340454
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.6635 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -0.7284191271981564
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -0.7284191272200753
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -0.7462891538918606
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -0.7284191272200753
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.7284367778128955
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1378, dtype=torch.float64), tensor(0.1949, dtype=torch.float64), tensor(0.0927, dtype=torch.float64), tensor(0.0326, dtype=torch.float64), tensor(0.0951, dtype=torch.float64), tensor(0.1403, dtype=torch.float64), tensor(0.1068, dtype=torch.float64), tensor(0.1998, dtype=torch.float64), 0, 13, 0, 1, 0, 0, 1, 65, 0.05898316017872403, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.1378, dtype=torch.float64), tensor(0.1949, dtype=torch.float64), tensor(0.0927, dtype=torch.float64), tensor(0.0326, dtype=torch.float64), tensor(0.0951, dtype=torch.float64), tensor(0.1403, dtype=torch.float64), tensor(0.1068, dtype=torch.float64), tensor(0.1998, dtype=torch.float64), tensor(2.1681e-17, dtype=torch.float64), tensor(0.4094, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5065, dtype=torch.float64), tensor(0.5898, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.138
  gsm8k: 0.195
  rowan_hellaswag: 0.093
  sciq: 0.033
  triviaqa: 0.095
  truthfulqa_gen: 0.14
  wikitext: 0.107
  mmlu: 0.2
  arc_challenge: 0

LoRA Parameters:
  lora_r: (65,)
  lora_dropout: (0.05898316017872403,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  65
lora dropout:  0.05898316017872403
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 19,901,440 || all params: 8,050,162,688 || trainable%: 0.2472
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9999, 'grad_norm': 1.0376118421554565, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.330057382583618, 'eval_runtime': 2.9234, 'eval_samples_per_second': 342.067, 'eval_steps_per_second': 21.55, 'epoch': 0.04}
{'loss': 1.7892, 'grad_norm': 0.686283528804779, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7139723300933838, 'eval_runtime': 2.9344, 'eval_samples_per_second': 340.785, 'eval_steps_per_second': 21.469, 'epoch': 0.08}
{'loss': 1.4691, 'grad_norm': 0.4678630530834198, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5288838148117065, 'eval_runtime': 2.9177, 'eval_samples_per_second': 342.741, 'eval_steps_per_second': 21.593, 'epoch': 0.12}
{'loss': 1.4071, 'grad_norm': 0.4545783996582031, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.370103359222412, 'eval_runtime': 2.9204, 'eval_samples_per_second': 342.42, 'eval_steps_per_second': 21.572, 'epoch': 0.16}
{'loss': 1.3508, 'grad_norm': 0.37960106134414673, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3014118671417236, 'eval_runtime': 2.9195, 'eval_samples_per_second': 342.523, 'eval_steps_per_second': 21.579, 'epoch': 0.2}
{'loss': 1.3018, 'grad_norm': 0.39954087138175964, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.308342456817627, 'eval_runtime': 2.9266, 'eval_samples_per_second': 341.696, 'eval_steps_per_second': 21.527, 'epoch': 0.24}
{'loss': 1.3212, 'grad_norm': 0.34113818407058716, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.278533935546875, 'eval_runtime': 2.922, 'eval_samples_per_second': 342.226, 'eval_steps_per_second': 21.56, 'epoch': 0.28}
{'loss': 1.2949, 'grad_norm': 0.3495793342590332, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2678922414779663, 'eval_runtime': 2.923, 'eval_samples_per_second': 342.109, 'eval_steps_per_second': 21.553, 'epoch': 0.32}
{'loss': 1.3057, 'grad_norm': 0.37797805666923523, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2879855632781982, 'eval_runtime': 2.9269, 'eval_samples_per_second': 341.656, 'eval_steps_per_second': 21.524, 'epoch': 0.36}
{'loss': 1.2739, 'grad_norm': 0.2928948700428009, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2543277740478516, 'eval_runtime': 2.925, 'eval_samples_per_second': 341.884, 'eval_steps_per_second': 21.539, 'epoch': 0.4}
{'loss': 1.2704, 'grad_norm': 0.2984204888343811, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.229915738105774, 'eval_runtime': 2.9423, 'eval_samples_per_second': 339.865, 'eval_steps_per_second': 21.411, 'epoch': 0.44}
{'loss': 1.2799, 'grad_norm': 0.2719460725784302, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2527921199798584, 'eval_runtime': 2.9573, 'eval_samples_per_second': 338.143, 'eval_steps_per_second': 21.303, 'epoch': 0.48}
{'loss': 1.2661, 'grad_norm': 0.2917536497116089, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.212614893913269, 'eval_runtime': 2.9454, 'eval_samples_per_second': 339.513, 'eval_steps_per_second': 21.389, 'epoch': 0.52}
{'loss': 1.2591, 'grad_norm': 0.3522114157676697, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1968438625335693, 'eval_runtime': 2.9361, 'eval_samples_per_second': 340.59, 'eval_steps_per_second': 21.457, 'epoch': 0.56}
{'loss': 1.2623, 'grad_norm': 0.3058014214038849, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1616777181625366, 'eval_runtime': 2.9318, 'eval_samples_per_second': 341.089, 'eval_steps_per_second': 21.489, 'epoch': 0.6}
{'loss': 1.2405, 'grad_norm': 0.31026124954223633, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.128505825996399, 'eval_runtime': 2.9293, 'eval_samples_per_second': 341.383, 'eval_steps_per_second': 21.507, 'epoch': 0.64}
{'loss': 1.2778, 'grad_norm': 0.35044509172439575, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1214780807495117, 'eval_runtime': 2.9352, 'eval_samples_per_second': 340.691, 'eval_steps_per_second': 21.464, 'epoch': 0.68}
{'loss': 1.2468, 'grad_norm': 0.30653655529022217, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.110971450805664, 'eval_runtime': 2.9332, 'eval_samples_per_second': 340.926, 'eval_steps_per_second': 21.478, 'epoch': 0.72}
{'loss': 1.178, 'grad_norm': 0.4522331655025482, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0807498693466187, 'eval_runtime': 2.9275, 'eval_samples_per_second': 341.588, 'eval_steps_per_second': 21.52, 'epoch': 0.76}
{'loss': 1.2064, 'grad_norm': 0.30684736371040344, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0793418884277344, 'eval_runtime': 2.9341, 'eval_samples_per_second': 340.816, 'eval_steps_per_second': 21.471, 'epoch': 0.8}
{'loss': 1.2067, 'grad_norm': 0.31233513355255127, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0793598890304565, 'eval_runtime': 2.9778, 'eval_samples_per_second': 335.822, 'eval_steps_per_second': 21.157, 'epoch': 0.84}
{'loss': 1.147, 'grad_norm': 0.3526769280433655, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0631515979766846, 'eval_runtime': 2.9563, 'eval_samples_per_second': 338.264, 'eval_steps_per_second': 21.311, 'epoch': 0.88}
{'loss': 1.168, 'grad_norm': 0.34354400634765625, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0630717277526855, 'eval_runtime': 2.9578, 'eval_samples_per_second': 338.094, 'eval_steps_per_second': 21.3, 'epoch': 0.92}
{'loss': 1.2238, 'grad_norm': 0.325349360704422, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0687570571899414, 'eval_runtime': 2.9563, 'eval_samples_per_second': 338.263, 'eval_steps_per_second': 21.311, 'epoch': 0.96}
{'loss': 1.16, 'grad_norm': 0.37586843967437744, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0696929693222046, 'eval_runtime': 2.9466, 'eval_samples_per_second': 339.37, 'eval_steps_per_second': 21.38, 'epoch': 1.0}
{'train_runtime': 274.9393, 'train_samples_per_second': 36.353, 'train_steps_per_second': 2.273, 'train_loss': 1.3562535888671876, 'epoch': 1.0}
train_results:  {'eval_loss': [2.330057382583618, 1.7139723300933838, 1.5288838148117065, 1.370103359222412, 1.3014118671417236, 1.308342456817627, 1.278533935546875, 1.2678922414779663, 1.2879855632781982, 1.2543277740478516, 1.229915738105774, 1.2527921199798584, 1.212614893913269, 1.1968438625335693, 1.1616777181625366, 1.128505825996399, 1.1214780807495117, 1.110971450805664, 1.0807498693466187, 1.0793418884277344, 1.0793598890304565, 1.0631515979766846, 1.0630717277526855, 1.0687570571899414, 1.0696929693222046], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.330057382583618, 1.7139723300933838, 1.5288838148117065, 1.370103359222412, 1.3014118671417236, 1.308342456817627, 1.278533935546875, 1.2678922414779663, 1.2879855632781982, 1.2543277740478516, 1.229915738105774, 1.2527921199798584, 1.212614893913269, 1.1968438625335693, 1.1616777181625366, 1.128505825996399, 1.1214780807495117, 1.110971450805664, 1.0807498693466187, 1.0793418884277344, 1.0793598890304565, 1.0631515979766846, 1.0630717277526855, 1.0687570571899414, 1.0696929693222046]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.878386974334717
current iteration best possible eval_loss (full train run):  -1.0696929693222046
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871, -2.878386974334717]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.8267 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -0.4815794526155217
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -0.48157945256547086
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -0.48102025201553134
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -0.4815794526155217
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -0.4816063576455718
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0616, dtype=torch.float64), tensor(0.0296, dtype=torch.float64), tensor(0.1694, dtype=torch.float64), 0, tensor(0.4978, dtype=torch.float64), tensor(0.1372, dtype=torch.float64), tensor(0.0879, dtype=torch.float64), 0, 13, 0, 0, 1, 0, 1, 78, 2.741358050066029e-18, 26.386450441831606, 1]
normalized proposed parameters for next round by BO: [tensor(0.0070, dtype=torch.float64), tensor(0.0616, dtype=torch.float64), tensor(0.0296, dtype=torch.float64), tensor(0.1694, dtype=torch.float64), tensor(0.0094, dtype=torch.float64), tensor(0.4978, dtype=torch.float64), tensor(0.1372, dtype=torch.float64), tensor(0.0879, dtype=torch.float64), tensor(7.3498e-19, dtype=torch.float64), tensor(0.4151, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6102, dtype=torch.float64), tensor(2.7414e-17, dtype=torch.float64), tensor(0.5497, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.062
  rowan_hellaswag: 0.03
  sciq: 0.169
  triviaqa: 0
  truthfulqa_gen: 0.498
  wikitext: 0.137
  mmlu: 0.088
  arc_challenge: 0

LoRA Parameters:
  lora_r: (78,)
  lora_dropout: (2.741358050066029e-18,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (26.386450441831606,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  78
lora dropout:  2.741358050066029e-18
lora alpha:  26.386450441831606
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 37,380,096 || all params: 8,067,641,344 || trainable%: 0.4633
length of training data:  9833
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4496, 'grad_norm': 0.6199820041656494, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4759511947631836, 'eval_runtime': 2.9501, 'eval_samples_per_second': 338.977, 'eval_steps_per_second': 21.356, 'epoch': 0.04}
{'loss': 1.8852, 'grad_norm': 0.31243032217025757, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.0654335021972656, 'eval_runtime': 2.9518, 'eval_samples_per_second': 338.772, 'eval_steps_per_second': 21.343, 'epoch': 0.08}
{'loss': 1.551, 'grad_norm': 0.3030501902103424, 'learning_rate': 0.00028725663716814155, 'epoch': 0.12}
{'eval_loss': 1.8972675800323486, 'eval_runtime': 2.9579, 'eval_samples_per_second': 338.073, 'eval_steps_per_second': 21.299, 'epoch': 0.12}
{'loss': 1.4404, 'grad_norm': 0.3232057988643646, 'learning_rate': 0.00027398230088495577, 'epoch': 0.16}
{'eval_loss': 1.8399736881256104, 'eval_runtime': 2.9477, 'eval_samples_per_second': 339.244, 'eval_steps_per_second': 21.372, 'epoch': 0.16}
{'loss': 1.3651, 'grad_norm': 0.2744186818599701, 'learning_rate': 0.0002607079646017699, 'epoch': 0.2}
{'eval_loss': 1.658910870552063, 'eval_runtime': 2.9482, 'eval_samples_per_second': 339.185, 'eval_steps_per_second': 21.369, 'epoch': 0.2}
{'loss': 1.3015, 'grad_norm': 0.24868805706501007, 'learning_rate': 0.00024743362831858404, 'epoch': 0.24}
{'eval_loss': 1.7592177391052246, 'eval_runtime': 2.9507, 'eval_samples_per_second': 338.907, 'eval_steps_per_second': 21.351, 'epoch': 0.24}
{'loss': 1.2501, 'grad_norm': 0.25570669770240784, 'learning_rate': 0.0002341592920353982, 'epoch': 0.28}
{'eval_loss': 1.689693808555603, 'eval_runtime': 2.954, 'eval_samples_per_second': 338.527, 'eval_steps_per_second': 21.327, 'epoch': 0.28}
{'loss': 1.1315, 'grad_norm': 0.29167574644088745, 'learning_rate': 0.00022088495575221237, 'epoch': 0.33}
{'eval_loss': 1.698934555053711, 'eval_runtime': 2.9581, 'eval_samples_per_second': 338.06, 'eval_steps_per_second': 21.298, 'epoch': 0.33}
{'loss': 1.2435, 'grad_norm': 0.29728221893310547, 'learning_rate': 0.00020761061946902653, 'epoch': 0.37}
{'eval_loss': 1.6829560995101929, 'eval_runtime': 2.9567, 'eval_samples_per_second': 338.214, 'eval_steps_per_second': 21.307, 'epoch': 0.37}
{'loss': 1.1792, 'grad_norm': 0.2876392602920532, 'learning_rate': 0.00019433628318584067, 'epoch': 0.41}
{'eval_loss': 1.702940583229065, 'eval_runtime': 2.9597, 'eval_samples_per_second': 337.869, 'eval_steps_per_second': 21.286, 'epoch': 0.41}
{'loss': 1.1602, 'grad_norm': 0.3490322530269623, 'learning_rate': 0.00018106194690265486, 'epoch': 0.45}
{'eval_loss': 1.75748872756958, 'eval_runtime': 2.9657, 'eval_samples_per_second': 337.185, 'eval_steps_per_second': 21.243, 'epoch': 0.45}
{'loss': 1.1157, 'grad_norm': 0.246591717004776, 'learning_rate': 0.00016778761061946902, 'epoch': 0.49}
{'eval_loss': 1.661116361618042, 'eval_runtime': 2.9583, 'eval_samples_per_second': 338.027, 'eval_steps_per_second': 21.296, 'epoch': 0.49}
{'loss': 1.083, 'grad_norm': 0.37196433544158936, 'learning_rate': 0.00015451327433628316, 'epoch': 0.53}
{'eval_loss': 1.7444970607757568, 'eval_runtime': 2.9627, 'eval_samples_per_second': 337.525, 'eval_steps_per_second': 21.264, 'epoch': 0.53}
{'loss': 1.1116, 'grad_norm': 0.33988305926322937, 'learning_rate': 0.00014123893805309735, 'epoch': 0.57}
{'eval_loss': 1.6778162717819214, 'eval_runtime': 2.9684, 'eval_samples_per_second': 336.884, 'eval_steps_per_second': 21.224, 'epoch': 0.57}
{'loss': 1.0958, 'grad_norm': 0.4273594617843628, 'learning_rate': 0.00012796460176991149, 'epoch': 0.61}
{'eval_loss': 1.6822360754013062, 'eval_runtime': 2.9702, 'eval_samples_per_second': 336.675, 'eval_steps_per_second': 21.211, 'epoch': 0.61}
{'loss': 1.0986, 'grad_norm': 0.40353524684906006, 'learning_rate': 0.00011469026548672565, 'epoch': 0.65}
{'eval_loss': 1.6611400842666626, 'eval_runtime': 2.9714, 'eval_samples_per_second': 336.543, 'eval_steps_per_second': 21.202, 'epoch': 0.65}
{'loss': 1.0334, 'grad_norm': 0.43294844031333923, 'learning_rate': 0.00010141592920353981, 'epoch': 0.69}
{'eval_loss': 1.6094318628311157, 'eval_runtime': 2.985, 'eval_samples_per_second': 335.007, 'eval_steps_per_second': 21.105, 'epoch': 0.69}
{'loss': 1.0121, 'grad_norm': 0.38603833317756653, 'learning_rate': 8.814159292035398e-05, 'epoch': 0.73}
{'eval_loss': 1.6316442489624023, 'eval_runtime': 2.9655, 'eval_samples_per_second': 337.21, 'eval_steps_per_second': 21.244, 'epoch': 0.73}
{'loss': 0.9707, 'grad_norm': 0.48986250162124634, 'learning_rate': 7.486725663716814e-05, 'epoch': 0.77}
{'eval_loss': 1.6503080129623413, 'eval_runtime': 2.9621, 'eval_samples_per_second': 337.593, 'eval_steps_per_second': 21.268, 'epoch': 0.77}
{'loss': 1.0307, 'grad_norm': 0.4006693661212921, 'learning_rate': 6.159292035398229e-05, 'epoch': 0.81}
{'eval_loss': 1.652376413345337, 'eval_runtime': 2.97, 'eval_samples_per_second': 336.705, 'eval_steps_per_second': 21.212, 'epoch': 0.81}
{'loss': 0.9775, 'grad_norm': 0.3501113951206207, 'learning_rate': 4.831858407079646e-05, 'epoch': 0.85}
{'eval_loss': 1.6333273649215698, 'eval_runtime': 2.9659, 'eval_samples_per_second': 337.164, 'eval_steps_per_second': 21.241, 'epoch': 0.85}
{'loss': 0.953, 'grad_norm': 0.48839902877807617, 'learning_rate': 3.504424778761062e-05, 'epoch': 0.89}
{'eval_loss': 1.6846691370010376, 'eval_runtime': 2.9643, 'eval_samples_per_second': 337.343, 'eval_steps_per_second': 21.253, 'epoch': 0.89}
{'loss': 0.9393, 'grad_norm': 0.5485963821411133, 'learning_rate': 2.1769911504424776e-05, 'epoch': 0.93}
{'eval_loss': 1.6715542078018188, 'eval_runtime': 2.9648, 'eval_samples_per_second': 337.295, 'eval_steps_per_second': 21.25, 'epoch': 0.93}
{'loss': 0.9709, 'grad_norm': 0.512729287147522, 'learning_rate': 8.495575221238936e-06, 'epoch': 0.98}
{'eval_loss': 1.6862322092056274, 'eval_runtime': 2.9708, 'eval_samples_per_second': 336.608, 'eval_steps_per_second': 21.206, 'epoch': 0.98}
{'train_runtime': 244.3428, 'train_samples_per_second': 40.243, 'train_steps_per_second': 2.517, 'train_loss': 1.2570565091885202, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4759511947631836, 2.0654335021972656, 1.8972675800323486, 1.8399736881256104, 1.658910870552063, 1.7592177391052246, 1.689693808555603, 1.698934555053711, 1.6829560995101929, 1.702940583229065, 1.75748872756958, 1.661116361618042, 1.7444970607757568, 1.6778162717819214, 1.6822360754013062, 1.6611400842666626, 1.6094318628311157, 1.6316442489624023, 1.6503080129623413, 1.652376413345337, 1.6333273649215698, 1.6846691370010376, 1.6715542078018188, 1.6862322092056274], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.4759511947631836, 2.0654335021972656, 1.8972675800323486, 1.8399736881256104, 1.658910870552063, 1.7592177391052246, 1.689693808555603, 1.698934555053711, 1.6829560995101929, 1.702940583229065, 1.75748872756958, 1.661116361618042, 1.7444970607757568, 1.6778162717819214, 1.6822360754013062, 1.6611400842666626, 1.6094318628311157, 1.6316442489624023, 1.6503080129623413, 1.652376413345337, 1.6333273649215698, 1.6846691370010376, 1.6715542078018188, 1.6862322092056274]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.052323341369629
current iteration best possible eval_loss (full train run):  -1.6862322092056274
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871, -2.878386974334717, -3.052323341369629]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.6419 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7418680787086487, 0.34305238723754883, 0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.24965649843215942, 0.4866626262664795, 0.6463397145271301, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.6175775527954102, 0.9655588865280151, 0.30760660767555237, 0.14850598573684692]  ‚Üí  acq = -0.6762689790705712
X = [0.5951085090637207, 0.6337894201278687, 0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.14596298336982727, 0.1414751410484314, 0.14421969652175903, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2858731746673584, 0.8522514700889587, 0.9279651641845703, 0.5002951622009277]  ‚Üí  acq = -0.6790581241179046
X = [0.5818041563034058, 0.9915117621421814, 0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.12225708365440369, 0.8182916045188904, 0.2420574426651001, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6762534976005554, 0.9445821046829224, 0.28039222955703735, 0.026326775550842285]  ‚Üí  acq = -0.6790581229407773
X = [0.375633180141449, 0.817596435546875, 0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9535425305366516, 0.6659542918205261, 0.4328734874725342, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6761766076087952, 0.7598890066146851, 0.8752217292785645, 0.10180890560150146]  ‚Üí  acq = -1.1735379138249513
X = [0.13599658012390137, 0.6423792243003845, 0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9463992118835449, 0.9866945743560791, 0.903969407081604, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.46579673886299133, 0.231636643409729, 0.8483097553253174, 0.5441073775291443]  ‚Üí  acq = -0.6790581241179063
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0292, dtype=torch.float64), tensor(0.0725, dtype=torch.float64), tensor(0.0912, dtype=torch.float64), tensor(0.7409, dtype=torch.float64), 0, tensor(0.0565, dtype=torch.float64), 0, 10, 0, 0, 0, 1, 1, 47, 0.06449199985136676, 12.843382946097936, 1]
normalized proposed parameters for next round by BO: [tensor(1.9673e-19, dtype=torch.float64), tensor(2.4193e-17, dtype=torch.float64), tensor(0.0292, dtype=torch.float64), tensor(0.0725, dtype=torch.float64), tensor(0.0912, dtype=torch.float64), tensor(0.7409, dtype=torch.float64), tensor(0.0097, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), tensor(6.7920e-20, dtype=torch.float64), tensor(0.2987, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3666, dtype=torch.float64), tensor(0.6449, dtype=torch.float64), tensor(0.2676, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.029
  sciq: 0.073
  triviaqa: 0.091
  truthfulqa_gen: 0.741
  wikitext: 0
  mmlu: 0.056
  arc_challenge: 0

LoRA Parameters:
  lora_r: (47,)
  lora_dropout: (0.06449199985136676,)
  num_layers_to_apply: (10,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (12.843382946097936,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  10
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  47
lora dropout:  0.06449199985136676
lora alpha:  12.843382946097936
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 17,326,080 || all params: 8,047,587,328 || trainable%: 0.2153
length of training data:  9901
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.419, 'grad_norm': 2.9952070713043213, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.7465639114379883, 'eval_runtime': 2.9485, 'eval_samples_per_second': 339.154, 'eval_steps_per_second': 21.367, 'epoch': 0.04}
{'loss': 1.9142, 'grad_norm': 0.843473494052887, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5915883779525757, 'eval_runtime': 2.9525, 'eval_samples_per_second': 338.692, 'eval_steps_per_second': 21.338, 'epoch': 0.08}
{'loss': 1.2739, 'grad_norm': 0.4966067671775818, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 1.3437058925628662, 'eval_runtime': 2.9512, 'eval_samples_per_second': 338.85, 'eval_steps_per_second': 21.348, 'epoch': 0.12}
{'loss': 1.1122, 'grad_norm': 0.5003861784934998, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 1.29888916015625, 'eval_runtime': 2.9494, 'eval_samples_per_second': 339.049, 'eval_steps_per_second': 21.36, 'epoch': 0.16}
{'loss': 1.0928, 'grad_norm': 0.45427921414375305, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 1.2428641319274902, 'eval_runtime': 2.9509, 'eval_samples_per_second': 338.885, 'eval_steps_per_second': 21.35, 'epoch': 0.2}
{'loss': 0.9637, 'grad_norm': 0.3781196177005768, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 1.1673107147216797, 'eval_runtime': 2.9354, 'eval_samples_per_second': 340.671, 'eval_steps_per_second': 21.462, 'epoch': 0.24}
{'loss': 0.9413, 'grad_norm': 0.29966673254966736, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 1.1713948249816895, 'eval_runtime': 2.9384, 'eval_samples_per_second': 340.327, 'eval_steps_per_second': 21.441, 'epoch': 0.28}
{'loss': 0.8928, 'grad_norm': 0.34981411695480347, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 1.1777609586715698, 'eval_runtime': 2.9355, 'eval_samples_per_second': 340.653, 'eval_steps_per_second': 21.461, 'epoch': 0.32}
{'loss': 0.8821, 'grad_norm': 0.488366961479187, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 1.12913978099823, 'eval_runtime': 2.9379, 'eval_samples_per_second': 340.378, 'eval_steps_per_second': 21.444, 'epoch': 0.36}
{'loss': 0.8954, 'grad_norm': 0.3747275769710541, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 1.1332284212112427, 'eval_runtime': 2.9367, 'eval_samples_per_second': 340.513, 'eval_steps_per_second': 21.452, 'epoch': 0.4}
{'loss': 0.843, 'grad_norm': 0.5177969336509705, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 1.1290487051010132, 'eval_runtime': 2.9423, 'eval_samples_per_second': 339.872, 'eval_steps_per_second': 21.412, 'epoch': 0.44}
{'loss': 0.8306, 'grad_norm': 0.511445939540863, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 1.1328092813491821, 'eval_runtime': 2.9417, 'eval_samples_per_second': 339.945, 'eval_steps_per_second': 21.417, 'epoch': 0.48}
{'loss': 0.7869, 'grad_norm': 0.5014877319335938, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 1.1093246936798096, 'eval_runtime': 2.9449, 'eval_samples_per_second': 339.573, 'eval_steps_per_second': 21.393, 'epoch': 0.53}
{'loss': 0.7758, 'grad_norm': 0.3607635498046875, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 1.118980884552002, 'eval_runtime': 2.9417, 'eval_samples_per_second': 339.934, 'eval_steps_per_second': 21.416, 'epoch': 0.57}
{'loss': 0.7744, 'grad_norm': 0.7597672939300537, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 1.105373740196228, 'eval_runtime': 2.9462, 'eval_samples_per_second': 339.419, 'eval_steps_per_second': 21.383, 'epoch': 0.61}
{'loss': 0.7824, 'grad_norm': 0.7323670387268066, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 1.1172704696655273, 'eval_runtime': 2.9461, 'eval_samples_per_second': 339.436, 'eval_steps_per_second': 21.384, 'epoch': 0.65}
{'loss': 0.724, 'grad_norm': 0.5266227126121521, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 1.1189223527908325, 'eval_runtime': 2.9555, 'eval_samples_per_second': 338.35, 'eval_steps_per_second': 21.316, 'epoch': 0.69}
{'loss': 0.7725, 'grad_norm': 0.6942099332809448, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 1.096381664276123, 'eval_runtime': 2.9536, 'eval_samples_per_second': 338.565, 'eval_steps_per_second': 21.33, 'epoch': 0.73}
{'loss': 0.7124, 'grad_norm': 0.6699919104576111, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 1.0988364219665527, 'eval_runtime': 2.9499, 'eval_samples_per_second': 338.999, 'eval_steps_per_second': 21.357, 'epoch': 0.77}
{'loss': 0.6929, 'grad_norm': 0.6297431588172913, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 1.0954762697219849, 'eval_runtime': 2.9503, 'eval_samples_per_second': 338.945, 'eval_steps_per_second': 21.354, 'epoch': 0.81}
{'loss': 0.6893, 'grad_norm': 0.9453050494194031, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 1.1034373044967651, 'eval_runtime': 2.9554, 'eval_samples_per_second': 338.361, 'eval_steps_per_second': 21.317, 'epoch': 0.85}
{'loss': 0.7026, 'grad_norm': 0.807719349861145, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 1.0990058183670044, 'eval_runtime': 2.9415, 'eval_samples_per_second': 339.958, 'eval_steps_per_second': 21.417, 'epoch': 0.89}
{'loss': 0.6174, 'grad_norm': 0.6383863091468811, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 1.0935883522033691, 'eval_runtime': 2.9536, 'eval_samples_per_second': 338.568, 'eval_steps_per_second': 21.33, 'epoch': 0.93}
{'loss': 0.6754, 'grad_norm': 0.9540861248970032, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 1.1023101806640625, 'eval_runtime': 2.946, 'eval_samples_per_second': 339.44, 'eval_steps_per_second': 21.385, 'epoch': 0.97}
{'train_runtime': 203.5264, 'train_samples_per_second': 48.647, 'train_steps_per_second': 3.041, 'train_loss': 1.0196943760688548, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7465639114379883, 1.5915883779525757, 1.3437058925628662, 1.29888916015625, 1.2428641319274902, 1.1673107147216797, 1.1713948249816895, 1.1777609586715698, 1.12913978099823, 1.1332284212112427, 1.1290487051010132, 1.1328092813491821, 1.1093246936798096, 1.118980884552002, 1.105373740196228, 1.1172704696655273, 1.1189223527908325, 1.096381664276123, 1.0988364219665527, 1.0954762697219849, 1.1034373044967651, 1.0990058183670044, 1.0935883522033691, 1.1023101806640625], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.7465639114379883, 1.5915883779525757, 1.3437058925628662, 1.29888916015625, 1.2428641319274902, 1.1673107147216797, 1.1713948249816895, 1.1777609586715698, 1.12913978099823, 1.1332284212112427, 1.1290487051010132, 1.1328092813491821, 1.1093246936798096, 1.118980884552002, 1.105373740196228, 1.1172704696655273, 1.1189223527908325, 1.096381664276123, 1.0988364219665527, 1.0954762697219849, 1.1034373044967651, 1.0990058183670044, 1.0935883522033691, 1.1023101806640625]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.3741111755371094
current iteration best possible eval_loss (full train run):  -1.1023101806640625
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871, -2.878386974334717, -3.052323341369629, -2.3741111755371094]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.7102 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3668936491012573, 0.891264021396637, 0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.8238186836242676, 0.9171960949897766, 0.8835806846618652, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.45111533999443054, 0.2139490246772766, 0.9017742872238159, 0.11642980575561523]  ‚Üí  acq = -0.49715677864380625
X = [0.10986042022705078, 0.20953714847564697, 0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9494266510009766, 0.8764203786849976, 0.9284661412239075, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6602510213851929, 0.18004006147384644, 0.9259414672851562, 0.9654239416122437]  ‚Üí  acq = -0.49789048282336634
X = [0.17275136709213257, 0.6849347949028015, 0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.31699109077453613, 0.9090394377708435, 0.2634289264678955, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.03518377244472504, 0.6907520294189453, 0.12004825472831726, 0.8972193002700806]  ‚Üí  acq = -0.49784783177541825
X = [0.6180925965309143, 0.16254359483718872, 0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4304628074169159, 0.8458959460258484, 0.5030843019485474, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.715866208076477, 0.002808213233947754, 0.6724920272827148, 0.5655561685562134]  ‚Üí  acq = -0.5481407316625728
X = [0.32551831007003784, 0.7355300188064575, 0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.8032635450363159, 0.7881297469139099, 0.10880237817764282, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.07442650943994522, 0.0027261972427368164, 0.3981722295284271, 0.7018656134605408]  ‚Üí  acq = -0.4666171149114964
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2103, dtype=torch.float64), tensor(0.1379, dtype=torch.float64), tensor(0.0103, dtype=torch.float64), tensor(0.2032, dtype=torch.float64), tensor(0.2595, dtype=torch.float64), 0, 0, tensor(0.1788, dtype=torch.float64), 0, 5, 1, 0, 1, 1, 1, 29, 0.016384832839622717, 28.176816447922665, 0]
normalized proposed parameters for next round by BO: [tensor(0.2103, dtype=torch.float64), tensor(0.1379, dtype=torch.float64), tensor(0.0103, dtype=torch.float64), tensor(0.2032, dtype=torch.float64), tensor(0.2595, dtype=torch.float64), tensor(1.7682e-17, dtype=torch.float64), tensor(7.0610e-22, dtype=torch.float64), tensor(0.1788, dtype=torch.float64), tensor(3.3082e-21, dtype=torch.float64), tensor(0.1610, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2276, dtype=torch.float64), tensor(0.1638, dtype=torch.float64), tensor(0.5870, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.21
  gsm8k: 0.138
  rowan_hellaswag: 0.01
  sciq: 0.203
  triviaqa: 0.26
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.179
  arc_challenge: 0

LoRA Parameters:
  lora_r: (29,)
  lora_dropout: (0.016384832839622717,)
  num_layers_to_apply: (5,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (28.176816447922665,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  5
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  29
lora dropout:  0.016384832839622717
lora alpha:  28.176816447922665
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 9,205,760 || all params: 8,039,467,008 || trainable%: 0.1145
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4957, 'grad_norm': 1.6792114973068237, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4643990993499756, 'eval_runtime': 2.9172, 'eval_samples_per_second': 342.791, 'eval_steps_per_second': 21.596, 'epoch': 0.04}
{'loss': 1.6456, 'grad_norm': 0.8463112711906433, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.393905520439148, 'eval_runtime': 2.9136, 'eval_samples_per_second': 343.219, 'eval_steps_per_second': 21.623, 'epoch': 0.08}
{'loss': 1.2342, 'grad_norm': 0.45327964425086975, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2628406286239624, 'eval_runtime': 2.9241, 'eval_samples_per_second': 341.98, 'eval_steps_per_second': 21.545, 'epoch': 0.12}
{'loss': 1.2193, 'grad_norm': 0.3137000501155853, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2310293912887573, 'eval_runtime': 2.9239, 'eval_samples_per_second': 342.006, 'eval_steps_per_second': 21.546, 'epoch': 0.16}
{'loss': 1.1361, 'grad_norm': 0.41074785590171814, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.228589653968811, 'eval_runtime': 2.9414, 'eval_samples_per_second': 339.979, 'eval_steps_per_second': 21.419, 'epoch': 0.2}
{'loss': 1.1345, 'grad_norm': 0.3800830841064453, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2024167776107788, 'eval_runtime': 2.9382, 'eval_samples_per_second': 340.349, 'eval_steps_per_second': 21.442, 'epoch': 0.24}
{'loss': 1.1135, 'grad_norm': 0.2999935746192932, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2071409225463867, 'eval_runtime': 2.9559, 'eval_samples_per_second': 338.309, 'eval_steps_per_second': 21.313, 'epoch': 0.28}
{'loss': 1.0751, 'grad_norm': 0.3267659842967987, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1979260444641113, 'eval_runtime': 2.9619, 'eval_samples_per_second': 337.621, 'eval_steps_per_second': 21.27, 'epoch': 0.32}
{'loss': 1.1349, 'grad_norm': 0.3194330036640167, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1786636114120483, 'eval_runtime': 2.9603, 'eval_samples_per_second': 337.808, 'eval_steps_per_second': 21.282, 'epoch': 0.36}
{'loss': 1.1326, 'grad_norm': 0.30835461616516113, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1920466423034668, 'eval_runtime': 2.9588, 'eval_samples_per_second': 337.972, 'eval_steps_per_second': 21.292, 'epoch': 0.4}
{'loss': 1.0848, 'grad_norm': 0.29455795884132385, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.16826593875885, 'eval_runtime': 2.9656, 'eval_samples_per_second': 337.204, 'eval_steps_per_second': 21.244, 'epoch': 0.44}
{'loss': 1.0914, 'grad_norm': 0.3266564905643463, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1635533571243286, 'eval_runtime': 2.9586, 'eval_samples_per_second': 337.997, 'eval_steps_per_second': 21.294, 'epoch': 0.48}
{'loss': 1.1292, 'grad_norm': 0.3170331120491028, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1640883684158325, 'eval_runtime': 2.9788, 'eval_samples_per_second': 335.707, 'eval_steps_per_second': 21.15, 'epoch': 0.52}
{'loss': 1.124, 'grad_norm': 0.31181731820106506, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1566001176834106, 'eval_runtime': 2.9659, 'eval_samples_per_second': 337.164, 'eval_steps_per_second': 21.241, 'epoch': 0.56}
{'loss': 1.1265, 'grad_norm': 0.2959575653076172, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1526461839675903, 'eval_runtime': 2.9656, 'eval_samples_per_second': 337.203, 'eval_steps_per_second': 21.244, 'epoch': 0.6}
{'loss': 1.1218, 'grad_norm': 0.34681281447410583, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1489264965057373, 'eval_runtime': 2.957, 'eval_samples_per_second': 338.186, 'eval_steps_per_second': 21.306, 'epoch': 0.64}
{'loss': 1.0492, 'grad_norm': 0.30951347947120667, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.143195629119873, 'eval_runtime': 2.9609, 'eval_samples_per_second': 337.735, 'eval_steps_per_second': 21.277, 'epoch': 0.68}
{'loss': 1.1422, 'grad_norm': 0.38234904408454895, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.144658088684082, 'eval_runtime': 2.9787, 'eval_samples_per_second': 335.716, 'eval_steps_per_second': 21.15, 'epoch': 0.72}
{'loss': 1.1378, 'grad_norm': 0.326587051153183, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1450343132019043, 'eval_runtime': 2.9707, 'eval_samples_per_second': 336.624, 'eval_steps_per_second': 21.207, 'epoch': 0.76}
{'loss': 1.087, 'grad_norm': 0.3883935213088989, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.151186227798462, 'eval_runtime': 2.9733, 'eval_samples_per_second': 336.322, 'eval_steps_per_second': 21.188, 'epoch': 0.8}
{'loss': 1.1204, 'grad_norm': 0.279007226228714, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1404902935028076, 'eval_runtime': 2.9644, 'eval_samples_per_second': 337.338, 'eval_steps_per_second': 21.252, 'epoch': 0.84}
{'loss': 1.1044, 'grad_norm': 0.31922292709350586, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1358329057693481, 'eval_runtime': 2.9802, 'eval_samples_per_second': 335.551, 'eval_steps_per_second': 21.14, 'epoch': 0.88}
{'loss': 1.0453, 'grad_norm': 0.3237820565700531, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.140674114227295, 'eval_runtime': 2.9669, 'eval_samples_per_second': 337.054, 'eval_steps_per_second': 21.234, 'epoch': 0.92}
{'loss': 1.1037, 'grad_norm': 0.3199961185455322, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.137756586074829, 'eval_runtime': 2.9757, 'eval_samples_per_second': 336.055, 'eval_steps_per_second': 21.171, 'epoch': 0.96}
{'loss': 1.0643, 'grad_norm': 0.3086393475532532, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1408958435058594, 'eval_runtime': 2.9735, 'eval_samples_per_second': 336.302, 'eval_steps_per_second': 21.187, 'epoch': 1.0}
{'train_runtime': 188.2176, 'train_samples_per_second': 53.114, 'train_steps_per_second': 3.321, 'train_loss': 1.2341422302246094, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4643990993499756, 1.393905520439148, 1.2628406286239624, 1.2310293912887573, 1.228589653968811, 1.2024167776107788, 1.2071409225463867, 1.1979260444641113, 1.1786636114120483, 1.1920466423034668, 1.16826593875885, 1.1635533571243286, 1.1640883684158325, 1.1566001176834106, 1.1526461839675903, 1.1489264965057373, 1.143195629119873, 1.144658088684082, 1.1450343132019043, 1.151186227798462, 1.1404902935028076, 1.1358329057693481, 1.140674114227295, 1.137756586074829, 1.1408958435058594], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.4643990993499756, 1.393905520439148, 1.2628406286239624, 1.2310293912887573, 1.228589653968811, 1.2024167776107788, 1.2071409225463867, 1.1979260444641113, 1.1786636114120483, 1.1920466423034668, 1.16826593875885, 1.1635533571243286, 1.1640883684158325, 1.1566001176834106, 1.1526461839675903, 1.1489264965057373, 1.143195629119873, 1.144658088684082, 1.1450343132019043, 1.151186227798462, 1.1404902935028076, 1.1358329057693481, 1.140674114227295, 1.137756586074829, 1.1408958435058594]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.9408042430877686
current iteration best possible eval_loss (full train run):  -1.1408958435058594
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871, -2.878386974334717, -3.052323341369629, -2.3741111755371094, -1.9408042430877686]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.6794 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -0.5764604275242122
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -0.5882804073411749
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -0.5783876457894048
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -0.5877411152697143
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -0.5875385756410909
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2204, dtype=torch.float64), 0, 0, tensor(0.1933, dtype=torch.float64), 0, 0, tensor(0.1910, dtype=torch.float64), tensor(0.3953, dtype=torch.float64), 29, 0, 1, 0, 0, 1, 91, 0.06907066364054007, 40.98000772493139, 1]
normalized proposed parameters for next round by BO: [tensor(2.7991e-21, dtype=torch.float64), tensor(0.2204, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1548e-17, dtype=torch.float64), tensor(0.1933, dtype=torch.float64), tensor(1.0831e-20, dtype=torch.float64), tensor(5.9049e-18, dtype=torch.float64), tensor(0.1910, dtype=torch.float64), tensor(0.3953, dtype=torch.float64), tensor(0.9140, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7089, dtype=torch.float64), tensor(0.6907, dtype=torch.float64), tensor(0.8538, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.22
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.193
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.191
  arc_challenge: 0.395

LoRA Parameters:
  lora_r: (91,)
  lora_dropout: (0.06907066364054007,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (40.98000772493139,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  91
lora dropout:  0.06907066364054007
lora alpha:  40.98000772493139
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 62,153,728 || all params: 8,092,414,976 || trainable%: 0.7680
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6257, 'grad_norm': 0.4182644486427307, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1498804092407227, 'eval_runtime': 3.1415, 'eval_samples_per_second': 318.321, 'eval_steps_per_second': 20.054, 'epoch': 0.04}
{'loss': 1.2584, 'grad_norm': 0.28692129254341125, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5037988424301147, 'eval_runtime': 3.1331, 'eval_samples_per_second': 319.168, 'eval_steps_per_second': 20.108, 'epoch': 0.08}
{'loss': 1.1087, 'grad_norm': 0.4231993854045868, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3732891082763672, 'eval_runtime': 3.1388, 'eval_samples_per_second': 318.591, 'eval_steps_per_second': 20.071, 'epoch': 0.12}
{'loss': 1.0682, 'grad_norm': 0.2819061577320099, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2667807340621948, 'eval_runtime': 3.1449, 'eval_samples_per_second': 317.978, 'eval_steps_per_second': 20.033, 'epoch': 0.16}
{'loss': 1.0407, 'grad_norm': 0.3034961223602295, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2355196475982666, 'eval_runtime': 3.1464, 'eval_samples_per_second': 317.821, 'eval_steps_per_second': 20.023, 'epoch': 0.2}
{'loss': 1.0239, 'grad_norm': 0.2618328034877777, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2160394191741943, 'eval_runtime': 3.1556, 'eval_samples_per_second': 316.894, 'eval_steps_per_second': 19.964, 'epoch': 0.24}
{'loss': 1.0096, 'grad_norm': 0.24006488919258118, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1796159744262695, 'eval_runtime': 3.1505, 'eval_samples_per_second': 317.413, 'eval_steps_per_second': 19.997, 'epoch': 0.28}
{'loss': 0.9747, 'grad_norm': 0.26496389508247375, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.145970344543457, 'eval_runtime': 3.1511, 'eval_samples_per_second': 317.348, 'eval_steps_per_second': 19.993, 'epoch': 0.32}
{'loss': 0.917, 'grad_norm': 0.22913357615470886, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0681557655334473, 'eval_runtime': 3.1557, 'eval_samples_per_second': 316.883, 'eval_steps_per_second': 19.964, 'epoch': 0.36}
{'loss': 0.9043, 'grad_norm': 0.29361647367477417, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.044785976409912, 'eval_runtime': 3.151, 'eval_samples_per_second': 317.362, 'eval_steps_per_second': 19.994, 'epoch': 0.4}
{'loss': 0.8571, 'grad_norm': 0.3134106993675232, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0345168113708496, 'eval_runtime': 3.1532, 'eval_samples_per_second': 317.134, 'eval_steps_per_second': 19.979, 'epoch': 0.44}
{'loss': 0.8731, 'grad_norm': 0.19818857312202454, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.04084050655365, 'eval_runtime': 3.1661, 'eval_samples_per_second': 315.846, 'eval_steps_per_second': 19.898, 'epoch': 0.48}
{'loss': 0.8784, 'grad_norm': 0.2643662095069885, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0509768724441528, 'eval_runtime': 3.1689, 'eval_samples_per_second': 315.571, 'eval_steps_per_second': 19.881, 'epoch': 0.52}
{'loss': 0.8584, 'grad_norm': 0.32756683230400085, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0348186492919922, 'eval_runtime': 3.1753, 'eval_samples_per_second': 314.935, 'eval_steps_per_second': 19.841, 'epoch': 0.56}
{'loss': 0.8243, 'grad_norm': 0.27041390538215637, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0246262550354004, 'eval_runtime': 3.1707, 'eval_samples_per_second': 315.391, 'eval_steps_per_second': 19.87, 'epoch': 0.6}
{'loss': 0.7884, 'grad_norm': 0.34567269682884216, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0297104120254517, 'eval_runtime': 3.1632, 'eval_samples_per_second': 316.137, 'eval_steps_per_second': 19.917, 'epoch': 0.64}
{'loss': 0.8032, 'grad_norm': 0.3293614089488983, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0319538116455078, 'eval_runtime': 3.1574, 'eval_samples_per_second': 316.717, 'eval_steps_per_second': 19.953, 'epoch': 0.68}
{'loss': 0.789, 'grad_norm': 0.3397388160228729, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0222225189208984, 'eval_runtime': 3.161, 'eval_samples_per_second': 316.356, 'eval_steps_per_second': 19.93, 'epoch': 0.72}
{'loss': 0.8504, 'grad_norm': 0.2827601432800293, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0233820676803589, 'eval_runtime': 3.1586, 'eval_samples_per_second': 316.596, 'eval_steps_per_second': 19.946, 'epoch': 0.76}
{'loss': 0.8289, 'grad_norm': 0.3271218240261078, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0239717960357666, 'eval_runtime': 3.1686, 'eval_samples_per_second': 315.601, 'eval_steps_per_second': 19.883, 'epoch': 0.8}
{'loss': 0.7832, 'grad_norm': 0.3393208086490631, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.025115966796875, 'eval_runtime': 3.1706, 'eval_samples_per_second': 315.399, 'eval_steps_per_second': 19.87, 'epoch': 0.84}
{'loss': 0.7849, 'grad_norm': 0.40626758337020874, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0218758583068848, 'eval_runtime': 3.1804, 'eval_samples_per_second': 314.421, 'eval_steps_per_second': 19.809, 'epoch': 0.88}
{'loss': 0.7989, 'grad_norm': 0.3827856481075287, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0200879573822021, 'eval_runtime': 3.1735, 'eval_samples_per_second': 315.106, 'eval_steps_per_second': 19.852, 'epoch': 0.92}
{'loss': 0.765, 'grad_norm': 0.31069180369377136, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0188426971435547, 'eval_runtime': 3.1702, 'eval_samples_per_second': 315.436, 'eval_steps_per_second': 19.872, 'epoch': 0.96}
{'loss': 0.7825, 'grad_norm': 0.4256643056869507, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0177829265594482, 'eval_runtime': 3.1556, 'eval_samples_per_second': 316.894, 'eval_steps_per_second': 19.964, 'epoch': 1.0}
{'train_runtime': 287.1602, 'train_samples_per_second': 34.817, 'train_steps_per_second': 2.176, 'train_loss': 0.9678734497070313, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1498804092407227, 1.5037988424301147, 1.3732891082763672, 1.2667807340621948, 1.2355196475982666, 1.2160394191741943, 1.1796159744262695, 1.145970344543457, 1.0681557655334473, 1.044785976409912, 1.0345168113708496, 1.04084050655365, 1.0509768724441528, 1.0348186492919922, 1.0246262550354004, 1.0297104120254517, 1.0319538116455078, 1.0222225189208984, 1.0233820676803589, 1.0239717960357666, 1.025115966796875, 1.0218758583068848, 1.0200879573822021, 1.0188426971435547, 1.0177829265594482], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1498804092407227, 1.5037988424301147, 1.3732891082763672, 1.2667807340621948, 1.2355196475982666, 1.2160394191741943, 1.1796159744262695, 1.145970344543457, 1.0681557655334473, 1.044785976409912, 1.0345168113708496, 1.04084050655365, 1.0509768724441528, 1.0348186492919922, 1.0246262550354004, 1.0297104120254517, 1.0319538116455078, 1.0222225189208984, 1.0233820676803589, 1.0239717960357666, 1.025115966796875, 1.0218758583068848, 1.0200879573822021, 1.0188426971435547, 1.0177829265594482]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.754873752593994
current iteration best possible eval_loss (full train run):  -1.0177829265594482
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871, -2.878386974334717, -3.052323341369629, -2.3741111755371094, -1.9408042430877686, -3.754873752593994]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.1343 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -0.8417751643652149
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -0.8203230108267106
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -0.858516767675531
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -0.8440054599284144
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -0.8610869682959827
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1611, dtype=torch.float64), tensor(0.1582, dtype=torch.float64), tensor(0.0602, dtype=torch.float64), tensor(0.0203, dtype=torch.float64), tensor(0.0803, dtype=torch.float64), tensor(0.1384, dtype=torch.float64), tensor(0.2979, dtype=torch.float64), 0, tensor(0.0837, dtype=torch.float64), 22, 0, 0, 1, 1, 1, 111, 0.07247322572700607, 30.362204674623356, 0]
normalized proposed parameters for next round by BO: [tensor(0.1611, dtype=torch.float64), tensor(0.1582, dtype=torch.float64), tensor(0.0602, dtype=torch.float64), tensor(0.0203, dtype=torch.float64), tensor(0.0803, dtype=torch.float64), tensor(0.1384, dtype=torch.float64), tensor(0.2979, dtype=torch.float64), tensor(2.5409e-18, dtype=torch.float64), tensor(0.0837, dtype=torch.float64), tensor(0.6831, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8666, dtype=torch.float64), tensor(0.7247, dtype=torch.float64), tensor(0.6325, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.161
  gsm8k: 0.158
  rowan_hellaswag: 0.06
  sciq: 0.02
  triviaqa: 0.08
  truthfulqa_gen: 0.138
  wikitext: 0.298
  mmlu: 0
  arc_challenge: 0.084

LoRA Parameters:
  lora_r: (111,)
  lora_dropout: (0.07247322572700607,)
  num_layers_to_apply: (22,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (30.362204674623356,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  22
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  111
lora dropout:  0.07247322572700607
lora alpha:  30.362204674623356
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 135,032,832 || all params: 8,165,294,080 || trainable%: 1.6537
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0037, 'grad_norm': 0.6158002614974976, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9718061685562134, 'eval_runtime': 3.4536, 'eval_samples_per_second': 289.552, 'eval_steps_per_second': 18.242, 'epoch': 0.04}
{'loss': 1.6154, 'grad_norm': 0.23771683871746063, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2781425714492798, 'eval_runtime': 3.4614, 'eval_samples_per_second': 288.902, 'eval_steps_per_second': 18.201, 'epoch': 0.08}
{'loss': 1.3045, 'grad_norm': 0.24645660817623138, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1762375831604004, 'eval_runtime': 3.4625, 'eval_samples_per_second': 288.806, 'eval_steps_per_second': 18.195, 'epoch': 0.12}
{'loss': 1.3412, 'grad_norm': 0.21786029636859894, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1262602806091309, 'eval_runtime': 3.4674, 'eval_samples_per_second': 288.398, 'eval_steps_per_second': 18.169, 'epoch': 0.16}
{'loss': 1.2748, 'grad_norm': 0.19113750755786896, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1280395984649658, 'eval_runtime': 3.4798, 'eval_samples_per_second': 287.376, 'eval_steps_per_second': 18.105, 'epoch': 0.2}
{'loss': 1.2569, 'grad_norm': 0.200256809592247, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1181936264038086, 'eval_runtime': 3.4757, 'eval_samples_per_second': 287.71, 'eval_steps_per_second': 18.126, 'epoch': 0.24}
{'loss': 1.2254, 'grad_norm': 0.19610507786273956, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.131988286972046, 'eval_runtime': 3.4615, 'eval_samples_per_second': 288.889, 'eval_steps_per_second': 18.2, 'epoch': 0.28}
{'loss': 1.176, 'grad_norm': 0.18475671112537384, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.101773977279663, 'eval_runtime': 3.4638, 'eval_samples_per_second': 288.701, 'eval_steps_per_second': 18.188, 'epoch': 0.32}
{'loss': 1.2081, 'grad_norm': 0.21150225400924683, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0917160511016846, 'eval_runtime': 3.4674, 'eval_samples_per_second': 288.398, 'eval_steps_per_second': 18.169, 'epoch': 0.36}
{'loss': 1.2453, 'grad_norm': 0.2352219521999359, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0880244970321655, 'eval_runtime': 3.4631, 'eval_samples_per_second': 288.756, 'eval_steps_per_second': 18.192, 'epoch': 0.4}
{'loss': 1.223, 'grad_norm': 0.20604275166988373, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0963975191116333, 'eval_runtime': 3.4629, 'eval_samples_per_second': 288.779, 'eval_steps_per_second': 18.193, 'epoch': 0.44}
{'loss': 1.1934, 'grad_norm': 0.1968795210123062, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0792348384857178, 'eval_runtime': 3.4647, 'eval_samples_per_second': 288.629, 'eval_steps_per_second': 18.184, 'epoch': 0.48}
{'loss': 1.2289, 'grad_norm': 0.25448453426361084, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0822744369506836, 'eval_runtime': 3.4672, 'eval_samples_per_second': 288.418, 'eval_steps_per_second': 18.17, 'epoch': 0.52}
{'loss': 1.2252, 'grad_norm': 0.21725495159626007, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0818440914154053, 'eval_runtime': 3.4714, 'eval_samples_per_second': 288.065, 'eval_steps_per_second': 18.148, 'epoch': 0.56}
{'loss': 1.2536, 'grad_norm': 0.19550080597400665, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0827410221099854, 'eval_runtime': 3.4709, 'eval_samples_per_second': 288.112, 'eval_steps_per_second': 18.151, 'epoch': 0.6}
{'loss': 1.2121, 'grad_norm': 0.17372380197048187, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0755033493041992, 'eval_runtime': 3.4689, 'eval_samples_per_second': 288.279, 'eval_steps_per_second': 18.162, 'epoch': 0.64}
{'loss': 1.2174, 'grad_norm': 0.2064596265554428, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0643850564956665, 'eval_runtime': 3.4711, 'eval_samples_per_second': 288.093, 'eval_steps_per_second': 18.15, 'epoch': 0.68}
{'loss': 1.1308, 'grad_norm': 0.22552359104156494, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0658198595046997, 'eval_runtime': 3.4757, 'eval_samples_per_second': 287.71, 'eval_steps_per_second': 18.126, 'epoch': 0.72}
{'loss': 1.2191, 'grad_norm': 0.25909802317619324, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0756651163101196, 'eval_runtime': 3.4945, 'eval_samples_per_second': 286.163, 'eval_steps_per_second': 18.028, 'epoch': 0.76}
{'loss': 1.238, 'grad_norm': 0.2134079486131668, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.072204828262329, 'eval_runtime': 3.5212, 'eval_samples_per_second': 283.996, 'eval_steps_per_second': 17.892, 'epoch': 0.8}
{'loss': 1.2025, 'grad_norm': 0.1812550276517868, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0712710618972778, 'eval_runtime': 3.5075, 'eval_samples_per_second': 285.105, 'eval_steps_per_second': 17.962, 'epoch': 0.84}
{'loss': 1.1585, 'grad_norm': 0.19990487396717072, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0654152631759644, 'eval_runtime': 3.5048, 'eval_samples_per_second': 285.326, 'eval_steps_per_second': 17.976, 'epoch': 0.88}
{'loss': 1.246, 'grad_norm': 0.24404962360858917, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0616813898086548, 'eval_runtime': 3.5005, 'eval_samples_per_second': 285.675, 'eval_steps_per_second': 17.998, 'epoch': 0.92}
{'loss': 1.3073, 'grad_norm': 0.21115466952323914, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.065725564956665, 'eval_runtime': 3.5215, 'eval_samples_per_second': 283.967, 'eval_steps_per_second': 17.89, 'epoch': 0.96}
{'loss': 1.1998, 'grad_norm': 0.23218199610710144, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0646694898605347, 'eval_runtime': 3.5246, 'eval_samples_per_second': 283.716, 'eval_steps_per_second': 17.874, 'epoch': 1.0}
{'train_runtime': 293.8837, 'train_samples_per_second': 34.01, 'train_steps_per_second': 2.127, 'train_loss': 1.3162745635986328, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9718061685562134, 1.2781425714492798, 1.1762375831604004, 1.1262602806091309, 1.1280395984649658, 1.1181936264038086, 1.131988286972046, 1.101773977279663, 1.0917160511016846, 1.0880244970321655, 1.0963975191116333, 1.0792348384857178, 1.0822744369506836, 1.0818440914154053, 1.0827410221099854, 1.0755033493041992, 1.0643850564956665, 1.0658198595046997, 1.0756651163101196, 1.072204828262329, 1.0712710618972778, 1.0654152631759644, 1.0616813898086548, 1.065725564956665, 1.0646694898605347], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9718061685562134, 1.2781425714492798, 1.1762375831604004, 1.1262602806091309, 1.1280395984649658, 1.1181936264038086, 1.131988286972046, 1.101773977279663, 1.0917160511016846, 1.0880244970321655, 1.0963975191116333, 1.0792348384857178, 1.0822744369506836, 1.0818440914154053, 1.0827410221099854, 1.0755033493041992, 1.0643850564956665, 1.0658198595046997, 1.0756651163101196, 1.072204828262329, 1.0712710618972778, 1.0654152631759644, 1.0616813898086548, 1.065725564956665, 1.0646694898605347]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.8874592781066895
current iteration best possible eval_loss (full train run):  -1.0646694898605347
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871, -2.878386974334717, -3.052323341369629, -2.3741111755371094, -1.9408042430877686, -3.754873752593994, -3.8874592781066895]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 19.1797 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -0.7420866481934691
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -0.7420870494352028
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -0.7420869495997604
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -0.742088853338978
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -0.742089209484667
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0561, dtype=torch.float64), tensor(0.3729, dtype=torch.float64), tensor(0.1578, dtype=torch.float64), 0, tensor(0.3148, dtype=torch.float64), 0, 0, 0, tensor(0.0984, dtype=torch.float64), 1, 0, 1, 0, 1, 1, 128, 0.021359316116515314, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0561, dtype=torch.float64), tensor(0.3729, dtype=torch.float64), tensor(0.1578, dtype=torch.float64), tensor(2.7200e-16, dtype=torch.float64), tensor(0.3148, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.8777e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0984, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2136, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.056
  gsm8k: 0.373
  rowan_hellaswag: 0.158
  sciq: 0
  triviaqa: 0.315
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.098

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.021359316116515314,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.021359316116515314
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 5,373,952 || all params: 8,035,635,200 || trainable%: 0.0669
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2321, 'grad_norm': 3.697641372680664, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.5704476833343506, 'eval_runtime': 2.776, 'eval_samples_per_second': 360.227, 'eval_steps_per_second': 22.694, 'epoch': 0.04}
{'loss': 2.3858, 'grad_norm': 1.1208107471466064, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.358205556869507, 'eval_runtime': 2.7763, 'eval_samples_per_second': 360.196, 'eval_steps_per_second': 22.692, 'epoch': 0.08}
{'loss': 1.8638, 'grad_norm': 1.8713754415512085, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8856451511383057, 'eval_runtime': 2.7807, 'eval_samples_per_second': 359.624, 'eval_steps_per_second': 22.656, 'epoch': 0.12}
{'loss': 1.7078, 'grad_norm': 0.7957717776298523, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8102264404296875, 'eval_runtime': 2.7817, 'eval_samples_per_second': 359.488, 'eval_steps_per_second': 22.648, 'epoch': 0.16}
{'loss': 1.5217, 'grad_norm': 0.7742120623588562, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5761305093765259, 'eval_runtime': 2.7843, 'eval_samples_per_second': 359.151, 'eval_steps_per_second': 22.627, 'epoch': 0.2}
{'loss': 1.4099, 'grad_norm': 1.1603366136550903, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5347211360931396, 'eval_runtime': 2.7798, 'eval_samples_per_second': 359.743, 'eval_steps_per_second': 22.664, 'epoch': 0.24}
{'loss': 1.3584, 'grad_norm': 0.8447003960609436, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5327270030975342, 'eval_runtime': 2.7777, 'eval_samples_per_second': 360.014, 'eval_steps_per_second': 22.681, 'epoch': 0.28}
{'loss': 1.3832, 'grad_norm': 0.39931705594062805, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.465484619140625, 'eval_runtime': 2.7895, 'eval_samples_per_second': 358.493, 'eval_steps_per_second': 22.585, 'epoch': 0.32}
{'loss': 1.3483, 'grad_norm': 0.7152372598648071, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.397569179534912, 'eval_runtime': 2.7915, 'eval_samples_per_second': 358.234, 'eval_steps_per_second': 22.569, 'epoch': 0.36}
{'loss': 1.2845, 'grad_norm': 0.6340638995170593, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.346935749053955, 'eval_runtime': 2.8044, 'eval_samples_per_second': 356.587, 'eval_steps_per_second': 22.465, 'epoch': 0.4}
{'loss': 1.3091, 'grad_norm': 0.6946872472763062, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3181579113006592, 'eval_runtime': 2.7865, 'eval_samples_per_second': 358.879, 'eval_steps_per_second': 22.609, 'epoch': 0.44}
{'loss': 1.2625, 'grad_norm': 0.5466641783714294, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3069953918457031, 'eval_runtime': 2.7907, 'eval_samples_per_second': 358.332, 'eval_steps_per_second': 22.575, 'epoch': 0.48}
{'loss': 1.2225, 'grad_norm': 0.5488077998161316, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3005858659744263, 'eval_runtime': 2.789, 'eval_samples_per_second': 358.548, 'eval_steps_per_second': 22.589, 'epoch': 0.52}
{'loss': 1.239, 'grad_norm': 0.5969336628913879, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2987861633300781, 'eval_runtime': 2.7869, 'eval_samples_per_second': 358.821, 'eval_steps_per_second': 22.606, 'epoch': 0.56}
{'loss': 1.2604, 'grad_norm': 0.46080636978149414, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2948638200759888, 'eval_runtime': 2.7922, 'eval_samples_per_second': 358.146, 'eval_steps_per_second': 22.563, 'epoch': 0.6}
{'loss': 1.2344, 'grad_norm': 0.7311955690383911, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.29017972946167, 'eval_runtime': 2.794, 'eval_samples_per_second': 357.91, 'eval_steps_per_second': 22.548, 'epoch': 0.64}
{'loss': 1.2637, 'grad_norm': 0.42644500732421875, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2722241878509521, 'eval_runtime': 2.7893, 'eval_samples_per_second': 358.509, 'eval_steps_per_second': 22.586, 'epoch': 0.68}
{'loss': 1.1937, 'grad_norm': 0.5005466938018799, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2777891159057617, 'eval_runtime': 2.792, 'eval_samples_per_second': 358.163, 'eval_steps_per_second': 22.564, 'epoch': 0.72}
{'loss': 1.2088, 'grad_norm': 0.5309239625930786, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2672221660614014, 'eval_runtime': 2.7883, 'eval_samples_per_second': 358.648, 'eval_steps_per_second': 22.595, 'epoch': 0.76}
{'loss': 1.2361, 'grad_norm': 1.225390911102295, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2593213319778442, 'eval_runtime': 2.7928, 'eval_samples_per_second': 358.06, 'eval_steps_per_second': 22.558, 'epoch': 0.8}
{'loss': 1.2434, 'grad_norm': 0.4585717022418976, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2592864036560059, 'eval_runtime': 2.8059, 'eval_samples_per_second': 356.388, 'eval_steps_per_second': 22.452, 'epoch': 0.84}
{'loss': 1.221, 'grad_norm': 0.5650960206985474, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.259804368019104, 'eval_runtime': 2.7894, 'eval_samples_per_second': 358.504, 'eval_steps_per_second': 22.586, 'epoch': 0.88}
{'loss': 1.208, 'grad_norm': 0.5763686299324036, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2616541385650635, 'eval_runtime': 2.7897, 'eval_samples_per_second': 358.462, 'eval_steps_per_second': 22.583, 'epoch': 0.92}
{'loss': 1.2277, 'grad_norm': 0.5386666655540466, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2614808082580566, 'eval_runtime': 2.7926, 'eval_samples_per_second': 358.087, 'eval_steps_per_second': 22.559, 'epoch': 0.96}
{'loss': 1.2107, 'grad_norm': 0.47078943252563477, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2622053623199463, 'eval_runtime': 2.7931, 'eval_samples_per_second': 358.03, 'eval_steps_per_second': 22.556, 'epoch': 1.0}
{'train_runtime': 261.7215, 'train_samples_per_second': 38.197, 'train_steps_per_second': 2.388, 'train_loss': 1.4414509979248047, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5704476833343506, 2.358205556869507, 1.8856451511383057, 1.8102264404296875, 1.5761305093765259, 1.5347211360931396, 1.5327270030975342, 1.465484619140625, 1.397569179534912, 1.346935749053955, 1.3181579113006592, 1.3069953918457031, 1.3005858659744263, 1.2987861633300781, 1.2948638200759888, 1.29017972946167, 1.2722241878509521, 1.2777891159057617, 1.2672221660614014, 1.2593213319778442, 1.2592864036560059, 1.259804368019104, 1.2616541385650635, 1.2614808082580566, 1.2622053623199463], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.5704476833343506, 2.358205556869507, 1.8856451511383057, 1.8102264404296875, 1.5761305093765259, 1.5347211360931396, 1.5327270030975342, 1.465484619140625, 1.397569179534912, 1.346935749053955, 1.3181579113006592, 1.3069953918457031, 1.3005858659744263, 1.2987861633300781, 1.2948638200759888, 1.29017972946167, 1.2722241878509521, 1.2777891159057617, 1.2672221660614014, 1.2593213319778442, 1.2592864036560059, 1.259804368019104, 1.2616541385650635, 1.2614808082580566, 1.2622053623199463]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.622516632080078
current iteration best possible eval_loss (full train run):  -1.2622053623199463
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871, -2.878386974334717, -3.052323341369629, -2.3741111755371094, -1.9408042430877686, -3.754873752593994, -3.8874592781066895, -3.622516632080078]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.3788 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -0.8146115241688083
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -0.8146115241688083
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -0.8146115241622984
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -0.8146115241688083
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -0.814393538396281
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0828, dtype=torch.float64), tensor(0.0797, dtype=torch.float64), tensor(0.0689, dtype=torch.float64), tensor(0.2285, dtype=torch.float64), 0, tensor(0.0693, dtype=torch.float64), tensor(0.1097, dtype=torch.float64), tensor(0.2344, dtype=torch.float64), tensor(0.1268, dtype=torch.float64), 19, 1, 0, 1, 1, 1, 29, 0.001970394210075999, 29.437157873163756, 0]
normalized proposed parameters for next round by BO: [tensor(0.0828, dtype=torch.float64), tensor(0.0797, dtype=torch.float64), tensor(0.0689, dtype=torch.float64), tensor(0.2285, dtype=torch.float64), tensor(9.7202e-19, dtype=torch.float64), tensor(0.0693, dtype=torch.float64), tensor(0.1097, dtype=torch.float64), tensor(0.2344, dtype=torch.float64), tensor(0.1268, dtype=torch.float64), tensor(0.5852, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2250, dtype=torch.float64), tensor(0.0197, dtype=torch.float64), tensor(0.6133, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.083
  gsm8k: 0.08
  rowan_hellaswag: 0.069
  sciq: 0.228
  triviaqa: 0
  truthfulqa_gen: 0.069
  wikitext: 0.11
  mmlu: 0.234
  arc_challenge: 0.127

LoRA Parameters:
  lora_r: (29,)
  lora_dropout: (0.001970394210075999,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (29.437157873163756,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  29
lora dropout:  0.001970394210075999
lora alpha:  29.437157873163756
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 34,981,888 || all params: 8,065,243,136 || trainable%: 0.4337
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0824, 'grad_norm': 1.2668237686157227, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1203038692474365, 'eval_runtime': 3.3188, 'eval_samples_per_second': 301.31, 'eval_steps_per_second': 18.983, 'epoch': 0.04}
{'loss': 1.5179, 'grad_norm': 0.5393983721733093, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5333056449890137, 'eval_runtime': 3.3295, 'eval_samples_per_second': 300.346, 'eval_steps_per_second': 18.922, 'epoch': 0.08}
{'loss': 1.3169, 'grad_norm': 0.4660146236419678, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4496725797653198, 'eval_runtime': 3.3218, 'eval_samples_per_second': 301.046, 'eval_steps_per_second': 18.966, 'epoch': 0.12}
{'loss': 1.2636, 'grad_norm': 0.31299424171447754, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.492954969406128, 'eval_runtime': 3.3222, 'eval_samples_per_second': 301.005, 'eval_steps_per_second': 18.963, 'epoch': 0.16}
{'loss': 1.2558, 'grad_norm': 0.3201088607311249, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5894001722335815, 'eval_runtime': 3.3252, 'eval_samples_per_second': 300.734, 'eval_steps_per_second': 18.946, 'epoch': 0.2}
{'loss': 1.2704, 'grad_norm': 0.5044770836830139, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5877834558486938, 'eval_runtime': 3.3312, 'eval_samples_per_second': 300.19, 'eval_steps_per_second': 18.912, 'epoch': 0.24}
{'loss': 1.223, 'grad_norm': 0.37661606073379517, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5660393238067627, 'eval_runtime': 3.352, 'eval_samples_per_second': 298.332, 'eval_steps_per_second': 18.795, 'epoch': 0.28}
{'loss': 1.2072, 'grad_norm': 0.3473268151283264, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6195268630981445, 'eval_runtime': 3.3652, 'eval_samples_per_second': 297.158, 'eval_steps_per_second': 18.721, 'epoch': 0.32}
{'loss': 1.2225, 'grad_norm': 0.4018358588218689, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5640023946762085, 'eval_runtime': 3.3756, 'eval_samples_per_second': 296.242, 'eval_steps_per_second': 18.663, 'epoch': 0.36}
{'loss': 1.234, 'grad_norm': 0.3098123073577881, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5502538681030273, 'eval_runtime': 3.3839, 'eval_samples_per_second': 295.517, 'eval_steps_per_second': 18.618, 'epoch': 0.4}
{'loss': 1.1928, 'grad_norm': 0.4221498966217041, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.619808554649353, 'eval_runtime': 3.3732, 'eval_samples_per_second': 296.457, 'eval_steps_per_second': 18.677, 'epoch': 0.44}
{'loss': 1.2065, 'grad_norm': 0.3044022023677826, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6068195104599, 'eval_runtime': 3.3752, 'eval_samples_per_second': 296.279, 'eval_steps_per_second': 18.666, 'epoch': 0.48}
{'loss': 1.2111, 'grad_norm': 0.4632779657840729, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5898022651672363, 'eval_runtime': 3.3977, 'eval_samples_per_second': 294.315, 'eval_steps_per_second': 18.542, 'epoch': 0.52}
{'loss': 1.1622, 'grad_norm': 0.33004745841026306, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.647904396057129, 'eval_runtime': 3.3615, 'eval_samples_per_second': 297.484, 'eval_steps_per_second': 18.742, 'epoch': 0.56}
{'loss': 1.1295, 'grad_norm': 0.4317684769630432, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5996767282485962, 'eval_runtime': 3.374, 'eval_samples_per_second': 296.387, 'eval_steps_per_second': 18.672, 'epoch': 0.6}
{'loss': 1.2124, 'grad_norm': 0.37277278304100037, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5808303356170654, 'eval_runtime': 3.3527, 'eval_samples_per_second': 298.264, 'eval_steps_per_second': 18.791, 'epoch': 0.64}
{'loss': 1.21, 'grad_norm': 0.2995789647102356, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.629218339920044, 'eval_runtime': 3.3538, 'eval_samples_per_second': 298.171, 'eval_steps_per_second': 18.785, 'epoch': 0.68}
{'loss': 1.1627, 'grad_norm': 0.3486836552619934, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6215111017227173, 'eval_runtime': 3.35, 'eval_samples_per_second': 298.506, 'eval_steps_per_second': 18.806, 'epoch': 0.72}
{'loss': 1.183, 'grad_norm': 0.33899638056755066, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6197965145111084, 'eval_runtime': 3.3474, 'eval_samples_per_second': 298.739, 'eval_steps_per_second': 18.821, 'epoch': 0.76}
{'loss': 1.1599, 'grad_norm': 0.32725319266319275, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6093626022338867, 'eval_runtime': 3.3507, 'eval_samples_per_second': 298.442, 'eval_steps_per_second': 18.802, 'epoch': 0.8}
{'loss': 1.1791, 'grad_norm': 0.3594144880771637, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.633589744567871, 'eval_runtime': 3.3462, 'eval_samples_per_second': 298.846, 'eval_steps_per_second': 18.827, 'epoch': 0.84}
{'loss': 1.1574, 'grad_norm': 0.3721645772457123, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6402889490127563, 'eval_runtime': 3.3563, 'eval_samples_per_second': 297.951, 'eval_steps_per_second': 18.771, 'epoch': 0.88}
{'loss': 1.1032, 'grad_norm': 0.38793203234672546, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6430734395980835, 'eval_runtime': 3.349, 'eval_samples_per_second': 298.601, 'eval_steps_per_second': 18.812, 'epoch': 0.92}
{'loss': 1.1344, 'grad_norm': 0.32701408863067627, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6464028358459473, 'eval_runtime': 3.3646, 'eval_samples_per_second': 297.213, 'eval_steps_per_second': 18.724, 'epoch': 0.96}
{'loss': 1.1415, 'grad_norm': 0.4449980854988098, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6471501588821411, 'eval_runtime': 3.3619, 'eval_samples_per_second': 297.448, 'eval_steps_per_second': 18.739, 'epoch': 1.0}
{'train_runtime': 280.2417, 'train_samples_per_second': 35.666, 'train_steps_per_second': 2.23, 'train_loss': 1.2855726928710938, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1203038692474365, 1.5333056449890137, 1.4496725797653198, 1.492954969406128, 1.5894001722335815, 1.5877834558486938, 1.5660393238067627, 1.6195268630981445, 1.5640023946762085, 1.5502538681030273, 1.619808554649353, 1.6068195104599, 1.5898022651672363, 1.647904396057129, 1.5996767282485962, 1.5808303356170654, 1.629218339920044, 1.6215111017227173, 1.6197965145111084, 1.6093626022338867, 1.633589744567871, 1.6402889490127563, 1.6430734395980835, 1.6464028358459473, 1.6471501588821411], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1203038692474365, 1.5333056449890137, 1.4496725797653198, 1.492954969406128, 1.5894001722335815, 1.5877834558486938, 1.5660393238067627, 1.6195268630981445, 1.5640023946762085, 1.5502538681030273, 1.619808554649353, 1.6068195104599, 1.5898022651672363, 1.647904396057129, 1.5996767282485962, 1.5808303356170654, 1.629218339920044, 1.6215111017227173, 1.6197965145111084, 1.6093626022338867, 1.633589744567871, 1.6402889490127563, 1.6430734395980835, 1.6464028358459473, 1.6471501588821411]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.3421449661254883
current iteration best possible eval_loss (full train run):  -1.6471501588821411
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871, -2.878386974334717, -3.052323341369629, -2.3741111755371094, -1.9408042430877686, -3.754873752593994, -3.8874592781066895, -3.622516632080078, -2.3421449661254883]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.8425 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -0.8994204724187533
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -0.8993368253134093
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -0.8994204724213868
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -0.9066492040621947
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -0.9001040675422864
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0124, dtype=torch.float64), 0, tensor(0.0993, dtype=torch.float64), tensor(0.2595, dtype=torch.float64), tensor(0.2032, dtype=torch.float64), tensor(0.0588, dtype=torch.float64), tensor(0.1336, dtype=torch.float64), tensor(0.1947, dtype=torch.float64), tensor(0.0288, dtype=torch.float64), 2, 0, 0, 0, 1, 0, 20, 0.06067205475414984, 17.709598502598823, 1]
normalized proposed parameters for next round by BO: [tensor(0.0124, dtype=torch.float64), tensor(0.0097, dtype=torch.float64), tensor(0.0993, dtype=torch.float64), tensor(0.2595, dtype=torch.float64), tensor(0.2032, dtype=torch.float64), tensor(0.0588, dtype=torch.float64), tensor(0.1336, dtype=torch.float64), tensor(0.1947, dtype=torch.float64), tensor(0.0288, dtype=torch.float64), tensor(0.0698, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1556, dtype=torch.float64), tensor(0.6067, dtype=torch.float64), tensor(0.3689, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.012
  gsm8k: 0
  rowan_hellaswag: 0.099
  sciq: 0.259
  triviaqa: 0.203
  truthfulqa_gen: 0.059
  wikitext: 0.134
  mmlu: 0.195
  arc_challenge: 0.029

LoRA Parameters:
  lora_r: (20,)
  lora_dropout: (0.06067205475414984,)
  num_layers_to_apply: (2,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (17.709598502598823,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  2
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  20
lora dropout:  0.06067205475414984
lora alpha:  17.709598502598823
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 737,280 || all params: 8,030,998,528 || trainable%: 0.0092
length of training data:  9899
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.2496, 'grad_norm': 7.2797160148620605, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.940159559249878, 'eval_runtime': 2.7626, 'eval_samples_per_second': 361.977, 'eval_steps_per_second': 22.805, 'epoch': 0.04}
{'loss': 2.8385, 'grad_norm': 2.0027027130126953, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9348149299621582, 'eval_runtime': 2.7849, 'eval_samples_per_second': 359.078, 'eval_steps_per_second': 22.622, 'epoch': 0.08}
{'loss': 2.0521, 'grad_norm': 1.7305316925048828, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 1.4616878032684326, 'eval_runtime': 2.7686, 'eval_samples_per_second': 361.198, 'eval_steps_per_second': 22.755, 'epoch': 0.12}
{'loss': 1.7778, 'grad_norm': 1.3678041696548462, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 1.3354682922363281, 'eval_runtime': 2.7723, 'eval_samples_per_second': 360.712, 'eval_steps_per_second': 22.725, 'epoch': 0.16}
{'loss': 1.7327, 'grad_norm': 1.6874502897262573, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 1.2307194471359253, 'eval_runtime': 2.7755, 'eval_samples_per_second': 360.296, 'eval_steps_per_second': 22.699, 'epoch': 0.2}
{'loss': 1.6834, 'grad_norm': 2.1296021938323975, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 1.2055081129074097, 'eval_runtime': 2.7801, 'eval_samples_per_second': 359.701, 'eval_steps_per_second': 22.661, 'epoch': 0.24}
{'loss': 1.6415, 'grad_norm': 1.1511414051055908, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 1.1493709087371826, 'eval_runtime': 2.7794, 'eval_samples_per_second': 359.791, 'eval_steps_per_second': 22.667, 'epoch': 0.28}
{'loss': 1.5493, 'grad_norm': 1.8623703718185425, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 1.1475971937179565, 'eval_runtime': 2.783, 'eval_samples_per_second': 359.321, 'eval_steps_per_second': 22.637, 'epoch': 0.32}
{'loss': 1.4581, 'grad_norm': 1.5328062772750854, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 1.146559715270996, 'eval_runtime': 2.7842, 'eval_samples_per_second': 359.168, 'eval_steps_per_second': 22.628, 'epoch': 0.36}
{'loss': 1.5144, 'grad_norm': 1.6956968307495117, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 1.116605281829834, 'eval_runtime': 2.7835, 'eval_samples_per_second': 359.261, 'eval_steps_per_second': 22.633, 'epoch': 0.4}
{'loss': 1.4619, 'grad_norm': 1.2142924070358276, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 1.128737211227417, 'eval_runtime': 2.7898, 'eval_samples_per_second': 358.444, 'eval_steps_per_second': 22.582, 'epoch': 0.44}
{'loss': 1.5044, 'grad_norm': 1.1648833751678467, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 1.1138042211532593, 'eval_runtime': 2.7899, 'eval_samples_per_second': 358.431, 'eval_steps_per_second': 22.581, 'epoch': 0.48}
{'loss': 1.4412, 'grad_norm': 1.4311010837554932, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 1.0937258005142212, 'eval_runtime': 2.7884, 'eval_samples_per_second': 358.628, 'eval_steps_per_second': 22.594, 'epoch': 0.53}
{'loss': 1.47, 'grad_norm': 0.7339392900466919, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 1.1211289167404175, 'eval_runtime': 2.7835, 'eval_samples_per_second': 359.256, 'eval_steps_per_second': 22.633, 'epoch': 0.57}
{'loss': 1.4276, 'grad_norm': 1.2446802854537964, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 1.1096245050430298, 'eval_runtime': 2.7856, 'eval_samples_per_second': 358.985, 'eval_steps_per_second': 22.616, 'epoch': 0.61}
{'loss': 1.4539, 'grad_norm': 1.2207964658737183, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 1.099175214767456, 'eval_runtime': 2.7858, 'eval_samples_per_second': 358.967, 'eval_steps_per_second': 22.615, 'epoch': 0.65}
{'loss': 1.4785, 'grad_norm': 0.7307403683662415, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 1.1046794652938843, 'eval_runtime': 2.7896, 'eval_samples_per_second': 358.473, 'eval_steps_per_second': 22.584, 'epoch': 0.69}
{'loss': 1.4904, 'grad_norm': 1.0120978355407715, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 1.0917394161224365, 'eval_runtime': 2.797, 'eval_samples_per_second': 357.53, 'eval_steps_per_second': 22.524, 'epoch': 0.73}
{'loss': 1.4478, 'grad_norm': 0.9130268096923828, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 1.0948232412338257, 'eval_runtime': 2.7927, 'eval_samples_per_second': 358.081, 'eval_steps_per_second': 22.559, 'epoch': 0.77}
{'loss': 1.4707, 'grad_norm': 1.5607253313064575, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 1.0852429866790771, 'eval_runtime': 2.7938, 'eval_samples_per_second': 357.93, 'eval_steps_per_second': 22.55, 'epoch': 0.81}
{'loss': 1.4507, 'grad_norm': 1.0184937715530396, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 1.0779348611831665, 'eval_runtime': 2.794, 'eval_samples_per_second': 357.91, 'eval_steps_per_second': 22.548, 'epoch': 0.85}
{'loss': 1.4199, 'grad_norm': 1.0432672500610352, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 1.0816059112548828, 'eval_runtime': 2.7928, 'eval_samples_per_second': 358.07, 'eval_steps_per_second': 22.558, 'epoch': 0.89}
{'loss': 1.4289, 'grad_norm': 0.7847515940666199, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 1.0836982727050781, 'eval_runtime': 2.795, 'eval_samples_per_second': 357.784, 'eval_steps_per_second': 22.54, 'epoch': 0.93}
{'loss': 1.4273, 'grad_norm': 0.8282461166381836, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 1.0733200311660767, 'eval_runtime': 2.7974, 'eval_samples_per_second': 357.475, 'eval_steps_per_second': 22.521, 'epoch': 0.97}
{'train_runtime': 243.9314, 'train_samples_per_second': 40.581, 'train_steps_per_second': 2.538, 'train_loss': 1.6942309223970033, 'epoch': 1.0}
train_results:  {'eval_loss': [3.940159559249878, 1.9348149299621582, 1.4616878032684326, 1.3354682922363281, 1.2307194471359253, 1.2055081129074097, 1.1493709087371826, 1.1475971937179565, 1.146559715270996, 1.116605281829834, 1.128737211227417, 1.1138042211532593, 1.0937258005142212, 1.1211289167404175, 1.1096245050430298, 1.099175214767456, 1.1046794652938843, 1.0917394161224365, 1.0948232412338257, 1.0852429866790771, 1.0779348611831665, 1.0816059112548828, 1.0836982727050781, 1.0733200311660767], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [3.940159559249878, 1.9348149299621582, 1.4616878032684326, 1.3354682922363281, 1.2307194471359253, 1.2055081129074097, 1.1493709087371826, 1.1475971937179565, 1.146559715270996, 1.116605281829834, 1.128737211227417, 1.1138042211532593, 1.0937258005142212, 1.1211289167404175, 1.1096245050430298, 1.099175214767456, 1.1046794652938843, 1.0917394161224365, 1.0948232412338257, 1.0852429866790771, 1.0779348611831665, 1.0816059112548828, 1.0836982727050781, 1.0733200311660767]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.7687005996704102
current iteration best possible eval_loss (full train run):  -1.0733200311660767
max eval_loss so far:  -0.9920860528945923
BO observations:  [-1.8673405647277832, -3.6618080139160156, -4.5206804275512695, -2.182877540588379, -2.265839099884033, -4.54395055770874, -3.3427348136901855, -2.8175196647644043, -1.676899790763855, -2.5180721282958984, -2.034248113632202, -1.8127351999282837, -2.698012351989746, -3.184635639190674, -2.2610106468200684, -1.5942556858062744, -4.6048903465271, -3.0058774948120117, -2.929731845855713, -3.436774253845215, -2.138716697692871, -2.878386974334717, -3.052323341369629, -2.3741111755371094, -1.9408042430877686, -3.754873752593994, -3.8874592781066895, -3.622516632080078, -2.3421449661254883, -1.7687005996704102]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.9672 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3357044458389282, 0.4876623749732971, 0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7566351294517517, 0.44088155031204224, 0.21595293283462524, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7294671535491943, 0.9934723973274231, 0.20285475254058838, 0.606965959072113]  ‚Üí  acq = -0.8381311113385694
X = [0.7454677820205688, 0.4424137473106384, 0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8704531192779541, 0.009343624114990234, 0.477742075920105, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7311053276062012, 0.7334456443786621, 0.34133514761924744, 0.4159647822380066]  ‚Üí  acq = -0.8381340716274817
X = [0.6070147752761841, 0.050814270973205566, 0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.06286248564720154, 0.38677525520324707, 0.3574179410934448, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5247937440872192, 0.7738797068595886, 0.6983144283294678, 0.9456675052642822]  ‚Üí  acq = -0.8381311163652976
X = [0.05165296792984009, 0.43891578912734985, 0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.34409016370773315, 0.6168457269668579, 0.23861968517303467, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.4990995526313782, 0.5673343539237976, 0.3437628448009491, 0.6652377843856812]  ‚Üí  acq = -0.832530119535688
X = [0.05172693729400635, 0.2110685110092163, 0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.34519943594932556, 0.5801001787185669, 0.783306360244751, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9125829935073853, 0.5967274308204651, 0.9407432079315186, 0.06500035524368286]  ‚Üí  acq = -0.838131132412407
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1009, dtype=torch.float64), 0, tensor(0.1967, dtype=torch.float64), tensor(0.0799, dtype=torch.float64), tensor(0.2515, dtype=torch.float64), 0, tensor(0.3353, dtype=torch.float64), tensor(0.0355, dtype=torch.float64), 7, 1, 0, 0, 1, 1, 57, 0.09999999999999991, 45.250919076167655, 0]
normalized proposed parameters for next round by BO: [tensor(9.4525e-17, dtype=torch.float64), tensor(0.1009, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1967, dtype=torch.float64), tensor(0.0799, dtype=torch.float64), tensor(0.2515, dtype=torch.float64), tensor(0.0002, dtype=torch.float64), tensor(0.3353, dtype=torch.float64), tensor(0.0355, dtype=torch.float64), tensor(0.2111, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4460, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.9427, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-1.0554845333099365, -1.0554845333099365, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.0161330699920654, -1.002569556236267, -1.002569556236267, -1.002569556236267, -1.002569556236267, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
triviaqa
evaluation dataset:
data domain:  triviaqa  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/triviaqa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 2, 0, 0, 0, 1, 0, 29, 0.05396168787624587, 37, 1]
Checking history sample input_X_between_0_1:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2265625, 0.5396168787624587, 0.7708333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1686161756515503
Checking history sample input_X:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 7, 0, 0, 1, 0, 0, 101, 0.003930648435578799, 29, 1]
Checking history sample input_X_between_0_1:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7890625, 0.039306484355787985, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.066257357597351
Checking history sample input_X:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 22, 1, 0, 0, 1, 1, 3, 0.04742737743265794, 11, 1]
Checking history sample input_X_between_0_1:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 0.6875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0234375, 0.47427377432657936, 0.22916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.7734289765357971
Checking history sample input_X:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 13, 1, 0, 1, 1, 1, 123, 0.09371289547650785, 48, 0]
Checking history sample input_X_between_0_1:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 0.40625, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9609375, 0.9371289547650785, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0014729499816895
Checking history sample input_X:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 13, 0, 0, 1, 1, 0, 127, 0.09560839072699708, 9, 1]
Checking history sample input_X_between_0_1:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 0.40625, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9921875, 0.9560839072699707, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2857557535171509
Checking history sample input_X:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 9, 0, 0, 1, 1, 1, 16, 0.08052765798611784, 41, 1]
Checking history sample input_X_between_0_1:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 0.28125, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.8052765798611784, 0.8541666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8996233940124512
Checking history sample input_X:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 5, 0, 1, 0, 1, 0, 59, 0.05350476033435025, 24, 1]
Checking history sample input_X_between_0_1:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 0.15625, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4609375, 0.5350476033435024, 0.5, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1949825286865234
Checking history sample input_X:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 2, 1, 0, 0, 1, 0, 52, 0.0964290393747826, 9, 0]
Checking history sample input_X_between_0_1:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 0.0625, 1.0, 0.0, 0.0, 1.0, 0.0, 0.40625, 0.964290393747826, 0.1875, 0.0]
Checking history sample eval_loss at 625 steps:  -1.385130763053894
Checking history sample input_X:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 7, 1, 1, 0, 0, 1, 121, 0.029877603091235272, 27, 0]
Checking history sample input_X_between_0_1:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 0.21875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9453125, 0.2987760309123527, 0.5625, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9837757349014282
Checking history sample input_X:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 5, 1, 1, 0, 0, 0, 99, 0.012004498902397865, 15, 0]
Checking history sample input_X_between_0_1:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 0.15625, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7734375, 0.12004498902397864, 0.3125, 0.0]
Checking history sample eval_loss at 625 steps:  -1.6018083095550537
Checking history sample input_X:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 24, 1, 0, 1, 1, 1, 39, 0.002627655727844236, 41, 0]
Checking history sample input_X_between_0_1:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 0.75, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3046875, 0.02627655727844236, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.8874176144599915
Checking history sample input_X:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 24, 0, 0, 0, 0, 1, 93, 0.03163409179138251, 48, 0]
Checking history sample input_X_between_0_1:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 0.75, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7265625, 0.31634091791382507, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.058224081993103
Checking history sample input_X:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 28, 1, 0, 0, 0, 0, 89, 0.0740409362882843, 48, 0]
Checking history sample input_X_between_0_1:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 0.875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6953125, 0.740409362882843, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.431011438369751
Checking history sample input_X:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 28, 1, 0, 1, 1, 1, 2, 0.052382422456560135, 28, 1]
Checking history sample input_X_between_0_1:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 0.875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.015625, 0.5238242245656013, 0.5833333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9802380204200745
Checking history sample input_X:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 11, 0, 0, 0, 1, 1, 54, 0.06588137610749685, 20, 1]
Checking history sample input_X_between_0_1:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 0.34375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.421875, 0.6588137610749685, 0.4166666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -0.7846323251724243
Checking history sample input_X:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 27, 0, 0, 1, 1, 1, 76, 0.04383658958637696, 17, 0]
Checking history sample input_X_between_0_1:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 0.84375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.59375, 0.4383658958637696, 0.3541666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2182475328445435
Checking history sample input_X:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 29, 0, 1, 0, 0, 1, 82, 0.07326489367366754, 40, 1]
Checking history sample input_X_between_0_1:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 0.90625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.640625, 0.7326489367366753, 0.8333333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.7634263634681702
Checking history sample input_X:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 1, 0, 0, 0, 1, 0, 44, 0.09959850306478138, 19, 0]
Checking history sample input_X_between_0_1:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.34375, 0.9959850306478137, 0.3958333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.4772111177444458
Checking history sample input_X:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 26, 1, 1, 1, 0, 1, 67, 0.08108306564701856, 9, 1]
Checking history sample input_X_between_0_1:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 0.8125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5234375, 0.8108306564701856, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4388036727905273
Checking history sample input_X:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 29, 0, 0, 0, 1, 1, 29, 0.08410329802499458, 20, 0]
Checking history sample input_X_between_0_1:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 0.90625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2265625, 0.8410329802499458, 0.4166666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0373977422714233
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1291 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9064942598342896, 0.976524829864502, 0.6585133075714111, 0.7847133278846741, 0.9498302340507507, 0.7241291403770447, 0.9349337220191956, 0.805751383304596, 0.7470259070396423, 0.6993736624717712, 0.9269919991493225, 0.7328242063522339, 0.8103835582733154, 0.08617490530014038, 0.3064950704574585, 0.6621503829956055, 0.8312833309173584, 0.20357516407966614, 0.6832324862480164]  ‚Üí  acq = -0.49305492472121504
X = [0.9669668078422546, 0.645179808139801, 0.7424335479736328, 0.4290943741798401, 0.13649654388427734, 0.1966944932937622, 0.6147449016571045, 0.17651331424713135, 0.861153244972229, 0.44963371753692627, 0.13359546661376953, 0.9463246464729309, 0.2892220616340637, 0.8717247247695923, 0.9571882486343384, 0.31667548418045044, 0.35048264265060425, 0.460758775472641, 0.5381892323493958]  ‚Üí  acq = -0.4930657378862723
X = [0.990956723690033, 0.6280871629714966, 0.3177298903465271, 0.5549265146255493, 0.7043983340263367, 0.0330316424369812, 0.19272619485855103, 0.03746390342712402, 0.25623035430908203, 0.41884103417396545, 0.9446585178375244, 0.2227610945701599, 0.6592106819152832, 0.9171887040138245, 0.4788052439689636, 0.7371795177459717, 0.14793390035629272, 0.37758901715278625, 0.6227999925613403]  ‚Üí  acq = -0.4989279234044016
X = [0.5374243855476379, 0.7382320165634155, 0.8273585438728333, 0.46042877435684204, 0.09010958671569824, 0.9529184699058533, 0.3012573719024658, 0.23239469528198242, 0.9751972556114197, 0.46754029393196106, 0.5620214343070984, 0.9112066030502319, 0.2667543888092041, 0.3244017958641052, 0.6341326236724854, 0.9121889472007751, 0.31351423263549805, 0.6713409423828125, 0.5501665472984314]  ‚Üí  acq = -0.4931292274381437
X = [0.9491512179374695, 0.04530757665634155, 0.5824505686759949, 0.41291725635528564, 0.3130989074707031, 0.31362855434417725, 0.5831677317619324, 0.25047677755355835, 0.3851444721221924, 0.5333307385444641, 0.6336560845375061, 0.8368954062461853, 0.7812232375144958, 0.4993699789047241, 0.419331431388855, 0.40896090865135193, 0.2990427017211914, 0.5679041147232056, 0.9977083802223206]  ‚Üí  acq = -0.4934862199540013
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.4324, dtype=torch.float64), 0, tensor(0.3819, dtype=torch.float64), tensor(0.0871, dtype=torch.float64), 0, 0, tensor(0.0984, dtype=torch.float64), 0, 19, 0, 1, 0, 1, 1, 2, 0.015192848619470714, 48.0, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(9.5835e-18, dtype=torch.float64), tensor(0.4324, dtype=torch.float64), tensor(0.0002, dtype=torch.float64), tensor(0.3819, dtype=torch.float64), tensor(0.0871, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0984, dtype=torch.float64), tensor(1.1621e-17, dtype=torch.float64), tensor(0.6007, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.1519, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.432
  rowan_hellaswag: 0
  sciq: 0.382
  triviaqa: 0.087
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.098
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.015192848619470714,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  2
lora dropout:  0.015192848619470714
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,595,392 || all params: 8,031,856,640 || trainable%: 0.0199
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.3524, 'grad_norm': 6.834221839904785, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8828504085540771, 'eval_runtime': 3.857, 'eval_samples_per_second': 259.27, 'eval_steps_per_second': 16.334, 'epoch': 0.04}
{'loss': 1.1109, 'grad_norm': 5.088356971740723, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3394674062728882, 'eval_runtime': 3.2849, 'eval_samples_per_second': 304.428, 'eval_steps_per_second': 19.179, 'epoch': 0.08}
{'loss': 0.9856, 'grad_norm': 2.1219840049743652, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.194192886352539, 'eval_runtime': 3.2572, 'eval_samples_per_second': 307.009, 'eval_steps_per_second': 19.342, 'epoch': 0.12}
{'loss': 0.9161, 'grad_norm': 1.9112054109573364, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1505727767944336, 'eval_runtime': 3.267, 'eval_samples_per_second': 306.094, 'eval_steps_per_second': 19.284, 'epoch': 0.16}
{'loss': 0.9436, 'grad_norm': 1.8469806909561157, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0968526601791382, 'eval_runtime': 3.2614, 'eval_samples_per_second': 306.615, 'eval_steps_per_second': 19.317, 'epoch': 0.2}
{'loss': 0.9022, 'grad_norm': 1.9612563848495483, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.112778663635254, 'eval_runtime': 3.2649, 'eval_samples_per_second': 306.287, 'eval_steps_per_second': 19.296, 'epoch': 0.24}
{'loss': 0.8949, 'grad_norm': 1.6813384294509888, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1006168127059937, 'eval_runtime': 3.2595, 'eval_samples_per_second': 306.798, 'eval_steps_per_second': 19.328, 'epoch': 0.28}
{'loss': 0.8774, 'grad_norm': 1.5178563594818115, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.096181869506836, 'eval_runtime': 3.2531, 'eval_samples_per_second': 307.4, 'eval_steps_per_second': 19.366, 'epoch': 0.32}
{'loss': 0.8974, 'grad_norm': 1.6743154525756836, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0897045135498047, 'eval_runtime': 3.2685, 'eval_samples_per_second': 305.952, 'eval_steps_per_second': 19.275, 'epoch': 0.36}
{'loss': 0.8869, 'grad_norm': 1.5534029006958008, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0874824523925781, 'eval_runtime': 3.2463, 'eval_samples_per_second': 308.043, 'eval_steps_per_second': 19.407, 'epoch': 0.4}
{'loss': 0.8395, 'grad_norm': 1.8651707172393799, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0870156288146973, 'eval_runtime': 3.2471, 'eval_samples_per_second': 307.97, 'eval_steps_per_second': 19.402, 'epoch': 0.44}
{'loss': 0.9121, 'grad_norm': 1.6427571773529053, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0804706811904907, 'eval_runtime': 3.2504, 'eval_samples_per_second': 307.654, 'eval_steps_per_second': 19.382, 'epoch': 0.48}
{'loss': 0.8918, 'grad_norm': 1.777970790863037, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0849623680114746, 'eval_runtime': 3.2531, 'eval_samples_per_second': 307.403, 'eval_steps_per_second': 19.366, 'epoch': 0.52}
{'loss': 0.8519, 'grad_norm': 1.7553937435150146, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0796089172363281, 'eval_runtime': 3.2549, 'eval_samples_per_second': 307.231, 'eval_steps_per_second': 19.356, 'epoch': 0.56}
{'loss': 0.8675, 'grad_norm': 1.8088375329971313, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0640878677368164, 'eval_runtime': 3.2487, 'eval_samples_per_second': 307.814, 'eval_steps_per_second': 19.392, 'epoch': 0.6}
{'loss': 0.8684, 'grad_norm': 1.2698839902877808, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0709294080734253, 'eval_runtime': 3.2584, 'eval_samples_per_second': 306.898, 'eval_steps_per_second': 19.335, 'epoch': 0.64}
{'loss': 0.8671, 'grad_norm': 2.0261011123657227, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0658879280090332, 'eval_runtime': 3.247, 'eval_samples_per_second': 307.975, 'eval_steps_per_second': 19.402, 'epoch': 0.68}
{'loss': 0.8448, 'grad_norm': 1.6338279247283936, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0656838417053223, 'eval_runtime': 3.2682, 'eval_samples_per_second': 305.976, 'eval_steps_per_second': 19.276, 'epoch': 0.72}
{'loss': 0.8662, 'grad_norm': 1.5904042720794678, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.059147596359253, 'eval_runtime': 3.2446, 'eval_samples_per_second': 308.2, 'eval_steps_per_second': 19.417, 'epoch': 0.76}
{'loss': 0.8674, 'grad_norm': 1.40283203125, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0598645210266113, 'eval_runtime': 3.2218, 'eval_samples_per_second': 310.389, 'eval_steps_per_second': 19.555, 'epoch': 0.8}
{'loss': 0.8573, 'grad_norm': 1.3500722646713257, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0578030347824097, 'eval_runtime': 3.2176, 'eval_samples_per_second': 310.793, 'eval_steps_per_second': 19.58, 'epoch': 0.84}
{'loss': 0.856, 'grad_norm': 1.2102768421173096, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0541545152664185, 'eval_runtime': 3.2175, 'eval_samples_per_second': 310.803, 'eval_steps_per_second': 19.581, 'epoch': 0.88}
{'loss': 0.8516, 'grad_norm': 1.946683645248413, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0575960874557495, 'eval_runtime': 3.2177, 'eval_samples_per_second': 310.781, 'eval_steps_per_second': 19.579, 'epoch': 0.92}
{'loss': 0.8805, 'grad_norm': 1.7660597562789917, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0580180883407593, 'eval_runtime': 3.2147, 'eval_samples_per_second': 311.073, 'eval_steps_per_second': 19.598, 'epoch': 0.96}
{'loss': 0.839, 'grad_norm': 1.4196757078170776, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0572949647903442, 'eval_runtime': 3.2146, 'eval_samples_per_second': 311.08, 'eval_steps_per_second': 19.598, 'epoch': 1.0}
{'train_runtime': 298.5836, 'train_samples_per_second': 33.478, 'train_steps_per_second': 2.093, 'train_loss': 0.9491366333007812, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8828504085540771, 1.3394674062728882, 1.194192886352539, 1.1505727767944336, 1.0968526601791382, 1.112778663635254, 1.1006168127059937, 1.096181869506836, 1.0897045135498047, 1.0874824523925781, 1.0870156288146973, 1.0804706811904907, 1.0849623680114746, 1.0796089172363281, 1.0640878677368164, 1.0709294080734253, 1.0658879280090332, 1.0656838417053223, 1.059147596359253, 1.0598645210266113, 1.0578030347824097, 1.0541545152664185, 1.0575960874557495, 1.0580180883407593, 1.0572949647903442], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8828504085540771, 1.3394674062728882, 1.194192886352539, 1.1505727767944336, 1.0968526601791382, 1.112778663635254, 1.1006168127059937, 1.096181869506836, 1.0897045135498047, 1.0874824523925781, 1.0870156288146973, 1.0804706811904907, 1.0849623680114746, 1.0796089172363281, 1.0640878677368164, 1.0709294080734253, 1.0658879280090332, 1.0656838417053223, 1.059147596359253, 1.0598645210266113, 1.0578030347824097, 1.0541545152664185, 1.0575960874557495, 1.0580180883407593, 1.0572949647903442]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.84628427028656
current iteration best possible eval_loss (full train run):  -1.0572949647903442
max eval_loss so far:  -1.0572949647903442
BO observations:  [-1.84628427028656]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.9096 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.6866727035536764
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.6866727035536764
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.6866727035536764
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.6866727035536764
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.6866727035536764
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3100, dtype=torch.float64), 0, tensor(0.0564, dtype=torch.float64), tensor(0.0951, dtype=torch.float64), 0, 0, 0, tensor(0.1207, dtype=torch.float64), tensor(0.4177, dtype=torch.float64), 1, 1, 0, 1, 1, 1, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.3100, dtype=torch.float64), tensor(3.6738e-17, dtype=torch.float64), tensor(0.0564, dtype=torch.float64), tensor(0.0951, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2159e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1207, dtype=torch.float64), tensor(0.4177, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.31
  gsm8k: 0
  rowan_hellaswag: 0.056
  sciq: 0.095
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.121
  arc_challenge: 0.418

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 8,126,464 || all params: 8,038,387,712 || trainable%: 0.1011
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9738, 'grad_norm': 1.054603934288025, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.6061389446258545, 'eval_runtime': 5.2821, 'eval_samples_per_second': 189.317, 'eval_steps_per_second': 11.927, 'epoch': 0.04}
{'loss': 2.5559, 'grad_norm': 4.387881278991699, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.58320951461792, 'eval_runtime': 2.8694, 'eval_samples_per_second': 348.509, 'eval_steps_per_second': 21.956, 'epoch': 0.08}
{'loss': 1.8385, 'grad_norm': 0.7133979201316833, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.3788323402404785, 'eval_runtime': 2.8762, 'eval_samples_per_second': 347.68, 'eval_steps_per_second': 21.904, 'epoch': 0.12}
{'loss': 1.5408, 'grad_norm': 0.55470871925354, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.215552568435669, 'eval_runtime': 2.8719, 'eval_samples_per_second': 348.197, 'eval_steps_per_second': 21.936, 'epoch': 0.16}
{'loss': 1.4689, 'grad_norm': 0.6230558156967163, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.1639859676361084, 'eval_runtime': 2.8764, 'eval_samples_per_second': 347.657, 'eval_steps_per_second': 21.902, 'epoch': 0.2}
{'loss': 1.4665, 'grad_norm': 0.8218235969543457, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9722895622253418, 'eval_runtime': 2.8854, 'eval_samples_per_second': 346.573, 'eval_steps_per_second': 21.834, 'epoch': 0.24}
{'loss': 1.3144, 'grad_norm': 0.5309191942214966, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8884018659591675, 'eval_runtime': 2.8883, 'eval_samples_per_second': 346.227, 'eval_steps_per_second': 21.812, 'epoch': 0.28}
{'loss': 1.286, 'grad_norm': 0.37874162197113037, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8743947744369507, 'eval_runtime': 2.8916, 'eval_samples_per_second': 345.826, 'eval_steps_per_second': 21.787, 'epoch': 0.32}
{'loss': 1.232, 'grad_norm': 0.5539937615394592, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7385292053222656, 'eval_runtime': 2.8868, 'eval_samples_per_second': 346.403, 'eval_steps_per_second': 21.823, 'epoch': 0.36}
{'loss': 1.2323, 'grad_norm': 0.41005459427833557, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.777336835861206, 'eval_runtime': 2.8918, 'eval_samples_per_second': 345.806, 'eval_steps_per_second': 21.786, 'epoch': 0.4}
{'loss': 1.1664, 'grad_norm': 0.4109646677970886, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7434464693069458, 'eval_runtime': 2.8869, 'eval_samples_per_second': 346.394, 'eval_steps_per_second': 21.823, 'epoch': 0.44}
{'loss': 1.1708, 'grad_norm': 0.5503392219543457, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7533788681030273, 'eval_runtime': 2.8899, 'eval_samples_per_second': 346.038, 'eval_steps_per_second': 21.8, 'epoch': 0.48}
{'loss': 1.1422, 'grad_norm': 0.9745458960533142, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7822637557983398, 'eval_runtime': 2.8873, 'eval_samples_per_second': 346.35, 'eval_steps_per_second': 21.82, 'epoch': 0.52}
{'loss': 1.1589, 'grad_norm': 0.5918689966201782, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7513059377670288, 'eval_runtime': 2.887, 'eval_samples_per_second': 346.377, 'eval_steps_per_second': 21.822, 'epoch': 0.56}
{'loss': 1.159, 'grad_norm': 0.3688434064388275, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.766620397567749, 'eval_runtime': 2.8985, 'eval_samples_per_second': 345.01, 'eval_steps_per_second': 21.736, 'epoch': 0.6}
{'loss': 1.1666, 'grad_norm': 0.4115981459617615, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7381905317306519, 'eval_runtime': 2.8959, 'eval_samples_per_second': 345.311, 'eval_steps_per_second': 21.755, 'epoch': 0.64}
{'loss': 1.2145, 'grad_norm': 0.34644100069999695, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.750400185585022, 'eval_runtime': 2.9116, 'eval_samples_per_second': 343.456, 'eval_steps_per_second': 21.638, 'epoch': 0.68}
{'loss': 1.1453, 'grad_norm': 0.2941169738769531, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7569297552108765, 'eval_runtime': 2.8996, 'eval_samples_per_second': 344.872, 'eval_steps_per_second': 21.727, 'epoch': 0.72}
{'loss': 1.188, 'grad_norm': 0.3738987445831299, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7287393808364868, 'eval_runtime': 2.9067, 'eval_samples_per_second': 344.036, 'eval_steps_per_second': 21.674, 'epoch': 0.76}
{'loss': 1.1186, 'grad_norm': 0.35880351066589355, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7745039463043213, 'eval_runtime': 2.9035, 'eval_samples_per_second': 344.412, 'eval_steps_per_second': 21.698, 'epoch': 0.8}
{'loss': 1.0998, 'grad_norm': 0.37335875630378723, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7870432138442993, 'eval_runtime': 2.9037, 'eval_samples_per_second': 344.394, 'eval_steps_per_second': 21.697, 'epoch': 0.84}
{'loss': 1.1173, 'grad_norm': 0.2971596121788025, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7912259101867676, 'eval_runtime': 2.9037, 'eval_samples_per_second': 344.39, 'eval_steps_per_second': 21.697, 'epoch': 0.88}
{'loss': 1.1604, 'grad_norm': 0.32568177580833435, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7784615755081177, 'eval_runtime': 2.8959, 'eval_samples_per_second': 345.318, 'eval_steps_per_second': 21.755, 'epoch': 0.92}
{'loss': 1.1239, 'grad_norm': 0.39240556955337524, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7724952697753906, 'eval_runtime': 2.8944, 'eval_samples_per_second': 345.49, 'eval_steps_per_second': 21.766, 'epoch': 0.96}
{'loss': 1.153, 'grad_norm': 0.41065502166748047, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7768371105194092, 'eval_runtime': 2.8955, 'eval_samples_per_second': 345.366, 'eval_steps_per_second': 21.758, 'epoch': 1.0}
{'train_runtime': 242.6755, 'train_samples_per_second': 41.199, 'train_steps_per_second': 2.575, 'train_loss': 1.4077566711425782, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6061389446258545, 2.58320951461792, 2.3788323402404785, 2.215552568435669, 2.1639859676361084, 1.9722895622253418, 1.8884018659591675, 1.8743947744369507, 1.7385292053222656, 1.777336835861206, 1.7434464693069458, 1.7533788681030273, 1.7822637557983398, 1.7513059377670288, 1.766620397567749, 1.7381905317306519, 1.750400185585022, 1.7569297552108765, 1.7287393808364868, 1.7745039463043213, 1.7870432138442993, 1.7912259101867676, 1.7784615755081177, 1.7724952697753906, 1.7768371105194092], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.6061389446258545, 2.58320951461792, 2.3788323402404785, 2.215552568435669, 2.1639859676361084, 1.9722895622253418, 1.8884018659591675, 1.8743947744369507, 1.7385292053222656, 1.777336835861206, 1.7434464693069458, 1.7533788681030273, 1.7822637557983398, 1.7513059377670288, 1.766620397567749, 1.7381905317306519, 1.750400185585022, 1.7569297552108765, 1.7287393808364868, 1.7745039463043213, 1.7870432138442993, 1.7912259101867676, 1.7784615755081177, 1.7724952697753906, 1.7768371105194092]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.655287265777588
current iteration best possible eval_loss (full train run):  -1.7768371105194092
max eval_loss so far:  -1.0572949647903442
BO observations:  [-1.84628427028656, -3.655287265777588]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0264 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.3461821783207768
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.5533473057133942
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.4551159704159433
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.44141987678223815
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.4242756528476521
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0878, dtype=torch.float64), 0, 0, 0, tensor(0.5053, dtype=torch.float64), tensor(0.2952, dtype=torch.float64), 0, 0, tensor(0.1118, dtype=torch.float64), 19, 1, 0, 1, 1, 1, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0878, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.1929e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5053, dtype=torch.float64), tensor(0.2952, dtype=torch.float64), tensor(2.4667e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1118, dtype=torch.float64), tensor(0.5941, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.088
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.505
  truthfulqa_gen: 0.295
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.112

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 154,402,816 || all params: 8,184,664,064 || trainable%: 1.8865
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2024, 'grad_norm': 0.9392902851104736, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6477577686309814, 'eval_runtime': 3.3594, 'eval_samples_per_second': 297.668, 'eval_steps_per_second': 18.753, 'epoch': 0.04}
{'loss': 1.2043, 'grad_norm': 0.6202528476715088, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2244694232940674, 'eval_runtime': 3.3578, 'eval_samples_per_second': 297.81, 'eval_steps_per_second': 18.762, 'epoch': 0.08}
{'loss': 1.048, 'grad_norm': 0.44387656450271606, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1037973165512085, 'eval_runtime': 3.3525, 'eval_samples_per_second': 298.284, 'eval_steps_per_second': 18.792, 'epoch': 0.12}
{'loss': 0.9234, 'grad_norm': 0.4454377591609955, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.055464267730713, 'eval_runtime': 3.3568, 'eval_samples_per_second': 297.9, 'eval_steps_per_second': 18.768, 'epoch': 0.16}
{'loss': 0.8975, 'grad_norm': 0.3340170085430145, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0285993814468384, 'eval_runtime': 3.3924, 'eval_samples_per_second': 294.779, 'eval_steps_per_second': 18.571, 'epoch': 0.2}
{'loss': 0.9053, 'grad_norm': 0.4055182635784149, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.009191632270813, 'eval_runtime': 3.3699, 'eval_samples_per_second': 296.741, 'eval_steps_per_second': 18.695, 'epoch': 0.24}
{'loss': 0.8787, 'grad_norm': 0.45882701873779297, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0041141510009766, 'eval_runtime': 3.3883, 'eval_samples_per_second': 295.135, 'eval_steps_per_second': 18.594, 'epoch': 0.28}
{'loss': 0.8324, 'grad_norm': 0.26123112440109253, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0006601810455322, 'eval_runtime': 3.3835, 'eval_samples_per_second': 295.553, 'eval_steps_per_second': 18.62, 'epoch': 0.32}
{'loss': 0.8035, 'grad_norm': 0.26128166913986206, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9992830157279968, 'eval_runtime': 3.4146, 'eval_samples_per_second': 292.863, 'eval_steps_per_second': 18.45, 'epoch': 0.36}
{'loss': 0.8216, 'grad_norm': 0.30680128931999207, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9914189577102661, 'eval_runtime': 3.4018, 'eval_samples_per_second': 293.964, 'eval_steps_per_second': 18.52, 'epoch': 0.4}
{'loss': 0.825, 'grad_norm': 0.30135345458984375, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9936157464981079, 'eval_runtime': 3.3952, 'eval_samples_per_second': 294.535, 'eval_steps_per_second': 18.556, 'epoch': 0.44}
{'loss': 0.8003, 'grad_norm': 0.36482715606689453, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.990626335144043, 'eval_runtime': 3.412, 'eval_samples_per_second': 293.083, 'eval_steps_per_second': 18.464, 'epoch': 0.48}
{'loss': 0.7862, 'grad_norm': 0.39112794399261475, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9900251626968384, 'eval_runtime': 3.3871, 'eval_samples_per_second': 295.242, 'eval_steps_per_second': 18.6, 'epoch': 0.52}
{'loss': 0.7667, 'grad_norm': 0.2826675772666931, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9886611104011536, 'eval_runtime': 3.3835, 'eval_samples_per_second': 295.554, 'eval_steps_per_second': 18.62, 'epoch': 0.56}
{'loss': 0.7676, 'grad_norm': 0.3107566833496094, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9851477146148682, 'eval_runtime': 3.405, 'eval_samples_per_second': 293.682, 'eval_steps_per_second': 18.502, 'epoch': 0.6}
{'loss': 0.7551, 'grad_norm': 0.3342355191707611, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9794144034385681, 'eval_runtime': 3.3785, 'eval_samples_per_second': 295.992, 'eval_steps_per_second': 18.647, 'epoch': 0.64}
{'loss': 0.7429, 'grad_norm': 0.30158230662345886, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9807301163673401, 'eval_runtime': 3.3687, 'eval_samples_per_second': 296.852, 'eval_steps_per_second': 18.702, 'epoch': 0.68}
{'loss': 0.7192, 'grad_norm': 0.4522210657596588, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9777392745018005, 'eval_runtime': 3.3649, 'eval_samples_per_second': 297.186, 'eval_steps_per_second': 18.723, 'epoch': 0.72}
{'loss': 0.7822, 'grad_norm': 0.19999535381793976, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9750963449478149, 'eval_runtime': 3.3537, 'eval_samples_per_second': 298.174, 'eval_steps_per_second': 18.785, 'epoch': 0.76}
{'loss': 0.7544, 'grad_norm': 0.2723692059516907, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9753988981246948, 'eval_runtime': 3.3528, 'eval_samples_per_second': 298.255, 'eval_steps_per_second': 18.79, 'epoch': 0.8}
{'loss': 0.7447, 'grad_norm': 0.3515726327896118, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9733189344406128, 'eval_runtime': 3.3525, 'eval_samples_per_second': 298.285, 'eval_steps_per_second': 18.792, 'epoch': 0.84}
{'loss': 0.7188, 'grad_norm': 0.37846383452415466, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9732325673103333, 'eval_runtime': 3.3567, 'eval_samples_per_second': 297.915, 'eval_steps_per_second': 18.769, 'epoch': 0.88}
{'loss': 0.719, 'grad_norm': 0.2645571827888489, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9717472195625305, 'eval_runtime': 3.3434, 'eval_samples_per_second': 299.094, 'eval_steps_per_second': 18.843, 'epoch': 0.92}
{'loss': 0.7258, 'grad_norm': 0.25680917501449585, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9704427719116211, 'eval_runtime': 3.3538, 'eval_samples_per_second': 298.174, 'eval_steps_per_second': 18.785, 'epoch': 0.96}
{'loss': 0.73, 'grad_norm': 0.32061633467674255, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9699225425720215, 'eval_runtime': 3.3416, 'eval_samples_per_second': 299.255, 'eval_steps_per_second': 18.853, 'epoch': 1.0}
{'train_runtime': 214.9857, 'train_samples_per_second': 46.505, 'train_steps_per_second': 2.907, 'train_loss': 0.9142079040527343, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6477577686309814, 1.2244694232940674, 1.1037973165512085, 1.055464267730713, 1.0285993814468384, 1.009191632270813, 1.0041141510009766, 1.0006601810455322, 0.9992830157279968, 0.9914189577102661, 0.9936157464981079, 0.990626335144043, 0.9900251626968384, 0.9886611104011536, 0.9851477146148682, 0.9794144034385681, 0.9807301163673401, 0.9777392745018005, 0.9750963449478149, 0.9753988981246948, 0.9733189344406128, 0.9732325673103333, 0.9717472195625305, 0.9704427719116211, 0.9699225425720215], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6477577686309814, 1.2244694232940674, 1.1037973165512085, 1.055464267730713, 1.0285993814468384, 1.009191632270813, 1.0041141510009766, 1.0006601810455322, 0.9992830157279968, 0.9914189577102661, 0.9936157464981079, 0.990626335144043, 0.9900251626968384, 0.9886611104011536, 0.9851477146148682, 0.9794144034385681, 0.9807301163673401, 0.9777392745018005, 0.9750963449478149, 0.9753988981246948, 0.9733189344406128, 0.9732325673103333, 0.9717472195625305, 0.9704427719116211, 0.9699225425720215]
current iteration observed (possibly low-fid or predicted) eval_loss:  -4.072971820831299
current iteration best possible eval_loss (full train run):  -0.9699225425720215
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2104 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.7310078451618045
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.7216184358184345
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.7222614443291322
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.7255517928730728
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.7335742186078695
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1706, dtype=torch.float64), tensor(0.1630, dtype=torch.float64), 0, 0, tensor(0.1560, dtype=torch.float64), tensor(0.5105, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.1706, dtype=torch.float64), tensor(0.1630, dtype=torch.float64), tensor(8.2864e-18, dtype=torch.float64), tensor(6.8211e-17, dtype=torch.float64), tensor(0.1560, dtype=torch.float64), tensor(0.5105, dtype=torch.float64), tensor(4.0119e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.171
  gsm8k: 0.163
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.156
  truthfulqa_gen: 0.511
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9648, 'grad_norm': 0.2759246826171875, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.142164468765259, 'eval_runtime': 3.7884, 'eval_samples_per_second': 263.963, 'eval_steps_per_second': 16.63, 'epoch': 0.04}
{'loss': 1.8861, 'grad_norm': 0.20426754653453827, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.636838674545288, 'eval_runtime': 3.7899, 'eval_samples_per_second': 263.861, 'eval_steps_per_second': 16.623, 'epoch': 0.08}
{'loss': 1.1575, 'grad_norm': 0.08161944150924683, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.332161545753479, 'eval_runtime': 3.7798, 'eval_samples_per_second': 264.566, 'eval_steps_per_second': 16.668, 'epoch': 0.12}
{'loss': 0.9761, 'grad_norm': 0.07278032600879669, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.141311526298523, 'eval_runtime': 3.7818, 'eval_samples_per_second': 264.425, 'eval_steps_per_second': 16.659, 'epoch': 0.16}
{'loss': 0.9093, 'grad_norm': 0.04661194235086441, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1039665937423706, 'eval_runtime': 3.7794, 'eval_samples_per_second': 264.59, 'eval_steps_per_second': 16.669, 'epoch': 0.2}
{'loss': 0.881, 'grad_norm': 0.047845594584941864, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0739964246749878, 'eval_runtime': 3.7849, 'eval_samples_per_second': 264.208, 'eval_steps_per_second': 16.645, 'epoch': 0.24}
{'loss': 0.8846, 'grad_norm': 0.052341774106025696, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0771434307098389, 'eval_runtime': 3.7835, 'eval_samples_per_second': 264.307, 'eval_steps_per_second': 16.651, 'epoch': 0.28}
{'loss': 0.8712, 'grad_norm': 0.048477742820978165, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0640476942062378, 'eval_runtime': 3.7868, 'eval_samples_per_second': 264.078, 'eval_steps_per_second': 16.637, 'epoch': 0.32}
{'loss': 0.8392, 'grad_norm': 0.04998813942074776, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0568225383758545, 'eval_runtime': 3.7921, 'eval_samples_per_second': 263.708, 'eval_steps_per_second': 16.614, 'epoch': 0.36}
{'loss': 0.8289, 'grad_norm': 0.0554937943816185, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.055993676185608, 'eval_runtime': 3.7915, 'eval_samples_per_second': 263.75, 'eval_steps_per_second': 16.616, 'epoch': 0.4}
{'loss': 0.827, 'grad_norm': 0.061931781470775604, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.065335750579834, 'eval_runtime': 3.7863, 'eval_samples_per_second': 264.111, 'eval_steps_per_second': 16.639, 'epoch': 0.44}
{'loss': 0.8136, 'grad_norm': 0.05641193687915802, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0624597072601318, 'eval_runtime': 3.7886, 'eval_samples_per_second': 263.949, 'eval_steps_per_second': 16.629, 'epoch': 0.48}
{'loss': 0.8176, 'grad_norm': 0.057859230786561966, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0528422594070435, 'eval_runtime': 3.7831, 'eval_samples_per_second': 264.334, 'eval_steps_per_second': 16.653, 'epoch': 0.52}
{'loss': 0.7705, 'grad_norm': 0.05941588804125786, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0644422769546509, 'eval_runtime': 3.7903, 'eval_samples_per_second': 263.831, 'eval_steps_per_second': 16.621, 'epoch': 0.56}
{'loss': 0.776, 'grad_norm': 0.07176212221384048, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0496110916137695, 'eval_runtime': 3.7894, 'eval_samples_per_second': 263.892, 'eval_steps_per_second': 16.625, 'epoch': 0.6}
{'loss': 0.7904, 'grad_norm': 0.1106729656457901, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.045211672782898, 'eval_runtime': 3.7885, 'eval_samples_per_second': 263.959, 'eval_steps_per_second': 16.629, 'epoch': 0.64}
{'loss': 0.7349, 'grad_norm': 0.07837624847888947, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0472851991653442, 'eval_runtime': 3.7868, 'eval_samples_per_second': 264.073, 'eval_steps_per_second': 16.637, 'epoch': 0.68}
{'loss': 0.7444, 'grad_norm': 0.07910584658384323, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0487923622131348, 'eval_runtime': 3.7852, 'eval_samples_per_second': 264.185, 'eval_steps_per_second': 16.644, 'epoch': 0.72}
{'loss': 0.7512, 'grad_norm': 0.08937916904687881, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0380852222442627, 'eval_runtime': 3.7962, 'eval_samples_per_second': 263.421, 'eval_steps_per_second': 16.596, 'epoch': 0.76}
{'loss': 0.7491, 'grad_norm': 0.0827929899096489, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.040705680847168, 'eval_runtime': 3.8121, 'eval_samples_per_second': 262.321, 'eval_steps_per_second': 16.526, 'epoch': 0.8}
{'loss': 0.7347, 'grad_norm': 0.06578459590673447, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0386872291564941, 'eval_runtime': 3.8192, 'eval_samples_per_second': 261.837, 'eval_steps_per_second': 16.496, 'epoch': 0.84}
{'loss': 0.7233, 'grad_norm': 0.13258349895477295, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0425077676773071, 'eval_runtime': 3.8268, 'eval_samples_per_second': 261.316, 'eval_steps_per_second': 16.463, 'epoch': 0.88}
{'loss': 0.7036, 'grad_norm': 0.07842735946178436, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0443557500839233, 'eval_runtime': 3.851, 'eval_samples_per_second': 259.676, 'eval_steps_per_second': 16.36, 'epoch': 0.92}
{'loss': 0.6937, 'grad_norm': 0.08714941143989563, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0424951314926147, 'eval_runtime': 3.8147, 'eval_samples_per_second': 262.147, 'eval_steps_per_second': 16.515, 'epoch': 0.96}
{'loss': 0.7082, 'grad_norm': 0.09343612939119339, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0419648885726929, 'eval_runtime': 3.8247, 'eval_samples_per_second': 261.457, 'eval_steps_per_second': 16.472, 'epoch': 1.0}
{'train_runtime': 328.4171, 'train_samples_per_second': 30.443, 'train_steps_per_second': 1.903, 'train_loss': 0.9814747253417969, 'epoch': 1.0}
train_results:  {'eval_loss': [3.142164468765259, 1.636838674545288, 1.332161545753479, 1.141311526298523, 1.1039665937423706, 1.0739964246749878, 1.0771434307098389, 1.0640476942062378, 1.0568225383758545, 1.055993676185608, 1.065335750579834, 1.0624597072601318, 1.0528422594070435, 1.0644422769546509, 1.0496110916137695, 1.045211672782898, 1.0472851991653442, 1.0487923622131348, 1.0380852222442627, 1.040705680847168, 1.0386872291564941, 1.0425077676773071, 1.0443557500839233, 1.0424951314926147, 1.0419648885726929], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.142164468765259, 1.636838674545288, 1.332161545753479, 1.141311526298523, 1.1039665937423706, 1.0739964246749878, 1.0771434307098389, 1.0640476942062378, 1.0568225383758545, 1.055993676185608, 1.065335750579834, 1.0624597072601318, 1.0528422594070435, 1.0644422769546509, 1.0496110916137695, 1.045211672782898, 1.0472851991653442, 1.0487923622131348, 1.0380852222442627, 1.040705680847168, 1.0386872291564941, 1.0425077676773071, 1.0443557500839233, 1.0424951314926147, 1.0419648885726929]
current iteration observed (possibly low-fid or predicted) eval_loss:  -4.561641216278076
current iteration best possible eval_loss (full train run):  -1.0419648885726929
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3980 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -1.2459616867454648
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -1.054470739617587
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -1.0110736526401252
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -1.011633563719914
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.6883272460103558
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4495, dtype=torch.float64), 0, 0, 0, tensor(0.0947, dtype=torch.float64), tensor(0.3479, dtype=torch.float64), 0, tensor(0.0108, dtype=torch.float64), tensor(0.0970, dtype=torch.float64), 13, 0, 1, 0, 0, 1, 2, 0.079022171771916, 35.62550846097296, 1]
normalized proposed parameters for next round by BO: [tensor(0.4495, dtype=torch.float64), tensor(3.1962e-17, dtype=torch.float64), tensor(4.3192e-17, dtype=torch.float64), tensor(2.2765e-17, dtype=torch.float64), tensor(0.0947, dtype=torch.float64), tensor(0.3479, dtype=torch.float64), tensor(4.9578e-17, dtype=torch.float64), tensor(0.0108, dtype=torch.float64), tensor(0.0970, dtype=torch.float64), tensor(0.3993, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.7902, dtype=torch.float64), tensor(0.7422, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.45
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.095
  truthfulqa_gen: 0.348
  wikitext: 0
  mmlu: 0.011
  arc_challenge: 0.097

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.079022171771916,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (35.62550846097296,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  2
lora dropout:  0.079022171771916
lora alpha:  35.62550846097296
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 612,352 || all params: 8,030,873,600 || trainable%: 0.0076
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.737, 'grad_norm': 7.543464660644531, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4105634689331055, 'eval_runtime': 3.121, 'eval_samples_per_second': 320.407, 'eval_steps_per_second': 20.186, 'epoch': 0.04}
{'loss': 1.6069, 'grad_norm': 2.2321269512176514, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.810490369796753, 'eval_runtime': 3.0315, 'eval_samples_per_second': 329.87, 'eval_steps_per_second': 20.782, 'epoch': 0.08}
{'loss': 1.1869, 'grad_norm': 3.6538567543029785, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4741443395614624, 'eval_runtime': 3.0004, 'eval_samples_per_second': 333.286, 'eval_steps_per_second': 20.997, 'epoch': 0.12}
{'loss': 1.0954, 'grad_norm': 1.6734086275100708, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.378753662109375, 'eval_runtime': 3.0259, 'eval_samples_per_second': 330.475, 'eval_steps_per_second': 20.82, 'epoch': 0.16}
{'loss': 1.0356, 'grad_norm': 1.3615787029266357, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.32502281665802, 'eval_runtime': 3.0266, 'eval_samples_per_second': 330.4, 'eval_steps_per_second': 20.815, 'epoch': 0.2}
{'loss': 0.9906, 'grad_norm': 2.0085649490356445, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.310418725013733, 'eval_runtime': 3.0249, 'eval_samples_per_second': 330.585, 'eval_steps_per_second': 20.827, 'epoch': 0.24}
{'loss': 0.9822, 'grad_norm': 1.7189185619354248, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.258955717086792, 'eval_runtime': 3.032, 'eval_samples_per_second': 329.818, 'eval_steps_per_second': 20.779, 'epoch': 0.28}
{'loss': 0.9753, 'grad_norm': 1.3377124071121216, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2331188917160034, 'eval_runtime': 3.026, 'eval_samples_per_second': 330.472, 'eval_steps_per_second': 20.82, 'epoch': 0.32}
{'loss': 0.9547, 'grad_norm': 1.4276081323623657, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1945911645889282, 'eval_runtime': 3.0265, 'eval_samples_per_second': 330.416, 'eval_steps_per_second': 20.816, 'epoch': 0.36}
{'loss': 0.8922, 'grad_norm': 1.6746948957443237, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.171166181564331, 'eval_runtime': 3.0337, 'eval_samples_per_second': 329.63, 'eval_steps_per_second': 20.767, 'epoch': 0.4}
{'loss': 0.8423, 'grad_norm': 1.6899350881576538, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1111894845962524, 'eval_runtime': 3.0207, 'eval_samples_per_second': 331.048, 'eval_steps_per_second': 20.856, 'epoch': 0.44}
{'loss': 0.8304, 'grad_norm': 1.434705376625061, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.097747802734375, 'eval_runtime': 3.175, 'eval_samples_per_second': 314.961, 'eval_steps_per_second': 19.843, 'epoch': 0.48}
{'loss': 0.846, 'grad_norm': 1.5132473707199097, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1095517873764038, 'eval_runtime': 3.0457, 'eval_samples_per_second': 328.337, 'eval_steps_per_second': 20.685, 'epoch': 0.52}
{'loss': 0.8331, 'grad_norm': 1.4032137393951416, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1228138208389282, 'eval_runtime': 3.0246, 'eval_samples_per_second': 330.622, 'eval_steps_per_second': 20.829, 'epoch': 0.56}
{'loss': 0.825, 'grad_norm': 1.9410979747772217, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.097406268119812, 'eval_runtime': 3.0332, 'eval_samples_per_second': 329.68, 'eval_steps_per_second': 20.77, 'epoch': 0.6}
{'loss': 0.8382, 'grad_norm': 1.8237959146499634, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0866061449050903, 'eval_runtime': 3.0281, 'eval_samples_per_second': 330.242, 'eval_steps_per_second': 20.805, 'epoch': 0.64}
{'loss': 0.7972, 'grad_norm': 1.6617541313171387, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0818419456481934, 'eval_runtime': 3.0256, 'eval_samples_per_second': 330.516, 'eval_steps_per_second': 20.822, 'epoch': 0.68}
{'loss': 0.7961, 'grad_norm': 1.7494065761566162, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.090710163116455, 'eval_runtime': 3.0263, 'eval_samples_per_second': 330.439, 'eval_steps_per_second': 20.818, 'epoch': 0.72}
{'loss': 0.8224, 'grad_norm': 1.806380271911621, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.074847936630249, 'eval_runtime': 3.034, 'eval_samples_per_second': 329.603, 'eval_steps_per_second': 20.765, 'epoch': 0.76}
{'loss': 0.8168, 'grad_norm': 1.7147035598754883, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0930482149124146, 'eval_runtime': 3.0216, 'eval_samples_per_second': 330.947, 'eval_steps_per_second': 20.85, 'epoch': 0.8}
{'loss': 0.7906, 'grad_norm': 1.4413275718688965, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0758332014083862, 'eval_runtime': 3.0235, 'eval_samples_per_second': 330.747, 'eval_steps_per_second': 20.837, 'epoch': 0.84}
{'loss': 0.7935, 'grad_norm': 2.193723201751709, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0834136009216309, 'eval_runtime': 3.0215, 'eval_samples_per_second': 330.961, 'eval_steps_per_second': 20.851, 'epoch': 0.88}
{'loss': 0.7832, 'grad_norm': 1.9940946102142334, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0753264427185059, 'eval_runtime': 3.0182, 'eval_samples_per_second': 331.326, 'eval_steps_per_second': 20.874, 'epoch': 0.92}
{'loss': 0.812, 'grad_norm': 1.5581591129302979, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.074236273765564, 'eval_runtime': 3.024, 'eval_samples_per_second': 330.683, 'eval_steps_per_second': 20.833, 'epoch': 0.96}
{'loss': 0.8058, 'grad_norm': 1.9959460496902466, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0743259191513062, 'eval_runtime': 3.0214, 'eval_samples_per_second': 330.977, 'eval_steps_per_second': 20.852, 'epoch': 1.0}
{'train_runtime': 192.4822, 'train_samples_per_second': 51.942, 'train_steps_per_second': 3.247, 'train_loss': 1.027578399658203, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4105634689331055, 1.810490369796753, 1.4741443395614624, 1.378753662109375, 1.32502281665802, 1.310418725013733, 1.258955717086792, 1.2331188917160034, 1.1945911645889282, 1.171166181564331, 1.1111894845962524, 1.097747802734375, 1.1095517873764038, 1.1228138208389282, 1.097406268119812, 1.0866061449050903, 1.0818419456481934, 1.090710163116455, 1.074847936630249, 1.0930482149124146, 1.0758332014083862, 1.0834136009216309, 1.0753264427185059, 1.074236273765564, 1.0743259191513062], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.4105634689331055, 1.810490369796753, 1.4741443395614624, 1.378753662109375, 1.32502281665802, 1.310418725013733, 1.258955717086792, 1.2331188917160034, 1.1945911645889282, 1.171166181564331, 1.1111894845962524, 1.097747802734375, 1.1095517873764038, 1.1228138208389282, 1.097406268119812, 1.0866061449050903, 1.0818419456481934, 1.090710163116455, 1.074847936630249, 1.0930482149124146, 1.0758332014083862, 1.0834136009216309, 1.0753264427185059, 1.074236273765564, 1.0743259191513062]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.7098222970962524
current iteration best possible eval_loss (full train run):  -1.0743259191513062
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1025 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.9813586830890682
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -1.029279812439627
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.23101050578870286
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.5124649341433138
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -1.0388115793929082
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.0673, dtype=torch.float64), tensor(0.3313, dtype=torch.float64), 0, tensor(0.4847, dtype=torch.float64), tensor(0.1167, dtype=torch.float64), 17, 1, 0, 0, 0, 0, 2, 0.07042970897097416, 37.14946038585987, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.6868e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0673, dtype=torch.float64), tensor(0.3313, dtype=torch.float64), tensor(6.3285e-16, dtype=torch.float64), tensor(0.4847, dtype=torch.float64), tensor(0.1167, dtype=torch.float64), tensor(0.5370, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.7043, dtype=torch.float64), tensor(0.7739, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.067
  truthfulqa_gen: 0.331
  wikitext: 0
  mmlu: 0.485
  arc_challenge: 0.117

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.07042970897097416,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (37.14946038585987,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.07042970897097416
lora alpha:  37.14946038585987
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 278,528 || all params: 8,030,539,776 || trainable%: 0.0035
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8321, 'grad_norm': 3.379042625427246, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.7153003215789795, 'eval_runtime': 3.643, 'eval_samples_per_second': 274.501, 'eval_steps_per_second': 17.294, 'epoch': 0.04}
{'loss': 2.2663, 'grad_norm': 1.4552786350250244, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.411898374557495, 'eval_runtime': 2.9255, 'eval_samples_per_second': 341.82, 'eval_steps_per_second': 21.535, 'epoch': 0.08}
{'loss': 1.7452, 'grad_norm': 0.7287982702255249, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1231868267059326, 'eval_runtime': 2.9152, 'eval_samples_per_second': 343.034, 'eval_steps_per_second': 21.611, 'epoch': 0.12}
{'loss': 1.6529, 'grad_norm': 1.0157548189163208, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1110260486602783, 'eval_runtime': 2.9209, 'eval_samples_per_second': 342.357, 'eval_steps_per_second': 21.569, 'epoch': 0.16}
{'loss': 1.5916, 'grad_norm': 1.293080449104309, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0392990112304688, 'eval_runtime': 2.9259, 'eval_samples_per_second': 341.774, 'eval_steps_per_second': 21.532, 'epoch': 0.2}
{'loss': 1.5497, 'grad_norm': 1.265580654144287, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.054623603820801, 'eval_runtime': 2.9319, 'eval_samples_per_second': 341.076, 'eval_steps_per_second': 21.488, 'epoch': 0.24}
{'loss': 1.5686, 'grad_norm': 0.8844402432441711, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9994428157806396, 'eval_runtime': 2.9316, 'eval_samples_per_second': 341.108, 'eval_steps_per_second': 21.49, 'epoch': 0.28}
{'loss': 1.5754, 'grad_norm': 1.1682066917419434, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.0300605297088623, 'eval_runtime': 2.9395, 'eval_samples_per_second': 340.195, 'eval_steps_per_second': 21.432, 'epoch': 0.32}
{'loss': 1.5808, 'grad_norm': 0.9024595618247986, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9942095279693604, 'eval_runtime': 2.939, 'eval_samples_per_second': 340.253, 'eval_steps_per_second': 21.436, 'epoch': 0.36}
{'loss': 1.5772, 'grad_norm': 0.9052177667617798, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.99538254737854, 'eval_runtime': 2.9396, 'eval_samples_per_second': 340.182, 'eval_steps_per_second': 21.431, 'epoch': 0.4}
{'loss': 1.4764, 'grad_norm': 0.6886670589447021, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0107436180114746, 'eval_runtime': 2.9438, 'eval_samples_per_second': 339.698, 'eval_steps_per_second': 21.401, 'epoch': 0.44}
{'loss': 1.5435, 'grad_norm': 1.010181188583374, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9983049631118774, 'eval_runtime': 2.9431, 'eval_samples_per_second': 339.777, 'eval_steps_per_second': 21.406, 'epoch': 0.48}
{'loss': 1.5345, 'grad_norm': 0.78570955991745, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9871147871017456, 'eval_runtime': 2.9494, 'eval_samples_per_second': 339.052, 'eval_steps_per_second': 21.36, 'epoch': 0.52}
{'loss': 1.572, 'grad_norm': 0.734512209892273, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9883140325546265, 'eval_runtime': 2.9423, 'eval_samples_per_second': 339.867, 'eval_steps_per_second': 21.412, 'epoch': 0.56}
{'loss': 1.5812, 'grad_norm': 0.776645302772522, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9882460832595825, 'eval_runtime': 2.9411, 'eval_samples_per_second': 340.008, 'eval_steps_per_second': 21.42, 'epoch': 0.6}
{'loss': 1.5202, 'grad_norm': 1.0095473527908325, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.98343825340271, 'eval_runtime': 2.9428, 'eval_samples_per_second': 339.812, 'eval_steps_per_second': 21.408, 'epoch': 0.64}
{'loss': 1.5492, 'grad_norm': 0.8187738060951233, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9725695848464966, 'eval_runtime': 2.9438, 'eval_samples_per_second': 339.701, 'eval_steps_per_second': 21.401, 'epoch': 0.68}
{'loss': 1.4878, 'grad_norm': 0.8222780227661133, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9912151098251343, 'eval_runtime': 2.9411, 'eval_samples_per_second': 340.014, 'eval_steps_per_second': 21.421, 'epoch': 0.72}
{'loss': 1.5506, 'grad_norm': 0.7573330998420715, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9711899757385254, 'eval_runtime': 2.9438, 'eval_samples_per_second': 339.699, 'eval_steps_per_second': 21.401, 'epoch': 0.76}
{'loss': 1.5447, 'grad_norm': 0.9103550314903259, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9626983404159546, 'eval_runtime': 2.9399, 'eval_samples_per_second': 340.151, 'eval_steps_per_second': 21.43, 'epoch': 0.8}
{'loss': 1.533, 'grad_norm': 1.083122730255127, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9614782333374023, 'eval_runtime': 2.9481, 'eval_samples_per_second': 339.199, 'eval_steps_per_second': 21.37, 'epoch': 0.84}
{'loss': 1.561, 'grad_norm': 0.9317106604576111, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.957370638847351, 'eval_runtime': 2.9408, 'eval_samples_per_second': 340.046, 'eval_steps_per_second': 21.423, 'epoch': 0.88}
{'loss': 1.5544, 'grad_norm': 0.7196218371391296, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9613524675369263, 'eval_runtime': 2.94, 'eval_samples_per_second': 340.141, 'eval_steps_per_second': 21.429, 'epoch': 0.92}
{'loss': 1.5598, 'grad_norm': 0.747488796710968, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9595024585723877, 'eval_runtime': 2.9428, 'eval_samples_per_second': 339.815, 'eval_steps_per_second': 21.408, 'epoch': 0.96}
{'loss': 1.5012, 'grad_norm': 1.0538262128829956, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9613860845565796, 'eval_runtime': 2.9417, 'eval_samples_per_second': 339.942, 'eval_steps_per_second': 21.416, 'epoch': 1.0}
{'train_runtime': 212.2022, 'train_samples_per_second': 47.115, 'train_steps_per_second': 2.945, 'train_loss': 1.6803635681152345, 'epoch': 1.0}
train_results:  {'eval_loss': [3.7153003215789795, 2.411898374557495, 2.1231868267059326, 2.1110260486602783, 2.0392990112304688, 2.054623603820801, 1.9994428157806396, 2.0300605297088623, 1.9942095279693604, 1.99538254737854, 2.0107436180114746, 1.9983049631118774, 1.9871147871017456, 1.9883140325546265, 1.9882460832595825, 1.98343825340271, 1.9725695848464966, 1.9912151098251343, 1.9711899757385254, 1.9626983404159546, 1.9614782333374023, 1.957370638847351, 1.9613524675369263, 1.9595024585723877, 1.9613860845565796], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.7153003215789795, 2.411898374557495, 2.1231868267059326, 2.1110260486602783, 2.0392990112304688, 2.054623603820801, 1.9994428157806396, 2.0300605297088623, 1.9942095279693604, 1.99538254737854, 2.0107436180114746, 1.9983049631118774, 1.9871147871017456, 1.9883140325546265, 1.9882460832595825, 1.98343825340271, 1.9725695848464966, 1.9912151098251343, 1.9711899757385254, 1.9626983404159546, 1.9614782333374023, 1.957370638847351, 1.9613524675369263, 1.9595024585723877, 1.9613860845565796]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.9850363731384277
current iteration best possible eval_loss (full train run):  -1.9613860845565796
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.4848 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -1.249812813895613
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -1.2855356884488454
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -1.2175769552393438
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.9063593049505463
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -1.2405824057482802
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1048, dtype=torch.float64), 0, 0, tensor(0.3318, dtype=torch.float64), tensor(0.1110, dtype=torch.float64), tensor(0.2246, dtype=torch.float64), 0, tensor(0.1177, dtype=torch.float64), tensor(0.1101, dtype=torch.float64), 15, 1, 1, 0, 0, 1, 128, 0.06265432744516708, 29.433840312472377, 1]
normalized proposed parameters for next round by BO: [tensor(0.1048, dtype=torch.float64), tensor(1.5150e-17, dtype=torch.float64), tensor(4.2651e-18, dtype=torch.float64), tensor(0.3318, dtype=torch.float64), tensor(0.1110, dtype=torch.float64), tensor(0.2246, dtype=torch.float64), tensor(2.7322e-17, dtype=torch.float64), tensor(0.1177, dtype=torch.float64), tensor(0.1101, dtype=torch.float64), tensor(0.4623, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.6265, dtype=torch.float64), tensor(0.6132, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.105
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.332
  triviaqa: 0.111
  truthfulqa_gen: 0.225
  wikitext: 0
  mmlu: 0.118
  arc_challenge: 0.11

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.06265432744516708,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (29.433840312472377,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  128
lora dropout:  0.06265432744516708
lora alpha:  29.433840312472377
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 60,948,480 || all params: 8,091,209,728 || trainable%: 0.7533
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.7956, 'grad_norm': 1.0295898914337158, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4155073165893555, 'eval_runtime': 3.3347, 'eval_samples_per_second': 299.876, 'eval_steps_per_second': 18.892, 'epoch': 0.04}
{'loss': 1.7309, 'grad_norm': 0.7402268648147583, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.8338069915771484, 'eval_runtime': 3.0519, 'eval_samples_per_second': 327.667, 'eval_steps_per_second': 20.643, 'epoch': 0.08}
{'loss': 1.3149, 'grad_norm': 0.3226631283760071, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5666072368621826, 'eval_runtime': 3.052, 'eval_samples_per_second': 327.654, 'eval_steps_per_second': 20.642, 'epoch': 0.12}
{'loss': 1.2084, 'grad_norm': 0.2648511230945587, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4367551803588867, 'eval_runtime': 3.0567, 'eval_samples_per_second': 327.145, 'eval_steps_per_second': 20.61, 'epoch': 0.16}
{'loss': 1.13, 'grad_norm': 0.24985559284687042, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3203431367874146, 'eval_runtime': 3.0488, 'eval_samples_per_second': 328.001, 'eval_steps_per_second': 20.664, 'epoch': 0.2}
{'loss': 1.1225, 'grad_norm': 0.24925051629543304, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2803274393081665, 'eval_runtime': 3.0503, 'eval_samples_per_second': 327.839, 'eval_steps_per_second': 20.654, 'epoch': 0.24}
{'loss': 1.1355, 'grad_norm': 0.3127780854701996, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2848966121673584, 'eval_runtime': 3.0544, 'eval_samples_per_second': 327.395, 'eval_steps_per_second': 20.626, 'epoch': 0.28}
{'loss': 1.0406, 'grad_norm': 0.22288444638252258, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2718857526779175, 'eval_runtime': 3.0716, 'eval_samples_per_second': 325.561, 'eval_steps_per_second': 20.51, 'epoch': 0.32}
{'loss': 1.0398, 'grad_norm': 0.22502103447914124, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.255064845085144, 'eval_runtime': 3.0773, 'eval_samples_per_second': 324.964, 'eval_steps_per_second': 20.473, 'epoch': 0.36}
{'loss': 1.0431, 'grad_norm': 0.2584359645843506, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2540507316589355, 'eval_runtime': 3.0756, 'eval_samples_per_second': 325.136, 'eval_steps_per_second': 20.484, 'epoch': 0.4}
{'loss': 1.0011, 'grad_norm': 0.23221072554588318, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2310246229171753, 'eval_runtime': 3.0789, 'eval_samples_per_second': 324.788, 'eval_steps_per_second': 20.462, 'epoch': 0.44}
{'loss': 1.0584, 'grad_norm': 0.21937596797943115, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2186530828475952, 'eval_runtime': 3.0725, 'eval_samples_per_second': 325.471, 'eval_steps_per_second': 20.505, 'epoch': 0.48}
{'loss': 1.0098, 'grad_norm': 0.2851153612136841, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2122905254364014, 'eval_runtime': 3.1108, 'eval_samples_per_second': 321.458, 'eval_steps_per_second': 20.252, 'epoch': 0.52}
{'loss': 1.0062, 'grad_norm': 0.27518096566200256, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2074040174484253, 'eval_runtime': 3.1069, 'eval_samples_per_second': 321.864, 'eval_steps_per_second': 20.277, 'epoch': 0.56}
{'loss': 0.9964, 'grad_norm': 0.25034523010253906, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1760088205337524, 'eval_runtime': 3.1132, 'eval_samples_per_second': 321.211, 'eval_steps_per_second': 20.236, 'epoch': 0.6}
{'loss': 1.0169, 'grad_norm': 0.21133625507354736, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1593486070632935, 'eval_runtime': 3.1094, 'eval_samples_per_second': 321.607, 'eval_steps_per_second': 20.261, 'epoch': 0.64}
{'loss': 0.9451, 'grad_norm': 0.22296251356601715, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1345739364624023, 'eval_runtime': 3.1085, 'eval_samples_per_second': 321.695, 'eval_steps_per_second': 20.267, 'epoch': 0.68}
{'loss': 0.929, 'grad_norm': 0.4862461984157562, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1131519079208374, 'eval_runtime': 3.1158, 'eval_samples_per_second': 320.943, 'eval_steps_per_second': 20.219, 'epoch': 0.72}
{'loss': 0.9267, 'grad_norm': 0.22102920711040497, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0735033750534058, 'eval_runtime': 3.1087, 'eval_samples_per_second': 321.679, 'eval_steps_per_second': 20.266, 'epoch': 0.76}
{'loss': 0.943, 'grad_norm': 0.2218438982963562, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0736156702041626, 'eval_runtime': 3.1236, 'eval_samples_per_second': 320.142, 'eval_steps_per_second': 20.169, 'epoch': 0.8}
{'loss': 0.927, 'grad_norm': 0.2386133223772049, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0653102397918701, 'eval_runtime': 3.1191, 'eval_samples_per_second': 320.61, 'eval_steps_per_second': 20.198, 'epoch': 0.84}
{'loss': 0.9074, 'grad_norm': 0.27497997879981995, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0688172578811646, 'eval_runtime': 3.1246, 'eval_samples_per_second': 320.045, 'eval_steps_per_second': 20.163, 'epoch': 0.88}
{'loss': 0.8711, 'grad_norm': 0.35495245456695557, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0639723539352417, 'eval_runtime': 3.1009, 'eval_samples_per_second': 322.487, 'eval_steps_per_second': 20.317, 'epoch': 0.92}
{'loss': 0.8963, 'grad_norm': 0.24466030299663544, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.064458966255188, 'eval_runtime': 3.1044, 'eval_samples_per_second': 322.123, 'eval_steps_per_second': 20.294, 'epoch': 0.96}
{'loss': 0.8986, 'grad_norm': 0.27580147981643677, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.06422758102417, 'eval_runtime': 3.1196, 'eval_samples_per_second': 320.559, 'eval_steps_per_second': 20.195, 'epoch': 1.0}
{'train_runtime': 223.24, 'train_samples_per_second': 44.786, 'train_steps_per_second': 2.8, 'train_loss': 1.155770474243164, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4155073165893555, 1.8338069915771484, 1.5666072368621826, 1.4367551803588867, 1.3203431367874146, 1.2803274393081665, 1.2848966121673584, 1.2718857526779175, 1.255064845085144, 1.2540507316589355, 1.2310246229171753, 1.2186530828475952, 1.2122905254364014, 1.2074040174484253, 1.1760088205337524, 1.1593486070632935, 1.1345739364624023, 1.1131519079208374, 1.0735033750534058, 1.0736156702041626, 1.0653102397918701, 1.0688172578811646, 1.0639723539352417, 1.064458966255188, 1.06422758102417], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.4155073165893555, 1.8338069915771484, 1.5666072368621826, 1.4367551803588867, 1.3203431367874146, 1.2803274393081665, 1.2848966121673584, 1.2718857526779175, 1.255064845085144, 1.2540507316589355, 1.2310246229171753, 1.2186530828475952, 1.2122905254364014, 1.2074040174484253, 1.1760088205337524, 1.1593486070632935, 1.1345739364624023, 1.1131519079208374, 1.0735033750534058, 1.0736156702041626, 1.0653102397918701, 1.0688172578811646, 1.0639723539352417, 1.064458966255188, 1.06422758102417]
current iteration observed (possibly low-fid or predicted) eval_loss:  -4.0099568367004395
current iteration best possible eval_loss (full train run):  -1.06422758102417
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6158 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.15243021507880172
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.33739052627368804
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.1499039585483315
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.1185257469170895
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.15211413677269814
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0257, dtype=torch.float64), 0, tensor(0.5697, dtype=torch.float64), 0, tensor(0.3186, dtype=torch.float64), tensor(0.0860, dtype=torch.float64), 0, 0, 32, 0, 0, 1, 1, 1, 34, 0.0, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(1.9770e-16, dtype=torch.float64), tensor(0.0257, dtype=torch.float64), tensor(4.3599e-17, dtype=torch.float64), tensor(0.5697, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3186, dtype=torch.float64), tensor(0.0860, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2684, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.026
  rowan_hellaswag: 0
  sciq: 0.57
  triviaqa: 0
  truthfulqa_gen: 0.319
  wikitext: 0.086
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (34,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  34
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 60,162,048 || all params: 8,090,423,296 || trainable%: 0.7436
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.8639, 'grad_norm': 0.7972720265388489, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.2578999996185303, 'eval_runtime': 4.8991, 'eval_samples_per_second': 204.118, 'eval_steps_per_second': 12.859, 'epoch': 0.04}
{'loss': 2.0371, 'grad_norm': 0.5908916592597961, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.2029685974121094, 'eval_runtime': 3.6266, 'eval_samples_per_second': 275.741, 'eval_steps_per_second': 17.372, 'epoch': 0.08}
{'loss': 1.2155, 'grad_norm': 0.2535399794578552, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8832318782806396, 'eval_runtime': 3.6065, 'eval_samples_per_second': 277.278, 'eval_steps_per_second': 17.468, 'epoch': 0.12}
{'loss': 1.1507, 'grad_norm': 0.18707634508609772, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7276769876480103, 'eval_runtime': 3.595, 'eval_samples_per_second': 278.168, 'eval_steps_per_second': 17.525, 'epoch': 0.16}
{'loss': 1.0151, 'grad_norm': 0.11307331174612045, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7550517320632935, 'eval_runtime': 3.5911, 'eval_samples_per_second': 278.467, 'eval_steps_per_second': 17.543, 'epoch': 0.2}
{'loss': 0.9247, 'grad_norm': 0.11374843865633011, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.772127628326416, 'eval_runtime': 3.5973, 'eval_samples_per_second': 277.985, 'eval_steps_per_second': 17.513, 'epoch': 0.24}
{'loss': 0.9422, 'grad_norm': 0.10076572746038437, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7529469728469849, 'eval_runtime': 3.5961, 'eval_samples_per_second': 278.08, 'eval_steps_per_second': 17.519, 'epoch': 0.28}
{'loss': 1.0156, 'grad_norm': 0.11703706532716751, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7816497087478638, 'eval_runtime': 3.5971, 'eval_samples_per_second': 278.003, 'eval_steps_per_second': 17.514, 'epoch': 0.32}
{'loss': 0.9349, 'grad_norm': 0.13853980600833893, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.781606674194336, 'eval_runtime': 3.6026, 'eval_samples_per_second': 277.575, 'eval_steps_per_second': 17.487, 'epoch': 0.36}
{'loss': 0.8693, 'grad_norm': 0.11542435735464096, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7733927965164185, 'eval_runtime': 3.5986, 'eval_samples_per_second': 277.885, 'eval_steps_per_second': 17.507, 'epoch': 0.4}
{'loss': 0.9552, 'grad_norm': 0.1126619502902031, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7650007009506226, 'eval_runtime': 3.5976, 'eval_samples_per_second': 277.96, 'eval_steps_per_second': 17.511, 'epoch': 0.44}
{'loss': 0.8571, 'grad_norm': 0.10668224841356277, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.770111322402954, 'eval_runtime': 3.5973, 'eval_samples_per_second': 277.986, 'eval_steps_per_second': 17.513, 'epoch': 0.48}
{'loss': 0.9311, 'grad_norm': 0.12514999508857727, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7815155982971191, 'eval_runtime': 3.5987, 'eval_samples_per_second': 277.878, 'eval_steps_per_second': 17.506, 'epoch': 0.52}
{'loss': 0.863, 'grad_norm': 0.17346493899822235, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.775051236152649, 'eval_runtime': 3.5984, 'eval_samples_per_second': 277.9, 'eval_steps_per_second': 17.508, 'epoch': 0.56}
{'loss': 0.865, 'grad_norm': 0.10933709144592285, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7705646753311157, 'eval_runtime': 3.5983, 'eval_samples_per_second': 277.906, 'eval_steps_per_second': 17.508, 'epoch': 0.6}
{'loss': 0.8951, 'grad_norm': 0.1334654688835144, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7986903190612793, 'eval_runtime': 3.6007, 'eval_samples_per_second': 277.722, 'eval_steps_per_second': 17.496, 'epoch': 0.64}
{'loss': 0.8885, 'grad_norm': 0.14053399860858917, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8219478130340576, 'eval_runtime': 3.5966, 'eval_samples_per_second': 278.037, 'eval_steps_per_second': 17.516, 'epoch': 0.68}
{'loss': 0.8603, 'grad_norm': 0.12524166703224182, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8208917379379272, 'eval_runtime': 3.5951, 'eval_samples_per_second': 278.159, 'eval_steps_per_second': 17.524, 'epoch': 0.72}
{'loss': 0.8052, 'grad_norm': 0.1459338665008545, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8215984106063843, 'eval_runtime': 3.5964, 'eval_samples_per_second': 278.053, 'eval_steps_per_second': 17.517, 'epoch': 0.76}
{'loss': 0.8584, 'grad_norm': 0.13514824211597443, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7957106828689575, 'eval_runtime': 3.5956, 'eval_samples_per_second': 278.118, 'eval_steps_per_second': 17.521, 'epoch': 0.8}
{'loss': 0.8441, 'grad_norm': 0.1647791862487793, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.819055438041687, 'eval_runtime': 3.5943, 'eval_samples_per_second': 278.221, 'eval_steps_per_second': 17.528, 'epoch': 0.84}
{'loss': 0.8951, 'grad_norm': 0.1566057950258255, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8159704208374023, 'eval_runtime': 3.6015, 'eval_samples_per_second': 277.665, 'eval_steps_per_second': 17.493, 'epoch': 0.88}
{'loss': 0.8707, 'grad_norm': 0.16467434167861938, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8193635940551758, 'eval_runtime': 3.6092, 'eval_samples_per_second': 277.073, 'eval_steps_per_second': 17.456, 'epoch': 0.92}
{'loss': 0.8096, 'grad_norm': 0.13443012535572052, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8219826221466064, 'eval_runtime': 3.5989, 'eval_samples_per_second': 277.859, 'eval_steps_per_second': 17.505, 'epoch': 0.96}
{'loss': 0.8603, 'grad_norm': 0.17104925215244293, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8229427337646484, 'eval_runtime': 3.6007, 'eval_samples_per_second': 277.725, 'eval_steps_per_second': 17.497, 'epoch': 1.0}
{'train_runtime': 239.4868, 'train_samples_per_second': 41.748, 'train_steps_per_second': 2.61, 'train_loss': 1.1211104949951172, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2578999996185303, 2.2029685974121094, 1.8832318782806396, 1.7276769876480103, 1.7550517320632935, 1.772127628326416, 1.7529469728469849, 1.7816497087478638, 1.781606674194336, 1.7733927965164185, 1.7650007009506226, 1.770111322402954, 1.7815155982971191, 1.775051236152649, 1.7705646753311157, 1.7986903190612793, 1.8219478130340576, 1.8208917379379272, 1.8215984106063843, 1.7957106828689575, 1.819055438041687, 1.8159704208374023, 1.8193635940551758, 1.8219826221466064, 1.8229427337646484], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.2578999996185303, 2.2029685974121094, 1.8832318782806396, 1.7276769876480103, 1.7550517320632935, 1.772127628326416, 1.7529469728469849, 1.7816497087478638, 1.781606674194336, 1.7733927965164185, 1.7650007009506226, 1.770111322402954, 1.7815155982971191, 1.775051236152649, 1.7705646753311157, 1.7986903190612793, 1.8219478130340576, 1.8208917379379272, 1.8215984106063843, 1.7957106828689575, 1.819055438041687, 1.8159704208374023, 1.8193635940551758, 1.8219826221466064, 1.8229427337646484]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.8967461585998535
current iteration best possible eval_loss (full train run):  -1.8229427337646484
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3501 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.3773804066423523
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.3802936056468844
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.3779213194947213
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.38123009851005474
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.3813204363662692
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1524, dtype=torch.float64), 0, tensor(0.3627, dtype=torch.float64), tensor(0.4849, dtype=torch.float64), 0, 0, 32, 0, 1, 1, 0, 1, 36, 1.1621788153653562e-17, 1.4800000190734874, 1]
normalized proposed parameters for next round by BO: [tensor(5.5365e-17, dtype=torch.float64), tensor(1.6762e-17, dtype=torch.float64), tensor(7.8489e-18, dtype=torch.float64), tensor(0.1524, dtype=torch.float64), tensor(1.2311e-17, dtype=torch.float64), tensor(0.3627, dtype=torch.float64), tensor(0.4849, dtype=torch.float64), tensor(5.0292e-17, dtype=torch.float64), tensor(2.5183e-18, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2837, dtype=torch.float64), tensor(1.1622e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.152
  triviaqa: 0
  truthfulqa_gen: 0.363
  wikitext: 0.485
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (36,)
  lora_dropout: (1.1621788153653562e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (1.4800000190734874,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  36
lora dropout:  1.1621788153653562e-17
lora alpha:  1.4800000190734874
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 48,365,568 || all params: 8,078,626,816 || trainable%: 0.5987
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.0742, 'grad_norm': 0.27054551243782043, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.5192432403564453, 'eval_runtime': 3.4472, 'eval_samples_per_second': 290.092, 'eval_steps_per_second': 18.276, 'epoch': 0.04}
{'loss': 2.6389, 'grad_norm': 0.11652196198701859, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.449873208999634, 'eval_runtime': 3.471, 'eval_samples_per_second': 288.098, 'eval_steps_per_second': 18.15, 'epoch': 0.08}
{'loss': 2.0384, 'grad_norm': 0.10768111050128937, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.363872766494751, 'eval_runtime': 3.4823, 'eval_samples_per_second': 287.163, 'eval_steps_per_second': 18.091, 'epoch': 0.12}
{'loss': 1.8525, 'grad_norm': 0.09045174717903137, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.2596895694732666, 'eval_runtime': 3.4867, 'eval_samples_per_second': 286.804, 'eval_steps_per_second': 18.069, 'epoch': 0.16}
{'loss': 1.7597, 'grad_norm': 0.1083303689956665, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.255533218383789, 'eval_runtime': 3.4871, 'eval_samples_per_second': 286.768, 'eval_steps_per_second': 18.066, 'epoch': 0.2}
{'loss': 1.7234, 'grad_norm': 0.09767111390829086, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.2324321269989014, 'eval_runtime': 3.5002, 'eval_samples_per_second': 285.699, 'eval_steps_per_second': 17.999, 'epoch': 0.24}
{'loss': 1.755, 'grad_norm': 0.10708395391702652, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.267317295074463, 'eval_runtime': 3.4926, 'eval_samples_per_second': 286.32, 'eval_steps_per_second': 18.038, 'epoch': 0.28}
{'loss': 1.6942, 'grad_norm': 0.1500534564256668, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.3020498752593994, 'eval_runtime': 3.4875, 'eval_samples_per_second': 286.737, 'eval_steps_per_second': 18.064, 'epoch': 0.32}
{'loss': 1.5833, 'grad_norm': 0.12182167917490005, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.263821601867676, 'eval_runtime': 3.4931, 'eval_samples_per_second': 286.275, 'eval_steps_per_second': 18.035, 'epoch': 0.36}
{'loss': 1.6516, 'grad_norm': 0.12746135890483856, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.2233638763427734, 'eval_runtime': 3.4687, 'eval_samples_per_second': 288.289, 'eval_steps_per_second': 18.162, 'epoch': 0.4}
{'loss': 1.5205, 'grad_norm': 0.14538075029850006, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.1629297733306885, 'eval_runtime': 3.4508, 'eval_samples_per_second': 289.784, 'eval_steps_per_second': 18.256, 'epoch': 0.44}
{'loss': 1.5095, 'grad_norm': 0.1894289255142212, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.1159873008728027, 'eval_runtime': 3.4601, 'eval_samples_per_second': 289.006, 'eval_steps_per_second': 18.207, 'epoch': 0.48}
{'loss': 1.404, 'grad_norm': 0.13067834079265594, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.101121664047241, 'eval_runtime': 3.4509, 'eval_samples_per_second': 289.78, 'eval_steps_per_second': 18.256, 'epoch': 0.52}
{'loss': 1.465, 'grad_norm': 0.16131208837032318, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.0501649379730225, 'eval_runtime': 3.4465, 'eval_samples_per_second': 290.15, 'eval_steps_per_second': 18.279, 'epoch': 0.56}
{'loss': 1.4528, 'grad_norm': 0.18015125393867493, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.0576424598693848, 'eval_runtime': 3.4399, 'eval_samples_per_second': 290.704, 'eval_steps_per_second': 18.314, 'epoch': 0.6}
{'loss': 1.3788, 'grad_norm': 0.15664353966712952, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.042714834213257, 'eval_runtime': 3.4419, 'eval_samples_per_second': 290.535, 'eval_steps_per_second': 18.304, 'epoch': 0.64}
{'loss': 1.3254, 'grad_norm': 0.18954411149024963, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9874842166900635, 'eval_runtime': 3.4733, 'eval_samples_per_second': 287.907, 'eval_steps_per_second': 18.138, 'epoch': 0.68}
{'loss': 1.3012, 'grad_norm': 0.16720245778560638, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.0547032356262207, 'eval_runtime': 3.4401, 'eval_samples_per_second': 290.686, 'eval_steps_per_second': 18.313, 'epoch': 0.72}
{'loss': 1.3074, 'grad_norm': 0.2292042225599289, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.084646463394165, 'eval_runtime': 3.4417, 'eval_samples_per_second': 290.551, 'eval_steps_per_second': 18.305, 'epoch': 0.76}
{'loss': 1.403, 'grad_norm': 0.16810134053230286, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.133835554122925, 'eval_runtime': 3.4435, 'eval_samples_per_second': 290.402, 'eval_steps_per_second': 18.295, 'epoch': 0.8}
{'loss': 1.3323, 'grad_norm': 0.17108620703220367, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.1641016006469727, 'eval_runtime': 3.4407, 'eval_samples_per_second': 290.637, 'eval_steps_per_second': 18.31, 'epoch': 0.84}
{'loss': 1.3443, 'grad_norm': 0.2347121387720108, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.15500545501709, 'eval_runtime': 3.4601, 'eval_samples_per_second': 289.006, 'eval_steps_per_second': 18.207, 'epoch': 0.88}
{'loss': 1.4031, 'grad_norm': 0.1469363421201706, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1955182552337646, 'eval_runtime': 3.4739, 'eval_samples_per_second': 287.864, 'eval_steps_per_second': 18.135, 'epoch': 0.92}
{'loss': 1.216, 'grad_norm': 0.2523117959499359, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.2029268741607666, 'eval_runtime': 3.4699, 'eval_samples_per_second': 288.191, 'eval_steps_per_second': 18.156, 'epoch': 0.96}
{'loss': 1.2666, 'grad_norm': 0.30165350437164307, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.2098183631896973, 'eval_runtime': 3.4574, 'eval_samples_per_second': 289.231, 'eval_steps_per_second': 18.222, 'epoch': 1.0}
{'train_runtime': 280.6895, 'train_samples_per_second': 35.623, 'train_steps_per_second': 2.227, 'train_loss': 1.6560369232177734, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5192432403564453, 2.449873208999634, 2.363872766494751, 2.2596895694732666, 2.255533218383789, 2.2324321269989014, 2.267317295074463, 2.3020498752593994, 2.263821601867676, 2.2233638763427734, 2.1629297733306885, 2.1159873008728027, 2.101121664047241, 2.0501649379730225, 2.0576424598693848, 2.042714834213257, 1.9874842166900635, 2.0547032356262207, 2.084646463394165, 2.133835554122925, 2.1641016006469727, 2.15500545501709, 2.1955182552337646, 2.2029268741607666, 2.2098183631896973], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.5192432403564453, 2.449873208999634, 2.363872766494751, 2.2596895694732666, 2.255533218383789, 2.2324321269989014, 2.267317295074463, 2.3020498752593994, 2.263821601867676, 2.2233638763427734, 2.1629297733306885, 2.1159873008728027, 2.101121664047241, 2.0501649379730225, 2.0576424598693848, 2.042714834213257, 1.9874842166900635, 2.0547032356262207, 2.084646463394165, 2.133835554122925, 2.1641016006469727, 2.15500545501709, 2.1955182552337646, 2.2029268741607666, 2.2098183631896973]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.9681906700134277
current iteration best possible eval_loss (full train run):  -2.2098183631896973
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.3740 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.3902252282757024
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.3931715573096528
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.2973959217022992
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.39685452068507665
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.3963847258603481
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0209, dtype=torch.float64), 0, 0, tensor(0.1421, dtype=torch.float64), tensor(0.0644, dtype=torch.float64), tensor(0.5616, dtype=torch.float64), 0, 0, tensor(0.2111, dtype=torch.float64), 16, 1, 0, 0, 1, 1, 2, 0.049317297166327156, 15.43183295064189, 1]
normalized proposed parameters for next round by BO: [tensor(0.0209, dtype=torch.float64), tensor(1.6675e-17, dtype=torch.float64), tensor(3.1843e-18, dtype=torch.float64), tensor(0.1421, dtype=torch.float64), tensor(0.0644, dtype=torch.float64), tensor(0.5616, dtype=torch.float64), tensor(7.0057e-18, dtype=torch.float64), tensor(1.3836e-17, dtype=torch.float64), tensor(0.2111, dtype=torch.float64), tensor(0.4861, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.4932, dtype=torch.float64), tensor(0.3215, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.021
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.142
  triviaqa: 0.064
  truthfulqa_gen: 0.562
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.211

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.049317297166327156,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (15.43183295064189,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.049317297166327156
lora alpha:  15.43183295064189
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,441,792 || all params: 8,031,703,040 || trainable%: 0.0180
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8878, 'grad_norm': 9.149285316467285, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2080421447753906, 'eval_runtime': 4.1493, 'eval_samples_per_second': 241.003, 'eval_steps_per_second': 15.183, 'epoch': 0.04}
{'loss': 1.4685, 'grad_norm': 4.5241594314575195, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5778671503067017, 'eval_runtime': 3.1907, 'eval_samples_per_second': 313.408, 'eval_steps_per_second': 19.745, 'epoch': 0.08}
{'loss': 1.005, 'grad_norm': 1.6991523504257202, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3630527257919312, 'eval_runtime': 3.1886, 'eval_samples_per_second': 313.614, 'eval_steps_per_second': 19.758, 'epoch': 0.12}
{'loss': 0.9369, 'grad_norm': 2.100905656814575, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.185669183731079, 'eval_runtime': 3.1655, 'eval_samples_per_second': 315.901, 'eval_steps_per_second': 19.902, 'epoch': 0.16}
{'loss': 0.8543, 'grad_norm': 1.7683000564575195, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1799360513687134, 'eval_runtime': 3.1609, 'eval_samples_per_second': 316.361, 'eval_steps_per_second': 19.931, 'epoch': 0.2}
{'loss': 0.7843, 'grad_norm': 1.502127766609192, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.172837257385254, 'eval_runtime': 3.169, 'eval_samples_per_second': 315.558, 'eval_steps_per_second': 19.88, 'epoch': 0.24}
{'loss': 0.7638, 'grad_norm': 1.5419933795928955, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.13096284866333, 'eval_runtime': 3.1656, 'eval_samples_per_second': 315.901, 'eval_steps_per_second': 19.902, 'epoch': 0.28}
{'loss': 0.7359, 'grad_norm': 1.6899369955062866, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.135053277015686, 'eval_runtime': 3.1683, 'eval_samples_per_second': 315.63, 'eval_steps_per_second': 19.885, 'epoch': 0.32}
{'loss': 0.7125, 'grad_norm': 2.217757225036621, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1187224388122559, 'eval_runtime': 3.1773, 'eval_samples_per_second': 314.731, 'eval_steps_per_second': 19.828, 'epoch': 0.36}
{'loss': 0.7375, 'grad_norm': 1.2854161262512207, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.152383804321289, 'eval_runtime': 3.1957, 'eval_samples_per_second': 312.923, 'eval_steps_per_second': 19.714, 'epoch': 0.4}
{'loss': 0.6724, 'grad_norm': 1.435131311416626, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1199404001235962, 'eval_runtime': 3.1758, 'eval_samples_per_second': 314.879, 'eval_steps_per_second': 19.837, 'epoch': 0.44}
{'loss': 0.698, 'grad_norm': 2.1265621185302734, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0914911031723022, 'eval_runtime': 3.1809, 'eval_samples_per_second': 314.372, 'eval_steps_per_second': 19.805, 'epoch': 0.48}
{'loss': 0.6727, 'grad_norm': 1.9652729034423828, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.093793511390686, 'eval_runtime': 3.1813, 'eval_samples_per_second': 314.339, 'eval_steps_per_second': 19.803, 'epoch': 0.52}
{'loss': 0.6591, 'grad_norm': 1.3940980434417725, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.10179603099823, 'eval_runtime': 3.1789, 'eval_samples_per_second': 314.575, 'eval_steps_per_second': 19.818, 'epoch': 0.56}
{'loss': 0.6536, 'grad_norm': 1.7875579595565796, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0953154563903809, 'eval_runtime': 3.178, 'eval_samples_per_second': 314.66, 'eval_steps_per_second': 19.824, 'epoch': 0.6}
{'loss': 0.6111, 'grad_norm': 2.2038583755493164, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1032803058624268, 'eval_runtime': 3.1843, 'eval_samples_per_second': 314.038, 'eval_steps_per_second': 19.784, 'epoch': 0.64}
{'loss': 0.578, 'grad_norm': 1.839699387550354, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0883303880691528, 'eval_runtime': 3.1737, 'eval_samples_per_second': 315.088, 'eval_steps_per_second': 19.851, 'epoch': 0.68}
{'loss': 0.5845, 'grad_norm': 1.8151600360870361, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.089292287826538, 'eval_runtime': 3.195, 'eval_samples_per_second': 312.992, 'eval_steps_per_second': 19.719, 'epoch': 0.72}
{'loss': 0.5637, 'grad_norm': 2.7271111011505127, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1008931398391724, 'eval_runtime': 3.1732, 'eval_samples_per_second': 315.138, 'eval_steps_per_second': 19.854, 'epoch': 0.76}
{'loss': 0.5426, 'grad_norm': 2.1951406002044678, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1066396236419678, 'eval_runtime': 3.1805, 'eval_samples_per_second': 314.413, 'eval_steps_per_second': 19.808, 'epoch': 0.8}
{'loss': 0.5405, 'grad_norm': 2.2941012382507324, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0874204635620117, 'eval_runtime': 3.208, 'eval_samples_per_second': 311.725, 'eval_steps_per_second': 19.639, 'epoch': 0.84}
{'loss': 0.5554, 'grad_norm': 2.1357743740081787, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0988826751708984, 'eval_runtime': 3.2038, 'eval_samples_per_second': 312.126, 'eval_steps_per_second': 19.664, 'epoch': 0.88}
{'loss': 0.5401, 'grad_norm': 1.817543864250183, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0858038663864136, 'eval_runtime': 3.2042, 'eval_samples_per_second': 312.087, 'eval_steps_per_second': 19.661, 'epoch': 0.92}
{'loss': 0.5279, 'grad_norm': 2.14631986618042, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0869951248168945, 'eval_runtime': 3.2183, 'eval_samples_per_second': 310.719, 'eval_steps_per_second': 19.575, 'epoch': 0.96}
{'loss': 0.5306, 'grad_norm': 2.4129109382629395, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0852500200271606, 'eval_runtime': 3.2176, 'eval_samples_per_second': 310.788, 'eval_steps_per_second': 19.58, 'epoch': 1.0}
{'train_runtime': 212.9499, 'train_samples_per_second': 46.945, 'train_steps_per_second': 2.935, 'train_loss': 0.8326769378662109, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2080421447753906, 1.5778671503067017, 1.3630527257919312, 1.185669183731079, 1.1799360513687134, 1.172837257385254, 1.13096284866333, 1.135053277015686, 1.1187224388122559, 1.152383804321289, 1.1199404001235962, 1.0914911031723022, 1.093793511390686, 1.10179603099823, 1.0953154563903809, 1.1032803058624268, 1.0883303880691528, 1.089292287826538, 1.1008931398391724, 1.1066396236419678, 1.0874204635620117, 1.0988826751708984, 1.0858038663864136, 1.0869951248168945, 1.0852500200271606], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2080421447753906, 1.5778671503067017, 1.3630527257919312, 1.185669183731079, 1.1799360513687134, 1.172837257385254, 1.13096284866333, 1.135053277015686, 1.1187224388122559, 1.152383804321289, 1.1199404001235962, 1.0914911031723022, 1.093793511390686, 1.10179603099823, 1.0953154563903809, 1.1032803058624268, 1.0883303880691528, 1.089292287826538, 1.1008931398391724, 1.1066396236419678, 1.0874204635620117, 1.0988826751708984, 1.0858038663864136, 1.0869951248168945, 1.0852500200271606]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.755387306213379
current iteration best possible eval_loss (full train run):  -1.0852500200271606
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.1822 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.1679734098214003
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.4181678683578769
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.3468563547707375
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.3466525827187035
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.3469317357118227
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2497, dtype=torch.float64), tensor(0.0587, dtype=torch.float64), tensor(0.0309, dtype=torch.float64), 0, 0, tensor(0.2837, dtype=torch.float64), 0, 0, tensor(0.3688, dtype=torch.float64), 32, 0, 0, 1, 1, 1, 2, 1.3010426069826055e-19, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.2497, dtype=torch.float64), tensor(0.0587, dtype=torch.float64), tensor(0.0309, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0082, dtype=torch.float64), tensor(0.2837, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2449e-16, dtype=torch.float64), tensor(0.3688, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.3010e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.25
  gsm8k: 0.059
  rowan_hellaswag: 0.031
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.284
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.369

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.3010426069826055e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  2
lora dropout:  1.3010426069826055e-19
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,538,944 || all params: 8,033,800,192 || trainable%: 0.0441
length of training data:  9915
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.0188, 'grad_norm': 2.1509177684783936, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.2593884468078613, 'eval_runtime': 6.0851, 'eval_samples_per_second': 164.335, 'eval_steps_per_second': 10.353, 'epoch': 0.04}
{'loss': 1.9496, 'grad_norm': 2.652367115020752, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.207632303237915, 'eval_runtime': 3.6023, 'eval_samples_per_second': 277.601, 'eval_steps_per_second': 17.489, 'epoch': 0.08}
{'loss': 1.1782, 'grad_norm': 0.5087034106254578, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.7445178031921387, 'eval_runtime': 3.6145, 'eval_samples_per_second': 276.661, 'eval_steps_per_second': 17.43, 'epoch': 0.12}
{'loss': 1.0589, 'grad_norm': 0.4089052379131317, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.7601747512817383, 'eval_runtime': 3.5949, 'eval_samples_per_second': 278.169, 'eval_steps_per_second': 17.525, 'epoch': 0.16}
{'loss': 0.997, 'grad_norm': 0.34999868273735046, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 1.7445976734161377, 'eval_runtime': 3.5912, 'eval_samples_per_second': 278.461, 'eval_steps_per_second': 17.543, 'epoch': 0.2}
{'loss': 0.9525, 'grad_norm': 0.3186268210411072, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 1.7784370183944702, 'eval_runtime': 3.6171, 'eval_samples_per_second': 276.464, 'eval_steps_per_second': 17.417, 'epoch': 0.24}
{'loss': 0.8886, 'grad_norm': 0.3517620265483856, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 1.742687702178955, 'eval_runtime': 3.6388, 'eval_samples_per_second': 274.815, 'eval_steps_per_second': 17.313, 'epoch': 0.28}
{'loss': 0.9208, 'grad_norm': 0.4351288080215454, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 1.8520363569259644, 'eval_runtime': 3.6373, 'eval_samples_per_second': 274.932, 'eval_steps_per_second': 17.321, 'epoch': 0.32}
{'loss': 0.9005, 'grad_norm': 0.3851103186607361, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 1.894882082939148, 'eval_runtime': 3.631, 'eval_samples_per_second': 275.408, 'eval_steps_per_second': 17.351, 'epoch': 0.36}
{'loss': 0.8861, 'grad_norm': 0.42288684844970703, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 1.8419240713119507, 'eval_runtime': 3.6148, 'eval_samples_per_second': 276.643, 'eval_steps_per_second': 17.428, 'epoch': 0.4}
{'loss': 0.8661, 'grad_norm': 0.39045092463493347, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 1.8144210577011108, 'eval_runtime': 3.6012, 'eval_samples_per_second': 277.689, 'eval_steps_per_second': 17.494, 'epoch': 0.44}
{'loss': 0.8439, 'grad_norm': 0.3907102942466736, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 1.9457451105117798, 'eval_runtime': 3.5999, 'eval_samples_per_second': 277.786, 'eval_steps_per_second': 17.501, 'epoch': 0.48}
{'loss': 0.8652, 'grad_norm': 0.45338597893714905, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 1.9350227117538452, 'eval_runtime': 3.6084, 'eval_samples_per_second': 277.129, 'eval_steps_per_second': 17.459, 'epoch': 0.52}
{'loss': 0.8529, 'grad_norm': 0.4401141405105591, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 1.8345669507980347, 'eval_runtime': 3.5971, 'eval_samples_per_second': 277.999, 'eval_steps_per_second': 17.514, 'epoch': 0.56}
{'loss': 0.8603, 'grad_norm': 0.42863723635673523, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 1.9297566413879395, 'eval_runtime': 3.6021, 'eval_samples_per_second': 277.615, 'eval_steps_per_second': 17.49, 'epoch': 0.6}
{'loss': 0.8489, 'grad_norm': 0.5120440125465393, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 1.89414381980896, 'eval_runtime': 3.6017, 'eval_samples_per_second': 277.645, 'eval_steps_per_second': 17.492, 'epoch': 0.65}
{'loss': 0.8437, 'grad_norm': 0.5149111151695251, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 1.9168598651885986, 'eval_runtime': 3.6005, 'eval_samples_per_second': 277.742, 'eval_steps_per_second': 17.498, 'epoch': 0.69}
{'loss': 0.8457, 'grad_norm': 0.6162524819374084, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 1.8162060976028442, 'eval_runtime': 3.6001, 'eval_samples_per_second': 277.772, 'eval_steps_per_second': 17.5, 'epoch': 0.73}
{'loss': 0.8325, 'grad_norm': 0.5475214123725891, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 1.87394380569458, 'eval_runtime': 3.6025, 'eval_samples_per_second': 277.583, 'eval_steps_per_second': 17.488, 'epoch': 0.77}
{'loss': 0.8464, 'grad_norm': 0.6444864869117737, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 1.8867112398147583, 'eval_runtime': 3.6035, 'eval_samples_per_second': 277.507, 'eval_steps_per_second': 17.483, 'epoch': 0.81}
{'loss': 0.8025, 'grad_norm': 0.5319252610206604, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 1.9228925704956055, 'eval_runtime': 3.6042, 'eval_samples_per_second': 277.457, 'eval_steps_per_second': 17.48, 'epoch': 0.85}
{'loss': 0.8251, 'grad_norm': 0.5405390858650208, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 1.933173656463623, 'eval_runtime': 3.6411, 'eval_samples_per_second': 274.639, 'eval_steps_per_second': 17.302, 'epoch': 0.89}
{'loss': 0.8017, 'grad_norm': 0.8249714374542236, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 1.9337360858917236, 'eval_runtime': 3.6145, 'eval_samples_per_second': 276.664, 'eval_steps_per_second': 17.43, 'epoch': 0.93}
{'loss': 0.762, 'grad_norm': 0.6727458238601685, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 1.9513287544250488, 'eval_runtime': 3.6261, 'eval_samples_per_second': 275.775, 'eval_steps_per_second': 17.374, 'epoch': 0.97}
{'train_runtime': 294.1546, 'train_samples_per_second': 33.707, 'train_steps_per_second': 2.108, 'train_loss': 1.0510282331897367, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2593884468078613, 2.207632303237915, 1.7445178031921387, 1.7601747512817383, 1.7445976734161377, 1.7784370183944702, 1.742687702178955, 1.8520363569259644, 1.894882082939148, 1.8419240713119507, 1.8144210577011108, 1.9457451105117798, 1.9350227117538452, 1.8345669507980347, 1.9297566413879395, 1.89414381980896, 1.9168598651885986, 1.8162060976028442, 1.87394380569458, 1.8867112398147583, 1.9228925704956055, 1.933173656463623, 1.9337360858917236, 1.9513287544250488], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [3.2593884468078613, 2.207632303237915, 1.7445178031921387, 1.7601747512817383, 1.7445976734161377, 1.7784370183944702, 1.742687702178955, 1.8520363569259644, 1.894882082939148, 1.8419240713119507, 1.8144210577011108, 1.9457451105117798, 1.9350227117538452, 1.8345669507980347, 1.9297566413879395, 1.89414381980896, 1.9168598651885986, 1.8162060976028442, 1.87394380569458, 1.8867112398147583, 1.9228925704956055, 1.933173656463623, 1.9337360858917236, 1.9513287544250488]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.2874722480773926
current iteration best possible eval_loss (full train run):  -1.9513287544250488
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 24.8959 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3405855894088745, 0.2697674036026001, 0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2527220845222473, 0.2958891987800598, 0.798973798751831, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.42427369952201843, 0.12062281370162964, 0.7909995317459106, 0.5773974061012268]  ‚Üí  acq = -0.8688652488490884
X = [0.33804601430892944, 0.07152366638183594, 0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.505533754825592, 0.6193363070487976, 0.5416111350059509, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9287145733833313, 0.6309018731117249, 0.6364489793777466, 0.2499348521232605]  ‚Üí  acq = -0.9219892559342435
X = [0.6847147345542908, 0.7650787830352783, 0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5654566884040833, 0.984917938709259, 0.907849907875061, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4765317142009735, 0.10925418138504028, 0.4907895624637604, 0.022005975246429443]  ‚Üí  acq = -0.8689520185484114
X = [0.15384602546691895, 0.49969446659088135, 0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7945950031280518, 0.5358787775039673, 0.48621666431427, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9266265034675598, 0.8326297998428345, 0.6380935907363892, 0.630294680595398]  ‚Üí  acq = -0.8695867724130124
X = [0.4769304394721985, 0.06750988960266113, 0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6422049403190613, 0.6458637714385986, 0.9798121452331543, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.285779744386673, 0.47408175468444824, 0.7827727794647217, 0.2598608732223511]  ‚Üí  acq = -0.8655800791266939
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2082, dtype=torch.float64), tensor(0.0265, dtype=torch.float64), tensor(0.0743, dtype=torch.float64), tensor(0.1585, dtype=torch.float64), tensor(0.1244, dtype=torch.float64), tensor(0.2874, dtype=torch.float64), tensor(0.0570, dtype=torch.float64), tensor(0.0637, dtype=torch.float64), 0, 24, 0, 0, 1, 1, 1, 18, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.2082, dtype=torch.float64), tensor(0.0265, dtype=torch.float64), tensor(0.0743, dtype=torch.float64), tensor(0.1585, dtype=torch.float64), tensor(0.1244, dtype=torch.float64), tensor(0.2874, dtype=torch.float64), tensor(0.0570, dtype=torch.float64), tensor(0.0637, dtype=torch.float64), tensor(8.7946e-17, dtype=torch.float64), tensor(0.7464, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1378, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.208
  gsm8k: 0.026
  rowan_hellaswag: 0.074
  sciq: 0.159
  triviaqa: 0.124
  truthfulqa_gen: 0.287
  wikitext: 0.057
  mmlu: 0.064
  arc_challenge: 0

LoRA Parameters:
  lora_r: (18,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  18
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 23,887,872 || all params: 8,054,149,120 || trainable%: 0.2966
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0609, 'grad_norm': 1.864882469177246, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6866447925567627, 'eval_runtime': 3.4342, 'eval_samples_per_second': 291.192, 'eval_steps_per_second': 18.345, 'epoch': 0.04}
{'loss': 1.43, 'grad_norm': 1.3033311367034912, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.328776478767395, 'eval_runtime': 3.4117, 'eval_samples_per_second': 293.107, 'eval_steps_per_second': 18.466, 'epoch': 0.08}
{'loss': 1.3276, 'grad_norm': 0.7882769107818604, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1642037630081177, 'eval_runtime': 3.3959, 'eval_samples_per_second': 294.473, 'eval_steps_per_second': 18.552, 'epoch': 0.12}
{'loss': 1.1959, 'grad_norm': 1.4467005729675293, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0824081897735596, 'eval_runtime': 3.4064, 'eval_samples_per_second': 293.562, 'eval_steps_per_second': 18.494, 'epoch': 0.16}
{'loss': 1.1058, 'grad_norm': 0.7587224245071411, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0669480562210083, 'eval_runtime': 3.4034, 'eval_samples_per_second': 293.825, 'eval_steps_per_second': 18.511, 'epoch': 0.2}
{'loss': 1.1641, 'grad_norm': 0.6703738570213318, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0577269792556763, 'eval_runtime': 3.4036, 'eval_samples_per_second': 293.809, 'eval_steps_per_second': 18.51, 'epoch': 0.24}
{'loss': 1.1202, 'grad_norm': 0.7571945190429688, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.053818941116333, 'eval_runtime': 3.4054, 'eval_samples_per_second': 293.655, 'eval_steps_per_second': 18.5, 'epoch': 0.28}
{'loss': 1.114, 'grad_norm': 0.9852341413497925, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0686184167861938, 'eval_runtime': 3.4103, 'eval_samples_per_second': 293.228, 'eval_steps_per_second': 18.473, 'epoch': 0.32}
{'loss': 1.0305, 'grad_norm': 1.072582483291626, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.05343496799469, 'eval_runtime': 3.4117, 'eval_samples_per_second': 293.112, 'eval_steps_per_second': 18.466, 'epoch': 0.36}
{'loss': 1.102, 'grad_norm': 0.7135087251663208, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0355219841003418, 'eval_runtime': 3.4096, 'eval_samples_per_second': 293.294, 'eval_steps_per_second': 18.478, 'epoch': 0.4}
{'loss': 1.0227, 'grad_norm': 0.8749772906303406, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0486021041870117, 'eval_runtime': 3.3997, 'eval_samples_per_second': 294.146, 'eval_steps_per_second': 18.531, 'epoch': 0.44}
{'loss': 1.0808, 'grad_norm': 0.7170262932777405, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.039772629737854, 'eval_runtime': 3.394, 'eval_samples_per_second': 294.64, 'eval_steps_per_second': 18.562, 'epoch': 0.48}
{'loss': 1.0761, 'grad_norm': 0.6386111378669739, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0458476543426514, 'eval_runtime': 3.3925, 'eval_samples_per_second': 294.772, 'eval_steps_per_second': 18.571, 'epoch': 0.52}
{'loss': 1.0036, 'grad_norm': 0.8404843211174011, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0430870056152344, 'eval_runtime': 3.3924, 'eval_samples_per_second': 294.772, 'eval_steps_per_second': 18.571, 'epoch': 0.56}
{'loss': 1.0519, 'grad_norm': 0.6742458939552307, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0323076248168945, 'eval_runtime': 3.3916, 'eval_samples_per_second': 294.849, 'eval_steps_per_second': 18.576, 'epoch': 0.6}
{'loss': 1.0036, 'grad_norm': 0.547950804233551, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0376441478729248, 'eval_runtime': 3.3942, 'eval_samples_per_second': 294.617, 'eval_steps_per_second': 18.561, 'epoch': 0.64}
{'loss': 1.0208, 'grad_norm': 0.7295571565628052, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0301591157913208, 'eval_runtime': 3.393, 'eval_samples_per_second': 294.723, 'eval_steps_per_second': 18.568, 'epoch': 0.68}
{'loss': 0.9791, 'grad_norm': 0.6186544299125671, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0335404872894287, 'eval_runtime': 3.3897, 'eval_samples_per_second': 295.007, 'eval_steps_per_second': 18.585, 'epoch': 0.72}
{'loss': 0.9898, 'grad_norm': 0.8464823365211487, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.028225064277649, 'eval_runtime': 3.3957, 'eval_samples_per_second': 294.492, 'eval_steps_per_second': 18.553, 'epoch': 0.76}
{'loss': 0.9182, 'grad_norm': 0.7367871403694153, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.023388147354126, 'eval_runtime': 3.3899, 'eval_samples_per_second': 294.998, 'eval_steps_per_second': 18.585, 'epoch': 0.8}
{'loss': 1.0824, 'grad_norm': 0.5326001048088074, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0243247747421265, 'eval_runtime': 3.3909, 'eval_samples_per_second': 294.906, 'eval_steps_per_second': 18.579, 'epoch': 0.84}
{'loss': 0.9324, 'grad_norm': 0.7563464641571045, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0277081727981567, 'eval_runtime': 3.3937, 'eval_samples_per_second': 294.661, 'eval_steps_per_second': 18.564, 'epoch': 0.88}
{'loss': 0.996, 'grad_norm': 0.5707030892372131, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0255008935928345, 'eval_runtime': 3.3965, 'eval_samples_per_second': 294.421, 'eval_steps_per_second': 18.549, 'epoch': 0.92}
{'loss': 0.9968, 'grad_norm': 0.621717631816864, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0236475467681885, 'eval_runtime': 3.4014, 'eval_samples_per_second': 293.993, 'eval_steps_per_second': 18.522, 'epoch': 0.96}
{'loss': 0.9438, 'grad_norm': 0.6304807066917419, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0225725173950195, 'eval_runtime': 3.4117, 'eval_samples_per_second': 293.109, 'eval_steps_per_second': 18.466, 'epoch': 1.0}
{'train_runtime': 289.1744, 'train_samples_per_second': 34.571, 'train_steps_per_second': 2.161, 'train_loss': 1.1499619842529296, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6866447925567627, 1.328776478767395, 1.1642037630081177, 1.0824081897735596, 1.0669480562210083, 1.0577269792556763, 1.053818941116333, 1.0686184167861938, 1.05343496799469, 1.0355219841003418, 1.0486021041870117, 1.039772629737854, 1.0458476543426514, 1.0430870056152344, 1.0323076248168945, 1.0376441478729248, 1.0301591157913208, 1.0335404872894287, 1.028225064277649, 1.023388147354126, 1.0243247747421265, 1.0277081727981567, 1.0255008935928345, 1.0236475467681885, 1.0225725173950195], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6866447925567627, 1.328776478767395, 1.1642037630081177, 1.0824081897735596, 1.0669480562210083, 1.0577269792556763, 1.053818941116333, 1.0686184167861938, 1.05343496799469, 1.0355219841003418, 1.0486021041870117, 1.039772629737854, 1.0458476543426514, 1.0430870056152344, 1.0323076248168945, 1.0376441478729248, 1.0301591157913208, 1.0335404872894287, 1.028225064277649, 1.023388147354126, 1.0243247747421265, 1.0277081727981567, 1.0255008935928345, 1.0236475467681885, 1.0225725173950195]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.222399950027466
current iteration best possible eval_loss (full train run):  -1.0225725173950195
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.5989 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -0.3247696636788364
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -0.40272606911082165
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -0.4710408106049746
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -0.5561979406765332
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -0.4709245605996977
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0106, dtype=torch.float64), tensor(0.0615, dtype=torch.float64), 0, 0, tensor(0.6550, dtype=torch.float64), tensor(0.2730, dtype=torch.float64), 0, 0, 0, 16, 0, 0, 1, 1, 1, 35, 0.06290496016505076, 13.840942762771705, 0]
normalized proposed parameters for next round by BO: [tensor(0.0106, dtype=torch.float64), tensor(0.0615, dtype=torch.float64), tensor(6.6760e-18, dtype=torch.float64), tensor(2.6040e-17, dtype=torch.float64), tensor(0.6550, dtype=torch.float64), tensor(0.2730, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.6135e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4845, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2699, dtype=torch.float64), tensor(0.6290, dtype=torch.float64), tensor(0.2884, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.011
  gsm8k: 0.061
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.655
  truthfulqa_gen: 0.273
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (35,)
  lora_dropout: (0.06290496016505076,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (13.840942762771705,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  35
lora dropout:  0.06290496016505076
lora alpha:  13.840942762771705
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 30,965,760 || all params: 8,061,227,008 || trainable%: 0.3841
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9851, 'grad_norm': 1.579538106918335, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.3907651901245117, 'eval_runtime': 4.0419, 'eval_samples_per_second': 247.407, 'eval_steps_per_second': 15.587, 'epoch': 0.04}
{'loss': 1.55, 'grad_norm': 0.5123990178108215, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1776621341705322, 'eval_runtime': 3.2885, 'eval_samples_per_second': 304.086, 'eval_steps_per_second': 19.157, 'epoch': 0.08}
{'loss': 1.0918, 'grad_norm': 0.359042763710022, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1019203662872314, 'eval_runtime': 3.2927, 'eval_samples_per_second': 303.698, 'eval_steps_per_second': 19.133, 'epoch': 0.12}
{'loss': 0.9963, 'grad_norm': 0.2894240617752075, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0918177366256714, 'eval_runtime': 3.283, 'eval_samples_per_second': 304.601, 'eval_steps_per_second': 19.19, 'epoch': 0.16}
{'loss': 0.978, 'grad_norm': 0.2663750648498535, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.070459008216858, 'eval_runtime': 3.2893, 'eval_samples_per_second': 304.016, 'eval_steps_per_second': 19.153, 'epoch': 0.2}
{'loss': 0.9585, 'grad_norm': 0.32813116908073425, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0578185319900513, 'eval_runtime': 3.2916, 'eval_samples_per_second': 303.804, 'eval_steps_per_second': 19.14, 'epoch': 0.24}
{'loss': 0.9477, 'grad_norm': 0.27358508110046387, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0657763481140137, 'eval_runtime': 3.2983, 'eval_samples_per_second': 303.19, 'eval_steps_per_second': 19.101, 'epoch': 0.28}
{'loss': 0.9694, 'grad_norm': 0.2878674864768982, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0577038526535034, 'eval_runtime': 3.2961, 'eval_samples_per_second': 303.393, 'eval_steps_per_second': 19.114, 'epoch': 0.32}
{'loss': 0.9401, 'grad_norm': 0.30493658781051636, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0517035722732544, 'eval_runtime': 3.2965, 'eval_samples_per_second': 303.35, 'eval_steps_per_second': 19.111, 'epoch': 0.36}
{'loss': 0.9247, 'grad_norm': 0.3150539696216583, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0441675186157227, 'eval_runtime': 3.3132, 'eval_samples_per_second': 301.82, 'eval_steps_per_second': 19.015, 'epoch': 0.4}
{'loss': 0.9526, 'grad_norm': 0.270509272813797, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0441317558288574, 'eval_runtime': 3.3218, 'eval_samples_per_second': 301.043, 'eval_steps_per_second': 18.966, 'epoch': 0.44}
{'loss': 0.9493, 'grad_norm': 0.2634078860282898, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0412206649780273, 'eval_runtime': 3.3242, 'eval_samples_per_second': 300.822, 'eval_steps_per_second': 18.952, 'epoch': 0.48}
{'loss': 0.9067, 'grad_norm': 0.3278297185897827, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.037096381187439, 'eval_runtime': 3.3269, 'eval_samples_per_second': 300.583, 'eval_steps_per_second': 18.937, 'epoch': 0.52}
{'loss': 0.8907, 'grad_norm': 0.3477707505226135, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0391236543655396, 'eval_runtime': 3.3111, 'eval_samples_per_second': 302.012, 'eval_steps_per_second': 19.027, 'epoch': 0.56}
{'loss': 0.8926, 'grad_norm': 0.3440139591693878, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0362480878829956, 'eval_runtime': 3.3248, 'eval_samples_per_second': 300.772, 'eval_steps_per_second': 18.949, 'epoch': 0.6}
{'loss': 0.889, 'grad_norm': 0.3901107907295227, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0349855422973633, 'eval_runtime': 3.342, 'eval_samples_per_second': 299.225, 'eval_steps_per_second': 18.851, 'epoch': 0.64}
{'loss': 0.8926, 'grad_norm': 0.3117181360721588, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0347753763198853, 'eval_runtime': 3.3402, 'eval_samples_per_second': 299.384, 'eval_steps_per_second': 18.861, 'epoch': 0.68}
{'loss': 0.8729, 'grad_norm': 0.3887988030910492, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0308068990707397, 'eval_runtime': 3.3508, 'eval_samples_per_second': 298.437, 'eval_steps_per_second': 18.802, 'epoch': 0.72}
{'loss': 0.901, 'grad_norm': 0.2942015826702118, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0288372039794922, 'eval_runtime': 3.3361, 'eval_samples_per_second': 299.75, 'eval_steps_per_second': 18.884, 'epoch': 0.76}
{'loss': 0.8514, 'grad_norm': 0.38912272453308105, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.030084252357483, 'eval_runtime': 3.3508, 'eval_samples_per_second': 298.433, 'eval_steps_per_second': 18.801, 'epoch': 0.8}
{'loss': 0.8791, 'grad_norm': 0.39391806721687317, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0283344984054565, 'eval_runtime': 3.3459, 'eval_samples_per_second': 298.874, 'eval_steps_per_second': 18.829, 'epoch': 0.84}
{'loss': 0.8425, 'grad_norm': 0.40342485904693604, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0280661582946777, 'eval_runtime': 3.3477, 'eval_samples_per_second': 298.716, 'eval_steps_per_second': 18.819, 'epoch': 0.88}
{'loss': 0.862, 'grad_norm': 0.40335288643836975, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.026455044746399, 'eval_runtime': 3.3221, 'eval_samples_per_second': 301.013, 'eval_steps_per_second': 18.964, 'epoch': 0.92}
{'loss': 0.8427, 'grad_norm': 0.36372658610343933, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.02634596824646, 'eval_runtime': 3.3179, 'eval_samples_per_second': 301.395, 'eval_steps_per_second': 18.988, 'epoch': 0.96}
{'loss': 0.8277, 'grad_norm': 0.32601919770240784, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0258206129074097, 'eval_runtime': 3.3344, 'eval_samples_per_second': 299.902, 'eval_steps_per_second': 18.894, 'epoch': 1.0}
{'train_runtime': 198.3678, 'train_samples_per_second': 50.401, 'train_steps_per_second': 3.151, 'train_loss': 1.0637766998291016, 'epoch': 1.0}
train_results:  {'eval_loss': [2.3907651901245117, 1.1776621341705322, 1.1019203662872314, 1.0918177366256714, 1.070459008216858, 1.0578185319900513, 1.0657763481140137, 1.0577038526535034, 1.0517035722732544, 1.0441675186157227, 1.0441317558288574, 1.0412206649780273, 1.037096381187439, 1.0391236543655396, 1.0362480878829956, 1.0349855422973633, 1.0347753763198853, 1.0308068990707397, 1.0288372039794922, 1.030084252357483, 1.0283344984054565, 1.0280661582946777, 1.026455044746399, 1.02634596824646, 1.0258206129074097], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.3907651901245117, 1.1776621341705322, 1.1019203662872314, 1.0918177366256714, 1.070459008216858, 1.0578185319900513, 1.0657763481140137, 1.0577038526535034, 1.0517035722732544, 1.0441675186157227, 1.0441317558288574, 1.0412206649780273, 1.037096381187439, 1.0391236543655396, 1.0362480878829956, 1.0349855422973633, 1.0347753763198853, 1.0308068990707397, 1.0288372039794922, 1.030084252357483, 1.0283344984054565, 1.0280661582946777, 1.026455044746399, 1.02634596824646, 1.0258206129074097]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.3334760665893555
current iteration best possible eval_loss (full train run):  -1.0258206129074097
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.5763 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -0.712028048006232
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -0.712028048006232
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -0.712028048006232
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.712028048006232
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -0.712028048006232
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4714, dtype=torch.float64), tensor(0.0521, dtype=torch.float64), tensor(0.1950, dtype=torch.float64), 0, 0, tensor(0.2814, dtype=torch.float64), 0, 0, 30, 0, 0, 1, 1, 0, 63, 0.08518476853207853, 22.528118046379586, 0]
normalized proposed parameters for next round by BO: [tensor(2.5764e-16, dtype=torch.float64), tensor(0.4714, dtype=torch.float64), tensor(0.0521, dtype=torch.float64), tensor(0.1950, dtype=torch.float64), tensor(1.0777e-16, dtype=torch.float64), tensor(3.6881e-16, dtype=torch.float64), tensor(0.2814, dtype=torch.float64), tensor(6.2356e-17, dtype=torch.float64), tensor(6.2868e-17, dtype=torch.float64), tensor(0.9316, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4912, dtype=torch.float64), tensor(0.8518, dtype=torch.float64), tensor(0.4693, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.471
  rowan_hellaswag: 0.052
  sciq: 0.195
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.281
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (63,)
  lora_dropout: (0.08518476853207853,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (22.528118046379586,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  63
lora dropout:  0.08518476853207853
lora alpha:  22.528118046379586
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 69,672,960 || all params: 8,099,934,208 || trainable%: 0.8602
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5116, 'grad_norm': 0.5916744470596313, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2274138927459717, 'eval_runtime': 3.4718, 'eval_samples_per_second': 288.038, 'eval_steps_per_second': 18.146, 'epoch': 0.04}
{'loss': 1.3805, 'grad_norm': 0.4281788766384125, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6153439283370972, 'eval_runtime': 3.4692, 'eval_samples_per_second': 288.252, 'eval_steps_per_second': 18.16, 'epoch': 0.08}
{'loss': 1.2888, 'grad_norm': 0.21079783141613007, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6430180072784424, 'eval_runtime': 3.4656, 'eval_samples_per_second': 288.554, 'eval_steps_per_second': 18.179, 'epoch': 0.12}
{'loss': 1.1321, 'grad_norm': 0.19840604066848755, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.656933069229126, 'eval_runtime': 3.4447, 'eval_samples_per_second': 290.302, 'eval_steps_per_second': 18.289, 'epoch': 0.16}
{'loss': 1.1701, 'grad_norm': 0.18017803132534027, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.710355281829834, 'eval_runtime': 3.4544, 'eval_samples_per_second': 289.488, 'eval_steps_per_second': 18.238, 'epoch': 0.2}
{'loss': 1.151, 'grad_norm': 0.18276134133338928, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6961297988891602, 'eval_runtime': 3.4476, 'eval_samples_per_second': 290.055, 'eval_steps_per_second': 18.273, 'epoch': 0.24}
{'loss': 1.1931, 'grad_norm': 0.20187245309352875, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7273527383804321, 'eval_runtime': 3.45, 'eval_samples_per_second': 289.855, 'eval_steps_per_second': 18.261, 'epoch': 0.28}
{'loss': 1.152, 'grad_norm': 0.2201969176530838, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6803025007247925, 'eval_runtime': 3.4527, 'eval_samples_per_second': 289.624, 'eval_steps_per_second': 18.246, 'epoch': 0.32}
{'loss': 1.1351, 'grad_norm': 0.158407524228096, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6613893508911133, 'eval_runtime': 3.4533, 'eval_samples_per_second': 289.578, 'eval_steps_per_second': 18.243, 'epoch': 0.36}
{'loss': 1.1185, 'grad_norm': 0.19087927043437958, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6848750114440918, 'eval_runtime': 3.4507, 'eval_samples_per_second': 289.797, 'eval_steps_per_second': 18.257, 'epoch': 0.4}
{'loss': 1.0704, 'grad_norm': 0.1916380524635315, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6931637525558472, 'eval_runtime': 3.4561, 'eval_samples_per_second': 289.345, 'eval_steps_per_second': 18.229, 'epoch': 0.44}
{'loss': 1.1017, 'grad_norm': 0.222966268658638, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.699832558631897, 'eval_runtime': 3.4555, 'eval_samples_per_second': 289.392, 'eval_steps_per_second': 18.232, 'epoch': 0.48}
{'loss': 1.0965, 'grad_norm': 0.18631087243556976, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6748312711715698, 'eval_runtime': 3.4577, 'eval_samples_per_second': 289.207, 'eval_steps_per_second': 18.22, 'epoch': 0.52}
{'loss': 1.0716, 'grad_norm': 0.20346738398075104, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7276536226272583, 'eval_runtime': 3.4572, 'eval_samples_per_second': 289.248, 'eval_steps_per_second': 18.223, 'epoch': 0.56}
{'loss': 1.0964, 'grad_norm': 0.21547478437423706, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7388081550598145, 'eval_runtime': 3.4553, 'eval_samples_per_second': 289.412, 'eval_steps_per_second': 18.233, 'epoch': 0.6}
{'loss': 1.0192, 'grad_norm': 0.19860975444316864, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7375245094299316, 'eval_runtime': 3.4909, 'eval_samples_per_second': 286.455, 'eval_steps_per_second': 18.047, 'epoch': 0.64}
{'loss': 1.0671, 'grad_norm': 0.23242805898189545, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.756474494934082, 'eval_runtime': 3.4811, 'eval_samples_per_second': 287.264, 'eval_steps_per_second': 18.098, 'epoch': 0.68}
{'loss': 1.1065, 'grad_norm': 0.17117460072040558, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7333754301071167, 'eval_runtime': 3.4738, 'eval_samples_per_second': 287.869, 'eval_steps_per_second': 18.136, 'epoch': 0.72}
{'loss': 1.095, 'grad_norm': 0.2224590927362442, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.73283052444458, 'eval_runtime': 3.4621, 'eval_samples_per_second': 288.843, 'eval_steps_per_second': 18.197, 'epoch': 0.76}
{'loss': 1.0588, 'grad_norm': 0.20161129534244537, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7195552587509155, 'eval_runtime': 3.4595, 'eval_samples_per_second': 289.057, 'eval_steps_per_second': 18.211, 'epoch': 0.8}
{'loss': 1.0804, 'grad_norm': 0.19165295362472534, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7330119609832764, 'eval_runtime': 3.4745, 'eval_samples_per_second': 287.812, 'eval_steps_per_second': 18.132, 'epoch': 0.84}
{'loss': 1.0398, 'grad_norm': 0.22172707319259644, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.752294659614563, 'eval_runtime': 3.4637, 'eval_samples_per_second': 288.705, 'eval_steps_per_second': 18.188, 'epoch': 0.88}
{'loss': 1.0881, 'grad_norm': 0.2008335143327713, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7440887689590454, 'eval_runtime': 3.4608, 'eval_samples_per_second': 288.952, 'eval_steps_per_second': 18.204, 'epoch': 0.92}
{'loss': 1.0411, 'grad_norm': 0.22036312520503998, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7468079328536987, 'eval_runtime': 3.4511, 'eval_samples_per_second': 289.76, 'eval_steps_per_second': 18.255, 'epoch': 0.96}
{'loss': 1.0823, 'grad_norm': 0.2009618878364563, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7531780004501343, 'eval_runtime': 3.4513, 'eval_samples_per_second': 289.749, 'eval_steps_per_second': 18.254, 'epoch': 1.0}
{'train_runtime': 319.7818, 'train_samples_per_second': 31.268, 'train_steps_per_second': 1.954, 'train_loss': 1.173911737060547, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2274138927459717, 1.6153439283370972, 1.6430180072784424, 1.656933069229126, 1.710355281829834, 1.6961297988891602, 1.7273527383804321, 1.6803025007247925, 1.6613893508911133, 1.6848750114440918, 1.6931637525558472, 1.699832558631897, 1.6748312711715698, 1.7276536226272583, 1.7388081550598145, 1.7375245094299316, 1.756474494934082, 1.7333754301071167, 1.73283052444458, 1.7195552587509155, 1.7330119609832764, 1.752294659614563, 1.7440887689590454, 1.7468079328536987, 1.7531780004501343], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2274138927459717, 1.6153439283370972, 1.6430180072784424, 1.656933069229126, 1.710355281829834, 1.6961297988891602, 1.7273527383804321, 1.6803025007247925, 1.6613893508911133, 1.6848750114440918, 1.6931637525558472, 1.699832558631897, 1.6748312711715698, 1.7276536226272583, 1.7388081550598145, 1.7375245094299316, 1.756474494934082, 1.7333754301071167, 1.73283052444458, 1.7195552587509155, 1.7330119609832764, 1.752294659614563, 1.7440887689590454, 1.7468079328536987, 1.7531780004501343]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.3616437911987305
current iteration best possible eval_loss (full train run):  -1.7531780004501343
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.1907 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.6883760518079935
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.6934643961066496
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.6954206605642845
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.6065156981988853
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.688424818827692
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2791, dtype=torch.float64), tensor(0.1939, dtype=torch.float64), 0, 0, tensor(0.0946, dtype=torch.float64), tensor(0.1232, dtype=torch.float64), tensor(0.2018, dtype=torch.float64), 0, tensor(0.1074, dtype=torch.float64), 18, 0, 0, 0, 1, 1, 18, 0.1, 20.515138757384108, 1]
normalized proposed parameters for next round by BO: [tensor(0.2791, dtype=torch.float64), tensor(0.1939, dtype=torch.float64), tensor(5.3032e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0946, dtype=torch.float64), tensor(0.1232, dtype=torch.float64), tensor(0.2018, dtype=torch.float64), tensor(1.6876e-17, dtype=torch.float64), tensor(0.1074, dtype=torch.float64), tensor(0.5608, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1389, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4274, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.279
  gsm8k: 0.194
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.095
  truthfulqa_gen: 0.123
  wikitext: 0.202
  mmlu: 0
  arc_challenge: 0.107

LoRA Parameters:
  lora_r: (18,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (20.515138757384108,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  18
lora dropout:  0.1
lora alpha:  20.515138757384108
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 11,943,936 || all params: 8,042,205,184 || trainable%: 0.1485
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0522, 'grad_norm': 2.106250286102295, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2166712284088135, 'eval_runtime': 3.1169, 'eval_samples_per_second': 320.833, 'eval_steps_per_second': 20.212, 'epoch': 0.04}
{'loss': 1.5851, 'grad_norm': 0.6389355063438416, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.515096664428711, 'eval_runtime': 3.1109, 'eval_samples_per_second': 321.449, 'eval_steps_per_second': 20.251, 'epoch': 0.08}
{'loss': 1.2413, 'grad_norm': 0.488521933555603, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3040566444396973, 'eval_runtime': 3.1084, 'eval_samples_per_second': 321.71, 'eval_steps_per_second': 20.268, 'epoch': 0.12}
{'loss': 1.2637, 'grad_norm': 0.47917428612709045, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2360782623291016, 'eval_runtime': 3.1155, 'eval_samples_per_second': 320.976, 'eval_steps_per_second': 20.222, 'epoch': 0.16}
{'loss': 1.1838, 'grad_norm': 0.4793946146965027, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.155288815498352, 'eval_runtime': 3.1179, 'eval_samples_per_second': 320.73, 'eval_steps_per_second': 20.206, 'epoch': 0.2}
{'loss': 1.0726, 'grad_norm': 0.531804621219635, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1123994588851929, 'eval_runtime': 3.1213, 'eval_samples_per_second': 320.379, 'eval_steps_per_second': 20.184, 'epoch': 0.24}
{'loss': 1.0823, 'grad_norm': 0.5656842589378357, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.088679552078247, 'eval_runtime': 3.1218, 'eval_samples_per_second': 320.327, 'eval_steps_per_second': 20.181, 'epoch': 0.28}
{'loss': 1.0186, 'grad_norm': 0.3652355968952179, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0759592056274414, 'eval_runtime': 3.1277, 'eval_samples_per_second': 319.719, 'eval_steps_per_second': 20.142, 'epoch': 0.32}
{'loss': 1.0942, 'grad_norm': 0.38741394877433777, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.074630618095398, 'eval_runtime': 3.128, 'eval_samples_per_second': 319.69, 'eval_steps_per_second': 20.14, 'epoch': 0.36}
{'loss': 1.0321, 'grad_norm': 0.386042982339859, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0662329196929932, 'eval_runtime': 3.1304, 'eval_samples_per_second': 319.446, 'eval_steps_per_second': 20.125, 'epoch': 0.4}
{'loss': 1.0669, 'grad_norm': 0.3963358700275421, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.054943323135376, 'eval_runtime': 3.1376, 'eval_samples_per_second': 318.715, 'eval_steps_per_second': 20.079, 'epoch': 0.44}
{'loss': 1.0607, 'grad_norm': 0.39099186658859253, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0681164264678955, 'eval_runtime': 3.1331, 'eval_samples_per_second': 319.17, 'eval_steps_per_second': 20.108, 'epoch': 0.48}
{'loss': 1.0527, 'grad_norm': 0.3659047484397888, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0508581399917603, 'eval_runtime': 3.1316, 'eval_samples_per_second': 319.33, 'eval_steps_per_second': 20.118, 'epoch': 0.52}
{'loss': 1.0461, 'grad_norm': 0.35785409808158875, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.057788372039795, 'eval_runtime': 3.1355, 'eval_samples_per_second': 318.926, 'eval_steps_per_second': 20.092, 'epoch': 0.56}
{'loss': 1.0144, 'grad_norm': 0.4159170389175415, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0558613538742065, 'eval_runtime': 3.1324, 'eval_samples_per_second': 319.247, 'eval_steps_per_second': 20.113, 'epoch': 0.6}
{'loss': 1.0121, 'grad_norm': 0.3936075270175934, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.052090048789978, 'eval_runtime': 3.1252, 'eval_samples_per_second': 319.984, 'eval_steps_per_second': 20.159, 'epoch': 0.64}
{'loss': 1.002, 'grad_norm': 0.36960116028785706, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0477436780929565, 'eval_runtime': 3.1343, 'eval_samples_per_second': 319.053, 'eval_steps_per_second': 20.1, 'epoch': 0.68}
{'loss': 1.006, 'grad_norm': 0.48734232783317566, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.050721526145935, 'eval_runtime': 3.1287, 'eval_samples_per_second': 319.617, 'eval_steps_per_second': 20.136, 'epoch': 0.72}
{'loss': 0.9862, 'grad_norm': 0.4754450023174286, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.045114517211914, 'eval_runtime': 3.1304, 'eval_samples_per_second': 319.447, 'eval_steps_per_second': 20.125, 'epoch': 0.76}
{'loss': 1.0259, 'grad_norm': 0.3795200288295746, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0393632650375366, 'eval_runtime': 3.1264, 'eval_samples_per_second': 319.857, 'eval_steps_per_second': 20.151, 'epoch': 0.8}
{'loss': 0.9988, 'grad_norm': 0.3968733847141266, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0432361364364624, 'eval_runtime': 3.1462, 'eval_samples_per_second': 317.839, 'eval_steps_per_second': 20.024, 'epoch': 0.84}
{'loss': 1.0319, 'grad_norm': 0.4540477991104126, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0459818840026855, 'eval_runtime': 3.1843, 'eval_samples_per_second': 314.037, 'eval_steps_per_second': 19.784, 'epoch': 0.88}
{'loss': 0.979, 'grad_norm': 0.46301913261413574, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0432789325714111, 'eval_runtime': 3.1788, 'eval_samples_per_second': 314.589, 'eval_steps_per_second': 19.819, 'epoch': 0.92}
{'loss': 0.9829, 'grad_norm': 0.401523232460022, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0426002740859985, 'eval_runtime': 3.16, 'eval_samples_per_second': 316.456, 'eval_steps_per_second': 19.937, 'epoch': 0.96}
{'loss': 1.0022, 'grad_norm': 0.566311776638031, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0419447422027588, 'eval_runtime': 3.2224, 'eval_samples_per_second': 310.329, 'eval_steps_per_second': 19.551, 'epoch': 1.0}
{'train_runtime': 275.7647, 'train_samples_per_second': 36.252, 'train_steps_per_second': 2.266, 'train_loss': 1.1557482055664063, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2166712284088135, 1.515096664428711, 1.3040566444396973, 1.2360782623291016, 1.155288815498352, 1.1123994588851929, 1.088679552078247, 1.0759592056274414, 1.074630618095398, 1.0662329196929932, 1.054943323135376, 1.0681164264678955, 1.0508581399917603, 1.057788372039795, 1.0558613538742065, 1.052090048789978, 1.0477436780929565, 1.050721526145935, 1.045114517211914, 1.0393632650375366, 1.0432361364364624, 1.0459818840026855, 1.0432789325714111, 1.0426002740859985, 1.0419447422027588], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2166712284088135, 1.515096664428711, 1.3040566444396973, 1.2360782623291016, 1.155288815498352, 1.1123994588851929, 1.088679552078247, 1.0759592056274414, 1.074630618095398, 1.0662329196929932, 1.054943323135376, 1.0681164264678955, 1.0508581399917603, 1.057788372039795, 1.0558613538742065, 1.052090048789978, 1.0477436780929565, 1.050721526145935, 1.045114517211914, 1.0393632650375366, 1.0432361364364624, 1.0459818840026855, 1.0432789325714111, 1.0426002740859985, 1.0419447422027588]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.0796561241149902
current iteration best possible eval_loss (full train run):  -1.0419447422027588
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.0718 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -0.8941643215406145
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.8949843872242016
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -0.8658460718482865
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -0.5994370254496255
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -0.8942465976625242
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1217, dtype=torch.float64), tensor(0.2089, dtype=torch.float64), 0, tensor(0.1575, dtype=torch.float64), tensor(0.1714, dtype=torch.float64), tensor(0.1312, dtype=torch.float64), tensor(0.0667, dtype=torch.float64), tensor(0.1290, dtype=torch.float64), tensor(0.0138, dtype=torch.float64), 16, 1, 0, 0, 1, 1, 92, 0.0511302672952863, 10.789477189814786, 1]
normalized proposed parameters for next round by BO: [tensor(0.1217, dtype=torch.float64), tensor(0.2089, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1575, dtype=torch.float64), tensor(0.1714, dtype=torch.float64), tensor(0.1312, dtype=torch.float64), tensor(0.0667, dtype=torch.float64), tensor(0.1290, dtype=torch.float64), tensor(0.0138, dtype=torch.float64), tensor(0.5128, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7158, dtype=torch.float64), tensor(0.5113, dtype=torch.float64), tensor(0.2248, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.122
  gsm8k: 0.209
  rowan_hellaswag: 0
  sciq: 0.157
  triviaqa: 0.171
  truthfulqa_gen: 0.131
  wikitext: 0.067
  mmlu: 0.129
  arc_challenge: 0.014

LoRA Parameters:
  lora_r: (92,)
  lora_dropout: (0.0511302672952863,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (10.789477189814786,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  92
lora dropout:  0.0511302672952863
lora alpha:  10.789477189814786
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 66,322,432 || all params: 8,096,583,680 || trainable%: 0.8191
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3369, 'grad_norm': 0.9893776178359985, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4515063762664795, 'eval_runtime': 5.5204, 'eval_samples_per_second': 181.147, 'eval_steps_per_second': 11.412, 'epoch': 0.04}
{'loss': 1.8069, 'grad_norm': 0.4526604115962982, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6777980327606201, 'eval_runtime': 3.2235, 'eval_samples_per_second': 310.224, 'eval_steps_per_second': 19.544, 'epoch': 0.08}
{'loss': 1.3404, 'grad_norm': 0.5076870918273926, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.374961018562317, 'eval_runtime': 3.2203, 'eval_samples_per_second': 310.527, 'eval_steps_per_second': 19.563, 'epoch': 0.12}
{'loss': 1.1453, 'grad_norm': 0.2256080061197281, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2631982564926147, 'eval_runtime': 3.2197, 'eval_samples_per_second': 310.587, 'eval_steps_per_second': 19.567, 'epoch': 0.16}
{'loss': 1.1746, 'grad_norm': 0.20110248029232025, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2314903736114502, 'eval_runtime': 3.2189, 'eval_samples_per_second': 310.666, 'eval_steps_per_second': 19.572, 'epoch': 0.2}
{'loss': 1.1391, 'grad_norm': 0.24357688426971436, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1872645616531372, 'eval_runtime': 3.2256, 'eval_samples_per_second': 310.019, 'eval_steps_per_second': 19.531, 'epoch': 0.24}
{'loss': 1.0629, 'grad_norm': 0.2530125379562378, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1486711502075195, 'eval_runtime': 3.2286, 'eval_samples_per_second': 309.73, 'eval_steps_per_second': 19.513, 'epoch': 0.28}
{'loss': 1.0839, 'grad_norm': 0.24838462471961975, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1072657108306885, 'eval_runtime': 3.2295, 'eval_samples_per_second': 309.644, 'eval_steps_per_second': 19.508, 'epoch': 0.32}
{'loss': 1.031, 'grad_norm': 0.18080107867717743, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.091783046722412, 'eval_runtime': 3.2213, 'eval_samples_per_second': 310.429, 'eval_steps_per_second': 19.557, 'epoch': 0.36}
{'loss': 1.0756, 'grad_norm': 0.16069738566875458, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0905561447143555, 'eval_runtime': 3.2102, 'eval_samples_per_second': 311.507, 'eval_steps_per_second': 19.625, 'epoch': 0.4}
{'loss': 1.0352, 'grad_norm': 0.20784799754619598, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0762338638305664, 'eval_runtime': 3.2106, 'eval_samples_per_second': 311.468, 'eval_steps_per_second': 19.622, 'epoch': 0.44}
{'loss': 1.0343, 'grad_norm': 0.19065871834754944, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0842760801315308, 'eval_runtime': 3.2189, 'eval_samples_per_second': 310.67, 'eval_steps_per_second': 19.572, 'epoch': 0.48}
{'loss': 1.0668, 'grad_norm': 0.18352003395557404, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.064084529876709, 'eval_runtime': 3.2151, 'eval_samples_per_second': 311.032, 'eval_steps_per_second': 19.595, 'epoch': 0.52}
{'loss': 1.0502, 'grad_norm': 0.17238137125968933, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0706466436386108, 'eval_runtime': 3.2236, 'eval_samples_per_second': 310.21, 'eval_steps_per_second': 19.543, 'epoch': 0.56}
{'loss': 1.0275, 'grad_norm': 0.19514931738376617, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0607585906982422, 'eval_runtime': 3.2425, 'eval_samples_per_second': 308.408, 'eval_steps_per_second': 19.43, 'epoch': 0.6}
{'loss': 1.0858, 'grad_norm': 0.16680897772312164, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0569746494293213, 'eval_runtime': 3.2184, 'eval_samples_per_second': 310.714, 'eval_steps_per_second': 19.575, 'epoch': 0.64}
{'loss': 1.0016, 'grad_norm': 0.17177602648735046, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0628859996795654, 'eval_runtime': 3.2296, 'eval_samples_per_second': 309.634, 'eval_steps_per_second': 19.507, 'epoch': 0.68}
{'loss': 0.9809, 'grad_norm': 0.2018137127161026, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0566906929016113, 'eval_runtime': 3.2264, 'eval_samples_per_second': 309.94, 'eval_steps_per_second': 19.526, 'epoch': 0.72}
{'loss': 1.101, 'grad_norm': 0.28005117177963257, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0495871305465698, 'eval_runtime': 3.2303, 'eval_samples_per_second': 309.566, 'eval_steps_per_second': 19.503, 'epoch': 0.76}
{'loss': 1.0406, 'grad_norm': 0.20476754009723663, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0501667261123657, 'eval_runtime': 3.2233, 'eval_samples_per_second': 310.238, 'eval_steps_per_second': 19.545, 'epoch': 0.8}
{'loss': 1.052, 'grad_norm': 0.28521057963371277, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.047088623046875, 'eval_runtime': 3.2295, 'eval_samples_per_second': 309.649, 'eval_steps_per_second': 19.508, 'epoch': 0.84}
{'loss': 1.018, 'grad_norm': 0.1592496782541275, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0421841144561768, 'eval_runtime': 3.2253, 'eval_samples_per_second': 310.05, 'eval_steps_per_second': 19.533, 'epoch': 0.88}
{'loss': 0.9913, 'grad_norm': 0.2550829350948334, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0401350259780884, 'eval_runtime': 3.2253, 'eval_samples_per_second': 310.045, 'eval_steps_per_second': 19.533, 'epoch': 0.92}
{'loss': 1.0461, 'grad_norm': 0.1658463478088379, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0402319431304932, 'eval_runtime': 3.2297, 'eval_samples_per_second': 309.625, 'eval_steps_per_second': 19.506, 'epoch': 0.96}
{'loss': 1.015, 'grad_norm': 0.1902884989976883, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0403896570205688, 'eval_runtime': 3.2302, 'eval_samples_per_second': 309.577, 'eval_steps_per_second': 19.503, 'epoch': 1.0}
{'train_runtime': 289.821, 'train_samples_per_second': 34.487, 'train_steps_per_second': 2.157, 'train_loss': 1.1897219848632812, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4515063762664795, 1.6777980327606201, 1.374961018562317, 1.2631982564926147, 1.2314903736114502, 1.1872645616531372, 1.1486711502075195, 1.1072657108306885, 1.091783046722412, 1.0905561447143555, 1.0762338638305664, 1.0842760801315308, 1.064084529876709, 1.0706466436386108, 1.0607585906982422, 1.0569746494293213, 1.0628859996795654, 1.0566906929016113, 1.0495871305465698, 1.0501667261123657, 1.047088623046875, 1.0421841144561768, 1.0401350259780884, 1.0402319431304932, 1.0403896570205688], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.4515063762664795, 1.6777980327606201, 1.374961018562317, 1.2631982564926147, 1.2314903736114502, 1.1872645616531372, 1.1486711502075195, 1.1072657108306885, 1.091783046722412, 1.0905561447143555, 1.0762338638305664, 1.0842760801315308, 1.064084529876709, 1.0706466436386108, 1.0607585906982422, 1.0569746494293213, 1.0628859996795654, 1.0566906929016113, 1.0495871305465698, 1.0501667261123657, 1.047088623046875, 1.0421841144561768, 1.0401350259780884, 1.0402319431304932, 1.0403896570205688]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.38761568069458
current iteration best possible eval_loss (full train run):  -1.0403896570205688
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.7295 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -0.8105782838046265
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -0.8236372064349335
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -0.7953171269945494
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.7827619200074012
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -0.8053008218377271
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3472, dtype=torch.float64), tensor(0.1790, dtype=torch.float64), tensor(0.1821, dtype=torch.float64), tensor(0.1018, dtype=torch.float64), 0, tensor(0.1527, dtype=torch.float64), tensor(0.0296, dtype=torch.float64), 0, 0, 7, 1, 1, 0, 0, 0, 61, 0.006344214693868151, 32.093886679560555, 0]
normalized proposed parameters for next round by BO: [tensor(0.3472, dtype=torch.float64), tensor(0.1790, dtype=torch.float64), tensor(0.1821, dtype=torch.float64), tensor(0.1018, dtype=torch.float64), tensor(0.0076, dtype=torch.float64), tensor(0.1527, dtype=torch.float64), tensor(0.0296, dtype=torch.float64), tensor(1.8088e-19, dtype=torch.float64), tensor(5.9280e-18, dtype=torch.float64), tensor(0.2054, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4800, dtype=torch.float64), tensor(0.0634, dtype=torch.float64), tensor(0.6686, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.347
  gsm8k: 0.179
  rowan_hellaswag: 0.182
  sciq: 0.102
  triviaqa: 0
  truthfulqa_gen: 0.153
  wikitext: 0.03
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (61,)
  lora_dropout: (0.006344214693868151,)
  num_layers_to_apply: (7,)
  five_dim_vector: ([1, 1, 0, 0, 0],)
  lora_alpha: (32.093886679560555,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  7
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 0]
lora rank:  61
lora dropout:  0.006344214693868151
lora alpha:  32.093886679560555
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 5,684,224 || all params: 8,035,945,472 || trainable%: 0.0707
length of training data:  9921
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9245, 'grad_norm': 0.8373491764068604, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.5512924194335938, 'eval_runtime': 4.2129, 'eval_samples_per_second': 237.363, 'eval_steps_per_second': 14.954, 'epoch': 0.04}
{'loss': 2.507, 'grad_norm': 0.49494025111198425, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.2616562843322754, 'eval_runtime': 2.9296, 'eval_samples_per_second': 341.346, 'eval_steps_per_second': 21.505, 'epoch': 0.08}
{'loss': 1.905, 'grad_norm': 0.3716905117034912, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 2.126805067062378, 'eval_runtime': 2.9403, 'eval_samples_per_second': 340.107, 'eval_steps_per_second': 21.427, 'epoch': 0.12}
{'loss': 1.6262, 'grad_norm': 0.3338543474674225, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 2.2365689277648926, 'eval_runtime': 2.9287, 'eval_samples_per_second': 341.449, 'eval_steps_per_second': 21.511, 'epoch': 0.16}
{'loss': 1.5652, 'grad_norm': 0.2761186957359314, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 2.232225179672241, 'eval_runtime': 2.9305, 'eval_samples_per_second': 341.233, 'eval_steps_per_second': 21.498, 'epoch': 0.2}
{'loss': 1.5304, 'grad_norm': 0.32303503155708313, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 2.105429172515869, 'eval_runtime': 2.9376, 'eval_samples_per_second': 340.41, 'eval_steps_per_second': 21.446, 'epoch': 0.24}
{'loss': 1.3988, 'grad_norm': 0.3067959249019623, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 2.057760000228882, 'eval_runtime': 2.9372, 'eval_samples_per_second': 340.462, 'eval_steps_per_second': 21.449, 'epoch': 0.28}
{'loss': 1.4711, 'grad_norm': 0.25640198588371277, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 2.0092525482177734, 'eval_runtime': 2.9408, 'eval_samples_per_second': 340.046, 'eval_steps_per_second': 21.423, 'epoch': 0.32}
{'loss': 1.3835, 'grad_norm': 0.23857983946800232, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.9678288698196411, 'eval_runtime': 2.9434, 'eval_samples_per_second': 339.746, 'eval_steps_per_second': 21.404, 'epoch': 0.36}
{'loss': 1.3605, 'grad_norm': 0.33368799090385437, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.8564571142196655, 'eval_runtime': 2.9417, 'eval_samples_per_second': 339.938, 'eval_steps_per_second': 21.416, 'epoch': 0.4}
{'loss': 1.3088, 'grad_norm': 0.3245137929916382, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.7569409608840942, 'eval_runtime': 2.9382, 'eval_samples_per_second': 340.343, 'eval_steps_per_second': 21.442, 'epoch': 0.44}
{'loss': 1.3422, 'grad_norm': 0.2999107837677002, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.8796261548995972, 'eval_runtime': 2.9418, 'eval_samples_per_second': 339.93, 'eval_steps_per_second': 21.416, 'epoch': 0.48}
{'loss': 1.3542, 'grad_norm': 0.24199548363685608, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.760534644126892, 'eval_runtime': 2.9494, 'eval_samples_per_second': 339.055, 'eval_steps_per_second': 21.36, 'epoch': 0.52}
{'loss': 1.3208, 'grad_norm': 0.24997301399707794, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.793220043182373, 'eval_runtime': 2.95, 'eval_samples_per_second': 338.986, 'eval_steps_per_second': 21.356, 'epoch': 0.56}
{'loss': 1.2753, 'grad_norm': 0.2848181128501892, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.7358334064483643, 'eval_runtime': 2.9537, 'eval_samples_per_second': 338.553, 'eval_steps_per_second': 21.329, 'epoch': 0.6}
{'loss': 1.3473, 'grad_norm': 0.28407788276672363, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.7881373167037964, 'eval_runtime': 2.9441, 'eval_samples_per_second': 339.665, 'eval_steps_per_second': 21.399, 'epoch': 0.64}
{'loss': 1.356, 'grad_norm': 0.25631779432296753, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.8398418426513672, 'eval_runtime': 2.9366, 'eval_samples_per_second': 340.533, 'eval_steps_per_second': 21.454, 'epoch': 0.68}
{'loss': 1.304, 'grad_norm': 0.25436121225357056, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.8432307243347168, 'eval_runtime': 2.9462, 'eval_samples_per_second': 339.42, 'eval_steps_per_second': 21.383, 'epoch': 0.72}
{'loss': 1.2489, 'grad_norm': 0.2736024558544159, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.8604642152786255, 'eval_runtime': 2.9376, 'eval_samples_per_second': 340.411, 'eval_steps_per_second': 21.446, 'epoch': 0.76}
{'loss': 1.3234, 'grad_norm': 0.25831925868988037, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.8331754207611084, 'eval_runtime': 2.94, 'eval_samples_per_second': 340.141, 'eval_steps_per_second': 21.429, 'epoch': 0.81}
{'loss': 1.3018, 'grad_norm': 0.23427662253379822, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.8391855955123901, 'eval_runtime': 2.9427, 'eval_samples_per_second': 339.822, 'eval_steps_per_second': 21.409, 'epoch': 0.85}
{'loss': 1.251, 'grad_norm': 0.22869323194026947, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.8478338718414307, 'eval_runtime': 2.9515, 'eval_samples_per_second': 338.814, 'eval_steps_per_second': 21.345, 'epoch': 0.89}
{'loss': 1.3009, 'grad_norm': 0.2565297484397888, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.836114525794983, 'eval_runtime': 2.942, 'eval_samples_per_second': 339.908, 'eval_steps_per_second': 21.414, 'epoch': 0.93}
{'loss': 1.3068, 'grad_norm': 0.2768722176551819, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.8517866134643555, 'eval_runtime': 2.9434, 'eval_samples_per_second': 339.748, 'eval_steps_per_second': 21.404, 'epoch': 0.97}
{'train_runtime': 192.0295, 'train_samples_per_second': 51.664, 'train_steps_per_second': 3.234, 'train_loss': 1.533891383000618, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5512924194335938, 2.2616562843322754, 2.126805067062378, 2.2365689277648926, 2.232225179672241, 2.105429172515869, 2.057760000228882, 2.0092525482177734, 1.9678288698196411, 1.8564571142196655, 1.7569409608840942, 1.8796261548995972, 1.760534644126892, 1.793220043182373, 1.7358334064483643, 1.7881373167037964, 1.8398418426513672, 1.8432307243347168, 1.8604642152786255, 1.8331754207611084, 1.8391855955123901, 1.8478338718414307, 1.836114525794983, 1.8517866134643555], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [3.5512924194335938, 2.2616562843322754, 2.126805067062378, 2.2365689277648926, 2.232225179672241, 2.105429172515869, 2.057760000228882, 2.0092525482177734, 1.9678288698196411, 1.8564571142196655, 1.7569409608840942, 1.8796261548995972, 1.760534644126892, 1.793220043182373, 1.7358334064483643, 1.7881373167037964, 1.8398418426513672, 1.8432307243347168, 1.8604642152786255, 1.8331754207611084, 1.8391855955123901, 1.8478338718414307, 1.836114525794983, 1.8517866134643555]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.6868460178375244
current iteration best possible eval_loss (full train run):  -1.8517866134643555
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0632 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.35491812229156494, 0.0927686095237732, 0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7262229919433594, 0.8165992498397827, 0.8520699143409729, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9559857845306396, 0.3273009657859802, 0.9828505516052246, 0.5352497100830078]  ‚Üí  acq = -0.2873264128953843
X = [0.8189860582351685, 0.7869956493377686, 0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8973821401596069, 0.7314273715019226, 0.9184417128562927, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9420246481895447, 0.9331071972846985, 0.31236356496810913, 0.36077266931533813]  ‚Üí  acq = -0.2873264128953843
X = [0.7738390564918518, 0.5021045207977295, 0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5741828680038452, 0.5517953038215637, 0.7892404794692993, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9668935537338257, 0.5031551122665405, 0.7251023054122925, 0.2269490361213684]  ‚Üí  acq = -0.2873264128953843
X = [0.20103693008422852, 0.27278316020965576, 0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.14547844231128693, 0.0291365385055542, 0.3460528254508972, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.32679685950279236, 0.4884251356124878, 0.9665257930755615, 0.2274898886680603]  ‚Üí  acq = -0.2873264128953843
X = [0.6374526023864746, 0.36883002519607544, 0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3681538701057434, 0.6981962323188782, 0.7622081637382507, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.5123441219329834, 0.3907546401023865, 0.21269100904464722, 0.09688013792037964]  ‚Üí  acq = -0.2873264128953843
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2104, dtype=torch.float64), tensor(0.4574, dtype=torch.float64), tensor(0.0123, dtype=torch.float64), 0, 0, tensor(0.1039, dtype=torch.float64), 0, tensor(0.1403, dtype=torch.float64), tensor(0.0756, dtype=torch.float64), 1, 0, 0, 1, 0, 1, 44, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.2104, dtype=torch.float64), tensor(0.4574, dtype=torch.float64), tensor(0.0123, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1039, dtype=torch.float64), tensor(7.4729e-22, dtype=torch.float64), tensor(0.1403, dtype=torch.float64), tensor(0.0756, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3409, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.21
  gsm8k: 0.457
  rowan_hellaswag: 0.012
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.104
  wikitext: 0
  mmlu: 0.14
  arc_challenge: 0.076

LoRA Parameters:
  lora_r: (44,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  44
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,622,016 || all params: 8,031,883,264 || trainable%: 0.0202
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1355, 'grad_norm': 1.0772994756698608, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.9645440578460693, 'eval_runtime': 4.4313, 'eval_samples_per_second': 225.666, 'eval_steps_per_second': 14.217, 'epoch': 0.04}
{'loss': 2.1356, 'grad_norm': 1.5259002447128296, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.611051082611084, 'eval_runtime': 2.8947, 'eval_samples_per_second': 345.457, 'eval_steps_per_second': 21.764, 'epoch': 0.08}
{'loss': 1.7214, 'grad_norm': 2.2148125171661377, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.3243775367736816, 'eval_runtime': 2.8653, 'eval_samples_per_second': 348.999, 'eval_steps_per_second': 21.987, 'epoch': 0.12}
{'loss': 1.5069, 'grad_norm': 1.0716111660003662, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.2303736209869385, 'eval_runtime': 2.8836, 'eval_samples_per_second': 346.794, 'eval_steps_per_second': 21.848, 'epoch': 0.16}
{'loss': 1.4433, 'grad_norm': 2.4181087017059326, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.095649480819702, 'eval_runtime': 2.8802, 'eval_samples_per_second': 347.201, 'eval_steps_per_second': 21.874, 'epoch': 0.2}
{'loss': 1.3333, 'grad_norm': 0.8090986609458923, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1265835762023926, 'eval_runtime': 2.8802, 'eval_samples_per_second': 347.203, 'eval_steps_per_second': 21.874, 'epoch': 0.24}
{'loss': 1.2749, 'grad_norm': 0.6513790488243103, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.1120283603668213, 'eval_runtime': 2.8822, 'eval_samples_per_second': 346.961, 'eval_steps_per_second': 21.859, 'epoch': 0.28}
{'loss': 1.2884, 'grad_norm': 0.9279956817626953, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.023293972015381, 'eval_runtime': 2.8804, 'eval_samples_per_second': 347.17, 'eval_steps_per_second': 21.872, 'epoch': 0.32}
{'loss': 1.3006, 'grad_norm': 0.6811950206756592, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.026979446411133, 'eval_runtime': 2.8821, 'eval_samples_per_second': 346.974, 'eval_steps_per_second': 21.859, 'epoch': 0.36}
{'loss': 1.2757, 'grad_norm': 0.5839680433273315, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9165325164794922, 'eval_runtime': 2.8778, 'eval_samples_per_second': 347.486, 'eval_steps_per_second': 21.892, 'epoch': 0.4}
{'loss': 1.2273, 'grad_norm': 0.6628313064575195, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.905566692352295, 'eval_runtime': 2.8828, 'eval_samples_per_second': 346.89, 'eval_steps_per_second': 21.854, 'epoch': 0.44}
{'loss': 1.1845, 'grad_norm': 0.5769913792610168, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8832082748413086, 'eval_runtime': 2.8856, 'eval_samples_per_second': 346.543, 'eval_steps_per_second': 21.832, 'epoch': 0.48}
{'loss': 1.1768, 'grad_norm': 0.8955420255661011, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8465583324432373, 'eval_runtime': 2.8825, 'eval_samples_per_second': 346.919, 'eval_steps_per_second': 21.856, 'epoch': 0.52}
{'loss': 1.1997, 'grad_norm': 0.7780109643936157, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.796148657798767, 'eval_runtime': 2.885, 'eval_samples_per_second': 346.618, 'eval_steps_per_second': 21.837, 'epoch': 0.56}
{'loss': 1.1955, 'grad_norm': 0.6694279313087463, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8084577322006226, 'eval_runtime': 2.8869, 'eval_samples_per_second': 346.394, 'eval_steps_per_second': 21.823, 'epoch': 0.6}
{'loss': 1.1878, 'grad_norm': 0.6347898244857788, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7844599485397339, 'eval_runtime': 2.885, 'eval_samples_per_second': 346.623, 'eval_steps_per_second': 21.837, 'epoch': 0.64}
{'loss': 1.1356, 'grad_norm': 0.5565000772476196, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7957842350006104, 'eval_runtime': 2.8756, 'eval_samples_per_second': 347.758, 'eval_steps_per_second': 21.909, 'epoch': 0.68}
{'loss': 1.1276, 'grad_norm': 0.685318112373352, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7209707498550415, 'eval_runtime': 2.8782, 'eval_samples_per_second': 347.441, 'eval_steps_per_second': 21.889, 'epoch': 0.72}
{'loss': 1.0726, 'grad_norm': 0.4364680051803589, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7053605318069458, 'eval_runtime': 2.881, 'eval_samples_per_second': 347.103, 'eval_steps_per_second': 21.867, 'epoch': 0.76}
{'loss': 1.1375, 'grad_norm': 0.8041461706161499, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6781704425811768, 'eval_runtime': 2.8788, 'eval_samples_per_second': 347.364, 'eval_steps_per_second': 21.884, 'epoch': 0.8}
{'loss': 1.1017, 'grad_norm': 0.6418334245681763, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6910654306411743, 'eval_runtime': 2.8764, 'eval_samples_per_second': 347.657, 'eval_steps_per_second': 21.902, 'epoch': 0.84}
{'loss': 1.1138, 'grad_norm': 0.5681694149971008, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7346643209457397, 'eval_runtime': 2.8899, 'eval_samples_per_second': 346.037, 'eval_steps_per_second': 21.8, 'epoch': 0.88}
{'loss': 1.0519, 'grad_norm': 0.6061418652534485, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7012139558792114, 'eval_runtime': 2.8801, 'eval_samples_per_second': 347.214, 'eval_steps_per_second': 21.874, 'epoch': 0.92}
{'loss': 1.0617, 'grad_norm': 0.7711749076843262, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.722414255142212, 'eval_runtime': 2.88, 'eval_samples_per_second': 347.226, 'eval_steps_per_second': 21.875, 'epoch': 0.96}
{'loss': 1.0703, 'grad_norm': 0.7381812930107117, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.736065149307251, 'eval_runtime': 2.881, 'eval_samples_per_second': 347.096, 'eval_steps_per_second': 21.867, 'epoch': 1.0}
{'train_runtime': 261.2251, 'train_samples_per_second': 38.27, 'train_steps_per_second': 2.393, 'train_loss': 1.3383902770996094, 'epoch': 1.0}
train_results:  {'eval_loss': [3.9645440578460693, 2.611051082611084, 2.3243775367736816, 2.2303736209869385, 2.095649480819702, 2.1265835762023926, 2.1120283603668213, 2.023293972015381, 2.026979446411133, 1.9165325164794922, 1.905566692352295, 1.8832082748413086, 1.8465583324432373, 1.796148657798767, 1.8084577322006226, 1.7844599485397339, 1.7957842350006104, 1.7209707498550415, 1.7053605318069458, 1.6781704425811768, 1.6910654306411743, 1.7346643209457397, 1.7012139558792114, 1.722414255142212, 1.736065149307251], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.9645440578460693, 2.611051082611084, 2.3243775367736816, 2.2303736209869385, 2.095649480819702, 2.1265835762023926, 2.1120283603668213, 2.023293972015381, 2.026979446411133, 1.9165325164794922, 1.905566692352295, 1.8832082748413086, 1.8465583324432373, 1.796148657798767, 1.8084577322006226, 1.7844599485397339, 1.7957842350006104, 1.7209707498550415, 1.7053605318069458, 1.6781704425811768, 1.6910654306411743, 1.7346643209457397, 1.7012139558792114, 1.722414255142212, 1.736065149307251]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.190642833709717
current iteration best possible eval_loss (full train run):  -1.736065149307251
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.4409 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -0.3913878107138642
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.3913878107138642
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -0.3913878107138642
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -0.3913878107138642
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -0.3913878107138642
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1948, dtype=torch.float64), tensor(0.0784, dtype=torch.float64), tensor(0.0339, dtype=torch.float64), tensor(0.2086, dtype=torch.float64), tensor(0.0123, dtype=torch.float64), tensor(0.1501, dtype=torch.float64), tensor(0.1516, dtype=torch.float64), 0, tensor(0.1703, dtype=torch.float64), 22, 1, 0, 0, 1, 1, 15, 0.06238737692724542, 1.480000019073487, 1]
normalized proposed parameters for next round by BO: [tensor(0.1948, dtype=torch.float64), tensor(0.0784, dtype=torch.float64), tensor(0.0339, dtype=torch.float64), tensor(0.2086, dtype=torch.float64), tensor(0.0123, dtype=torch.float64), tensor(0.1501, dtype=torch.float64), tensor(0.1516, dtype=torch.float64), tensor(1.2103e-17, dtype=torch.float64), tensor(0.1703, dtype=torch.float64), tensor(0.6839, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1153, dtype=torch.float64), tensor(0.6239, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.195
  gsm8k: 0.078
  rowan_hellaswag: 0.034
  sciq: 0.209
  triviaqa: 0.012
  truthfulqa_gen: 0.15
  wikitext: 0.152
  mmlu: 0
  arc_challenge: 0.17

LoRA Parameters:
  lora_r: (15,)
  lora_dropout: (0.06238737692724542,)
  num_layers_to_apply: (22,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (1.480000019073487,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  22
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  15
lora dropout:  0.06238737692724542
lora alpha:  1.480000019073487
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 14,868,480 || all params: 8,045,129,728 || trainable%: 0.1848
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.1426, 'grad_norm': 1.1529710292816162, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.8617818355560303, 'eval_runtime': 3.3536, 'eval_samples_per_second': 298.188, 'eval_steps_per_second': 18.786, 'epoch': 0.04}
{'loss': 2.4924, 'grad_norm': 1.370490550994873, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.3850347995758057, 'eval_runtime': 3.3606, 'eval_samples_per_second': 297.569, 'eval_steps_per_second': 18.747, 'epoch': 0.08}
{'loss': 1.6012, 'grad_norm': 0.35303494334220886, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8010920286178589, 'eval_runtime': 3.3872, 'eval_samples_per_second': 295.233, 'eval_steps_per_second': 18.6, 'epoch': 0.12}
{'loss': 1.4747, 'grad_norm': 0.270166277885437, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6840918064117432, 'eval_runtime': 3.365, 'eval_samples_per_second': 297.173, 'eval_steps_per_second': 18.722, 'epoch': 0.16}
{'loss': 1.451, 'grad_norm': 0.3629947602748871, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5984324216842651, 'eval_runtime': 3.3604, 'eval_samples_per_second': 297.586, 'eval_steps_per_second': 18.748, 'epoch': 0.2}
{'loss': 1.3734, 'grad_norm': 0.1900857537984848, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4620290994644165, 'eval_runtime': 3.3768, 'eval_samples_per_second': 296.138, 'eval_steps_per_second': 18.657, 'epoch': 0.24}
{'loss': 1.2681, 'grad_norm': 0.24063719809055328, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4291826486587524, 'eval_runtime': 3.3663, 'eval_samples_per_second': 297.058, 'eval_steps_per_second': 18.715, 'epoch': 0.28}
{'loss': 1.2092, 'grad_norm': 0.1781957745552063, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3777421712875366, 'eval_runtime': 3.3745, 'eval_samples_per_second': 296.337, 'eval_steps_per_second': 18.669, 'epoch': 0.32}
{'loss': 1.2554, 'grad_norm': 0.188614159822464, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2931904792785645, 'eval_runtime': 3.3654, 'eval_samples_per_second': 297.143, 'eval_steps_per_second': 18.72, 'epoch': 0.36}
{'loss': 1.2222, 'grad_norm': 0.14041666686534882, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2854511737823486, 'eval_runtime': 3.3746, 'eval_samples_per_second': 296.329, 'eval_steps_per_second': 18.669, 'epoch': 0.4}
{'loss': 1.1606, 'grad_norm': 0.211215540766716, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2852449417114258, 'eval_runtime': 3.3558, 'eval_samples_per_second': 297.991, 'eval_steps_per_second': 18.773, 'epoch': 0.44}
{'loss': 1.1601, 'grad_norm': 0.13732782006263733, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2896487712860107, 'eval_runtime': 3.3684, 'eval_samples_per_second': 296.873, 'eval_steps_per_second': 18.703, 'epoch': 0.48}
{'loss': 1.1534, 'grad_norm': 0.17364835739135742, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2256247997283936, 'eval_runtime': 3.3637, 'eval_samples_per_second': 297.289, 'eval_steps_per_second': 18.729, 'epoch': 0.52}
{'loss': 1.129, 'grad_norm': 0.14550432562828064, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2353922128677368, 'eval_runtime': 3.4025, 'eval_samples_per_second': 293.905, 'eval_steps_per_second': 18.516, 'epoch': 0.56}
{'loss': 1.1037, 'grad_norm': 0.18081815540790558, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2517696619033813, 'eval_runtime': 3.4, 'eval_samples_per_second': 294.118, 'eval_steps_per_second': 18.529, 'epoch': 0.6}
{'loss': 1.0809, 'grad_norm': 0.16579194366931915, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.267767071723938, 'eval_runtime': 3.3914, 'eval_samples_per_second': 294.867, 'eval_steps_per_second': 18.577, 'epoch': 0.64}
{'loss': 1.09, 'grad_norm': 0.14137525856494904, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2361596822738647, 'eval_runtime': 3.4, 'eval_samples_per_second': 294.121, 'eval_steps_per_second': 18.53, 'epoch': 0.68}
{'loss': 1.0571, 'grad_norm': 0.18777231872081757, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2253637313842773, 'eval_runtime': 3.375, 'eval_samples_per_second': 296.297, 'eval_steps_per_second': 18.667, 'epoch': 0.72}
{'loss': 1.0915, 'grad_norm': 0.19920803606510162, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2040663957595825, 'eval_runtime': 3.3704, 'eval_samples_per_second': 296.703, 'eval_steps_per_second': 18.692, 'epoch': 0.76}
{'loss': 1.1184, 'grad_norm': 0.21415278315544128, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.20732581615448, 'eval_runtime': 3.3683, 'eval_samples_per_second': 296.882, 'eval_steps_per_second': 18.704, 'epoch': 0.8}
{'loss': 1.0641, 'grad_norm': 0.14996303617954254, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2230643033981323, 'eval_runtime': 3.3643, 'eval_samples_per_second': 297.242, 'eval_steps_per_second': 18.726, 'epoch': 0.84}
{'loss': 1.1224, 'grad_norm': 0.25928226113319397, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2091071605682373, 'eval_runtime': 3.368, 'eval_samples_per_second': 296.913, 'eval_steps_per_second': 18.705, 'epoch': 0.88}
{'loss': 1.0644, 'grad_norm': 0.17964480817317963, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2207813262939453, 'eval_runtime': 3.3687, 'eval_samples_per_second': 296.848, 'eval_steps_per_second': 18.701, 'epoch': 0.92}
{'loss': 1.0736, 'grad_norm': 0.19764867424964905, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2093815803527832, 'eval_runtime': 3.3755, 'eval_samples_per_second': 296.248, 'eval_steps_per_second': 18.664, 'epoch': 0.96}
{'loss': 1.0706, 'grad_norm': 0.22538989782333374, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2057669162750244, 'eval_runtime': 3.3679, 'eval_samples_per_second': 296.918, 'eval_steps_per_second': 18.706, 'epoch': 1.0}
{'train_runtime': 296.2215, 'train_samples_per_second': 33.742, 'train_steps_per_second': 2.11, 'train_loss': 1.361205044555664, 'epoch': 1.0}
train_results:  {'eval_loss': [3.8617818355560303, 2.3850347995758057, 1.8010920286178589, 1.6840918064117432, 1.5984324216842651, 1.4620290994644165, 1.4291826486587524, 1.3777421712875366, 1.2931904792785645, 1.2854511737823486, 1.2852449417114258, 1.2896487712860107, 1.2256247997283936, 1.2353922128677368, 1.2517696619033813, 1.267767071723938, 1.2361596822738647, 1.2253637313842773, 1.2040663957595825, 1.20732581615448, 1.2230643033981323, 1.2091071605682373, 1.2207813262939453, 1.2093815803527832, 1.2057669162750244], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.8617818355560303, 2.3850347995758057, 1.8010920286178589, 1.6840918064117432, 1.5984324216842651, 1.4620290994644165, 1.4291826486587524, 1.3777421712875366, 1.2931904792785645, 1.2854511737823486, 1.2852449417114258, 1.2896487712860107, 1.2256247997283936, 1.2353922128677368, 1.2517696619033813, 1.267767071723938, 1.2361596822738647, 1.2253637313842773, 1.2040663957595825, 1.20732581615448, 1.2230643033981323, 1.2091071605682373, 1.2207813262939453, 1.2093815803527832, 1.2057669162750244]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.331563949584961
current iteration best possible eval_loss (full train run):  -1.2057669162750244
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.1712 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.8394305027543312
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.778150763927481
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.9746370037042325
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.9584787896730833
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -0.9859547396742336
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2382, dtype=torch.float64), tensor(0.0460, dtype=torch.float64), 0, tensor(0.1523, dtype=torch.float64), tensor(0.2963, dtype=torch.float64), 0, tensor(0.0390, dtype=torch.float64), tensor(0.2282, dtype=torch.float64), 24, 1, 0, 0, 1, 1, 69, 0.0, 36.95260587119836, 0]
normalized proposed parameters for next round by BO: [tensor(5.6498e-20, dtype=torch.float64), tensor(0.2382, dtype=torch.float64), tensor(0.0460, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1523, dtype=torch.float64), tensor(0.2963, dtype=torch.float64), tensor(3.7608e-20, dtype=torch.float64), tensor(0.0390, dtype=torch.float64), tensor(0.2282, dtype=torch.float64), tensor(0.7475, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5356, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7698, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.238
  rowan_hellaswag: 0.046
  sciq: 0
  triviaqa: 0.152
  truthfulqa_gen: 0.296
  wikitext: 0
  mmlu: 0.039
  arc_challenge: 0.228

LoRA Parameters:
  lora_r: (69,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (36.95260587119836,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  69
lora dropout:  0.0
lora alpha:  36.95260587119836
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 74,612,736 || all params: 8,104,873,984 || trainable%: 0.9206
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.881, 'grad_norm': 0.918908417224884, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9464365243911743, 'eval_runtime': 4.1357, 'eval_samples_per_second': 241.796, 'eval_steps_per_second': 15.233, 'epoch': 0.04}
{'loss': 1.2632, 'grad_norm': 0.4079027771949768, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2239413261413574, 'eval_runtime': 3.4645, 'eval_samples_per_second': 288.642, 'eval_steps_per_second': 18.184, 'epoch': 0.08}
{'loss': 1.0047, 'grad_norm': 0.279338538646698, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.122735857963562, 'eval_runtime': 3.4721, 'eval_samples_per_second': 288.012, 'eval_steps_per_second': 18.145, 'epoch': 0.12}
{'loss': 0.9822, 'grad_norm': 0.22542910277843475, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1037956476211548, 'eval_runtime': 3.4642, 'eval_samples_per_second': 288.667, 'eval_steps_per_second': 18.186, 'epoch': 0.16}
{'loss': 1.0, 'grad_norm': 0.22011743485927582, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0981268882751465, 'eval_runtime': 3.4801, 'eval_samples_per_second': 287.346, 'eval_steps_per_second': 18.103, 'epoch': 0.2}
{'loss': 0.9885, 'grad_norm': 0.19704516232013702, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0670315027236938, 'eval_runtime': 3.4913, 'eval_samples_per_second': 286.426, 'eval_steps_per_second': 18.045, 'epoch': 0.24}
{'loss': 0.9092, 'grad_norm': 0.25789013504981995, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0773382186889648, 'eval_runtime': 3.5041, 'eval_samples_per_second': 285.382, 'eval_steps_per_second': 17.979, 'epoch': 0.28}
{'loss': 0.9285, 'grad_norm': 0.20146259665489197, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0592924356460571, 'eval_runtime': 3.4896, 'eval_samples_per_second': 286.569, 'eval_steps_per_second': 18.054, 'epoch': 0.32}
{'loss': 0.8917, 'grad_norm': 0.24905934929847717, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0700244903564453, 'eval_runtime': 3.4908, 'eval_samples_per_second': 286.469, 'eval_steps_per_second': 18.048, 'epoch': 0.36}
{'loss': 0.8933, 'grad_norm': 0.2529246509075165, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0614194869995117, 'eval_runtime': 3.4854, 'eval_samples_per_second': 286.908, 'eval_steps_per_second': 18.075, 'epoch': 0.4}
{'loss': 0.886, 'grad_norm': 0.2974492013454437, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0649443864822388, 'eval_runtime': 3.4867, 'eval_samples_per_second': 286.806, 'eval_steps_per_second': 18.069, 'epoch': 0.44}
{'loss': 0.9335, 'grad_norm': 0.21685686707496643, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0609198808670044, 'eval_runtime': 3.4866, 'eval_samples_per_second': 286.811, 'eval_steps_per_second': 18.069, 'epoch': 0.48}
{'loss': 0.8756, 'grad_norm': 0.2738172709941864, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.059896469116211, 'eval_runtime': 3.4896, 'eval_samples_per_second': 286.569, 'eval_steps_per_second': 18.054, 'epoch': 0.52}
{'loss': 0.8457, 'grad_norm': 0.289130836725235, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.055896282196045, 'eval_runtime': 3.4921, 'eval_samples_per_second': 286.363, 'eval_steps_per_second': 18.041, 'epoch': 0.56}
{'loss': 0.8367, 'grad_norm': 0.2640378475189209, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0524424314498901, 'eval_runtime': 3.4901, 'eval_samples_per_second': 286.525, 'eval_steps_per_second': 18.051, 'epoch': 0.6}
{'loss': 0.8934, 'grad_norm': 0.3224641680717468, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0467276573181152, 'eval_runtime': 3.4871, 'eval_samples_per_second': 286.768, 'eval_steps_per_second': 18.066, 'epoch': 0.64}
{'loss': 0.8213, 'grad_norm': 0.35628339648246765, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0568974018096924, 'eval_runtime': 3.4865, 'eval_samples_per_second': 286.818, 'eval_steps_per_second': 18.07, 'epoch': 0.68}
{'loss': 0.8613, 'grad_norm': 0.28494319319725037, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.046813726425171, 'eval_runtime': 3.4963, 'eval_samples_per_second': 286.016, 'eval_steps_per_second': 18.019, 'epoch': 0.72}
{'loss': 0.8045, 'grad_norm': 0.2601886987686157, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0447430610656738, 'eval_runtime': 3.4921, 'eval_samples_per_second': 286.358, 'eval_steps_per_second': 18.041, 'epoch': 0.76}
{'loss': 0.8433, 'grad_norm': 0.26258906722068787, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0404514074325562, 'eval_runtime': 3.4946, 'eval_samples_per_second': 286.157, 'eval_steps_per_second': 18.028, 'epoch': 0.8}
{'loss': 0.7931, 'grad_norm': 0.24955907464027405, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.04608154296875, 'eval_runtime': 3.5073, 'eval_samples_per_second': 285.123, 'eval_steps_per_second': 17.963, 'epoch': 0.84}
{'loss': 0.8049, 'grad_norm': 0.3656594455242157, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0407192707061768, 'eval_runtime': 3.4988, 'eval_samples_per_second': 285.816, 'eval_steps_per_second': 18.006, 'epoch': 0.88}
{'loss': 0.8035, 'grad_norm': 0.3251228630542755, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0429117679595947, 'eval_runtime': 3.5007, 'eval_samples_per_second': 285.655, 'eval_steps_per_second': 17.996, 'epoch': 0.92}
{'loss': 0.8063, 'grad_norm': 0.30778801441192627, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0418540239334106, 'eval_runtime': 3.4965, 'eval_samples_per_second': 285.998, 'eval_steps_per_second': 18.018, 'epoch': 0.96}
{'loss': 0.841, 'grad_norm': 0.4569731652736664, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0413023233413696, 'eval_runtime': 3.5074, 'eval_samples_per_second': 285.115, 'eval_steps_per_second': 17.962, 'epoch': 1.0}
{'train_runtime': 288.5766, 'train_samples_per_second': 34.646, 'train_steps_per_second': 2.166, 'train_loss': 0.9756935089111328, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9464365243911743, 1.2239413261413574, 1.122735857963562, 1.1037956476211548, 1.0981268882751465, 1.0670315027236938, 1.0773382186889648, 1.0592924356460571, 1.0700244903564453, 1.0614194869995117, 1.0649443864822388, 1.0609198808670044, 1.059896469116211, 1.055896282196045, 1.0524424314498901, 1.0467276573181152, 1.0568974018096924, 1.046813726425171, 1.0447430610656738, 1.0404514074325562, 1.04608154296875, 1.0407192707061768, 1.0429117679595947, 1.0418540239334106, 1.0413023233413696], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9464365243911743, 1.2239413261413574, 1.122735857963562, 1.1037956476211548, 1.0981268882751465, 1.0670315027236938, 1.0773382186889648, 1.0592924356460571, 1.0700244903564453, 1.0614194869995117, 1.0649443864822388, 1.0609198808670044, 1.059896469116211, 1.055896282196045, 1.0524424314498901, 1.0467276573181152, 1.0568974018096924, 1.046813726425171, 1.0447430610656738, 1.0404514074325562, 1.04608154296875, 1.0407192707061768, 1.0429117679595947, 1.0418540239334106, 1.0413023233413696]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.2177906036376953
current iteration best possible eval_loss (full train run):  -1.0413023233413696
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.1315 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.9309844809115835
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -0.9796094254319669
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.9862570417103977
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -0.9798091744376667
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -0.979609425437626
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0564, dtype=torch.float64), tensor(0.1041, dtype=torch.float64), tensor(0.1675, dtype=torch.float64), tensor(0.0927, dtype=torch.float64), tensor(0.0837, dtype=torch.float64), tensor(0.0752, dtype=torch.float64), tensor(0.2333, dtype=torch.float64), tensor(0.1872, dtype=torch.float64), 25, 0, 1, 1, 0, 1, 39, 0.044988865548113315, 19.75568290132993, 1]
normalized proposed parameters for next round by BO: [tensor(8.9394e-19, dtype=torch.float64), tensor(0.0564, dtype=torch.float64), tensor(0.1041, dtype=torch.float64), tensor(0.1675, dtype=torch.float64), tensor(0.0927, dtype=torch.float64), tensor(0.0837, dtype=torch.float64), tensor(0.0752, dtype=torch.float64), tensor(0.2333, dtype=torch.float64), tensor(0.1872, dtype=torch.float64), tensor(0.7929, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3034, dtype=torch.float64), tensor(0.4499, dtype=torch.float64), tensor(0.4116, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.056
  rowan_hellaswag: 0.104
  sciq: 0.167
  triviaqa: 0.093
  truthfulqa_gen: 0.084
  wikitext: 0.075
  mmlu: 0.233
  arc_challenge: 0.187

LoRA Parameters:
  lora_r: (39,)
  lora_dropout: (0.044988865548113315,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (19.75568290132993,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  39
lora dropout:  0.044988865548113315
lora alpha:  19.75568290132993
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 40,934,400 || all params: 8,071,195,648 || trainable%: 0.5072
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.04, 'grad_norm': 0.4329412281513214, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2415425777435303, 'eval_runtime': 3.3873, 'eval_samples_per_second': 295.224, 'eval_steps_per_second': 18.599, 'epoch': 0.04}
{'loss': 1.7275, 'grad_norm': 0.28942570090293884, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6229727268218994, 'eval_runtime': 3.3864, 'eval_samples_per_second': 295.3, 'eval_steps_per_second': 18.604, 'epoch': 0.08}
{'loss': 1.5001, 'grad_norm': 0.5076326727867126, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5116881132125854, 'eval_runtime': 3.3872, 'eval_samples_per_second': 295.229, 'eval_steps_per_second': 18.599, 'epoch': 0.12}
{'loss': 1.4413, 'grad_norm': 0.29326891899108887, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3638733625411987, 'eval_runtime': 3.3833, 'eval_samples_per_second': 295.565, 'eval_steps_per_second': 18.621, 'epoch': 0.16}
{'loss': 1.3114, 'grad_norm': 0.27155008912086487, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2761905193328857, 'eval_runtime': 3.3888, 'eval_samples_per_second': 295.086, 'eval_steps_per_second': 18.59, 'epoch': 0.2}
{'loss': 1.317, 'grad_norm': 0.32112401723861694, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2872087955474854, 'eval_runtime': 3.3837, 'eval_samples_per_second': 295.536, 'eval_steps_per_second': 18.619, 'epoch': 0.24}
{'loss': 1.2613, 'grad_norm': 0.2891176640987396, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.252170205116272, 'eval_runtime': 3.4037, 'eval_samples_per_second': 293.802, 'eval_steps_per_second': 18.51, 'epoch': 0.28}
{'loss': 1.3633, 'grad_norm': 0.25784340500831604, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2551918029785156, 'eval_runtime': 3.3871, 'eval_samples_per_second': 295.24, 'eval_steps_per_second': 18.6, 'epoch': 0.32}
{'loss': 1.2749, 'grad_norm': 0.3204859793186188, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.231760025024414, 'eval_runtime': 3.4019, 'eval_samples_per_second': 293.957, 'eval_steps_per_second': 18.519, 'epoch': 0.36}
{'loss': 1.2912, 'grad_norm': 0.2701796591281891, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2322604656219482, 'eval_runtime': 3.4263, 'eval_samples_per_second': 291.862, 'eval_steps_per_second': 18.387, 'epoch': 0.4}
{'loss': 1.3302, 'grad_norm': 0.2530316412448883, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.183035135269165, 'eval_runtime': 3.3963, 'eval_samples_per_second': 294.44, 'eval_steps_per_second': 18.55, 'epoch': 0.44}
{'loss': 1.2778, 'grad_norm': 0.2722359895706177, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.140276312828064, 'eval_runtime': 3.3882, 'eval_samples_per_second': 295.142, 'eval_steps_per_second': 18.594, 'epoch': 0.48}
{'loss': 1.2585, 'grad_norm': 0.3729779124259949, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0760202407836914, 'eval_runtime': 3.3863, 'eval_samples_per_second': 295.307, 'eval_steps_per_second': 18.604, 'epoch': 0.52}
{'loss': 1.248, 'grad_norm': 0.4099496006965637, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0722891092300415, 'eval_runtime': 3.3824, 'eval_samples_per_second': 295.652, 'eval_steps_per_second': 18.626, 'epoch': 0.56}
{'loss': 1.1857, 'grad_norm': 0.33386722207069397, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0648086071014404, 'eval_runtime': 3.3792, 'eval_samples_per_second': 295.93, 'eval_steps_per_second': 18.644, 'epoch': 0.6}
{'loss': 1.1796, 'grad_norm': 0.2612136900424957, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0611951351165771, 'eval_runtime': 3.3914, 'eval_samples_per_second': 294.867, 'eval_steps_per_second': 18.577, 'epoch': 0.64}
{'loss': 1.1911, 'grad_norm': 0.3278677463531494, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0515409708023071, 'eval_runtime': 3.3876, 'eval_samples_per_second': 295.19, 'eval_steps_per_second': 18.597, 'epoch': 0.68}
{'loss': 1.2659, 'grad_norm': 0.25719672441482544, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.055996298789978, 'eval_runtime': 3.3781, 'eval_samples_per_second': 296.022, 'eval_steps_per_second': 18.649, 'epoch': 0.72}
{'loss': 1.1469, 'grad_norm': 0.3465668261051178, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.054519772529602, 'eval_runtime': 3.3748, 'eval_samples_per_second': 296.316, 'eval_steps_per_second': 18.668, 'epoch': 0.76}
{'loss': 1.208, 'grad_norm': 0.25991570949554443, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0542529821395874, 'eval_runtime': 3.3751, 'eval_samples_per_second': 296.292, 'eval_steps_per_second': 18.666, 'epoch': 0.8}
{'loss': 1.1677, 'grad_norm': 0.2897120416164398, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0504933595657349, 'eval_runtime': 3.3807, 'eval_samples_per_second': 295.8, 'eval_steps_per_second': 18.635, 'epoch': 0.84}
{'loss': 1.1885, 'grad_norm': 0.2779572010040283, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.055753469467163, 'eval_runtime': 3.38, 'eval_samples_per_second': 295.859, 'eval_steps_per_second': 18.639, 'epoch': 0.88}
{'loss': 1.141, 'grad_norm': 0.27432864904403687, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0562939643859863, 'eval_runtime': 3.3835, 'eval_samples_per_second': 295.554, 'eval_steps_per_second': 18.62, 'epoch': 0.92}
{'loss': 1.1931, 'grad_norm': 0.2895539402961731, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.048728346824646, 'eval_runtime': 3.3873, 'eval_samples_per_second': 295.223, 'eval_steps_per_second': 18.599, 'epoch': 0.96}
{'loss': 1.1842, 'grad_norm': 0.39502716064453125, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0486185550689697, 'eval_runtime': 3.3882, 'eval_samples_per_second': 295.143, 'eval_steps_per_second': 18.594, 'epoch': 1.0}
{'train_runtime': 307.5111, 'train_samples_per_second': 32.503, 'train_steps_per_second': 2.032, 'train_loss': 1.347769155883789, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2415425777435303, 1.6229727268218994, 1.5116881132125854, 1.3638733625411987, 1.2761905193328857, 1.2872087955474854, 1.252170205116272, 1.2551918029785156, 1.231760025024414, 1.2322604656219482, 1.183035135269165, 1.140276312828064, 1.0760202407836914, 1.0722891092300415, 1.0648086071014404, 1.0611951351165771, 1.0515409708023071, 1.055996298789978, 1.054519772529602, 1.0542529821395874, 1.0504933595657349, 1.055753469467163, 1.0562939643859863, 1.048728346824646, 1.0486185550689697], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2415425777435303, 1.6229727268218994, 1.5116881132125854, 1.3638733625411987, 1.2761905193328857, 1.2872087955474854, 1.252170205116272, 1.2551918029785156, 1.231760025024414, 1.2322604656219482, 1.183035135269165, 1.140276312828064, 1.0760202407836914, 1.0722891092300415, 1.0648086071014404, 1.0611951351165771, 1.0515409708023071, 1.055996298789978, 1.054519772529602, 1.0542529821395874, 1.0504933595657349, 1.055753469467163, 1.0562939643859863, 1.048728346824646, 1.0486185550689697]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.698913097381592
current iteration best possible eval_loss (full train run):  -1.0486185550689697
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.3988 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -0.7560596617560209
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -0.7560596617560209
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -0.7560596617560209
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -0.7560596617560209
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.7560596617560209
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0626, dtype=torch.float64), tensor(0.1568, dtype=torch.float64), 0, tensor(0.4627, dtype=torch.float64), tensor(0.1445, dtype=torch.float64), tensor(0.1651, dtype=torch.float64), 0, 28, 0, 0, 0, 1, 1, 2, 0.007812853312195145, 21.0713794064989, 1]
normalized proposed parameters for next round by BO: [tensor(1.7533e-17, dtype=torch.float64), tensor(0.0083, dtype=torch.float64), tensor(0.0626, dtype=torch.float64), tensor(0.1568, dtype=torch.float64), tensor(6.8238e-18, dtype=torch.float64), tensor(0.4627, dtype=torch.float64), tensor(0.1445, dtype=torch.float64), tensor(0.1651, dtype=torch.float64), tensor(1.9177e-17, dtype=torch.float64), tensor(0.8729, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.0781, dtype=torch.float64), tensor(0.4390, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.063
  sciq: 0.157
  triviaqa: 0
  truthfulqa_gen: 0.463
  wikitext: 0.144
  mmlu: 0.165
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.007812853312195145,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (21.0713794064989,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.007812853312195145
lora alpha:  21.0713794064989
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,064,384 || all params: 8,032,325,632 || trainable%: 0.0257
length of training data:  9914
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4224, 'grad_norm': 7.186952590942383, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.168710470199585, 'eval_runtime': 3.5489, 'eval_samples_per_second': 281.775, 'eval_steps_per_second': 17.752, 'epoch': 0.04}
{'loss': 1.7779, 'grad_norm': 3.4996023178100586, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6285046339035034, 'eval_runtime': 3.2931, 'eval_samples_per_second': 303.667, 'eval_steps_per_second': 19.131, 'epoch': 0.08}
{'loss': 1.425, 'grad_norm': 2.720949649810791, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.6243302822113037, 'eval_runtime': 3.2762, 'eval_samples_per_second': 305.228, 'eval_steps_per_second': 19.229, 'epoch': 0.12}
{'loss': 1.2419, 'grad_norm': 3.6450483798980713, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.6078280210494995, 'eval_runtime': 3.2941, 'eval_samples_per_second': 303.572, 'eval_steps_per_second': 19.125, 'epoch': 0.16}
{'loss': 1.2325, 'grad_norm': 2.1402974128723145, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 1.5899275541305542, 'eval_runtime': 3.2946, 'eval_samples_per_second': 303.526, 'eval_steps_per_second': 19.122, 'epoch': 0.2}
{'loss': 1.1863, 'grad_norm': 1.7916948795318604, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 1.6715850830078125, 'eval_runtime': 3.2972, 'eval_samples_per_second': 303.283, 'eval_steps_per_second': 19.107, 'epoch': 0.24}
{'loss': 1.2097, 'grad_norm': 5.059585094451904, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 1.6563056707382202, 'eval_runtime': 3.364, 'eval_samples_per_second': 297.266, 'eval_steps_per_second': 18.728, 'epoch': 0.28}
{'loss': 1.1982, 'grad_norm': 1.535321593284607, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 1.665561318397522, 'eval_runtime': 3.3506, 'eval_samples_per_second': 298.452, 'eval_steps_per_second': 18.802, 'epoch': 0.32}
{'loss': 1.1124, 'grad_norm': 1.7234187126159668, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 1.7534960508346558, 'eval_runtime': 3.3536, 'eval_samples_per_second': 298.187, 'eval_steps_per_second': 18.786, 'epoch': 0.36}
{'loss': 1.1675, 'grad_norm': 2.315889596939087, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 1.7711386680603027, 'eval_runtime': 3.3538, 'eval_samples_per_second': 298.167, 'eval_steps_per_second': 18.785, 'epoch': 0.4}
{'loss': 1.178, 'grad_norm': 1.7895795106887817, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 1.6374866962432861, 'eval_runtime': 3.371, 'eval_samples_per_second': 296.645, 'eval_steps_per_second': 18.689, 'epoch': 0.44}
{'loss': 1.1162, 'grad_norm': 1.784040927886963, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 1.6619760990142822, 'eval_runtime': 3.3481, 'eval_samples_per_second': 298.676, 'eval_steps_per_second': 18.817, 'epoch': 0.48}
{'loss': 1.1523, 'grad_norm': 2.9278767108917236, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 1.7300549745559692, 'eval_runtime': 3.3819, 'eval_samples_per_second': 295.689, 'eval_steps_per_second': 18.628, 'epoch': 0.52}
{'loss': 1.0535, 'grad_norm': 1.581556797027588, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 1.812701940536499, 'eval_runtime': 3.3433, 'eval_samples_per_second': 299.109, 'eval_steps_per_second': 18.844, 'epoch': 0.56}
{'loss': 1.0625, 'grad_norm': 1.1210455894470215, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 1.734466314315796, 'eval_runtime': 3.3401, 'eval_samples_per_second': 299.395, 'eval_steps_per_second': 18.862, 'epoch': 0.6}
{'loss': 1.0902, 'grad_norm': 1.175158977508545, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 1.816344976425171, 'eval_runtime': 3.3459, 'eval_samples_per_second': 298.872, 'eval_steps_per_second': 18.829, 'epoch': 0.65}
{'loss': 1.0118, 'grad_norm': 1.6652535200119019, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 1.8158795833587646, 'eval_runtime': 3.3582, 'eval_samples_per_second': 297.778, 'eval_steps_per_second': 18.76, 'epoch': 0.69}
{'loss': 0.9908, 'grad_norm': 1.547337532043457, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 1.7915316820144653, 'eval_runtime': 3.3784, 'eval_samples_per_second': 296.001, 'eval_steps_per_second': 18.648, 'epoch': 0.73}
{'loss': 0.9828, 'grad_norm': 2.0232932567596436, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 1.7688403129577637, 'eval_runtime': 3.3409, 'eval_samples_per_second': 299.321, 'eval_steps_per_second': 18.857, 'epoch': 0.77}
{'loss': 1.0585, 'grad_norm': 1.4797002077102661, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 1.81765615940094, 'eval_runtime': 3.3581, 'eval_samples_per_second': 297.784, 'eval_steps_per_second': 18.76, 'epoch': 0.81}
{'loss': 0.9975, 'grad_norm': 1.4105743169784546, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 1.8575313091278076, 'eval_runtime': 3.342, 'eval_samples_per_second': 299.222, 'eval_steps_per_second': 18.851, 'epoch': 0.85}
{'loss': 1.097, 'grad_norm': 1.632078766822815, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 1.8273683786392212, 'eval_runtime': 3.35, 'eval_samples_per_second': 298.508, 'eval_steps_per_second': 18.806, 'epoch': 0.89}
{'loss': 1.0751, 'grad_norm': 2.049194812774658, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 1.8123005628585815, 'eval_runtime': 3.3402, 'eval_samples_per_second': 299.381, 'eval_steps_per_second': 18.861, 'epoch': 0.93}
{'loss': 1.026, 'grad_norm': 1.5122367143630981, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 1.851155400276184, 'eval_runtime': 3.348, 'eval_samples_per_second': 298.687, 'eval_steps_per_second': 18.817, 'epoch': 0.97}
{'train_runtime': 281.2603, 'train_samples_per_second': 35.248, 'train_steps_per_second': 2.204, 'train_loss': 1.2317654763498613, 'epoch': 1.0}
train_results:  {'eval_loss': [2.168710470199585, 1.6285046339035034, 1.6243302822113037, 1.6078280210494995, 1.5899275541305542, 1.6715850830078125, 1.6563056707382202, 1.665561318397522, 1.7534960508346558, 1.7711386680603027, 1.6374866962432861, 1.6619760990142822, 1.7300549745559692, 1.812701940536499, 1.734466314315796, 1.816344976425171, 1.8158795833587646, 1.7915316820144653, 1.7688403129577637, 1.81765615940094, 1.8575313091278076, 1.8273683786392212, 1.8123005628585815, 1.851155400276184], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.168710470199585, 1.6285046339035034, 1.6243302822113037, 1.6078280210494995, 1.5899275541305542, 1.6715850830078125, 1.6563056707382202, 1.665561318397522, 1.7534960508346558, 1.7711386680603027, 1.6374866962432861, 1.6619760990142822, 1.7300549745559692, 1.812701940536499, 1.734466314315796, 1.816344976425171, 1.8158795833587646, 1.7915316820144653, 1.7688403129577637, 1.81765615940094, 1.8575313091278076, 1.8273683786392212, 1.8123005628585815, 1.851155400276184]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.0973877906799316
current iteration best possible eval_loss (full train run):  -1.851155400276184
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592, -2.0973877906799316]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.2139 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1793595552444458, 0.3586342930793762, 0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.34960928559303284, 0.057854533195495605, 0.7710180282592773, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6546881198883057, 0.3577408194541931, 0.8840495347976685, 0.5400984883308411]  ‚Üí  acq = -0.8038592093091663
X = [0.7988576889038086, 0.1653779149055481, 0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9513463973999023, 0.8898367881774902, 0.2988312840461731, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.24033895134925842, 0.2050917148590088, 0.051226988434791565, 0.8115118741989136]  ‚Üí  acq = -0.7784422897199312
X = [0.22445803880691528, 0.07812052965164185, 0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9895481467247009, 0.6373879313468933, 0.9490465521812439, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6339337825775146, 0.44919973611831665, 0.9787859916687012, 0.45290279388427734]  ‚Üí  acq = -0.8189664299805686
X = [0.3654218316078186, 0.6524207592010498, 0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.8971459269523621, 0.9170647859573364, 0.6597838997840881, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.8601893186569214, 0.164650559425354, 0.14350587129592896, 0.7966952323913574]  ‚Üí  acq = -0.778475315453478
X = [0.4475772976875305, 0.6951091885566711, 0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.06307335197925568, 0.7526708841323853, 0.20946532487869263, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.10499551892280579, 0.07692551612854004, 0.719855546951294, 0.04362046718597412]  ‚Üí  acq = -0.7996302100793564
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0196, dtype=torch.float64), tensor(0.4257, dtype=torch.float64), tensor(0.1567, dtype=torch.float64), 0, 0, tensor(0.1001, dtype=torch.float64), tensor(0.1329, dtype=torch.float64), 0, tensor(0.1648, dtype=torch.float64), 32, 0, 1, 0, 0, 1, 115, 0.040591611131585006, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0196, dtype=torch.float64), tensor(0.4257, dtype=torch.float64), tensor(0.1567, dtype=torch.float64), tensor(1.4163e-18, dtype=torch.float64), tensor(0.0001, dtype=torch.float64), tensor(0.1001, dtype=torch.float64), tensor(0.1329, dtype=torch.float64), tensor(3.4298e-18, dtype=torch.float64), tensor(0.1648, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9017, dtype=torch.float64), tensor(0.4059, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.02
  gsm8k: 0.426
  rowan_hellaswag: 0.157
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.1
  wikitext: 0.133
  mmlu: 0
  arc_challenge: 0.165

LoRA Parameters:
  lora_r: (115,)
  lora_dropout: (0.040591611131585006,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  115
lora dropout:  0.040591611131585006
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 86,671,360 || all params: 8,116,932,608 || trainable%: 1.0678
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5723, 'grad_norm': 0.3302184045314789, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.404470443725586, 'eval_runtime': 3.3498, 'eval_samples_per_second': 298.524, 'eval_steps_per_second': 18.807, 'epoch': 0.04}
{'loss': 1.417, 'grad_norm': 0.23532423377037048, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9590702056884766, 'eval_runtime': 3.333, 'eval_samples_per_second': 300.032, 'eval_steps_per_second': 18.902, 'epoch': 0.08}
{'loss': 1.3267, 'grad_norm': 0.232199028134346, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.950460433959961, 'eval_runtime': 3.3272, 'eval_samples_per_second': 300.555, 'eval_steps_per_second': 18.935, 'epoch': 0.12}
{'loss': 1.2286, 'grad_norm': 0.2523605525493622, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.856006145477295, 'eval_runtime': 3.3211, 'eval_samples_per_second': 301.106, 'eval_steps_per_second': 18.97, 'epoch': 0.16}
{'loss': 1.1751, 'grad_norm': 0.19630402326583862, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.771237850189209, 'eval_runtime': 3.3252, 'eval_samples_per_second': 300.734, 'eval_steps_per_second': 18.946, 'epoch': 0.2}
{'loss': 1.163, 'grad_norm': 0.20856451988220215, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7738782167434692, 'eval_runtime': 3.3274, 'eval_samples_per_second': 300.532, 'eval_steps_per_second': 18.934, 'epoch': 0.24}
{'loss': 1.1248, 'grad_norm': 0.17131184041500092, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.821384310722351, 'eval_runtime': 3.3241, 'eval_samples_per_second': 300.837, 'eval_steps_per_second': 18.953, 'epoch': 0.28}
{'loss': 1.1058, 'grad_norm': 0.18416182696819305, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7284001111984253, 'eval_runtime': 3.3301, 'eval_samples_per_second': 300.292, 'eval_steps_per_second': 18.918, 'epoch': 0.32}
{'loss': 1.0947, 'grad_norm': 0.22708646953105927, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8992063999176025, 'eval_runtime': 3.3255, 'eval_samples_per_second': 300.706, 'eval_steps_per_second': 18.944, 'epoch': 0.36}
{'loss': 1.0931, 'grad_norm': 0.22445012629032135, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7745388746261597, 'eval_runtime': 3.3258, 'eval_samples_per_second': 300.68, 'eval_steps_per_second': 18.943, 'epoch': 0.4}
{'loss': 1.1247, 'grad_norm': 0.20818383991718292, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7032971382141113, 'eval_runtime': 3.3271, 'eval_samples_per_second': 300.563, 'eval_steps_per_second': 18.935, 'epoch': 0.44}
{'loss': 1.1177, 'grad_norm': 0.2023753970861435, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7379997968673706, 'eval_runtime': 3.3337, 'eval_samples_per_second': 299.965, 'eval_steps_per_second': 18.898, 'epoch': 0.48}
{'loss': 1.0303, 'grad_norm': 0.18891948461532593, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7638702392578125, 'eval_runtime': 3.329, 'eval_samples_per_second': 300.391, 'eval_steps_per_second': 18.925, 'epoch': 0.52}
{'loss': 1.1108, 'grad_norm': 0.18109889328479767, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7337191104888916, 'eval_runtime': 3.3444, 'eval_samples_per_second': 299.006, 'eval_steps_per_second': 18.837, 'epoch': 0.56}
{'loss': 1.118, 'grad_norm': 0.18658031523227692, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8004417419433594, 'eval_runtime': 3.36, 'eval_samples_per_second': 297.62, 'eval_steps_per_second': 18.75, 'epoch': 0.6}
{'loss': 1.0719, 'grad_norm': 0.1941884458065033, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8291231393814087, 'eval_runtime': 3.356, 'eval_samples_per_second': 297.972, 'eval_steps_per_second': 18.772, 'epoch': 0.64}
{'loss': 1.1054, 'grad_norm': 0.20005235075950623, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.822135329246521, 'eval_runtime': 3.3475, 'eval_samples_per_second': 298.733, 'eval_steps_per_second': 18.82, 'epoch': 0.68}
{'loss': 1.0561, 'grad_norm': 0.2114599496126175, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7882615327835083, 'eval_runtime': 3.351, 'eval_samples_per_second': 298.419, 'eval_steps_per_second': 18.8, 'epoch': 0.72}
{'loss': 1.0696, 'grad_norm': 0.2592361271381378, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7728703022003174, 'eval_runtime': 3.3307, 'eval_samples_per_second': 300.234, 'eval_steps_per_second': 18.915, 'epoch': 0.76}
{'loss': 1.0458, 'grad_norm': 0.1819377839565277, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.766737699508667, 'eval_runtime': 3.3311, 'eval_samples_per_second': 300.198, 'eval_steps_per_second': 18.912, 'epoch': 0.8}
{'loss': 1.1029, 'grad_norm': 0.17980104684829712, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8033673763275146, 'eval_runtime': 3.3321, 'eval_samples_per_second': 300.111, 'eval_steps_per_second': 18.907, 'epoch': 0.84}
{'loss': 1.0709, 'grad_norm': 0.20177598297595978, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8528292179107666, 'eval_runtime': 3.3415, 'eval_samples_per_second': 299.262, 'eval_steps_per_second': 18.854, 'epoch': 0.88}
{'loss': 1.1209, 'grad_norm': 0.24946430325508118, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.838948369026184, 'eval_runtime': 3.3444, 'eval_samples_per_second': 299.004, 'eval_steps_per_second': 18.837, 'epoch': 0.92}
{'loss': 1.0373, 'grad_norm': 0.23662027716636658, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8389911651611328, 'eval_runtime': 3.3552, 'eval_samples_per_second': 298.049, 'eval_steps_per_second': 18.777, 'epoch': 0.96}
{'loss': 1.0306, 'grad_norm': 0.2712021470069885, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8330087661743164, 'eval_runtime': 3.3522, 'eval_samples_per_second': 298.313, 'eval_steps_per_second': 18.794, 'epoch': 1.0}
{'train_runtime': 310.4636, 'train_samples_per_second': 32.197, 'train_steps_per_second': 2.013, 'train_loss': 1.1805664794921875, 'epoch': 1.0}
train_results:  {'eval_loss': [2.404470443725586, 1.9590702056884766, 1.950460433959961, 1.856006145477295, 1.771237850189209, 1.7738782167434692, 1.821384310722351, 1.7284001111984253, 1.8992063999176025, 1.7745388746261597, 1.7032971382141113, 1.7379997968673706, 1.7638702392578125, 1.7337191104888916, 1.8004417419433594, 1.8291231393814087, 1.822135329246521, 1.7882615327835083, 1.7728703022003174, 1.766737699508667, 1.8033673763275146, 1.8528292179107666, 1.838948369026184, 1.8389911651611328, 1.8330087661743164], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.404470443725586, 1.9590702056884766, 1.950460433959961, 1.856006145477295, 1.771237850189209, 1.7738782167434692, 1.821384310722351, 1.7284001111984253, 1.8992063999176025, 1.7745388746261597, 1.7032971382141113, 1.7379997968673706, 1.7638702392578125, 1.7337191104888916, 1.8004417419433594, 1.8291231393814087, 1.822135329246521, 1.7882615327835083, 1.7728703022003174, 1.766737699508667, 1.8033673763275146, 1.8528292179107666, 1.838948369026184, 1.8389911651611328, 1.8330087661743164]
current iteration observed (possibly low-fid or predicted) eval_loss:  -4.346683025360107
current iteration best possible eval_loss (full train run):  -1.8330087661743164
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592, -2.0973877906799316, -4.346683025360107]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.3788 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -0.6265059033464011
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -0.5624737613451609
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -0.624133274347425
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -0.6316300024858206
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -0.6258327185304127
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0950, dtype=torch.float64), tensor(0.0256, dtype=torch.float64), tensor(0.0302, dtype=torch.float64), tensor(0.0660, dtype=torch.float64), tensor(0.2919, dtype=torch.float64), tensor(0.2510, dtype=torch.float64), tensor(0.0907, dtype=torch.float64), tensor(0.0352, dtype=torch.float64), tensor(0.1143, dtype=torch.float64), 9, 1, 0, 1, 1, 1, 19, 0.013979734770157884, 27.10072024694999, 0]
normalized proposed parameters for next round by BO: [tensor(0.0950, dtype=torch.float64), tensor(0.0256, dtype=torch.float64), tensor(0.0302, dtype=torch.float64), tensor(0.0660, dtype=torch.float64), tensor(0.2919, dtype=torch.float64), tensor(0.2510, dtype=torch.float64), tensor(0.0907, dtype=torch.float64), tensor(0.0352, dtype=torch.float64), tensor(0.1143, dtype=torch.float64), tensor(0.2817, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1458, dtype=torch.float64), tensor(0.1398, dtype=torch.float64), tensor(0.5646, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.095
  gsm8k: 0.026
  rowan_hellaswag: 0.03
  sciq: 0.066
  triviaqa: 0.292
  truthfulqa_gen: 0.251
  wikitext: 0.091
  mmlu: 0.035
  arc_challenge: 0.114

LoRA Parameters:
  lora_r: (19,)
  lora_dropout: (0.013979734770157884,)
  num_layers_to_apply: (9,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (27.10072024694999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  9
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  19
lora dropout:  0.013979734770157884
lora alpha:  27.10072024694999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 10,856,448 || all params: 8,041,117,696 || trainable%: 0.1350
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8228, 'grad_norm': 2.1076607704162598, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2844576835632324, 'eval_runtime': 4.8197, 'eval_samples_per_second': 207.481, 'eval_steps_per_second': 13.071, 'epoch': 0.04}
{'loss': 1.6902, 'grad_norm': 0.7111268639564514, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3003313541412354, 'eval_runtime': 3.1357, 'eval_samples_per_second': 318.906, 'eval_steps_per_second': 20.091, 'epoch': 0.08}
{'loss': 1.3117, 'grad_norm': 0.6058187484741211, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.216378927230835, 'eval_runtime': 3.1394, 'eval_samples_per_second': 318.534, 'eval_steps_per_second': 20.068, 'epoch': 0.12}
{'loss': 1.2237, 'grad_norm': 0.4313374161720276, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2013200521469116, 'eval_runtime': 3.1506, 'eval_samples_per_second': 317.398, 'eval_steps_per_second': 19.996, 'epoch': 0.16}
{'loss': 1.2863, 'grad_norm': 0.4818668067455292, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1951549053192139, 'eval_runtime': 3.1279, 'eval_samples_per_second': 319.7, 'eval_steps_per_second': 20.141, 'epoch': 0.2}
{'loss': 1.1688, 'grad_norm': 0.43830564618110657, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1950634717941284, 'eval_runtime': 3.1403, 'eval_samples_per_second': 318.442, 'eval_steps_per_second': 20.062, 'epoch': 0.24}
{'loss': 1.1434, 'grad_norm': 0.3703872263431549, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.172487735748291, 'eval_runtime': 3.1356, 'eval_samples_per_second': 318.923, 'eval_steps_per_second': 20.092, 'epoch': 0.28}
{'loss': 1.1434, 'grad_norm': 0.4413088262081146, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.147650122642517, 'eval_runtime': 3.1442, 'eval_samples_per_second': 318.042, 'eval_steps_per_second': 20.037, 'epoch': 0.32}
{'loss': 1.1166, 'grad_norm': 0.40609899163246155, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.161988377571106, 'eval_runtime': 3.1425, 'eval_samples_per_second': 318.216, 'eval_steps_per_second': 20.048, 'epoch': 0.36}
{'loss': 1.2092, 'grad_norm': 0.5355935096740723, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1510283946990967, 'eval_runtime': 3.1407, 'eval_samples_per_second': 318.398, 'eval_steps_per_second': 20.059, 'epoch': 0.4}
{'loss': 1.1589, 'grad_norm': 0.6398899555206299, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.141374111175537, 'eval_runtime': 3.1342, 'eval_samples_per_second': 319.063, 'eval_steps_per_second': 20.101, 'epoch': 0.44}
{'loss': 1.1528, 'grad_norm': 0.489452600479126, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1435019969940186, 'eval_runtime': 3.1399, 'eval_samples_per_second': 318.486, 'eval_steps_per_second': 20.065, 'epoch': 0.48}
{'loss': 1.1263, 'grad_norm': 0.49115830659866333, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1371618509292603, 'eval_runtime': 3.1598, 'eval_samples_per_second': 316.477, 'eval_steps_per_second': 19.938, 'epoch': 0.52}
{'loss': 1.122, 'grad_norm': 0.5012274980545044, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.13155198097229, 'eval_runtime': 3.1615, 'eval_samples_per_second': 316.307, 'eval_steps_per_second': 19.927, 'epoch': 0.56}
{'loss': 1.1024, 'grad_norm': 0.5905569791793823, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1368398666381836, 'eval_runtime': 3.1492, 'eval_samples_per_second': 317.541, 'eval_steps_per_second': 20.005, 'epoch': 0.6}
{'loss': 1.1431, 'grad_norm': 0.5070608854293823, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1317431926727295, 'eval_runtime': 3.1581, 'eval_samples_per_second': 316.643, 'eval_steps_per_second': 19.948, 'epoch': 0.64}
{'loss': 1.1137, 'grad_norm': 0.4433915913105011, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1263442039489746, 'eval_runtime': 3.1557, 'eval_samples_per_second': 316.887, 'eval_steps_per_second': 19.964, 'epoch': 0.68}
{'loss': 1.1736, 'grad_norm': 0.5734524130821228, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1282457113265991, 'eval_runtime': 3.1492, 'eval_samples_per_second': 317.54, 'eval_steps_per_second': 20.005, 'epoch': 0.72}
{'loss': 1.0696, 'grad_norm': 0.5998029112815857, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.126999855041504, 'eval_runtime': 3.1549, 'eval_samples_per_second': 316.966, 'eval_steps_per_second': 19.969, 'epoch': 0.76}
{'loss': 1.1451, 'grad_norm': 0.6043528318405151, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1219391822814941, 'eval_runtime': 3.182, 'eval_samples_per_second': 314.266, 'eval_steps_per_second': 19.799, 'epoch': 0.8}
{'loss': 1.1668, 'grad_norm': 0.5383024215698242, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1189607381820679, 'eval_runtime': 3.1747, 'eval_samples_per_second': 314.991, 'eval_steps_per_second': 19.844, 'epoch': 0.84}
{'loss': 1.1213, 'grad_norm': 0.5764548182487488, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1177492141723633, 'eval_runtime': 3.1761, 'eval_samples_per_second': 314.856, 'eval_steps_per_second': 19.836, 'epoch': 0.88}
{'loss': 1.108, 'grad_norm': 0.609412431716919, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1187255382537842, 'eval_runtime': 3.1665, 'eval_samples_per_second': 315.803, 'eval_steps_per_second': 19.896, 'epoch': 0.92}
{'loss': 1.1357, 'grad_norm': 0.536264955997467, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1182341575622559, 'eval_runtime': 3.1581, 'eval_samples_per_second': 316.645, 'eval_steps_per_second': 19.949, 'epoch': 0.96}
{'loss': 1.0959, 'grad_norm': 0.8091198205947876, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1185102462768555, 'eval_runtime': 3.1946, 'eval_samples_per_second': 313.027, 'eval_steps_per_second': 19.721, 'epoch': 1.0}
{'train_runtime': 198.0794, 'train_samples_per_second': 50.465, 'train_steps_per_second': 3.155, 'train_loss': 1.2820479705810548, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2844576835632324, 1.3003313541412354, 1.216378927230835, 1.2013200521469116, 1.1951549053192139, 1.1950634717941284, 1.172487735748291, 1.147650122642517, 1.161988377571106, 1.1510283946990967, 1.141374111175537, 1.1435019969940186, 1.1371618509292603, 1.13155198097229, 1.1368398666381836, 1.1317431926727295, 1.1263442039489746, 1.1282457113265991, 1.126999855041504, 1.1219391822814941, 1.1189607381820679, 1.1177492141723633, 1.1187255382537842, 1.1182341575622559, 1.1185102462768555], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2844576835632324, 1.3003313541412354, 1.216378927230835, 1.2013200521469116, 1.1951549053192139, 1.1950634717941284, 1.172487735748291, 1.147650122642517, 1.161988377571106, 1.1510283946990967, 1.141374111175537, 1.1435019969940186, 1.1371618509292603, 1.13155198097229, 1.1368398666381836, 1.1317431926727295, 1.1263442039489746, 1.1282457113265991, 1.126999855041504, 1.1219391822814941, 1.1189607381820679, 1.1177492141723633, 1.1187255382537842, 1.1182341575622559, 1.1185102462768555]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.8457581996917725
current iteration best possible eval_loss (full train run):  -1.1185102462768555
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592, -2.0973877906799316, -4.346683025360107, -1.8457581996917725]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.2109 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -0.5779325524520553
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -0.5779325524520553
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -0.5779325524520553
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -0.5779325524520553
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -0.5779325524520553
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0121, dtype=torch.float64), 0, tensor(0.0285, dtype=torch.float64), tensor(0.2003, dtype=torch.float64), tensor(0.1564, dtype=torch.float64), tensor(0.5750, dtype=torch.float64), tensor(0.0124, dtype=torch.float64), tensor(0.0154, dtype=torch.float64), 0, 12, 0, 0, 0, 1, 1, 90, 0.07476041509448453, 27.01331555057135, 0]
normalized proposed parameters for next round by BO: [tensor(0.0121, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0285, dtype=torch.float64), tensor(0.2003, dtype=torch.float64), tensor(0.1564, dtype=torch.float64), tensor(0.5750, dtype=torch.float64), tensor(0.0124, dtype=torch.float64), tensor(0.0154, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3785, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7009, dtype=torch.float64), tensor(0.7476, dtype=torch.float64), tensor(0.5628, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.012
  gsm8k: 0
  rowan_hellaswag: 0.029
  sciq: 0.2
  triviaqa: 0.156
  truthfulqa_gen: 0.575
  wikitext: 0.012
  mmlu: 0.015
  arc_challenge: 0

LoRA Parameters:
  lora_r: (90,)
  lora_dropout: (0.07476041509448453,)
  num_layers_to_apply: (12,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (27.01331555057135,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  12
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  90
lora dropout:  0.07476041509448453
lora alpha:  27.01331555057135
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 39,813,120 || all params: 8,070,074,368 || trainable%: 0.4933
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.1685, 'grad_norm': 1.4470949172973633, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.5064568519592285, 'eval_runtime': 5.8029, 'eval_samples_per_second': 172.327, 'eval_steps_per_second': 10.857, 'epoch': 0.04}
{'loss': 1.5958, 'grad_norm': 0.5720715522766113, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.429828405380249, 'eval_runtime': 3.059, 'eval_samples_per_second': 326.9, 'eval_steps_per_second': 20.595, 'epoch': 0.08}
{'loss': 1.1995, 'grad_norm': 0.26676785945892334, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2671531438827515, 'eval_runtime': 3.064, 'eval_samples_per_second': 326.369, 'eval_steps_per_second': 20.561, 'epoch': 0.12}
{'loss': 1.0664, 'grad_norm': 0.20682409405708313, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2637104988098145, 'eval_runtime': 3.0656, 'eval_samples_per_second': 326.198, 'eval_steps_per_second': 20.55, 'epoch': 0.16}
{'loss': 0.9791, 'grad_norm': 0.2378082573413849, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2400175333023071, 'eval_runtime': 3.0637, 'eval_samples_per_second': 326.405, 'eval_steps_per_second': 20.564, 'epoch': 0.2}
{'loss': 0.9817, 'grad_norm': 0.2488037347793579, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2360345125198364, 'eval_runtime': 3.0505, 'eval_samples_per_second': 327.816, 'eval_steps_per_second': 20.652, 'epoch': 0.24}
{'loss': 0.9661, 'grad_norm': 0.2700382471084595, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2195098400115967, 'eval_runtime': 3.0549, 'eval_samples_per_second': 327.348, 'eval_steps_per_second': 20.623, 'epoch': 0.28}
{'loss': 0.9265, 'grad_norm': 0.2590687870979309, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2110334634780884, 'eval_runtime': 3.0603, 'eval_samples_per_second': 326.77, 'eval_steps_per_second': 20.586, 'epoch': 0.32}
{'loss': 0.8748, 'grad_norm': 0.27160993218421936, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.209458827972412, 'eval_runtime': 3.0689, 'eval_samples_per_second': 325.85, 'eval_steps_per_second': 20.529, 'epoch': 0.36}
{'loss': 0.8899, 'grad_norm': 0.30711549520492554, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2001956701278687, 'eval_runtime': 3.0625, 'eval_samples_per_second': 326.531, 'eval_steps_per_second': 20.571, 'epoch': 0.4}
{'loss': 0.9093, 'grad_norm': 0.3258676826953888, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2203845977783203, 'eval_runtime': 3.0928, 'eval_samples_per_second': 323.336, 'eval_steps_per_second': 20.37, 'epoch': 0.44}
{'loss': 0.8553, 'grad_norm': 0.32803672552108765, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2022294998168945, 'eval_runtime': 3.1088, 'eval_samples_per_second': 321.671, 'eval_steps_per_second': 20.265, 'epoch': 0.48}
{'loss': 0.8571, 'grad_norm': 0.34943535923957825, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1988391876220703, 'eval_runtime': 3.1137, 'eval_samples_per_second': 321.162, 'eval_steps_per_second': 20.233, 'epoch': 0.52}
{'loss': 0.7887, 'grad_norm': 0.24324972927570343, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1925365924835205, 'eval_runtime': 3.1278, 'eval_samples_per_second': 319.715, 'eval_steps_per_second': 20.142, 'epoch': 0.56}
{'loss': 0.765, 'grad_norm': 0.41895410418510437, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1891601085662842, 'eval_runtime': 3.1257, 'eval_samples_per_second': 319.932, 'eval_steps_per_second': 20.156, 'epoch': 0.6}
{'loss': 0.8083, 'grad_norm': 0.2568109333515167, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1822922229766846, 'eval_runtime': 3.1145, 'eval_samples_per_second': 321.075, 'eval_steps_per_second': 20.228, 'epoch': 0.64}
{'loss': 0.748, 'grad_norm': 0.2978859841823578, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1889594793319702, 'eval_runtime': 3.121, 'eval_samples_per_second': 320.415, 'eval_steps_per_second': 20.186, 'epoch': 0.68}
{'loss': 0.7881, 'grad_norm': 0.3985861837863922, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1800322532653809, 'eval_runtime': 3.1142, 'eval_samples_per_second': 321.108, 'eval_steps_per_second': 20.23, 'epoch': 0.72}
{'loss': 0.7942, 'grad_norm': 0.48020216822624207, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.166206955909729, 'eval_runtime': 3.1334, 'eval_samples_per_second': 319.145, 'eval_steps_per_second': 20.106, 'epoch': 0.76}
{'loss': 0.6937, 'grad_norm': 0.32750844955444336, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.168817162513733, 'eval_runtime': 3.1083, 'eval_samples_per_second': 321.716, 'eval_steps_per_second': 20.268, 'epoch': 0.8}
{'loss': 0.6896, 'grad_norm': 0.37490829825401306, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1732960939407349, 'eval_runtime': 3.0946, 'eval_samples_per_second': 323.144, 'eval_steps_per_second': 20.358, 'epoch': 0.84}
{'loss': 0.6832, 'grad_norm': 0.31452932953834534, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1688190698623657, 'eval_runtime': 3.0921, 'eval_samples_per_second': 323.405, 'eval_steps_per_second': 20.375, 'epoch': 0.88}
{'loss': 0.7719, 'grad_norm': 0.27026432752609253, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1657319068908691, 'eval_runtime': 3.1078, 'eval_samples_per_second': 321.776, 'eval_steps_per_second': 20.272, 'epoch': 0.92}
{'loss': 0.6863, 'grad_norm': 0.3425144851207733, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1639487743377686, 'eval_runtime': 3.1083, 'eval_samples_per_second': 321.717, 'eval_steps_per_second': 20.268, 'epoch': 0.96}
{'loss': 0.6703, 'grad_norm': 0.3304937183856964, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1634087562561035, 'eval_runtime': 3.1125, 'eval_samples_per_second': 321.289, 'eval_steps_per_second': 20.241, 'epoch': 1.0}
{'train_runtime': 168.8694, 'train_samples_per_second': 59.2, 'train_steps_per_second': 3.701, 'train_loss': 1.0062874572753906, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5064568519592285, 1.429828405380249, 1.2671531438827515, 1.2637104988098145, 1.2400175333023071, 1.2360345125198364, 1.2195098400115967, 1.2110334634780884, 1.209458827972412, 1.2001956701278687, 1.2203845977783203, 1.2022294998168945, 1.1988391876220703, 1.1925365924835205, 1.1891601085662842, 1.1822922229766846, 1.1889594793319702, 1.1800322532653809, 1.166206955909729, 1.168817162513733, 1.1732960939407349, 1.1688190698623657, 1.1657319068908691, 1.1639487743377686, 1.1634087562561035], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.5064568519592285, 1.429828405380249, 1.2671531438827515, 1.2637104988098145, 1.2400175333023071, 1.2360345125198364, 1.2195098400115967, 1.2110334634780884, 1.209458827972412, 1.2001956701278687, 1.2203845977783203, 1.2022294998168945, 1.1988391876220703, 1.1925365924835205, 1.1891601085662842, 1.1822922229766846, 1.1889594793319702, 1.1800322532653809, 1.166206955909729, 1.168817162513733, 1.1732960939407349, 1.1688190698623657, 1.1657319068908691, 1.1639487743377686, 1.1634087562561035]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.1989521980285645
current iteration best possible eval_loss (full train run):  -1.1634087562561035
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592, -2.0973877906799316, -4.346683025360107, -1.8457581996917725, -3.1989521980285645]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 18.4658 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -0.5335723349378918
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -0.5335723349378918
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -0.5335723349378918
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -0.5335723349378918
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -0.5335723349378918
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3436, dtype=torch.float64), tensor(0.0926, dtype=torch.float64), tensor(0.0513, dtype=torch.float64), tensor(0.1557, dtype=torch.float64), 0, 0, tensor(0.2906, dtype=torch.float64), 0, tensor(0.0663, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 5, 0.01811631925323592, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.3436, dtype=torch.float64), tensor(0.0926, dtype=torch.float64), tensor(0.0513, dtype=torch.float64), tensor(0.1557, dtype=torch.float64), tensor(2.9326e-21, dtype=torch.float64), tensor(4.9193e-18, dtype=torch.float64), tensor(0.2906, dtype=torch.float64), tensor(9.9626e-17, dtype=torch.float64), tensor(0.0663, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0356, dtype=torch.float64), tensor(0.1812, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.344
  gsm8k: 0.093
  rowan_hellaswag: 0.051
  sciq: 0.156
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.291
  mmlu: 0
  arc_challenge: 0.066

LoRA Parameters:
  lora_r: (5,)
  lora_dropout: (0.01811631925323592,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  5
lora dropout:  0.01811631925323592
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 10,158,080 || all params: 8,040,419,328 || trainable%: 0.1263
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8017, 'grad_norm': 0.8628695011138916, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.2606546878814697, 'eval_runtime': 3.9129, 'eval_samples_per_second': 255.566, 'eval_steps_per_second': 16.101, 'epoch': 0.04}
{'loss': 2.1945, 'grad_norm': 0.7752149105072021, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.0930776596069336, 'eval_runtime': 3.8542, 'eval_samples_per_second': 259.455, 'eval_steps_per_second': 16.346, 'epoch': 0.08}
{'loss': 1.5243, 'grad_norm': 0.5701509714126587, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7657256126403809, 'eval_runtime': 3.853, 'eval_samples_per_second': 259.536, 'eval_steps_per_second': 16.351, 'epoch': 0.12}
{'loss': 1.4979, 'grad_norm': 0.35100939869880676, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7496147155761719, 'eval_runtime': 3.8473, 'eval_samples_per_second': 259.92, 'eval_steps_per_second': 16.375, 'epoch': 0.16}
{'loss': 1.2802, 'grad_norm': 0.29602137207984924, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6772879362106323, 'eval_runtime': 3.845, 'eval_samples_per_second': 260.076, 'eval_steps_per_second': 16.385, 'epoch': 0.2}
{'loss': 1.3199, 'grad_norm': 0.2616056203842163, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6978087425231934, 'eval_runtime': 3.8402, 'eval_samples_per_second': 260.4, 'eval_steps_per_second': 16.405, 'epoch': 0.24}
{'loss': 1.1921, 'grad_norm': 0.3259256184101105, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6748563051223755, 'eval_runtime': 3.8367, 'eval_samples_per_second': 260.637, 'eval_steps_per_second': 16.42, 'epoch': 0.28}
{'loss': 1.237, 'grad_norm': 0.25266388058662415, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7110799551010132, 'eval_runtime': 3.8518, 'eval_samples_per_second': 259.619, 'eval_steps_per_second': 16.356, 'epoch': 0.32}
{'loss': 1.2768, 'grad_norm': 0.2675377428531647, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6538625955581665, 'eval_runtime': 3.8449, 'eval_samples_per_second': 260.084, 'eval_steps_per_second': 16.385, 'epoch': 0.36}
{'loss': 1.2354, 'grad_norm': 0.24496878683567047, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.820327639579773, 'eval_runtime': 3.8484, 'eval_samples_per_second': 259.845, 'eval_steps_per_second': 16.37, 'epoch': 0.4}
{'loss': 1.2651, 'grad_norm': 0.24367755651474, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.732553482055664, 'eval_runtime': 3.8501, 'eval_samples_per_second': 259.732, 'eval_steps_per_second': 16.363, 'epoch': 0.44}
{'loss': 1.2374, 'grad_norm': 0.2802976369857788, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.800101399421692, 'eval_runtime': 3.8527, 'eval_samples_per_second': 259.555, 'eval_steps_per_second': 16.352, 'epoch': 0.48}
{'loss': 1.2499, 'grad_norm': 0.24987033009529114, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7025119066238403, 'eval_runtime': 3.8491, 'eval_samples_per_second': 259.801, 'eval_steps_per_second': 16.367, 'epoch': 0.52}
{'loss': 1.2111, 'grad_norm': 0.24471043050289154, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7126965522766113, 'eval_runtime': 3.846, 'eval_samples_per_second': 260.012, 'eval_steps_per_second': 16.381, 'epoch': 0.56}
{'loss': 1.2853, 'grad_norm': 0.32642999291419983, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7350060939788818, 'eval_runtime': 3.8594, 'eval_samples_per_second': 259.106, 'eval_steps_per_second': 16.324, 'epoch': 0.6}
{'loss': 1.2098, 'grad_norm': 0.27476391196250916, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.734377145767212, 'eval_runtime': 3.8773, 'eval_samples_per_second': 257.914, 'eval_steps_per_second': 16.249, 'epoch': 0.64}
{'loss': 1.2267, 'grad_norm': 0.26793545484542847, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7950575351715088, 'eval_runtime': 3.8693, 'eval_samples_per_second': 258.446, 'eval_steps_per_second': 16.282, 'epoch': 0.68}
{'loss': 1.1571, 'grad_norm': 0.31158795952796936, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7732551097869873, 'eval_runtime': 3.8638, 'eval_samples_per_second': 258.81, 'eval_steps_per_second': 16.305, 'epoch': 0.72}
{'loss': 1.2171, 'grad_norm': 0.27020734548568726, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.784305453300476, 'eval_runtime': 3.862, 'eval_samples_per_second': 258.934, 'eval_steps_per_second': 16.313, 'epoch': 0.76}
{'loss': 1.2774, 'grad_norm': 0.2752210199832916, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7653216123580933, 'eval_runtime': 3.8647, 'eval_samples_per_second': 258.755, 'eval_steps_per_second': 16.302, 'epoch': 0.8}
{'loss': 1.2231, 'grad_norm': 0.291752427816391, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7516058683395386, 'eval_runtime': 3.8642, 'eval_samples_per_second': 258.783, 'eval_steps_per_second': 16.303, 'epoch': 0.84}
{'loss': 1.2106, 'grad_norm': 0.23771952092647552, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7476930618286133, 'eval_runtime': 3.8557, 'eval_samples_per_second': 259.358, 'eval_steps_per_second': 16.34, 'epoch': 0.88}
{'loss': 1.1827, 'grad_norm': 0.27338677644729614, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.755357265472412, 'eval_runtime': 3.8552, 'eval_samples_per_second': 259.393, 'eval_steps_per_second': 16.342, 'epoch': 0.92}
{'loss': 1.1809, 'grad_norm': 0.23900556564331055, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.757020354270935, 'eval_runtime': 3.8572, 'eval_samples_per_second': 259.257, 'eval_steps_per_second': 16.333, 'epoch': 0.96}
{'loss': 1.2646, 'grad_norm': 0.2742500603199005, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7603992223739624, 'eval_runtime': 3.8588, 'eval_samples_per_second': 259.147, 'eval_steps_per_second': 16.326, 'epoch': 1.0}
{'train_runtime': 349.5413, 'train_samples_per_second': 28.6, 'train_steps_per_second': 1.788, 'train_loss': 1.3983502044677734, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2606546878814697, 2.0930776596069336, 1.7657256126403809, 1.7496147155761719, 1.6772879362106323, 1.6978087425231934, 1.6748563051223755, 1.7110799551010132, 1.6538625955581665, 1.820327639579773, 1.732553482055664, 1.800101399421692, 1.7025119066238403, 1.7126965522766113, 1.7350060939788818, 1.734377145767212, 1.7950575351715088, 1.7732551097869873, 1.784305453300476, 1.7653216123580933, 1.7516058683395386, 1.7476930618286133, 1.755357265472412, 1.757020354270935, 1.7603992223739624], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.2606546878814697, 2.0930776596069336, 1.7657256126403809, 1.7496147155761719, 1.6772879362106323, 1.6978087425231934, 1.6748563051223755, 1.7110799551010132, 1.6538625955581665, 1.820327639579773, 1.732553482055664, 1.800101399421692, 1.7025119066238403, 1.7126965522766113, 1.7350060939788818, 1.734377145767212, 1.7950575351715088, 1.7732551097869873, 1.784305453300476, 1.7653216123580933, 1.7516058683395386, 1.7476930618286133, 1.755357265472412, 1.757020354270935, 1.7603992223739624]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.421963691711426
current iteration best possible eval_loss (full train run):  -1.7603992223739624
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592, -2.0973877906799316, -4.346683025360107, -1.8457581996917725, -3.1989521980285645, -2.421963691711426]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.2131 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -0.6524853759938436
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -0.6524853759938436
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -0.6524853759938436
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -0.6524853759938436
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -0.6524853759938436
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0260, dtype=torch.float64), 0, 0, 0, tensor(0.2229, dtype=torch.float64), tensor(0.1640, dtype=torch.float64), 0, tensor(0.5870, dtype=torch.float64), 32, 1, 0, 0, 1, 1, 2, 3.6190956100079925e-19, 1.4800000190734892, 1]
normalized proposed parameters for next round by BO: [tensor(1.0100e-17, dtype=torch.float64), tensor(0.0260, dtype=torch.float64), tensor(4.1666e-17, dtype=torch.float64), tensor(6.3626e-18, dtype=torch.float64), tensor(3.7994e-18, dtype=torch.float64), tensor(0.2229, dtype=torch.float64), tensor(0.1640, dtype=torch.float64), tensor(3.9718e-18, dtype=torch.float64), tensor(0.5870, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.6191e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.026
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.223
  wikitext: 0.164
  mmlu: 0
  arc_challenge: 0.587

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.6190956100079925e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (1.4800000190734892,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  2
lora dropout:  3.6190956100079925e-19
lora alpha:  1.4800000190734892
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,883,584 || all params: 8,033,144,832 || trainable%: 0.0359
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8555, 'grad_norm': 2.495950222015381, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.6303815841674805, 'eval_runtime': 4.5791, 'eval_samples_per_second': 218.385, 'eval_steps_per_second': 13.758, 'epoch': 0.04}
{'loss': 2.0174, 'grad_norm': 2.1901702880859375, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.1906685829162598, 'eval_runtime': 3.5183, 'eval_samples_per_second': 284.232, 'eval_steps_per_second': 17.907, 'epoch': 0.08}
{'loss': 1.1836, 'grad_norm': 0.34275442361831665, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6728928089141846, 'eval_runtime': 3.5106, 'eval_samples_per_second': 284.855, 'eval_steps_per_second': 17.946, 'epoch': 0.12}
{'loss': 1.0447, 'grad_norm': 0.4446791410446167, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.617580533027649, 'eval_runtime': 3.5101, 'eval_samples_per_second': 284.894, 'eval_steps_per_second': 17.948, 'epoch': 0.16}
{'loss': 1.0842, 'grad_norm': 0.3899260461330414, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6059556007385254, 'eval_runtime': 3.5126, 'eval_samples_per_second': 284.686, 'eval_steps_per_second': 17.935, 'epoch': 0.2}
{'loss': 1.0626, 'grad_norm': 0.3759378492832184, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6295284032821655, 'eval_runtime': 3.5112, 'eval_samples_per_second': 284.8, 'eval_steps_per_second': 17.942, 'epoch': 0.24}
{'loss': 1.0235, 'grad_norm': 0.30351099371910095, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.633453607559204, 'eval_runtime': 3.5148, 'eval_samples_per_second': 284.511, 'eval_steps_per_second': 17.924, 'epoch': 0.28}
{'loss': 1.0132, 'grad_norm': 0.3189461827278137, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6684943437576294, 'eval_runtime': 3.5193, 'eval_samples_per_second': 284.144, 'eval_steps_per_second': 17.901, 'epoch': 0.32}
{'loss': 0.9288, 'grad_norm': 0.40920642018318176, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7022910118103027, 'eval_runtime': 3.5057, 'eval_samples_per_second': 285.252, 'eval_steps_per_second': 17.971, 'epoch': 0.36}
{'loss': 0.949, 'grad_norm': 0.45767688751220703, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6727588176727295, 'eval_runtime': 3.4951, 'eval_samples_per_second': 286.113, 'eval_steps_per_second': 18.025, 'epoch': 0.4}
{'loss': 0.8958, 'grad_norm': 0.5004759430885315, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6772323846817017, 'eval_runtime': 3.5007, 'eval_samples_per_second': 285.657, 'eval_steps_per_second': 17.996, 'epoch': 0.44}
{'loss': 0.9116, 'grad_norm': 0.48815736174583435, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.650979995727539, 'eval_runtime': 3.4984, 'eval_samples_per_second': 285.841, 'eval_steps_per_second': 18.008, 'epoch': 0.48}
{'loss': 0.8592, 'grad_norm': 0.6035081744194031, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.692685604095459, 'eval_runtime': 3.5029, 'eval_samples_per_second': 285.481, 'eval_steps_per_second': 17.985, 'epoch': 0.52}
{'loss': 0.9246, 'grad_norm': 0.5367089509963989, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7285634279251099, 'eval_runtime': 3.5089, 'eval_samples_per_second': 284.991, 'eval_steps_per_second': 17.954, 'epoch': 0.56}
{'loss': 0.8874, 'grad_norm': 0.5980311036109924, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7173572778701782, 'eval_runtime': 3.5383, 'eval_samples_per_second': 282.623, 'eval_steps_per_second': 17.805, 'epoch': 0.6}
{'loss': 0.829, 'grad_norm': 0.8007839322090149, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.709131121635437, 'eval_runtime': 3.5381, 'eval_samples_per_second': 282.639, 'eval_steps_per_second': 17.806, 'epoch': 0.64}
{'loss': 0.8133, 'grad_norm': 0.7345266938209534, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7519960403442383, 'eval_runtime': 3.5208, 'eval_samples_per_second': 284.024, 'eval_steps_per_second': 17.894, 'epoch': 0.68}
{'loss': 0.8098, 'grad_norm': 0.7612220644950867, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.737445592880249, 'eval_runtime': 3.5232, 'eval_samples_per_second': 283.831, 'eval_steps_per_second': 17.881, 'epoch': 0.72}
{'loss': 0.8435, 'grad_norm': 0.7201218008995056, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.74924898147583, 'eval_runtime': 3.5273, 'eval_samples_per_second': 283.504, 'eval_steps_per_second': 17.861, 'epoch': 0.76}
{'loss': 0.7949, 'grad_norm': 1.140810489654541, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7291027307510376, 'eval_runtime': 3.5243, 'eval_samples_per_second': 283.746, 'eval_steps_per_second': 17.876, 'epoch': 0.8}
{'loss': 0.7345, 'grad_norm': 1.0090960264205933, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7524052858352661, 'eval_runtime': 3.53, 'eval_samples_per_second': 283.285, 'eval_steps_per_second': 17.847, 'epoch': 0.84}
{'loss': 0.7767, 'grad_norm': 0.9902129769325256, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7393925189971924, 'eval_runtime': 3.5378, 'eval_samples_per_second': 282.662, 'eval_steps_per_second': 17.808, 'epoch': 0.88}
{'loss': 0.7893, 'grad_norm': 1.0958822965621948, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7356224060058594, 'eval_runtime': 3.563, 'eval_samples_per_second': 280.664, 'eval_steps_per_second': 17.682, 'epoch': 0.92}
{'loss': 0.7459, 'grad_norm': 0.9388883709907532, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7492966651916504, 'eval_runtime': 3.5357, 'eval_samples_per_second': 282.832, 'eval_steps_per_second': 17.818, 'epoch': 0.96}
{'loss': 0.7521, 'grad_norm': 1.0635100603103638, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7520419359207153, 'eval_runtime': 3.5474, 'eval_samples_per_second': 281.896, 'eval_steps_per_second': 17.759, 'epoch': 1.0}
{'train_runtime': 284.5357, 'train_samples_per_second': 35.138, 'train_steps_per_second': 2.197, 'train_loss': 1.0612071716308593, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6303815841674805, 2.1906685829162598, 1.6728928089141846, 1.617580533027649, 1.6059556007385254, 1.6295284032821655, 1.633453607559204, 1.6684943437576294, 1.7022910118103027, 1.6727588176727295, 1.6772323846817017, 1.650979995727539, 1.692685604095459, 1.7285634279251099, 1.7173572778701782, 1.709131121635437, 1.7519960403442383, 1.737445592880249, 1.74924898147583, 1.7291027307510376, 1.7524052858352661, 1.7393925189971924, 1.7356224060058594, 1.7492966651916504, 1.7520419359207153], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.6303815841674805, 2.1906685829162598, 1.6728928089141846, 1.617580533027649, 1.6059556007385254, 1.6295284032821655, 1.633453607559204, 1.6684943437576294, 1.7022910118103027, 1.6727588176727295, 1.6772323846817017, 1.650979995727539, 1.692685604095459, 1.7285634279251099, 1.7173572778701782, 1.709131121635437, 1.7519960403442383, 1.737445592880249, 1.74924898147583, 1.7291027307510376, 1.7524052858352661, 1.7393925189971924, 1.7356224060058594, 1.7492966651916504, 1.7520419359207153]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.3282458782196045
current iteration best possible eval_loss (full train run):  -1.7520419359207153
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592, -2.0973877906799316, -4.346683025360107, -1.8457581996917725, -3.1989521980285645, -2.421963691711426, -2.3282458782196045]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.5290 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -0.7074097042525822
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -0.7074097042525822
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -0.7074097042525822
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -0.7074097042525822
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -0.7074097042525822
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2815, dtype=torch.float64), tensor(0.0318, dtype=torch.float64), 0, 0, 0, tensor(0.6766, dtype=torch.float64), tensor(0.0100, dtype=torch.float64), 0, 0, 26, 1, 0, 0, 1, 1, 2, 0.08852578400602816, 1.4800000190734872, 1]
normalized proposed parameters for next round by BO: [tensor(0.2815, dtype=torch.float64), tensor(0.0318, dtype=torch.float64), tensor(9.3881e-18, dtype=torch.float64), tensor(1.5103e-16, dtype=torch.float64), tensor(1.0917e-17, dtype=torch.float64), tensor(0.6766, dtype=torch.float64), tensor(0.0100, dtype=torch.float64), tensor(1.5468e-17, dtype=torch.float64), tensor(1.2799e-17, dtype=torch.float64), tensor(0.8183, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.8853, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.282
  gsm8k: 0.032
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.677
  wikitext: 0.01
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.08852578400602816,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.08852578400602816
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,342,912 || all params: 8,032,604,160 || trainable%: 0.0292
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.6889, 'grad_norm': 3.4412710666656494, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.8257060050964355, 'eval_runtime': 4.5014, 'eval_samples_per_second': 222.155, 'eval_steps_per_second': 13.996, 'epoch': 0.04}
{'loss': 2.4382, 'grad_norm': 3.961980104446411, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.4772024154663086, 'eval_runtime': 3.3845, 'eval_samples_per_second': 295.467, 'eval_steps_per_second': 18.614, 'epoch': 0.08}
{'loss': 1.3868, 'grad_norm': 1.160778522491455, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1089141368865967, 'eval_runtime': 3.378, 'eval_samples_per_second': 296.037, 'eval_steps_per_second': 18.65, 'epoch': 0.12}
{'loss': 1.115, 'grad_norm': 1.025678038597107, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1467931270599365, 'eval_runtime': 3.3853, 'eval_samples_per_second': 295.392, 'eval_steps_per_second': 18.61, 'epoch': 0.16}
{'loss': 1.0412, 'grad_norm': 0.4212501645088196, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9641432762145996, 'eval_runtime': 3.385, 'eval_samples_per_second': 295.419, 'eval_steps_per_second': 18.611, 'epoch': 0.2}
{'loss': 0.9575, 'grad_norm': 0.5623320937156677, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9666801691055298, 'eval_runtime': 3.3874, 'eval_samples_per_second': 295.209, 'eval_steps_per_second': 18.598, 'epoch': 0.24}
{'loss': 0.8669, 'grad_norm': 0.4467636048793793, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8831686973571777, 'eval_runtime': 3.3891, 'eval_samples_per_second': 295.06, 'eval_steps_per_second': 18.589, 'epoch': 0.28}
{'loss': 0.8287, 'grad_norm': 0.41192254424095154, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9593654870986938, 'eval_runtime': 3.3889, 'eval_samples_per_second': 295.081, 'eval_steps_per_second': 18.59, 'epoch': 0.32}
{'loss': 0.8182, 'grad_norm': 0.46270477771759033, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8677575588226318, 'eval_runtime': 3.3972, 'eval_samples_per_second': 294.363, 'eval_steps_per_second': 18.545, 'epoch': 0.36}
{'loss': 0.82, 'grad_norm': 0.5282606482505798, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9259014129638672, 'eval_runtime': 3.3919, 'eval_samples_per_second': 294.823, 'eval_steps_per_second': 18.574, 'epoch': 0.4}
{'loss': 0.7904, 'grad_norm': 0.5754606127738953, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8455568552017212, 'eval_runtime': 3.3941, 'eval_samples_per_second': 294.628, 'eval_steps_per_second': 18.562, 'epoch': 0.44}
{'loss': 0.7208, 'grad_norm': 0.6711410880088806, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8865759372711182, 'eval_runtime': 3.398, 'eval_samples_per_second': 294.293, 'eval_steps_per_second': 18.54, 'epoch': 0.48}
{'loss': 0.6782, 'grad_norm': 0.6654700040817261, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8758083581924438, 'eval_runtime': 3.3922, 'eval_samples_per_second': 294.794, 'eval_steps_per_second': 18.572, 'epoch': 0.52}
{'loss': 0.6995, 'grad_norm': 0.8827781081199646, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.985626459121704, 'eval_runtime': 3.6241, 'eval_samples_per_second': 275.932, 'eval_steps_per_second': 17.384, 'epoch': 0.56}
{'loss': 0.6709, 'grad_norm': 0.7537951469421387, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9989317655563354, 'eval_runtime': 3.3918, 'eval_samples_per_second': 294.829, 'eval_steps_per_second': 18.574, 'epoch': 0.6}
{'loss': 0.6277, 'grad_norm': 1.0139360427856445, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.0490150451660156, 'eval_runtime': 3.4184, 'eval_samples_per_second': 292.538, 'eval_steps_per_second': 18.43, 'epoch': 0.64}
{'loss': 0.6631, 'grad_norm': 0.8948482275009155, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9866524934768677, 'eval_runtime': 3.4041, 'eval_samples_per_second': 293.767, 'eval_steps_per_second': 18.507, 'epoch': 0.68}
{'loss': 0.6017, 'grad_norm': 0.863988995552063, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.0325734615325928, 'eval_runtime': 3.391, 'eval_samples_per_second': 294.898, 'eval_steps_per_second': 18.579, 'epoch': 0.72}
{'loss': 0.6137, 'grad_norm': 0.8499041199684143, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.048006296157837, 'eval_runtime': 3.395, 'eval_samples_per_second': 294.554, 'eval_steps_per_second': 18.557, 'epoch': 0.76}
{'loss': 0.5894, 'grad_norm': 0.7491849064826965, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.0739424228668213, 'eval_runtime': 3.3918, 'eval_samples_per_second': 294.83, 'eval_steps_per_second': 18.574, 'epoch': 0.8}
{'loss': 0.5937, 'grad_norm': 0.7135556936264038, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0917413234710693, 'eval_runtime': 3.3993, 'eval_samples_per_second': 294.179, 'eval_steps_per_second': 18.533, 'epoch': 0.84}
{'loss': 0.5856, 'grad_norm': 1.1192355155944824, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0986974239349365, 'eval_runtime': 3.385, 'eval_samples_per_second': 295.422, 'eval_steps_per_second': 18.612, 'epoch': 0.88}
{'loss': 0.5881, 'grad_norm': 0.773312509059906, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0931901931762695, 'eval_runtime': 3.3837, 'eval_samples_per_second': 295.531, 'eval_steps_per_second': 18.618, 'epoch': 0.92}
{'loss': 0.5289, 'grad_norm': 0.867782711982727, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1034677028656006, 'eval_runtime': 3.3838, 'eval_samples_per_second': 295.53, 'eval_steps_per_second': 18.618, 'epoch': 0.96}
{'loss': 0.6228, 'grad_norm': 0.8865841627120972, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.0990703105926514, 'eval_runtime': 3.3795, 'eval_samples_per_second': 295.899, 'eval_steps_per_second': 18.642, 'epoch': 1.0}
{'train_runtime': 231.0564, 'train_samples_per_second': 43.271, 'train_steps_per_second': 2.705, 'train_loss': 0.981446728515625, 'epoch': 1.0}
train_results:  {'eval_loss': [3.8257060050964355, 2.4772024154663086, 2.1089141368865967, 2.1467931270599365, 1.9641432762145996, 1.9666801691055298, 1.8831686973571777, 1.9593654870986938, 1.8677575588226318, 1.9259014129638672, 1.8455568552017212, 1.8865759372711182, 1.8758083581924438, 1.985626459121704, 1.9989317655563354, 2.0490150451660156, 1.9866524934768677, 2.0325734615325928, 2.048006296157837, 2.0739424228668213, 2.0917413234710693, 2.0986974239349365, 2.0931901931762695, 2.1034677028656006, 2.0990703105926514], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.8257060050964355, 2.4772024154663086, 2.1089141368865967, 2.1467931270599365, 1.9641432762145996, 1.9666801691055298, 1.8831686973571777, 1.9593654870986938, 1.8677575588226318, 1.9259014129638672, 1.8455568552017212, 1.8865759372711182, 1.8758083581924438, 1.985626459121704, 1.9989317655563354, 2.0490150451660156, 1.9866524934768677, 2.0325734615325928, 2.048006296157837, 2.0739424228668213, 2.0917413234710693, 2.0986974239349365, 2.0931901931762695, 2.1034677028656006, 2.0990703105926514]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.1850762367248535
current iteration best possible eval_loss (full train run):  -2.0990703105926514
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592, -2.0973877906799316, -4.346683025360107, -1.8457581996917725, -3.1989521980285645, -2.421963691711426, -2.3282458782196045, -2.1850762367248535]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.8168 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -0.9016934749167822
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -0.9016951549716581
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -0.9015244928703472
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -0.9030228433465293
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -0.9016929448653237
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0953, dtype=torch.float64), tensor(0.2413, dtype=torch.float64), tensor(0.0323, dtype=torch.float64), tensor(0.1650, dtype=torch.float64), tensor(0.1338, dtype=torch.float64), tensor(0.1527, dtype=torch.float64), 0, tensor(0.0768, dtype=torch.float64), tensor(0.1029, dtype=torch.float64), 1, 0, 0, 1, 0, 0, 107, 0.025631277947454983, 32.95105397776969, 1]
normalized proposed parameters for next round by BO: [tensor(0.0953, dtype=torch.float64), tensor(0.2413, dtype=torch.float64), tensor(0.0323, dtype=torch.float64), tensor(0.1650, dtype=torch.float64), tensor(0.1338, dtype=torch.float64), tensor(0.1527, dtype=torch.float64), tensor(4.0801e-18, dtype=torch.float64), tensor(0.0768, dtype=torch.float64), tensor(0.1029, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8370, dtype=torch.float64), tensor(0.2563, dtype=torch.float64), tensor(0.6865, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.095
  gsm8k: 0.241
  rowan_hellaswag: 0.032
  sciq: 0.165
  triviaqa: 0.134
  truthfulqa_gen: 0.153
  wikitext: 0
  mmlu: 0.077
  arc_challenge: 0.103

LoRA Parameters:
  lora_r: (107,)
  lora_dropout: (0.025631277947454983,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (32.95105397776969,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  107
lora dropout:  0.025631277947454983
lora alpha:  32.95105397776969
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,972,224 || all params: 8,032,233,472 || trainable%: 0.0246
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8048, 'grad_norm': 0.6871116161346436, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 4.4652485847473145, 'eval_runtime': 5.1566, 'eval_samples_per_second': 193.924, 'eval_steps_per_second': 12.217, 'epoch': 0.04}
{'loss': 2.6794, 'grad_norm': 0.8500654697418213, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.540292978286743, 'eval_runtime': 2.8759, 'eval_samples_per_second': 347.715, 'eval_steps_per_second': 21.906, 'epoch': 0.08}
{'loss': 2.0952, 'grad_norm': 1.487352967262268, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.2620675563812256, 'eval_runtime': 2.8554, 'eval_samples_per_second': 350.213, 'eval_steps_per_second': 22.063, 'epoch': 0.12}
{'loss': 1.7036, 'grad_norm': 1.0143014192581177, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0671911239624023, 'eval_runtime': 2.8654, 'eval_samples_per_second': 348.985, 'eval_steps_per_second': 21.986, 'epoch': 0.16}
{'loss': 1.6464, 'grad_norm': 0.9010300636291504, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9902209043502808, 'eval_runtime': 2.8792, 'eval_samples_per_second': 347.313, 'eval_steps_per_second': 21.881, 'epoch': 0.2}
{'loss': 1.5893, 'grad_norm': 0.980498194694519, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9794460535049438, 'eval_runtime': 2.8943, 'eval_samples_per_second': 345.506, 'eval_steps_per_second': 21.767, 'epoch': 0.24}
{'loss': 1.5496, 'grad_norm': 1.1197046041488647, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9676706790924072, 'eval_runtime': 2.8955, 'eval_samples_per_second': 345.359, 'eval_steps_per_second': 21.758, 'epoch': 0.28}
{'loss': 1.4543, 'grad_norm': 0.9041350483894348, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9095170497894287, 'eval_runtime': 2.8828, 'eval_samples_per_second': 346.881, 'eval_steps_per_second': 21.854, 'epoch': 0.32}
{'loss': 1.4254, 'grad_norm': 0.7049930691719055, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9107234477996826, 'eval_runtime': 2.8893, 'eval_samples_per_second': 346.105, 'eval_steps_per_second': 21.805, 'epoch': 0.36}
{'loss': 1.489, 'grad_norm': 0.8966974020004272, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8532134294509888, 'eval_runtime': 2.8875, 'eval_samples_per_second': 346.317, 'eval_steps_per_second': 21.818, 'epoch': 0.4}
{'loss': 1.4391, 'grad_norm': 0.8159583210945129, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8227317333221436, 'eval_runtime': 2.8852, 'eval_samples_per_second': 346.591, 'eval_steps_per_second': 21.835, 'epoch': 0.44}
{'loss': 1.4995, 'grad_norm': 1.8096307516098022, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8478957414627075, 'eval_runtime': 2.8847, 'eval_samples_per_second': 346.651, 'eval_steps_per_second': 21.839, 'epoch': 0.48}
{'loss': 1.4258, 'grad_norm': 0.6893461346626282, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.816781759262085, 'eval_runtime': 2.8857, 'eval_samples_per_second': 346.542, 'eval_steps_per_second': 21.832, 'epoch': 0.52}
{'loss': 1.4015, 'grad_norm': 0.6437739729881287, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.787046194076538, 'eval_runtime': 2.8869, 'eval_samples_per_second': 346.392, 'eval_steps_per_second': 21.823, 'epoch': 0.56}
{'loss': 1.4178, 'grad_norm': 0.9451180696487427, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8101649284362793, 'eval_runtime': 2.8906, 'eval_samples_per_second': 345.946, 'eval_steps_per_second': 21.795, 'epoch': 0.6}
{'loss': 1.3613, 'grad_norm': 0.6933344006538391, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7496626377105713, 'eval_runtime': 2.8839, 'eval_samples_per_second': 346.75, 'eval_steps_per_second': 21.845, 'epoch': 0.64}
{'loss': 1.3446, 'grad_norm': 0.6417885422706604, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6769676208496094, 'eval_runtime': 2.8921, 'eval_samples_per_second': 345.764, 'eval_steps_per_second': 21.783, 'epoch': 0.68}
{'loss': 1.3641, 'grad_norm': 0.7809875011444092, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6373544931411743, 'eval_runtime': 2.8911, 'eval_samples_per_second': 345.887, 'eval_steps_per_second': 21.791, 'epoch': 0.72}
{'loss': 1.3475, 'grad_norm': 0.7811504006385803, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5823853015899658, 'eval_runtime': 2.8826, 'eval_samples_per_second': 346.907, 'eval_steps_per_second': 21.855, 'epoch': 0.76}
{'loss': 1.2826, 'grad_norm': 0.5739524364471436, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5949231386184692, 'eval_runtime': 2.8891, 'eval_samples_per_second': 346.125, 'eval_steps_per_second': 21.806, 'epoch': 0.8}
{'loss': 1.3218, 'grad_norm': 0.7164327502250671, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.56509268283844, 'eval_runtime': 2.8898, 'eval_samples_per_second': 346.041, 'eval_steps_per_second': 21.801, 'epoch': 0.84}
{'loss': 1.2697, 'grad_norm': 0.7017332911491394, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5829230546951294, 'eval_runtime': 2.8889, 'eval_samples_per_second': 346.148, 'eval_steps_per_second': 21.807, 'epoch': 0.88}
{'loss': 1.2773, 'grad_norm': 0.7936630845069885, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5456939935684204, 'eval_runtime': 2.8995, 'eval_samples_per_second': 344.891, 'eval_steps_per_second': 21.728, 'epoch': 0.92}
{'loss': 1.2703, 'grad_norm': 0.6611153483390808, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5672903060913086, 'eval_runtime': 2.8989, 'eval_samples_per_second': 344.957, 'eval_steps_per_second': 21.732, 'epoch': 0.96}
{'loss': 1.2886, 'grad_norm': 0.6100087761878967, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.550482153892517, 'eval_runtime': 2.9244, 'eval_samples_per_second': 341.945, 'eval_steps_per_second': 21.543, 'epoch': 1.0}
{'train_runtime': 254.7372, 'train_samples_per_second': 39.24, 'train_steps_per_second': 2.454, 'train_loss': 1.589939224243164, 'epoch': 1.0}
train_results:  {'eval_loss': [4.4652485847473145, 2.540292978286743, 2.2620675563812256, 2.0671911239624023, 1.9902209043502808, 1.9794460535049438, 1.9676706790924072, 1.9095170497894287, 1.9107234477996826, 1.8532134294509888, 1.8227317333221436, 1.8478957414627075, 1.816781759262085, 1.787046194076538, 1.8101649284362793, 1.7496626377105713, 1.6769676208496094, 1.6373544931411743, 1.5823853015899658, 1.5949231386184692, 1.56509268283844, 1.5829230546951294, 1.5456939935684204, 1.5672903060913086, 1.550482153892517], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [4.4652485847473145, 2.540292978286743, 2.2620675563812256, 2.0671911239624023, 1.9902209043502808, 1.9794460535049438, 1.9676706790924072, 1.9095170497894287, 1.9107234477996826, 1.8532134294509888, 1.8227317333221436, 1.8478957414627075, 1.816781759262085, 1.787046194076538, 1.8101649284362793, 1.7496626377105713, 1.6769676208496094, 1.6373544931411743, 1.5823853015899658, 1.5949231386184692, 1.56509268283844, 1.5829230546951294, 1.5456939935684204, 1.5672903060913086, 1.550482153892517]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.310150623321533
current iteration best possible eval_loss (full train run):  -1.550482153892517
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592, -2.0973877906799316, -4.346683025360107, -1.8457581996917725, -3.1989521980285645, -2.421963691711426, -2.3282458782196045, -2.1850762367248535, -3.310150623321533]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.2109 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -0.7343227138389274
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -0.7343227138389274
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -0.7343227138389274
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -0.7343227138389274
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -0.7343227138389274
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0480, dtype=torch.float64), tensor(0.6765, dtype=torch.float64), tensor(0.0634, dtype=torch.float64), 0, tensor(0.0734, dtype=torch.float64), 0, tensor(0.1387, dtype=torch.float64), 5, 1, 0, 1, 1, 1, 3, 8.794381065933759e-20, 33.197296134671035, 1]
normalized proposed parameters for next round by BO: [tensor(2.3419e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0480, dtype=torch.float64), tensor(0.6765, dtype=torch.float64), tensor(0.0634, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0734, dtype=torch.float64), tensor(3.7315e-18, dtype=torch.float64), tensor(0.1387, dtype=torch.float64), tensor(0.1415, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0267, dtype=torch.float64), tensor(8.7944e-19, dtype=torch.float64), tensor(0.6916, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.048
  sciq: 0.677
  triviaqa: 0.063
  truthfulqa_gen: 0
  wikitext: 0.073
  mmlu: 0
  arc_challenge: 0.139

LoRA Parameters:
  lora_r: (3,)
  lora_dropout: (8.794381065933759e-20,)
  num_layers_to_apply: (5,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (33.197296134671035,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  5
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  3
lora dropout:  8.794381065933759e-20
lora alpha:  33.197296134671035
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 952,320 || all params: 8,031,213,568 || trainable%: 0.0119
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9576, 'grad_norm': 12.674461364746094, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.325777292251587, 'eval_runtime': 4.7041, 'eval_samples_per_second': 212.579, 'eval_steps_per_second': 13.392, 'epoch': 0.04}
{'loss': 1.7851, 'grad_norm': 6.585290908813477, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7228641510009766, 'eval_runtime': 3.0188, 'eval_samples_per_second': 331.256, 'eval_steps_per_second': 20.869, 'epoch': 0.08}
{'loss': 1.2263, 'grad_norm': 4.541415691375732, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4870160818099976, 'eval_runtime': 3.0003, 'eval_samples_per_second': 333.303, 'eval_steps_per_second': 20.998, 'epoch': 0.12}
{'loss': 1.2301, 'grad_norm': 2.010812520980835, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2813657522201538, 'eval_runtime': 3.001, 'eval_samples_per_second': 333.223, 'eval_steps_per_second': 20.993, 'epoch': 0.16}
{'loss': 1.0787, 'grad_norm': 4.173926830291748, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1775517463684082, 'eval_runtime': 3.0015, 'eval_samples_per_second': 333.172, 'eval_steps_per_second': 20.99, 'epoch': 0.2}
{'loss': 1.1629, 'grad_norm': 1.9649016857147217, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1421422958374023, 'eval_runtime': 3.0055, 'eval_samples_per_second': 332.722, 'eval_steps_per_second': 20.961, 'epoch': 0.24}
{'loss': 1.0807, 'grad_norm': 1.9784296751022339, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.143762469291687, 'eval_runtime': 3.0075, 'eval_samples_per_second': 332.506, 'eval_steps_per_second': 20.948, 'epoch': 0.28}
{'loss': 1.0061, 'grad_norm': 1.915527582168579, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1219902038574219, 'eval_runtime': 3.0175, 'eval_samples_per_second': 331.404, 'eval_steps_per_second': 20.878, 'epoch': 0.32}
{'loss': 1.12, 'grad_norm': 1.490705966949463, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.135443925857544, 'eval_runtime': 3.014, 'eval_samples_per_second': 331.784, 'eval_steps_per_second': 20.902, 'epoch': 0.36}
{'loss': 0.9843, 'grad_norm': 1.732378363609314, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1590608358383179, 'eval_runtime': 3.0153, 'eval_samples_per_second': 331.647, 'eval_steps_per_second': 20.894, 'epoch': 0.4}
{'loss': 1.0097, 'grad_norm': 1.9695309400558472, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1194329261779785, 'eval_runtime': 3.016, 'eval_samples_per_second': 331.569, 'eval_steps_per_second': 20.889, 'epoch': 0.44}
{'loss': 1.0584, 'grad_norm': 1.493998646736145, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.118787407875061, 'eval_runtime': 3.0134, 'eval_samples_per_second': 331.852, 'eval_steps_per_second': 20.907, 'epoch': 0.48}
{'loss': 1.0446, 'grad_norm': 1.5716655254364014, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.111255168914795, 'eval_runtime': 3.0181, 'eval_samples_per_second': 331.336, 'eval_steps_per_second': 20.874, 'epoch': 0.52}
{'loss': 1.0788, 'grad_norm': 1.7960821390151978, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0974218845367432, 'eval_runtime': 3.0173, 'eval_samples_per_second': 331.421, 'eval_steps_per_second': 20.88, 'epoch': 0.56}
{'loss': 1.0797, 'grad_norm': 2.093907117843628, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1118736267089844, 'eval_runtime': 3.0137, 'eval_samples_per_second': 331.813, 'eval_steps_per_second': 20.904, 'epoch': 0.6}
{'loss': 1.0197, 'grad_norm': 1.5208821296691895, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1096293926239014, 'eval_runtime': 3.0105, 'eval_samples_per_second': 332.166, 'eval_steps_per_second': 20.926, 'epoch': 0.64}
{'loss': 1.1008, 'grad_norm': 1.6070287227630615, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1229780912399292, 'eval_runtime': 3.0035, 'eval_samples_per_second': 332.949, 'eval_steps_per_second': 20.976, 'epoch': 0.68}
{'loss': 0.9672, 'grad_norm': 1.7821754217147827, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1178680658340454, 'eval_runtime': 3.0, 'eval_samples_per_second': 333.338, 'eval_steps_per_second': 21.0, 'epoch': 0.72}
{'loss': 0.9434, 'grad_norm': 1.384924292564392, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1033971309661865, 'eval_runtime': 2.9975, 'eval_samples_per_second': 333.615, 'eval_steps_per_second': 21.018, 'epoch': 0.76}
{'loss': 1.0359, 'grad_norm': 1.4578425884246826, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.090403437614441, 'eval_runtime': 2.9999, 'eval_samples_per_second': 333.34, 'eval_steps_per_second': 21.0, 'epoch': 0.8}
{'loss': 1.0029, 'grad_norm': 1.3258399963378906, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0984231233596802, 'eval_runtime': 3.0071, 'eval_samples_per_second': 332.544, 'eval_steps_per_second': 20.95, 'epoch': 0.84}
{'loss': 1.0141, 'grad_norm': 1.1712501049041748, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0932495594024658, 'eval_runtime': 3.0041, 'eval_samples_per_second': 332.874, 'eval_steps_per_second': 20.971, 'epoch': 0.88}
{'loss': 1.0833, 'grad_norm': 1.4664665460586548, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.093899130821228, 'eval_runtime': 3.0058, 'eval_samples_per_second': 332.694, 'eval_steps_per_second': 20.96, 'epoch': 0.92}
{'loss': 1.0363, 'grad_norm': 1.5233232975006104, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0897372961044312, 'eval_runtime': 3.0192, 'eval_samples_per_second': 331.21, 'eval_steps_per_second': 20.866, 'epoch': 0.96}
{'loss': 0.9922, 'grad_norm': 2.182741165161133, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.090000033378601, 'eval_runtime': 3.0122, 'eval_samples_per_second': 331.979, 'eval_steps_per_second': 20.915, 'epoch': 1.0}
{'train_runtime': 234.7369, 'train_samples_per_second': 42.592, 'train_steps_per_second': 2.663, 'train_loss': 1.2039528472900392, 'epoch': 1.0}
train_results:  {'eval_loss': [2.325777292251587, 1.7228641510009766, 1.4870160818099976, 1.2813657522201538, 1.1775517463684082, 1.1421422958374023, 1.143762469291687, 1.1219902038574219, 1.135443925857544, 1.1590608358383179, 1.1194329261779785, 1.118787407875061, 1.111255168914795, 1.0974218845367432, 1.1118736267089844, 1.1096293926239014, 1.1229780912399292, 1.1178680658340454, 1.1033971309661865, 1.090403437614441, 1.0984231233596802, 1.0932495594024658, 1.093899130821228, 1.0897372961044312, 1.090000033378601], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.325777292251587, 1.7228641510009766, 1.4870160818099976, 1.2813657522201538, 1.1775517463684082, 1.1421422958374023, 1.143762469291687, 1.1219902038574219, 1.135443925857544, 1.1590608358383179, 1.1194329261779785, 1.118787407875061, 1.111255168914795, 1.0974218845367432, 1.1118736267089844, 1.1096293926239014, 1.1229780912399292, 1.1178680658340454, 1.1033971309661865, 1.090403437614441, 1.0984231233596802, 1.0932495594024658, 1.093899130821228, 1.0897372961044312, 1.090000033378601]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.5089263916015625
current iteration best possible eval_loss (full train run):  -1.090000033378601
max eval_loss so far:  -0.9699225425720215
BO observations:  [-1.84628427028656, -3.655287265777588, -4.072971820831299, -4.561641216278076, -1.7098222970962524, -1.9850363731384277, -4.0099568367004395, -2.8967461585998535, -2.9681906700134277, -1.755387306213379, -2.2874722480773926, -2.222399950027466, -2.3334760665893555, -3.3616437911987305, -2.0796561241149902, -3.38761568069458, -2.6868460178375244, -2.190642833709717, -2.331563949584961, -3.2177906036376953, -2.698913097381592, -2.0973877906799316, -4.346683025360107, -1.8457581996917725, -3.1989521980285645, -2.421963691711426, -2.3282458782196045, -2.1850762367248535, -3.310150623321533, -1.5089263916015625]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.3599 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -0.7648254596094963
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -0.7648254596094963
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -0.7648254596094963
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -0.7648254596094963
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -0.7648254596094963
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3038, dtype=torch.float64), tensor(0.4044, dtype=torch.float64), tensor(0.2187, dtype=torch.float64), 0, tensor(0.0724, dtype=torch.float64), 0, 27, 1, 0, 0, 1, 1, 2, 0.09999999999999996, 39.772278774480235, 1]
normalized proposed parameters for next round by BO: [tensor(2.7427e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.3543e-15, dtype=torch.float64), tensor(0.3038, dtype=torch.float64), tensor(0.4044, dtype=torch.float64), tensor(0.2187, dtype=torch.float64), tensor(3.8126e-16, dtype=torch.float64), tensor(0.0724, dtype=torch.float64), tensor(0.0007, dtype=torch.float64), tensor(0.8422, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.8286, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-1.0572949647903442, -1.0572949647903442, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
triviaqa
evaluation dataset:
data domain:  triviaqa  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/triviaqa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 2, 0, 0, 0, 1, 0, 29, 0.05396168787624587, 37, 1]
Checking history sample input_X_between_0_1:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2265625, 0.5396168787624587, 0.7708333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1686161756515503
Checking history sample input_X:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 7, 0, 0, 1, 0, 0, 101, 0.003930648435578799, 29, 1]
Checking history sample input_X_between_0_1:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7890625, 0.039306484355787985, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.066257357597351
Checking history sample input_X:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 22, 1, 0, 0, 1, 1, 3, 0.04742737743265794, 11, 1]
Checking history sample input_X_between_0_1:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 0.6875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0234375, 0.47427377432657936, 0.22916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.7734289765357971
Checking history sample input_X:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 13, 1, 0, 1, 1, 1, 123, 0.09371289547650785, 48, 0]
Checking history sample input_X_between_0_1:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 0.40625, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9609375, 0.9371289547650785, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0014729499816895
Checking history sample input_X:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 13, 0, 0, 1, 1, 0, 127, 0.09560839072699708, 9, 1]
Checking history sample input_X_between_0_1:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 0.40625, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9921875, 0.9560839072699707, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2857557535171509
Checking history sample input_X:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 9, 0, 0, 1, 1, 1, 16, 0.08052765798611784, 41, 1]
Checking history sample input_X_between_0_1:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 0.28125, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.8052765798611784, 0.8541666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8996233940124512
Checking history sample input_X:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 5, 0, 1, 0, 1, 0, 59, 0.05350476033435025, 24, 1]
Checking history sample input_X_between_0_1:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 0.15625, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4609375, 0.5350476033435024, 0.5, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1949825286865234
Checking history sample input_X:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 2, 1, 0, 0, 1, 0, 52, 0.0964290393747826, 9, 0]
Checking history sample input_X_between_0_1:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 0.0625, 1.0, 0.0, 0.0, 1.0, 0.0, 0.40625, 0.964290393747826, 0.1875, 0.0]
Checking history sample eval_loss at 625 steps:  -1.385130763053894
Checking history sample input_X:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 7, 1, 1, 0, 0, 1, 121, 0.029877603091235272, 27, 0]
Checking history sample input_X_between_0_1:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 0.21875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9453125, 0.2987760309123527, 0.5625, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9837757349014282
Checking history sample input_X:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 5, 1, 1, 0, 0, 0, 99, 0.012004498902397865, 15, 0]
Checking history sample input_X_between_0_1:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 0.15625, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7734375, 0.12004498902397864, 0.3125, 0.0]
Checking history sample eval_loss at 625 steps:  -1.6018083095550537
Checking history sample input_X:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 24, 1, 0, 1, 1, 1, 39, 0.002627655727844236, 41, 0]
Checking history sample input_X_between_0_1:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 0.75, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3046875, 0.02627655727844236, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.8874176144599915
Checking history sample input_X:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 24, 0, 0, 0, 0, 1, 93, 0.03163409179138251, 48, 0]
Checking history sample input_X_between_0_1:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 0.75, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7265625, 0.31634091791382507, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.058224081993103
Checking history sample input_X:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 28, 1, 0, 0, 0, 0, 89, 0.0740409362882843, 48, 0]
Checking history sample input_X_between_0_1:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 0.875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6953125, 0.740409362882843, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.431011438369751
Checking history sample input_X:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 28, 1, 0, 1, 1, 1, 2, 0.052382422456560135, 28, 1]
Checking history sample input_X_between_0_1:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 0.875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.015625, 0.5238242245656013, 0.5833333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9802380204200745
Checking history sample input_X:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 11, 0, 0, 0, 1, 1, 54, 0.06588137610749685, 20, 1]
Checking history sample input_X_between_0_1:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 0.34375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.421875, 0.6588137610749685, 0.4166666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -0.7846323251724243
Checking history sample input_X:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 27, 0, 0, 1, 1, 1, 76, 0.04383658958637696, 17, 0]
Checking history sample input_X_between_0_1:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 0.84375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.59375, 0.4383658958637696, 0.3541666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2182475328445435
Checking history sample input_X:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 29, 0, 1, 0, 0, 1, 82, 0.07326489367366754, 40, 1]
Checking history sample input_X_between_0_1:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 0.90625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.640625, 0.7326489367366753, 0.8333333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.7634263634681702
Checking history sample input_X:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 1, 0, 0, 0, 1, 0, 44, 0.09959850306478138, 19, 0]
Checking history sample input_X_between_0_1:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.34375, 0.9959850306478137, 0.3958333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.4772111177444458
Checking history sample input_X:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 26, 1, 1, 1, 0, 1, 67, 0.08108306564701856, 9, 1]
Checking history sample input_X_between_0_1:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 0.8125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5234375, 0.8108306564701856, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4388036727905273
Checking history sample input_X:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 29, 0, 0, 0, 1, 1, 29, 0.08410329802499458, 20, 0]
Checking history sample input_X_between_0_1:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 0.90625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2265625, 0.8410329802499458, 0.4166666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0373977422714233
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1549 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.22837674617767334, 0.4785420298576355, 0.687170684337616, 0.875913143157959, 0.34268462657928467, 0.13135671615600586, 0.5217075347900391, 0.5346872210502625, 0.3129348158836365, 0.9251148104667664, 0.6651479601860046, 0.6997314095497131, 0.7013499736785889, 0.5754836201667786, 0.2565208673477173, 0.28079620003700256, 0.6394091248512268, 0.6252473592758179, 0.11650270223617554]  ‚Üí  acq = -0.4931909413544098
X = [0.6007985472679138, 0.6734792590141296, 0.14090436697006226, 0.7128353714942932, 0.6872708797454834, 0.937778651714325, 0.9724430441856384, 0.9944669008255005, 0.05142951011657715, 0.36564430594444275, 0.434969961643219, 0.950384795665741, 0.15209734439849854, 0.6141502857208252, 0.9576845169067383, 0.9773681163787842, 0.1673140525817871, 0.6881717443466187, 0.8837282061576843]  ‚Üí  acq = -0.4930566054145976
X = [0.1571710705757141, 0.9264158606529236, 0.9699968099594116, 0.20489519834518433, 0.4686161279678345, 0.6898694634437561, 0.4198032021522522, 0.21791815757751465, 0.42358022928237915, 0.3872278332710266, 0.6143887042999268, 0.7122960686683655, 0.08111494779586792, 0.29090559482574463, 0.4341798424720764, 0.05863045156002045, 0.5466355681419373, 0.446211576461792, 0.6572873592376709]  ‚Üí  acq = -0.49305741245162826
X = [0.02727675437927246, 0.06622374057769775, 0.6891517043113708, 0.08736252784729004, 0.7689311504364014, 0.40606510639190674, 0.46344149112701416, 0.9359027743339539, 0.5546330213546753, 0.2090778350830078, 0.6198529601097107, 0.15606045722961426, 0.8810778856277466, 0.14611035585403442, 0.536634087562561, 0.5650687217712402, 0.9413138628005981, 0.6718274354934692, 0.5841108560562134]  ‚Üí  acq = -0.49354513183774096
X = [0.5869691967964172, 0.431522011756897, 0.20124149322509766, 0.5672672986984253, 0.5105143189430237, 0.15650159120559692, 0.2572283148765564, 0.5396044254302979, 0.1212151050567627, 0.40843766927719116, 0.920595645904541, 0.18616461753845215, 0.2543662190437317, 0.43692445755004883, 0.6949977874755859, 0.555536150932312, 0.3461461663246155, 0.05282863229513168, 0.159490168094635]  ‚Üí  acq = -0.5028879997207071
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.4324, dtype=torch.float64), 0, tensor(0.3815, dtype=torch.float64), tensor(0.0865, dtype=torch.float64), 0, 0, tensor(0.0993, dtype=torch.float64), 0, 20, 0, 1, 0, 1, 1, 2, 0.015055211658025609, 48.0, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(1.4253e-17, dtype=torch.float64), tensor(0.4324, dtype=torch.float64), tensor(0.0003, dtype=torch.float64), tensor(0.3815, dtype=torch.float64), tensor(0.0865, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.8476e-18, dtype=torch.float64), tensor(0.0993, dtype=torch.float64), tensor(6.5530e-18, dtype=torch.float64), tensor(0.6099, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.1506, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.432
  rowan_hellaswag: 0
  sciq: 0.382
  triviaqa: 0.086
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.099
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.015055211658025609,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  2
lora dropout:  0.015055211658025609
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,679,360 || all params: 8,031,940,608 || trainable%: 0.0209
length of training data:  9994
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 2.3036, 'grad_norm': 4.860440731048584, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.789932131767273, 'eval_runtime': 3.7779, 'eval_samples_per_second': 264.694, 'eval_steps_per_second': 16.676, 'epoch': 0.04}
{'loss': 1.0838, 'grad_norm': 2.947467803955078, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3342570066452026, 'eval_runtime': 3.2141, 'eval_samples_per_second': 311.13, 'eval_steps_per_second': 19.601, 'epoch': 0.08}
{'loss': 0.9827, 'grad_norm': 2.3682363033294678, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1991429328918457, 'eval_runtime': 3.1963, 'eval_samples_per_second': 312.864, 'eval_steps_per_second': 19.71, 'epoch': 0.12}
{'loss': 0.9132, 'grad_norm': 1.641026496887207, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.142950415611267, 'eval_runtime': 3.2017, 'eval_samples_per_second': 312.337, 'eval_steps_per_second': 19.677, 'epoch': 0.16}
{'loss': 0.9139, 'grad_norm': 1.869778037071228, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1205363273620605, 'eval_runtime': 3.2012, 'eval_samples_per_second': 312.382, 'eval_steps_per_second': 19.68, 'epoch': 0.2}
{'loss': 0.8984, 'grad_norm': 1.4829157590866089, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.11542809009552, 'eval_runtime': 3.2055, 'eval_samples_per_second': 311.96, 'eval_steps_per_second': 19.653, 'epoch': 0.24}
{'loss': 0.895, 'grad_norm': 4.8751678466796875, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1118515729904175, 'eval_runtime': 3.3539, 'eval_samples_per_second': 298.165, 'eval_steps_per_second': 18.784, 'epoch': 0.28}
{'loss': 0.8844, 'grad_norm': 1.52393639087677, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0959302186965942, 'eval_runtime': 3.217, 'eval_samples_per_second': 310.852, 'eval_steps_per_second': 19.584, 'epoch': 0.32}
{'loss': 0.9095, 'grad_norm': 1.7073285579681396, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1082607507705688, 'eval_runtime': 3.2282, 'eval_samples_per_second': 309.772, 'eval_steps_per_second': 19.516, 'epoch': 0.36}
{'loss': 0.8935, 'grad_norm': 1.6426360607147217, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0982487201690674, 'eval_runtime': 3.2081, 'eval_samples_per_second': 311.707, 'eval_steps_per_second': 19.638, 'epoch': 0.4}
{'loss': 0.8748, 'grad_norm': 1.5608110427856445, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0937045812606812, 'eval_runtime': 3.2076, 'eval_samples_per_second': 311.755, 'eval_steps_per_second': 19.641, 'epoch': 0.44}
{'loss': 0.8731, 'grad_norm': 1.4849634170532227, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0965520143508911, 'eval_runtime': 3.2026, 'eval_samples_per_second': 312.242, 'eval_steps_per_second': 19.671, 'epoch': 0.48}
{'loss': 0.8753, 'grad_norm': 1.480512022972107, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0888142585754395, 'eval_runtime': 3.2071, 'eval_samples_per_second': 311.805, 'eval_steps_per_second': 19.644, 'epoch': 0.52}
{'loss': 0.8608, 'grad_norm': 1.4942418336868286, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0805680751800537, 'eval_runtime': 3.1894, 'eval_samples_per_second': 313.535, 'eval_steps_per_second': 19.753, 'epoch': 0.56}
{'loss': 0.8519, 'grad_norm': 1.808054804801941, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0836963653564453, 'eval_runtime': 3.2059, 'eval_samples_per_second': 311.927, 'eval_steps_per_second': 19.651, 'epoch': 0.6}
{'loss': 0.8481, 'grad_norm': 1.3772728443145752, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0821735858917236, 'eval_runtime': 3.1946, 'eval_samples_per_second': 313.028, 'eval_steps_per_second': 19.721, 'epoch': 0.64}
{'loss': 0.8368, 'grad_norm': 1.4060349464416504, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0862770080566406, 'eval_runtime': 3.1965, 'eval_samples_per_second': 312.838, 'eval_steps_per_second': 19.709, 'epoch': 0.68}
{'loss': 0.8566, 'grad_norm': 1.6661583185195923, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.074691653251648, 'eval_runtime': 3.1972, 'eval_samples_per_second': 312.77, 'eval_steps_per_second': 19.705, 'epoch': 0.72}
{'loss': 0.8742, 'grad_norm': 1.289920449256897, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.073898434638977, 'eval_runtime': 3.1948, 'eval_samples_per_second': 313.009, 'eval_steps_per_second': 19.72, 'epoch': 0.76}
{'loss': 0.8191, 'grad_norm': 1.6107476949691772, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0788265466690063, 'eval_runtime': 3.1968, 'eval_samples_per_second': 312.816, 'eval_steps_per_second': 19.707, 'epoch': 0.8}
{'loss': 0.9066, 'grad_norm': 7.646119594573975, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.068932294845581, 'eval_runtime': 3.198, 'eval_samples_per_second': 312.693, 'eval_steps_per_second': 19.7, 'epoch': 0.84}
{'loss': 0.8264, 'grad_norm': 2.060192584991455, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0696802139282227, 'eval_runtime': 3.1977, 'eval_samples_per_second': 312.726, 'eval_steps_per_second': 19.702, 'epoch': 0.88}
{'loss': 0.8312, 'grad_norm': 1.4430011510849, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0674450397491455, 'eval_runtime': 3.1994, 'eval_samples_per_second': 312.562, 'eval_steps_per_second': 19.691, 'epoch': 0.92}
{'loss': 0.834, 'grad_norm': 1.528800129890442, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0667775869369507, 'eval_runtime': 3.21, 'eval_samples_per_second': 311.527, 'eval_steps_per_second': 19.626, 'epoch': 0.96}
{'loss': 0.8874, 'grad_norm': 1.6168887615203857, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0653173923492432, 'eval_runtime': 3.2118, 'eval_samples_per_second': 311.354, 'eval_steps_per_second': 19.615, 'epoch': 1.0}
{'train_runtime': 297.794, 'train_samples_per_second': 33.56, 'train_steps_per_second': 2.099, 'train_loss': 0.9413690124511719, 'epoch': 1.0}
train_results:  {'eval_loss': [1.789932131767273, 1.3342570066452026, 1.1991429328918457, 1.142950415611267, 1.1205363273620605, 1.11542809009552, 1.1118515729904175, 1.0959302186965942, 1.1082607507705688, 1.0982487201690674, 1.0937045812606812, 1.0965520143508911, 1.0888142585754395, 1.0805680751800537, 1.0836963653564453, 1.0821735858917236, 1.0862770080566406, 1.074691653251648, 1.073898434638977, 1.0788265466690063, 1.068932294845581, 1.0696802139282227, 1.0674450397491455, 1.0667775869369507, 1.0653173923492432], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.789932131767273, 1.3342570066452026, 1.1991429328918457, 1.142950415611267, 1.1205363273620605, 1.11542809009552, 1.1118515729904175, 1.0959302186965942, 1.1082607507705688, 1.0982487201690674, 1.0937045812606812, 1.0965520143508911, 1.0888142585754395, 1.0805680751800537, 1.0836963653564453, 1.0821735858917236, 1.0862770080566406, 1.074691653251648, 1.073898434638977, 1.0788265466690063, 1.068932294845581, 1.0696802139282227, 1.0674450397491455, 1.0667775869369507, 1.0653173923492432]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.8678375482559204
current iteration best possible eval_loss (full train run):  -1.0653173923492432
max eval_loss so far:  -1.0653173923492432
BO observations:  [-1.8678375482559204]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5646 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.695120213976727
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.6924991372389926
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.6924991269410311
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.6932613940329141
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.6925984388410679
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1358, dtype=torch.float64), tensor(0.0227, dtype=torch.float64), tensor(0.1144, dtype=torch.float64), 0, 0, 0, tensor(0.2181, dtype=torch.float64), tensor(0.5017, dtype=torch.float64), 1, 1, 0, 1, 0, 1, 128, 8.67361737988404e-19, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0073, dtype=torch.float64), tensor(0.1358, dtype=torch.float64), tensor(0.0227, dtype=torch.float64), tensor(0.1144, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.9766e-17, dtype=torch.float64), tensor(0.2181, dtype=torch.float64), tensor(0.5017, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(8.6736e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.136
  rowan_hellaswag: 0.023
  sciq: 0.114
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.218
  arc_challenge: 0.502

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (8.67361737988404e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  8.67361737988404e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 5,767,168 || all params: 8,036,028,416 || trainable%: 0.0718
length of training data:  9924
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5508, 'grad_norm': 0.8500556349754333, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.903364419937134, 'eval_runtime': 4.2916, 'eval_samples_per_second': 233.014, 'eval_steps_per_second': 14.68, 'epoch': 0.04}
{'loss': 2.4141, 'grad_norm': 1.2410573959350586, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.420156240463257, 'eval_runtime': 2.8419, 'eval_samples_per_second': 351.874, 'eval_steps_per_second': 22.168, 'epoch': 0.08}
{'loss': 1.7735, 'grad_norm': 1.0301213264465332, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 2.433459997177124, 'eval_runtime': 2.8459, 'eval_samples_per_second': 351.384, 'eval_steps_per_second': 22.137, 'epoch': 0.12}
{'loss': 1.5364, 'grad_norm': 0.4940360486507416, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 2.234063148498535, 'eval_runtime': 2.8694, 'eval_samples_per_second': 348.508, 'eval_steps_per_second': 21.956, 'epoch': 0.16}
{'loss': 1.4074, 'grad_norm': 1.412531852722168, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 2.08243465423584, 'eval_runtime': 2.8638, 'eval_samples_per_second': 349.185, 'eval_steps_per_second': 21.999, 'epoch': 0.2}
{'loss': 1.4296, 'grad_norm': 0.42359453439712524, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 2.1265947818756104, 'eval_runtime': 2.8668, 'eval_samples_per_second': 348.827, 'eval_steps_per_second': 21.976, 'epoch': 0.24}
{'loss': 1.3421, 'grad_norm': 0.37889134883880615, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 2.114013195037842, 'eval_runtime': 2.8633, 'eval_samples_per_second': 349.246, 'eval_steps_per_second': 22.002, 'epoch': 0.28}
{'loss': 1.3634, 'grad_norm': 0.3470410406589508, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.977159857749939, 'eval_runtime': 2.8605, 'eval_samples_per_second': 349.588, 'eval_steps_per_second': 22.024, 'epoch': 0.32}
{'loss': 1.2473, 'grad_norm': 0.31674060225486755, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.8661415576934814, 'eval_runtime': 2.8655, 'eval_samples_per_second': 348.975, 'eval_steps_per_second': 21.985, 'epoch': 0.36}
{'loss': 1.2339, 'grad_norm': 0.4241889715194702, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.8779563903808594, 'eval_runtime': 2.8597, 'eval_samples_per_second': 349.684, 'eval_steps_per_second': 22.03, 'epoch': 0.4}
{'loss': 1.1846, 'grad_norm': 0.30515578389167786, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.7725696563720703, 'eval_runtime': 2.8655, 'eval_samples_per_second': 348.978, 'eval_steps_per_second': 21.986, 'epoch': 0.44}
{'loss': 1.2641, 'grad_norm': 0.4929378032684326, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.801168441772461, 'eval_runtime': 2.8676, 'eval_samples_per_second': 348.726, 'eval_steps_per_second': 21.97, 'epoch': 0.48}
{'loss': 1.1722, 'grad_norm': 0.42933303117752075, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.6925500631332397, 'eval_runtime': 2.8625, 'eval_samples_per_second': 349.34, 'eval_steps_per_second': 22.008, 'epoch': 0.52}
{'loss': 1.16, 'grad_norm': 0.461733341217041, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.7160511016845703, 'eval_runtime': 2.8675, 'eval_samples_per_second': 348.732, 'eval_steps_per_second': 21.97, 'epoch': 0.56}
{'loss': 1.1528, 'grad_norm': 0.3871248960494995, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.6931511163711548, 'eval_runtime': 2.8613, 'eval_samples_per_second': 349.494, 'eval_steps_per_second': 22.018, 'epoch': 0.6}
{'loss': 1.1796, 'grad_norm': 0.3654524087905884, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.760172724723816, 'eval_runtime': 2.8601, 'eval_samples_per_second': 349.643, 'eval_steps_per_second': 22.028, 'epoch': 0.64}
{'loss': 1.1256, 'grad_norm': 0.39870786666870117, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.7523781061172485, 'eval_runtime': 2.8613, 'eval_samples_per_second': 349.498, 'eval_steps_per_second': 22.018, 'epoch': 0.68}
{'loss': 1.2018, 'grad_norm': 0.3160645663738251, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.6970133781433105, 'eval_runtime': 2.8535, 'eval_samples_per_second': 350.444, 'eval_steps_per_second': 22.078, 'epoch': 0.72}
{'loss': 1.1083, 'grad_norm': 0.35254621505737305, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.7199394702911377, 'eval_runtime': 2.8503, 'eval_samples_per_second': 350.84, 'eval_steps_per_second': 22.103, 'epoch': 0.76}
{'loss': 1.1633, 'grad_norm': 0.32981231808662415, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.7105486392974854, 'eval_runtime': 2.8529, 'eval_samples_per_second': 350.52, 'eval_steps_per_second': 22.083, 'epoch': 0.81}
{'loss': 1.1402, 'grad_norm': 0.40917614102363586, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.7907472848892212, 'eval_runtime': 2.8609, 'eval_samples_per_second': 349.54, 'eval_steps_per_second': 22.021, 'epoch': 0.85}
{'loss': 1.0896, 'grad_norm': 0.294920951128006, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.750704050064087, 'eval_runtime': 2.8506, 'eval_samples_per_second': 350.8, 'eval_steps_per_second': 22.1, 'epoch': 0.89}
{'loss': 1.087, 'grad_norm': 0.20894818007946014, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.7497862577438354, 'eval_runtime': 2.855, 'eval_samples_per_second': 350.261, 'eval_steps_per_second': 22.066, 'epoch': 0.93}
{'loss': 1.1454, 'grad_norm': 0.32956433296203613, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.7554330825805664, 'eval_runtime': 2.8582, 'eval_samples_per_second': 349.871, 'eval_steps_per_second': 22.042, 'epoch': 0.97}
{'train_runtime': 250.8525, 'train_samples_per_second': 39.561, 'train_steps_per_second': 2.476, 'train_loss': 1.3855198416349008, 'epoch': 1.0}
train_results:  {'eval_loss': [3.903364419937134, 2.420156240463257, 2.433459997177124, 2.234063148498535, 2.08243465423584, 2.1265947818756104, 2.114013195037842, 1.977159857749939, 1.8661415576934814, 1.8779563903808594, 1.7725696563720703, 1.801168441772461, 1.6925500631332397, 1.7160511016845703, 1.6931511163711548, 1.760172724723816, 1.7523781061172485, 1.6970133781433105, 1.7199394702911377, 1.7105486392974854, 1.7907472848892212, 1.750704050064087, 1.7497862577438354, 1.7554330825805664], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [3.903364419937134, 2.420156240463257, 2.433459997177124, 2.234063148498535, 2.08243465423584, 2.1265947818756104, 2.114013195037842, 1.977159857749939, 1.8661415576934814, 1.8779563903808594, 1.7725696563720703, 1.801168441772461, 1.6925500631332397, 1.7160511016845703, 1.6931511163711548, 1.760172724723816, 1.7523781061172485, 1.6970133781433105, 1.7199394702911377, 1.7105486392974854, 1.7907472848892212, 1.750704050064087, 1.7497862577438354, 1.7554330825805664]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.6595993041992188
current iteration best possible eval_loss (full train run):  -1.7554330825805664
max eval_loss so far:  -1.0653173923492432
BO observations:  [-1.8678375482559204, -3.6595993041992188]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6037 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5931183099746704, 0.48910677433013916, 0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.43514037132263184, 0.4408547878265381, 0.28891366720199585, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7229896187782288, 0.5257965922355652, 0.7089747190475464, 0.6451549530029297]  ‚Üí  acq = -0.618435999979956
X = [0.8045198321342468, 0.5732041597366333, 0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5429293513298035, 0.6168438792228699, 0.6032367944717407, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.042392924427986145, 0.2894328832626343, 0.9503340721130371, 0.8085587620735168]  ‚Üí  acq = -0.5997460902237379
X = [0.18200689554214478, 0.36882972717285156, 0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.7035383582115173, 0.5326343774795532, 0.1735246777534485, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.2245919555425644, 0.050669074058532715, 0.8647359609603882, 0.040459275245666504]  ‚Üí  acq = -0.5842454499275942
X = [0.6313091516494751, 0.9872360825538635, 0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.09140855073928833, 0.0616917610168457, 0.16755545139312744, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.26512572169303894, 0.2105121612548828, 0.8316733837127686, 0.5374197959899902]  ‚Üí  acq = -0.5806113293445132
X = [0.04147899150848389, 0.4950082302093506, 0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6635695695877075, 0.1939259171485901, 0.9576328992843628, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7788231372833252, 0.13258826732635498, 0.11039420962333679, 0.1842997670173645]  ‚Üí  acq = -0.4358360988386627
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.1928, dtype=torch.float64), tensor(0.3476, dtype=torch.float64), 0, tensor(0.3331, dtype=torch.float64), tensor(0.1265, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.1, 47.99999999999997, 0]
normalized proposed parameters for next round by BO: [tensor(1.5700e-17, dtype=torch.float64), tensor(1.6592e-16, dtype=torch.float64), tensor(3.6268e-16, dtype=torch.float64), tensor(3.2222e-17, dtype=torch.float64), tensor(0.1928, dtype=torch.float64), tensor(0.3476, dtype=torch.float64), tensor(8.3566e-17, dtype=torch.float64), tensor(0.3331, dtype=torch.float64), tensor(0.1265, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.193
  truthfulqa_gen: 0.348
  wikitext: 0
  mmlu: 0.333
  arc_challenge: 0.127

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.99999999999997,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  47.99999999999997
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7003, 'grad_norm': 0.7609568238258362, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4939078092575073, 'eval_runtime': 3.6362, 'eval_samples_per_second': 275.014, 'eval_steps_per_second': 17.326, 'epoch': 0.04}
{'loss': 1.2408, 'grad_norm': 0.45134609937667847, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1153026819229126, 'eval_runtime': 3.6301, 'eval_samples_per_second': 275.472, 'eval_steps_per_second': 17.355, 'epoch': 0.08}
{'loss': 1.0243, 'grad_norm': 0.3059494197368622, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.065891981124878, 'eval_runtime': 3.6283, 'eval_samples_per_second': 275.607, 'eval_steps_per_second': 17.363, 'epoch': 0.12}
{'loss': 1.0575, 'grad_norm': 0.9335728883743286, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0638974905014038, 'eval_runtime': 3.6223, 'eval_samples_per_second': 276.067, 'eval_steps_per_second': 17.392, 'epoch': 0.16}
{'loss': 0.9827, 'grad_norm': 0.2827892005443573, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.060441017150879, 'eval_runtime': 3.6316, 'eval_samples_per_second': 275.361, 'eval_steps_per_second': 17.348, 'epoch': 0.2}
{'loss': 0.968, 'grad_norm': 0.3505016565322876, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0664899349212646, 'eval_runtime': 3.6275, 'eval_samples_per_second': 275.671, 'eval_steps_per_second': 17.367, 'epoch': 0.24}
{'loss': 0.9851, 'grad_norm': 0.347812682390213, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0459593534469604, 'eval_runtime': 3.6265, 'eval_samples_per_second': 275.749, 'eval_steps_per_second': 17.372, 'epoch': 0.28}
{'loss': 0.9715, 'grad_norm': 0.2732115089893341, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.038089632987976, 'eval_runtime': 3.6307, 'eval_samples_per_second': 275.43, 'eval_steps_per_second': 17.352, 'epoch': 0.32}
{'loss': 0.9359, 'grad_norm': 0.2654606103897095, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0357558727264404, 'eval_runtime': 3.6288, 'eval_samples_per_second': 275.57, 'eval_steps_per_second': 17.361, 'epoch': 0.36}
{'loss': 0.9363, 'grad_norm': 0.2963070571422577, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0310508012771606, 'eval_runtime': 3.6569, 'eval_samples_per_second': 273.457, 'eval_steps_per_second': 17.228, 'epoch': 0.4}
{'loss': 0.9187, 'grad_norm': 0.30607956647872925, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0381574630737305, 'eval_runtime': 3.6608, 'eval_samples_per_second': 273.164, 'eval_steps_per_second': 17.209, 'epoch': 0.44}
{'loss': 0.8812, 'grad_norm': 0.3294081687927246, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0297212600708008, 'eval_runtime': 3.6423, 'eval_samples_per_second': 274.553, 'eval_steps_per_second': 17.297, 'epoch': 0.48}
{'loss': 0.9124, 'grad_norm': 0.2593854069709778, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0248165130615234, 'eval_runtime': 3.6548, 'eval_samples_per_second': 273.61, 'eval_steps_per_second': 17.237, 'epoch': 0.52}
{'loss': 0.9398, 'grad_norm': 0.3177781105041504, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0237458944320679, 'eval_runtime': 3.6534, 'eval_samples_per_second': 273.72, 'eval_steps_per_second': 17.244, 'epoch': 0.56}
{'loss': 0.8588, 'grad_norm': 0.4125140309333801, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0261895656585693, 'eval_runtime': 3.6475, 'eval_samples_per_second': 274.157, 'eval_steps_per_second': 17.272, 'epoch': 0.6}
{'loss': 0.8471, 'grad_norm': 0.2828969657421112, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0143136978149414, 'eval_runtime': 3.6395, 'eval_samples_per_second': 274.761, 'eval_steps_per_second': 17.31, 'epoch': 0.64}
{'loss': 0.8678, 'grad_norm': 0.2710001468658447, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0103923082351685, 'eval_runtime': 3.6376, 'eval_samples_per_second': 274.905, 'eval_steps_per_second': 17.319, 'epoch': 0.68}
{'loss': 0.8945, 'grad_norm': 0.273861825466156, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0168514251708984, 'eval_runtime': 3.6314, 'eval_samples_per_second': 275.374, 'eval_steps_per_second': 17.349, 'epoch': 0.72}
{'loss': 0.8707, 'grad_norm': 0.307509183883667, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0130059719085693, 'eval_runtime': 3.6304, 'eval_samples_per_second': 275.453, 'eval_steps_per_second': 17.354, 'epoch': 0.76}
{'loss': 0.8538, 'grad_norm': 0.28903084993362427, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0108424425125122, 'eval_runtime': 3.6325, 'eval_samples_per_second': 275.293, 'eval_steps_per_second': 17.343, 'epoch': 0.8}
{'loss': 0.8463, 'grad_norm': 0.26599329710006714, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0142742395401, 'eval_runtime': 3.6238, 'eval_samples_per_second': 275.957, 'eval_steps_per_second': 17.385, 'epoch': 0.84}
{'loss': 0.7989, 'grad_norm': 0.3293154239654541, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0099620819091797, 'eval_runtime': 3.6245, 'eval_samples_per_second': 275.902, 'eval_steps_per_second': 17.382, 'epoch': 0.88}
{'loss': 0.8068, 'grad_norm': 0.2985285222530365, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.010056734085083, 'eval_runtime': 3.6258, 'eval_samples_per_second': 275.801, 'eval_steps_per_second': 17.375, 'epoch': 0.92}
{'loss': 0.7854, 'grad_norm': 0.2587165832519531, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0089176893234253, 'eval_runtime': 3.6277, 'eval_samples_per_second': 275.661, 'eval_steps_per_second': 17.367, 'epoch': 0.96}
{'loss': 0.8302, 'grad_norm': 0.3419916331768036, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.007631540298462, 'eval_runtime': 3.6297, 'eval_samples_per_second': 275.508, 'eval_steps_per_second': 17.357, 'epoch': 1.0}
{'train_runtime': 310.6005, 'train_samples_per_second': 32.189, 'train_steps_per_second': 2.012, 'train_loss': 0.9885906646728516, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4939078092575073, 1.1153026819229126, 1.065891981124878, 1.0638974905014038, 1.060441017150879, 1.0664899349212646, 1.0459593534469604, 1.038089632987976, 1.0357558727264404, 1.0310508012771606, 1.0381574630737305, 1.0297212600708008, 1.0248165130615234, 1.0237458944320679, 1.0261895656585693, 1.0143136978149414, 1.0103923082351685, 1.0168514251708984, 1.0130059719085693, 1.0108424425125122, 1.0142742395401, 1.0099620819091797, 1.010056734085083, 1.0089176893234253, 1.007631540298462], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4939078092575073, 1.1153026819229126, 1.065891981124878, 1.0638974905014038, 1.060441017150879, 1.0664899349212646, 1.0459593534469604, 1.038089632987976, 1.0357558727264404, 1.0310508012771606, 1.0381574630737305, 1.0297212600708008, 1.0248165130615234, 1.0237458944320679, 1.0261895656585693, 1.0143136978149414, 1.0103923082351685, 1.0168514251708984, 1.0130059719085693, 1.0108424425125122, 1.0142742395401, 1.0099620819091797, 1.010056734085083, 1.0089176893234253, 1.007631540298462]
current iteration observed (possibly low-fid or predicted) eval_loss:  -4.5169548988342285
current iteration best possible eval_loss (full train run):  -1.007631540298462
max eval_loss so far:  -1.007631540298462
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.2021 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = 0.269304430038809
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = 0.3371379873928595
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = 0.11358662174958178
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = 0.25817567092552407
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = 0.29264292872784425
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2038, dtype=torch.float64), 0, 0, 0, tensor(0.7281, dtype=torch.float64), 0, tensor(0.0472, dtype=torch.float64), tensor(0.0209, dtype=torch.float64), 32, 0, 1, 0, 0, 1, 43, 0.09999999999999991, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(3.4529e-17, dtype=torch.float64), tensor(0.2038, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7281, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0472, dtype=torch.float64), tensor(0.0209, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3380, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.204
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.728
  wikitext: 0
  mmlu: 0.047
  arc_challenge: 0.021

LoRA Parameters:
  lora_r: (43,)
  lora_dropout: (0.09999999999999991,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  43
lora dropout:  0.09999999999999991
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 32,407,552 || all params: 8,062,668,800 || trainable%: 0.4019
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7076, 'grad_norm': 0.8450372815132141, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1015355587005615, 'eval_runtime': 3.2501, 'eval_samples_per_second': 307.68, 'eval_steps_per_second': 19.384, 'epoch': 0.04}
{'loss': 1.1643, 'grad_norm': 0.49941062927246094, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7171880006790161, 'eval_runtime': 3.2441, 'eval_samples_per_second': 308.256, 'eval_steps_per_second': 19.42, 'epoch': 0.08}
{'loss': 0.9211, 'grad_norm': 0.5302943587303162, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5910569429397583, 'eval_runtime': 3.2456, 'eval_samples_per_second': 308.11, 'eval_steps_per_second': 19.411, 'epoch': 0.12}
{'loss': 0.7775, 'grad_norm': 0.4382343292236328, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7440245151519775, 'eval_runtime': 3.2596, 'eval_samples_per_second': 306.786, 'eval_steps_per_second': 19.328, 'epoch': 0.16}
{'loss': 0.7345, 'grad_norm': 0.532914936542511, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6263922452926636, 'eval_runtime': 3.2268, 'eval_samples_per_second': 309.904, 'eval_steps_per_second': 19.524, 'epoch': 0.2}
{'loss': 0.6954, 'grad_norm': 0.41750797629356384, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.666504979133606, 'eval_runtime': 3.2128, 'eval_samples_per_second': 311.258, 'eval_steps_per_second': 19.609, 'epoch': 0.24}
{'loss': 0.5984, 'grad_norm': 0.500893235206604, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7242114543914795, 'eval_runtime': 3.2212, 'eval_samples_per_second': 310.446, 'eval_steps_per_second': 19.558, 'epoch': 0.28}
{'loss': 0.6274, 'grad_norm': 0.3400709927082062, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7386075258255005, 'eval_runtime': 3.2216, 'eval_samples_per_second': 310.402, 'eval_steps_per_second': 19.555, 'epoch': 0.32}
{'loss': 0.6127, 'grad_norm': 0.3647622764110565, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7331184148788452, 'eval_runtime': 3.2172, 'eval_samples_per_second': 310.832, 'eval_steps_per_second': 19.582, 'epoch': 0.36}
{'loss': 0.581, 'grad_norm': 0.41651982069015503, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.668517827987671, 'eval_runtime': 3.2188, 'eval_samples_per_second': 310.68, 'eval_steps_per_second': 19.573, 'epoch': 0.4}
{'loss': 0.6457, 'grad_norm': 0.36923930048942566, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.705476999282837, 'eval_runtime': 3.2198, 'eval_samples_per_second': 310.576, 'eval_steps_per_second': 19.566, 'epoch': 0.44}
{'loss': 0.5852, 'grad_norm': 0.41205981373786926, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.717414379119873, 'eval_runtime': 3.2159, 'eval_samples_per_second': 310.954, 'eval_steps_per_second': 19.59, 'epoch': 0.48}
{'loss': 0.6071, 'grad_norm': 0.2961862087249756, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7276804447174072, 'eval_runtime': 3.2275, 'eval_samples_per_second': 309.838, 'eval_steps_per_second': 19.52, 'epoch': 0.52}
{'loss': 0.5018, 'grad_norm': 0.3408335745334625, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8070650100708008, 'eval_runtime': 3.24, 'eval_samples_per_second': 308.646, 'eval_steps_per_second': 19.445, 'epoch': 0.56}
{'loss': 0.5545, 'grad_norm': 0.2799513041973114, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8302271366119385, 'eval_runtime': 3.2299, 'eval_samples_per_second': 309.604, 'eval_steps_per_second': 19.505, 'epoch': 0.6}
{'loss': 0.5539, 'grad_norm': 0.31426286697387695, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8460910320281982, 'eval_runtime': 3.2194, 'eval_samples_per_second': 310.613, 'eval_steps_per_second': 19.569, 'epoch': 0.64}
{'loss': 0.5057, 'grad_norm': 0.40570956468582153, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7991859912872314, 'eval_runtime': 3.22, 'eval_samples_per_second': 310.563, 'eval_steps_per_second': 19.565, 'epoch': 0.68}
{'loss': 0.5335, 'grad_norm': 0.36100924015045166, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9127717018127441, 'eval_runtime': 3.2211, 'eval_samples_per_second': 310.45, 'eval_steps_per_second': 19.558, 'epoch': 0.72}
{'loss': 0.531, 'grad_norm': 0.29152053594589233, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.841539740562439, 'eval_runtime': 3.2283, 'eval_samples_per_second': 309.761, 'eval_steps_per_second': 19.515, 'epoch': 0.76}
{'loss': 0.5233, 'grad_norm': 0.33835336565971375, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8380120992660522, 'eval_runtime': 3.2018, 'eval_samples_per_second': 312.323, 'eval_steps_per_second': 19.676, 'epoch': 0.8}
{'loss': 0.5209, 'grad_norm': 0.6615425944328308, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9102801084518433, 'eval_runtime': 3.2068, 'eval_samples_per_second': 311.837, 'eval_steps_per_second': 19.646, 'epoch': 0.84}
{'loss': 0.5208, 'grad_norm': 0.3247310519218445, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9144247770309448, 'eval_runtime': 3.207, 'eval_samples_per_second': 311.821, 'eval_steps_per_second': 19.645, 'epoch': 0.88}
{'loss': 0.5361, 'grad_norm': 0.3185783326625824, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9043720960617065, 'eval_runtime': 3.2071, 'eval_samples_per_second': 311.812, 'eval_steps_per_second': 19.644, 'epoch': 0.92}
{'loss': 0.5138, 'grad_norm': 0.364693820476532, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9084057807922363, 'eval_runtime': 3.205, 'eval_samples_per_second': 312.008, 'eval_steps_per_second': 19.657, 'epoch': 0.96}
{'loss': 0.4834, 'grad_norm': 0.26273882389068604, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9016895294189453, 'eval_runtime': 3.214, 'eval_samples_per_second': 311.135, 'eval_steps_per_second': 19.601, 'epoch': 1.0}
{'train_runtime': 287.0908, 'train_samples_per_second': 34.829, 'train_steps_per_second': 2.177, 'train_loss': 0.7014675079345704, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1015355587005615, 1.7171880006790161, 1.5910569429397583, 1.7440245151519775, 1.6263922452926636, 1.666504979133606, 1.7242114543914795, 1.7386075258255005, 1.7331184148788452, 1.668517827987671, 1.705476999282837, 1.717414379119873, 1.7276804447174072, 1.8070650100708008, 1.8302271366119385, 1.8460910320281982, 1.7991859912872314, 1.9127717018127441, 1.841539740562439, 1.8380120992660522, 1.9102801084518433, 1.9144247770309448, 1.9043720960617065, 1.9084057807922363, 1.9016895294189453], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1015355587005615, 1.7171880006790161, 1.5910569429397583, 1.7440245151519775, 1.6263922452926636, 1.666504979133606, 1.7242114543914795, 1.7386075258255005, 1.7331184148788452, 1.668517827987671, 1.705476999282837, 1.717414379119873, 1.7276804447174072, 1.8070650100708008, 1.8302271366119385, 1.8460910320281982, 1.7991859912872314, 1.9127717018127441, 1.841539740562439, 1.8380120992660522, 1.9102801084518433, 1.9144247770309448, 1.9043720960617065, 1.9084057807922363, 1.9016895294189453]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.9427738189697266
current iteration best possible eval_loss (full train run):  -1.9016895294189453
max eval_loss so far:  -1.007631540298462
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.8753 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.18579580548158914
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.2349436121532258
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.24510362861486978
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.2315034688895543
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.23159753787135218
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.4701, dtype=torch.float64), tensor(0.0533, dtype=torch.float64), tensor(0.4766, dtype=torch.float64), 0, 0, 0, 1, 0, 0, 1, 1, 1, 2, 0.0, 1.4800000190734883, 1]
normalized proposed parameters for next round by BO: [tensor(4.1864e-17, dtype=torch.float64), tensor(1.3772e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4701, dtype=torch.float64), tensor(0.0533, dtype=torch.float64), tensor(0.4766, dtype=torch.float64), tensor(4.7417e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.4279e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.47
  triviaqa: 0.053
  truthfulqa_gen: 0.477
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734883,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  1.4800000190734883
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 110,592 || all params: 8,030,371,840 || trainable%: 0.0014
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 5.5412, 'grad_norm': 1.7458504438400269, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 5.157149314880371, 'eval_runtime': 2.8394, 'eval_samples_per_second': 352.184, 'eval_steps_per_second': 22.188, 'epoch': 0.04}
{'loss': 4.6097, 'grad_norm': 2.5205471515655518, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 3.7142815589904785, 'eval_runtime': 2.8495, 'eval_samples_per_second': 350.943, 'eval_steps_per_second': 22.109, 'epoch': 0.08}
{'loss': 3.3393, 'grad_norm': 2.7291948795318604, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.6660192012786865, 'eval_runtime': 2.8561, 'eval_samples_per_second': 350.133, 'eval_steps_per_second': 22.058, 'epoch': 0.12}
{'loss': 2.1735, 'grad_norm': 2.4995386600494385, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.4152653217315674, 'eval_runtime': 2.8485, 'eval_samples_per_second': 351.057, 'eval_steps_per_second': 22.117, 'epoch': 0.16}
{'loss': 1.6544, 'grad_norm': 5.5445637702941895, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0999562740325928, 'eval_runtime': 2.8512, 'eval_samples_per_second': 350.731, 'eval_steps_per_second': 22.096, 'epoch': 0.2}
{'loss': 1.5536, 'grad_norm': 1.6457453966140747, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1033923625946045, 'eval_runtime': 2.863, 'eval_samples_per_second': 349.289, 'eval_steps_per_second': 22.005, 'epoch': 0.24}
{'loss': 1.5645, 'grad_norm': 1.6816014051437378, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.1349337100982666, 'eval_runtime': 2.8604, 'eval_samples_per_second': 349.602, 'eval_steps_per_second': 22.025, 'epoch': 0.28}
{'loss': 1.5375, 'grad_norm': 1.1560639142990112, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.2272567749023438, 'eval_runtime': 2.8599, 'eval_samples_per_second': 349.662, 'eval_steps_per_second': 22.029, 'epoch': 0.32}
{'loss': 1.5391, 'grad_norm': 1.2060176134109497, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.0207247734069824, 'eval_runtime': 2.8684, 'eval_samples_per_second': 348.627, 'eval_steps_per_second': 21.964, 'epoch': 0.36}
{'loss': 1.5065, 'grad_norm': 1.6314167976379395, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.085190773010254, 'eval_runtime': 2.8683, 'eval_samples_per_second': 348.634, 'eval_steps_per_second': 21.964, 'epoch': 0.4}
{'loss': 1.5147, 'grad_norm': 1.1703555583953857, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.038534641265869, 'eval_runtime': 2.8663, 'eval_samples_per_second': 348.878, 'eval_steps_per_second': 21.979, 'epoch': 0.44}
{'loss': 1.5009, 'grad_norm': 1.2752882242202759, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.0162501335144043, 'eval_runtime': 2.8709, 'eval_samples_per_second': 348.319, 'eval_steps_per_second': 21.944, 'epoch': 0.48}
{'loss': 1.4778, 'grad_norm': 0.9383641481399536, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9936684370040894, 'eval_runtime': 2.8744, 'eval_samples_per_second': 347.897, 'eval_steps_per_second': 21.918, 'epoch': 0.52}
{'loss': 1.4429, 'grad_norm': 2.5966577529907227, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9593806266784668, 'eval_runtime': 2.8712, 'eval_samples_per_second': 348.285, 'eval_steps_per_second': 21.942, 'epoch': 0.56}
{'loss': 1.4221, 'grad_norm': 1.6069397926330566, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9629286527633667, 'eval_runtime': 2.8745, 'eval_samples_per_second': 347.885, 'eval_steps_per_second': 21.917, 'epoch': 0.6}
{'loss': 1.4639, 'grad_norm': 2.2811696529388428, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9452228546142578, 'eval_runtime': 2.8739, 'eval_samples_per_second': 347.964, 'eval_steps_per_second': 21.922, 'epoch': 0.64}
{'loss': 1.4102, 'grad_norm': 1.2138327360153198, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9121277332305908, 'eval_runtime': 2.8736, 'eval_samples_per_second': 347.999, 'eval_steps_per_second': 21.924, 'epoch': 0.68}
{'loss': 1.4016, 'grad_norm': 0.8524707555770874, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9166523218154907, 'eval_runtime': 2.8712, 'eval_samples_per_second': 348.292, 'eval_steps_per_second': 21.942, 'epoch': 0.72}
{'loss': 1.4091, 'grad_norm': 3.1590309143066406, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8566198348999023, 'eval_runtime': 2.8694, 'eval_samples_per_second': 348.503, 'eval_steps_per_second': 21.956, 'epoch': 0.76}
{'loss': 1.3876, 'grad_norm': 1.0602359771728516, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9076792001724243, 'eval_runtime': 2.8687, 'eval_samples_per_second': 348.589, 'eval_steps_per_second': 21.961, 'epoch': 0.8}
{'loss': 1.3485, 'grad_norm': 1.0954680442810059, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8785032033920288, 'eval_runtime': 2.8739, 'eval_samples_per_second': 347.959, 'eval_steps_per_second': 21.921, 'epoch': 0.84}
{'loss': 1.406, 'grad_norm': 1.3634309768676758, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8461672067642212, 'eval_runtime': 2.878, 'eval_samples_per_second': 347.461, 'eval_steps_per_second': 21.89, 'epoch': 0.88}
{'loss': 1.371, 'grad_norm': 1.4426991939544678, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.887447714805603, 'eval_runtime': 2.8799, 'eval_samples_per_second': 347.234, 'eval_steps_per_second': 21.876, 'epoch': 0.92}
{'loss': 1.3743, 'grad_norm': 2.941011905670166, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8614193201065063, 'eval_runtime': 2.8692, 'eval_samples_per_second': 348.533, 'eval_steps_per_second': 21.958, 'epoch': 0.96}
{'loss': 1.4284, 'grad_norm': 2.371443271636963, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8701083660125732, 'eval_runtime': 2.8741, 'eval_samples_per_second': 347.93, 'eval_steps_per_second': 21.92, 'epoch': 1.0}
{'train_runtime': 138.5432, 'train_samples_per_second': 72.165, 'train_steps_per_second': 4.511, 'train_loss': 1.8551283569335937, 'epoch': 1.0}
train_results:  {'eval_loss': [5.157149314880371, 3.7142815589904785, 2.6660192012786865, 2.4152653217315674, 2.0999562740325928, 2.1033923625946045, 2.1349337100982666, 2.2272567749023438, 2.0207247734069824, 2.085190773010254, 2.038534641265869, 2.0162501335144043, 1.9936684370040894, 1.9593806266784668, 1.9629286527633667, 1.9452228546142578, 1.9121277332305908, 1.9166523218154907, 1.8566198348999023, 1.9076792001724243, 1.8785032033920288, 1.8461672067642212, 1.887447714805603, 1.8614193201065063, 1.8701083660125732], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [5.157149314880371, 3.7142815589904785, 2.6660192012786865, 2.4152653217315674, 2.0999562740325928, 2.1033923625946045, 2.1349337100982666, 2.2272567749023438, 2.0207247734069824, 2.085190773010254, 2.038534641265869, 2.0162501335144043, 1.9936684370040894, 1.9593806266784668, 1.9629286527633667, 1.9452228546142578, 1.9121277332305908, 1.9166523218154907, 1.8566198348999023, 1.9076792001724243, 1.8785032033920288, 1.8461672067642212, 1.887447714805603, 1.8614193201065063, 1.8701083660125732]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.4833648204803467
current iteration best possible eval_loss (full train run):  -1.8701083660125732
max eval_loss so far:  -1.007631540298462
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.9930 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.6289058963960641
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.42683798643019566
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.4601306363492892
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.5827799291394771
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.4365046757896667
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4012, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2147, dtype=torch.float64), 0, tensor(0.0250, dtype=torch.float64), tensor(0.3591, dtype=torch.float64), 32, 0, 0, 1, 1, 1, 2, 0.09999999999999999, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.4012, dtype=torch.float64), tensor(1.0092e-18, dtype=torch.float64), tensor(3.3545e-18, dtype=torch.float64), tensor(2.1812e-18, dtype=torch.float64), tensor(1.1631e-16, dtype=torch.float64), tensor(0.2147, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0250, dtype=torch.float64), tensor(0.3591, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.401
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.215
  wikitext: 0
  mmlu: 0.025
  arc_challenge: 0.359

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.09999999999999999,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  2
lora dropout:  0.09999999999999999
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,538,944 || all params: 8,033,800,192 || trainable%: 0.0441
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.1843, 'grad_norm': 2.221467971801758, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.181392192840576, 'eval_runtime': 5.7212, 'eval_samples_per_second': 174.79, 'eval_steps_per_second': 11.012, 'epoch': 0.04}
{'loss': 1.8205, 'grad_norm': 1.4503200054168701, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.1238300800323486, 'eval_runtime': 3.5611, 'eval_samples_per_second': 280.814, 'eval_steps_per_second': 17.691, 'epoch': 0.08}
{'loss': 1.0856, 'grad_norm': 0.6795150637626648, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.693359136581421, 'eval_runtime': 3.5596, 'eval_samples_per_second': 280.932, 'eval_steps_per_second': 17.699, 'epoch': 0.12}
{'loss': 0.9362, 'grad_norm': 0.3839173913002014, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6322699785232544, 'eval_runtime': 3.5597, 'eval_samples_per_second': 280.922, 'eval_steps_per_second': 17.698, 'epoch': 0.16}
{'loss': 0.8846, 'grad_norm': 0.33596786856651306, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.707076072692871, 'eval_runtime': 3.5812, 'eval_samples_per_second': 279.239, 'eval_steps_per_second': 17.592, 'epoch': 0.2}
{'loss': 0.8805, 'grad_norm': 0.31878232955932617, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6425713300704956, 'eval_runtime': 3.5753, 'eval_samples_per_second': 279.7, 'eval_steps_per_second': 17.621, 'epoch': 0.24}
{'loss': 0.895, 'grad_norm': 0.3542384207248688, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7144428491592407, 'eval_runtime': 3.5968, 'eval_samples_per_second': 278.026, 'eval_steps_per_second': 17.516, 'epoch': 0.28}
{'loss': 0.876, 'grad_norm': 0.3270879089832306, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6737087965011597, 'eval_runtime': 3.5701, 'eval_samples_per_second': 280.103, 'eval_steps_per_second': 17.647, 'epoch': 0.32}
{'loss': 0.837, 'grad_norm': 0.3823832869529724, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.717761754989624, 'eval_runtime': 3.5699, 'eval_samples_per_second': 280.117, 'eval_steps_per_second': 17.647, 'epoch': 0.36}
{'loss': 0.8554, 'grad_norm': 0.44304853677749634, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7510522603988647, 'eval_runtime': 3.5626, 'eval_samples_per_second': 280.696, 'eval_steps_per_second': 17.684, 'epoch': 0.4}
{'loss': 0.855, 'grad_norm': 0.3661397099494934, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6608973741531372, 'eval_runtime': 3.5651, 'eval_samples_per_second': 280.501, 'eval_steps_per_second': 17.672, 'epoch': 0.44}
{'loss': 0.8205, 'grad_norm': 0.3827064633369446, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7922714948654175, 'eval_runtime': 3.5651, 'eval_samples_per_second': 280.499, 'eval_steps_per_second': 17.671, 'epoch': 0.48}
{'loss': 0.8147, 'grad_norm': 0.4345664978027344, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7183212041854858, 'eval_runtime': 3.5667, 'eval_samples_per_second': 280.374, 'eval_steps_per_second': 17.664, 'epoch': 0.52}
{'loss': 0.8087, 'grad_norm': 0.424978107213974, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7348639965057373, 'eval_runtime': 3.5829, 'eval_samples_per_second': 279.103, 'eval_steps_per_second': 17.583, 'epoch': 0.56}
{'loss': 0.8234, 'grad_norm': 0.46923840045928955, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8144351243972778, 'eval_runtime': 3.5804, 'eval_samples_per_second': 279.301, 'eval_steps_per_second': 17.596, 'epoch': 0.6}
{'loss': 0.8016, 'grad_norm': 0.5113479495048523, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8121637105941772, 'eval_runtime': 3.5751, 'eval_samples_per_second': 279.712, 'eval_steps_per_second': 17.622, 'epoch': 0.64}
{'loss': 0.7828, 'grad_norm': 0.48733091354370117, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8288301229476929, 'eval_runtime': 3.5796, 'eval_samples_per_second': 279.358, 'eval_steps_per_second': 17.6, 'epoch': 0.68}
{'loss': 0.7915, 'grad_norm': 0.511651873588562, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7693493366241455, 'eval_runtime': 3.5658, 'eval_samples_per_second': 280.442, 'eval_steps_per_second': 17.668, 'epoch': 0.72}
{'loss': 0.7875, 'grad_norm': 0.528137743473053, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8403199911117554, 'eval_runtime': 3.5645, 'eval_samples_per_second': 280.547, 'eval_steps_per_second': 17.674, 'epoch': 0.76}
{'loss': 0.7747, 'grad_norm': 0.5234367251396179, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8242853879928589, 'eval_runtime': 3.566, 'eval_samples_per_second': 280.43, 'eval_steps_per_second': 17.667, 'epoch': 0.8}
{'loss': 0.7703, 'grad_norm': 0.5274209976196289, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.849798321723938, 'eval_runtime': 3.5605, 'eval_samples_per_second': 280.863, 'eval_steps_per_second': 17.694, 'epoch': 0.84}
{'loss': 0.7576, 'grad_norm': 0.6092601418495178, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8105480670928955, 'eval_runtime': 3.5597, 'eval_samples_per_second': 280.919, 'eval_steps_per_second': 17.698, 'epoch': 0.88}
{'loss': 0.7722, 'grad_norm': 0.5580804944038391, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8219213485717773, 'eval_runtime': 3.5622, 'eval_samples_per_second': 280.728, 'eval_steps_per_second': 17.686, 'epoch': 0.92}
{'loss': 0.7584, 'grad_norm': 0.6142558455467224, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8205519914627075, 'eval_runtime': 3.5657, 'eval_samples_per_second': 280.448, 'eval_steps_per_second': 17.668, 'epoch': 0.96}
{'loss': 0.7433, 'grad_norm': 0.5982431173324585, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8139086961746216, 'eval_runtime': 3.5685, 'eval_samples_per_second': 280.229, 'eval_steps_per_second': 17.654, 'epoch': 1.0}
{'train_runtime': 259.2508, 'train_samples_per_second': 38.565, 'train_steps_per_second': 2.411, 'train_loss': 1.004692788696289, 'epoch': 1.0}
train_results:  {'eval_loss': [3.181392192840576, 2.1238300800323486, 1.693359136581421, 1.6322699785232544, 1.707076072692871, 1.6425713300704956, 1.7144428491592407, 1.6737087965011597, 1.717761754989624, 1.7510522603988647, 1.6608973741531372, 1.7922714948654175, 1.7183212041854858, 1.7348639965057373, 1.8144351243972778, 1.8121637105941772, 1.8288301229476929, 1.7693493366241455, 1.8403199911117554, 1.8242853879928589, 1.849798321723938, 1.8105480670928955, 1.8219213485717773, 1.8205519914627075, 1.8139086961746216], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.181392192840576, 2.1238300800323486, 1.693359136581421, 1.6322699785232544, 1.707076072692871, 1.6425713300704956, 1.7144428491592407, 1.6737087965011597, 1.717761754989624, 1.7510522603988647, 1.6608973741531372, 1.7922714948654175, 1.7183212041854858, 1.7348639965057373, 1.8144351243972778, 1.8121637105941772, 1.8288301229476929, 1.7693493366241455, 1.8403199911117554, 1.8242853879928589, 1.849798321723938, 1.8105480670928955, 1.8219213485717773, 1.8205519914627075, 1.8139086961746216]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.288141965866089
current iteration best possible eval_loss (full train run):  -1.8139086961746216
max eval_loss so far:  -1.007631540298462
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.8274 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.6216540636579355
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.64944853805851
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.6156850433471721
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.26914812889342365
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.6224974147124027
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0161, dtype=torch.float64), tensor(0.0107, dtype=torch.float64), 0, 0, tensor(0.5114, dtype=torch.float64), tensor(0.1827, dtype=torch.float64), 0, tensor(0.0778, dtype=torch.float64), tensor(0.2014, dtype=torch.float64), 32, 0, 0, 0, 1, 0, 2, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0161, dtype=torch.float64), tensor(0.0107, dtype=torch.float64), tensor(4.1742e-17, dtype=torch.float64), tensor(2.4154e-17, dtype=torch.float64), tensor(0.5114, dtype=torch.float64), tensor(0.1827, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0778, dtype=torch.float64), tensor(0.2014, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.016
  gsm8k: 0.011
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.511
  truthfulqa_gen: 0.183
  wikitext: 0
  mmlu: 0.078
  arc_challenge: 0.201

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,179,648 || all params: 8,031,440,896 || trainable%: 0.0147
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0983, 'grad_norm': 10.876684188842773, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.532882809638977, 'eval_runtime': 3.0642, 'eval_samples_per_second': 326.354, 'eval_steps_per_second': 20.56, 'epoch': 0.04}
{'loss': 1.2064, 'grad_norm': 4.0847063064575195, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0904475450515747, 'eval_runtime': 3.074, 'eval_samples_per_second': 325.314, 'eval_steps_per_second': 20.495, 'epoch': 0.08}
{'loss': 1.0045, 'grad_norm': 1.8347880840301514, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0358818769454956, 'eval_runtime': 3.0683, 'eval_samples_per_second': 325.916, 'eval_steps_per_second': 20.533, 'epoch': 0.12}
{'loss': 0.9758, 'grad_norm': 1.9131110906600952, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0367977619171143, 'eval_runtime': 3.0633, 'eval_samples_per_second': 326.448, 'eval_steps_per_second': 20.566, 'epoch': 0.16}
{'loss': 0.9796, 'grad_norm': 1.564418077468872, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0181695222854614, 'eval_runtime': 3.0665, 'eval_samples_per_second': 326.102, 'eval_steps_per_second': 20.544, 'epoch': 0.2}
{'loss': 0.9774, 'grad_norm': 1.699174165725708, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0142900943756104, 'eval_runtime': 3.0702, 'eval_samples_per_second': 325.708, 'eval_steps_per_second': 20.52, 'epoch': 0.24}
{'loss': 0.9113, 'grad_norm': 2.0636069774627686, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.015260934829712, 'eval_runtime': 3.073, 'eval_samples_per_second': 325.411, 'eval_steps_per_second': 20.501, 'epoch': 0.28}
{'loss': 0.9444, 'grad_norm': 2.017237663269043, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0084683895111084, 'eval_runtime': 3.0774, 'eval_samples_per_second': 324.953, 'eval_steps_per_second': 20.472, 'epoch': 0.32}
{'loss': 0.9479, 'grad_norm': 1.567237138748169, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0186476707458496, 'eval_runtime': 3.0741, 'eval_samples_per_second': 325.302, 'eval_steps_per_second': 20.494, 'epoch': 0.36}
{'loss': 0.9026, 'grad_norm': 1.5543262958526611, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.009446144104004, 'eval_runtime': 3.0791, 'eval_samples_per_second': 324.77, 'eval_steps_per_second': 20.461, 'epoch': 0.4}
{'loss': 0.9165, 'grad_norm': 2.3191494941711426, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0137419700622559, 'eval_runtime': 3.0756, 'eval_samples_per_second': 325.136, 'eval_steps_per_second': 20.484, 'epoch': 0.44}
{'loss': 0.9433, 'grad_norm': 2.3179495334625244, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0031466484069824, 'eval_runtime': 3.0786, 'eval_samples_per_second': 324.824, 'eval_steps_per_second': 20.464, 'epoch': 0.48}
{'loss': 0.9271, 'grad_norm': 2.050243377685547, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0002031326293945, 'eval_runtime': 3.0795, 'eval_samples_per_second': 324.726, 'eval_steps_per_second': 20.458, 'epoch': 0.52}
{'loss': 0.9203, 'grad_norm': 1.8792201280593872, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0036057233810425, 'eval_runtime': 3.0808, 'eval_samples_per_second': 324.59, 'eval_steps_per_second': 20.449, 'epoch': 0.56}
{'loss': 0.8759, 'grad_norm': 1.690030574798584, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9985547065734863, 'eval_runtime': 3.0846, 'eval_samples_per_second': 324.188, 'eval_steps_per_second': 20.424, 'epoch': 0.6}
{'loss': 0.8958, 'grad_norm': 1.7908258438110352, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0023880004882812, 'eval_runtime': 3.0797, 'eval_samples_per_second': 324.702, 'eval_steps_per_second': 20.456, 'epoch': 0.64}
{'loss': 0.8537, 'grad_norm': 1.4979023933410645, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0024402141571045, 'eval_runtime': 3.0885, 'eval_samples_per_second': 323.778, 'eval_steps_per_second': 20.398, 'epoch': 0.68}
{'loss': 0.8854, 'grad_norm': 1.6936168670654297, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9993314743041992, 'eval_runtime': 3.0922, 'eval_samples_per_second': 323.394, 'eval_steps_per_second': 20.374, 'epoch': 0.72}
{'loss': 0.8563, 'grad_norm': 1.5273231267929077, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9945493340492249, 'eval_runtime': 3.121, 'eval_samples_per_second': 320.406, 'eval_steps_per_second': 20.186, 'epoch': 0.76}
{'loss': 0.9304, 'grad_norm': 2.1959779262542725, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9972652196884155, 'eval_runtime': 3.1271, 'eval_samples_per_second': 319.782, 'eval_steps_per_second': 20.146, 'epoch': 0.8}
{'loss': 0.8423, 'grad_norm': 2.5871644020080566, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9935893416404724, 'eval_runtime': 3.1284, 'eval_samples_per_second': 319.648, 'eval_steps_per_second': 20.138, 'epoch': 0.84}
{'loss': 0.8708, 'grad_norm': 2.3411784172058105, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9946143627166748, 'eval_runtime': 3.1269, 'eval_samples_per_second': 319.801, 'eval_steps_per_second': 20.147, 'epoch': 0.88}
{'loss': 0.8725, 'grad_norm': 1.5963486433029175, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9921131730079651, 'eval_runtime': 3.1035, 'eval_samples_per_second': 322.217, 'eval_steps_per_second': 20.3, 'epoch': 0.92}
{'loss': 0.843, 'grad_norm': 1.9475008249282837, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9921483397483826, 'eval_runtime': 3.0987, 'eval_samples_per_second': 322.713, 'eval_steps_per_second': 20.331, 'epoch': 0.96}
{'loss': 0.92, 'grad_norm': 1.8026785850524902, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9916144609451294, 'eval_runtime': 3.1031, 'eval_samples_per_second': 322.256, 'eval_steps_per_second': 20.302, 'epoch': 1.0}
{'train_runtime': 228.1441, 'train_samples_per_second': 43.819, 'train_steps_per_second': 2.739, 'train_loss': 1.0120623626708984, 'epoch': 1.0}
train_results:  {'eval_loss': [1.532882809638977, 1.0904475450515747, 1.0358818769454956, 1.0367977619171143, 1.0181695222854614, 1.0142900943756104, 1.015260934829712, 1.0084683895111084, 1.0186476707458496, 1.009446144104004, 1.0137419700622559, 1.0031466484069824, 1.0002031326293945, 1.0036057233810425, 0.9985547065734863, 1.0023880004882812, 1.0024402141571045, 0.9993314743041992, 0.9945493340492249, 0.9972652196884155, 0.9935893416404724, 0.9946143627166748, 0.9921131730079651, 0.9921483397483826, 0.9916144609451294], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.532882809638977, 1.0904475450515747, 1.0358818769454956, 1.0367977619171143, 1.0181695222854614, 1.0142900943756104, 1.015260934829712, 1.0084683895111084, 1.0186476707458496, 1.009446144104004, 1.0137419700622559, 1.0031466484069824, 1.0002031326293945, 1.0036057233810425, 0.9985547065734863, 1.0023880004882812, 1.0024402141571045, 0.9993314743041992, 0.9945493340492249, 0.9972652196884155, 0.9935893416404724, 0.9946143627166748, 0.9921131730079651, 0.9921483397483826, 0.9916144609451294]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.2381882667541504
current iteration best possible eval_loss (full train run):  -0.9916144609451294
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9967 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.4032430977416286
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.26453031579389985
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.40893022129687195
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.382033492948191
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.4054494698469904
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2131, dtype=torch.float64), 0, 0, tensor(0.3856, dtype=torch.float64), tensor(0.2720, dtype=torch.float64), 0, tensor(0.0734, dtype=torch.float64), tensor(0.0559, dtype=torch.float64), 15, 0, 1, 0, 0, 1, 5, 0.05613173269979176, 1.480000019073488, 1]
normalized proposed parameters for next round by BO: [tensor(6.9063e-17, dtype=torch.float64), tensor(0.2131, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3856, dtype=torch.float64), tensor(0.2720, dtype=torch.float64), tensor(1.1329e-17, dtype=torch.float64), tensor(0.0734, dtype=torch.float64), tensor(0.0559, dtype=torch.float64), tensor(0.4660, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0406, dtype=torch.float64), tensor(0.5613, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.213
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.386
  truthfulqa_gen: 0.272
  wikitext: 0
  mmlu: 0.073
  arc_challenge: 0.056

LoRA Parameters:
  lora_r: (5,)
  lora_dropout: (0.05613173269979176,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (1.480000019073488,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  5
lora dropout:  0.05613173269979176
lora alpha:  1.480000019073488
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,766,400 || all params: 8,032,027,648 || trainable%: 0.0220
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.0086, 'grad_norm': 1.2629103660583496, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 4.475337028503418, 'eval_runtime': 4.9692, 'eval_samples_per_second': 201.238, 'eval_steps_per_second': 12.678, 'epoch': 0.04}
{'loss': 2.5661, 'grad_norm': 0.7149932384490967, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.306586503982544, 'eval_runtime': 2.9924, 'eval_samples_per_second': 334.181, 'eval_steps_per_second': 21.053, 'epoch': 0.08}
{'loss': 1.8087, 'grad_norm': 0.47326311469078064, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.016012668609619, 'eval_runtime': 3.0037, 'eval_samples_per_second': 332.928, 'eval_steps_per_second': 20.974, 'epoch': 0.12}
{'loss': 1.5996, 'grad_norm': 0.49759331345558167, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8232176303863525, 'eval_runtime': 3.0108, 'eval_samples_per_second': 332.135, 'eval_steps_per_second': 20.925, 'epoch': 0.16}
{'loss': 1.4438, 'grad_norm': 0.5298592448234558, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7936294078826904, 'eval_runtime': 2.994, 'eval_samples_per_second': 334.005, 'eval_steps_per_second': 21.042, 'epoch': 0.2}
{'loss': 1.3904, 'grad_norm': 0.5315103530883789, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.732254147529602, 'eval_runtime': 2.9977, 'eval_samples_per_second': 333.586, 'eval_steps_per_second': 21.016, 'epoch': 0.24}
{'loss': 1.3653, 'grad_norm': 0.4114791750907898, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6426466703414917, 'eval_runtime': 3.0072, 'eval_samples_per_second': 332.53, 'eval_steps_per_second': 20.949, 'epoch': 0.28}
{'loss': 1.31, 'grad_norm': 0.4936642050743103, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5499472618103027, 'eval_runtime': 3.0131, 'eval_samples_per_second': 331.879, 'eval_steps_per_second': 20.908, 'epoch': 0.32}
{'loss': 1.2358, 'grad_norm': 0.3797931969165802, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5220352411270142, 'eval_runtime': 3.0064, 'eval_samples_per_second': 332.62, 'eval_steps_per_second': 20.955, 'epoch': 0.36}
{'loss': 1.2487, 'grad_norm': 0.3243020176887512, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.486698031425476, 'eval_runtime': 3.0103, 'eval_samples_per_second': 332.197, 'eval_steps_per_second': 20.928, 'epoch': 0.4}
{'loss': 1.2146, 'grad_norm': 0.4830514192581177, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4693379402160645, 'eval_runtime': 3.0066, 'eval_samples_per_second': 332.599, 'eval_steps_per_second': 20.954, 'epoch': 0.44}
{'loss': 1.162, 'grad_norm': 0.3226843774318695, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4331486225128174, 'eval_runtime': 3.0082, 'eval_samples_per_second': 332.428, 'eval_steps_per_second': 20.943, 'epoch': 0.48}
{'loss': 1.1614, 'grad_norm': 0.3301716148853302, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.402787685394287, 'eval_runtime': 3.022, 'eval_samples_per_second': 330.911, 'eval_steps_per_second': 20.847, 'epoch': 0.52}
{'loss': 1.125, 'grad_norm': 0.42129388451576233, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.378493070602417, 'eval_runtime': 3.0188, 'eval_samples_per_second': 331.255, 'eval_steps_per_second': 20.869, 'epoch': 0.56}
{'loss': 1.1471, 'grad_norm': 0.424548864364624, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3660848140716553, 'eval_runtime': 3.0191, 'eval_samples_per_second': 331.225, 'eval_steps_per_second': 20.867, 'epoch': 0.6}
{'loss': 1.1247, 'grad_norm': 0.5337219834327698, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3591117858886719, 'eval_runtime': 3.0208, 'eval_samples_per_second': 331.034, 'eval_steps_per_second': 20.855, 'epoch': 0.64}
{'loss': 1.1326, 'grad_norm': 0.3869473934173584, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.349077582359314, 'eval_runtime': 3.0213, 'eval_samples_per_second': 330.981, 'eval_steps_per_second': 20.852, 'epoch': 0.68}
{'loss': 1.1092, 'grad_norm': 0.47055554389953613, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3418694734573364, 'eval_runtime': 3.018, 'eval_samples_per_second': 331.345, 'eval_steps_per_second': 20.875, 'epoch': 0.72}
{'loss': 1.1283, 'grad_norm': 0.465474396944046, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3263424634933472, 'eval_runtime': 3.0207, 'eval_samples_per_second': 331.053, 'eval_steps_per_second': 20.856, 'epoch': 0.76}
{'loss': 1.1096, 'grad_norm': 0.3584817349910736, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3196372985839844, 'eval_runtime': 3.0382, 'eval_samples_per_second': 329.142, 'eval_steps_per_second': 20.736, 'epoch': 0.8}
{'loss': 1.1098, 'grad_norm': 0.46019434928894043, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3139351606369019, 'eval_runtime': 3.0456, 'eval_samples_per_second': 328.346, 'eval_steps_per_second': 20.686, 'epoch': 0.84}
{'loss': 1.07, 'grad_norm': 0.40002331137657166, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3117283582687378, 'eval_runtime': 3.0172, 'eval_samples_per_second': 331.432, 'eval_steps_per_second': 20.88, 'epoch': 0.88}
{'loss': 1.0754, 'grad_norm': 0.31952840089797974, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3005766868591309, 'eval_runtime': 3.0204, 'eval_samples_per_second': 331.087, 'eval_steps_per_second': 20.858, 'epoch': 0.92}
{'loss': 1.0991, 'grad_norm': 0.5927207469940186, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2988361120224, 'eval_runtime': 3.0231, 'eval_samples_per_second': 330.781, 'eval_steps_per_second': 20.839, 'epoch': 0.96}
{'loss': 1.0828, 'grad_norm': 0.4377773106098175, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2968602180480957, 'eval_runtime': 3.0179, 'eval_samples_per_second': 331.36, 'eval_steps_per_second': 20.876, 'epoch': 1.0}
{'train_runtime': 265.6323, 'train_samples_per_second': 37.635, 'train_steps_per_second': 2.353, 'train_loss': 1.393148434448242, 'epoch': 1.0}
train_results:  {'eval_loss': [4.475337028503418, 2.306586503982544, 2.016012668609619, 1.8232176303863525, 1.7936294078826904, 1.732254147529602, 1.6426466703414917, 1.5499472618103027, 1.5220352411270142, 1.486698031425476, 1.4693379402160645, 1.4331486225128174, 1.402787685394287, 1.378493070602417, 1.3660848140716553, 1.3591117858886719, 1.349077582359314, 1.3418694734573364, 1.3263424634933472, 1.3196372985839844, 1.3139351606369019, 1.3117283582687378, 1.3005766868591309, 1.2988361120224, 1.2968602180480957], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [4.475337028503418, 2.306586503982544, 2.016012668609619, 1.8232176303863525, 1.7936294078826904, 1.732254147529602, 1.6426466703414917, 1.5499472618103027, 1.5220352411270142, 1.486698031425476, 1.4693379402160645, 1.4331486225128174, 1.402787685394287, 1.378493070602417, 1.3660848140716553, 1.3591117858886719, 1.349077582359314, 1.3418694734573364, 1.3263424634933472, 1.3196372985839844, 1.3139351606369019, 1.3117283582687378, 1.3005766868591309, 1.2988361120224, 1.2968602180480957]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.936268925666809
current iteration best possible eval_loss (full train run):  -1.2968602180480957
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.5386 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.6466215135646771
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.24811173363316108
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.6954386030696846
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.7164414409577822
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.6237011835121491
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2926, dtype=torch.float64), tensor(0.5620, dtype=torch.float64), 0, 0, 0, tensor(0.1089, dtype=torch.float64), 0, 0, tensor(0.0365, dtype=torch.float64), 32, 1, 1, 0, 1, 0, 128, 0.1, 34.33197864879717, 1]
normalized proposed parameters for next round by BO: [tensor(0.2926, dtype=torch.float64), tensor(0.5620, dtype=torch.float64), tensor(2.9653e-17, dtype=torch.float64), tensor(2.1964e-17, dtype=torch.float64), tensor(1.0380e-16, dtype=torch.float64), tensor(0.1089, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1675e-16, dtype=torch.float64), tensor(0.0365, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7152, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.293
  gsm8k: 0.562
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.109
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.037

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (34.33197864879717,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  34.33197864879717
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.3978, 'grad_norm': 0.403187096118927, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1365976333618164, 'eval_runtime': 3.2985, 'eval_samples_per_second': 303.172, 'eval_steps_per_second': 19.1, 'epoch': 0.04}
{'loss': 1.0581, 'grad_norm': 0.3719448745250702, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9341291189193726, 'eval_runtime': 3.2805, 'eval_samples_per_second': 304.835, 'eval_steps_per_second': 19.205, 'epoch': 0.08}
{'loss': 0.9006, 'grad_norm': 0.18104247748851776, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6707278490066528, 'eval_runtime': 3.2894, 'eval_samples_per_second': 304.003, 'eval_steps_per_second': 19.152, 'epoch': 0.12}
{'loss': 0.8841, 'grad_norm': 0.16840189695358276, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8371821641921997, 'eval_runtime': 3.2888, 'eval_samples_per_second': 304.065, 'eval_steps_per_second': 19.156, 'epoch': 0.16}
{'loss': 0.8744, 'grad_norm': 0.14609287679195404, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7798097133636475, 'eval_runtime': 3.2981, 'eval_samples_per_second': 303.205, 'eval_steps_per_second': 19.102, 'epoch': 0.2}
{'loss': 0.8571, 'grad_norm': 0.16811898350715637, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7755258083343506, 'eval_runtime': 3.304, 'eval_samples_per_second': 302.661, 'eval_steps_per_second': 19.068, 'epoch': 0.24}
{'loss': 0.856, 'grad_norm': 0.16613416373729706, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8053138256072998, 'eval_runtime': 3.3095, 'eval_samples_per_second': 302.157, 'eval_steps_per_second': 19.036, 'epoch': 0.28}
{'loss': 0.8472, 'grad_norm': 0.23373067378997803, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8337318897247314, 'eval_runtime': 3.3587, 'eval_samples_per_second': 297.735, 'eval_steps_per_second': 18.757, 'epoch': 0.32}
{'loss': 0.844, 'grad_norm': 0.15282434225082397, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.876861333847046, 'eval_runtime': 3.3438, 'eval_samples_per_second': 299.062, 'eval_steps_per_second': 18.841, 'epoch': 0.36}
{'loss': 0.8357, 'grad_norm': 0.17939044535160065, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9061778783798218, 'eval_runtime': 3.3321, 'eval_samples_per_second': 300.107, 'eval_steps_per_second': 18.907, 'epoch': 0.4}
{'loss': 0.8242, 'grad_norm': 0.16313804686069489, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.931867241859436, 'eval_runtime': 3.3313, 'eval_samples_per_second': 300.18, 'eval_steps_per_second': 18.911, 'epoch': 0.44}
{'loss': 0.8203, 'grad_norm': 0.1703159511089325, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8127624988555908, 'eval_runtime': 3.3332, 'eval_samples_per_second': 300.015, 'eval_steps_per_second': 18.901, 'epoch': 0.48}
{'loss': 0.8285, 'grad_norm': 0.1793571263551712, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9432554244995117, 'eval_runtime': 3.342, 'eval_samples_per_second': 299.225, 'eval_steps_per_second': 18.851, 'epoch': 0.52}
{'loss': 0.8238, 'grad_norm': 0.15375898778438568, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9367321729660034, 'eval_runtime': 3.3266, 'eval_samples_per_second': 300.604, 'eval_steps_per_second': 18.938, 'epoch': 0.56}
{'loss': 0.8225, 'grad_norm': 0.167830228805542, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8783596754074097, 'eval_runtime': 3.335, 'eval_samples_per_second': 299.851, 'eval_steps_per_second': 18.891, 'epoch': 0.6}
{'loss': 0.794, 'grad_norm': 0.16165609657764435, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8591628074645996, 'eval_runtime': 3.3745, 'eval_samples_per_second': 296.339, 'eval_steps_per_second': 18.669, 'epoch': 0.64}
{'loss': 0.7956, 'grad_norm': 0.1933281272649765, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8466013669967651, 'eval_runtime': 3.3815, 'eval_samples_per_second': 295.728, 'eval_steps_per_second': 18.631, 'epoch': 0.68}
{'loss': 0.8099, 'grad_norm': 0.15649721026420593, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8810431957244873, 'eval_runtime': 3.345, 'eval_samples_per_second': 298.954, 'eval_steps_per_second': 18.834, 'epoch': 0.72}
{'loss': 0.8067, 'grad_norm': 0.16360680758953094, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8904389142990112, 'eval_runtime': 3.332, 'eval_samples_per_second': 300.119, 'eval_steps_per_second': 18.908, 'epoch': 0.76}
{'loss': 0.799, 'grad_norm': 0.16209866106510162, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8995134830474854, 'eval_runtime': 3.3359, 'eval_samples_per_second': 299.766, 'eval_steps_per_second': 18.885, 'epoch': 0.8}
{'loss': 0.8013, 'grad_norm': 0.1836300790309906, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8814762830734253, 'eval_runtime': 3.3162, 'eval_samples_per_second': 301.548, 'eval_steps_per_second': 18.998, 'epoch': 0.84}
{'loss': 0.7976, 'grad_norm': 0.16368818283081055, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9181112051010132, 'eval_runtime': 3.3107, 'eval_samples_per_second': 302.047, 'eval_steps_per_second': 19.029, 'epoch': 0.88}
{'loss': 0.8107, 'grad_norm': 0.15399067103862762, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.960402250289917, 'eval_runtime': 3.2988, 'eval_samples_per_second': 303.141, 'eval_steps_per_second': 19.098, 'epoch': 0.92}
{'loss': 0.7777, 'grad_norm': 0.17938511073589325, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9619554281234741, 'eval_runtime': 3.2932, 'eval_samples_per_second': 303.659, 'eval_steps_per_second': 19.13, 'epoch': 0.96}
{'loss': 0.8004, 'grad_norm': 0.1645529717206955, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9560856819152832, 'eval_runtime': 3.2983, 'eval_samples_per_second': 303.188, 'eval_steps_per_second': 19.101, 'epoch': 1.0}
{'train_runtime': 317.0503, 'train_samples_per_second': 31.538, 'train_steps_per_second': 1.971, 'train_loss': 0.898688851928711, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1365976333618164, 1.9341291189193726, 1.6707278490066528, 1.8371821641921997, 1.7798097133636475, 1.7755258083343506, 1.8053138256072998, 1.8337318897247314, 1.876861333847046, 1.9061778783798218, 1.931867241859436, 1.8127624988555908, 1.9432554244995117, 1.9367321729660034, 1.8783596754074097, 1.8591628074645996, 1.8466013669967651, 1.8810431957244873, 1.8904389142990112, 1.8995134830474854, 1.8814762830734253, 1.9181112051010132, 1.960402250289917, 1.9619554281234741, 1.9560856819152832], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1365976333618164, 1.9341291189193726, 1.6707278490066528, 1.8371821641921997, 1.7798097133636475, 1.7755258083343506, 1.8053138256072998, 1.8337318897247314, 1.876861333847046, 1.9061778783798218, 1.931867241859436, 1.8127624988555908, 1.9432554244995117, 1.9367321729660034, 1.8783596754074097, 1.8591628074645996, 1.8466013669967651, 1.8810431957244873, 1.8904389142990112, 1.8995134830474854, 1.8814762830734253, 1.9181112051010132, 1.960402250289917, 1.9619554281234741, 1.9560856819152832]
current iteration observed (possibly low-fid or predicted) eval_loss:  -4.582779407501221
current iteration best possible eval_loss (full train run):  -1.9560856819152832
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3698 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.18174402031296522
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.4606673338072822
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.3476079381760431
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.4856207172445952
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.4560424574673254
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0921, dtype=torch.float64), 0, 0, tensor(0.1369, dtype=torch.float64), tensor(0.1754, dtype=torch.float64), 0, tensor(0.1855, dtype=torch.float64), tensor(0.4101, dtype=torch.float64), 25, 0, 1, 0, 0, 1, 48, 0.05124491130777471, 40.90124734787581, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0921, dtype=torch.float64), tensor(2.7498e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1369, dtype=torch.float64), tensor(0.1754, dtype=torch.float64), tensor(1.4131e-17, dtype=torch.float64), tensor(0.1855, dtype=torch.float64), tensor(0.4101, dtype=torch.float64), tensor(0.7964, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3749, dtype=torch.float64), tensor(0.5124, dtype=torch.float64), tensor(0.8521, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.092
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.137
  truthfulqa_gen: 0.175
  wikitext: 0
  mmlu: 0.186
  arc_challenge: 0.41

LoRA Parameters:
  lora_r: (48,)
  lora_dropout: (0.05124491130777471,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (40.90124734787581,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  48
lora dropout:  0.05124491130777471
lora alpha:  40.90124734787581
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 28,262,400 || all params: 8,058,523,648 || trainable%: 0.3507
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8536, 'grad_norm': 0.8215934038162231, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.150118589401245, 'eval_runtime': 3.0553, 'eval_samples_per_second': 327.301, 'eval_steps_per_second': 20.62, 'epoch': 0.04}
{'loss': 1.3102, 'grad_norm': 0.37939536571502686, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5489975214004517, 'eval_runtime': 3.0464, 'eval_samples_per_second': 328.254, 'eval_steps_per_second': 20.68, 'epoch': 0.08}
{'loss': 1.1474, 'grad_norm': 0.4513968229293823, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3871630430221558, 'eval_runtime': 3.0317, 'eval_samples_per_second': 329.852, 'eval_steps_per_second': 20.781, 'epoch': 0.12}
{'loss': 1.0551, 'grad_norm': 0.40599268674850464, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3227415084838867, 'eval_runtime': 3.0377, 'eval_samples_per_second': 329.191, 'eval_steps_per_second': 20.739, 'epoch': 0.16}
{'loss': 1.0222, 'grad_norm': 0.3653051257133484, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2747589349746704, 'eval_runtime': 3.0424, 'eval_samples_per_second': 328.685, 'eval_steps_per_second': 20.707, 'epoch': 0.2}
{'loss': 1.0051, 'grad_norm': 0.3897407352924347, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2685123682022095, 'eval_runtime': 3.0559, 'eval_samples_per_second': 327.241, 'eval_steps_per_second': 20.616, 'epoch': 0.24}
{'loss': 0.9944, 'grad_norm': 0.3150807023048401, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2588205337524414, 'eval_runtime': 3.052, 'eval_samples_per_second': 327.65, 'eval_steps_per_second': 20.642, 'epoch': 0.28}
{'loss': 1.0026, 'grad_norm': 0.39902809262275696, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2230240106582642, 'eval_runtime': 3.0451, 'eval_samples_per_second': 328.395, 'eval_steps_per_second': 20.689, 'epoch': 0.32}
{'loss': 0.976, 'grad_norm': 0.5021737813949585, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2179049253463745, 'eval_runtime': 3.0478, 'eval_samples_per_second': 328.105, 'eval_steps_per_second': 20.671, 'epoch': 0.36}
{'loss': 0.9403, 'grad_norm': 0.4736041724681854, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.169605016708374, 'eval_runtime': 3.0473, 'eval_samples_per_second': 328.156, 'eval_steps_per_second': 20.674, 'epoch': 0.4}
{'loss': 0.8888, 'grad_norm': 0.5277610421180725, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.100130319595337, 'eval_runtime': 3.0579, 'eval_samples_per_second': 327.023, 'eval_steps_per_second': 20.602, 'epoch': 0.44}
{'loss': 0.897, 'grad_norm': 0.47067904472351074, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0748759508132935, 'eval_runtime': 3.0622, 'eval_samples_per_second': 326.567, 'eval_steps_per_second': 20.574, 'epoch': 0.48}
{'loss': 0.8184, 'grad_norm': 0.5203531384468079, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.059549331665039, 'eval_runtime': 3.0594, 'eval_samples_per_second': 326.863, 'eval_steps_per_second': 20.592, 'epoch': 0.52}
{'loss': 0.7804, 'grad_norm': 0.4387277364730835, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0657460689544678, 'eval_runtime': 3.0538, 'eval_samples_per_second': 327.465, 'eval_steps_per_second': 20.63, 'epoch': 0.56}
{'loss': 0.8237, 'grad_norm': 0.5552963614463806, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.069286584854126, 'eval_runtime': 3.0561, 'eval_samples_per_second': 327.218, 'eval_steps_per_second': 20.615, 'epoch': 0.6}
{'loss': 0.8204, 'grad_norm': 0.4945674538612366, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.067266583442688, 'eval_runtime': 3.058, 'eval_samples_per_second': 327.011, 'eval_steps_per_second': 20.602, 'epoch': 0.64}
{'loss': 0.8003, 'grad_norm': 0.4449388086795807, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0622705221176147, 'eval_runtime': 3.0543, 'eval_samples_per_second': 327.402, 'eval_steps_per_second': 20.626, 'epoch': 0.68}
{'loss': 0.7788, 'grad_norm': 0.48213955760002136, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0616869926452637, 'eval_runtime': 3.0604, 'eval_samples_per_second': 326.754, 'eval_steps_per_second': 20.585, 'epoch': 0.72}
{'loss': 0.845, 'grad_norm': 0.45736628770828247, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0534358024597168, 'eval_runtime': 3.0639, 'eval_samples_per_second': 326.378, 'eval_steps_per_second': 20.562, 'epoch': 0.76}
{'loss': 0.749, 'grad_norm': 0.5423191785812378, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0542609691619873, 'eval_runtime': 3.0494, 'eval_samples_per_second': 327.934, 'eval_steps_per_second': 20.66, 'epoch': 0.8}
{'loss': 0.7566, 'grad_norm': 0.5459607243537903, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0507299900054932, 'eval_runtime': 3.0767, 'eval_samples_per_second': 325.026, 'eval_steps_per_second': 20.477, 'epoch': 0.84}
{'loss': 0.7361, 'grad_norm': 0.582273006439209, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.050495982170105, 'eval_runtime': 3.0526, 'eval_samples_per_second': 327.587, 'eval_steps_per_second': 20.638, 'epoch': 0.88}
{'loss': 0.7349, 'grad_norm': 0.6721652746200562, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.051059603691101, 'eval_runtime': 3.0449, 'eval_samples_per_second': 328.416, 'eval_steps_per_second': 20.69, 'epoch': 0.92}
{'loss': 0.7493, 'grad_norm': 0.5466068983078003, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0505203008651733, 'eval_runtime': 3.055, 'eval_samples_per_second': 327.329, 'eval_steps_per_second': 20.622, 'epoch': 0.96}
{'loss': 0.7175, 'grad_norm': 0.5098089575767517, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0501009225845337, 'eval_runtime': 3.0544, 'eval_samples_per_second': 327.4, 'eval_steps_per_second': 20.626, 'epoch': 1.0}
{'train_runtime': 261.9921, 'train_samples_per_second': 38.161, 'train_steps_per_second': 2.386, 'train_loss': 0.9681209259033203, 'epoch': 1.0}
train_results:  {'eval_loss': [2.150118589401245, 1.5489975214004517, 1.3871630430221558, 1.3227415084838867, 1.2747589349746704, 1.2685123682022095, 1.2588205337524414, 1.2230240106582642, 1.2179049253463745, 1.169605016708374, 1.100130319595337, 1.0748759508132935, 1.059549331665039, 1.0657460689544678, 1.069286584854126, 1.067266583442688, 1.0622705221176147, 1.0616869926452637, 1.0534358024597168, 1.0542609691619873, 1.0507299900054932, 1.050495982170105, 1.051059603691101, 1.0505203008651733, 1.0501009225845337], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.150118589401245, 1.5489975214004517, 1.3871630430221558, 1.3227415084838867, 1.2747589349746704, 1.2685123682022095, 1.2588205337524414, 1.2230240106582642, 1.2179049253463745, 1.169605016708374, 1.100130319595337, 1.0748759508132935, 1.059549331665039, 1.0657460689544678, 1.069286584854126, 1.067266583442688, 1.0622705221176147, 1.0616869926452637, 1.0534358024597168, 1.0542609691619873, 1.0507299900054932, 1.050495982170105, 1.051059603691101, 1.0505203008651733, 1.0501009225845337]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.859452486038208
current iteration best possible eval_loss (full train run):  -1.0501009225845337
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.1146 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.8354280030200989
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.9872904900479615
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.9382257064752697
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.9327028153913599
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.9282126057202158
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2618, dtype=torch.float64), 0, tensor(0.1779, dtype=torch.float64), tensor(0.2861, dtype=torch.float64), 0, 0, tensor(0.0794, dtype=torch.float64), tensor(0.1949, dtype=torch.float64), 1, 0, 0, 0, 1, 1, 87, 0.04459271043945777, 2.981637095364092, 0]
normalized proposed parameters for next round by BO: [tensor(9.0318e-18, dtype=torch.float64), tensor(0.2618, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1779, dtype=torch.float64), tensor(0.2861, dtype=torch.float64), tensor(2.4807e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0794, dtype=torch.float64), tensor(0.1949, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6834, dtype=torch.float64), tensor(0.4459, dtype=torch.float64), tensor(0.0621, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.262
  rowan_hellaswag: 0
  sciq: 0.178
  triviaqa: 0.286
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.079
  arc_challenge: 0.195

LoRA Parameters:
  lora_r: (87,)
  lora_dropout: (0.04459271043945777,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (2.981637095364092,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  87
lora dropout:  0.04459271043945777
lora alpha:  2.981637095364092
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,207,168 || all params: 8,033,468,416 || trainable%: 0.0399
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.7618, 'grad_norm': 0.19827599823474884, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 5.335298538208008, 'eval_runtime': 3.6252, 'eval_samples_per_second': 275.845, 'eval_steps_per_second': 17.378, 'epoch': 0.04}
{'loss': 2.9797, 'grad_norm': 0.40596145391464233, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.6841652393341064, 'eval_runtime': 2.8921, 'eval_samples_per_second': 345.765, 'eval_steps_per_second': 21.783, 'epoch': 0.08}
{'loss': 1.9247, 'grad_norm': 0.15478549897670746, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9199764728546143, 'eval_runtime': 2.8845, 'eval_samples_per_second': 346.681, 'eval_steps_per_second': 21.841, 'epoch': 0.12}
{'loss': 1.5852, 'grad_norm': 0.13753513991832733, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6779999732971191, 'eval_runtime': 2.8948, 'eval_samples_per_second': 345.447, 'eval_steps_per_second': 21.763, 'epoch': 0.16}
{'loss': 1.4745, 'grad_norm': 0.16415317356586456, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5849848985671997, 'eval_runtime': 2.8875, 'eval_samples_per_second': 346.319, 'eval_steps_per_second': 21.818, 'epoch': 0.2}
{'loss': 1.3552, 'grad_norm': 0.10039190202951431, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5444226264953613, 'eval_runtime': 2.8858, 'eval_samples_per_second': 346.525, 'eval_steps_per_second': 21.831, 'epoch': 0.24}
{'loss': 1.2949, 'grad_norm': 0.08195585012435913, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5009996891021729, 'eval_runtime': 2.8903, 'eval_samples_per_second': 345.981, 'eval_steps_per_second': 21.797, 'epoch': 0.28}
{'loss': 1.2909, 'grad_norm': 0.08773572742938995, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.484191656112671, 'eval_runtime': 2.8939, 'eval_samples_per_second': 345.554, 'eval_steps_per_second': 21.77, 'epoch': 0.32}
{'loss': 1.2758, 'grad_norm': 0.09045174717903137, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.465728759765625, 'eval_runtime': 2.8762, 'eval_samples_per_second': 347.675, 'eval_steps_per_second': 21.904, 'epoch': 0.36}
{'loss': 1.2213, 'grad_norm': 0.10045600682497025, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4497137069702148, 'eval_runtime': 2.8714, 'eval_samples_per_second': 348.263, 'eval_steps_per_second': 21.941, 'epoch': 0.4}
{'loss': 1.231, 'grad_norm': 0.09800121188163757, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4322131872177124, 'eval_runtime': 2.8729, 'eval_samples_per_second': 348.082, 'eval_steps_per_second': 21.929, 'epoch': 0.44}
{'loss': 1.2251, 'grad_norm': 0.08312977850437164, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4220681190490723, 'eval_runtime': 2.8721, 'eval_samples_per_second': 348.181, 'eval_steps_per_second': 21.935, 'epoch': 0.48}
{'loss': 1.1764, 'grad_norm': 0.09158498793840408, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4222501516342163, 'eval_runtime': 2.8705, 'eval_samples_per_second': 348.368, 'eval_steps_per_second': 21.947, 'epoch': 0.52}
{'loss': 1.1974, 'grad_norm': 0.0951712355017662, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4253524541854858, 'eval_runtime': 2.8728, 'eval_samples_per_second': 348.087, 'eval_steps_per_second': 21.929, 'epoch': 0.56}
{'loss': 1.1951, 'grad_norm': 0.09566093236207962, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4087923765182495, 'eval_runtime': 2.8719, 'eval_samples_per_second': 348.199, 'eval_steps_per_second': 21.937, 'epoch': 0.6}
{'loss': 1.1918, 'grad_norm': 0.08985123783349991, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4032769203186035, 'eval_runtime': 2.8759, 'eval_samples_per_second': 347.721, 'eval_steps_per_second': 21.906, 'epoch': 0.64}
{'loss': 1.1606, 'grad_norm': 0.1300913244485855, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3965895175933838, 'eval_runtime': 2.8709, 'eval_samples_per_second': 348.321, 'eval_steps_per_second': 21.944, 'epoch': 0.68}
{'loss': 1.1571, 'grad_norm': 0.0976652279496193, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.389818549156189, 'eval_runtime': 2.8685, 'eval_samples_per_second': 348.615, 'eval_steps_per_second': 21.963, 'epoch': 0.72}
{'loss': 1.1696, 'grad_norm': 0.09084656089544296, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.396310806274414, 'eval_runtime': 2.8728, 'eval_samples_per_second': 348.088, 'eval_steps_per_second': 21.93, 'epoch': 0.76}
{'loss': 1.1659, 'grad_norm': 0.1420123130083084, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.394382357597351, 'eval_runtime': 2.8779, 'eval_samples_per_second': 347.47, 'eval_steps_per_second': 21.891, 'epoch': 0.8}
{'loss': 1.1538, 'grad_norm': 0.10940401256084442, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3851423263549805, 'eval_runtime': 2.8766, 'eval_samples_per_second': 347.627, 'eval_steps_per_second': 21.9, 'epoch': 0.84}
{'loss': 1.172, 'grad_norm': 0.13452886044979095, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3859376907348633, 'eval_runtime': 2.8736, 'eval_samples_per_second': 347.998, 'eval_steps_per_second': 21.924, 'epoch': 0.88}
{'loss': 1.1496, 'grad_norm': 0.10512970387935638, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3891977071762085, 'eval_runtime': 2.8733, 'eval_samples_per_second': 348.03, 'eval_steps_per_second': 21.926, 'epoch': 0.92}
{'loss': 1.13, 'grad_norm': 0.08690071851015091, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3841110467910767, 'eval_runtime': 2.8684, 'eval_samples_per_second': 348.627, 'eval_steps_per_second': 21.964, 'epoch': 0.96}
{'loss': 1.1804, 'grad_norm': 0.10547292977571487, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.384283185005188, 'eval_runtime': 2.8723, 'eval_samples_per_second': 348.148, 'eval_steps_per_second': 21.933, 'epoch': 1.0}
{'train_runtime': 165.941, 'train_samples_per_second': 60.244, 'train_steps_per_second': 3.766, 'train_loss': 1.4327881713867188, 'epoch': 1.0}
train_results:  {'eval_loss': [5.335298538208008, 2.6841652393341064, 1.9199764728546143, 1.6779999732971191, 1.5849848985671997, 1.5444226264953613, 1.5009996891021729, 1.484191656112671, 1.465728759765625, 1.4497137069702148, 1.4322131872177124, 1.4220681190490723, 1.4222501516342163, 1.4253524541854858, 1.4087923765182495, 1.4032769203186035, 1.3965895175933838, 1.389818549156189, 1.396310806274414, 1.394382357597351, 1.3851423263549805, 1.3859376907348633, 1.3891977071762085, 1.3841110467910767, 1.384283185005188], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [5.335298538208008, 2.6841652393341064, 1.9199764728546143, 1.6779999732971191, 1.5849848985671997, 1.5444226264953613, 1.5009996891021729, 1.484191656112671, 1.465728759765625, 1.4497137069702148, 1.4322131872177124, 1.4220681190490723, 1.4222501516342163, 1.4253524541854858, 1.4087923765182495, 1.4032769203186035, 1.3965895175933838, 1.389818549156189, 1.396310806274414, 1.394382357597351, 1.3851423263549805, 1.3859376907348633, 1.3891977071762085, 1.3841110467910767, 1.384283185005188]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.9843177795410156
current iteration best possible eval_loss (full train run):  -1.384283185005188
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.1836 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -1.2909535930332372
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -1.2931830323749756
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -1.2914796693842514
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -1.2913695858514354
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -1.2859304294069904
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1300, dtype=torch.float64), tensor(0.1576, dtype=torch.float64), 0, 0, tensor(0.3749, dtype=torch.float64), tensor(0.1693, dtype=torch.float64), 0, tensor(0.1203, dtype=torch.float64), tensor(0.0433, dtype=torch.float64), 32, 0, 0, 1, 1, 1, 2, 0.05413224901059108, 28.435128151584752, 1]
normalized proposed parameters for next round by BO: [tensor(0.1300, dtype=torch.float64), tensor(0.1576, dtype=torch.float64), tensor(1.2049e-17, dtype=torch.float64), tensor(0.0046, dtype=torch.float64), tensor(0.3749, dtype=torch.float64), tensor(0.1693, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1203, dtype=torch.float64), tensor(0.0433, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.5413, dtype=torch.float64), tensor(0.5924, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.13
  gsm8k: 0.158
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.375
  truthfulqa_gen: 0.169
  wikitext: 0
  mmlu: 0.12
  arc_challenge: 0.043

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.05413224901059108,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (28.435128151584752,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  2
lora dropout:  0.05413224901059108
lora alpha:  28.435128151584752
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,538,944 || all params: 8,033,800,192 || trainable%: 0.0441
length of training data:  9951
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7198, 'grad_norm': 3.3471248149871826, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5371310710906982, 'eval_runtime': 4.7101, 'eval_samples_per_second': 212.311, 'eval_steps_per_second': 13.376, 'epoch': 0.04}
{'loss': 1.1764, 'grad_norm': 2.2632055282592773, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0853872299194336, 'eval_runtime': 3.5696, 'eval_samples_per_second': 280.142, 'eval_steps_per_second': 17.649, 'epoch': 0.08}
{'loss': 0.9798, 'grad_norm': 1.5389765501022339, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.0470623970031738, 'eval_runtime': 3.5596, 'eval_samples_per_second': 280.931, 'eval_steps_per_second': 17.699, 'epoch': 0.12}
{'loss': 0.9996, 'grad_norm': 1.2980364561080933, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.0377825498580933, 'eval_runtime': 3.5616, 'eval_samples_per_second': 280.776, 'eval_steps_per_second': 17.689, 'epoch': 0.16}
{'loss': 0.9712, 'grad_norm': 1.7754780054092407, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.042501449584961, 'eval_runtime': 3.5696, 'eval_samples_per_second': 280.146, 'eval_steps_per_second': 17.649, 'epoch': 0.2}
{'loss': 0.9954, 'grad_norm': 1.5395503044128418, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.016404628753662, 'eval_runtime': 3.5543, 'eval_samples_per_second': 281.351, 'eval_steps_per_second': 17.725, 'epoch': 0.24}
{'loss': 0.9449, 'grad_norm': 1.0041817426681519, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 1.028994083404541, 'eval_runtime': 3.5538, 'eval_samples_per_second': 281.392, 'eval_steps_per_second': 17.728, 'epoch': 0.28}
{'loss': 0.9402, 'grad_norm': 1.0749077796936035, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 1.0219072103500366, 'eval_runtime': 3.5575, 'eval_samples_per_second': 281.097, 'eval_steps_per_second': 17.709, 'epoch': 0.32}
{'loss': 0.9719, 'grad_norm': 1.2607319355010986, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 1.0162931680679321, 'eval_runtime': 3.5742, 'eval_samples_per_second': 279.784, 'eval_steps_per_second': 17.626, 'epoch': 0.36}
{'loss': 0.9516, 'grad_norm': 1.0869131088256836, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 1.016052007675171, 'eval_runtime': 3.5702, 'eval_samples_per_second': 280.099, 'eval_steps_per_second': 17.646, 'epoch': 0.4}
{'loss': 0.9284, 'grad_norm': 1.1865004301071167, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 1.0158921480178833, 'eval_runtime': 3.5733, 'eval_samples_per_second': 279.857, 'eval_steps_per_second': 17.631, 'epoch': 0.44}
{'loss': 0.9411, 'grad_norm': 1.0690630674362183, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 1.0109206438064575, 'eval_runtime': 3.5719, 'eval_samples_per_second': 279.961, 'eval_steps_per_second': 17.638, 'epoch': 0.48}
{'loss': 0.9296, 'grad_norm': 1.1328407526016235, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 1.0090203285217285, 'eval_runtime': 3.5688, 'eval_samples_per_second': 280.207, 'eval_steps_per_second': 17.653, 'epoch': 0.52}
{'loss': 0.9399, 'grad_norm': 1.5719012022018433, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 1.0069383382797241, 'eval_runtime': 3.5873, 'eval_samples_per_second': 278.759, 'eval_steps_per_second': 17.562, 'epoch': 0.56}
{'loss': 0.9526, 'grad_norm': 1.1497423648834229, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 1.008779764175415, 'eval_runtime': 3.5747, 'eval_samples_per_second': 279.747, 'eval_steps_per_second': 17.624, 'epoch': 0.6}
{'loss': 0.9009, 'grad_norm': 1.5883877277374268, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 1.0009737014770508, 'eval_runtime': 3.5675, 'eval_samples_per_second': 280.308, 'eval_steps_per_second': 17.659, 'epoch': 0.64}
{'loss': 0.9245, 'grad_norm': 0.9499706625938416, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 1.0068376064300537, 'eval_runtime': 3.5646, 'eval_samples_per_second': 280.539, 'eval_steps_per_second': 17.674, 'epoch': 0.68}
{'loss': 0.8657, 'grad_norm': 1.1733717918395996, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 1.004105806350708, 'eval_runtime': 3.5796, 'eval_samples_per_second': 279.365, 'eval_steps_per_second': 17.6, 'epoch': 0.72}
{'loss': 0.904, 'grad_norm': 1.2578723430633545, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 1.0013134479522705, 'eval_runtime': 3.5917, 'eval_samples_per_second': 278.417, 'eval_steps_per_second': 17.54, 'epoch': 0.76}
{'loss': 0.936, 'grad_norm': 1.2323969602584839, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.999312162399292, 'eval_runtime': 3.5703, 'eval_samples_per_second': 280.089, 'eval_steps_per_second': 17.646, 'epoch': 0.8}
{'loss': 0.848, 'grad_norm': 1.2653474807739258, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 1.0008455514907837, 'eval_runtime': 3.5828, 'eval_samples_per_second': 279.112, 'eval_steps_per_second': 17.584, 'epoch': 0.84}
{'loss': 0.8888, 'grad_norm': 1.4014729261398315, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.9974380135536194, 'eval_runtime': 3.574, 'eval_samples_per_second': 279.8, 'eval_steps_per_second': 17.627, 'epoch': 0.88}
{'loss': 0.8995, 'grad_norm': 1.1327756643295288, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.9954048991203308, 'eval_runtime': 3.5715, 'eval_samples_per_second': 279.996, 'eval_steps_per_second': 17.64, 'epoch': 0.92}
{'loss': 0.87, 'grad_norm': 1.2808443307876587, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.9961082935333252, 'eval_runtime': 3.6176, 'eval_samples_per_second': 276.425, 'eval_steps_per_second': 17.415, 'epoch': 0.96}
{'train_runtime': 307.9123, 'train_samples_per_second': 32.318, 'train_steps_per_second': 2.02, 'train_loss': 1.0110254901015108, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5371310710906982, 1.0853872299194336, 1.0470623970031738, 1.0377825498580933, 1.042501449584961, 1.016404628753662, 1.028994083404541, 1.0219072103500366, 1.0162931680679321, 1.016052007675171, 1.0158921480178833, 1.0109206438064575, 1.0090203285217285, 1.0069383382797241, 1.008779764175415, 1.0009737014770508, 1.0068376064300537, 1.004105806350708, 1.0013134479522705, 0.999312162399292, 1.0008455514907837, 0.9974380135536194, 0.9954048991203308, 0.9961082935333252], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.5371310710906982, 1.0853872299194336, 1.0470623970031738, 1.0377825498580933, 1.042501449584961, 1.016404628753662, 1.028994083404541, 1.0219072103500366, 1.0162931680679321, 1.016052007675171, 1.0158921480178833, 1.0109206438064575, 1.0090203285217285, 1.0069383382797241, 1.008779764175415, 1.0009737014770508, 1.0068376064300537, 1.004105806350708, 1.0013134479522705, 0.999312162399292, 1.0008455514907837, 0.9974380135536194, 0.9954048991203308, 0.9961082935333252]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.1346352100372314
current iteration best possible eval_loss (full train run):  -0.9961082935333252
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.3209 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1795889139175415, 0.5639668703079224, 0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.6092051267623901, 0.8282220363616943, 0.8343875408172607, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.22647850215435028, 0.034817516803741455, 0.2369377166032791, 0.8301550149917603]  ‚Üí  acq = -1.0119488044267764
X = [0.3627225160598755, 0.4275026321411133, 0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.20957553386688232, 0.29544466733932495, 0.45958811044692993, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7996509075164795, 0.16252559423446655, 0.23583632707595825, 0.46233898401260376]  ‚Üí  acq = -1.1199386671274962
X = [0.3497363328933716, 0.9076562523841858, 0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.451727032661438, 0.6221595406532288, 0.021270394325256348, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.990592360496521, 0.6705264449119568, 0.5292245149612427, 0.4145408272743225]  ‚Üí  acq = -1.1114944440667271
X = [0.8545095920562744, 0.7290428876876831, 0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.1896294802427292, 0.13708847761154175, 0.2295888066291809, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.42133286595344543, 0.9889391660690308, 0.27955883741378784, 0.647262454032898]  ‚Üí  acq = -1.114806267083461
X = [0.25802141427993774, 0.15090978145599365, 0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.16085723042488098, 0.13615381717681885, 0.3176853060722351, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7902160882949829, 0.012897372245788574, 0.5332701206207275, 0.154333233833313]  ‚Üí  acq = -1.0687023724927296
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2383, dtype=torch.float64), 0, tensor(0.1444, dtype=torch.float64), tensor(0.2368, dtype=torch.float64), tensor(0.0125, dtype=torch.float64), 0, tensor(0.2339, dtype=torch.float64), tensor(0.0621, dtype=torch.float64), tensor(0.0648, dtype=torch.float64), 13, 0, 0, 1, 0, 1, 2, 0.03233549602939531, 25.304959352633944, 1]
normalized proposed parameters for next round by BO: [tensor(0.2383, dtype=torch.float64), tensor(0.0070, dtype=torch.float64), tensor(0.1444, dtype=torch.float64), tensor(0.2368, dtype=torch.float64), tensor(0.0125, dtype=torch.float64), tensor(2.1486e-17, dtype=torch.float64), tensor(0.2339, dtype=torch.float64), tensor(0.0621, dtype=torch.float64), tensor(0.0648, dtype=torch.float64), tensor(0.3994, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.3234, dtype=torch.float64), tensor(0.5272, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.238
  gsm8k: 0
  rowan_hellaswag: 0.144
  sciq: 0.237
  triviaqa: 0.013
  truthfulqa_gen: 0
  wikitext: 0.234
  mmlu: 0.062
  arc_challenge: 0.065

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.03233549602939531,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (25.304959352633944,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.03233549602939531
lora alpha:  25.304959352633944
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 958,464 || all params: 8,031,219,712 || trainable%: 0.0119
length of training data:  9927
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6474, 'grad_norm': 3.1132683753967285, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4587440490722656, 'eval_runtime': 3.7459, 'eval_samples_per_second': 266.957, 'eval_steps_per_second': 16.818, 'epoch': 0.04}
{'loss': 2.1563, 'grad_norm': 2.197004795074463, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.0616140365600586, 'eval_runtime': 3.0363, 'eval_samples_per_second': 329.348, 'eval_steps_per_second': 20.749, 'epoch': 0.08}
{'loss': 1.7785, 'grad_norm': 1.629960536956787, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.7963383197784424, 'eval_runtime': 3.0114, 'eval_samples_per_second': 332.075, 'eval_steps_per_second': 20.921, 'epoch': 0.12}
{'loss': 1.6012, 'grad_norm': 1.4061239957809448, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.6297343969345093, 'eval_runtime': 3.0276, 'eval_samples_per_second': 330.292, 'eval_steps_per_second': 20.808, 'epoch': 0.16}
{'loss': 1.5958, 'grad_norm': 1.1034480333328247, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.462282657623291, 'eval_runtime': 3.0254, 'eval_samples_per_second': 330.532, 'eval_steps_per_second': 20.824, 'epoch': 0.2}
{'loss': 1.4522, 'grad_norm': 0.9934784770011902, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.4239815473556519, 'eval_runtime': 3.0442, 'eval_samples_per_second': 328.495, 'eval_steps_per_second': 20.695, 'epoch': 0.24}
{'loss': 1.4445, 'grad_norm': 0.9449360370635986, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.4206151962280273, 'eval_runtime': 3.0341, 'eval_samples_per_second': 329.591, 'eval_steps_per_second': 20.764, 'epoch': 0.28}
{'loss': 1.5023, 'grad_norm': 1.1148524284362793, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.34233558177948, 'eval_runtime': 3.0229, 'eval_samples_per_second': 330.805, 'eval_steps_per_second': 20.841, 'epoch': 0.32}
{'loss': 1.4352, 'grad_norm': 0.8940118551254272, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.3388478755950928, 'eval_runtime': 3.0151, 'eval_samples_per_second': 331.664, 'eval_steps_per_second': 20.895, 'epoch': 0.36}
{'loss': 1.4442, 'grad_norm': 1.1377558708190918, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.2821195125579834, 'eval_runtime': 3.016, 'eval_samples_per_second': 331.564, 'eval_steps_per_second': 20.889, 'epoch': 0.4}
{'loss': 1.4553, 'grad_norm': 1.0942100286483765, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.1965012550354004, 'eval_runtime': 3.0258, 'eval_samples_per_second': 330.487, 'eval_steps_per_second': 20.821, 'epoch': 0.44}
{'loss': 1.398, 'grad_norm': 1.4051475524902344, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.198466181755066, 'eval_runtime': 3.0296, 'eval_samples_per_second': 330.079, 'eval_steps_per_second': 20.795, 'epoch': 0.48}
{'loss': 1.3988, 'grad_norm': 1.0344141721725464, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.2268785238265991, 'eval_runtime': 3.0154, 'eval_samples_per_second': 331.63, 'eval_steps_per_second': 20.893, 'epoch': 0.52}
{'loss': 1.3998, 'grad_norm': 1.6345093250274658, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.1859277486801147, 'eval_runtime': 3.0169, 'eval_samples_per_second': 331.464, 'eval_steps_per_second': 20.882, 'epoch': 0.56}
{'loss': 1.3727, 'grad_norm': 0.9845395684242249, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.1781678199768066, 'eval_runtime': 3.0161, 'eval_samples_per_second': 331.556, 'eval_steps_per_second': 20.888, 'epoch': 0.6}
{'loss': 1.4145, 'grad_norm': 0.966042160987854, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.1621785163879395, 'eval_runtime': 3.014, 'eval_samples_per_second': 331.785, 'eval_steps_per_second': 20.902, 'epoch': 0.64}
{'loss': 1.3884, 'grad_norm': 1.4564759731292725, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.1780216693878174, 'eval_runtime': 3.0047, 'eval_samples_per_second': 332.813, 'eval_steps_per_second': 20.967, 'epoch': 0.68}
{'loss': 1.4293, 'grad_norm': 1.6708378791809082, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.176554799079895, 'eval_runtime': 3.0109, 'eval_samples_per_second': 332.126, 'eval_steps_per_second': 20.924, 'epoch': 0.72}
{'loss': 1.3748, 'grad_norm': 1.0423156023025513, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.177645206451416, 'eval_runtime': 3.0083, 'eval_samples_per_second': 332.412, 'eval_steps_per_second': 20.942, 'epoch': 0.76}
{'loss': 1.4015, 'grad_norm': 0.8375450372695923, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.196933627128601, 'eval_runtime': 2.998, 'eval_samples_per_second': 333.555, 'eval_steps_per_second': 21.014, 'epoch': 0.81}
{'loss': 1.3335, 'grad_norm': 1.1243395805358887, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.167986273765564, 'eval_runtime': 3.0027, 'eval_samples_per_second': 333.032, 'eval_steps_per_second': 20.981, 'epoch': 0.85}
{'loss': 1.3999, 'grad_norm': 1.402441143989563, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.1549150943756104, 'eval_runtime': 3.0066, 'eval_samples_per_second': 332.597, 'eval_steps_per_second': 20.954, 'epoch': 0.89}
{'loss': 1.3878, 'grad_norm': 1.2647368907928467, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.163541316986084, 'eval_runtime': 3.0004, 'eval_samples_per_second': 333.293, 'eval_steps_per_second': 20.997, 'epoch': 0.93}
{'loss': 1.4108, 'grad_norm': 1.2460691928863525, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.170220136642456, 'eval_runtime': 3.0036, 'eval_samples_per_second': 332.935, 'eval_steps_per_second': 20.975, 'epoch': 0.97}
{'train_runtime': 267.1535, 'train_samples_per_second': 37.158, 'train_steps_per_second': 2.325, 'train_loss': 1.5626139740629088, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4587440490722656, 2.0616140365600586, 1.7963383197784424, 1.6297343969345093, 1.462282657623291, 1.4239815473556519, 1.4206151962280273, 1.34233558177948, 1.3388478755950928, 1.2821195125579834, 1.1965012550354004, 1.198466181755066, 1.2268785238265991, 1.1859277486801147, 1.1781678199768066, 1.1621785163879395, 1.1780216693878174, 1.176554799079895, 1.177645206451416, 1.196933627128601, 1.167986273765564, 1.1549150943756104, 1.163541316986084, 1.170220136642456], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.4587440490722656, 2.0616140365600586, 1.7963383197784424, 1.6297343969345093, 1.462282657623291, 1.4239815473556519, 1.4206151962280273, 1.34233558177948, 1.3388478755950928, 1.2821195125579834, 1.1965012550354004, 1.198466181755066, 1.2268785238265991, 1.1859277486801147, 1.1781678199768066, 1.1621785163879395, 1.1780216693878174, 1.176554799079895, 1.177645206451416, 1.196933627128601, 1.167986273765564, 1.1549150943756104, 1.163541316986084, 1.170220136642456]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.7009090185165405
current iteration best possible eval_loss (full train run):  -1.170220136642456
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.5296 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9384865760803223, 0.07229626178741455, 0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6294164657592773, 0.5043690800666809, 0.12173128128051758, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.45333993434906006, 0.9395368695259094, 0.38172125816345215, 0.5809779763221741]  ‚Üí  acq = -0.9654193169714924
X = [0.25227582454681396, 0.03358107805252075, 0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.7713519334793091, 0.4265725612640381, 0.10601025819778442, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.3584800362586975, 0.16592669486999512, 0.6665055751800537, 0.5913098454475403]  ‚Üí  acq = -0.9840326851430818
X = [0.019490361213684082, 0.33564668893814087, 0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.24613288044929504, 0.783804178237915, 0.05754297971725464, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6837789416313171, 0.5803513526916504, 0.10683952271938324, 0.5482228994369507]  ‚Üí  acq = -0.9656124586381127
X = [0.08164948225021362, 0.510372519493103, 0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3423933982849121, 0.566781759262085, 0.5619614124298096, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9544339776039124, 0.7185770869255066, 0.4849817454814911, 0.03939622640609741]  ‚Üí  acq = -0.9653601996126782
X = [0.9194858074188232, 0.7918861508369446, 0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5424157381057739, 0.4093313217163086, 0.6975671648979187, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.975213885307312, 0.7470415830612183, 0.2785099744796753, 0.9093899726867676]  ‚Üí  acq = -0.9654191223552941
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1154, dtype=torch.float64), tensor(0.1551, dtype=torch.float64), 0, 0, tensor(0.1141, dtype=torch.float64), tensor(0.2407, dtype=torch.float64), tensor(0.1410, dtype=torch.float64), tensor(0.1414, dtype=torch.float64), tensor(0.0924, dtype=torch.float64), 10, 0, 1, 0, 0, 1, 2, 0.1, 21.2821470543047, 1]
normalized proposed parameters for next round by BO: [tensor(0.1154, dtype=torch.float64), tensor(0.1551, dtype=torch.float64), tensor(4.6629e-18, dtype=torch.float64), tensor(6.0878e-17, dtype=torch.float64), tensor(0.1141, dtype=torch.float64), tensor(0.2407, dtype=torch.float64), tensor(0.1410, dtype=torch.float64), tensor(0.1414, dtype=torch.float64), tensor(0.0924, dtype=torch.float64), tensor(0.3188, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4434, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.115
  gsm8k: 0.155
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.114
  truthfulqa_gen: 0.241
  wikitext: 0.141
  mmlu: 0.141
  arc_challenge: 0.092

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (10,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (21.2821470543047,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  10
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  21.2821470543047
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 471,040 || all params: 8,030,732,288 || trainable%: 0.0059
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.441, 'grad_norm': 5.818380832672119, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.660935163497925, 'eval_runtime': 5.4746, 'eval_samples_per_second': 182.663, 'eval_steps_per_second': 11.508, 'epoch': 0.04}
{'loss': 1.9546, 'grad_norm': 3.9987990856170654, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.0875136852264404, 'eval_runtime': 2.9153, 'eval_samples_per_second': 343.017, 'eval_steps_per_second': 21.61, 'epoch': 0.08}
{'loss': 1.5174, 'grad_norm': 3.218038558959961, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6681127548217773, 'eval_runtime': 2.9085, 'eval_samples_per_second': 343.825, 'eval_steps_per_second': 21.661, 'epoch': 0.12}
{'loss': 1.3966, 'grad_norm': 2.818415880203247, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.533171534538269, 'eval_runtime': 2.9091, 'eval_samples_per_second': 343.743, 'eval_steps_per_second': 21.656, 'epoch': 0.16}
{'loss': 1.2664, 'grad_norm': 1.7592990398406982, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.44585382938385, 'eval_runtime': 2.9144, 'eval_samples_per_second': 343.121, 'eval_steps_per_second': 21.617, 'epoch': 0.2}
{'loss': 1.2833, 'grad_norm': 1.8133548498153687, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3412883281707764, 'eval_runtime': 2.9261, 'eval_samples_per_second': 341.756, 'eval_steps_per_second': 21.531, 'epoch': 0.24}
{'loss': 1.2465, 'grad_norm': 1.62205970287323, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3181453943252563, 'eval_runtime': 2.9205, 'eval_samples_per_second': 342.406, 'eval_steps_per_second': 21.572, 'epoch': 0.28}
{'loss': 1.2006, 'grad_norm': 1.9022762775421143, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3153283596038818, 'eval_runtime': 2.9216, 'eval_samples_per_second': 342.282, 'eval_steps_per_second': 21.564, 'epoch': 0.32}
{'loss': 1.223, 'grad_norm': 1.4530173540115356, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3003100156784058, 'eval_runtime': 2.9208, 'eval_samples_per_second': 342.369, 'eval_steps_per_second': 21.569, 'epoch': 0.36}
{'loss': 1.2007, 'grad_norm': 1.2756329774856567, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2936680316925049, 'eval_runtime': 2.9238, 'eval_samples_per_second': 342.017, 'eval_steps_per_second': 21.547, 'epoch': 0.4}
{'loss': 1.2193, 'grad_norm': 1.3777047395706177, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2703678607940674, 'eval_runtime': 2.924, 'eval_samples_per_second': 341.992, 'eval_steps_per_second': 21.545, 'epoch': 0.44}
{'loss': 1.2107, 'grad_norm': 1.539837121963501, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2838994264602661, 'eval_runtime': 2.9215, 'eval_samples_per_second': 342.289, 'eval_steps_per_second': 21.564, 'epoch': 0.48}
{'loss': 1.2246, 'grad_norm': 1.22109055519104, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2544597387313843, 'eval_runtime': 2.9584, 'eval_samples_per_second': 338.021, 'eval_steps_per_second': 21.295, 'epoch': 0.52}
{'loss': 1.12, 'grad_norm': 1.3980696201324463, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.238027572631836, 'eval_runtime': 2.9521, 'eval_samples_per_second': 338.746, 'eval_steps_per_second': 21.341, 'epoch': 0.56}
{'loss': 1.1282, 'grad_norm': 1.250166893005371, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2472312450408936, 'eval_runtime': 2.9694, 'eval_samples_per_second': 336.766, 'eval_steps_per_second': 21.216, 'epoch': 0.6}
{'loss': 1.156, 'grad_norm': 1.9958105087280273, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2329610586166382, 'eval_runtime': 2.9612, 'eval_samples_per_second': 337.696, 'eval_steps_per_second': 21.275, 'epoch': 0.64}
{'loss': 1.207, 'grad_norm': 1.2488001585006714, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2267539501190186, 'eval_runtime': 2.9641, 'eval_samples_per_second': 337.373, 'eval_steps_per_second': 21.255, 'epoch': 0.68}
{'loss': 1.152, 'grad_norm': 1.4001325368881226, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2143664360046387, 'eval_runtime': 2.9791, 'eval_samples_per_second': 335.671, 'eval_steps_per_second': 21.147, 'epoch': 0.72}
{'loss': 1.1671, 'grad_norm': 1.6299916505813599, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2163158655166626, 'eval_runtime': 2.9415, 'eval_samples_per_second': 339.958, 'eval_steps_per_second': 21.417, 'epoch': 0.76}
{'loss': 1.1675, 'grad_norm': 1.3113808631896973, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1924939155578613, 'eval_runtime': 2.93, 'eval_samples_per_second': 341.293, 'eval_steps_per_second': 21.501, 'epoch': 0.8}
{'loss': 1.095, 'grad_norm': 1.4486583471298218, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.195125937461853, 'eval_runtime': 2.9393, 'eval_samples_per_second': 340.222, 'eval_steps_per_second': 21.434, 'epoch': 0.84}
{'loss': 1.196, 'grad_norm': 1.042949914932251, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1812297105789185, 'eval_runtime': 2.9432, 'eval_samples_per_second': 339.772, 'eval_steps_per_second': 21.406, 'epoch': 0.88}
{'loss': 1.0833, 'grad_norm': 1.331594705581665, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1848666667938232, 'eval_runtime': 2.9447, 'eval_samples_per_second': 339.593, 'eval_steps_per_second': 21.394, 'epoch': 0.92}
{'loss': 1.1724, 'grad_norm': 1.132183313369751, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1744043827056885, 'eval_runtime': 2.9364, 'eval_samples_per_second': 340.549, 'eval_steps_per_second': 21.455, 'epoch': 0.96}
{'loss': 1.1014, 'grad_norm': 1.3600515127182007, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1720843315124512, 'eval_runtime': 2.9361, 'eval_samples_per_second': 340.586, 'eval_steps_per_second': 21.457, 'epoch': 1.0}
{'train_runtime': 261.9414, 'train_samples_per_second': 38.161, 'train_steps_per_second': 2.386, 'train_loss': 1.3252326049804688, 'epoch': 1.0}
train_results:  {'eval_loss': [2.660935163497925, 2.0875136852264404, 1.6681127548217773, 1.533171534538269, 1.44585382938385, 1.3412883281707764, 1.3181453943252563, 1.3153283596038818, 1.3003100156784058, 1.2936680316925049, 1.2703678607940674, 1.2838994264602661, 1.2544597387313843, 1.238027572631836, 1.2472312450408936, 1.2329610586166382, 1.2267539501190186, 1.2143664360046387, 1.2163158655166626, 1.1924939155578613, 1.195125937461853, 1.1812297105789185, 1.1848666667938232, 1.1744043827056885, 1.1720843315124512], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.660935163497925, 2.0875136852264404, 1.6681127548217773, 1.533171534538269, 1.44585382938385, 1.3412883281707764, 1.3181453943252563, 1.3153283596038818, 1.3003100156784058, 1.2936680316925049, 1.2703678607940674, 1.2838994264602661, 1.2544597387313843, 1.238027572631836, 1.2472312450408936, 1.2329610586166382, 1.2267539501190186, 1.2143664360046387, 1.2163158655166626, 1.1924939155578613, 1.195125937461853, 1.1812297105789185, 1.1848666667938232, 1.1744043827056885, 1.1720843315124512]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.6125328540802002
current iteration best possible eval_loss (full train run):  -1.1720843315124512
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.0127 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.6970622760690999
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.6975640543230355
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -1.0264735024860865
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.7128709249226668
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.7005642931715967
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1413, dtype=torch.float64), 0, 0, tensor(0.1178, dtype=torch.float64), 0, tensor(0.1353, dtype=torch.float64), tensor(0.5156, dtype=torch.float64), 0, tensor(0.0900, dtype=torch.float64), 25, 0, 0, 0, 1, 1, 2, 6.938893903907226e-19, 15.18202875193839, 0]
normalized proposed parameters for next round by BO: [tensor(0.1413, dtype=torch.float64), tensor(4.4597e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1178, dtype=torch.float64), tensor(9.0347e-18, dtype=torch.float64), tensor(0.1353, dtype=torch.float64), tensor(0.5156, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0900, dtype=torch.float64), tensor(0.7726, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(6.9389e-18, dtype=torch.float64), tensor(0.3163, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.141
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.118
  triviaqa: 0
  truthfulqa_gen: 0.135
  wikitext: 0.516
  mmlu: 0
  arc_challenge: 0.09

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (6.938893903907226e-19,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (15.18202875193839,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  6.938893903907226e-19
lora alpha:  15.18202875193839
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,843,200 || all params: 8,032,104,448 || trainable%: 0.0229
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4372, 'grad_norm': 3.6254851818084717, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.47304105758667, 'eval_runtime': 4.8039, 'eval_samples_per_second': 208.166, 'eval_steps_per_second': 13.114, 'epoch': 0.04}
{'loss': 1.9397, 'grad_norm': 4.992376804351807, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7930710315704346, 'eval_runtime': 3.2007, 'eval_samples_per_second': 312.428, 'eval_steps_per_second': 19.683, 'epoch': 0.08}
{'loss': 1.6273, 'grad_norm': 1.9293043613433838, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.662175178527832, 'eval_runtime': 3.349, 'eval_samples_per_second': 298.593, 'eval_steps_per_second': 18.811, 'epoch': 0.12}
{'loss': 1.5534, 'grad_norm': 1.9359405040740967, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6645642518997192, 'eval_runtime': 3.2045, 'eval_samples_per_second': 312.059, 'eval_steps_per_second': 19.66, 'epoch': 0.16}
{'loss': 1.4867, 'grad_norm': 1.3350008726119995, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7126350402832031, 'eval_runtime': 3.1964, 'eval_samples_per_second': 312.849, 'eval_steps_per_second': 19.709, 'epoch': 0.2}
{'loss': 1.4115, 'grad_norm': 0.9833936095237732, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7266294956207275, 'eval_runtime': 3.1936, 'eval_samples_per_second': 313.125, 'eval_steps_per_second': 19.727, 'epoch': 0.24}
{'loss': 1.3544, 'grad_norm': 1.3433592319488525, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7156938314437866, 'eval_runtime': 3.195, 'eval_samples_per_second': 312.99, 'eval_steps_per_second': 19.718, 'epoch': 0.28}
{'loss': 1.4758, 'grad_norm': 1.245477557182312, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7094810009002686, 'eval_runtime': 3.2184, 'eval_samples_per_second': 310.712, 'eval_steps_per_second': 19.575, 'epoch': 0.32}
{'loss': 1.5352, 'grad_norm': 0.98720782995224, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7396037578582764, 'eval_runtime': 3.2458, 'eval_samples_per_second': 308.094, 'eval_steps_per_second': 19.41, 'epoch': 0.36}
{'loss': 1.4021, 'grad_norm': 1.2954305410385132, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6962714195251465, 'eval_runtime': 3.2588, 'eval_samples_per_second': 306.863, 'eval_steps_per_second': 19.332, 'epoch': 0.4}
{'loss': 1.5004, 'grad_norm': 1.6310514211654663, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7779992818832397, 'eval_runtime': 3.2547, 'eval_samples_per_second': 307.246, 'eval_steps_per_second': 19.356, 'epoch': 0.44}
{'loss': 1.4302, 'grad_norm': 1.3537620306015015, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7818787097930908, 'eval_runtime': 3.2541, 'eval_samples_per_second': 307.304, 'eval_steps_per_second': 19.36, 'epoch': 0.48}
{'loss': 1.4672, 'grad_norm': 1.1531267166137695, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.781110405921936, 'eval_runtime': 3.2645, 'eval_samples_per_second': 306.324, 'eval_steps_per_second': 19.298, 'epoch': 0.52}
{'loss': 1.408, 'grad_norm': 1.5143152475357056, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.783178687095642, 'eval_runtime': 3.2554, 'eval_samples_per_second': 307.181, 'eval_steps_per_second': 19.352, 'epoch': 0.56}
{'loss': 1.4752, 'grad_norm': 1.4919869899749756, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8047292232513428, 'eval_runtime': 3.2296, 'eval_samples_per_second': 309.64, 'eval_steps_per_second': 19.507, 'epoch': 0.6}
{'loss': 1.4221, 'grad_norm': 1.2556047439575195, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8242628574371338, 'eval_runtime': 3.2284, 'eval_samples_per_second': 309.748, 'eval_steps_per_second': 19.514, 'epoch': 0.64}
{'loss': 1.4312, 'grad_norm': 1.7229442596435547, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8239469528198242, 'eval_runtime': 3.2168, 'eval_samples_per_second': 310.869, 'eval_steps_per_second': 19.585, 'epoch': 0.68}
{'loss': 1.344, 'grad_norm': 1.6693642139434814, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8051913976669312, 'eval_runtime': 3.2175, 'eval_samples_per_second': 310.798, 'eval_steps_per_second': 19.58, 'epoch': 0.72}
{'loss': 1.3871, 'grad_norm': 1.8912723064422607, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8360167741775513, 'eval_runtime': 3.216, 'eval_samples_per_second': 310.948, 'eval_steps_per_second': 19.59, 'epoch': 0.76}
{'loss': 1.5252, 'grad_norm': 1.304722547531128, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8024787902832031, 'eval_runtime': 3.207, 'eval_samples_per_second': 311.815, 'eval_steps_per_second': 19.644, 'epoch': 0.8}
{'loss': 1.4245, 'grad_norm': 1.473254680633545, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8197580575942993, 'eval_runtime': 3.2206, 'eval_samples_per_second': 310.504, 'eval_steps_per_second': 19.562, 'epoch': 0.84}
{'loss': 1.4038, 'grad_norm': 1.074313759803772, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.818390965461731, 'eval_runtime': 3.2085, 'eval_samples_per_second': 311.671, 'eval_steps_per_second': 19.635, 'epoch': 0.88}
{'loss': 1.41, 'grad_norm': 1.1459935903549194, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8243792057037354, 'eval_runtime': 3.2163, 'eval_samples_per_second': 310.917, 'eval_steps_per_second': 19.588, 'epoch': 0.92}
{'loss': 1.3431, 'grad_norm': 1.473983883857727, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8237895965576172, 'eval_runtime': 3.2143, 'eval_samples_per_second': 311.113, 'eval_steps_per_second': 19.6, 'epoch': 0.96}
{'loss': 1.3957, 'grad_norm': 1.4787606000900269, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.827252984046936, 'eval_runtime': 3.2219, 'eval_samples_per_second': 310.378, 'eval_steps_per_second': 19.554, 'epoch': 1.0}
{'train_runtime': 255.4238, 'train_samples_per_second': 39.139, 'train_steps_per_second': 2.447, 'train_loss': 1.543642156982422, 'epoch': 1.0}
train_results:  {'eval_loss': [2.47304105758667, 1.7930710315704346, 1.662175178527832, 1.6645642518997192, 1.7126350402832031, 1.7266294956207275, 1.7156938314437866, 1.7094810009002686, 1.7396037578582764, 1.6962714195251465, 1.7779992818832397, 1.7818787097930908, 1.781110405921936, 1.783178687095642, 1.8047292232513428, 1.8242628574371338, 1.8239469528198242, 1.8051913976669312, 1.8360167741775513, 1.8024787902832031, 1.8197580575942993, 1.818390965461731, 1.8243792057037354, 1.8237895965576172, 1.827252984046936], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.47304105758667, 1.7930710315704346, 1.662175178527832, 1.6645642518997192, 1.7126350402832031, 1.7266294956207275, 1.7156938314437866, 1.7094810009002686, 1.7396037578582764, 1.6962714195251465, 1.7779992818832397, 1.7818787097930908, 1.781110405921936, 1.783178687095642, 1.8047292232513428, 1.8242628574371338, 1.8239469528198242, 1.8051913976669312, 1.8360167741775513, 1.8024787902832031, 1.8197580575942993, 1.818390965461731, 1.8243792057037354, 1.8237895965576172, 1.827252984046936]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.0853421688079834
current iteration best possible eval_loss (full train run):  -1.827252984046936
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.9293 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -0.8309450299819154
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.8309463461280338
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -0.8309448126742571
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -0.8312516793234708
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -0.8309450746122322
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1819, dtype=torch.float64), tensor(0.1274, dtype=torch.float64), tensor(0.1091, dtype=torch.float64), 0, tensor(0.4403, dtype=torch.float64), tensor(0.1087, dtype=torch.float64), tensor(0.0325, dtype=torch.float64), 4, 0, 0, 1, 1, 1, 128, 0.1, 32.074292495490624, 1]
normalized proposed parameters for next round by BO: [tensor(9.7389e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1819, dtype=torch.float64), tensor(0.1274, dtype=torch.float64), tensor(0.1091, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4403, dtype=torch.float64), tensor(0.1087, dtype=torch.float64), tensor(0.0325, dtype=torch.float64), tensor(0.1288, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6682, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.182
  sciq: 0.127
  triviaqa: 0.109
  truthfulqa_gen: 0
  wikitext: 0.44
  mmlu: 0.109
  arc_challenge: 0.033

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (4,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (32.074292495490624,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  4
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  32.074292495490624
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 28,311,552 || all params: 8,058,572,800 || trainable%: 0.3513
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5963, 'grad_norm': 0.8641052842140198, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.608077049255371, 'eval_runtime': 2.9333, 'eval_samples_per_second': 340.912, 'eval_steps_per_second': 21.477, 'epoch': 0.04}
{'loss': 2.5662, 'grad_norm': 0.6707673072814941, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.731442928314209, 'eval_runtime': 2.9161, 'eval_samples_per_second': 342.918, 'eval_steps_per_second': 21.604, 'epoch': 0.08}
{'loss': 2.0412, 'grad_norm': 0.38435518741607666, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4049712419509888, 'eval_runtime': 2.9161, 'eval_samples_per_second': 342.919, 'eval_steps_per_second': 21.604, 'epoch': 0.12}
{'loss': 1.9241, 'grad_norm': 0.39131632447242737, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3507843017578125, 'eval_runtime': 2.9212, 'eval_samples_per_second': 342.324, 'eval_steps_per_second': 21.566, 'epoch': 0.16}
{'loss': 1.8627, 'grad_norm': 0.318447083234787, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2967884540557861, 'eval_runtime': 2.9542, 'eval_samples_per_second': 338.506, 'eval_steps_per_second': 21.326, 'epoch': 0.2}
{'loss': 1.8691, 'grad_norm': 0.25712817907333374, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.258376121520996, 'eval_runtime': 2.944, 'eval_samples_per_second': 339.673, 'eval_steps_per_second': 21.399, 'epoch': 0.24}
{'loss': 1.7976, 'grad_norm': 0.32776227593421936, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2311981916427612, 'eval_runtime': 2.9505, 'eval_samples_per_second': 338.923, 'eval_steps_per_second': 21.352, 'epoch': 0.28}
{'loss': 1.7773, 'grad_norm': 0.3240412473678589, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2018561363220215, 'eval_runtime': 2.9771, 'eval_samples_per_second': 335.902, 'eval_steps_per_second': 21.162, 'epoch': 0.32}
{'loss': 1.7776, 'grad_norm': 0.35943499207496643, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1062349081039429, 'eval_runtime': 2.9545, 'eval_samples_per_second': 338.471, 'eval_steps_per_second': 21.324, 'epoch': 0.36}
{'loss': 1.6873, 'grad_norm': 0.32010674476623535, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.101861834526062, 'eval_runtime': 2.932, 'eval_samples_per_second': 341.069, 'eval_steps_per_second': 21.487, 'epoch': 0.4}
{'loss': 1.7265, 'grad_norm': 0.2822853624820709, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0829684734344482, 'eval_runtime': 2.9356, 'eval_samples_per_second': 340.647, 'eval_steps_per_second': 21.461, 'epoch': 0.44}
{'loss': 1.6988, 'grad_norm': 0.3017299771308899, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.085595965385437, 'eval_runtime': 2.9348, 'eval_samples_per_second': 340.737, 'eval_steps_per_second': 21.466, 'epoch': 0.48}
{'loss': 1.7632, 'grad_norm': 0.3310323655605316, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0696698427200317, 'eval_runtime': 2.9389, 'eval_samples_per_second': 340.263, 'eval_steps_per_second': 21.437, 'epoch': 0.52}
{'loss': 1.6984, 'grad_norm': 0.4423520863056183, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0740196704864502, 'eval_runtime': 2.9426, 'eval_samples_per_second': 339.834, 'eval_steps_per_second': 21.41, 'epoch': 0.56}
{'loss': 1.6453, 'grad_norm': 0.21929016709327698, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.09540855884552, 'eval_runtime': 2.9175, 'eval_samples_per_second': 342.763, 'eval_steps_per_second': 21.594, 'epoch': 0.6}
{'loss': 1.6713, 'grad_norm': 0.21870176494121552, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0820668935775757, 'eval_runtime': 2.9223, 'eval_samples_per_second': 342.201, 'eval_steps_per_second': 21.559, 'epoch': 0.64}
{'loss': 1.7512, 'grad_norm': 0.4521947205066681, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0834267139434814, 'eval_runtime': 2.9232, 'eval_samples_per_second': 342.09, 'eval_steps_per_second': 21.552, 'epoch': 0.68}
{'loss': 1.6668, 'grad_norm': 0.2435470074415207, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0677094459533691, 'eval_runtime': 2.9226, 'eval_samples_per_second': 342.161, 'eval_steps_per_second': 21.556, 'epoch': 0.72}
{'loss': 1.7086, 'grad_norm': 0.21185094118118286, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0778405666351318, 'eval_runtime': 2.9225, 'eval_samples_per_second': 342.174, 'eval_steps_per_second': 21.557, 'epoch': 0.76}
{'loss': 1.7426, 'grad_norm': 0.4331464171409607, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0744625329971313, 'eval_runtime': 2.9247, 'eval_samples_per_second': 341.921, 'eval_steps_per_second': 21.541, 'epoch': 0.8}
{'loss': 1.7315, 'grad_norm': 0.26861104369163513, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0777153968811035, 'eval_runtime': 2.9271, 'eval_samples_per_second': 341.64, 'eval_steps_per_second': 21.523, 'epoch': 0.84}
{'loss': 1.6812, 'grad_norm': 0.2415740191936493, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0712147951126099, 'eval_runtime': 2.9279, 'eval_samples_per_second': 341.546, 'eval_steps_per_second': 21.517, 'epoch': 0.88}
{'loss': 1.6929, 'grad_norm': 0.27400732040405273, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0705865621566772, 'eval_runtime': 2.9333, 'eval_samples_per_second': 340.909, 'eval_steps_per_second': 21.477, 'epoch': 0.92}
{'loss': 1.6762, 'grad_norm': 0.5127452611923218, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0652239322662354, 'eval_runtime': 2.9278, 'eval_samples_per_second': 341.558, 'eval_steps_per_second': 21.518, 'epoch': 0.96}
{'loss': 1.6798, 'grad_norm': 0.3407498598098755, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.062615990638733, 'eval_runtime': 2.9255, 'eval_samples_per_second': 341.822, 'eval_steps_per_second': 21.535, 'epoch': 1.0}
{'train_runtime': 266.6468, 'train_samples_per_second': 37.499, 'train_steps_per_second': 2.344, 'train_loss': 1.8573490966796875, 'epoch': 1.0}
train_results:  {'eval_loss': [2.608077049255371, 1.731442928314209, 1.4049712419509888, 1.3507843017578125, 1.2967884540557861, 1.258376121520996, 1.2311981916427612, 1.2018561363220215, 1.1062349081039429, 1.101861834526062, 1.0829684734344482, 1.085595965385437, 1.0696698427200317, 1.0740196704864502, 1.09540855884552, 1.0820668935775757, 1.0834267139434814, 1.0677094459533691, 1.0778405666351318, 1.0744625329971313, 1.0777153968811035, 1.0712147951126099, 1.0705865621566772, 1.0652239322662354, 1.062615990638733], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.608077049255371, 1.731442928314209, 1.4049712419509888, 1.3507843017578125, 1.2967884540557861, 1.258376121520996, 1.2311981916427612, 1.2018561363220215, 1.1062349081039429, 1.101861834526062, 1.0829684734344482, 1.085595965385437, 1.0696698427200317, 1.0740196704864502, 1.09540855884552, 1.0820668935775757, 1.0834267139434814, 1.0677094459533691, 1.0778405666351318, 1.0744625329971313, 1.0777153968811035, 1.0712147951126099, 1.0705865621566772, 1.0652239322662354, 1.062615990638733]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.70357084274292
current iteration best possible eval_loss (full train run):  -1.062615990638733
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.3803 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -1.1723596889002097
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -1.1686022899793347
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -1.1729091552815756
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -1.1125819215340256
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -1.1600884843337738
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0394, dtype=torch.float64), tensor(0.2234, dtype=torch.float64), tensor(0.0365, dtype=torch.float64), tensor(0.1212, dtype=torch.float64), tensor(0.2184, dtype=torch.float64), tensor(0.0648, dtype=torch.float64), tensor(0.1298, dtype=torch.float64), tensor(0.1064, dtype=torch.float64), tensor(0.0604, dtype=torch.float64), 14, 0, 1, 0, 0, 1, 87, 0.06940412332691014, 25.604197778320664, 1]
normalized proposed parameters for next round by BO: [tensor(0.0394, dtype=torch.float64), tensor(0.2234, dtype=torch.float64), tensor(0.0365, dtype=torch.float64), tensor(0.1212, dtype=torch.float64), tensor(0.2184, dtype=torch.float64), tensor(0.0648, dtype=torch.float64), tensor(0.1298, dtype=torch.float64), tensor(0.1064, dtype=torch.float64), tensor(0.0604, dtype=torch.float64), tensor(0.4411, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6764, dtype=torch.float64), tensor(0.6940, dtype=torch.float64), tensor(0.5334, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.039
  gsm8k: 0.223
  rowan_hellaswag: 0.036
  sciq: 0.121
  triviaqa: 0.218
  truthfulqa_gen: 0.065
  wikitext: 0.13
  mmlu: 0.106
  arc_challenge: 0.06

LoRA Parameters:
  lora_r: (87,)
  lora_dropout: (0.06940412332691014,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (25.604197778320664,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  87
lora dropout:  0.06940412332691014
lora alpha:  25.604197778320664
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 28,686,336 || all params: 8,058,947,584 || trainable%: 0.3560
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1307, 'grad_norm': 0.630123496055603, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4270644187927246, 'eval_runtime': 3.0, 'eval_samples_per_second': 333.337, 'eval_steps_per_second': 21.0, 'epoch': 0.04}
{'loss': 1.8613, 'grad_norm': 0.4377538561820984, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.8752974271774292, 'eval_runtime': 3.0042, 'eval_samples_per_second': 332.863, 'eval_steps_per_second': 20.97, 'epoch': 0.08}
{'loss': 1.5198, 'grad_norm': 0.3116861581802368, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5597753524780273, 'eval_runtime': 3.0159, 'eval_samples_per_second': 331.575, 'eval_steps_per_second': 20.889, 'epoch': 0.12}
{'loss': 1.375, 'grad_norm': 0.27945494651794434, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4742844104766846, 'eval_runtime': 3.0024, 'eval_samples_per_second': 333.068, 'eval_steps_per_second': 20.983, 'epoch': 0.16}
{'loss': 1.369, 'grad_norm': 0.2831043601036072, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3960373401641846, 'eval_runtime': 3.0169, 'eval_samples_per_second': 331.461, 'eval_steps_per_second': 20.882, 'epoch': 0.2}
{'loss': 1.295, 'grad_norm': 0.26317688822746277, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.309520959854126, 'eval_runtime': 3.0076, 'eval_samples_per_second': 332.495, 'eval_steps_per_second': 20.947, 'epoch': 0.24}
{'loss': 1.2643, 'grad_norm': 0.3050288259983063, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2670083045959473, 'eval_runtime': 3.0165, 'eval_samples_per_second': 331.514, 'eval_steps_per_second': 20.885, 'epoch': 0.28}
{'loss': 1.2352, 'grad_norm': 0.23334023356437683, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2631351947784424, 'eval_runtime': 3.0172, 'eval_samples_per_second': 331.437, 'eval_steps_per_second': 20.881, 'epoch': 0.32}
{'loss': 1.2125, 'grad_norm': 0.24485093355178833, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.260614275932312, 'eval_runtime': 3.0149, 'eval_samples_per_second': 331.686, 'eval_steps_per_second': 20.896, 'epoch': 0.36}
{'loss': 1.2428, 'grad_norm': 0.23164135217666626, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2423409223556519, 'eval_runtime': 3.0712, 'eval_samples_per_second': 325.61, 'eval_steps_per_second': 20.513, 'epoch': 0.4}
{'loss': 1.2425, 'grad_norm': 0.26116782426834106, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.228116512298584, 'eval_runtime': 3.029, 'eval_samples_per_second': 330.146, 'eval_steps_per_second': 20.799, 'epoch': 0.44}
{'loss': 1.2209, 'grad_norm': 0.21775302290916443, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2373133897781372, 'eval_runtime': 3.0314, 'eval_samples_per_second': 329.885, 'eval_steps_per_second': 20.783, 'epoch': 0.48}
{'loss': 1.2526, 'grad_norm': 0.24970407783985138, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2191877365112305, 'eval_runtime': 3.0247, 'eval_samples_per_second': 330.613, 'eval_steps_per_second': 20.829, 'epoch': 0.52}
{'loss': 1.2665, 'grad_norm': 0.3132763206958771, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.217165470123291, 'eval_runtime': 3.0173, 'eval_samples_per_second': 331.418, 'eval_steps_per_second': 20.879, 'epoch': 0.56}
{'loss': 1.1536, 'grad_norm': 0.2431044727563858, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.203697919845581, 'eval_runtime': 3.0142, 'eval_samples_per_second': 331.763, 'eval_steps_per_second': 20.901, 'epoch': 0.6}
{'loss': 1.1927, 'grad_norm': 0.21357281506061554, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.200015664100647, 'eval_runtime': 3.0219, 'eval_samples_per_second': 330.916, 'eval_steps_per_second': 20.848, 'epoch': 0.64}
{'loss': 1.132, 'grad_norm': 0.19708940386772156, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.193542718887329, 'eval_runtime': 3.0123, 'eval_samples_per_second': 331.972, 'eval_steps_per_second': 20.914, 'epoch': 0.68}
{'loss': 1.2036, 'grad_norm': 0.3847449719905853, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.184682846069336, 'eval_runtime': 3.0134, 'eval_samples_per_second': 331.846, 'eval_steps_per_second': 20.906, 'epoch': 0.72}
{'loss': 1.1784, 'grad_norm': 0.24933677911758423, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1800063848495483, 'eval_runtime': 3.0156, 'eval_samples_per_second': 331.61, 'eval_steps_per_second': 20.891, 'epoch': 0.76}
{'loss': 1.2057, 'grad_norm': 0.22301293909549713, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.174360752105713, 'eval_runtime': 3.0147, 'eval_samples_per_second': 331.709, 'eval_steps_per_second': 20.898, 'epoch': 0.8}
{'loss': 1.169, 'grad_norm': 0.2434108704328537, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1738994121551514, 'eval_runtime': 3.0103, 'eval_samples_per_second': 332.194, 'eval_steps_per_second': 20.928, 'epoch': 0.84}
{'loss': 1.1738, 'grad_norm': 0.24040117859840393, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1632490158081055, 'eval_runtime': 3.0108, 'eval_samples_per_second': 332.138, 'eval_steps_per_second': 20.925, 'epoch': 0.88}
{'loss': 1.1646, 'grad_norm': 0.23955051600933075, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1597092151641846, 'eval_runtime': 3.0134, 'eval_samples_per_second': 331.854, 'eval_steps_per_second': 20.907, 'epoch': 0.92}
{'loss': 1.1793, 'grad_norm': 0.21240852773189545, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1600050926208496, 'eval_runtime': 3.0146, 'eval_samples_per_second': 331.717, 'eval_steps_per_second': 20.898, 'epoch': 0.96}
{'loss': 1.1979, 'grad_norm': 0.27993762493133545, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.159082055091858, 'eval_runtime': 3.016, 'eval_samples_per_second': 331.564, 'eval_steps_per_second': 20.889, 'epoch': 1.0}
{'train_runtime': 272.4555, 'train_samples_per_second': 36.685, 'train_steps_per_second': 2.294, 'train_loss': 1.3375404235839843, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4270644187927246, 1.8752974271774292, 1.5597753524780273, 1.4742844104766846, 1.3960373401641846, 1.309520959854126, 1.2670083045959473, 1.2631351947784424, 1.260614275932312, 1.2423409223556519, 1.228116512298584, 1.2373133897781372, 1.2191877365112305, 1.217165470123291, 1.203697919845581, 1.200015664100647, 1.193542718887329, 1.184682846069336, 1.1800063848495483, 1.174360752105713, 1.1738994121551514, 1.1632490158081055, 1.1597092151641846, 1.1600050926208496, 1.159082055091858], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.4270644187927246, 1.8752974271774292, 1.5597753524780273, 1.4742844104766846, 1.3960373401641846, 1.309520959854126, 1.2670083045959473, 1.2631351947784424, 1.260614275932312, 1.2423409223556519, 1.228116512298584, 1.2373133897781372, 1.2191877365112305, 1.217165470123291, 1.203697919845581, 1.200015664100647, 1.193542718887329, 1.184682846069336, 1.1800063848495483, 1.174360752105713, 1.1738994121551514, 1.1632490158081055, 1.1597092151641846, 1.1600050926208496, 1.159082055091858]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.230616569519043
current iteration best possible eval_loss (full train run):  -1.159082055091858
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.2699 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -0.9315073356663128
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -0.9315395827742772
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -0.931540429601537
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -0.931540803355805
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -0.9315407999960272
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0883, dtype=torch.float64), tensor(0.3056, dtype=torch.float64), tensor(0.0586, dtype=torch.float64), tensor(0.2683, dtype=torch.float64), tensor(0.0721, dtype=torch.float64), tensor(0.0780, dtype=torch.float64), 0, tensor(0.0645, dtype=torch.float64), tensor(0.0647, dtype=torch.float64), 23, 0, 0, 1, 1, 1, 32, 0.07241185047507408, 11.896247618176, 1]
normalized proposed parameters for next round by BO: [tensor(0.0883, dtype=torch.float64), tensor(0.3056, dtype=torch.float64), tensor(0.0586, dtype=torch.float64), tensor(0.2683, dtype=torch.float64), tensor(0.0721, dtype=torch.float64), tensor(0.0780, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0645, dtype=torch.float64), tensor(0.0647, dtype=torch.float64), tensor(0.7108, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2492, dtype=torch.float64), tensor(0.7241, dtype=torch.float64), tensor(0.2478, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.088
  gsm8k: 0.306
  rowan_hellaswag: 0.059
  sciq: 0.268
  triviaqa: 0.072
  truthfulqa_gen: 0.078
  wikitext: 0
  mmlu: 0.064
  arc_challenge: 0.065

LoRA Parameters:
  lora_r: (32,)
  lora_dropout: (0.07241185047507408,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (11.896247618176,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  32
lora dropout:  0.07241185047507408
lora alpha:  11.896247618176
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 40,697,856 || all params: 8,070,959,104 || trainable%: 0.5043
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0686, 'grad_norm': 1.759698510169983, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.308180570602417, 'eval_runtime': 4.6548, 'eval_samples_per_second': 214.831, 'eval_steps_per_second': 13.534, 'epoch': 0.04}
{'loss': 1.5197, 'grad_norm': 0.6460015177726746, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5887399911880493, 'eval_runtime': 3.3138, 'eval_samples_per_second': 301.773, 'eval_steps_per_second': 19.012, 'epoch': 0.08}
{'loss': 1.2464, 'grad_norm': 0.3259652554988861, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3826090097427368, 'eval_runtime': 3.3021, 'eval_samples_per_second': 302.837, 'eval_steps_per_second': 19.079, 'epoch': 0.12}
{'loss': 1.1564, 'grad_norm': 0.37694206833839417, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2891008853912354, 'eval_runtime': 3.2882, 'eval_samples_per_second': 304.119, 'eval_steps_per_second': 19.159, 'epoch': 0.16}
{'loss': 1.073, 'grad_norm': 0.2201339155435562, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.253790020942688, 'eval_runtime': 3.2916, 'eval_samples_per_second': 303.802, 'eval_steps_per_second': 19.14, 'epoch': 0.2}
{'loss': 1.0955, 'grad_norm': 0.42517346143722534, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1731210947036743, 'eval_runtime': 3.2797, 'eval_samples_per_second': 304.904, 'eval_steps_per_second': 19.209, 'epoch': 0.24}
{'loss': 0.9782, 'grad_norm': 0.27610477805137634, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.166227102279663, 'eval_runtime': 3.2834, 'eval_samples_per_second': 304.566, 'eval_steps_per_second': 19.188, 'epoch': 0.28}
{'loss': 0.9999, 'grad_norm': 0.31063371896743774, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1249865293502808, 'eval_runtime': 3.2812, 'eval_samples_per_second': 304.766, 'eval_steps_per_second': 19.2, 'epoch': 0.32}
{'loss': 1.0395, 'grad_norm': 0.25554338097572327, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.105177879333496, 'eval_runtime': 3.2792, 'eval_samples_per_second': 304.95, 'eval_steps_per_second': 19.212, 'epoch': 0.36}
{'loss': 0.9963, 'grad_norm': 0.24793407320976257, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0996081829071045, 'eval_runtime': 3.2759, 'eval_samples_per_second': 305.263, 'eval_steps_per_second': 19.232, 'epoch': 0.4}
{'loss': 1.0046, 'grad_norm': 0.2380857765674591, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.088975191116333, 'eval_runtime': 3.289, 'eval_samples_per_second': 304.044, 'eval_steps_per_second': 19.155, 'epoch': 0.44}
{'loss': 0.9591, 'grad_norm': 0.230669304728508, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0973278284072876, 'eval_runtime': 3.292, 'eval_samples_per_second': 303.763, 'eval_steps_per_second': 19.137, 'epoch': 0.48}
{'loss': 0.9742, 'grad_norm': 0.20063039660453796, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0769531726837158, 'eval_runtime': 3.2876, 'eval_samples_per_second': 304.17, 'eval_steps_per_second': 19.163, 'epoch': 0.52}
{'loss': 0.9603, 'grad_norm': 0.20368729531764984, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1051745414733887, 'eval_runtime': 3.2874, 'eval_samples_per_second': 304.192, 'eval_steps_per_second': 19.164, 'epoch': 0.56}
{'loss': 0.9471, 'grad_norm': 0.26464492082595825, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0780352354049683, 'eval_runtime': 3.2945, 'eval_samples_per_second': 303.54, 'eval_steps_per_second': 19.123, 'epoch': 0.6}
{'loss': 0.9212, 'grad_norm': 0.23123033344745636, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0861707925796509, 'eval_runtime': 3.2962, 'eval_samples_per_second': 303.38, 'eval_steps_per_second': 19.113, 'epoch': 0.64}
{'loss': 0.9507, 'grad_norm': 0.20422199368476868, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0769264698028564, 'eval_runtime': 3.2887, 'eval_samples_per_second': 304.071, 'eval_steps_per_second': 19.156, 'epoch': 0.68}
{'loss': 0.928, 'grad_norm': 0.19746948778629303, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0674992799758911, 'eval_runtime': 3.2881, 'eval_samples_per_second': 304.13, 'eval_steps_per_second': 19.16, 'epoch': 0.72}
{'loss': 1.0167, 'grad_norm': 0.22994562983512878, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.078382968902588, 'eval_runtime': 3.2898, 'eval_samples_per_second': 303.968, 'eval_steps_per_second': 19.15, 'epoch': 0.76}
{'loss': 0.9402, 'grad_norm': 0.25303012132644653, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.067940354347229, 'eval_runtime': 3.2945, 'eval_samples_per_second': 303.536, 'eval_steps_per_second': 19.123, 'epoch': 0.8}
{'loss': 0.956, 'grad_norm': 0.2082059681415558, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0782808065414429, 'eval_runtime': 3.3108, 'eval_samples_per_second': 302.038, 'eval_steps_per_second': 19.028, 'epoch': 0.84}
{'loss': 0.9888, 'grad_norm': 0.1962936818599701, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0750287771224976, 'eval_runtime': 3.2881, 'eval_samples_per_second': 304.128, 'eval_steps_per_second': 19.16, 'epoch': 0.88}
{'loss': 0.9514, 'grad_norm': 0.22514034807682037, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0714948177337646, 'eval_runtime': 3.2758, 'eval_samples_per_second': 305.265, 'eval_steps_per_second': 19.232, 'epoch': 0.92}
{'loss': 0.9345, 'grad_norm': 0.2141362726688385, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0678030252456665, 'eval_runtime': 3.3011, 'eval_samples_per_second': 302.929, 'eval_steps_per_second': 19.084, 'epoch': 0.96}
{'loss': 0.9542, 'grad_norm': 0.27233320474624634, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0690475702285767, 'eval_runtime': 3.3024, 'eval_samples_per_second': 302.811, 'eval_steps_per_second': 19.077, 'epoch': 1.0}
{'train_runtime': 306.5488, 'train_samples_per_second': 32.608, 'train_steps_per_second': 2.039, 'train_loss': 1.1024189331054688, 'epoch': 1.0}
train_results:  {'eval_loss': [2.308180570602417, 1.5887399911880493, 1.3826090097427368, 1.2891008853912354, 1.253790020942688, 1.1731210947036743, 1.166227102279663, 1.1249865293502808, 1.105177879333496, 1.0996081829071045, 1.088975191116333, 1.0973278284072876, 1.0769531726837158, 1.1051745414733887, 1.0780352354049683, 1.0861707925796509, 1.0769264698028564, 1.0674992799758911, 1.078382968902588, 1.067940354347229, 1.0782808065414429, 1.0750287771224976, 1.0714948177337646, 1.0678030252456665, 1.0690475702285767], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.308180570602417, 1.5887399911880493, 1.3826090097427368, 1.2891008853912354, 1.253790020942688, 1.1731210947036743, 1.166227102279663, 1.1249865293502808, 1.105177879333496, 1.0996081829071045, 1.088975191116333, 1.0973278284072876, 1.0769531726837158, 1.1051745414733887, 1.0780352354049683, 1.0861707925796509, 1.0769264698028564, 1.0674992799758911, 1.078382968902588, 1.067940354347229, 1.0782808065414429, 1.0750287771224976, 1.0714948177337646, 1.0678030252456665, 1.0690475702285767]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.4995973110198975
current iteration best possible eval_loss (full train run):  -1.0690475702285767
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 32.5823 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -0.7939215454767581
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.762122973589056
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -0.7620200601690246
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -0.7620540888334724
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -0.7620540436685379
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0955, dtype=torch.float64), tensor(0.0164, dtype=torch.float64), 0, tensor(0.3881, dtype=torch.float64), tensor(0.1286, dtype=torch.float64), tensor(0.1692, dtype=torch.float64), tensor(0.0994, dtype=torch.float64), 0, tensor(0.0985, dtype=torch.float64), 5, 0, 0, 0, 1, 0, 30, 0.05400215848045482, 10.3000233254071, 0]
normalized proposed parameters for next round by BO: [tensor(0.0955, dtype=torch.float64), tensor(0.0164, dtype=torch.float64), tensor(7.3144e-19, dtype=torch.float64), tensor(0.3881, dtype=torch.float64), tensor(0.1286, dtype=torch.float64), tensor(0.1692, dtype=torch.float64), tensor(0.0994, dtype=torch.float64), tensor(0.0043, dtype=torch.float64), tensor(0.0985, dtype=torch.float64), tensor(0.1592, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2351, dtype=torch.float64), tensor(0.5400, dtype=torch.float64), tensor(0.2146, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.096
  gsm8k: 0.016
  rowan_hellaswag: 0
  sciq: 0.388
  triviaqa: 0.129
  truthfulqa_gen: 0.169
  wikitext: 0.099
  mmlu: 0
  arc_challenge: 0.098

LoRA Parameters:
  lora_r: (30,)
  lora_dropout: (0.05400215848045482,)
  num_layers_to_apply: (5,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (10.3000233254071,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  5
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  30
lora dropout:  0.05400215848045482
lora alpha:  10.3000233254071
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,764,800 || all params: 8,033,026,048 || trainable%: 0.0344
length of training data:  9953
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.6953, 'grad_norm': 1.3482881784439087, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.7371225357055664, 'eval_runtime': 5.1111, 'eval_samples_per_second': 195.654, 'eval_steps_per_second': 12.326, 'epoch': 0.04}
{'loss': 2.4144, 'grad_norm': 0.96637362241745, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7847082614898682, 'eval_runtime': 2.8708, 'eval_samples_per_second': 348.332, 'eval_steps_per_second': 21.945, 'epoch': 0.08}
{'loss': 1.5009, 'grad_norm': 0.4999479055404663, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.4183670282363892, 'eval_runtime': 2.8749, 'eval_samples_per_second': 347.836, 'eval_steps_per_second': 21.914, 'epoch': 0.12}
{'loss': 1.2924, 'grad_norm': 0.3421246409416199, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.4149209260940552, 'eval_runtime': 2.8821, 'eval_samples_per_second': 346.975, 'eval_steps_per_second': 21.859, 'epoch': 0.16}
{'loss': 1.308, 'grad_norm': 0.23695529997348785, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.3661086559295654, 'eval_runtime': 2.8804, 'eval_samples_per_second': 347.176, 'eval_steps_per_second': 21.872, 'epoch': 0.2}
{'loss': 1.2415, 'grad_norm': 0.3303523659706116, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.3420625925064087, 'eval_runtime': 2.8647, 'eval_samples_per_second': 349.071, 'eval_steps_per_second': 21.992, 'epoch': 0.24}
{'loss': 1.2462, 'grad_norm': 0.2831515967845917, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.3289570808410645, 'eval_runtime': 2.8638, 'eval_samples_per_second': 349.191, 'eval_steps_per_second': 21.999, 'epoch': 0.28}
{'loss': 1.1079, 'grad_norm': 0.27251580357551575, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.3215930461883545, 'eval_runtime': 2.8658, 'eval_samples_per_second': 348.944, 'eval_steps_per_second': 21.983, 'epoch': 0.32}
{'loss': 1.2492, 'grad_norm': 0.24966882169246674, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.3244917392730713, 'eval_runtime': 2.866, 'eval_samples_per_second': 348.92, 'eval_steps_per_second': 21.982, 'epoch': 0.36}
{'loss': 1.1929, 'grad_norm': 0.34358736872673035, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.3345847129821777, 'eval_runtime': 2.8747, 'eval_samples_per_second': 347.868, 'eval_steps_per_second': 21.916, 'epoch': 0.4}
{'loss': 1.1955, 'grad_norm': 0.3217976987361908, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.3149632215499878, 'eval_runtime': 2.8784, 'eval_samples_per_second': 347.421, 'eval_steps_per_second': 21.888, 'epoch': 0.44}
{'loss': 1.1706, 'grad_norm': 0.310558021068573, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.3076220750808716, 'eval_runtime': 2.8865, 'eval_samples_per_second': 346.441, 'eval_steps_per_second': 21.826, 'epoch': 0.48}
{'loss': 1.1083, 'grad_norm': 0.30704545974731445, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.2919018268585205, 'eval_runtime': 2.91, 'eval_samples_per_second': 343.648, 'eval_steps_per_second': 21.65, 'epoch': 0.52}
{'loss': 1.168, 'grad_norm': 0.302055686712265, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.2886466979980469, 'eval_runtime': 2.8828, 'eval_samples_per_second': 346.885, 'eval_steps_per_second': 21.854, 'epoch': 0.56}
{'loss': 1.153, 'grad_norm': 0.2776108682155609, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.291061282157898, 'eval_runtime': 2.8932, 'eval_samples_per_second': 345.638, 'eval_steps_per_second': 21.775, 'epoch': 0.6}
{'loss': 1.0843, 'grad_norm': 0.2910671532154083, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 1.2825309038162231, 'eval_runtime': 2.8946, 'eval_samples_per_second': 345.473, 'eval_steps_per_second': 21.765, 'epoch': 0.64}
{'loss': 1.1634, 'grad_norm': 0.2910545766353607, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 1.2807331085205078, 'eval_runtime': 2.899, 'eval_samples_per_second': 344.944, 'eval_steps_per_second': 21.731, 'epoch': 0.68}
{'loss': 1.1331, 'grad_norm': 0.42068004608154297, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 1.276295781135559, 'eval_runtime': 2.9066, 'eval_samples_per_second': 344.043, 'eval_steps_per_second': 21.675, 'epoch': 0.72}
{'loss': 1.1704, 'grad_norm': 0.3533380329608917, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 1.2746050357818604, 'eval_runtime': 2.9024, 'eval_samples_per_second': 344.539, 'eval_steps_per_second': 21.706, 'epoch': 0.76}
{'loss': 1.1261, 'grad_norm': 0.3124324679374695, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 1.2851425409317017, 'eval_runtime': 2.9023, 'eval_samples_per_second': 344.56, 'eval_steps_per_second': 21.707, 'epoch': 0.8}
{'loss': 1.1468, 'grad_norm': 0.31183817982673645, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 1.2699013948440552, 'eval_runtime': 2.9015, 'eval_samples_per_second': 344.648, 'eval_steps_per_second': 21.713, 'epoch': 0.84}
{'loss': 1.1544, 'grad_norm': 0.32071658968925476, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 1.266540288925171, 'eval_runtime': 2.8971, 'eval_samples_per_second': 345.172, 'eval_steps_per_second': 21.746, 'epoch': 0.88}
{'loss': 1.1173, 'grad_norm': 0.27856162190437317, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 1.2704790830612183, 'eval_runtime': 2.9038, 'eval_samples_per_second': 344.371, 'eval_steps_per_second': 21.695, 'epoch': 0.92}
{'loss': 1.0894, 'grad_norm': 0.47153690457344055, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 1.2842663526535034, 'eval_runtime': 2.888, 'eval_samples_per_second': 346.258, 'eval_steps_per_second': 21.814, 'epoch': 0.96}
{'train_runtime': 145.7808, 'train_samples_per_second': 68.274, 'train_steps_per_second': 4.274, 'train_loss': 1.3771545691819291, 'epoch': 1.0}
train_results:  {'eval_loss': [3.7371225357055664, 1.7847082614898682, 1.4183670282363892, 1.4149209260940552, 1.3661086559295654, 1.3420625925064087, 1.3289570808410645, 1.3215930461883545, 1.3244917392730713, 1.3345847129821777, 1.3149632215499878, 1.3076220750808716, 1.2919018268585205, 1.2886466979980469, 1.291061282157898, 1.2825309038162231, 1.2807331085205078, 1.276295781135559, 1.2746050357818604, 1.2851425409317017, 1.2699013948440552, 1.266540288925171, 1.2704790830612183, 1.2842663526535034], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [3.7371225357055664, 1.7847082614898682, 1.4183670282363892, 1.4149209260940552, 1.3661086559295654, 1.3420625925064087, 1.3289570808410645, 1.3215930461883545, 1.3244917392730713, 1.3345847129821777, 1.3149632215499878, 1.3076220750808716, 1.2919018268585205, 1.2886466979980469, 1.291061282157898, 1.2825309038162231, 1.2807331085205078, 1.276295781135559, 1.2746050357818604, 1.2851425409317017, 1.2699013948440552, 1.266540288925171, 1.2704790830612183, 1.2842663526535034]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.002169609069824
current iteration best possible eval_loss (full train run):  -1.2842663526535034
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.5810 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.769386351108551, 0.48460155725479126, 0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.871427059173584, 0.9379879832267761, 0.9101236462593079, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.43461495637893677, 0.05005854368209839, 0.09836432337760925, 0.9400144815444946]  ‚Üí  acq = -0.3538027603798608
X = [0.9526360034942627, 0.8012327551841736, 0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5360714197158813, 0.12353461980819702, 0.3528776168823242, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5302047729492188, 0.31215590238571167, 0.6163817644119263, 0.9380749464035034]  ‚Üí  acq = -0.6159708437188569
X = [0.34730666875839233, 0.06246674060821533, 0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3383713960647583, 0.12947320938110352, 0.50956130027771, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.48614978790283203, 0.43715083599090576, 0.6119240522384644, 0.47161221504211426]  ‚Üí  acq = -0.49502859775856556
X = [0.45309585332870483, 0.0678371787071228, 0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7435163855552673, 0.12291663885116577, 0.4224488139152527, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.23402123153209686, 0.37334394454956055, 0.5466699600219727, 0.06654441356658936]  ‚Üí  acq = -0.6159708437186646
X = [0.18636363744735718, 0.6823987364768982, 0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9869434833526611, 0.8804963827133179, 0.9389960169792175, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.33730313181877136, 0.484236478805542, 0.44418343901634216, 0.7705504298210144]  ‚Üí  acq = -0.6159708449259844
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3233, dtype=torch.float64), tensor(0.1283, dtype=torch.float64), 0, 0, 0, tensor(0.3787, dtype=torch.float64), tensor(0.0704, dtype=torch.float64), 0, tensor(0.0994, dtype=torch.float64), 22, 0, 0, 0, 1, 1, 2, 2.7704328601991323e-18, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.3233, dtype=torch.float64), tensor(0.1283, dtype=torch.float64), tensor(1.6243e-20, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7640e-17, dtype=torch.float64), tensor(0.3787, dtype=torch.float64), tensor(0.0704, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0994, dtype=torch.float64), tensor(0.6935, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(2.7704e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.323
  gsm8k: 0.128
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.379
  wikitext: 0.07
  mmlu: 0
  arc_challenge: 0.099

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (2.7704328601991323e-18,)
  num_layers_to_apply: (22,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  22
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  2.7704328601991323e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,622,016 || all params: 8,031,883,264 || trainable%: 0.0202
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8683, 'grad_norm': 5.258729457855225, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9196537733078003, 'eval_runtime': 5.0576, 'eval_samples_per_second': 197.72, 'eval_steps_per_second': 12.456, 'epoch': 0.04}
{'loss': 1.117, 'grad_norm': 1.6254832744598389, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7865049839019775, 'eval_runtime': 3.1467, 'eval_samples_per_second': 317.791, 'eval_steps_per_second': 20.021, 'epoch': 0.08}
{'loss': 0.9668, 'grad_norm': 1.3826556205749512, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8467607498168945, 'eval_runtime': 3.1494, 'eval_samples_per_second': 317.518, 'eval_steps_per_second': 20.004, 'epoch': 0.12}
{'loss': 0.935, 'grad_norm': 1.6573245525360107, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8524364233016968, 'eval_runtime': 3.152, 'eval_samples_per_second': 317.263, 'eval_steps_per_second': 19.988, 'epoch': 0.16}
{'loss': 0.9249, 'grad_norm': 1.5460835695266724, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8646892309188843, 'eval_runtime': 3.1507, 'eval_samples_per_second': 317.387, 'eval_steps_per_second': 19.995, 'epoch': 0.2}
{'loss': 0.899, 'grad_norm': 1.9720470905303955, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8130311965942383, 'eval_runtime': 3.1469, 'eval_samples_per_second': 317.776, 'eval_steps_per_second': 20.02, 'epoch': 0.24}
{'loss': 0.8565, 'grad_norm': 1.4438735246658325, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8705707788467407, 'eval_runtime': 3.1491, 'eval_samples_per_second': 317.553, 'eval_steps_per_second': 20.006, 'epoch': 0.28}
{'loss': 0.8206, 'grad_norm': 1.3090906143188477, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9064747095108032, 'eval_runtime': 3.1512, 'eval_samples_per_second': 317.34, 'eval_steps_per_second': 19.992, 'epoch': 0.32}
{'loss': 0.8393, 'grad_norm': 1.544248342514038, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7322250604629517, 'eval_runtime': 3.1545, 'eval_samples_per_second': 317.008, 'eval_steps_per_second': 19.971, 'epoch': 0.36}
{'loss': 0.8568, 'grad_norm': 1.54917311668396, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8043859004974365, 'eval_runtime': 3.1636, 'eval_samples_per_second': 316.093, 'eval_steps_per_second': 19.914, 'epoch': 0.4}
{'loss': 0.8535, 'grad_norm': 1.2524877786636353, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.903079867362976, 'eval_runtime': 3.1535, 'eval_samples_per_second': 317.107, 'eval_steps_per_second': 19.978, 'epoch': 0.44}
{'loss': 0.8686, 'grad_norm': 1.604461431503296, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8172216415405273, 'eval_runtime': 3.1581, 'eval_samples_per_second': 316.642, 'eval_steps_per_second': 19.948, 'epoch': 0.48}
{'loss': 0.8401, 'grad_norm': 1.5161341428756714, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9481149911880493, 'eval_runtime': 3.157, 'eval_samples_per_second': 316.756, 'eval_steps_per_second': 19.956, 'epoch': 0.52}
{'loss': 0.7971, 'grad_norm': 1.249213695526123, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.928363561630249, 'eval_runtime': 3.1644, 'eval_samples_per_second': 316.017, 'eval_steps_per_second': 19.909, 'epoch': 0.56}
{'loss': 0.8089, 'grad_norm': 1.8110922574996948, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9546525478363037, 'eval_runtime': 3.1709, 'eval_samples_per_second': 315.364, 'eval_steps_per_second': 19.868, 'epoch': 0.6}
{'loss': 0.7348, 'grad_norm': 1.2154383659362793, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9273518323898315, 'eval_runtime': 3.1749, 'eval_samples_per_second': 314.966, 'eval_steps_per_second': 19.843, 'epoch': 0.64}
{'loss': 0.7984, 'grad_norm': 1.2347630262374878, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9567979574203491, 'eval_runtime': 3.1835, 'eval_samples_per_second': 314.121, 'eval_steps_per_second': 19.79, 'epoch': 0.68}
{'loss': 0.7677, 'grad_norm': 1.3916904926300049, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.926953911781311, 'eval_runtime': 3.2182, 'eval_samples_per_second': 310.736, 'eval_steps_per_second': 19.576, 'epoch': 0.72}
{'loss': 0.801, 'grad_norm': 1.426712989807129, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.967748999595642, 'eval_runtime': 3.2313, 'eval_samples_per_second': 309.472, 'eval_steps_per_second': 19.497, 'epoch': 0.76}
{'loss': 0.7734, 'grad_norm': 1.356324315071106, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9907493591308594, 'eval_runtime': 3.2327, 'eval_samples_per_second': 309.343, 'eval_steps_per_second': 19.489, 'epoch': 0.8}
{'loss': 0.7717, 'grad_norm': 1.4800117015838623, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9820976257324219, 'eval_runtime': 3.201, 'eval_samples_per_second': 312.402, 'eval_steps_per_second': 19.681, 'epoch': 0.84}
{'loss': 0.7516, 'grad_norm': 1.5422357320785522, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.046969175338745, 'eval_runtime': 3.226, 'eval_samples_per_second': 309.979, 'eval_steps_per_second': 19.529, 'epoch': 0.88}
{'loss': 0.7667, 'grad_norm': 1.439971923828125, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0189621448516846, 'eval_runtime': 3.2193, 'eval_samples_per_second': 310.629, 'eval_steps_per_second': 19.57, 'epoch': 0.92}
{'loss': 0.7263, 'grad_norm': 1.50010347366333, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.032564401626587, 'eval_runtime': 3.2241, 'eval_samples_per_second': 310.163, 'eval_steps_per_second': 19.54, 'epoch': 0.96}
{'loss': 0.7527, 'grad_norm': 1.4951461553573608, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.0383248329162598, 'eval_runtime': 3.2134, 'eval_samples_per_second': 311.201, 'eval_steps_per_second': 19.606, 'epoch': 1.0}
{'train_runtime': 242.0951, 'train_samples_per_second': 41.294, 'train_steps_per_second': 2.582, 'train_loss': 0.9158734924316406, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9196537733078003, 1.7865049839019775, 1.8467607498168945, 1.8524364233016968, 1.8646892309188843, 1.8130311965942383, 1.8705707788467407, 1.9064747095108032, 1.7322250604629517, 1.8043859004974365, 1.903079867362976, 1.8172216415405273, 1.9481149911880493, 1.928363561630249, 1.9546525478363037, 1.9273518323898315, 1.9567979574203491, 1.926953911781311, 1.967748999595642, 1.9907493591308594, 1.9820976257324219, 2.046969175338745, 2.0189621448516846, 2.032564401626587, 2.0383248329162598], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9196537733078003, 1.7865049839019775, 1.8467607498168945, 1.8524364233016968, 1.8646892309188843, 1.8130311965942383, 1.8705707788467407, 1.9064747095108032, 1.7322250604629517, 1.8043859004974365, 1.903079867362976, 1.8172216415405273, 1.9481149911880493, 1.928363561630249, 1.9546525478363037, 1.9273518323898315, 1.9567979574203491, 1.926953911781311, 1.967748999595642, 1.9907493591308594, 1.9820976257324219, 2.046969175338745, 2.0189621448516846, 2.032564401626587, 2.0383248329162598]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.9748255014419556
current iteration best possible eval_loss (full train run):  -2.0383248329162598
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.7625 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.5634870483117644
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -0.8550631346323885
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.9567680056994179
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -0.8553186598239106
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -0.8550631318871451
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1019, dtype=torch.float64), tensor(0.0503, dtype=torch.float64), tensor(0.1129, dtype=torch.float64), tensor(0.0864, dtype=torch.float64), tensor(0.0449, dtype=torch.float64), tensor(0.2978, dtype=torch.float64), tensor(0.0597, dtype=torch.float64), tensor(0.2462, dtype=torch.float64), 0, 10, 1, 0, 0, 1, 1, 2, 0.08570145488691097, 39.26682617118926, 0]
normalized proposed parameters for next round by BO: [tensor(0.1019, dtype=torch.float64), tensor(0.0503, dtype=torch.float64), tensor(0.1129, dtype=torch.float64), tensor(0.0864, dtype=torch.float64), tensor(0.0449, dtype=torch.float64), tensor(0.2978, dtype=torch.float64), tensor(0.0597, dtype=torch.float64), tensor(0.2462, dtype=torch.float64), tensor(1.5787e-16, dtype=torch.float64), tensor(0.3281, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.8570, dtype=torch.float64), tensor(0.8181, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.102
  gsm8k: 0.05
  rowan_hellaswag: 0.113
  sciq: 0.086
  triviaqa: 0.045
  truthfulqa_gen: 0.298
  wikitext: 0.06
  mmlu: 0.246
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.08570145488691097,)
  num_layers_to_apply: (10,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (39.26682617118926,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  10
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.08570145488691097
lora alpha:  39.26682617118926
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 901,120 || all params: 8,031,162,368 || trainable%: 0.0112
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4446, 'grad_norm': 7.39769983291626, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2430663108825684, 'eval_runtime': 4.2194, 'eval_samples_per_second': 237.002, 'eval_steps_per_second': 14.931, 'epoch': 0.04}
{'loss': 1.8663, 'grad_norm': 2.44535756111145, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4347434043884277, 'eval_runtime': 3.0119, 'eval_samples_per_second': 332.016, 'eval_steps_per_second': 20.917, 'epoch': 0.08}
{'loss': 1.5617, 'grad_norm': 1.8262584209442139, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3120616674423218, 'eval_runtime': 2.998, 'eval_samples_per_second': 333.552, 'eval_steps_per_second': 21.014, 'epoch': 0.12}
{'loss': 1.41, 'grad_norm': 1.1899163722991943, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.332021951675415, 'eval_runtime': 3.016, 'eval_samples_per_second': 331.561, 'eval_steps_per_second': 20.888, 'epoch': 0.16}
{'loss': 1.3686, 'grad_norm': 1.4941785335540771, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2826539278030396, 'eval_runtime': 3.0258, 'eval_samples_per_second': 330.488, 'eval_steps_per_second': 20.821, 'epoch': 0.2}
{'loss': 1.3761, 'grad_norm': 1.3971729278564453, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3235522508621216, 'eval_runtime': 3.0239, 'eval_samples_per_second': 330.701, 'eval_steps_per_second': 20.834, 'epoch': 0.24}
{'loss': 1.3024, 'grad_norm': 1.284017562866211, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2602027654647827, 'eval_runtime': 3.0269, 'eval_samples_per_second': 330.371, 'eval_steps_per_second': 20.813, 'epoch': 0.28}
{'loss': 1.2794, 'grad_norm': 1.2803329229354858, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3089808225631714, 'eval_runtime': 3.0259, 'eval_samples_per_second': 330.477, 'eval_steps_per_second': 20.82, 'epoch': 0.32}
{'loss': 1.3412, 'grad_norm': 1.4193451404571533, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2847098112106323, 'eval_runtime': 3.0312, 'eval_samples_per_second': 329.897, 'eval_steps_per_second': 20.784, 'epoch': 0.36}
{'loss': 1.367, 'grad_norm': 1.2279361486434937, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2902333736419678, 'eval_runtime': 3.0337, 'eval_samples_per_second': 329.627, 'eval_steps_per_second': 20.766, 'epoch': 0.4}
{'loss': 1.31, 'grad_norm': 1.4246481657028198, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.274850845336914, 'eval_runtime': 3.0351, 'eval_samples_per_second': 329.479, 'eval_steps_per_second': 20.757, 'epoch': 0.44}
{'loss': 1.2977, 'grad_norm': 1.2447341680526733, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2837331295013428, 'eval_runtime': 3.0382, 'eval_samples_per_second': 329.137, 'eval_steps_per_second': 20.736, 'epoch': 0.48}
{'loss': 1.2659, 'grad_norm': 1.3764064311981201, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.292535662651062, 'eval_runtime': 3.0416, 'eval_samples_per_second': 328.778, 'eval_steps_per_second': 20.713, 'epoch': 0.52}
{'loss': 1.3122, 'grad_norm': 1.2358742952346802, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2636622190475464, 'eval_runtime': 3.0373, 'eval_samples_per_second': 329.238, 'eval_steps_per_second': 20.742, 'epoch': 0.56}
{'loss': 1.2711, 'grad_norm': 1.5349757671356201, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.275457739830017, 'eval_runtime': 3.0365, 'eval_samples_per_second': 329.327, 'eval_steps_per_second': 20.748, 'epoch': 0.6}
{'loss': 1.331, 'grad_norm': 1.1857645511627197, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2753961086273193, 'eval_runtime': 3.0368, 'eval_samples_per_second': 329.296, 'eval_steps_per_second': 20.746, 'epoch': 0.64}
{'loss': 1.2714, 'grad_norm': 1.2541557550430298, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2640620470046997, 'eval_runtime': 3.0361, 'eval_samples_per_second': 329.368, 'eval_steps_per_second': 20.75, 'epoch': 0.68}
{'loss': 1.2084, 'grad_norm': 1.437915325164795, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.268892526626587, 'eval_runtime': 3.0437, 'eval_samples_per_second': 328.543, 'eval_steps_per_second': 20.698, 'epoch': 0.72}
{'loss': 1.2992, 'grad_norm': 1.6022727489471436, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2627689838409424, 'eval_runtime': 3.0373, 'eval_samples_per_second': 329.24, 'eval_steps_per_second': 20.742, 'epoch': 0.76}
{'loss': 1.2631, 'grad_norm': 1.1648176908493042, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2591749429702759, 'eval_runtime': 3.0456, 'eval_samples_per_second': 328.341, 'eval_steps_per_second': 20.685, 'epoch': 0.8}
{'loss': 1.2127, 'grad_norm': 1.1943583488464355, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.248923897743225, 'eval_runtime': 3.0524, 'eval_samples_per_second': 327.608, 'eval_steps_per_second': 20.639, 'epoch': 0.84}
{'loss': 1.2394, 'grad_norm': 1.3233931064605713, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2673832178115845, 'eval_runtime': 3.0586, 'eval_samples_per_second': 326.946, 'eval_steps_per_second': 20.598, 'epoch': 0.88}
{'loss': 1.2146, 'grad_norm': 1.3594061136245728, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2648847103118896, 'eval_runtime': 3.061, 'eval_samples_per_second': 326.686, 'eval_steps_per_second': 20.581, 'epoch': 0.92}
{'loss': 1.2722, 'grad_norm': 1.287123680114746, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2530008554458618, 'eval_runtime': 3.0533, 'eval_samples_per_second': 327.514, 'eval_steps_per_second': 20.633, 'epoch': 0.96}
{'loss': 1.2383, 'grad_norm': 1.817795753479004, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2530968189239502, 'eval_runtime': 3.0577, 'eval_samples_per_second': 327.045, 'eval_steps_per_second': 20.604, 'epoch': 1.0}
{'train_runtime': 214.1234, 'train_samples_per_second': 46.679, 'train_steps_per_second': 2.919, 'train_loss': 1.4129776275634767, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2430663108825684, 1.4347434043884277, 1.3120616674423218, 1.332021951675415, 1.2826539278030396, 1.3235522508621216, 1.2602027654647827, 1.3089808225631714, 1.2847098112106323, 1.2902333736419678, 1.274850845336914, 1.2837331295013428, 1.292535662651062, 1.2636622190475464, 1.275457739830017, 1.2753961086273193, 1.2640620470046997, 1.268892526626587, 1.2627689838409424, 1.2591749429702759, 1.248923897743225, 1.2673832178115845, 1.2648847103118896, 1.2530008554458618, 1.2530968189239502], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2430663108825684, 1.4347434043884277, 1.3120616674423218, 1.332021951675415, 1.2826539278030396, 1.3235522508621216, 1.2602027654647827, 1.3089808225631714, 1.2847098112106323, 1.2902333736419678, 1.274850845336914, 1.2837331295013428, 1.292535662651062, 1.2636622190475464, 1.275457739830017, 1.2753961086273193, 1.2640620470046997, 1.268892526626587, 1.2627689838409424, 1.2591749429702759, 1.248923897743225, 1.2673832178115845, 1.2648847103118896, 1.2530008554458618, 1.2530968189239502]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.6635209321975708
current iteration best possible eval_loss (full train run):  -1.2530968189239502
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.2362 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -0.7552401047624535
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -0.7552424655541208
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -0.755239912700159
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -0.7622575390854978
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.755749251665462
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0549, dtype=torch.float64), tensor(0.1031, dtype=torch.float64), tensor(0.1963, dtype=torch.float64), tensor(0.0728, dtype=torch.float64), 0, 0, tensor(0.3547, dtype=torch.float64), tensor(0.2181, dtype=torch.float64), 32, 0, 0, 1, 1, 1, 16, 0.009435433931352431, 37.91527928248978, 1]
normalized proposed parameters for next round by BO: [tensor(3.6223e-18, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), tensor(0.1031, dtype=torch.float64), tensor(0.1963, dtype=torch.float64), tensor(0.0728, dtype=torch.float64), tensor(2.3519e-18, dtype=torch.float64), tensor(1.7677e-17, dtype=torch.float64), tensor(0.3547, dtype=torch.float64), tensor(0.2181, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1224, dtype=torch.float64), tensor(0.0944, dtype=torch.float64), tensor(0.7899, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.055
  rowan_hellaswag: 0.103
  sciq: 0.196
  triviaqa: 0.073
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.355
  arc_challenge: 0.218

LoRA Parameters:
  lora_r: (16,)
  lora_dropout: (0.009435433931352431,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (37.91527928248978,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  16
lora dropout:  0.009435433931352431
lora alpha:  37.91527928248978
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 28,311,552 || all params: 8,058,572,800 || trainable%: 0.3513
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7464, 'grad_norm': 1.1739537715911865, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5237067937850952, 'eval_runtime': 3.4735, 'eval_samples_per_second': 287.898, 'eval_steps_per_second': 18.138, 'epoch': 0.04}
{'loss': 1.3817, 'grad_norm': 1.312366008758545, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2036374807357788, 'eval_runtime': 3.4722, 'eval_samples_per_second': 287.998, 'eval_steps_per_second': 18.144, 'epoch': 0.08}
{'loss': 1.1616, 'grad_norm': 0.5564138293266296, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1492198705673218, 'eval_runtime': 3.4618, 'eval_samples_per_second': 288.865, 'eval_steps_per_second': 18.199, 'epoch': 0.12}
{'loss': 1.1816, 'grad_norm': 0.5555304288864136, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1122252941131592, 'eval_runtime': 3.4743, 'eval_samples_per_second': 287.827, 'eval_steps_per_second': 18.133, 'epoch': 0.16}
{'loss': 1.179, 'grad_norm': 0.5033987164497375, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.081607460975647, 'eval_runtime': 3.478, 'eval_samples_per_second': 287.518, 'eval_steps_per_second': 18.114, 'epoch': 0.2}
{'loss': 1.1842, 'grad_norm': 0.5244759917259216, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.089544653892517, 'eval_runtime': 3.4857, 'eval_samples_per_second': 286.883, 'eval_steps_per_second': 18.074, 'epoch': 0.24}
{'loss': 1.1388, 'grad_norm': 0.5097399950027466, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0932987928390503, 'eval_runtime': 3.4793, 'eval_samples_per_second': 287.411, 'eval_steps_per_second': 18.107, 'epoch': 0.28}
{'loss': 1.1691, 'grad_norm': 0.49448513984680176, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0800659656524658, 'eval_runtime': 3.4741, 'eval_samples_per_second': 287.846, 'eval_steps_per_second': 18.134, 'epoch': 0.32}
{'loss': 1.1305, 'grad_norm': 0.51981520652771, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0730090141296387, 'eval_runtime': 3.4685, 'eval_samples_per_second': 288.31, 'eval_steps_per_second': 18.164, 'epoch': 0.36}
{'loss': 1.1267, 'grad_norm': 0.5660609006881714, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.067163348197937, 'eval_runtime': 3.4784, 'eval_samples_per_second': 287.49, 'eval_steps_per_second': 18.112, 'epoch': 0.4}
{'loss': 1.0959, 'grad_norm': 0.5423875451087952, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.067524790763855, 'eval_runtime': 3.4957, 'eval_samples_per_second': 286.069, 'eval_steps_per_second': 18.022, 'epoch': 0.44}
{'loss': 1.1384, 'grad_norm': 0.7531328201293945, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0697766542434692, 'eval_runtime': 3.4894, 'eval_samples_per_second': 286.579, 'eval_steps_per_second': 18.054, 'epoch': 0.48}
{'loss': 1.1474, 'grad_norm': 0.6024965047836304, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0667380094528198, 'eval_runtime': 3.4935, 'eval_samples_per_second': 286.247, 'eval_steps_per_second': 18.034, 'epoch': 0.52}
{'loss': 1.1098, 'grad_norm': 0.5033653378486633, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0597223043441772, 'eval_runtime': 3.4784, 'eval_samples_per_second': 287.491, 'eval_steps_per_second': 18.112, 'epoch': 0.56}
{'loss': 1.1535, 'grad_norm': 0.7271779775619507, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.057285189628601, 'eval_runtime': 3.5158, 'eval_samples_per_second': 284.431, 'eval_steps_per_second': 17.919, 'epoch': 0.6}
{'loss': 1.1157, 'grad_norm': 0.5418503880500793, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.060770034790039, 'eval_runtime': 3.4747, 'eval_samples_per_second': 287.795, 'eval_steps_per_second': 18.131, 'epoch': 0.64}
{'loss': 1.0647, 'grad_norm': 0.7812848687171936, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.051500678062439, 'eval_runtime': 3.479, 'eval_samples_per_second': 287.439, 'eval_steps_per_second': 18.109, 'epoch': 0.68}
{'loss': 1.0904, 'grad_norm': 0.7492527961730957, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.059250831604004, 'eval_runtime': 3.4717, 'eval_samples_per_second': 288.04, 'eval_steps_per_second': 18.147, 'epoch': 0.72}
{'loss': 1.0646, 'grad_norm': 0.6121591925621033, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0523288249969482, 'eval_runtime': 3.4734, 'eval_samples_per_second': 287.901, 'eval_steps_per_second': 18.138, 'epoch': 0.76}
{'loss': 1.1158, 'grad_norm': 0.5986754894256592, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0496811866760254, 'eval_runtime': 3.4793, 'eval_samples_per_second': 287.411, 'eval_steps_per_second': 18.107, 'epoch': 0.8}
{'loss': 1.045, 'grad_norm': 0.6262965798377991, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0502277612686157, 'eval_runtime': 3.4776, 'eval_samples_per_second': 287.552, 'eval_steps_per_second': 18.116, 'epoch': 0.84}
{'loss': 1.1446, 'grad_norm': 0.7644350528717041, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0483899116516113, 'eval_runtime': 3.4806, 'eval_samples_per_second': 287.306, 'eval_steps_per_second': 18.1, 'epoch': 0.88}
{'loss': 1.0357, 'grad_norm': 0.5369671583175659, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0462100505828857, 'eval_runtime': 3.4903, 'eval_samples_per_second': 286.505, 'eval_steps_per_second': 18.05, 'epoch': 0.92}
{'loss': 1.0179, 'grad_norm': 0.576978862285614, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0452150106430054, 'eval_runtime': 3.4742, 'eval_samples_per_second': 287.834, 'eval_steps_per_second': 18.134, 'epoch': 0.96}
{'loss': 0.9713, 'grad_norm': 0.8569660186767578, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.044002890586853, 'eval_runtime': 3.4779, 'eval_samples_per_second': 287.53, 'eval_steps_per_second': 18.114, 'epoch': 1.0}
{'train_runtime': 323.1073, 'train_samples_per_second': 30.94, 'train_steps_per_second': 1.934, 'train_loss': 1.1884033905029296, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5237067937850952, 1.2036374807357788, 1.1492198705673218, 1.1122252941131592, 1.081607460975647, 1.089544653892517, 1.0932987928390503, 1.0800659656524658, 1.0730090141296387, 1.067163348197937, 1.067524790763855, 1.0697766542434692, 1.0667380094528198, 1.0597223043441772, 1.057285189628601, 1.060770034790039, 1.051500678062439, 1.059250831604004, 1.0523288249969482, 1.0496811866760254, 1.0502277612686157, 1.0483899116516113, 1.0462100505828857, 1.0452150106430054, 1.044002890586853], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5237067937850952, 1.2036374807357788, 1.1492198705673218, 1.1122252941131592, 1.081607460975647, 1.089544653892517, 1.0932987928390503, 1.0800659656524658, 1.0730090141296387, 1.067163348197937, 1.067524790763855, 1.0697766542434692, 1.0667380094528198, 1.0597223043441772, 1.057285189628601, 1.060770034790039, 1.051500678062439, 1.059250831604004, 1.0523288249969482, 1.0496811866760254, 1.0502277612686157, 1.0483899116516113, 1.0462100505828857, 1.0452150106430054, 1.044002890586853]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.4318766593933105
current iteration best possible eval_loss (full train run):  -1.044002890586853
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708, -2.4318766593933105]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 17.2017 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -0.9294937464788082
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -0.9294937464778215
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -0.9294937464781718
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -0.929493746477821
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -0.9277274318763893
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0932, dtype=torch.float64), tensor(0.2318, dtype=torch.float64), 0, tensor(0.1364, dtype=torch.float64), tensor(0.0752, dtype=torch.float64), tensor(0.0477, dtype=torch.float64), tensor(0.0919, dtype=torch.float64), tensor(0.0998, dtype=torch.float64), tensor(0.2241, dtype=torch.float64), 13, 1, 0, 1, 1, 1, 53, 0.0709688333378147, 45.96024247538632, 0]
normalized proposed parameters for next round by BO: [tensor(0.0932, dtype=torch.float64), tensor(0.2318, dtype=torch.float64), tensor(7.8470e-20, dtype=torch.float64), tensor(0.1364, dtype=torch.float64), tensor(0.0752, dtype=torch.float64), tensor(0.0477, dtype=torch.float64), tensor(0.0919, dtype=torch.float64), tensor(0.0998, dtype=torch.float64), tensor(0.2241, dtype=torch.float64), tensor(0.4125, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4175, dtype=torch.float64), tensor(0.7097, dtype=torch.float64), tensor(0.9575, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.093
  gsm8k: 0.232
  rowan_hellaswag: 0
  sciq: 0.136
  triviaqa: 0.075
  truthfulqa_gen: 0.048
  wikitext: 0.092
  mmlu: 0.1
  arc_challenge: 0.224

LoRA Parameters:
  lora_r: (53,)
  lora_dropout: (0.0709688333378147,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (45.96024247538632,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  53
lora dropout:  0.0709688333378147
lora alpha:  45.96024247538632
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 43,743,232 || all params: 8,074,004,480 || trainable%: 0.5418
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7065, 'grad_norm': 1.0764697790145874, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.015214681625366, 'eval_runtime': 3.2611, 'eval_samples_per_second': 306.643, 'eval_steps_per_second': 19.318, 'epoch': 0.04}
{'loss': 1.2614, 'grad_norm': 0.38136136531829834, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4211376905441284, 'eval_runtime': 3.2847, 'eval_samples_per_second': 304.44, 'eval_steps_per_second': 19.18, 'epoch': 0.08}
{'loss': 1.1338, 'grad_norm': 0.31660303473472595, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2758971452713013, 'eval_runtime': 3.2596, 'eval_samples_per_second': 306.788, 'eval_steps_per_second': 19.328, 'epoch': 0.12}
{'loss': 1.0846, 'grad_norm': 0.2668006718158722, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2454370260238647, 'eval_runtime': 3.2464, 'eval_samples_per_second': 308.033, 'eval_steps_per_second': 19.406, 'epoch': 0.16}
{'loss': 1.0686, 'grad_norm': 0.2900541126728058, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2458456754684448, 'eval_runtime': 3.2611, 'eval_samples_per_second': 306.647, 'eval_steps_per_second': 19.319, 'epoch': 0.2}
{'loss': 1.032, 'grad_norm': 0.3158426284790039, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2629035711288452, 'eval_runtime': 3.2784, 'eval_samples_per_second': 305.029, 'eval_steps_per_second': 19.217, 'epoch': 0.24}
{'loss': 1.0754, 'grad_norm': 0.2728867530822754, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2122926712036133, 'eval_runtime': 3.299, 'eval_samples_per_second': 303.119, 'eval_steps_per_second': 19.096, 'epoch': 0.28}
{'loss': 1.0346, 'grad_norm': 0.3138678967952728, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1957290172576904, 'eval_runtime': 3.3014, 'eval_samples_per_second': 302.905, 'eval_steps_per_second': 19.083, 'epoch': 0.32}
{'loss': 1.0521, 'grad_norm': 0.278656929731369, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2154122591018677, 'eval_runtime': 3.2964, 'eval_samples_per_second': 303.362, 'eval_steps_per_second': 19.112, 'epoch': 0.36}
{'loss': 1.0001, 'grad_norm': 0.2698063850402832, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2025580406188965, 'eval_runtime': 3.2624, 'eval_samples_per_second': 306.523, 'eval_steps_per_second': 19.311, 'epoch': 0.4}
{'loss': 1.0564, 'grad_norm': 0.3166659474372864, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1808161735534668, 'eval_runtime': 3.2557, 'eval_samples_per_second': 307.157, 'eval_steps_per_second': 19.351, 'epoch': 0.44}
{'loss': 0.9838, 'grad_norm': 0.26845985651016235, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2050060033798218, 'eval_runtime': 3.2607, 'eval_samples_per_second': 306.685, 'eval_steps_per_second': 19.321, 'epoch': 0.48}
{'loss': 1.001, 'grad_norm': 0.37642940878868103, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1833997964859009, 'eval_runtime': 3.2567, 'eval_samples_per_second': 307.056, 'eval_steps_per_second': 19.345, 'epoch': 0.52}
{'loss': 1.0543, 'grad_norm': 0.318389892578125, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.176444411277771, 'eval_runtime': 3.2589, 'eval_samples_per_second': 306.855, 'eval_steps_per_second': 19.332, 'epoch': 0.56}
{'loss': 1.0138, 'grad_norm': 0.38217926025390625, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1669059991836548, 'eval_runtime': 3.2621, 'eval_samples_per_second': 306.552, 'eval_steps_per_second': 19.313, 'epoch': 0.6}
{'loss': 0.9839, 'grad_norm': 0.32470986247062683, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1658796072006226, 'eval_runtime': 3.259, 'eval_samples_per_second': 306.845, 'eval_steps_per_second': 19.331, 'epoch': 0.64}
{'loss': 1.0123, 'grad_norm': 0.33893632888793945, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.176869511604309, 'eval_runtime': 3.2581, 'eval_samples_per_second': 306.93, 'eval_steps_per_second': 19.337, 'epoch': 0.68}
{'loss': 0.9944, 'grad_norm': 0.3005376160144806, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1589879989624023, 'eval_runtime': 3.2606, 'eval_samples_per_second': 306.694, 'eval_steps_per_second': 19.322, 'epoch': 0.72}
{'loss': 0.9886, 'grad_norm': 0.3738210201263428, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1727700233459473, 'eval_runtime': 3.2592, 'eval_samples_per_second': 306.822, 'eval_steps_per_second': 19.33, 'epoch': 0.76}
{'loss': 0.9972, 'grad_norm': 0.35217246413230896, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.158013105392456, 'eval_runtime': 3.263, 'eval_samples_per_second': 306.47, 'eval_steps_per_second': 19.308, 'epoch': 0.8}
{'loss': 0.9613, 'grad_norm': 0.367734432220459, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.167056679725647, 'eval_runtime': 3.2597, 'eval_samples_per_second': 306.779, 'eval_steps_per_second': 19.327, 'epoch': 0.84}
{'loss': 0.9993, 'grad_norm': 0.4286034405231476, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1558561325073242, 'eval_runtime': 3.2612, 'eval_samples_per_second': 306.639, 'eval_steps_per_second': 19.318, 'epoch': 0.88}
{'loss': 0.9764, 'grad_norm': 0.432628333568573, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.149810791015625, 'eval_runtime': 3.2654, 'eval_samples_per_second': 306.241, 'eval_steps_per_second': 19.293, 'epoch': 0.92}
{'loss': 0.9298, 'grad_norm': 0.3465941548347473, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1548205614089966, 'eval_runtime': 3.2607, 'eval_samples_per_second': 306.679, 'eval_steps_per_second': 19.321, 'epoch': 0.96}
{'loss': 0.9675, 'grad_norm': 0.5002087950706482, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.155346155166626, 'eval_runtime': 3.2626, 'eval_samples_per_second': 306.506, 'eval_steps_per_second': 19.31, 'epoch': 1.0}
{'train_runtime': 242.8394, 'train_samples_per_second': 41.159, 'train_steps_per_second': 2.574, 'train_loss': 1.0947592712402343, 'epoch': 1.0}
train_results:  {'eval_loss': [2.015214681625366, 1.4211376905441284, 1.2758971452713013, 1.2454370260238647, 1.2458456754684448, 1.2629035711288452, 1.2122926712036133, 1.1957290172576904, 1.2154122591018677, 1.2025580406188965, 1.1808161735534668, 1.2050060033798218, 1.1833997964859009, 1.176444411277771, 1.1669059991836548, 1.1658796072006226, 1.176869511604309, 1.1589879989624023, 1.1727700233459473, 1.158013105392456, 1.167056679725647, 1.1558561325073242, 1.149810791015625, 1.1548205614089966, 1.155346155166626], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.015214681625366, 1.4211376905441284, 1.2758971452713013, 1.2454370260238647, 1.2458456754684448, 1.2629035711288452, 1.2122926712036133, 1.1957290172576904, 1.2154122591018677, 1.2025580406188965, 1.1808161735534668, 1.2050060033798218, 1.1833997964859009, 1.176444411277771, 1.1669059991836548, 1.1658796072006226, 1.176869511604309, 1.1589879989624023, 1.1727700233459473, 1.158013105392456, 1.167056679725647, 1.1558561325073242, 1.149810791015625, 1.1548205614089966, 1.155346155166626]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.6227867603302
current iteration best possible eval_loss (full train run):  -1.155346155166626
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708, -2.4318766593933105, -2.6227867603302]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.2625 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -0.6029337102639427
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -0.6029337102496435
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -0.6029337102496435
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -0.7942048284326093
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -0.5025067365377558
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0918, dtype=torch.float64), tensor(0.3695, dtype=torch.float64), tensor(0.0790, dtype=torch.float64), tensor(0.0124, dtype=torch.float64), tensor(0.1187, dtype=torch.float64), tensor(0.0714, dtype=torch.float64), tensor(0.2572, dtype=torch.float64), 23, 1, 0, 1, 1, 1, 45, 0.07467627965381037, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(1.2316e-18, dtype=torch.float64), tensor(5.1019e-18, dtype=torch.float64), tensor(0.0918, dtype=torch.float64), tensor(0.3695, dtype=torch.float64), tensor(0.0790, dtype=torch.float64), tensor(0.0124, dtype=torch.float64), tensor(0.1187, dtype=torch.float64), tensor(0.0714, dtype=torch.float64), tensor(0.2572, dtype=torch.float64), tensor(0.7276, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3484, dtype=torch.float64), tensor(0.7468, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.092
  sciq: 0.37
  triviaqa: 0.079
  truthfulqa_gen: 0.012
  wikitext: 0.119
  mmlu: 0.071
  arc_challenge: 0.257

LoRA Parameters:
  lora_r: (45,)
  lora_dropout: (0.07467627965381037,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  45
lora dropout:  0.07467627965381037
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 65,710,080 || all params: 8,095,971,328 || trainable%: 0.8116
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.1892, 'grad_norm': 0.4168476462364197, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.528073787689209, 'eval_runtime': 3.5733, 'eval_samples_per_second': 279.854, 'eval_steps_per_second': 17.631, 'epoch': 0.04}
{'loss': 2.4074, 'grad_norm': 0.7063324451446533, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.979476809501648, 'eval_runtime': 3.5703, 'eval_samples_per_second': 280.086, 'eval_steps_per_second': 17.645, 'epoch': 0.08}
{'loss': 1.6738, 'grad_norm': 0.272560179233551, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.685473918914795, 'eval_runtime': 3.5703, 'eval_samples_per_second': 280.086, 'eval_steps_per_second': 17.645, 'epoch': 0.12}
{'loss': 1.5572, 'grad_norm': 0.1698368638753891, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.564604640007019, 'eval_runtime': 3.5721, 'eval_samples_per_second': 279.947, 'eval_steps_per_second': 17.637, 'epoch': 0.16}
{'loss': 1.4537, 'grad_norm': 0.17810676991939545, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4552593231201172, 'eval_runtime': 3.577, 'eval_samples_per_second': 279.565, 'eval_steps_per_second': 17.613, 'epoch': 0.2}
{'loss': 1.3727, 'grad_norm': 0.09520340710878372, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.348175287246704, 'eval_runtime': 3.5713, 'eval_samples_per_second': 280.009, 'eval_steps_per_second': 17.641, 'epoch': 0.24}
{'loss': 1.3929, 'grad_norm': 0.30603551864624023, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2690037488937378, 'eval_runtime': 3.5714, 'eval_samples_per_second': 280.006, 'eval_steps_per_second': 17.64, 'epoch': 0.28}
{'loss': 1.2764, 'grad_norm': 0.09246456623077393, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2269443273544312, 'eval_runtime': 3.5908, 'eval_samples_per_second': 278.489, 'eval_steps_per_second': 17.545, 'epoch': 0.32}
{'loss': 1.3105, 'grad_norm': 0.11143460869789124, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1398998498916626, 'eval_runtime': 3.5973, 'eval_samples_per_second': 277.99, 'eval_steps_per_second': 17.513, 'epoch': 0.36}
{'loss': 1.296, 'grad_norm': 0.10672526061534882, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1035714149475098, 'eval_runtime': 3.601, 'eval_samples_per_second': 277.698, 'eval_steps_per_second': 17.495, 'epoch': 0.4}
{'loss': 1.2299, 'grad_norm': 0.16498325765132904, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0998121500015259, 'eval_runtime': 3.5916, 'eval_samples_per_second': 278.43, 'eval_steps_per_second': 17.541, 'epoch': 0.44}
{'loss': 1.2275, 'grad_norm': 0.10993264615535736, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.101493000984192, 'eval_runtime': 3.5857, 'eval_samples_per_second': 278.886, 'eval_steps_per_second': 17.57, 'epoch': 0.48}
{'loss': 1.1718, 'grad_norm': 0.11374486982822418, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.11519193649292, 'eval_runtime': 3.5882, 'eval_samples_per_second': 278.694, 'eval_steps_per_second': 17.558, 'epoch': 0.52}
{'loss': 1.2549, 'grad_norm': 0.12269303947687149, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0959782600402832, 'eval_runtime': 3.5926, 'eval_samples_per_second': 278.353, 'eval_steps_per_second': 17.536, 'epoch': 0.56}
{'loss': 1.221, 'grad_norm': 0.0994390994310379, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0928137302398682, 'eval_runtime': 3.5923, 'eval_samples_per_second': 278.374, 'eval_steps_per_second': 17.538, 'epoch': 0.6}
{'loss': 1.1967, 'grad_norm': 0.10068222135305405, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0896328687667847, 'eval_runtime': 3.6075, 'eval_samples_per_second': 277.202, 'eval_steps_per_second': 17.464, 'epoch': 0.64}
{'loss': 1.1644, 'grad_norm': 0.09384102374315262, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0855857133865356, 'eval_runtime': 3.6248, 'eval_samples_per_second': 275.874, 'eval_steps_per_second': 17.38, 'epoch': 0.68}
{'loss': 1.1869, 'grad_norm': 0.09677574038505554, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0961169004440308, 'eval_runtime': 3.592, 'eval_samples_per_second': 278.396, 'eval_steps_per_second': 17.539, 'epoch': 0.72}
{'loss': 1.1678, 'grad_norm': 0.09395690262317657, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.078861117362976, 'eval_runtime': 3.5984, 'eval_samples_per_second': 277.898, 'eval_steps_per_second': 17.508, 'epoch': 0.76}
{'loss': 1.1797, 'grad_norm': 0.1084849014878273, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0778107643127441, 'eval_runtime': 3.5938, 'eval_samples_per_second': 278.257, 'eval_steps_per_second': 17.53, 'epoch': 0.8}
{'loss': 1.1751, 'grad_norm': 0.09105654060840607, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0788902044296265, 'eval_runtime': 3.5876, 'eval_samples_per_second': 278.737, 'eval_steps_per_second': 17.56, 'epoch': 0.84}
{'loss': 1.118, 'grad_norm': 0.09678434580564499, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0789601802825928, 'eval_runtime': 3.5891, 'eval_samples_per_second': 278.62, 'eval_steps_per_second': 17.553, 'epoch': 0.88}
{'loss': 1.1604, 'grad_norm': 0.09561977535486221, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0763083696365356, 'eval_runtime': 3.5926, 'eval_samples_per_second': 278.352, 'eval_steps_per_second': 17.536, 'epoch': 0.92}
{'loss': 1.1686, 'grad_norm': 0.09206987917423248, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.074122428894043, 'eval_runtime': 3.5937, 'eval_samples_per_second': 278.267, 'eval_steps_per_second': 17.531, 'epoch': 0.96}
{'loss': 1.1682, 'grad_norm': 0.11526298522949219, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.074126958847046, 'eval_runtime': 3.5931, 'eval_samples_per_second': 278.315, 'eval_steps_per_second': 17.534, 'epoch': 1.0}
{'train_runtime': 320.2483, 'train_samples_per_second': 31.213, 'train_steps_per_second': 1.952, 'train_loss': 1.4288306884765625, 'epoch': 1.0}
train_results:  {'eval_loss': [3.528073787689209, 1.979476809501648, 1.685473918914795, 1.564604640007019, 1.4552593231201172, 1.348175287246704, 1.2690037488937378, 1.2269443273544312, 1.1398998498916626, 1.1035714149475098, 1.0998121500015259, 1.101493000984192, 1.11519193649292, 1.0959782600402832, 1.0928137302398682, 1.0896328687667847, 1.0855857133865356, 1.0961169004440308, 1.078861117362976, 1.0778107643127441, 1.0788902044296265, 1.0789601802825928, 1.0763083696365356, 1.074122428894043, 1.074126958847046], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.528073787689209, 1.979476809501648, 1.685473918914795, 1.564604640007019, 1.4552593231201172, 1.348175287246704, 1.2690037488937378, 1.2269443273544312, 1.1398998498916626, 1.1035714149475098, 1.0998121500015259, 1.101493000984192, 1.11519193649292, 1.0959782600402832, 1.0928137302398682, 1.0896328687667847, 1.0855857133865356, 1.0961169004440308, 1.078861117362976, 1.0778107643127441, 1.0788902044296265, 1.0789601802825928, 1.0763083696365356, 1.074122428894043, 1.074126958847046]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.8240251541137695
current iteration best possible eval_loss (full train run):  -1.074126958847046
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708, -2.4318766593933105, -2.6227867603302, -2.8240251541137695]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.7419 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -0.6751613759903157
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -0.679654220658761
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -0.6796543191351929
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -0.6796464539197482
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -0.7329385875503165
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1266, dtype=torch.float64), tensor(0.0713, dtype=torch.float64), tensor(0.2956, dtype=torch.float64), 0, tensor(0.3920, dtype=torch.float64), 0, tensor(0.0922, dtype=torch.float64), tensor(0.0224, dtype=torch.float64), 11, 1, 0, 1, 1, 1, 113, 0.08787708619886861, 47.66118790360915, 0]
normalized proposed parameters for next round by BO: [tensor(1.0062e-07, dtype=torch.float64), tensor(0.1266, dtype=torch.float64), tensor(0.0713, dtype=torch.float64), tensor(0.2956, dtype=torch.float64), tensor(1.7558e-19, dtype=torch.float64), tensor(0.3920, dtype=torch.float64), tensor(7.7016e-18, dtype=torch.float64), tensor(0.0922, dtype=torch.float64), tensor(0.0224, dtype=torch.float64), tensor(0.3593, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8829, dtype=torch.float64), tensor(0.8788, dtype=torch.float64), tensor(0.9929, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.127
  rowan_hellaswag: 0.071
  sciq: 0.296
  triviaqa: 0
  truthfulqa_gen: 0.392
  wikitext: 0
  mmlu: 0.092
  arc_challenge: 0.022

LoRA Parameters:
  lora_r: (113,)
  lora_dropout: (0.08787708619886861,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.66118790360915,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  113
lora dropout:  0.08787708619886861
lora alpha:  47.66118790360915
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 78,915,584 || all params: 8,109,176,832 || trainable%: 0.9732
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1403, 'grad_norm': 0.706267237663269, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.14621901512146, 'eval_runtime': 4.8824, 'eval_samples_per_second': 204.818, 'eval_steps_per_second': 12.904, 'epoch': 0.04}
{'loss': 1.5733, 'grad_norm': 0.5272188782691956, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6099776029586792, 'eval_runtime': 3.2528, 'eval_samples_per_second': 307.425, 'eval_steps_per_second': 19.368, 'epoch': 0.08}
{'loss': 1.1704, 'grad_norm': 0.24596619606018066, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.549446702003479, 'eval_runtime': 3.2594, 'eval_samples_per_second': 306.803, 'eval_steps_per_second': 19.329, 'epoch': 0.12}
{'loss': 1.1003, 'grad_norm': 0.21731539070606232, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4989087581634521, 'eval_runtime': 3.2423, 'eval_samples_per_second': 308.422, 'eval_steps_per_second': 19.431, 'epoch': 0.16}
{'loss': 1.0834, 'grad_norm': 0.22405219078063965, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5724401473999023, 'eval_runtime': 3.2504, 'eval_samples_per_second': 307.653, 'eval_steps_per_second': 19.382, 'epoch': 0.2}
{'loss': 1.067, 'grad_norm': 0.21581409871578217, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5478047132492065, 'eval_runtime': 3.2429, 'eval_samples_per_second': 308.37, 'eval_steps_per_second': 19.427, 'epoch': 0.24}
{'loss': 1.0471, 'grad_norm': 0.19786225259304047, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6001379489898682, 'eval_runtime': 3.2418, 'eval_samples_per_second': 308.466, 'eval_steps_per_second': 19.433, 'epoch': 0.28}
{'loss': 1.1344, 'grad_norm': 0.2159278243780136, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5839622020721436, 'eval_runtime': 3.2449, 'eval_samples_per_second': 308.175, 'eval_steps_per_second': 19.415, 'epoch': 0.32}
{'loss': 1.0318, 'grad_norm': 0.26753172278404236, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6037707328796387, 'eval_runtime': 3.251, 'eval_samples_per_second': 307.597, 'eval_steps_per_second': 19.379, 'epoch': 0.36}
{'loss': 1.0431, 'grad_norm': 0.28146305680274963, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.656014323234558, 'eval_runtime': 3.2497, 'eval_samples_per_second': 307.725, 'eval_steps_per_second': 19.387, 'epoch': 0.4}
{'loss': 0.9705, 'grad_norm': 0.3107430636882782, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.627715826034546, 'eval_runtime': 3.248, 'eval_samples_per_second': 307.883, 'eval_steps_per_second': 19.397, 'epoch': 0.44}
{'loss': 0.9525, 'grad_norm': 0.21735039353370667, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6327136754989624, 'eval_runtime': 3.2523, 'eval_samples_per_second': 307.477, 'eval_steps_per_second': 19.371, 'epoch': 0.48}
{'loss': 1.0446, 'grad_norm': 0.2099035084247589, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.588515043258667, 'eval_runtime': 3.2496, 'eval_samples_per_second': 307.731, 'eval_steps_per_second': 19.387, 'epoch': 0.52}
{'loss': 1.0, 'grad_norm': 0.28868213295936584, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6169004440307617, 'eval_runtime': 3.2516, 'eval_samples_per_second': 307.542, 'eval_steps_per_second': 19.375, 'epoch': 0.56}
{'loss': 0.9602, 'grad_norm': 0.3986870348453522, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6872878074645996, 'eval_runtime': 3.2518, 'eval_samples_per_second': 307.526, 'eval_steps_per_second': 19.374, 'epoch': 0.6}
{'loss': 0.9826, 'grad_norm': 0.2762990891933441, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6388018131256104, 'eval_runtime': 3.2909, 'eval_samples_per_second': 303.87, 'eval_steps_per_second': 19.144, 'epoch': 0.64}
{'loss': 0.9243, 'grad_norm': 0.23108163475990295, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6748337745666504, 'eval_runtime': 3.3018, 'eval_samples_per_second': 302.864, 'eval_steps_per_second': 19.08, 'epoch': 0.68}
{'loss': 1.0031, 'grad_norm': 0.2726687490940094, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6657044887542725, 'eval_runtime': 3.2683, 'eval_samples_per_second': 305.967, 'eval_steps_per_second': 19.276, 'epoch': 0.72}
{'loss': 0.9086, 'grad_norm': 0.33167892694473267, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6883609294891357, 'eval_runtime': 3.2696, 'eval_samples_per_second': 305.846, 'eval_steps_per_second': 19.268, 'epoch': 0.76}
{'loss': 0.9747, 'grad_norm': 0.22888289391994476, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6999560594558716, 'eval_runtime': 3.2693, 'eval_samples_per_second': 305.871, 'eval_steps_per_second': 19.27, 'epoch': 0.8}
{'loss': 0.9608, 'grad_norm': 0.29712337255477905, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7102723121643066, 'eval_runtime': 3.2614, 'eval_samples_per_second': 306.616, 'eval_steps_per_second': 19.317, 'epoch': 0.84}
{'loss': 0.9587, 'grad_norm': 0.2482215315103531, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6944319009780884, 'eval_runtime': 3.2524, 'eval_samples_per_second': 307.469, 'eval_steps_per_second': 19.371, 'epoch': 0.88}
{'loss': 0.8742, 'grad_norm': 0.3063015639781952, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7335948944091797, 'eval_runtime': 3.2621, 'eval_samples_per_second': 306.554, 'eval_steps_per_second': 19.313, 'epoch': 0.92}
{'loss': 0.9581, 'grad_norm': 0.24535925686359406, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7307277917861938, 'eval_runtime': 3.2629, 'eval_samples_per_second': 306.476, 'eval_steps_per_second': 19.308, 'epoch': 0.96}
{'loss': 0.9841, 'grad_norm': 0.3542734384536743, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7245124578475952, 'eval_runtime': 3.2682, 'eval_samples_per_second': 305.977, 'eval_steps_per_second': 19.277, 'epoch': 1.0}
{'train_runtime': 233.8084, 'train_samples_per_second': 42.753, 'train_steps_per_second': 2.673, 'train_loss': 1.113924557495117, 'epoch': 1.0}
train_results:  {'eval_loss': [2.14621901512146, 1.6099776029586792, 1.549446702003479, 1.4989087581634521, 1.5724401473999023, 1.5478047132492065, 1.6001379489898682, 1.5839622020721436, 1.6037707328796387, 1.656014323234558, 1.627715826034546, 1.6327136754989624, 1.588515043258667, 1.6169004440307617, 1.6872878074645996, 1.6388018131256104, 1.6748337745666504, 1.6657044887542725, 1.6883609294891357, 1.6999560594558716, 1.7102723121643066, 1.6944319009780884, 1.7335948944091797, 1.7307277917861938, 1.7245124578475952], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.14621901512146, 1.6099776029586792, 1.549446702003479, 1.4989087581634521, 1.5724401473999023, 1.5478047132492065, 1.6001379489898682, 1.5839622020721436, 1.6037707328796387, 1.656014323234558, 1.627715826034546, 1.6327136754989624, 1.588515043258667, 1.6169004440307617, 1.6872878074645996, 1.6388018131256104, 1.6748337745666504, 1.6657044887542725, 1.6883609294891357, 1.6999560594558716, 1.7102723121643066, 1.6944319009780884, 1.7335948944091797, 1.7307277917861938, 1.7245124578475952]
current iteration observed (possibly low-fid or predicted) eval_loss:  -3.603632926940918
current iteration best possible eval_loss (full train run):  -1.7245124578475952
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708, -2.4318766593933105, -2.6227867603302, -2.8240251541137695, -3.603632926940918]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.7156 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -0.5826213181177633
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -0.5826213181177633
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -0.5826213181177633
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -0.5826213181177633
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -0.5826207926083953
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0503, dtype=torch.float64), tensor(0.1818, dtype=torch.float64), tensor(0.1419, dtype=torch.float64), 0, 0, tensor(0.1540, dtype=torch.float64), tensor(0.3666, dtype=torch.float64), tensor(0.0244, dtype=torch.float64), tensor(0.0809, dtype=torch.float64), 32, 0, 0, 0, 1, 0, 30, 0.0, 36.30623404571663, 0]
normalized proposed parameters for next round by BO: [tensor(0.0503, dtype=torch.float64), tensor(0.1818, dtype=torch.float64), tensor(0.1419, dtype=torch.float64), tensor(1.4195e-18, dtype=torch.float64), tensor(3.1123e-17, dtype=torch.float64), tensor(0.1540, dtype=torch.float64), tensor(0.3666, dtype=torch.float64), tensor(0.0244, dtype=torch.float64), tensor(0.0809, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2316, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7564, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.05
  gsm8k: 0.182
  rowan_hellaswag: 0.142
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.154
  wikitext: 0.367
  mmlu: 0.024
  arc_challenge: 0.081

LoRA Parameters:
  lora_r: (30,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (36.30623404571663,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  30
lora dropout:  0.0
lora alpha:  36.30623404571663
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 17,694,720 || all params: 8,047,955,968 || trainable%: 0.2199
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9985, 'grad_norm': 0.9243574142456055, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0567970275878906, 'eval_runtime': 3.7553, 'eval_samples_per_second': 266.293, 'eval_steps_per_second': 16.776, 'epoch': 0.04}
{'loss': 1.6992, 'grad_norm': 1.110378623008728, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5353748798370361, 'eval_runtime': 3.0848, 'eval_samples_per_second': 324.172, 'eval_steps_per_second': 20.423, 'epoch': 0.08}
{'loss': 1.4707, 'grad_norm': 0.5302263498306274, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4555505514144897, 'eval_runtime': 3.0787, 'eval_samples_per_second': 324.811, 'eval_steps_per_second': 20.463, 'epoch': 0.12}
{'loss': 1.4531, 'grad_norm': 0.3824622333049774, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.489504337310791, 'eval_runtime': 3.082, 'eval_samples_per_second': 324.46, 'eval_steps_per_second': 20.441, 'epoch': 0.16}
{'loss': 1.4017, 'grad_norm': 0.5134539604187012, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4049253463745117, 'eval_runtime': 3.0936, 'eval_samples_per_second': 323.252, 'eval_steps_per_second': 20.365, 'epoch': 0.2}
{'loss': 1.3727, 'grad_norm': 0.5048723816871643, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4896085262298584, 'eval_runtime': 3.0898, 'eval_samples_per_second': 323.647, 'eval_steps_per_second': 20.39, 'epoch': 0.24}
{'loss': 1.4194, 'grad_norm': 0.3669836223125458, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5089828968048096, 'eval_runtime': 3.1004, 'eval_samples_per_second': 322.542, 'eval_steps_per_second': 20.32, 'epoch': 0.28}
{'loss': 1.4508, 'grad_norm': 0.534520149230957, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.659645438194275, 'eval_runtime': 3.0971, 'eval_samples_per_second': 322.879, 'eval_steps_per_second': 20.341, 'epoch': 0.32}
{'loss': 1.4324, 'grad_norm': 0.4443487226963043, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5790454149246216, 'eval_runtime': 3.0995, 'eval_samples_per_second': 322.632, 'eval_steps_per_second': 20.326, 'epoch': 0.36}
{'loss': 1.3817, 'grad_norm': 0.3919181823730469, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.484417200088501, 'eval_runtime': 3.0975, 'eval_samples_per_second': 322.836, 'eval_steps_per_second': 20.339, 'epoch': 0.4}
{'loss': 1.394, 'grad_norm': 0.6287433505058289, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4646950960159302, 'eval_runtime': 3.1005, 'eval_samples_per_second': 322.532, 'eval_steps_per_second': 20.32, 'epoch': 0.44}
{'loss': 1.3624, 'grad_norm': 0.43346837162971497, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5086092948913574, 'eval_runtime': 3.0972, 'eval_samples_per_second': 322.873, 'eval_steps_per_second': 20.341, 'epoch': 0.48}
{'loss': 1.3498, 'grad_norm': 0.5392991900444031, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4996670484542847, 'eval_runtime': 3.1048, 'eval_samples_per_second': 322.08, 'eval_steps_per_second': 20.291, 'epoch': 0.52}
{'loss': 1.3856, 'grad_norm': 0.5682222843170166, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4902724027633667, 'eval_runtime': 3.092, 'eval_samples_per_second': 323.412, 'eval_steps_per_second': 20.375, 'epoch': 0.56}
{'loss': 1.407, 'grad_norm': 0.6285200119018555, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5919153690338135, 'eval_runtime': 3.3378, 'eval_samples_per_second': 299.598, 'eval_steps_per_second': 18.875, 'epoch': 0.6}
{'loss': 1.2834, 'grad_norm': 0.416268914937973, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.529043436050415, 'eval_runtime': 3.0938, 'eval_samples_per_second': 323.224, 'eval_steps_per_second': 20.363, 'epoch': 0.64}
{'loss': 1.3755, 'grad_norm': 0.48230329155921936, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.530285358428955, 'eval_runtime': 3.0956, 'eval_samples_per_second': 323.044, 'eval_steps_per_second': 20.352, 'epoch': 0.68}
{'loss': 1.4365, 'grad_norm': 0.6497888565063477, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.531438946723938, 'eval_runtime': 3.0871, 'eval_samples_per_second': 323.924, 'eval_steps_per_second': 20.407, 'epoch': 0.72}
{'loss': 1.3594, 'grad_norm': 0.5914286971092224, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6002236604690552, 'eval_runtime': 3.0874, 'eval_samples_per_second': 323.902, 'eval_steps_per_second': 20.406, 'epoch': 0.76}
{'loss': 1.3309, 'grad_norm': 0.7937660813331604, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5604387521743774, 'eval_runtime': 3.0911, 'eval_samples_per_second': 323.51, 'eval_steps_per_second': 20.381, 'epoch': 0.8}
{'loss': 1.4048, 'grad_norm': 0.6867600679397583, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5650372505187988, 'eval_runtime': 3.0898, 'eval_samples_per_second': 323.641, 'eval_steps_per_second': 20.389, 'epoch': 0.84}
{'loss': 1.3194, 'grad_norm': 0.5672349333763123, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5927075147628784, 'eval_runtime': 3.0855, 'eval_samples_per_second': 324.099, 'eval_steps_per_second': 20.418, 'epoch': 0.88}
{'loss': 1.3502, 'grad_norm': 0.4581691324710846, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6137332916259766, 'eval_runtime': 3.0834, 'eval_samples_per_second': 324.315, 'eval_steps_per_second': 20.432, 'epoch': 0.92}
{'loss': 1.3589, 'grad_norm': 0.409137099981308, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6129506826400757, 'eval_runtime': 3.0994, 'eval_samples_per_second': 322.644, 'eval_steps_per_second': 20.327, 'epoch': 0.96}
{'loss': 1.3431, 'grad_norm': 0.5252527594566345, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6126593351364136, 'eval_runtime': 3.0826, 'eval_samples_per_second': 324.405, 'eval_steps_per_second': 20.438, 'epoch': 1.0}
{'train_runtime': 288.0846, 'train_samples_per_second': 34.702, 'train_steps_per_second': 2.17, 'train_loss': 1.461640985107422, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0567970275878906, 1.5353748798370361, 1.4555505514144897, 1.489504337310791, 1.4049253463745117, 1.4896085262298584, 1.5089828968048096, 1.659645438194275, 1.5790454149246216, 1.484417200088501, 1.4646950960159302, 1.5086092948913574, 1.4996670484542847, 1.4902724027633667, 1.5919153690338135, 1.529043436050415, 1.530285358428955, 1.531438946723938, 1.6002236604690552, 1.5604387521743774, 1.5650372505187988, 1.5927075147628784, 1.6137332916259766, 1.6129506826400757, 1.6126593351364136], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0567970275878906, 1.5353748798370361, 1.4555505514144897, 1.489504337310791, 1.4049253463745117, 1.4896085262298584, 1.5089828968048096, 1.659645438194275, 1.5790454149246216, 1.484417200088501, 1.4646950960159302, 1.5086092948913574, 1.4996670484542847, 1.4902724027633667, 1.5919153690338135, 1.529043436050415, 1.530285358428955, 1.531438946723938, 1.6002236604690552, 1.5604387521743774, 1.5650372505187988, 1.5927075147628784, 1.6137332916259766, 1.6129506826400757, 1.6126593351364136]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.786780595779419
current iteration best possible eval_loss (full train run):  -1.6126593351364136
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708, -2.4318766593933105, -2.6227867603302, -2.8240251541137695, -3.603632926940918, -2.786780595779419]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.9042 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -0.8329446814601358
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -0.8329556539570133
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -0.8276239669567356
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -0.822109470622508
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -0.8329445605207435
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0585, dtype=torch.float64), tensor(0.0531, dtype=torch.float64), 0, tensor(0.2371, dtype=torch.float64), 0, tensor(0.3014, dtype=torch.float64), 0, tensor(0.2281, dtype=torch.float64), tensor(0.1218, dtype=torch.float64), 20, 1, 0, 0, 1, 1, 2, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0585, dtype=torch.float64), tensor(0.0531, dtype=torch.float64), tensor(9.0526e-17, dtype=torch.float64), tensor(0.2371, dtype=torch.float64), tensor(3.6427e-18, dtype=torch.float64), tensor(0.3014, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2281, dtype=torch.float64), tensor(0.1218, dtype=torch.float64), tensor(0.6254, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.059
  gsm8k: 0.053
  rowan_hellaswag: 0
  sciq: 0.237
  triviaqa: 0
  truthfulqa_gen: 0.301
  wikitext: 0
  mmlu: 0.228
  arc_challenge: 0.122

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,802,240 || all params: 8,032,063,488 || trainable%: 0.0224
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.2102, 'grad_norm': 2.488771677017212, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.975383996963501, 'eval_runtime': 3.8306, 'eval_samples_per_second': 261.053, 'eval_steps_per_second': 16.446, 'epoch': 0.04}
{'loss': 2.4319, 'grad_norm': 4.56439733505249, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.3942642211914062, 'eval_runtime': 3.2439, 'eval_samples_per_second': 308.272, 'eval_steps_per_second': 19.421, 'epoch': 0.08}
{'loss': 1.6322, 'grad_norm': 1.7131636142730713, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7927719354629517, 'eval_runtime': 3.2439, 'eval_samples_per_second': 308.269, 'eval_steps_per_second': 19.421, 'epoch': 0.12}
{'loss': 1.3137, 'grad_norm': 0.9589419364929199, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6988552808761597, 'eval_runtime': 3.2478, 'eval_samples_per_second': 307.903, 'eval_steps_per_second': 19.398, 'epoch': 0.16}
{'loss': 1.231, 'grad_norm': 0.446481317281723, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5591416358947754, 'eval_runtime': 3.2291, 'eval_samples_per_second': 309.687, 'eval_steps_per_second': 19.51, 'epoch': 0.2}
{'loss': 1.15, 'grad_norm': 0.39475247263908386, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.61670982837677, 'eval_runtime': 3.2289, 'eval_samples_per_second': 309.701, 'eval_steps_per_second': 19.511, 'epoch': 0.24}
{'loss': 1.0612, 'grad_norm': 0.5019235610961914, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.498935341835022, 'eval_runtime': 3.2351, 'eval_samples_per_second': 309.113, 'eval_steps_per_second': 19.474, 'epoch': 0.28}
{'loss': 1.0188, 'grad_norm': 0.42118364572525024, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5411516427993774, 'eval_runtime': 3.2289, 'eval_samples_per_second': 309.702, 'eval_steps_per_second': 19.511, 'epoch': 0.32}
{'loss': 1.0991, 'grad_norm': 0.43933266401290894, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6277241706848145, 'eval_runtime': 3.2317, 'eval_samples_per_second': 309.434, 'eval_steps_per_second': 19.494, 'epoch': 0.36}
{'loss': 0.9929, 'grad_norm': 0.48517003655433655, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6382991075515747, 'eval_runtime': 3.2411, 'eval_samples_per_second': 308.541, 'eval_steps_per_second': 19.438, 'epoch': 0.4}
{'loss': 1.0183, 'grad_norm': 0.5267269015312195, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5833463668823242, 'eval_runtime': 3.2363, 'eval_samples_per_second': 308.992, 'eval_steps_per_second': 19.467, 'epoch': 0.44}
{'loss': 0.989, 'grad_norm': 0.4998067021369934, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6317611932754517, 'eval_runtime': 3.2413, 'eval_samples_per_second': 308.518, 'eval_steps_per_second': 19.437, 'epoch': 0.48}
{'loss': 0.9992, 'grad_norm': 0.4079112708568573, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6182730197906494, 'eval_runtime': 3.2478, 'eval_samples_per_second': 307.896, 'eval_steps_per_second': 19.397, 'epoch': 0.52}
{'loss': 1.0042, 'grad_norm': 0.9898113012313843, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.57102370262146, 'eval_runtime': 3.2392, 'eval_samples_per_second': 308.719, 'eval_steps_per_second': 19.449, 'epoch': 0.56}
{'loss': 0.974, 'grad_norm': 0.5907600522041321, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5851073265075684, 'eval_runtime': 3.2351, 'eval_samples_per_second': 309.107, 'eval_steps_per_second': 19.474, 'epoch': 0.6}
{'loss': 0.9305, 'grad_norm': 0.4776318669319153, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.613836646080017, 'eval_runtime': 3.2505, 'eval_samples_per_second': 307.649, 'eval_steps_per_second': 19.382, 'epoch': 0.64}
{'loss': 0.9126, 'grad_norm': 0.5188254117965698, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6068089008331299, 'eval_runtime': 3.2393, 'eval_samples_per_second': 308.706, 'eval_steps_per_second': 19.448, 'epoch': 0.68}
{'loss': 0.9467, 'grad_norm': 0.45920121669769287, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.604582667350769, 'eval_runtime': 3.2582, 'eval_samples_per_second': 306.915, 'eval_steps_per_second': 19.336, 'epoch': 0.72}
{'loss': 0.9652, 'grad_norm': 0.510493814945221, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6227612495422363, 'eval_runtime': 3.2513, 'eval_samples_per_second': 307.568, 'eval_steps_per_second': 19.377, 'epoch': 0.76}
{'loss': 0.9771, 'grad_norm': 0.5003206729888916, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6373848915100098, 'eval_runtime': 3.2522, 'eval_samples_per_second': 307.487, 'eval_steps_per_second': 19.372, 'epoch': 0.8}
{'loss': 0.9619, 'grad_norm': 0.4146825969219208, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6209243535995483, 'eval_runtime': 3.2448, 'eval_samples_per_second': 308.186, 'eval_steps_per_second': 19.416, 'epoch': 0.84}
{'loss': 0.9904, 'grad_norm': 0.4512180984020233, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.637094259262085, 'eval_runtime': 3.2521, 'eval_samples_per_second': 307.494, 'eval_steps_per_second': 19.372, 'epoch': 0.88}
{'loss': 0.93, 'grad_norm': 0.49254709482192993, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6364142894744873, 'eval_runtime': 3.2427, 'eval_samples_per_second': 308.382, 'eval_steps_per_second': 19.428, 'epoch': 0.92}
{'loss': 0.9088, 'grad_norm': 0.44779083132743835, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6471089124679565, 'eval_runtime': 3.2532, 'eval_samples_per_second': 307.391, 'eval_steps_per_second': 19.366, 'epoch': 0.96}
{'loss': 0.9626, 'grad_norm': 0.5282375812530518, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6502131223678589, 'eval_runtime': 3.2554, 'eval_samples_per_second': 307.182, 'eval_steps_per_second': 19.352, 'epoch': 1.0}
{'train_runtime': 276.8153, 'train_samples_per_second': 36.114, 'train_steps_per_second': 2.258, 'train_loss': 1.2244671966552734, 'epoch': 1.0}
train_results:  {'eval_loss': [3.975383996963501, 2.3942642211914062, 1.7927719354629517, 1.6988552808761597, 1.5591416358947754, 1.61670982837677, 1.498935341835022, 1.5411516427993774, 1.6277241706848145, 1.6382991075515747, 1.5833463668823242, 1.6317611932754517, 1.6182730197906494, 1.57102370262146, 1.5851073265075684, 1.613836646080017, 1.6068089008331299, 1.604582667350769, 1.6227612495422363, 1.6373848915100098, 1.6209243535995483, 1.637094259262085, 1.6364142894744873, 1.6471089124679565, 1.6502131223678589], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.975383996963501, 2.3942642211914062, 1.7927719354629517, 1.6988552808761597, 1.5591416358947754, 1.61670982837677, 1.498935341835022, 1.5411516427993774, 1.6277241706848145, 1.6382991075515747, 1.5833463668823242, 1.6317611932754517, 1.6182730197906494, 1.57102370262146, 1.5851073265075684, 1.613836646080017, 1.6068089008331299, 1.604582667350769, 1.6227612495422363, 1.6373848915100098, 1.6209243535995483, 1.637094259262085, 1.6364142894744873, 1.6471089124679565, 1.6502131223678589]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.038760185241699
current iteration best possible eval_loss (full train run):  -1.6502131223678589
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708, -2.4318766593933105, -2.6227867603302, -2.8240251541137695, -3.603632926940918, -2.786780595779419, -2.038760185241699]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.2869 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -0.5846105860449862
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -0.6808303583129156
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -1.8998847763663826
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -0.5320079570656622
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -1.331666870917662
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.1243, dtype=torch.float64), tensor(0.8757, dtype=torch.float64), 0, 0, 0, 11, 0, 1, 0, 0, 1, 30, 1.4730404396257059e-18, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.3947e-20, dtype=torch.float64), tensor(0.1243, dtype=torch.float64), tensor(0.8757, dtype=torch.float64), tensor(6.4500e-18, dtype=torch.float64), tensor(2.3127e-21, dtype=torch.float64), tensor(3.2769e-18, dtype=torch.float64), tensor(0.3534, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2346, dtype=torch.float64), tensor(1.4730e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.124
  truthfulqa_gen: 0.876
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (30,)
  lora_dropout: (1.4730404396257059e-18,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  30
lora dropout:  1.4730404396257059e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 7,772,160 || all params: 8,038,033,408 || trainable%: 0.0967
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 5.2852, 'grad_norm': 0.7311988472938538, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 4.6214375495910645, 'eval_runtime': 2.9188, 'eval_samples_per_second': 342.609, 'eval_steps_per_second': 21.584, 'epoch': 0.04}
{'loss': 3.4342, 'grad_norm': 0.4021588861942291, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.5347275733947754, 'eval_runtime': 2.9228, 'eval_samples_per_second': 342.142, 'eval_steps_per_second': 21.555, 'epoch': 0.08}
{'loss': 1.9185, 'grad_norm': 0.29112866520881653, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.140777826309204, 'eval_runtime': 2.9266, 'eval_samples_per_second': 341.698, 'eval_steps_per_second': 21.527, 'epoch': 0.12}
{'loss': 1.5542, 'grad_norm': 0.1842835545539856, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9589790105819702, 'eval_runtime': 2.9199, 'eval_samples_per_second': 342.478, 'eval_steps_per_second': 21.576, 'epoch': 0.16}
{'loss': 1.4744, 'grad_norm': 0.1966714859008789, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9146227836608887, 'eval_runtime': 2.9208, 'eval_samples_per_second': 342.367, 'eval_steps_per_second': 21.569, 'epoch': 0.2}
{'loss': 1.4573, 'grad_norm': 0.16555818915367126, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.876726508140564, 'eval_runtime': 2.9322, 'eval_samples_per_second': 341.044, 'eval_steps_per_second': 21.486, 'epoch': 0.24}
{'loss': 1.3707, 'grad_norm': 0.19533859193325043, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.833949089050293, 'eval_runtime': 2.936, 'eval_samples_per_second': 340.597, 'eval_steps_per_second': 21.458, 'epoch': 0.28}
{'loss': 1.346, 'grad_norm': 0.19163450598716736, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.771389126777649, 'eval_runtime': 2.9317, 'eval_samples_per_second': 341.103, 'eval_steps_per_second': 21.49, 'epoch': 0.32}
{'loss': 1.2695, 'grad_norm': 0.3283195197582245, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6745134592056274, 'eval_runtime': 2.9402, 'eval_samples_per_second': 340.111, 'eval_steps_per_second': 21.427, 'epoch': 0.36}
{'loss': 1.234, 'grad_norm': 0.21213586628437042, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6144895553588867, 'eval_runtime': 2.9318, 'eval_samples_per_second': 341.083, 'eval_steps_per_second': 21.488, 'epoch': 0.4}
{'loss': 1.1836, 'grad_norm': 0.2619762122631073, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5758291482925415, 'eval_runtime': 2.9419, 'eval_samples_per_second': 339.914, 'eval_steps_per_second': 21.415, 'epoch': 0.44}
{'loss': 1.1668, 'grad_norm': 0.24556685984134674, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5474907159805298, 'eval_runtime': 2.9557, 'eval_samples_per_second': 338.332, 'eval_steps_per_second': 21.315, 'epoch': 0.48}
{'loss': 1.1196, 'grad_norm': 0.2485445886850357, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5116676092147827, 'eval_runtime': 2.9553, 'eval_samples_per_second': 338.374, 'eval_steps_per_second': 21.318, 'epoch': 0.52}
{'loss': 1.0745, 'grad_norm': 0.22687803208827972, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4811533689498901, 'eval_runtime': 2.964, 'eval_samples_per_second': 337.38, 'eval_steps_per_second': 21.255, 'epoch': 0.56}
{'loss': 1.0516, 'grad_norm': 0.2957596778869629, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4726899862289429, 'eval_runtime': 2.9695, 'eval_samples_per_second': 336.758, 'eval_steps_per_second': 21.216, 'epoch': 0.6}
{'loss': 1.0706, 'grad_norm': 0.32712727785110474, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4535781145095825, 'eval_runtime': 2.9629, 'eval_samples_per_second': 337.508, 'eval_steps_per_second': 21.263, 'epoch': 0.64}
{'loss': 1.0291, 'grad_norm': 0.2795301675796509, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4434316158294678, 'eval_runtime': 2.9622, 'eval_samples_per_second': 337.586, 'eval_steps_per_second': 21.268, 'epoch': 0.68}
{'loss': 1.0284, 'grad_norm': 0.25791192054748535, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4325854778289795, 'eval_runtime': 2.9478, 'eval_samples_per_second': 339.234, 'eval_steps_per_second': 21.372, 'epoch': 0.72}
{'loss': 1.0366, 'grad_norm': 0.2586613893508911, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.424694299697876, 'eval_runtime': 2.9549, 'eval_samples_per_second': 338.426, 'eval_steps_per_second': 21.321, 'epoch': 0.76}
{'loss': 1.0057, 'grad_norm': 0.24813592433929443, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4192514419555664, 'eval_runtime': 2.9526, 'eval_samples_per_second': 338.684, 'eval_steps_per_second': 21.337, 'epoch': 0.8}
{'loss': 1.0214, 'grad_norm': 0.32683053612709045, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.413970947265625, 'eval_runtime': 2.9567, 'eval_samples_per_second': 338.214, 'eval_steps_per_second': 21.307, 'epoch': 0.84}
{'loss': 1.0127, 'grad_norm': 0.31858330965042114, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.411942720413208, 'eval_runtime': 2.9553, 'eval_samples_per_second': 338.372, 'eval_steps_per_second': 21.317, 'epoch': 0.88}
{'loss': 0.9985, 'grad_norm': 0.3087330758571625, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.409554123878479, 'eval_runtime': 2.9607, 'eval_samples_per_second': 337.758, 'eval_steps_per_second': 21.279, 'epoch': 0.92}
{'loss': 0.9947, 'grad_norm': 0.2866124212741852, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4099853038787842, 'eval_runtime': 2.9803, 'eval_samples_per_second': 335.533, 'eval_steps_per_second': 21.139, 'epoch': 0.96}
{'loss': 0.9766, 'grad_norm': 0.30496400594711304, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4079443216323853, 'eval_runtime': 2.9881, 'eval_samples_per_second': 334.66, 'eval_steps_per_second': 21.084, 'epoch': 1.0}
{'train_runtime': 149.3358, 'train_samples_per_second': 66.956, 'train_steps_per_second': 4.185, 'train_loss': 1.4445771728515624, 'epoch': 1.0}
train_results:  {'eval_loss': [4.6214375495910645, 2.5347275733947754, 2.140777826309204, 1.9589790105819702, 1.9146227836608887, 1.876726508140564, 1.833949089050293, 1.771389126777649, 1.6745134592056274, 1.6144895553588867, 1.5758291482925415, 1.5474907159805298, 1.5116676092147827, 1.4811533689498901, 1.4726899862289429, 1.4535781145095825, 1.4434316158294678, 1.4325854778289795, 1.424694299697876, 1.4192514419555664, 1.413970947265625, 1.411942720413208, 1.409554123878479, 1.4099853038787842, 1.4079443216323853], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [4.6214375495910645, 2.5347275733947754, 2.140777826309204, 1.9589790105819702, 1.9146227836608887, 1.876726508140564, 1.833949089050293, 1.771389126777649, 1.6745134592056274, 1.6144895553588867, 1.5758291482925415, 1.5474907159805298, 1.5116676092147827, 1.4811533689498901, 1.4726899862289429, 1.4535781145095825, 1.4434316158294678, 1.4325854778289795, 1.424694299697876, 1.4192514419555664, 1.413970947265625, 1.411942720413208, 1.409554123878479, 1.4099853038787842, 1.4079443216323853]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.188371181488037
current iteration best possible eval_loss (full train run):  -1.4079443216323853
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708, -2.4318766593933105, -2.6227867603302, -2.8240251541137695, -3.603632926940918, -2.786780595779419, -2.038760185241699, -2.188371181488037]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.6689 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -0.7658072188634748
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -0.8033381400649788
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -0.7676927007145937
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -0.784943473515249
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -0.4870152032609809
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1094, dtype=torch.float64), tensor(0.0313, dtype=torch.float64), tensor(0.4383, dtype=torch.float64), tensor(0.0955, dtype=torch.float64), 0, tensor(0.3255, dtype=torch.float64), 22, 1, 0, 0, 1, 1, 2, 0.08340210436602336, 1.4800000190734872, 1]
normalized proposed parameters for next round by BO: [tensor(1.0337e-20, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9407e-21, dtype=torch.float64), tensor(0.1094, dtype=torch.float64), tensor(0.0313, dtype=torch.float64), tensor(0.4383, dtype=torch.float64), tensor(0.0955, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3255, dtype=torch.float64), tensor(0.6732, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.8340, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.109
  triviaqa: 0.031
  truthfulqa_gen: 0.438
  wikitext: 0.095
  mmlu: 0
  arc_challenge: 0.326

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.08340210436602336,)
  num_layers_to_apply: (22,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  22
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.08340210436602336
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,982,464 || all params: 8,032,243,712 || trainable%: 0.0247
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.3967, 'grad_norm': 2.3626303672790527, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.8915462493896484, 'eval_runtime': 4.4373, 'eval_samples_per_second': 225.36, 'eval_steps_per_second': 14.198, 'epoch': 0.04}
{'loss': 2.3761, 'grad_norm': 4.823749542236328, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.4803481101989746, 'eval_runtime': 3.2589, 'eval_samples_per_second': 306.85, 'eval_steps_per_second': 19.332, 'epoch': 0.08}
{'loss': 1.4587, 'grad_norm': 1.642600178718567, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.838446855545044, 'eval_runtime': 3.2812, 'eval_samples_per_second': 304.764, 'eval_steps_per_second': 19.2, 'epoch': 0.12}
{'loss': 1.2647, 'grad_norm': 0.8424807190895081, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.638117790222168, 'eval_runtime': 3.3642, 'eval_samples_per_second': 297.248, 'eval_steps_per_second': 18.727, 'epoch': 0.16}
{'loss': 1.1103, 'grad_norm': 0.4059831500053406, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.428168773651123, 'eval_runtime': 3.2615, 'eval_samples_per_second': 306.61, 'eval_steps_per_second': 19.316, 'epoch': 0.2}
{'loss': 1.0679, 'grad_norm': 0.3925299644470215, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3466973304748535, 'eval_runtime': 3.2638, 'eval_samples_per_second': 306.388, 'eval_steps_per_second': 19.302, 'epoch': 0.24}
{'loss': 0.9708, 'grad_norm': 0.5931561589241028, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.28846275806427, 'eval_runtime': 3.2614, 'eval_samples_per_second': 306.621, 'eval_steps_per_second': 19.317, 'epoch': 0.28}
{'loss': 1.0627, 'grad_norm': 0.4033297002315521, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2033966779708862, 'eval_runtime': 3.2599, 'eval_samples_per_second': 306.758, 'eval_steps_per_second': 19.326, 'epoch': 0.32}
{'loss': 1.0376, 'grad_norm': 0.40888512134552, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2371909618377686, 'eval_runtime': 3.2539, 'eval_samples_per_second': 307.325, 'eval_steps_per_second': 19.361, 'epoch': 0.36}
{'loss': 0.9809, 'grad_norm': 0.46479082107543945, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2047346830368042, 'eval_runtime': 3.2555, 'eval_samples_per_second': 307.177, 'eval_steps_per_second': 19.352, 'epoch': 0.4}
{'loss': 0.9164, 'grad_norm': 0.5015192031860352, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2385456562042236, 'eval_runtime': 3.2571, 'eval_samples_per_second': 307.025, 'eval_steps_per_second': 19.343, 'epoch': 0.44}
{'loss': 0.9041, 'grad_norm': 0.5452484488487244, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1905474662780762, 'eval_runtime': 3.2571, 'eval_samples_per_second': 307.026, 'eval_steps_per_second': 19.343, 'epoch': 0.48}
{'loss': 0.9233, 'grad_norm': 0.5479629039764404, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1791118383407593, 'eval_runtime': 3.2555, 'eval_samples_per_second': 307.176, 'eval_steps_per_second': 19.352, 'epoch': 0.52}
{'loss': 0.8799, 'grad_norm': 0.7374204993247986, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.189400315284729, 'eval_runtime': 3.2852, 'eval_samples_per_second': 304.391, 'eval_steps_per_second': 19.177, 'epoch': 0.56}
{'loss': 0.8596, 'grad_norm': 0.5529064536094666, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1936893463134766, 'eval_runtime': 3.2895, 'eval_samples_per_second': 304.001, 'eval_steps_per_second': 19.152, 'epoch': 0.6}
{'loss': 0.8953, 'grad_norm': 0.6788696050643921, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1864632368087769, 'eval_runtime': 3.267, 'eval_samples_per_second': 306.094, 'eval_steps_per_second': 19.284, 'epoch': 0.64}
{'loss': 0.8855, 'grad_norm': 0.6501486897468567, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1889699697494507, 'eval_runtime': 3.2713, 'eval_samples_per_second': 305.688, 'eval_steps_per_second': 19.258, 'epoch': 0.68}
{'loss': 0.8231, 'grad_norm': 0.6260895729064941, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1977612972259521, 'eval_runtime': 3.2576, 'eval_samples_per_second': 306.976, 'eval_steps_per_second': 19.34, 'epoch': 0.72}
{'loss': 0.838, 'grad_norm': 0.6591687202453613, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1730449199676514, 'eval_runtime': 3.2642, 'eval_samples_per_second': 306.351, 'eval_steps_per_second': 19.3, 'epoch': 0.76}
{'loss': 0.8626, 'grad_norm': 0.6730211973190308, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.161380410194397, 'eval_runtime': 3.257, 'eval_samples_per_second': 307.028, 'eval_steps_per_second': 19.343, 'epoch': 0.8}
{'loss': 0.8681, 'grad_norm': 0.7475618720054626, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.175429344177246, 'eval_runtime': 3.271, 'eval_samples_per_second': 305.716, 'eval_steps_per_second': 19.26, 'epoch': 0.84}
{'loss': 0.8608, 'grad_norm': 0.6678256988525391, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1659941673278809, 'eval_runtime': 3.2655, 'eval_samples_per_second': 306.232, 'eval_steps_per_second': 19.293, 'epoch': 0.88}
{'loss': 0.8126, 'grad_norm': 0.6294008493423462, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.177221655845642, 'eval_runtime': 3.3005, 'eval_samples_per_second': 302.988, 'eval_steps_per_second': 19.088, 'epoch': 0.92}
{'loss': 0.9076, 'grad_norm': 0.7458582520484924, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1691519021987915, 'eval_runtime': 3.3003, 'eval_samples_per_second': 303.004, 'eval_steps_per_second': 19.089, 'epoch': 0.96}
{'loss': 0.8332, 'grad_norm': 0.745099663734436, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1678239107131958, 'eval_runtime': 3.2925, 'eval_samples_per_second': 303.724, 'eval_steps_per_second': 19.135, 'epoch': 1.0}
{'train_runtime': 244.8604, 'train_samples_per_second': 40.827, 'train_steps_per_second': 2.552, 'train_loss': 1.151861148071289, 'epoch': 1.0}
train_results:  {'eval_loss': [3.8915462493896484, 2.4803481101989746, 1.838446855545044, 1.638117790222168, 1.428168773651123, 1.3466973304748535, 1.28846275806427, 1.2033966779708862, 1.2371909618377686, 1.2047346830368042, 1.2385456562042236, 1.1905474662780762, 1.1791118383407593, 1.189400315284729, 1.1936893463134766, 1.1864632368087769, 1.1889699697494507, 1.1977612972259521, 1.1730449199676514, 1.161380410194397, 1.175429344177246, 1.1659941673278809, 1.177221655845642, 1.1691519021987915, 1.1678239107131958], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.8915462493896484, 2.4803481101989746, 1.838446855545044, 1.638117790222168, 1.428168773651123, 1.3466973304748535, 1.28846275806427, 1.2033966779708862, 1.2371909618377686, 1.2047346830368042, 1.2385456562042236, 1.1905474662780762, 1.1791118383407593, 1.189400315284729, 1.1936893463134766, 1.1864632368087769, 1.1889699697494507, 1.1977612972259521, 1.1730449199676514, 1.161380410194397, 1.175429344177246, 1.1659941673278809, 1.177221655845642, 1.1691519021987915, 1.1678239107131958]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.08113956451416
current iteration best possible eval_loss (full train run):  -1.1678239107131958
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708, -2.4318766593933105, -2.6227867603302, -2.8240251541137695, -3.603632926940918, -2.786780595779419, -2.038760185241699, -2.188371181488037, -2.08113956451416]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.0168 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -0.798458077289455
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -0.8114460866959434
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -0.8393568793433472
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -0.9515873414687708
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -0.8418662163265351
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1319, dtype=torch.float64), 0, tensor(0.0421, dtype=torch.float64), tensor(0.2404, dtype=torch.float64), tensor(0.0995, dtype=torch.float64), tensor(0.2143, dtype=torch.float64), 0, tensor(0.2087, dtype=torch.float64), tensor(0.0630, dtype=torch.float64), 24, 1, 0, 0, 1, 1, 2, 0.04860338496394051, 19.786273386043224, 1]
normalized proposed parameters for next round by BO: [tensor(0.1319, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0421, dtype=torch.float64), tensor(0.2404, dtype=torch.float64), tensor(0.0995, dtype=torch.float64), tensor(0.2143, dtype=torch.float64), tensor(6.7503e-18, dtype=torch.float64), tensor(0.2087, dtype=torch.float64), tensor(0.0630, dtype=torch.float64), tensor(0.7633, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.4860, dtype=torch.float64), tensor(0.4122, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.132
  gsm8k: 0
  rowan_hellaswag: 0.042
  sciq: 0.24
  triviaqa: 0.099
  truthfulqa_gen: 0.214
  wikitext: 0
  mmlu: 0.209
  arc_challenge: 0.063

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.04860338496394051,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (19.786273386043224,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.04860338496394051
lora alpha:  19.786273386043224
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,162,688 || all params: 8,032,423,936 || trainable%: 0.0269
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5879, 'grad_norm': 15.384865760803223, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.38140869140625, 'eval_runtime': 4.5819, 'eval_samples_per_second': 218.249, 'eval_steps_per_second': 13.75, 'epoch': 0.04}
{'loss': 1.7001, 'grad_norm': 7.323188781738281, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5114785432815552, 'eval_runtime': 3.3035, 'eval_samples_per_second': 302.709, 'eval_steps_per_second': 19.071, 'epoch': 0.08}
{'loss': 1.2641, 'grad_norm': 2.259857177734375, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3208746910095215, 'eval_runtime': 3.2923, 'eval_samples_per_second': 303.739, 'eval_steps_per_second': 19.136, 'epoch': 0.12}
{'loss': 1.1664, 'grad_norm': 1.8713886737823486, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.17136549949646, 'eval_runtime': 3.3008, 'eval_samples_per_second': 302.958, 'eval_steps_per_second': 19.086, 'epoch': 0.16}
{'loss': 1.1702, 'grad_norm': 1.4223142862319946, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1439341306686401, 'eval_runtime': 3.4276, 'eval_samples_per_second': 291.749, 'eval_steps_per_second': 18.38, 'epoch': 0.2}
{'loss': 1.1371, 'grad_norm': 1.7512422800064087, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1136376857757568, 'eval_runtime': 3.3024, 'eval_samples_per_second': 302.807, 'eval_steps_per_second': 19.077, 'epoch': 0.24}
{'loss': 1.0477, 'grad_norm': 1.3681235313415527, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1053881645202637, 'eval_runtime': 3.2987, 'eval_samples_per_second': 303.151, 'eval_steps_per_second': 19.099, 'epoch': 0.28}
{'loss': 1.0332, 'grad_norm': 1.0345045328140259, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0747052431106567, 'eval_runtime': 3.287, 'eval_samples_per_second': 304.232, 'eval_steps_per_second': 19.167, 'epoch': 0.32}
{'loss': 1.0946, 'grad_norm': 1.1073802709579468, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0818085670471191, 'eval_runtime': 3.2933, 'eval_samples_per_second': 303.651, 'eval_steps_per_second': 19.13, 'epoch': 0.36}
{'loss': 1.1161, 'grad_norm': 0.8671092987060547, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.059924602508545, 'eval_runtime': 3.2968, 'eval_samples_per_second': 303.327, 'eval_steps_per_second': 19.11, 'epoch': 0.4}
{'loss': 1.0846, 'grad_norm': 0.9933390617370605, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0673145055770874, 'eval_runtime': 3.2993, 'eval_samples_per_second': 303.098, 'eval_steps_per_second': 19.095, 'epoch': 0.44}
{'loss': 1.0556, 'grad_norm': 0.9529693126678467, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0700961351394653, 'eval_runtime': 3.3109, 'eval_samples_per_second': 302.03, 'eval_steps_per_second': 19.028, 'epoch': 0.48}
{'loss': 1.0608, 'grad_norm': 1.1760810613632202, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0621896982192993, 'eval_runtime': 3.3126, 'eval_samples_per_second': 301.879, 'eval_steps_per_second': 19.018, 'epoch': 0.52}
{'loss': 1.0213, 'grad_norm': 0.9927433729171753, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0589019060134888, 'eval_runtime': 3.3029, 'eval_samples_per_second': 302.76, 'eval_steps_per_second': 19.074, 'epoch': 0.56}
{'loss': 1.0576, 'grad_norm': 1.160404086112976, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0547451972961426, 'eval_runtime': 3.3059, 'eval_samples_per_second': 302.487, 'eval_steps_per_second': 19.057, 'epoch': 0.6}
{'loss': 0.9797, 'grad_norm': 1.0079950094223022, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0625343322753906, 'eval_runtime': 3.3093, 'eval_samples_per_second': 302.177, 'eval_steps_per_second': 19.037, 'epoch': 0.64}
{'loss': 0.9893, 'grad_norm': 1.3124465942382812, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0514659881591797, 'eval_runtime': 3.3417, 'eval_samples_per_second': 299.253, 'eval_steps_per_second': 18.853, 'epoch': 0.68}
{'loss': 0.9697, 'grad_norm': 1.180423378944397, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0582932233810425, 'eval_runtime': 3.3508, 'eval_samples_per_second': 298.438, 'eval_steps_per_second': 18.802, 'epoch': 0.72}
{'loss': 1.0116, 'grad_norm': 1.1091818809509277, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0524699687957764, 'eval_runtime': 3.3727, 'eval_samples_per_second': 296.5, 'eval_steps_per_second': 18.68, 'epoch': 0.76}
{'loss': 0.9903, 'grad_norm': 1.3947217464447021, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.053534984588623, 'eval_runtime': 3.3598, 'eval_samples_per_second': 297.633, 'eval_steps_per_second': 18.751, 'epoch': 0.8}
{'loss': 1.0256, 'grad_norm': 1.1113210916519165, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0588901042938232, 'eval_runtime': 3.3415, 'eval_samples_per_second': 299.268, 'eval_steps_per_second': 18.854, 'epoch': 0.84}
{'loss': 1.0911, 'grad_norm': 0.94361412525177, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0522116422653198, 'eval_runtime': 3.3457, 'eval_samples_per_second': 298.892, 'eval_steps_per_second': 18.83, 'epoch': 0.88}
{'loss': 0.9627, 'grad_norm': 0.9117979407310486, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.048362135887146, 'eval_runtime': 3.3689, 'eval_samples_per_second': 296.837, 'eval_steps_per_second': 18.701, 'epoch': 0.92}
{'loss': 1.0014, 'grad_norm': 1.2689645290374756, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0487242937088013, 'eval_runtime': 3.3684, 'eval_samples_per_second': 296.874, 'eval_steps_per_second': 18.703, 'epoch': 0.96}
{'loss': 0.9705, 'grad_norm': 1.2833958864212036, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.047879934310913, 'eval_runtime': 3.3515, 'eval_samples_per_second': 298.373, 'eval_steps_per_second': 18.797, 'epoch': 1.0}
{'train_runtime': 282.1068, 'train_samples_per_second': 35.437, 'train_steps_per_second': 2.215, 'train_loss': 1.1835695678710938, 'epoch': 1.0}
train_results:  {'eval_loss': [2.38140869140625, 1.5114785432815552, 1.3208746910095215, 1.17136549949646, 1.1439341306686401, 1.1136376857757568, 1.1053881645202637, 1.0747052431106567, 1.0818085670471191, 1.059924602508545, 1.0673145055770874, 1.0700961351394653, 1.0621896982192993, 1.0589019060134888, 1.0547451972961426, 1.0625343322753906, 1.0514659881591797, 1.0582932233810425, 1.0524699687957764, 1.053534984588623, 1.0588901042938232, 1.0522116422653198, 1.048362135887146, 1.0487242937088013, 1.047879934310913], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.38140869140625, 1.5114785432815552, 1.3208746910095215, 1.17136549949646, 1.1439341306686401, 1.1136376857757568, 1.1053881645202637, 1.0747052431106567, 1.0818085670471191, 1.059924602508545, 1.0673145055770874, 1.0700961351394653, 1.0621896982192993, 1.0589019060134888, 1.0547451972961426, 1.0625343322753906, 1.0514659881591797, 1.0582932233810425, 1.0524699687957764, 1.053534984588623, 1.0588901042938232, 1.0522116422653198, 1.048362135887146, 1.0487242937088013, 1.047879934310913]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.0122570991516113
current iteration best possible eval_loss (full train run):  -1.047879934310913
max eval_loss so far:  -0.9916144609451294
BO observations:  [-1.8678375482559204, -3.6595993041992188, -4.5169548988342285, -2.9427738189697266, -1.4833648204803467, -2.288141965866089, -2.2381882667541504, -1.936268925666809, -4.582779407501221, -2.859452486038208, -2.9843177795410156, -2.1346352100372314, -1.7009090185165405, -1.6125328540802002, -2.0853421688079834, -3.70357084274292, -3.230616569519043, -2.4995973110198975, -2.002169609069824, -1.9748255014419556, -1.6635209321975708, -2.4318766593933105, -2.6227867603302, -2.8240251541137695, -3.603632926940918, -2.786780595779419, -2.038760185241699, -2.188371181488037, -2.08113956451416, -2.0122570991516113]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.6420 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -0.7434790879044075
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -0.8306410437701317
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -0.9991300148760707
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -0.6424680412393267
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -0.7941839917015279
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0430, dtype=torch.float64), tensor(0.1178, dtype=torch.float64), 0, tensor(0.0999, dtype=torch.float64), tensor(0.1666, dtype=torch.float64), tensor(0.1790, dtype=torch.float64), 0, tensor(0.2823, dtype=torch.float64), tensor(0.1107, dtype=torch.float64), 29, 0, 1, 1, 0, 1, 48, 0.054501441670073994, 31.527092550846582, 1]
normalized proposed parameters for next round by BO: [tensor(0.0430, dtype=torch.float64), tensor(0.1178, dtype=torch.float64), tensor(6.0491e-18, dtype=torch.float64), tensor(0.0999, dtype=torch.float64), tensor(0.1666, dtype=torch.float64), tensor(0.1790, dtype=torch.float64), tensor(0.0007, dtype=torch.float64), tensor(0.2823, dtype=torch.float64), tensor(0.1107, dtype=torch.float64), tensor(0.8944, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3755, dtype=torch.float64), tensor(0.5450, dtype=torch.float64), tensor(0.6568, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-1.0653173923492432, -1.0653173923492432, -1.007631540298462, -1.007631540298462, -1.007631540298462, -1.007631540298462, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'triviaqa', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_triviaqa_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['triviaqa'], 'weight': [1.0], 'random': [[-1.0554845333099365, -1.0554845333099365, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.022034764289856, -1.0161330699920654, -1.002569556236267, -1.002569556236267, -1.002569556236267, -1.002569556236267, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923, -0.9920860528945923], [-1.0572949647903442, -1.0572949647903442, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215, -0.9699225425720215], [-1.0653173923492432, -1.0653173923492432, -1.007631540298462, -1.007631540298462, -1.007631540298462, -1.007631540298462, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294, -0.9916144609451294]], 'random_full_inputs': [[[0, 0.43344608851365884, 0, 0.3811176499317749, 0.08520148329265512, 0, 0, 0.10023477826191117, 0, 20, 0, 1, 0, 1, 1, 2, 0.016001276169592498, 48.0, 1], [0.010771161339845998, 0.12670526798044543, 0.024248526780359405, 0.11325355193206837, 0, 0, 0, 0.22298617009844687, 0.502035321868834, 1, 1, 0, 1, 0, 1, 128, 1.5088409119918436e-18, 48.0, 1], [0, 0, 0, 0, 0.10894576285118912, 0.3128965240582423, 0, 0.4467310381723545, 0.13142667491821436, 32, 1, 0, 1, 1, 1, 128, 7.070722313504211e-18, 48.0, 0], [0, 0.11344859151764149, 0, 0, 0.40083121109767555, 0.3575780617404256, 0, 0.12814213564425683, 0, 23, 0, 1, 0, 0, 0, 12, 0.011290650405569095, 44.94784649994607, 1], [0.033580187775053914, 0, 0, 0.14502024221211832, 0, 0.5463557991006425, 0, 0, 0.27504377091218546, 3, 0, 0, 1, 1, 1, 47, 0.1, 1.4800000190734883, 1], [0.1548353354143881, 0, 0, 0, 0, 0.6137798481665279, 0, 0.13095966778877477, 0.10042514863030919, 32, 0, 1, 0, 0, 1, 128, 0.05150083061893645, 26.744626045815792, 1], [0, 0.3915587246200522, 0.050620447801911425, 0.34868515851264703, 0.20913566906538933, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 107, 0.08725557331767518, 18.62627501178458, 1], [0.06959069814003808, 0.2925732300395043, 0, 0, 0.22654001996291512, 0.08674421695282697, 0, 0, 0.3245518349047156, 18, 1, 1, 1, 0, 0, 54, 0.09993012740129961, 31.225536622143487, 1], [0.32375263746997596, 0, 0, 0, 0, 0.27660789862441165, 0.17630956476028292, 0.17089683919750848, 0.05243305994782118, 8, 0, 0, 0, 1, 1, 2, 0.09435120808013689, 1.4800000190734863, 1], [0, 0.19253758336725718, 0, 0, 0.24721959723088513, 0.2341444722083411, 0, 0, 0.32609834719351655, 29, 0, 0, 0, 1, 1, 21, 0.05887531104273486, 13.126751468842766, 0], [0, 0, 0, 0.1690527186516657, 0.35431081055362496, 0.21327660939671253, 0, 0.1543656665834593, 0.10899419481453745, 26, 0, 1, 0, 1, 1, 2, 0.1, 48.0, 0], [0.06166256988774231, 0.03765427847677077, 0, 0, 0.12802373414552773, 0.28081403658356563, 0.3447759137130196, 0.14635873614783018, 0, 16, 0, 0, 0, 1, 1, 2, 0.1, 48.0, 0], [0.17867024295586853, 0.11002800065315424, 0, 0.1533680678564144, 0.12894610391968003, 0.18439414550725314, 0.013932671207579746, 0.14621123483285384, 0.0761966744173534, 12, 0, 1, 0, 0, 1, 58, 0.0701835485681922, 33.126831708357024, 1], [0.07581040499590688, 0.22954782355675352, 0.0904398795650076, 0, 0.09601196771438819, 0.31568868648811643, 0.08062806320487768, 0.11187317447494978, 0, 20, 0, 0, 1, 1, 1, 75, 0.03659911297533354, 13.596966742888617, 1], [0, 0.29539105513163144, 0.11708018399890086, 0, 0.25343020623104706, 0, 0.08379614889855194, 0.18543275369548748, 0.06486965204438068, 32, 0, 1, 0, 1, 1, 6, 0.1, 47.99999999999997, 1], [0.027684838247980067, 0.5701326051999491, 0.034200094188994584, 0.14515695320162814, 0.10614484468612166, 0, 0.032565538296977274, 0, 0.08411512617834911, 1, 0, 0, 1, 1, 1, 11, 0.1, 41.00748952346063, 1], [0, 0.37981359328005954, 0, 0, 0.25510410822823804, 0.07589073878271668, 0, 0, 0.28919155970898314, 32, 0, 1, 0, 0, 1, 128, 1.4476073581606425e-18, 1.4800000190734894, 1], [0.11790560278119203, 0.17292004663465918, 0.08588114887981955, 0.20198936849289473, 0.01615404034597592, 0, 0.271178461014499, 0.01931088946773445, 0.11466044238322518, 14, 1, 0, 1, 1, 0, 72, 0.0727561254406501, 26.541880994595605, 1], [0.09795499692034244, 0.17870574266121983, 0.0962682931645946, 0.315146467524515, 0.05779985058853045, 0.1336030301010443, 0.07570549676573333, 0.044816122274020195, 0, 21, 1, 0, 1, 1, 1, 57, 0.0873667913653274, 38.98837176613616, 0], [0.010310853138242366, 0.16296548179614534, 0.19237156133640723, 0.17426761292456017, 0.03589695942440453, 0.04231051697968224, 0.14176181846833755, 0.14687265650495931, 0.09324253942726131, 30, 0, 1, 0, 0, 1, 69, 0.055593431613738954, 36.50005180216975, 0], [0.2841258555332915, 0, 0.021262420196652875, 0.21373069041605255, 0, 0.14622986466829158, 0.16533448367797826, 0, 0.16931668550773318, 23, 1, 0, 1, 1, 1, 3, 1.9071165025763359e-19, 1.4800000190734899, 1], [0.13777123276166642, 0.19491278210202023, 0.09267563211666192, 0.03259366288857232, 0.09508042521917198, 0.1403438347892031, 0.10678410790627689, 0.19983832221642722, 0, 13, 0, 1, 0, 0, 1, 65, 0.05898316017872403, 48.0, 1], [0, 0.06164447678292574, 0.02959482542829345, 0.16944785964162187, 0, 0.49783304212080637, 0.13723490426344837, 0.08786351535312996, 0, 13, 0, 0, 1, 0, 1, 78, 2.741358050066029e-18, 26.386450441831606, 1], [0, 0, 0.029214579367853852, 0.07254278272364058, 0.09123406458073778, 0.7408559497187461, 0, 0.05648759788314907, 0, 10, 0, 0, 0, 1, 1, 47, 0.06449199985136676, 12.843382946097936, 1], [0.21025588993350044, 0.1378981877179186, 0.010342312786693173, 0.20315200950637285, 0.25954639696761367, 0, 0, 0.17880520308790118, 0, 5, 1, 0, 1, 1, 1, 29, 0.016384832839622717, 28.176816447922665, 0], [0, 0.22042786369391332, 0, 0, 0.19326028516564736, 0, 0, 0.1910312450651225, 0.39528060607531684, 29, 0, 1, 0, 0, 1, 91, 0.06907066364054007, 40.98000772493139, 1], [0.16107924707567306, 0.15824942833382355, 0.06021044623433057, 0.020256228977063587, 0.08028671211623492, 0.13835160883751857, 0.2978822354677879, 0, 0.08368409295756782, 22, 0, 0, 1, 1, 1, 111, 0.07247322572700607, 30.362204674623356, 0], [0.05608018448622925, 0.3728993960680593, 0.15783487222816056, 0, 0.31475261724196735, 0, 0, 0, 0.0984329299755841, 1, 0, 1, 0, 1, 1, 128, 0.021359316116515314, 48.0, 1], [0.08275222138993871, 0.07969044044958963, 0.06887534971903503, 0.22847184007989982, 0, 0.06928158569004661, 0.10971078657179899, 0.23437518218316217, 0.12684259391652902, 19, 1, 0, 1, 1, 1, 29, 0.001970394210075999, 29.437157873163756, 0], [0.012372764495731346, 0, 0.09933032156917948, 0.25946036358942637, 0.20324764810802448, 0.05877726654245506, 0.1336459181285787, 0.19471210224570681, 0.028750021809266262, 2, 0, 0, 0, 1, 0, 20, 0.06067205475414984, 17.709598502598823, 1]], [[0, 0.4323808151009794, 0, 0.3819431608974698, 0.08706796422870898, 0, 0, 0.09844251144253185, 0, 19, 0, 1, 0, 1, 1, 2, 0.015192848619470714, 48.0, 1], [0.31004357030128893, 0, 0.05642388530667118, 0.09507008590632504, 0, 0, 0, 0.12072839682598396, 0.417734061659731, 1, 1, 0, 1, 1, 1, 128, 0.1, 48.0, 1], [0.0877831397848698, 0, 0, 0, 0.5053066165974258, 0.29515063303954037, 0, 0, 0.11175961057816368, 19, 1, 0, 1, 1, 1, 128, 0.1, 48.0, 1], [0.1705568602384688, 0.16298745613788482, 0, 0, 0.15595431126793832, 0.5105013723557077, 0, 0, 0, 32, 1, 1, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0], [0.4495464737867796, 0, 0, 0, 0.09468104268146796, 0.3479105367539681, 0, 0.010819484190861471, 0.09704246258692288, 13, 0, 1, 0, 0, 1, 2, 0.079022171771916, 35.62550846097296, 1], [0, 0, 0, 0, 0.06730338055169494, 0.33129380564396477, 0, 0.4846707947475051, 0.11673201905683583, 17, 1, 0, 0, 0, 0, 2, 0.07042970897097416, 37.14946038585987, 0], [0.10476417012189963, 0, 0, 0.33182327819043045, 0.11097891329690768, 0.22460298841807702, 0, 0.1177236078787197, 0.1101070420939656, 15, 1, 1, 0, 0, 1, 128, 0.06265432744516708, 29.433840312472377, 1], [0, 0.025673184256605492, 0, 0.569705478727333, 0, 0.3185765685120317, 0.08604476850402977, 0, 0, 32, 0, 0, 1, 1, 1, 34, 0.0, 1.4800000190734863, 1], [0, 0, 0, 0.1523799437831959, 0, 0.3627010417530496, 0.4849190144637545, 0, 0, 32, 0, 1, 1, 0, 1, 36, 1.1621788153653562e-17, 1.4800000190734874, 1], [0.020861372275656487, 0, 0, 0.14208731940876432, 0.06440099150258051, 0.5615761694723033, 0, 0, 0.21107414734069546, 16, 1, 0, 0, 1, 1, 2, 0.049317297166327156, 15.43183295064189, 1], [0.2496519677957967, 0.05871850905662572, 0.030897002675006283, 0, 0, 0.28373665742779997, 0, 0, 0.36875651734011716, 32, 0, 0, 1, 1, 1, 2, 1.3010426069826055e-19, 1.4800000190734863, 1], [0.20820045379617158, 0.026457601672742068, 0.07432602717629688, 0.15854554553303582, 0.1243580998924889, 0.28740258141823727, 0.05701053325668936, 0.06369915725433802, 0, 24, 0, 0, 1, 1, 1, 18, 0.1, 48.0, 1], [0.010561048110759103, 0.061455734847799044, 0, 0, 0.6549629727567848, 0.27302024428465704, 0, 0, 0, 16, 0, 0, 1, 1, 1, 35, 0.06290496016505076, 13.840942762771705, 0], [0, 0.471411064028747, 0.05212016579549621, 0.19503128496262848, 0, 0, 0.28143748521312817, 0, 0, 30, 0, 0, 1, 1, 0, 63, 0.08518476853207853, 22.528118046379586, 0], [0.27906021185474444, 0.19389381734271774, 0, 0, 0.0946100299677958, 0.12324519409889391, 0.20181986684102501, 0, 0.10737087989482312, 18, 0, 0, 0, 1, 1, 18, 0.1, 20.515138757384108, 1], [0.1216609454602343, 0.20885230284024003, 0, 0.15745541447817718, 0.17139106843640772, 0.1311991338929086, 0.0666838649020023, 0.12900214010350317, 0.013755129886526801, 16, 1, 0, 0, 1, 1, 92, 0.0511302672952863, 10.789477189814786, 1], [0.3471665276103035, 0.1790383022704611, 0.18206608093779145, 0.10181641271113197, 0, 0.1527486901669844, 0.029594657395486036, 0, 0, 7, 1, 1, 0, 0, 0, 61, 0.006344214693868151, 32.093886679560555, 0], [0.21039177681242505, 0.4574413279527275, 0.01234773930122268, 0, 0, 0.10390198091033097, 0, 0.14033725852006235, 0.07557991650323145, 1, 0, 0, 1, 0, 1, 44, 0.1, 48.0, 1], [0.19479657952873294, 0.07835286902044984, 0.03387434344911114, 0.20863700423882345, 0.012261490819029976, 0.15014679604369702, 0.1516351976938262, 0, 0.1702957192063293, 22, 1, 0, 0, 1, 1, 15, 0.06238737692724542, 1.480000019073487, 1], [0, 0.23815291901827432, 0.045969321592206305, 0, 0.15231800878376026, 0.29630166640299804, 0, 0.03902886320633067, 0.22822922099643045, 24, 1, 0, 0, 1, 1, 69, 0.0, 36.95260587119836, 0], [0, 0.05636014665138384, 0.10414203875095268, 0.16746473619891905, 0.09269931899862184, 0.08366366180293126, 0.07522144141139911, 0.23329804255641204, 0.18715061362938035, 25, 0, 1, 1, 0, 1, 39, 0.044988865548113315, 19.75568290132993, 1], [0, 0, 0.06259512496367083, 0.15684389154739875, 0, 0.46270204395664627, 0.14448627904634906, 0.1650651853785582, 0, 28, 0, 0, 0, 1, 1, 2, 0.007812853312195145, 21.0713794064989, 1], [0.019607956971069585, 0.4257174186652572, 0.15669864312738305, 0, 0, 0.10014367569744607, 0.13294610794920786, 0, 0.16476457533976416, 32, 0, 1, 0, 0, 1, 115, 0.040591611131585006, 48.0, 0], [0.09495096693174922, 0.02555187965393776, 0.030221485634427978, 0.0660392106566643, 0.29187696712345595, 0.2510437217891698, 0.0907446517706605, 0.03523458424497465, 0.1143365321949598, 9, 1, 0, 1, 1, 1, 19, 0.013979734770157884, 27.10072024694999, 0], [0.012088117491320379, 0, 0.028506188594131864, 0.20030556099834937, 0.15635519454430818, 0.5749632865397463, 0.012356492997167503, 0.015425158834976565, 0, 12, 0, 0, 0, 1, 1, 90, 0.07476041509448453, 27.01331555057135, 0], [0.3436209797297963, 0.09260033255071241, 0.05125739768584486, 0.15569324899777168, 0, 0, 0.2905560864641854, 0, 0.06627195457168927, 32, 1, 0, 1, 1, 1, 5, 0.01811631925323592, 1.4800000190734863, 1], [0, 0.02604218888648718, 0, 0, 0, 0.22293226947026837, 0.16404735653308017, 0, 0.5869781851101646, 32, 1, 0, 0, 1, 1, 2, 3.6190956100079925e-19, 1.4800000190734892, 1], [0.2815498857332306, 0.03181800510019391, 0, 0, 0, 0.676592039598763, 0.010040069567812424, 0, 0, 26, 1, 0, 0, 1, 1, 2, 0.08852578400602816, 1.4800000190734872, 1], [0.09530584916012677, 0.24125723350098743, 0.0323420811953794, 0.16502091251780784, 0.13376722424100956, 0.15265882206025244, 0, 0.07675127092997336, 0.10289660639446319, 1, 0, 0, 1, 0, 0, 107, 0.025631277947454983, 32.95105397776969, 1], [0, 0, 0.0480000199908507, 0.6765224198391299, 0.0633799788198625, 0, 0.07341289516690044, 0, 0.13868468618325652, 5, 1, 0, 1, 1, 1, 3, 8.794381065933759e-20, 33.197296134671035, 1]], [[0, 0.4323905848743851, 0, 0.3815333370720861, 0.08646685348536817, 0, 0, 0.09928417701254043, 0, 20, 0, 1, 0, 1, 1, 2, 0.015055211658025609, 48.0, 1], [0, 0.1357736224996318, 0.022693679126703396, 0.1144037898019047, 0, 0, 0, 0.21808916948728807, 0.5017319844816209, 1, 1, 0, 1, 0, 1, 128, 8.67361737988404e-19, 48.0, 1], [0, 0, 0, 0, 0.19279253037296465, 0.34757459804305024, 0, 0.33311664849442013, 0.1265162230895648, 32, 1, 0, 1, 1, 1, 128, 0.1, 47.99999999999997, 0], [0, 0.20375603877649187, 0, 0, 0, 0.7281027025026027, 0, 0.04720503353010209, 0.0209362251908035, 32, 0, 1, 0, 0, 1, 43, 0.09999999999999991, 48.0, 1], [0, 0, 0, 0.47007484102222397, 0.05327656696235948, 0.47664859201541643, 0, 0, 0, 1, 0, 0, 1, 1, 1, 2, 0.0, 1.4800000190734883, 1], [0.40124571475347515, 0, 0, 0, 0, 0.2147222993035878, 0, 0.02495773226473392, 0.3590742536782034, 32, 0, 0, 1, 1, 1, 2, 0.09999999999999999, 1.4800000190734863, 1], [0.016107383911060952, 0.010692236672003043, 0, 0, 0.5113791117750276, 0.1827057328753529, 0, 0.0777648131342399, 0.20135072163231565, 32, 0, 0, 0, 1, 0, 2, 0.1, 48.0, 1], [0, 0.21314684190113706, 0, 0, 0.38557386953390277, 0.2719576537691092, 0, 0.0733963253075553, 0.055925309488295674, 15, 0, 1, 0, 0, 1, 5, 0.05613173269979176, 1.480000019073488, 1], [0.2926145845999105, 0.5619500161595674, 0, 0, 0, 0.10893122801706014, 0, 0, 0.03650417122346173, 32, 1, 1, 0, 1, 0, 128, 0.1, 34.33197864879717, 1], [0, 0.09206224275468436, 0, 0, 0.13690917420275706, 0.17543551223062107, 0, 0.18552840752582048, 0.4100646632861169, 25, 0, 1, 0, 0, 1, 48, 0.05124491130777471, 40.90124734787581, 1], [0, 0.2617850754585956, 0, 0.1779259881837649, 0.2860578153399426, 0, 0, 0.07937258931820262, 0.1948585316994941, 1, 0, 0, 0, 1, 1, 87, 0.04459271043945777, 2.981637095364092, 0], [0.13001195695533554, 0.15757835125297986, 0, 0, 0.37492544494989866, 0.1693213812542441, 0, 0.12028865846393486, 0.04329388387031187, 32, 0, 0, 1, 1, 1, 2, 0.05413224901059108, 28.435128151584752, 1], [0.2383432103871811, 0, 0.14439643552236828, 0.23683185248879488, 0.012508242580800731, 0, 0.23394355180108464, 0.06214904180490547, 0.06483886999840305, 13, 0, 0, 1, 0, 1, 2, 0.03233549602939531, 25.304959352633944, 1], [0.11537529765802787, 0.15506320290025954, 0, 0, 0.11413147836136323, 0.2407183271435037, 0.14095007738941928, 0.1413688215847526, 0.0923927949626736, 10, 0, 1, 0, 0, 1, 2, 0.1, 21.2821470543047, 1], [0.14129720529167114, 0, 0, 0.11783122570964599, 0, 0.13525766420256088, 0.5156428994374717, 0, 0.08997100535865027, 25, 0, 0, 0, 1, 1, 2, 6.938893903907226e-19, 15.18202875193839, 0], [0, 0, 0.18193088156180426, 0.1274129738791714, 0.10910220810677324, 0, 0.4403193489862397, 0.10872156213823268, 0.03251302532777884, 4, 0, 0, 1, 1, 1, 128, 0.1, 32.074292495490624, 1], [0.03936360480232542, 0.22340627034529648, 0.03645409765509333, 0.12115375818510947, 0.21837300430602857, 0.06476478087110063, 0.12977019184542513, 0.10636117010324116, 0.060353121886379785, 14, 0, 1, 0, 0, 1, 87, 0.06940412332691014, 25.604197778320664, 1], [0.08827907356322323, 0.305601467903277, 0.05855240570915062, 0.26829074942636333, 0.07205364249240513, 0.07802819400203612, 0, 0.06446649127474718, 0.06472797562879738, 23, 0, 0, 1, 1, 1, 32, 0.07241185047507408, 11.896247618176, 1], [0.0955430422980331, 0.016429141221010857, 0, 0.3880595916464283, 0.12859777874526446, 0.16921175191990748, 0.09936368699649906, 0, 0.09848663659309245, 5, 0, 0, 0, 1, 0, 30, 0.05400215848045482, 10.3000233254071, 0], [0.3232768470803378, 0.12825285165595154, 0, 0, 0, 0.37867676288577334, 0.07037427519661406, 0, 0.09941926318132344, 22, 0, 0, 0, 1, 1, 2, 2.7704328601991323e-18, 48.0, 0], [0.10185724835882777, 0.050286237359512674, 0.11294149052342868, 0.08639087163652621, 0.04488256493746977, 0.2978149820773459, 0.05966784753779953, 0.2461587575690895, 0, 10, 1, 0, 0, 1, 1, 2, 0.08570145488691097, 39.26682617118926, 0], [0, 0.05493551556024237, 0.10308428428970097, 0.19631091316149296, 0.07283921458321242, 0, 0, 0.35469159792740457, 0.21813847447794665, 32, 0, 0, 1, 1, 1, 16, 0.009435433931352431, 37.91527928248978, 1], [0.09315993980053262, 0.23177848270332987, 0, 0.13636249809565823, 0.07524634766655837, 0.047679514861878075, 0.09188648307219273, 0.09983421495181272, 0.2240525188480375, 13, 1, 0, 1, 1, 1, 53, 0.0709688333378147, 45.96024247538632, 0], [0, 0, 0.09176532146036132, 0.3695366997512747, 0.07899583071856812, 0.012380727234240288, 0.11874329737071294, 0.07140326344751195, 0.2571748600173308, 23, 1, 0, 1, 1, 1, 45, 0.07467627965381037, 1.4800000190734863, 1], [0, 0.12655597463796642, 0.07128150372971379, 0.2956296031253923, 0, 0.3919970144598444, 0, 0.09217007711322016, 0.02236572631267467, 11, 1, 0, 1, 1, 1, 113, 0.08787708619886861, 47.66118790360915, 0], [0.050314320457476185, 0.18183102324423536, 0.14186617910740723, 0, 0, 0.15402802906750915, 0.36664514147885796, 0.024386270990187017, 0.08092903565432713, 32, 0, 0, 0, 1, 0, 30, 0.0, 36.30623404571663, 0], [0.05851515628616151, 0.05309926045221097, 0, 0.23711468330994653, 0, 0.3013751586887048, 0, 0.22805227951780363, 0.12184346174517266, 20, 1, 0, 0, 1, 1, 2, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0, 0.12426659103640424, 0.8757334089635957, 0, 0, 0, 11, 0, 1, 0, 0, 1, 30, 1.4730404396257059e-18, 1.4800000190734863, 1], [0, 0, 0, 0.10936189303336401, 0.03133926505686431, 0.4382574268129099, 0.09549814273002574, 0, 0.32554327236683633, 22, 1, 0, 0, 1, 1, 2, 0.08340210436602336, 1.4800000190734872, 1], [0.13193411605607255, 0, 0.04214412440437406, 0.24043213864949553, 0.0994701185605382, 0.21432574640495167, 0, 0.2086845827098633, 0.06300917321470483, 24, 1, 0, 0, 1, 1, 2, 0.04860338496394051, 19.786273386043224, 1]]], 'random_full_train_performance': [-1.0653173923492432, -1.7554330825805664, -1.007631540298462, -1.9016895294189453, -1.8701083660125732, -1.8139086961746216, -0.9916144609451294, -1.2968602180480957, -1.9560856819152832, -1.0501009225845337, -1.384283185005188, -0.9961082935333252, -1.170220136642456, -1.1720843315124512, -1.827252984046936, -1.062615990638733, -1.159082055091858, -1.0690475702285767, -1.2842663526535034, -2.0383248329162598, -1.2530968189239502, -1.044002890586853, -1.155346155166626, -1.074126958847046, -1.7245124578475952, -1.6126593351364136, -1.6502131223678589, -1.4079443216323853, -1.1678239107131958, -1.047879934310913]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.055 MB of 0.055 MB uploadedwandb: - 0.055 MB of 0.084 MB uploadedwandb: \ 0.709 MB of 1.260 MB uploadedwandb: | 1.260 MB of 1.260 MB uploadedwandb: / 1.260 MB of 1.260 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÜ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÑ‚ñÇ
wandb:            eval/runtime ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb: eval/samples_per_second ‚ñà‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ
wandb:   eval/steps_per_second ‚ñà‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ
wandb:             train/epoch ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñá‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñÅ‚ñÖ
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñá‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÅ‚ñÖ
wandb:         train/grad_norm ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÇ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñà‚ñÑ‚ñá‚ñÉ
wandb:              train/loss ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                eval/loss 1.04788
wandb:             eval/runtime 3.3515
wandb:  eval/samples_per_second 298.373
wandb:    eval/steps_per_second 18.797
wandb:               total_flos 9.62540724289536e+16
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 1.2834
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.9705
wandb:               train_loss 1.18357
wandb:            train_runtime 282.1068
wandb: train_samples_per_second 35.437
wandb:   train_steps_per_second 2.215
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/hoxe9otw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260101_054216-hoxe9otw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

2026-01-01 05:33:56.212259: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-01 05:33:56.238898: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-01 05:33:56.238975: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-01 05:33:56.239914: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-01 05:33:56.244763: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-01 05:33:57.135750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'arc_challenge', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_arc_challenge_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['arc_challenge']
evaluation tasks and weights:  {'arc_challenge': (1.0, 'acc,none')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
arc_challenge
evaluation dataset:
data domain:  arc_challenge  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/arc_challenge/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 17, 1, 0, 1, 1, 0, 59, 0.00445603671922955, 41, 0]
Checking history sample input_X_between_0_1:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 0.53125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4609375, 0.044560367192295496, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3072175979614258
Checking history sample input_X:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 22, 1, 1, 1, 0, 0, 61, 0.08406775506720655, 5, 1]
Checking history sample input_X_between_0_1:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 0.6875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4765625, 0.8406775506720655, 0.10416666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1613417863845825
Checking history sample input_X:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 15, 1, 1, 1, 1, 0, 24, 0.05173325143341287, 28, 0]
Checking history sample input_X_between_0_1:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 0.46875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1875, 0.5173325143341286, 0.5833333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1443150043487549
Checking history sample input_X:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 1, 0, 1, 1, 0, 0, 114, 0.021033555465481826, 23, 1]
Checking history sample input_X_between_0_1:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 0.03125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.890625, 0.21033555465481824, 0.4791666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.3097282648086548
Checking history sample input_X:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 27, 1, 1, 1, 1, 1, 70, 0.005963494977880357, 46, 1]
Checking history sample input_X_between_0_1:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 0.05963494977880357, 0.9583333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9743626117706299
Checking history sample input_X:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 12, 1, 0, 1, 0, 0, 3, 0.017954470577765235, 12, 1]
Checking history sample input_X_between_0_1:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0234375, 0.17954470577765233, 0.25, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2079665660858154
Checking history sample input_X:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 18, 1, 0, 0, 0, 1, 5, 0.028541773579282805, 35, 0]
Checking history sample input_X_between_0_1:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 0.5625, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0390625, 0.28541773579282803, 0.7291666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0695475339889526
Checking history sample input_X:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 24, 0, 1, 0, 1, 0, 55, 0.08420649661865921, 22, 0]
Checking history sample input_X_between_0_1:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 0.75, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4296875, 0.8420649661865921, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0792094469070435
Checking history sample input_X:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 30, 1, 0, 1, 1, 0, 5, 0.09059974254780842, 29, 1]
Checking history sample input_X_between_0_1:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 0.9375, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0390625, 0.9059974254780842, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9744297862052917
Checking history sample input_X:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 20, 1, 1, 0, 1, 0, 27, 0.07188580123073206, 22, 0]
Checking history sample input_X_between_0_1:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 0.625, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2109375, 0.7188580123073205, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0874465703964233
Checking history sample input_X:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 8, 0, 0, 0, 1, 1, 31, 0.06288146497812035, 13, 1]
Checking history sample input_X_between_0_1:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2421875, 0.6288146497812034, 0.2708333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0237352848052979
Checking history sample input_X:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 32, 1, 1, 0, 1, 1, 114, 0.08252011949389138, 13, 0]
Checking history sample input_X_between_0_1:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.890625, 0.8252011949389138, 0.2708333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1076443195343018
Checking history sample input_X:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 11, 0, 1, 0, 0, 1, 100, 0.09203832421630076, 16, 1]
Checking history sample input_X_between_0_1:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 0.34375, 0.0, 1.0, 0.0, 0.0, 1.0, 0.78125, 0.9203832421630076, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.139199137687683
Checking history sample input_X:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 31, 1, 0, 1, 1, 1, 60, 0.008025447056787238, 26, 0]
Checking history sample input_X_between_0_1:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 0.96875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.46875, 0.08025447056787237, 0.5416666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2780427932739258
Checking history sample input_X:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 7, 0, 0, 1, 0, 0, 10, 0.06890300584266877, 3, 1]
Checking history sample input_X_between_0_1:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.078125, 0.6890300584266876, 0.0625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.5037378072738647
Checking history sample input_X:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 1, 1, 1, 0, 1, 0, 28, 0.0879909943648135, 16, 0]
Checking history sample input_X_between_0_1:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 0.03125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.21875, 0.879909943648135, 0.3333333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1370042562484741
Checking history sample input_X:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 1, 1, 0, 0, 1, 0, 47, 0.04979780998318395, 37, 0]
Checking history sample input_X_between_0_1:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 0.03125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3671875, 0.4979780998318395, 0.7708333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1642110347747803
Checking history sample input_X:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 30, 1, 0, 1, 0, 0, 86, 0.08085387005284786, 6, 1]
Checking history sample input_X_between_0_1:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 0.9375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.671875, 0.8085387005284785, 0.125, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1513490676879883
Checking history sample input_X:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 31, 0, 1, 1, 1, 1, 84, 0.015776955723827136, 41, 1]
Checking history sample input_X_between_0_1:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 0.96875, 0.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.15776955723827135, 0.8541666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0676783323287964
Checking history sample input_X:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 24, 0, 1, 1, 0, 1, 1, 0.07052967196513893, 22, 0]
Checking history sample input_X_between_0_1:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0078125, 0.7052967196513893, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -0.999065637588501
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.2544 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9342038035392761, 0.7716194987297058, 0.5622552633285522, 0.5859347581863403, 0.22425806522369385, 0.15402144193649292, 0.19154107570648193, 0.00569838285446167, 0.4175538420677185, 0.27936410903930664, 0.41459572315216064, 0.09090030193328857, 0.9370310306549072, 0.4577820897102356, 0.012432396411895752, 0.8190106153488159, 0.9976365566253662, 0.7442904710769653, 0.36555248498916626]  ‚Üí  acq = -0.7634608780873455
X = [0.6096956729888916, 0.25469017028808594, 0.22867631912231445, 0.6045724749565125, 0.21544355154037476, 0.7620557546615601, 0.6122860312461853, 0.4729759097099304, 0.43483293056488037, 0.12775275111198425, 0.6221440434455872, 0.20262807607650757, 0.696590781211853, 0.17627471685409546, 0.3828175663948059, 0.7139682769775391, 0.5977556705474854, 0.7611218690872192, 0.6826452016830444]  ‚Üí  acq = -0.7600743084270839
X = [0.23661714792251587, 0.9719348549842834, 0.17977654933929443, 0.8495668172836304, 0.6785058379173279, 0.9251617789268494, 0.6921418309211731, 0.1855379343032837, 0.005543828010559082, 0.7273501753807068, 0.48137104511260986, 0.4403567314147949, 0.7962950468063354, 0.6856983304023743, 0.48504751920700073, 0.2607495188713074, 0.4003183841705322, 0.7366205453872681, 0.679166316986084]  ‚Üí  acq = -0.7600743592269934
X = [0.8403749465942383, 0.016608238220214844, 0.8758180141448975, 0.9852007031440735, 0.47721415758132935, 0.27970731258392334, 0.23790115118026733, 0.06004369258880615, 0.1743408441543579, 0.9727658629417419, 0.052172183990478516, 0.5271077156066895, 0.39680635929107666, 0.7000727653503418, 0.1639963984489441, 0.08217495679855347, 0.5093923807144165, 0.7217562198638916, 0.9267884492874146]  ‚Üí  acq = -0.7601295662795127
X = [0.6784719228744507, 0.22830796241760254, 0.5708042979240417, 0.5389095544815063, 0.9518447518348694, 0.0794522762298584, 0.5620474219322205, 0.8176521062850952, 0.25459158420562744, 0.3829672634601593, 0.030730366706848145, 0.2456105351448059, 0.12168461084365845, 0.7115315794944763, 0.8274378180503845, 0.5764290690422058, 0.7988794445991516, 0.8493024110794067, 0.6088458895683289]  ‚Üí  acq = -0.7680362299686814
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.3909, dtype=torch.float64), 0, 0, tensor(0.1470, dtype=torch.float64), tensor(0.0591, dtype=torch.float64), tensor(0.1501, dtype=torch.float64), 0, 0, tensor(0.2529, dtype=torch.float64), 28, 0, 0, 0, 1, 1, 2, 0.1, 40.96875764355262, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.3909, dtype=torch.float64), tensor(1.4723e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1470, dtype=torch.float64), tensor(0.0591, dtype=torch.float64), tensor(0.1501, dtype=torch.float64), tensor(1.5351e-17, dtype=torch.float64), tensor(6.7624e-18, dtype=torch.float64), tensor(0.2529, dtype=torch.float64), tensor(0.8741, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8535, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.391
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.147
  triviaqa: 0.059
  truthfulqa_gen: 0.15
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.253

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (40.96875764355262,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  40.96875764355262
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,064,384 || all params: 8,032,325,632 || trainable%: 0.0257
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260101_054053-8xamcowb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/8xamcowb
{'loss': 3.0607, 'grad_norm': 4.842031478881836, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2057397365570068, 'eval_runtime': 7.158, 'eval_samples_per_second': 139.703, 'eval_steps_per_second': 8.801, 'epoch': 0.04}
{'loss': 1.12, 'grad_norm': 3.836935043334961, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.001760482788086, 'eval_runtime': 7.156, 'eval_samples_per_second': 139.743, 'eval_steps_per_second': 8.804, 'epoch': 0.08}
{'loss': 0.9347, 'grad_norm': 2.0229172706604004, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9176381826400757, 'eval_runtime': 7.1803, 'eval_samples_per_second': 139.271, 'eval_steps_per_second': 8.774, 'epoch': 0.12}
{'loss': 0.8794, 'grad_norm': 1.9560096263885498, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9078055620193481, 'eval_runtime': 7.2101, 'eval_samples_per_second': 138.693, 'eval_steps_per_second': 8.738, 'epoch': 0.16}
{'loss': 0.8338, 'grad_norm': 1.7628892660140991, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.895514965057373, 'eval_runtime': 7.1846, 'eval_samples_per_second': 139.186, 'eval_steps_per_second': 8.769, 'epoch': 0.2}
{'loss': 0.8651, 'grad_norm': 1.6613030433654785, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8917459845542908, 'eval_runtime': 7.1861, 'eval_samples_per_second': 139.157, 'eval_steps_per_second': 8.767, 'epoch': 0.24}
{'loss': 0.8482, 'grad_norm': 1.211417555809021, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8951755166053772, 'eval_runtime': 7.2117, 'eval_samples_per_second': 138.663, 'eval_steps_per_second': 8.736, 'epoch': 0.28}
{'loss': 0.8283, 'grad_norm': 1.4206699132919312, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9038439393043518, 'eval_runtime': 7.2157, 'eval_samples_per_second': 138.587, 'eval_steps_per_second': 8.731, 'epoch': 0.32}
{'loss': 0.8096, 'grad_norm': 1.5032916069030762, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9013237357139587, 'eval_runtime': 7.1998, 'eval_samples_per_second': 138.894, 'eval_steps_per_second': 8.75, 'epoch': 0.36}
{'loss': 0.783, 'grad_norm': 1.4850316047668457, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9121026396751404, 'eval_runtime': 7.1628, 'eval_samples_per_second': 139.61, 'eval_steps_per_second': 8.795, 'epoch': 0.4}
{'loss': 0.8008, 'grad_norm': 1.6267471313476562, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9037986993789673, 'eval_runtime': 7.1615, 'eval_samples_per_second': 139.636, 'eval_steps_per_second': 8.797, 'epoch': 0.44}
{'loss': 0.789, 'grad_norm': 1.5716536045074463, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9058725833892822, 'eval_runtime': 7.1483, 'eval_samples_per_second': 139.893, 'eval_steps_per_second': 8.813, 'epoch': 0.48}
{'loss': 0.7724, 'grad_norm': 1.6974520683288574, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9209529757499695, 'eval_runtime': 7.1462, 'eval_samples_per_second': 139.935, 'eval_steps_per_second': 8.816, 'epoch': 0.52}
{'loss': 0.7567, 'grad_norm': 1.5192087888717651, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9160587191581726, 'eval_runtime': 7.1492, 'eval_samples_per_second': 139.875, 'eval_steps_per_second': 8.812, 'epoch': 0.56}
{'loss': 0.7723, 'grad_norm': 1.6049458980560303, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9217063188552856, 'eval_runtime': 7.1431, 'eval_samples_per_second': 139.994, 'eval_steps_per_second': 8.82, 'epoch': 0.6}
{'loss': 0.7333, 'grad_norm': 1.8343333005905151, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9296151995658875, 'eval_runtime': 7.154, 'eval_samples_per_second': 139.782, 'eval_steps_per_second': 8.806, 'epoch': 0.64}
{'loss': 0.7251, 'grad_norm': 1.5054810047149658, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9249467849731445, 'eval_runtime': 7.1609, 'eval_samples_per_second': 139.648, 'eval_steps_per_second': 8.798, 'epoch': 0.68}
{'loss': 0.733, 'grad_norm': 1.9418333768844604, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9435790181159973, 'eval_runtime': 7.1884, 'eval_samples_per_second': 139.112, 'eval_steps_per_second': 8.764, 'epoch': 0.72}
{'loss': 0.7252, 'grad_norm': 1.7042680978775024, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9483857154846191, 'eval_runtime': 7.1856, 'eval_samples_per_second': 139.168, 'eval_steps_per_second': 8.768, 'epoch': 0.76}
{'loss': 0.6988, 'grad_norm': 1.6809495687484741, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9455758929252625, 'eval_runtime': 7.1748, 'eval_samples_per_second': 139.377, 'eval_steps_per_second': 8.781, 'epoch': 0.8}
{'loss': 0.729, 'grad_norm': 1.926170825958252, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9539124965667725, 'eval_runtime': 7.1531, 'eval_samples_per_second': 139.8, 'eval_steps_per_second': 8.807, 'epoch': 0.84}
{'loss': 0.6985, 'grad_norm': 1.8400691747665405, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9533359408378601, 'eval_runtime': 7.1338, 'eval_samples_per_second': 140.178, 'eval_steps_per_second': 8.831, 'epoch': 0.88}
{'loss': 0.7168, 'grad_norm': 2.401294231414795, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9608876705169678, 'eval_runtime': 7.1287, 'eval_samples_per_second': 140.279, 'eval_steps_per_second': 8.838, 'epoch': 0.92}
{'loss': 0.6869, 'grad_norm': 2.0981287956237793, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9678502678871155, 'eval_runtime': 7.1114, 'eval_samples_per_second': 140.62, 'eval_steps_per_second': 8.859, 'epoch': 0.96}
{'loss': 0.7176, 'grad_norm': 2.178701877593994, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9691433906555176, 'eval_runtime': 7.1166, 'eval_samples_per_second': 140.516, 'eval_steps_per_second': 8.852, 'epoch': 1.0}
{'train_runtime': 332.7378, 'train_samples_per_second': 30.045, 'train_steps_per_second': 1.878, 'train_loss': 0.8807239593505859, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2057397365570068, 1.001760482788086, 0.9176381826400757, 0.9078055620193481, 0.895514965057373, 0.8917459845542908, 0.8951755166053772, 0.9038439393043518, 0.9013237357139587, 0.9121026396751404, 0.9037986993789673, 0.9058725833892822, 0.9209529757499695, 0.9160587191581726, 0.9217063188552856, 0.9296151995658875, 0.9249467849731445, 0.9435790181159973, 0.9483857154846191, 0.9455758929252625, 0.9539124965667725, 0.9533359408378601, 0.9608876705169678, 0.9678502678871155, 0.9691433906555176], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2057397365570068, 1.001760482788086, 0.9176381826400757, 0.9078055620193481, 0.895514965057373, 0.8917459845542908, 0.8951755166053772, 0.9038439393043518, 0.9013237357139587, 0.9121026396751404, 0.9037986993789673, 0.9058725833892822, 0.9209529757499695, 0.9160587191581726, 0.9217063188552856, 0.9296151995658875, 0.9249467849731445, 0.9435790181159973, 0.9483857154846191, 0.9455758929252625, 0.9539124965667725, 0.9533359408378601, 0.9608876705169678, 0.9678502678871155, 0.9691433906555176]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760282278060913
current iteration best possible eval_loss (full train run):  -0.9691433906555176
max eval_loss so far:  -0.9691433906555176
BO observations:  [-1.0760282278060913]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.5039 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.8565935890336174
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.8568934615811762
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.8566572645156132
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.856664395330631
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.8555547900725838
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4244, dtype=torch.float64), 0, 0, tensor(0.2994, dtype=torch.float64), tensor(0.0319, dtype=torch.float64), 0, 0, tensor(0.2364, dtype=torch.float64), 25, 0, 1, 0, 1, 0, 2, 0.1, 42.8162850213463, 0]
normalized proposed parameters for next round by BO: [tensor(4.0078e-18, dtype=torch.float64), tensor(0.4244, dtype=torch.float64), tensor(0.0079, dtype=torch.float64), tensor(2.3947e-18, dtype=torch.float64), tensor(0.2994, dtype=torch.float64), tensor(0.0319, dtype=torch.float64), tensor(5.2591e-19, dtype=torch.float64), tensor(2.4277e-18, dtype=torch.float64), tensor(0.2364, dtype=torch.float64), tensor(0.7824, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8920, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.424
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.299
  truthfulqa_gen: 0.032
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.236

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (42.8162850213463,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  42.8162850213463
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,177,600 || all params: 8,031,438,848 || trainable%: 0.0147
length of training data:  9919
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4452, 'grad_norm': 7.537330627441406, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6072700023651123, 'eval_runtime': 6.7744, 'eval_samples_per_second': 147.614, 'eval_steps_per_second': 9.3, 'epoch': 0.04}
{'loss': 1.0596, 'grad_norm': 1.6474779844284058, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9479520320892334, 'eval_runtime': 6.8231, 'eval_samples_per_second': 146.561, 'eval_steps_per_second': 9.233, 'epoch': 0.08}
{'loss': 0.9137, 'grad_norm': 1.2506684064865112, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 0.9103530049324036, 'eval_runtime': 6.8808, 'eval_samples_per_second': 145.333, 'eval_steps_per_second': 9.156, 'epoch': 0.12}
{'loss': 0.911, 'grad_norm': 1.1792351007461548, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.9057601094245911, 'eval_runtime': 6.8605, 'eval_samples_per_second': 145.761, 'eval_steps_per_second': 9.183, 'epoch': 0.16}
{'loss': 0.9082, 'grad_norm': 1.0181596279144287, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.9062744975090027, 'eval_runtime': 6.866, 'eval_samples_per_second': 145.645, 'eval_steps_per_second': 9.176, 'epoch': 0.2}
{'loss': 0.8995, 'grad_norm': 1.2475943565368652, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.8947024345397949, 'eval_runtime': 6.8732, 'eval_samples_per_second': 145.492, 'eval_steps_per_second': 9.166, 'epoch': 0.24}
{'loss': 0.9038, 'grad_norm': 1.2529330253601074, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.8941255807876587, 'eval_runtime': 6.8615, 'eval_samples_per_second': 145.74, 'eval_steps_per_second': 9.182, 'epoch': 0.28}
{'loss': 0.8649, 'grad_norm': 1.1999530792236328, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.8967466950416565, 'eval_runtime': 6.8731, 'eval_samples_per_second': 145.494, 'eval_steps_per_second': 9.166, 'epoch': 0.32}
{'loss': 0.8676, 'grad_norm': 1.3125901222229004, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.8902542591094971, 'eval_runtime': 6.8742, 'eval_samples_per_second': 145.471, 'eval_steps_per_second': 9.165, 'epoch': 0.36}
{'loss': 0.8344, 'grad_norm': 1.0569713115692139, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.8906038403511047, 'eval_runtime': 6.8653, 'eval_samples_per_second': 145.659, 'eval_steps_per_second': 9.177, 'epoch': 0.4}
{'loss': 0.8816, 'grad_norm': 1.2967966794967651, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.8940390348434448, 'eval_runtime': 6.8633, 'eval_samples_per_second': 145.703, 'eval_steps_per_second': 9.179, 'epoch': 0.44}
{'loss': 0.8501, 'grad_norm': 1.2604941129684448, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.8958190679550171, 'eval_runtime': 6.8494, 'eval_samples_per_second': 145.998, 'eval_steps_per_second': 9.198, 'epoch': 0.48}
{'loss': 0.8402, 'grad_norm': 1.3287056684494019, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.8910598158836365, 'eval_runtime': 6.8254, 'eval_samples_per_second': 146.511, 'eval_steps_per_second': 9.23, 'epoch': 0.52}
{'loss': 0.8247, 'grad_norm': 1.1120458841323853, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.8918123245239258, 'eval_runtime': 6.8442, 'eval_samples_per_second': 146.109, 'eval_steps_per_second': 9.205, 'epoch': 0.56}
{'loss': 0.8573, 'grad_norm': 1.1199018955230713, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.8939113616943359, 'eval_runtime': 6.8298, 'eval_samples_per_second': 146.418, 'eval_steps_per_second': 9.224, 'epoch': 0.6}
{'loss': 0.8438, 'grad_norm': 1.0623726844787598, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.8950274586677551, 'eval_runtime': 6.8397, 'eval_samples_per_second': 146.205, 'eval_steps_per_second': 9.211, 'epoch': 0.65}
{'loss': 0.8326, 'grad_norm': 1.118841528892517, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.8956486582756042, 'eval_runtime': 6.8324, 'eval_samples_per_second': 146.362, 'eval_steps_per_second': 9.221, 'epoch': 0.69}
{'loss': 0.8274, 'grad_norm': 1.3257341384887695, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.8918110728263855, 'eval_runtime': 6.8304, 'eval_samples_per_second': 146.403, 'eval_steps_per_second': 9.223, 'epoch': 0.73}
{'loss': 0.8398, 'grad_norm': 1.2813724279403687, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.8936403393745422, 'eval_runtime': 6.842, 'eval_samples_per_second': 146.156, 'eval_steps_per_second': 9.208, 'epoch': 0.77}
{'loss': 0.7901, 'grad_norm': 1.2581560611724854, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.8953571915626526, 'eval_runtime': 6.8381, 'eval_samples_per_second': 146.239, 'eval_steps_per_second': 9.213, 'epoch': 0.81}
{'loss': 0.8213, 'grad_norm': 1.3098502159118652, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.8972166776657104, 'eval_runtime': 6.833, 'eval_samples_per_second': 146.349, 'eval_steps_per_second': 9.22, 'epoch': 0.85}
{'loss': 0.8091, 'grad_norm': 1.2292965650558472, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.8955821394920349, 'eval_runtime': 6.8479, 'eval_samples_per_second': 146.031, 'eval_steps_per_second': 9.2, 'epoch': 0.89}
{'loss': 0.8218, 'grad_norm': 1.1526658535003662, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.8945533633232117, 'eval_runtime': 6.927, 'eval_samples_per_second': 144.362, 'eval_steps_per_second': 9.095, 'epoch': 0.93}
{'loss': 0.8323, 'grad_norm': 1.039150357246399, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.8967759013175964, 'eval_runtime': 6.919, 'eval_samples_per_second': 144.529, 'eval_steps_per_second': 9.105, 'epoch': 0.97}
{'train_runtime': 354.7376, 'train_samples_per_second': 27.962, 'train_steps_per_second': 1.748, 'train_loss': 0.9248526327071651, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6072700023651123, 0.9479520320892334, 0.9103530049324036, 0.9057601094245911, 0.9062744975090027, 0.8947024345397949, 0.8941255807876587, 0.8967466950416565, 0.8902542591094971, 0.8906038403511047, 0.8940390348434448, 0.8958190679550171, 0.8910598158836365, 0.8918123245239258, 0.8939113616943359, 0.8950274586677551, 0.8956486582756042, 0.8918110728263855, 0.8936403393745422, 0.8953571915626526, 0.8972166776657104, 0.8955821394920349, 0.8945533633232117, 0.8967759013175964], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.6072700023651123, 0.9479520320892334, 0.9103530049324036, 0.9057601094245911, 0.9062744975090027, 0.8947024345397949, 0.8941255807876587, 0.8967466950416565, 0.8902542591094971, 0.8906038403511047, 0.8940390348434448, 0.8958190679550171, 0.8910598158836365, 0.8918123245239258, 0.8939113616943359, 0.8950274586677551, 0.8956486582756042, 0.8918110728263855, 0.8936403393745422, 0.8953571915626526, 0.8972166776657104, 0.8955821394920349, 0.8945533633232117, 0.8967759013175964]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.076029658317566
current iteration best possible eval_loss (full train run):  -0.8967759013175964
max eval_loss so far:  -0.8967759013175964
BO observations:  [-1.0760282278060913, -1.076029658317566]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.6123 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5931183099746704, 0.48910677433013916, 0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.43514037132263184, 0.4408547878265381, 0.28891366720199585, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7229896187782288, 0.5257965922355652, 0.7089747190475464, 0.6451549530029297]  ‚Üí  acq = -0.8969615000909934
X = [0.8045198321342468, 0.5732041597366333, 0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5429293513298035, 0.6168438792228699, 0.6032367944717407, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.042392924427986145, 0.2894328832626343, 0.9503340721130371, 0.8085587620735168]  ‚Üí  acq = -0.9021269408375804
X = [0.18200689554214478, 0.36882972717285156, 0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.7035383582115173, 0.5326343774795532, 0.1735246777534485, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.2245919555425644, 0.050669074058532715, 0.8647359609603882, 0.040459275245666504]  ‚Üí  acq = -0.9021312620113281
X = [0.6313091516494751, 0.9872360825538635, 0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.09140855073928833, 0.0616917610168457, 0.16755545139312744, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.26512572169303894, 0.2105121612548828, 0.8316733837127686, 0.5374197959899902]  ‚Üí  acq = -0.902126940868489
X = [0.04147899150848389, 0.4950082302093506, 0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6635695695877075, 0.1939259171485901, 0.9576328992843628, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7788231372833252, 0.13258826732635498, 0.11039420962333679, 0.1842997670173645]  ‚Üí  acq = -0.9021269408669071
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3555, dtype=torch.float64), 0, 0, 0, tensor(0.6006, dtype=torch.float64), tensor(0.0366, dtype=torch.float64), 0, 0, 0, 30, 1, 0, 0, 0, 1, 2, 0.1, 40.80605172568954, 0]
normalized proposed parameters for next round by BO: [tensor(0.3555, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0074, dtype=torch.float64), tensor(0.6006, dtype=torch.float64), tensor(0.0366, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9368, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8501, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.355
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.601
  truthfulqa_gen: 0.037
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (40.80605172568954,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  40.80605172568954
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,597,440 || all params: 8,031,858,688 || trainable%: 0.0199
length of training data:  9924
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6521, 'grad_norm': 4.745738506317139, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9772528409957886, 'eval_runtime': 6.8405, 'eval_samples_per_second': 146.188, 'eval_steps_per_second': 9.21, 'epoch': 0.04}
{'loss': 1.4166, 'grad_norm': 1.8643747568130493, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6347240209579468, 'eval_runtime': 6.8674, 'eval_samples_per_second': 145.615, 'eval_steps_per_second': 9.174, 'epoch': 0.08}
{'loss': 1.0771, 'grad_norm': 1.5388247966766357, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.5525906085968018, 'eval_runtime': 6.8606, 'eval_samples_per_second': 145.76, 'eval_steps_per_second': 9.183, 'epoch': 0.12}
{'loss': 0.9713, 'grad_norm': 1.3437762260437012, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.571139931678772, 'eval_runtime': 6.8465, 'eval_samples_per_second': 146.06, 'eval_steps_per_second': 9.202, 'epoch': 0.16}
{'loss': 0.9352, 'grad_norm': 1.2616947889328003, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.600658655166626, 'eval_runtime': 6.8757, 'eval_samples_per_second': 145.439, 'eval_steps_per_second': 9.163, 'epoch': 0.2}
{'loss': 0.9354, 'grad_norm': 1.0737394094467163, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.5342048406600952, 'eval_runtime': 6.8686, 'eval_samples_per_second': 145.591, 'eval_steps_per_second': 9.172, 'epoch': 0.24}
{'loss': 0.9261, 'grad_norm': 1.1643317937850952, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.6087409257888794, 'eval_runtime': 6.895, 'eval_samples_per_second': 145.033, 'eval_steps_per_second': 9.137, 'epoch': 0.28}
{'loss': 0.936, 'grad_norm': 0.9722961187362671, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.55385422706604, 'eval_runtime': 6.8885, 'eval_samples_per_second': 145.169, 'eval_steps_per_second': 9.146, 'epoch': 0.32}
{'loss': 0.9103, 'grad_norm': 1.2128424644470215, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.575368881225586, 'eval_runtime': 6.9045, 'eval_samples_per_second': 144.832, 'eval_steps_per_second': 9.124, 'epoch': 0.36}
{'loss': 0.919, 'grad_norm': 1.2063723802566528, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.631636381149292, 'eval_runtime': 6.906, 'eval_samples_per_second': 144.802, 'eval_steps_per_second': 9.123, 'epoch': 0.4}
{'loss': 0.9207, 'grad_norm': 1.1398564577102661, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.6566404104232788, 'eval_runtime': 6.9011, 'eval_samples_per_second': 144.904, 'eval_steps_per_second': 9.129, 'epoch': 0.44}
{'loss': 0.8972, 'grad_norm': 1.648315191268921, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.618808388710022, 'eval_runtime': 6.9097, 'eval_samples_per_second': 144.724, 'eval_steps_per_second': 9.118, 'epoch': 0.48}
{'loss': 0.9034, 'grad_norm': 1.0547266006469727, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.608712911605835, 'eval_runtime': 6.9057, 'eval_samples_per_second': 144.808, 'eval_steps_per_second': 9.123, 'epoch': 0.52}
{'loss': 0.8859, 'grad_norm': 1.0274324417114258, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.597983717918396, 'eval_runtime': 6.9063, 'eval_samples_per_second': 144.795, 'eval_steps_per_second': 9.122, 'epoch': 0.56}
{'loss': 0.9053, 'grad_norm': 1.104546070098877, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.5690373182296753, 'eval_runtime': 6.9015, 'eval_samples_per_second': 144.896, 'eval_steps_per_second': 9.128, 'epoch': 0.6}
{'loss': 0.9167, 'grad_norm': 1.1351189613342285, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.5933208465576172, 'eval_runtime': 6.9059, 'eval_samples_per_second': 144.804, 'eval_steps_per_second': 9.123, 'epoch': 0.64}
{'loss': 0.8866, 'grad_norm': 1.1777405738830566, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.6399322748184204, 'eval_runtime': 6.8998, 'eval_samples_per_second': 144.931, 'eval_steps_per_second': 9.131, 'epoch': 0.68}
{'loss': 0.9068, 'grad_norm': 1.2014490365982056, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.6443605422973633, 'eval_runtime': 6.9078, 'eval_samples_per_second': 144.764, 'eval_steps_per_second': 9.12, 'epoch': 0.72}
{'loss': 0.8671, 'grad_norm': 1.1259348392486572, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.6516655683517456, 'eval_runtime': 6.9123, 'eval_samples_per_second': 144.669, 'eval_steps_per_second': 9.114, 'epoch': 0.76}
{'loss': 0.8681, 'grad_norm': 1.1327072381973267, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.621207594871521, 'eval_runtime': 6.8964, 'eval_samples_per_second': 145.004, 'eval_steps_per_second': 9.135, 'epoch': 0.81}
{'loss': 0.8812, 'grad_norm': 1.2627015113830566, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.6399257183074951, 'eval_runtime': 6.9034, 'eval_samples_per_second': 144.857, 'eval_steps_per_second': 9.126, 'epoch': 0.85}
{'loss': 0.8861, 'grad_norm': 1.1486693620681763, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.6438884735107422, 'eval_runtime': 6.9004, 'eval_samples_per_second': 144.919, 'eval_steps_per_second': 9.13, 'epoch': 0.89}
{'loss': 0.8812, 'grad_norm': 1.046955943107605, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.649823784828186, 'eval_runtime': 6.8972, 'eval_samples_per_second': 144.986, 'eval_steps_per_second': 9.134, 'epoch': 0.93}
{'loss': 0.8929, 'grad_norm': 1.1465644836425781, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.6421676874160767, 'eval_runtime': 6.9089, 'eval_samples_per_second': 144.741, 'eval_steps_per_second': 9.119, 'epoch': 0.97}
{'train_runtime': 270.8637, 'train_samples_per_second': 36.638, 'train_steps_per_second': 2.293, 'train_loss': 1.0430395430412844, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9772528409957886, 1.6347240209579468, 1.5525906085968018, 1.571139931678772, 1.600658655166626, 1.5342048406600952, 1.6087409257888794, 1.55385422706604, 1.575368881225586, 1.631636381149292, 1.6566404104232788, 1.618808388710022, 1.608712911605835, 1.597983717918396, 1.5690373182296753, 1.5933208465576172, 1.6399322748184204, 1.6443605422973633, 1.6516655683517456, 1.621207594871521, 1.6399257183074951, 1.6438884735107422, 1.649823784828186, 1.6421676874160767], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9772528409957886, 1.6347240209579468, 1.5525906085968018, 1.571139931678772, 1.600658655166626, 1.5342048406600952, 1.6087409257888794, 1.55385422706604, 1.575368881225586, 1.631636381149292, 1.6566404104232788, 1.618808388710022, 1.608712911605835, 1.597983717918396, 1.5690373182296753, 1.5933208465576172, 1.6399322748184204, 1.6443605422973633, 1.6516655683517456, 1.621207594871521, 1.6399257183074951, 1.6438884735107422, 1.649823784828186, 1.6421676874160767]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760287046432495
current iteration best possible eval_loss (full train run):  -1.6421676874160767
max eval_loss so far:  -0.8967759013175964
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7274 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.04070591926574707, 0.2670907974243164, 0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.11440835893154144, 0.8343492150306702, 0.6285832524299622, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5520853400230408, 0.8785960674285889, 0.7897642850875854, 0.10385686159133911]  ‚Üí  acq = -0.9323800584182964
X = [0.1669445037841797, 0.07206535339355469, 0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.29802340269088745, 0.02810537815093994, 0.37732040882110596, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4432501196861267, 0.8628382086753845, 0.2149072289466858, 0.16291123628616333]  ‚Üí  acq = -0.9323548960471122
X = [0.5642975568771362, 0.34905433654785156, 0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.45446842908859253, 0.9301089644432068, 0.8571122884750366, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9834365248680115, 0.5003925561904907, 0.9843506813049316, 0.11054939031600952]  ‚Üí  acq = -0.9323538001211826
X = [0.16956406831741333, 0.7232905030250549, 0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.11824709177017212, 0.728330671787262, 0.602367103099823, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4289284646511078, 0.0390055775642395, 0.09640960395336151, 0.9996876120567322]  ‚Üí  acq = -0.9323538001237437
X = [0.7488081455230713, 0.8432244062423706, 0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.5128886699676514, 0.7522366642951965, 0.6226845979690552, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.46897920966148376, 0.9671209454536438, 0.12299968302249908, 0.006412029266357422]  ‚Üí  acq = -0.9323539237336904
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.5418, dtype=torch.float64), 0, tensor(0.0598, dtype=torch.float64), 0, tensor(0.2244, dtype=torch.float64), tensor(0.1623, dtype=torch.float64), 25, 0, 0, 0, 1, 1, 2, 4.33680868994202e-19, 36.95713489865037, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0058, dtype=torch.float64), tensor(0.5418, dtype=torch.float64), tensor(0.0059, dtype=torch.float64), tensor(0.0598, dtype=torch.float64), tensor(3.5885e-18, dtype=torch.float64), tensor(0.2244, dtype=torch.float64), tensor(0.1623, dtype=torch.float64), tensor(0.7808, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(4.3368e-18, dtype=torch.float64), tensor(0.7699, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.542
  triviaqa: 0
  truthfulqa_gen: 0.06
  wikitext: 0
  mmlu: 0.224
  arc_challenge: 0.162

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (4.33680868994202e-19,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (36.95713489865037,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  4.33680868994202e-19
lora alpha:  36.95713489865037
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,843,200 || all params: 8,032,104,448 || trainable%: 0.0229
length of training data:  9880
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0328, 'grad_norm': 4.728011131286621, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3988782167434692, 'eval_runtime': 7.0202, 'eval_samples_per_second': 142.445, 'eval_steps_per_second': 8.974, 'epoch': 0.04}
{'loss': 1.2365, 'grad_norm': 3.1158535480499268, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0104080438613892, 'eval_runtime': 7.0147, 'eval_samples_per_second': 142.558, 'eval_steps_per_second': 8.981, 'epoch': 0.08}
{'loss': 1.0979, 'grad_norm': 2.8041305541992188, 'learning_rate': 0.0002873239436619718, 'epoch': 0.12}
{'eval_loss': 0.9536665081977844, 'eval_runtime': 7.0211, 'eval_samples_per_second': 142.427, 'eval_steps_per_second': 8.973, 'epoch': 0.12}
{'loss': 1.0251, 'grad_norm': 2.049790382385254, 'learning_rate': 0.0002741197183098591, 'epoch': 0.16}
{'eval_loss': 0.9174946546554565, 'eval_runtime': 7.0163, 'eval_samples_per_second': 142.525, 'eval_steps_per_second': 8.979, 'epoch': 0.16}
{'loss': 0.9934, 'grad_norm': 2.228877305984497, 'learning_rate': 0.0002609154929577464, 'epoch': 0.2}
{'eval_loss': 0.9198796153068542, 'eval_runtime': 7.0335, 'eval_samples_per_second': 142.176, 'eval_steps_per_second': 8.957, 'epoch': 0.2}
{'loss': 0.9724, 'grad_norm': 1.9184144735336304, 'learning_rate': 0.0002477112676056338, 'epoch': 0.24}
{'eval_loss': 0.9113120436668396, 'eval_runtime': 7.0469, 'eval_samples_per_second': 141.907, 'eval_steps_per_second': 8.94, 'epoch': 0.24}
{'loss': 1.0075, 'grad_norm': 1.8147659301757812, 'learning_rate': 0.00023450704225352109, 'epoch': 0.28}
{'eval_loss': 0.9093142747879028, 'eval_runtime': 7.0211, 'eval_samples_per_second': 142.428, 'eval_steps_per_second': 8.973, 'epoch': 0.28}
{'loss': 0.921, 'grad_norm': 1.664109468460083, 'learning_rate': 0.00022130281690140843, 'epoch': 0.32}
{'eval_loss': 0.9039561748504639, 'eval_runtime': 7.0045, 'eval_samples_per_second': 142.765, 'eval_steps_per_second': 8.994, 'epoch': 0.32}
{'loss': 0.9925, 'grad_norm': 1.6248270273208618, 'learning_rate': 0.00020809859154929575, 'epoch': 0.36}
{'eval_loss': 0.8981598019599915, 'eval_runtime': 7.0071, 'eval_samples_per_second': 142.712, 'eval_steps_per_second': 8.991, 'epoch': 0.36}
{'loss': 0.9119, 'grad_norm': 1.645485758781433, 'learning_rate': 0.00019489436619718307, 'epoch': 0.4}
{'eval_loss': 0.8946889638900757, 'eval_runtime': 7.0072, 'eval_samples_per_second': 142.711, 'eval_steps_per_second': 8.991, 'epoch': 0.4}
{'loss': 0.9072, 'grad_norm': 1.634547472000122, 'learning_rate': 0.0001816901408450704, 'epoch': 0.44}
{'eval_loss': 0.9029617309570312, 'eval_runtime': 7.0048, 'eval_samples_per_second': 142.76, 'eval_steps_per_second': 8.994, 'epoch': 0.44}
{'loss': 0.9297, 'grad_norm': 1.7182186841964722, 'learning_rate': 0.0001684859154929577, 'epoch': 0.49}
{'eval_loss': 0.898595929145813, 'eval_runtime': 7.0028, 'eval_samples_per_second': 142.8, 'eval_steps_per_second': 8.996, 'epoch': 0.49}
{'loss': 0.9436, 'grad_norm': 1.467000961303711, 'learning_rate': 0.00015528169014084506, 'epoch': 0.53}
{'eval_loss': 0.890389621257782, 'eval_runtime': 7.0072, 'eval_samples_per_second': 142.709, 'eval_steps_per_second': 8.991, 'epoch': 0.53}
{'loss': 0.9126, 'grad_norm': 1.627204418182373, 'learning_rate': 0.00014207746478873238, 'epoch': 0.57}
{'eval_loss': 0.8920367360115051, 'eval_runtime': 7.0021, 'eval_samples_per_second': 142.814, 'eval_steps_per_second': 8.997, 'epoch': 0.57}
{'loss': 0.8956, 'grad_norm': 1.8385075330734253, 'learning_rate': 0.0001288732394366197, 'epoch': 0.61}
{'eval_loss': 0.894477903842926, 'eval_runtime': 7.0214, 'eval_samples_per_second': 142.422, 'eval_steps_per_second': 8.973, 'epoch': 0.61}
{'loss': 0.9375, 'grad_norm': 1.4163763523101807, 'learning_rate': 0.00011566901408450703, 'epoch': 0.65}
{'eval_loss': 0.8918171525001526, 'eval_runtime': 7.0796, 'eval_samples_per_second': 141.251, 'eval_steps_per_second': 8.899, 'epoch': 0.65}
{'loss': 0.9232, 'grad_norm': 1.7957035303115845, 'learning_rate': 0.00010246478873239435, 'epoch': 0.69}
{'eval_loss': 0.8870978951454163, 'eval_runtime': 7.1023, 'eval_samples_per_second': 140.8, 'eval_steps_per_second': 8.87, 'epoch': 0.69}
{'loss': 0.9374, 'grad_norm': 2.0253069400787354, 'learning_rate': 8.926056338028169e-05, 'epoch': 0.73}
{'eval_loss': 0.8932977318763733, 'eval_runtime': 7.0906, 'eval_samples_per_second': 141.032, 'eval_steps_per_second': 8.885, 'epoch': 0.73}
{'loss': 0.9028, 'grad_norm': 1.687731146812439, 'learning_rate': 7.6056338028169e-05, 'epoch': 0.77}
{'eval_loss': 0.8894047737121582, 'eval_runtime': 7.088, 'eval_samples_per_second': 141.083, 'eval_steps_per_second': 8.888, 'epoch': 0.77}
{'loss': 0.8919, 'grad_norm': 1.562983751296997, 'learning_rate': 6.285211267605634e-05, 'epoch': 0.81}
{'eval_loss': 0.8931608200073242, 'eval_runtime': 7.0634, 'eval_samples_per_second': 141.574, 'eval_steps_per_second': 8.919, 'epoch': 0.81}
{'loss': 0.8735, 'grad_norm': 1.7430602312088013, 'learning_rate': 4.964788732394366e-05, 'epoch': 0.85}
{'eval_loss': 0.8965736031532288, 'eval_runtime': 7.0997, 'eval_samples_per_second': 140.85, 'eval_steps_per_second': 8.874, 'epoch': 0.85}
{'loss': 0.8536, 'grad_norm': 1.6779388189315796, 'learning_rate': 3.6443661971830985e-05, 'epoch': 0.89}
{'eval_loss': 0.8928030729293823, 'eval_runtime': 7.052, 'eval_samples_per_second': 141.803, 'eval_steps_per_second': 8.934, 'epoch': 0.89}
{'loss': 0.8511, 'grad_norm': 1.5437581539154053, 'learning_rate': 2.3239436619718305e-05, 'epoch': 0.93}
{'eval_loss': 0.8926308155059814, 'eval_runtime': 7.0591, 'eval_samples_per_second': 141.661, 'eval_steps_per_second': 8.925, 'epoch': 0.93}
{'loss': 0.8848, 'grad_norm': 1.3881474733352661, 'learning_rate': 1.0035211267605631e-05, 'epoch': 0.97}
{'eval_loss': 0.8955367207527161, 'eval_runtime': 7.0548, 'eval_samples_per_second': 141.747, 'eval_steps_per_second': 8.93, 'epoch': 0.97}
{'train_runtime': 345.9544, 'train_samples_per_second': 28.559, 'train_steps_per_second': 1.786, 'train_loss': 1.03139626401142, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3988782167434692, 1.0104080438613892, 0.9536665081977844, 0.9174946546554565, 0.9198796153068542, 0.9113120436668396, 0.9093142747879028, 0.9039561748504639, 0.8981598019599915, 0.8946889638900757, 0.9029617309570312, 0.898595929145813, 0.890389621257782, 0.8920367360115051, 0.894477903842926, 0.8918171525001526, 0.8870978951454163, 0.8932977318763733, 0.8894047737121582, 0.8931608200073242, 0.8965736031532288, 0.8928030729293823, 0.8926308155059814, 0.8955367207527161], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.3988782167434692, 1.0104080438613892, 0.9536665081977844, 0.9174946546554565, 0.9198796153068542, 0.9113120436668396, 0.9093142747879028, 0.9039561748504639, 0.8981598019599915, 0.8946889638900757, 0.9029617309570312, 0.898595929145813, 0.890389621257782, 0.8920367360115051, 0.894477903842926, 0.8918171525001526, 0.8870978951454163, 0.8932977318763733, 0.8894047737121582, 0.8931608200073242, 0.8965736031532288, 0.8928030729293823, 0.8926308155059814, 0.8955367207527161]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760300159454346
current iteration best possible eval_loss (full train run):  -0.8955367207527161
max eval_loss so far:  -0.8955367207527161
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3056 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5662546753883362, 0.8651972413063049, 0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6881975531578064, 0.07535994052886963, 0.38411790132522583, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8597249984741211, 0.2481454610824585, 0.4851251542568207, 0.9228593707084656]  ‚Üí  acq = -0.952681849593119
X = [0.36866605281829834, 0.9746328592300415, 0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.10368144512176514, 0.6828930377960205, 0.015926599502563477, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8190843462944031, 0.06521856784820557, 0.7280534505844116, 0.8793015480041504]  ‚Üí  acq = -0.9526818340758539
X = [0.43594950437545776, 0.018241524696350098, 0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.23293758928775787, 0.9870834350585938, 0.3120540976524353, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.0616411417722702, 0.7074243426322937, 0.4130009710788727, 0.3545602560043335]  ‚Üí  acq = -0.9523568289720286
X = [0.21759581565856934, 0.7204521298408508, 0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9501417279243469, 0.84186190366745, 0.104533851146698, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.23558637499809265, 0.2996382713317871, 0.04963042959570885, 0.6170431971549988]  ‚Üí  acq = -0.9526818340782918
X = [0.2948945164680481, 0.971221923828125, 0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9141191244125366, 0.7222160696983337, 0.033279359340667725, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.831488847732544, 0.14262902736663818, 0.3206331431865692, 0.8077713251113892]  ‚Üí  acq = -0.9526818340779738
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0815, dtype=torch.float64), 0, tensor(0.0658, dtype=torch.float64), 0, tensor(0.0159, dtype=torch.float64), tensor(0.0478, dtype=torch.float64), 0, tensor(0.6099, dtype=torch.float64), tensor(0.1790, dtype=torch.float64), 27, 0, 1, 0, 1, 0, 2, 0.1, 38.009221572600296, 0]
normalized proposed parameters for next round by BO: [tensor(0.0815, dtype=torch.float64), tensor(9.1020e-18, dtype=torch.float64), tensor(0.0658, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0159, dtype=torch.float64), tensor(0.0478, dtype=torch.float64), tensor(3.6655e-18, dtype=torch.float64), tensor(0.6099, dtype=torch.float64), tensor(0.1790, dtype=torch.float64), tensor(0.8329, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7919, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.082
  gsm8k: 0
  rowan_hellaswag: 0.066
  sciq: 0
  triviaqa: 0.016
  truthfulqa_gen: 0.048
  wikitext: 0
  mmlu: 0.61
  arc_challenge: 0.179

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (38.009221572600296,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  38.009221572600296
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,271,808 || all params: 8,031,533,056 || trainable%: 0.0158
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8833, 'grad_norm': 5.837625503540039, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5500954389572144, 'eval_runtime': 6.8551, 'eval_samples_per_second': 145.877, 'eval_steps_per_second': 9.19, 'epoch': 0.04}
{'loss': 1.454, 'grad_norm': 2.1365084648132324, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.974294900894165, 'eval_runtime': 6.8079, 'eval_samples_per_second': 146.888, 'eval_steps_per_second': 9.254, 'epoch': 0.08}
{'loss': 1.2867, 'grad_norm': 4.5241851806640625, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9723551869392395, 'eval_runtime': 6.8468, 'eval_samples_per_second': 146.053, 'eval_steps_per_second': 9.201, 'epoch': 0.12}
{'loss': 1.2557, 'grad_norm': 1.2635360956192017, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9233025908470154, 'eval_runtime': 6.8512, 'eval_samples_per_second': 145.961, 'eval_steps_per_second': 9.196, 'epoch': 0.16}
{'loss': 1.2095, 'grad_norm': 1.2009772062301636, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9066781997680664, 'eval_runtime': 6.8672, 'eval_samples_per_second': 145.619, 'eval_steps_per_second': 9.174, 'epoch': 0.2}
{'loss': 1.2524, 'grad_norm': 1.2247315645217896, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9100793600082397, 'eval_runtime': 6.8755, 'eval_samples_per_second': 145.443, 'eval_steps_per_second': 9.163, 'epoch': 0.24}
{'loss': 1.2639, 'grad_norm': 1.3164976835250854, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9045007228851318, 'eval_runtime': 6.8755, 'eval_samples_per_second': 145.444, 'eval_steps_per_second': 9.163, 'epoch': 0.28}
{'loss': 1.2339, 'grad_norm': 1.329840898513794, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9014334082603455, 'eval_runtime': 6.8744, 'eval_samples_per_second': 145.467, 'eval_steps_per_second': 9.164, 'epoch': 0.32}
{'loss': 1.2191, 'grad_norm': 1.3252105712890625, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.902588963508606, 'eval_runtime': 6.8822, 'eval_samples_per_second': 145.303, 'eval_steps_per_second': 9.154, 'epoch': 0.36}
{'loss': 1.2543, 'grad_norm': 1.6053272485733032, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8947314620018005, 'eval_runtime': 6.9134, 'eval_samples_per_second': 144.646, 'eval_steps_per_second': 9.113, 'epoch': 0.4}
{'loss': 1.2384, 'grad_norm': 1.3209582567214966, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8989250659942627, 'eval_runtime': 6.9324, 'eval_samples_per_second': 144.251, 'eval_steps_per_second': 9.088, 'epoch': 0.44}
{'loss': 1.2259, 'grad_norm': 1.2166787385940552, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8880306482315063, 'eval_runtime': 6.915, 'eval_samples_per_second': 144.613, 'eval_steps_per_second': 9.111, 'epoch': 0.48}
{'loss': 1.176, 'grad_norm': 1.320785641670227, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8916198015213013, 'eval_runtime': 6.9106, 'eval_samples_per_second': 144.706, 'eval_steps_per_second': 9.116, 'epoch': 0.52}
{'loss': 1.1471, 'grad_norm': 1.230807900428772, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8945534825325012, 'eval_runtime': 6.9193, 'eval_samples_per_second': 144.524, 'eval_steps_per_second': 9.105, 'epoch': 0.56}
{'loss': 1.1996, 'grad_norm': 1.2088699340820312, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.892816424369812, 'eval_runtime': 6.9199, 'eval_samples_per_second': 144.51, 'eval_steps_per_second': 9.104, 'epoch': 0.6}
{'loss': 1.1668, 'grad_norm': 1.265605092048645, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8953099250793457, 'eval_runtime': 6.9131, 'eval_samples_per_second': 144.652, 'eval_steps_per_second': 9.113, 'epoch': 0.64}
{'loss': 1.1693, 'grad_norm': 1.2140090465545654, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8917682766914368, 'eval_runtime': 6.9171, 'eval_samples_per_second': 144.57, 'eval_steps_per_second': 9.108, 'epoch': 0.68}
{'loss': 1.191, 'grad_norm': 1.3463040590286255, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.891127347946167, 'eval_runtime': 6.8912, 'eval_samples_per_second': 145.112, 'eval_steps_per_second': 9.142, 'epoch': 0.72}
{'loss': 1.1639, 'grad_norm': 1.3465641736984253, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8930336833000183, 'eval_runtime': 6.8692, 'eval_samples_per_second': 145.577, 'eval_steps_per_second': 9.171, 'epoch': 0.76}
{'loss': 1.2359, 'grad_norm': 1.2727502584457397, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8922330141067505, 'eval_runtime': 6.8726, 'eval_samples_per_second': 145.505, 'eval_steps_per_second': 9.167, 'epoch': 0.8}
{'loss': 1.1717, 'grad_norm': 1.2726184129714966, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8864071369171143, 'eval_runtime': 6.8721, 'eval_samples_per_second': 145.516, 'eval_steps_per_second': 9.168, 'epoch': 0.84}
{'loss': 1.1288, 'grad_norm': 1.148457646369934, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8867841362953186, 'eval_runtime': 6.8698, 'eval_samples_per_second': 145.564, 'eval_steps_per_second': 9.171, 'epoch': 0.88}
{'loss': 1.1706, 'grad_norm': 1.1896958351135254, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8858457207679749, 'eval_runtime': 6.8762, 'eval_samples_per_second': 145.43, 'eval_steps_per_second': 9.162, 'epoch': 0.92}
{'loss': 1.1543, 'grad_norm': 1.4118542671203613, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8850032687187195, 'eval_runtime': 6.8665, 'eval_samples_per_second': 145.634, 'eval_steps_per_second': 9.175, 'epoch': 0.96}
{'loss': 1.1938, 'grad_norm': 1.3589439392089844, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8860146403312683, 'eval_runtime': 6.8682, 'eval_samples_per_second': 145.599, 'eval_steps_per_second': 9.173, 'epoch': 1.0}
{'train_runtime': 372.4, 'train_samples_per_second': 26.847, 'train_steps_per_second': 1.678, 'train_loss': 1.2818381622314454, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5500954389572144, 0.974294900894165, 0.9723551869392395, 0.9233025908470154, 0.9066781997680664, 0.9100793600082397, 0.9045007228851318, 0.9014334082603455, 0.902588963508606, 0.8947314620018005, 0.8989250659942627, 0.8880306482315063, 0.8916198015213013, 0.8945534825325012, 0.892816424369812, 0.8953099250793457, 0.8917682766914368, 0.891127347946167, 0.8930336833000183, 0.8922330141067505, 0.8864071369171143, 0.8867841362953186, 0.8858457207679749, 0.8850032687187195, 0.8860146403312683], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5500954389572144, 0.974294900894165, 0.9723551869392395, 0.9233025908470154, 0.9066781997680664, 0.9100793600082397, 0.9045007228851318, 0.9014334082603455, 0.902588963508606, 0.8947314620018005, 0.8989250659942627, 0.8880306482315063, 0.8916198015213013, 0.8945534825325012, 0.892816424369812, 0.8953099250793457, 0.8917682766914368, 0.891127347946167, 0.8930336833000183, 0.8922330141067505, 0.8864071369171143, 0.8867841362953186, 0.8858457207679749, 0.8850032687187195, 0.8860146403312683]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760294198989868
current iteration best possible eval_loss (full train run):  -0.8860146403312683
max eval_loss so far:  -0.8860146403312683
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.4851 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.9474703337976615
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.9451268891714768
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.9451268891785436
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.9472128918985184
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.9451270091183789
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3538, dtype=torch.float64), tensor(0.0551, dtype=torch.float64), tensor(0.0310, dtype=torch.float64), tensor(0.1620, dtype=torch.float64), 0, tensor(0.1266, dtype=torch.float64), 0, tensor(0.2716, dtype=torch.float64), 26, 0, 1, 1, 0, 1, 112, 0.09999040184166648, 1.5106428402215637, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.3538, dtype=torch.float64), tensor(0.0551, dtype=torch.float64), tensor(0.0310, dtype=torch.float64), tensor(0.1620, dtype=torch.float64), tensor(1.3445e-18, dtype=torch.float64), tensor(0.1266, dtype=torch.float64), tensor(1.5354e-18, dtype=torch.float64), tensor(0.2716, dtype=torch.float64), tensor(0.8188, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8737, dtype=torch.float64), tensor(0.9999, dtype=torch.float64), tensor(0.0315, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.354
  rowan_hellaswag: 0.055
  sciq: 0.031
  triviaqa: 0.162
  truthfulqa_gen: 0
  wikitext: 0.127
  mmlu: 0
  arc_challenge: 0.272

LoRA Parameters:
  lora_r: (112,)
  lora_dropout: (0.09999040184166648,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (1.5106428402215637,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  112
lora dropout:  0.09999040184166648
lora alpha:  1.5106428402215637
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 122,257,408 || all params: 8,152,518,656 || trainable%: 1.4996
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3094, 'grad_norm': 0.10801320523023605, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.9793472290039062, 'eval_runtime': 7.0709, 'eval_samples_per_second': 141.425, 'eval_steps_per_second': 8.91, 'epoch': 0.04}
{'loss': 2.0312, 'grad_norm': 0.05548732727766037, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4604475498199463, 'eval_runtime': 7.0781, 'eval_samples_per_second': 141.281, 'eval_steps_per_second': 8.901, 'epoch': 0.08}
{'loss': 1.4547, 'grad_norm': 0.060067806392908096, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.155847430229187, 'eval_runtime': 7.0615, 'eval_samples_per_second': 141.613, 'eval_steps_per_second': 8.922, 'epoch': 0.12}
{'loss': 1.3259, 'grad_norm': 0.03856261819601059, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.098660945892334, 'eval_runtime': 7.0884, 'eval_samples_per_second': 141.075, 'eval_steps_per_second': 8.888, 'epoch': 0.16}
{'loss': 1.2368, 'grad_norm': 0.0421338714659214, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0745534896850586, 'eval_runtime': 7.0661, 'eval_samples_per_second': 141.52, 'eval_steps_per_second': 8.916, 'epoch': 0.2}
{'loss': 1.2221, 'grad_norm': 0.04717802256345749, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0583988428115845, 'eval_runtime': 7.0931, 'eval_samples_per_second': 140.983, 'eval_steps_per_second': 8.882, 'epoch': 0.24}
{'loss': 1.2369, 'grad_norm': 0.04546612501144409, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0493803024291992, 'eval_runtime': 7.0799, 'eval_samples_per_second': 141.244, 'eval_steps_per_second': 8.898, 'epoch': 0.28}
{'loss': 1.2221, 'grad_norm': 0.04248417541384697, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0323240756988525, 'eval_runtime': 7.073, 'eval_samples_per_second': 141.383, 'eval_steps_per_second': 8.907, 'epoch': 0.32}
{'loss': 1.2046, 'grad_norm': 0.04605461284518242, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0141557455062866, 'eval_runtime': 7.105, 'eval_samples_per_second': 140.745, 'eval_steps_per_second': 8.867, 'epoch': 0.36}
{'loss': 1.1566, 'grad_norm': 0.050304532051086426, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9975846409797668, 'eval_runtime': 7.0916, 'eval_samples_per_second': 141.012, 'eval_steps_per_second': 8.884, 'epoch': 0.4}
{'loss': 1.1031, 'grad_norm': 0.059614915400743484, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9429915547370911, 'eval_runtime': 7.1059, 'eval_samples_per_second': 140.727, 'eval_steps_per_second': 8.866, 'epoch': 0.44}
{'loss': 1.0865, 'grad_norm': 0.04708818718791008, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9324629902839661, 'eval_runtime': 7.1123, 'eval_samples_per_second': 140.601, 'eval_steps_per_second': 8.858, 'epoch': 0.48}
{'loss': 1.0878, 'grad_norm': 0.05276474356651306, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9200304746627808, 'eval_runtime': 7.1124, 'eval_samples_per_second': 140.6, 'eval_steps_per_second': 8.858, 'epoch': 0.52}
{'loss': 1.0966, 'grad_norm': 0.04911638796329498, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8983715772628784, 'eval_runtime': 7.0982, 'eval_samples_per_second': 140.881, 'eval_steps_per_second': 8.876, 'epoch': 0.56}
{'loss': 1.0169, 'grad_norm': 0.045650824904441833, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8862669467926025, 'eval_runtime': 7.1001, 'eval_samples_per_second': 140.844, 'eval_steps_per_second': 8.873, 'epoch': 0.6}
{'loss': 1.1078, 'grad_norm': 0.04487467557191849, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8845013976097107, 'eval_runtime': 7.1006, 'eval_samples_per_second': 140.834, 'eval_steps_per_second': 8.873, 'epoch': 0.64}
{'loss': 1.0583, 'grad_norm': 0.05252052843570709, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.882100522518158, 'eval_runtime': 7.1172, 'eval_samples_per_second': 140.505, 'eval_steps_per_second': 8.852, 'epoch': 0.68}
{'loss': 1.0624, 'grad_norm': 0.061702627688646317, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8800362348556519, 'eval_runtime': 7.1254, 'eval_samples_per_second': 140.342, 'eval_steps_per_second': 8.842, 'epoch': 0.72}
{'loss': 1.0082, 'grad_norm': 0.05545791611075401, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8821665048599243, 'eval_runtime': 7.1303, 'eval_samples_per_second': 140.247, 'eval_steps_per_second': 8.836, 'epoch': 0.76}
{'loss': 1.0972, 'grad_norm': 0.05117492377758026, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8839941620826721, 'eval_runtime': 7.1348, 'eval_samples_per_second': 140.158, 'eval_steps_per_second': 8.83, 'epoch': 0.8}
{'loss': 1.0645, 'grad_norm': 0.05114546790719032, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8765091300010681, 'eval_runtime': 7.0984, 'eval_samples_per_second': 140.876, 'eval_steps_per_second': 8.875, 'epoch': 0.84}
{'loss': 1.0646, 'grad_norm': 0.05447513610124588, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.879633903503418, 'eval_runtime': 7.0802, 'eval_samples_per_second': 141.239, 'eval_steps_per_second': 8.898, 'epoch': 0.88}
{'loss': 1.0457, 'grad_norm': 0.05045340210199356, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8784341216087341, 'eval_runtime': 7.0983, 'eval_samples_per_second': 140.88, 'eval_steps_per_second': 8.875, 'epoch': 0.92}
{'loss': 1.0556, 'grad_norm': 0.05781463906168938, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8761627078056335, 'eval_runtime': 7.0883, 'eval_samples_per_second': 141.078, 'eval_steps_per_second': 8.888, 'epoch': 0.96}
{'loss': 1.0536, 'grad_norm': 0.05537945032119751, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8768962621688843, 'eval_runtime': 7.1103, 'eval_samples_per_second': 140.641, 'eval_steps_per_second': 8.86, 'epoch': 1.0}
{'train_runtime': 378.7709, 'train_samples_per_second': 26.393, 'train_steps_per_second': 1.65, 'train_loss': 1.2563593994140625, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9793472290039062, 1.4604475498199463, 1.155847430229187, 1.098660945892334, 1.0745534896850586, 1.0583988428115845, 1.0493803024291992, 1.0323240756988525, 1.0141557455062866, 0.9975846409797668, 0.9429915547370911, 0.9324629902839661, 0.9200304746627808, 0.8983715772628784, 0.8862669467926025, 0.8845013976097107, 0.882100522518158, 0.8800362348556519, 0.8821665048599243, 0.8839941620826721, 0.8765091300010681, 0.879633903503418, 0.8784341216087341, 0.8761627078056335, 0.8768962621688843], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.9793472290039062, 1.4604475498199463, 1.155847430229187, 1.098660945892334, 1.0745534896850586, 1.0583988428115845, 1.0493803024291992, 1.0323240756988525, 1.0141557455062866, 0.9975846409797668, 0.9429915547370911, 0.9324629902839661, 0.9200304746627808, 0.8983715772628784, 0.8862669467926025, 0.8845013976097107, 0.882100522518158, 0.8800362348556519, 0.8821665048599243, 0.8839941620826721, 0.8765091300010681, 0.879633903503418, 0.8784341216087341, 0.8761627078056335, 0.8768962621688843]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.816397786140442
current iteration best possible eval_loss (full train run):  -0.8768962621688843
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.3010 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.8815867725590052
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.8815866703534403
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.8815854672891141
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.8822224250558697
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.8815867706624128
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0716, dtype=torch.float64), tensor(0.2132, dtype=torch.float64), 0, 0, tensor(0.1460, dtype=torch.float64), tensor(0.1123, dtype=torch.float64), tensor(0.1097, dtype=torch.float64), tensor(0.2684, dtype=torch.float64), tensor(0.0788, dtype=torch.float64), 32, 0, 0, 0, 1, 1, 2, 0.07796840064425789, 37.182492308655085, 1]
normalized proposed parameters for next round by BO: [tensor(0.0716, dtype=torch.float64), tensor(0.2132, dtype=torch.float64), tensor(1.2214e-16, dtype=torch.float64), tensor(1.5277e-17, dtype=torch.float64), tensor(0.1460, dtype=torch.float64), tensor(0.1123, dtype=torch.float64), tensor(0.1097, dtype=torch.float64), tensor(0.2684, dtype=torch.float64), tensor(0.0788, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.7797, dtype=torch.float64), tensor(0.7746, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.072
  gsm8k: 0.213
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.146
  truthfulqa_gen: 0.112
  wikitext: 0.11
  mmlu: 0.268
  arc_challenge: 0.079

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.07796840064425789,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (37.182492308655085,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.07796840064425789
lora alpha:  37.182492308655085
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,359,296 || all params: 8,032,620,544 || trainable%: 0.0294
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5341, 'grad_norm': 4.394901752471924, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3352241516113281, 'eval_runtime': 7.2397, 'eval_samples_per_second': 138.127, 'eval_steps_per_second': 8.702, 'epoch': 0.04}
{'loss': 1.298, 'grad_norm': 2.2975142002105713, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0053458213806152, 'eval_runtime': 7.2822, 'eval_samples_per_second': 137.322, 'eval_steps_per_second': 8.651, 'epoch': 0.08}
{'loss': 1.1746, 'grad_norm': 1.60027015209198, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9572060704231262, 'eval_runtime': 7.2595, 'eval_samples_per_second': 137.75, 'eval_steps_per_second': 8.678, 'epoch': 0.12}
{'loss': 1.1295, 'grad_norm': 1.349743127822876, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9377519488334656, 'eval_runtime': 7.2498, 'eval_samples_per_second': 137.936, 'eval_steps_per_second': 8.69, 'epoch': 0.16}
{'loss': 1.1067, 'grad_norm': 1.1037694215774536, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9289700388908386, 'eval_runtime': 7.2244, 'eval_samples_per_second': 138.42, 'eval_steps_per_second': 8.72, 'epoch': 0.2}
{'loss': 1.0844, 'grad_norm': 1.388656735420227, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9288455843925476, 'eval_runtime': 7.2271, 'eval_samples_per_second': 138.369, 'eval_steps_per_second': 8.717, 'epoch': 0.24}
{'loss': 1.0979, 'grad_norm': 1.1866466999053955, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9020135402679443, 'eval_runtime': 7.2199, 'eval_samples_per_second': 138.505, 'eval_steps_per_second': 8.726, 'epoch': 0.28}
{'loss': 1.032, 'grad_norm': 0.9865831732749939, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.910037636756897, 'eval_runtime': 7.2349, 'eval_samples_per_second': 138.219, 'eval_steps_per_second': 8.708, 'epoch': 0.32}
{'loss': 1.1696, 'grad_norm': 1.1586682796478271, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9022626876831055, 'eval_runtime': 7.2253, 'eval_samples_per_second': 138.403, 'eval_steps_per_second': 8.719, 'epoch': 0.36}
{'loss': 1.0658, 'grad_norm': 1.614670991897583, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8974241018295288, 'eval_runtime': 7.2365, 'eval_samples_per_second': 138.189, 'eval_steps_per_second': 8.706, 'epoch': 0.4}
{'loss': 1.0542, 'grad_norm': 1.3445285558700562, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9034013748168945, 'eval_runtime': 7.275, 'eval_samples_per_second': 137.458, 'eval_steps_per_second': 8.66, 'epoch': 0.44}
{'loss': 1.1108, 'grad_norm': 1.2336422204971313, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8969220519065857, 'eval_runtime': 7.2426, 'eval_samples_per_second': 138.073, 'eval_steps_per_second': 8.699, 'epoch': 0.48}
{'loss': 1.0007, 'grad_norm': 1.0854161977767944, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9059920907020569, 'eval_runtime': 7.2543, 'eval_samples_per_second': 137.849, 'eval_steps_per_second': 8.684, 'epoch': 0.52}
{'loss': 1.0023, 'grad_norm': 1.4704517126083374, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8956267237663269, 'eval_runtime': 7.2846, 'eval_samples_per_second': 137.275, 'eval_steps_per_second': 8.648, 'epoch': 0.56}
{'loss': 1.0674, 'grad_norm': 1.4106661081314087, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8997862935066223, 'eval_runtime': 7.2642, 'eval_samples_per_second': 137.661, 'eval_steps_per_second': 8.673, 'epoch': 0.6}
{'loss': 1.0644, 'grad_norm': 1.5300101041793823, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8886353969573975, 'eval_runtime': 7.2553, 'eval_samples_per_second': 137.829, 'eval_steps_per_second': 8.683, 'epoch': 0.64}
{'loss': 1.0579, 'grad_norm': 1.3682186603546143, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8961142301559448, 'eval_runtime': 7.2643, 'eval_samples_per_second': 137.659, 'eval_steps_per_second': 8.673, 'epoch': 0.68}
{'loss': 1.0806, 'grad_norm': 1.5084255933761597, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8930957317352295, 'eval_runtime': 7.2593, 'eval_samples_per_second': 137.755, 'eval_steps_per_second': 8.679, 'epoch': 0.72}
{'loss': 1.0294, 'grad_norm': 1.544310450553894, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8939101099967957, 'eval_runtime': 7.282, 'eval_samples_per_second': 137.324, 'eval_steps_per_second': 8.651, 'epoch': 0.76}
{'loss': 1.1385, 'grad_norm': 1.6437160968780518, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8905304074287415, 'eval_runtime': 7.2843, 'eval_samples_per_second': 137.282, 'eval_steps_per_second': 8.649, 'epoch': 0.8}
{'loss': 1.0282, 'grad_norm': 1.4241282939910889, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.889814019203186, 'eval_runtime': 7.2741, 'eval_samples_per_second': 137.474, 'eval_steps_per_second': 8.661, 'epoch': 0.84}
{'loss': 1.0916, 'grad_norm': 1.7687174081802368, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.886374831199646, 'eval_runtime': 7.2743, 'eval_samples_per_second': 137.469, 'eval_steps_per_second': 8.661, 'epoch': 0.88}
{'loss': 1.0452, 'grad_norm': 1.3914153575897217, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.886816143989563, 'eval_runtime': 7.2666, 'eval_samples_per_second': 137.616, 'eval_steps_per_second': 8.67, 'epoch': 0.92}
{'loss': 1.0097, 'grad_norm': 1.3465721607208252, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8850506544113159, 'eval_runtime': 7.2294, 'eval_samples_per_second': 138.325, 'eval_steps_per_second': 8.714, 'epoch': 0.96}
{'loss': 0.9984, 'grad_norm': 1.274185061454773, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8851062059402466, 'eval_runtime': 7.2406, 'eval_samples_per_second': 138.11, 'eval_steps_per_second': 8.701, 'epoch': 1.0}
{'train_runtime': 404.8478, 'train_samples_per_second': 24.693, 'train_steps_per_second': 1.544, 'train_loss': 1.1388669586181641, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3352241516113281, 1.0053458213806152, 0.9572060704231262, 0.9377519488334656, 0.9289700388908386, 0.9288455843925476, 0.9020135402679443, 0.910037636756897, 0.9022626876831055, 0.8974241018295288, 0.9034013748168945, 0.8969220519065857, 0.9059920907020569, 0.8956267237663269, 0.8997862935066223, 0.8886353969573975, 0.8961142301559448, 0.8930957317352295, 0.8939101099967957, 0.8905304074287415, 0.889814019203186, 0.886374831199646, 0.886816143989563, 0.8850506544113159, 0.8851062059402466], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3352241516113281, 1.0053458213806152, 0.9572060704231262, 0.9377519488334656, 0.9289700388908386, 0.9288455843925476, 0.9020135402679443, 0.910037636756897, 0.9022626876831055, 0.8974241018295288, 0.9034013748168945, 0.8969220519065857, 0.9059920907020569, 0.8956267237663269, 0.8997862935066223, 0.8886353969573975, 0.8961142301559448, 0.8930957317352295, 0.8939101099967957, 0.8905304074287415, 0.889814019203186, 0.886374831199646, 0.886816143989563, 0.8850506544113159, 0.8851062059402466]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760276317596436
current iteration best possible eval_loss (full train run):  -0.8851062059402466
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 17.8851 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.9006246575500574
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.9014274528006934
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.9014274528006934
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.9014274295036331
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.9020919475393698
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3993, dtype=torch.float64), 0, 0, 0, 0, tensor(0.0120, dtype=torch.float64), tensor(0.0567, dtype=torch.float64), tensor(0.5320, dtype=torch.float64), 0, 24, 0, 1, 0, 1, 0, 2, 0.06364669255584127, 25.04444140863432, 1]
normalized proposed parameters for next round by BO: [tensor(0.3993, dtype=torch.float64), tensor(1.0645e-17, dtype=torch.float64), tensor(3.0095e-17, dtype=torch.float64), tensor(6.1588e-18, dtype=torch.float64), tensor(2.4980e-17, dtype=torch.float64), tensor(0.0120, dtype=torch.float64), tensor(0.0567, dtype=torch.float64), tensor(0.5320, dtype=torch.float64), tensor(1.1480e-17, dtype=torch.float64), tensor(0.7606, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.6365, dtype=torch.float64), tensor(0.5218, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.399
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.012
  wikitext: 0.057
  mmlu: 0.532
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.06364669255584127,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (25.04444140863432,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.06364669255584127
lora alpha:  25.04444140863432
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,130,496 || all params: 8,031,391,744 || trainable%: 0.0141
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1454, 'grad_norm': 8.644393920898438, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7472347021102905, 'eval_runtime': 6.762, 'eval_samples_per_second': 147.885, 'eval_steps_per_second': 9.317, 'epoch': 0.04}
{'loss': 1.5406, 'grad_norm': 6.414443016052246, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2765514850616455, 'eval_runtime': 6.7967, 'eval_samples_per_second': 147.13, 'eval_steps_per_second': 9.269, 'epoch': 0.08}
{'loss': 1.2997, 'grad_norm': 1.6722252368927002, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2458009719848633, 'eval_runtime': 6.7713, 'eval_samples_per_second': 147.682, 'eval_steps_per_second': 9.304, 'epoch': 0.12}
{'loss': 1.2569, 'grad_norm': 2.430786371231079, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2727160453796387, 'eval_runtime': 6.7913, 'eval_samples_per_second': 147.248, 'eval_steps_per_second': 9.277, 'epoch': 0.16}
{'loss': 1.2061, 'grad_norm': 1.6738317012786865, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2607346773147583, 'eval_runtime': 6.7973, 'eval_samples_per_second': 147.118, 'eval_steps_per_second': 9.268, 'epoch': 0.2}
{'loss': 1.2236, 'grad_norm': 2.606806993484497, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2900171279907227, 'eval_runtime': 6.8073, 'eval_samples_per_second': 146.9, 'eval_steps_per_second': 9.255, 'epoch': 0.24}
{'loss': 1.1822, 'grad_norm': 1.3498228788375854, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1992756128311157, 'eval_runtime': 6.8156, 'eval_samples_per_second': 146.723, 'eval_steps_per_second': 9.244, 'epoch': 0.28}
{'loss': 1.1313, 'grad_norm': 1.4090917110443115, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.262217402458191, 'eval_runtime': 6.8154, 'eval_samples_per_second': 146.727, 'eval_steps_per_second': 9.244, 'epoch': 0.32}
{'loss': 1.1445, 'grad_norm': 1.7803778648376465, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3412773609161377, 'eval_runtime': 6.8784, 'eval_samples_per_second': 145.383, 'eval_steps_per_second': 9.159, 'epoch': 0.36}
{'loss': 1.1284, 'grad_norm': 5.093479156494141, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.300826907157898, 'eval_runtime': 6.8205, 'eval_samples_per_second': 146.616, 'eval_steps_per_second': 9.237, 'epoch': 0.4}
{'loss': 1.1694, 'grad_norm': 0.9434710741043091, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2955495119094849, 'eval_runtime': 6.8099, 'eval_samples_per_second': 146.845, 'eval_steps_per_second': 9.251, 'epoch': 0.44}
{'loss': 1.1173, 'grad_norm': 1.4752269983291626, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2783076763153076, 'eval_runtime': 6.7947, 'eval_samples_per_second': 147.174, 'eval_steps_per_second': 9.272, 'epoch': 0.48}
{'loss': 1.1701, 'grad_norm': 1.7360752820968628, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.274217963218689, 'eval_runtime': 6.7901, 'eval_samples_per_second': 147.274, 'eval_steps_per_second': 9.278, 'epoch': 0.52}
{'loss': 1.145, 'grad_norm': 2.181187868118286, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3161046504974365, 'eval_runtime': 6.7841, 'eval_samples_per_second': 147.404, 'eval_steps_per_second': 9.286, 'epoch': 0.56}
{'loss': 1.1369, 'grad_norm': 1.1943700313568115, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3416274785995483, 'eval_runtime': 6.82, 'eval_samples_per_second': 146.628, 'eval_steps_per_second': 9.238, 'epoch': 0.6}
{'loss': 1.1875, 'grad_norm': 1.2650425434112549, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.325809359550476, 'eval_runtime': 6.7889, 'eval_samples_per_second': 147.299, 'eval_steps_per_second': 9.28, 'epoch': 0.64}
{'loss': 1.1876, 'grad_norm': 1.711061954498291, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3237487077713013, 'eval_runtime': 6.7814, 'eval_samples_per_second': 147.461, 'eval_steps_per_second': 9.29, 'epoch': 0.68}
{'loss': 1.1257, 'grad_norm': 1.3729290962219238, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.304234266281128, 'eval_runtime': 6.7873, 'eval_samples_per_second': 147.333, 'eval_steps_per_second': 9.282, 'epoch': 0.72}
{'loss': 1.1068, 'grad_norm': 1.0763136148452759, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3348782062530518, 'eval_runtime': 6.7911, 'eval_samples_per_second': 147.252, 'eval_steps_per_second': 9.277, 'epoch': 0.76}
{'loss': 1.1131, 'grad_norm': 1.236512541770935, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3334022760391235, 'eval_runtime': 6.7851, 'eval_samples_per_second': 147.381, 'eval_steps_per_second': 9.285, 'epoch': 0.8}
{'loss': 1.1187, 'grad_norm': 1.2236624956130981, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.316656231880188, 'eval_runtime': 6.8186, 'eval_samples_per_second': 146.658, 'eval_steps_per_second': 9.239, 'epoch': 0.84}
{'loss': 1.0865, 'grad_norm': 1.378539800643921, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3242230415344238, 'eval_runtime': 6.8139, 'eval_samples_per_second': 146.76, 'eval_steps_per_second': 9.246, 'epoch': 0.88}
{'loss': 1.1338, 'grad_norm': 1.1045842170715332, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3408520221710205, 'eval_runtime': 6.8128, 'eval_samples_per_second': 146.784, 'eval_steps_per_second': 9.247, 'epoch': 0.92}
{'loss': 1.1019, 'grad_norm': 1.3381881713867188, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.347619652748108, 'eval_runtime': 6.7966, 'eval_samples_per_second': 147.133, 'eval_steps_per_second': 9.269, 'epoch': 0.96}
{'loss': 1.1241, 'grad_norm': 0.980785608291626, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3437349796295166, 'eval_runtime': 6.791, 'eval_samples_per_second': 147.253, 'eval_steps_per_second': 9.277, 'epoch': 1.0}
{'train_runtime': 373.0114, 'train_samples_per_second': 26.803, 'train_steps_per_second': 1.676, 'train_loss': 1.251323538208008, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7472347021102905, 1.2765514850616455, 1.2458009719848633, 1.2727160453796387, 1.2607346773147583, 1.2900171279907227, 1.1992756128311157, 1.262217402458191, 1.3412773609161377, 1.300826907157898, 1.2955495119094849, 1.2783076763153076, 1.274217963218689, 1.3161046504974365, 1.3416274785995483, 1.325809359550476, 1.3237487077713013, 1.304234266281128, 1.3348782062530518, 1.3334022760391235, 1.316656231880188, 1.3242230415344238, 1.3408520221710205, 1.347619652748108, 1.3437349796295166], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7472347021102905, 1.2765514850616455, 1.2458009719848633, 1.2727160453796387, 1.2607346773147583, 1.2900171279907227, 1.1992756128311157, 1.262217402458191, 1.3412773609161377, 1.300826907157898, 1.2955495119094849, 1.2783076763153076, 1.274217963218689, 1.3161046504974365, 1.3416274785995483, 1.325809359550476, 1.3237487077713013, 1.304234266281128, 1.3348782062530518, 1.3334022760391235, 1.316656231880188, 1.3242230415344238, 1.3408520221710205, 1.347619652748108, 1.3437349796295166]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0746192932128906
current iteration best possible eval_loss (full train run):  -1.3437349796295166
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5073 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.9230344248315928
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.9232987858517832
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.9230365555840486
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.9230344243720106
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.9230343557973721
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1093, dtype=torch.float64), 0, tensor(0.0452, dtype=torch.float64), tensor(0.6682, dtype=torch.float64), tensor(0.0211, dtype=torch.float64), 0, 0, tensor(0.1562, dtype=torch.float64), 26, 0, 1, 0, 1, 0, 128, 0.0, 40.88092978266088, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.1093, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0452, dtype=torch.float64), tensor(0.6682, dtype=torch.float64), tensor(0.0211, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.0489e-19, dtype=torch.float64), tensor(0.1562, dtype=torch.float64), tensor(0.8269, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8517, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.109
  rowan_hellaswag: 0
  sciq: 0.045
  triviaqa: 0.668
  truthfulqa_gen: 0.021
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.156

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (40.88092978266088,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  40.88092978266088
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 78,381,056 || all params: 8,108,642,304 || trainable%: 0.9666
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1333, 'grad_norm': 0.8037868738174438, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.643912672996521, 'eval_runtime': 6.7593, 'eval_samples_per_second': 147.944, 'eval_steps_per_second': 9.32, 'epoch': 0.04}
{'loss': 1.2666, 'grad_norm': 0.8216338753700256, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0418037176132202, 'eval_runtime': 6.744, 'eval_samples_per_second': 148.28, 'eval_steps_per_second': 9.342, 'epoch': 0.08}
{'loss': 1.0432, 'grad_norm': 0.33964329957962036, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9619011282920837, 'eval_runtime': 6.7775, 'eval_samples_per_second': 147.548, 'eval_steps_per_second': 9.296, 'epoch': 0.12}
{'loss': 0.9705, 'grad_norm': 0.6115435361862183, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9201875925064087, 'eval_runtime': 6.7881, 'eval_samples_per_second': 147.318, 'eval_steps_per_second': 9.281, 'epoch': 0.16}
{'loss': 0.9754, 'grad_norm': 0.3417013883590698, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9084815382957458, 'eval_runtime': 6.7943, 'eval_samples_per_second': 147.183, 'eval_steps_per_second': 9.273, 'epoch': 0.2}
{'loss': 0.9464, 'grad_norm': 0.32307469844818115, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9030910134315491, 'eval_runtime': 6.7977, 'eval_samples_per_second': 147.109, 'eval_steps_per_second': 9.268, 'epoch': 0.24}
{'loss': 0.9435, 'grad_norm': 0.302677184343338, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8982565999031067, 'eval_runtime': 6.8042, 'eval_samples_per_second': 146.969, 'eval_steps_per_second': 9.259, 'epoch': 0.28}
{'loss': 0.9488, 'grad_norm': 0.2993031442165375, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8983519673347473, 'eval_runtime': 6.806, 'eval_samples_per_second': 146.93, 'eval_steps_per_second': 9.257, 'epoch': 0.32}
{'loss': 0.9215, 'grad_norm': 0.2800767719745636, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8888233304023743, 'eval_runtime': 6.813, 'eval_samples_per_second': 146.779, 'eval_steps_per_second': 9.247, 'epoch': 0.36}
{'loss': 0.9226, 'grad_norm': 0.25989994406700134, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8905080556869507, 'eval_runtime': 6.8688, 'eval_samples_per_second': 145.586, 'eval_steps_per_second': 9.172, 'epoch': 0.4}
{'loss': 0.9044, 'grad_norm': 0.3378356695175171, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8894767165184021, 'eval_runtime': 6.8378, 'eval_samples_per_second': 146.247, 'eval_steps_per_second': 9.214, 'epoch': 0.44}
{'loss': 0.9286, 'grad_norm': 0.31977179646492004, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8884699940681458, 'eval_runtime': 6.8015, 'eval_samples_per_second': 147.026, 'eval_steps_per_second': 9.263, 'epoch': 0.48}
{'loss': 0.9244, 'grad_norm': 0.332672655582428, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8869447112083435, 'eval_runtime': 6.8116, 'eval_samples_per_second': 146.808, 'eval_steps_per_second': 9.249, 'epoch': 0.52}
{'loss': 0.8772, 'grad_norm': 0.26322653889656067, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8860527276992798, 'eval_runtime': 6.8975, 'eval_samples_per_second': 144.981, 'eval_steps_per_second': 9.134, 'epoch': 0.56}
{'loss': 0.8965, 'grad_norm': 0.2930963635444641, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8877260088920593, 'eval_runtime': 6.8741, 'eval_samples_per_second': 145.474, 'eval_steps_per_second': 9.165, 'epoch': 0.6}
{'loss': 0.9073, 'grad_norm': 0.20367993414402008, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8857954144477844, 'eval_runtime': 6.8458, 'eval_samples_per_second': 146.075, 'eval_steps_per_second': 9.203, 'epoch': 0.64}
{'loss': 0.9053, 'grad_norm': 0.293549507856369, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8853759765625, 'eval_runtime': 6.8467, 'eval_samples_per_second': 146.056, 'eval_steps_per_second': 9.202, 'epoch': 0.68}
{'loss': 0.9063, 'grad_norm': 0.26035842299461365, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8809775710105896, 'eval_runtime': 6.8292, 'eval_samples_per_second': 146.431, 'eval_steps_per_second': 9.225, 'epoch': 0.72}
{'loss': 0.8657, 'grad_norm': 0.3090342879295349, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8810306787490845, 'eval_runtime': 6.8318, 'eval_samples_per_second': 146.375, 'eval_steps_per_second': 9.222, 'epoch': 0.76}
{'loss': 0.8723, 'grad_norm': 0.3466093838214874, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8829351663589478, 'eval_runtime': 6.8438, 'eval_samples_per_second': 146.118, 'eval_steps_per_second': 9.205, 'epoch': 0.8}
{'loss': 0.8787, 'grad_norm': 0.2896869480609894, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8818630576133728, 'eval_runtime': 6.8427, 'eval_samples_per_second': 146.141, 'eval_steps_per_second': 9.207, 'epoch': 0.84}
{'loss': 0.8625, 'grad_norm': 0.250735878944397, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8805099129676819, 'eval_runtime': 6.8349, 'eval_samples_per_second': 146.308, 'eval_steps_per_second': 9.217, 'epoch': 0.88}
{'loss': 0.8751, 'grad_norm': 0.2665294110774994, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.882355809211731, 'eval_runtime': 6.8404, 'eval_samples_per_second': 146.189, 'eval_steps_per_second': 9.21, 'epoch': 0.92}
{'loss': 0.8717, 'grad_norm': 0.28439366817474365, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8806982636451721, 'eval_runtime': 6.8443, 'eval_samples_per_second': 146.106, 'eval_steps_per_second': 9.205, 'epoch': 0.96}
{'loss': 0.8682, 'grad_norm': 0.26490727066993713, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8800045251846313, 'eval_runtime': 6.8459, 'eval_samples_per_second': 146.072, 'eval_steps_per_second': 9.203, 'epoch': 1.0}
{'train_runtime': 342.2923, 'train_samples_per_second': 29.206, 'train_steps_per_second': 1.826, 'train_loss': 1.0166466033935546, 'epoch': 1.0}
train_results:  {'eval_loss': [1.643912672996521, 1.0418037176132202, 0.9619011282920837, 0.9201875925064087, 0.9084815382957458, 0.9030910134315491, 0.8982565999031067, 0.8983519673347473, 0.8888233304023743, 0.8905080556869507, 0.8894767165184021, 0.8884699940681458, 0.8869447112083435, 0.8860527276992798, 0.8877260088920593, 0.8857954144477844, 0.8853759765625, 0.8809775710105896, 0.8810306787490845, 0.8829351663589478, 0.8818630576133728, 0.8805099129676819, 0.882355809211731, 0.8806982636451721, 0.8800045251846313], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.643912672996521, 1.0418037176132202, 0.9619011282920837, 0.9201875925064087, 0.9084815382957458, 0.9030910134315491, 0.8982565999031067, 0.8983519673347473, 0.8888233304023743, 0.8905080556869507, 0.8894767165184021, 0.8884699940681458, 0.8869447112083435, 0.8860527276992798, 0.8877260088920593, 0.8857954144477844, 0.8853759765625, 0.8809775710105896, 0.8810306787490845, 0.8829351663589478, 0.8818630576133728, 0.8805099129676819, 0.882355809211731, 0.8806982636451721, 0.8800045251846313]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.4944487810134888
current iteration best possible eval_loss (full train run):  -0.8800045251846313
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0320 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.8993339544485742
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.8964834262507372
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.8993339668852454
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.8991381488229372
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.8992855405716531
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.6622, dtype=torch.float64), 0, tensor(0.0646, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2651, dtype=torch.float64), 0, 28, 1, 1, 0, 1, 0, 53, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.6622, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0646, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1793e-17, dtype=torch.float64), tensor(1.0818e-18, dtype=torch.float64), tensor(0.0080, dtype=torch.float64), tensor(0.2651, dtype=torch.float64), tensor(7.0599e-18, dtype=torch.float64), tensor(0.8810, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4106, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.662
  gsm8k: 0
  rowan_hellaswag: 0.065
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.265
  arc_challenge: 0

LoRA Parameters:
  lora_r: (53,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  53
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 47,108,096 || all params: 8,077,369,344 || trainable%: 0.5832
length of training data:  9919
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1179, 'grad_norm': 1.4784162044525146, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3264987468719482, 'eval_runtime': 7.2899, 'eval_samples_per_second': 137.177, 'eval_steps_per_second': 8.642, 'epoch': 0.04}
{'loss': 1.2962, 'grad_norm': 0.44190365076065063, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3004536628723145, 'eval_runtime': 7.2847, 'eval_samples_per_second': 137.275, 'eval_steps_per_second': 8.648, 'epoch': 0.08}
{'loss': 1.2023, 'grad_norm': 0.3502434492111206, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.2617919445037842, 'eval_runtime': 7.2832, 'eval_samples_per_second': 137.302, 'eval_steps_per_second': 8.65, 'epoch': 0.12}
{'loss': 1.1016, 'grad_norm': 0.32674017548561096, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.2760142087936401, 'eval_runtime': 7.3148, 'eval_samples_per_second': 136.709, 'eval_steps_per_second': 8.613, 'epoch': 0.16}
{'loss': 1.0647, 'grad_norm': 0.32988667488098145, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 1.2498632669448853, 'eval_runtime': 7.3306, 'eval_samples_per_second': 136.415, 'eval_steps_per_second': 8.594, 'epoch': 0.2}
{'loss': 1.1342, 'grad_norm': 0.43773722648620605, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 1.276638150215149, 'eval_runtime': 7.335, 'eval_samples_per_second': 136.332, 'eval_steps_per_second': 8.589, 'epoch': 0.24}
{'loss': 1.0951, 'grad_norm': 0.29735106229782104, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 1.2688668966293335, 'eval_runtime': 7.35, 'eval_samples_per_second': 136.054, 'eval_steps_per_second': 8.571, 'epoch': 0.28}
{'loss': 1.056, 'grad_norm': 0.39301684498786926, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 1.257810354232788, 'eval_runtime': 7.3607, 'eval_samples_per_second': 135.857, 'eval_steps_per_second': 8.559, 'epoch': 0.32}
{'loss': 1.0813, 'grad_norm': 0.32733044028282166, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 1.2612805366516113, 'eval_runtime': 7.3666, 'eval_samples_per_second': 135.748, 'eval_steps_per_second': 8.552, 'epoch': 0.36}
{'loss': 1.0829, 'grad_norm': 0.3143374025821686, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 1.2590173482894897, 'eval_runtime': 7.3694, 'eval_samples_per_second': 135.697, 'eval_steps_per_second': 8.549, 'epoch': 0.4}
{'loss': 1.1056, 'grad_norm': 0.29312431812286377, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 1.2738745212554932, 'eval_runtime': 7.4306, 'eval_samples_per_second': 134.578, 'eval_steps_per_second': 8.478, 'epoch': 0.44}
{'loss': 1.0733, 'grad_norm': 0.28672298789024353, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 1.2687623500823975, 'eval_runtime': 7.4167, 'eval_samples_per_second': 134.831, 'eval_steps_per_second': 8.494, 'epoch': 0.48}
{'loss': 1.045, 'grad_norm': 0.3192891776561737, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 1.2431288957595825, 'eval_runtime': 7.4143, 'eval_samples_per_second': 134.874, 'eval_steps_per_second': 8.497, 'epoch': 0.52}
{'loss': 1.043, 'grad_norm': 0.32610228657722473, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 1.2449140548706055, 'eval_runtime': 7.447, 'eval_samples_per_second': 134.282, 'eval_steps_per_second': 8.46, 'epoch': 0.56}
{'loss': 1.0674, 'grad_norm': 0.30860435962677, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 1.261899471282959, 'eval_runtime': 7.3739, 'eval_samples_per_second': 135.613, 'eval_steps_per_second': 8.544, 'epoch': 0.6}
{'loss': 1.0418, 'grad_norm': 0.31187447905540466, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 1.239119052886963, 'eval_runtime': 7.3603, 'eval_samples_per_second': 135.864, 'eval_steps_per_second': 8.559, 'epoch': 0.65}
{'loss': 1.068, 'grad_norm': 0.3305830955505371, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 1.2724586725234985, 'eval_runtime': 7.3607, 'eval_samples_per_second': 135.858, 'eval_steps_per_second': 8.559, 'epoch': 0.69}
{'loss': 1.0924, 'grad_norm': 0.35101616382598877, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 1.235573649406433, 'eval_runtime': 7.3673, 'eval_samples_per_second': 135.734, 'eval_steps_per_second': 8.551, 'epoch': 0.73}
{'loss': 1.0541, 'grad_norm': 0.3548502027988434, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 1.2538647651672363, 'eval_runtime': 7.3839, 'eval_samples_per_second': 135.43, 'eval_steps_per_second': 8.532, 'epoch': 0.77}
{'loss': 1.0653, 'grad_norm': 0.31561756134033203, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 1.2564570903778076, 'eval_runtime': 7.3468, 'eval_samples_per_second': 136.114, 'eval_steps_per_second': 8.575, 'epoch': 0.81}
{'loss': 1.0481, 'grad_norm': 0.27791905403137207, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 1.2508203983306885, 'eval_runtime': 7.3426, 'eval_samples_per_second': 136.192, 'eval_steps_per_second': 8.58, 'epoch': 0.85}
{'loss': 1.0099, 'grad_norm': 0.3255646526813507, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 1.2530620098114014, 'eval_runtime': 7.3264, 'eval_samples_per_second': 136.493, 'eval_steps_per_second': 8.599, 'epoch': 0.89}
{'loss': 1.0343, 'grad_norm': 0.30130523443222046, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 1.2466628551483154, 'eval_runtime': 7.3282, 'eval_samples_per_second': 136.459, 'eval_steps_per_second': 8.597, 'epoch': 0.93}
{'loss': 1.0567, 'grad_norm': 0.31299811601638794, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 1.2508457899093628, 'eval_runtime': 7.3855, 'eval_samples_per_second': 135.401, 'eval_steps_per_second': 8.53, 'epoch': 0.97}
{'train_runtime': 383.1045, 'train_samples_per_second': 25.891, 'train_steps_per_second': 1.618, 'train_loss': 1.1641929903338033, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3264987468719482, 1.3004536628723145, 1.2617919445037842, 1.2760142087936401, 1.2498632669448853, 1.276638150215149, 1.2688668966293335, 1.257810354232788, 1.2612805366516113, 1.2590173482894897, 1.2738745212554932, 1.2687623500823975, 1.2431288957595825, 1.2449140548706055, 1.261899471282959, 1.239119052886963, 1.2724586725234985, 1.235573649406433, 1.2538647651672363, 1.2564570903778076, 1.2508203983306885, 1.2530620098114014, 1.2466628551483154, 1.2508457899093628], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.3264987468719482, 1.3004536628723145, 1.2617919445037842, 1.2760142087936401, 1.2498632669448853, 1.276638150215149, 1.2688668966293335, 1.257810354232788, 1.2612805366516113, 1.2590173482894897, 1.2738745212554932, 1.2687623500823975, 1.2431288957595825, 1.2449140548706055, 1.261899471282959, 1.239119052886963, 1.2724586725234985, 1.235573649406433, 1.2538647651672363, 1.2564570903778076, 1.2508203983306885, 1.2530620098114014, 1.2466628551483154, 1.2508457899093628]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0830825567245483
current iteration best possible eval_loss (full train run):  -1.2508457899093628
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3698 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5646550059318542, 0.9397340416908264, 0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.2664431631565094, 0.2574614882469177, 0.06447935104370117, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.30395135283470154, 0.8692778944969177, 0.17745260894298553, 0.919781506061554]  ‚Üí  acq = -0.928086401832898
X = [0.9089382886886597, 0.2508368492126465, 0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6453872919082642, 0.29114675521850586, 0.1965603232383728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6267572045326233, 0.9949815273284912, 0.7691781520843506, 0.9943764209747314]  ‚Üí  acq = -0.9280863757940305
X = [0.8503569960594177, 0.5030623078346252, 0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.18871216475963593, 0.41532599925994873, 0.3550078272819519, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8731500506401062, 0.6550924777984619, 0.047274813055992126, 0.9721140265464783]  ‚Üí  acq = -0.9280839257490537
X = [0.044066667556762695, 0.3236018419265747, 0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8502388000488281, 0.7014566659927368, 0.3285099267959595, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.33359673619270325, 0.12362712621688843, 0.09682413935661316, 0.7159355282783508]  ‚Üí  acq = -0.9271500420100145
X = [0.8981380462646484, 0.41329818964004517, 0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8451046347618103, 0.6754447817802429, 0.39419329166412354, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9611881971359253, 0.1054425835609436, 0.06551053375005722, 0.9501203298568726]  ‚Üí  acq = -0.9280859966371025
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2529, dtype=torch.float64), 0, tensor(0.3730, dtype=torch.float64), 0, tensor(0.0682, dtype=torch.float64), tensor(0.2959, dtype=torch.float64), tensor(0.0101, dtype=torch.float64), 28, 1, 1, 0, 1, 1, 35, 1.7347234759768084e-19, 22.213978851097316, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.7393e-18, dtype=torch.float64), tensor(0.2529, dtype=torch.float64), tensor(6.8293e-18, dtype=torch.float64), tensor(0.3730, dtype=torch.float64), tensor(6.7208e-18, dtype=torch.float64), tensor(0.0682, dtype=torch.float64), tensor(0.2959, dtype=torch.float64), tensor(0.0101, dtype=torch.float64), tensor(0.8876, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2766, dtype=torch.float64), tensor(1.7347e-18, dtype=torch.float64), tensor(0.4628, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.253
  sciq: 0
  triviaqa: 0.373
  truthfulqa_gen: 0
  wikitext: 0.068
  mmlu: 0.296
  arc_challenge: 0.01

LoRA Parameters:
  lora_r: (35,)
  lora_dropout: (1.7347234759768084e-19,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (22.213978851097316,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  35
lora dropout:  1.7347234759768084e-19
lora alpha:  22.213978851097316
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 49,172,480 || all params: 8,079,433,728 || trainable%: 0.6086
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3387, 'grad_norm': 1.0087264776229858, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9103626012802124, 'eval_runtime': 7.726, 'eval_samples_per_second': 129.433, 'eval_steps_per_second': 8.154, 'epoch': 0.04}
{'loss': 1.8757, 'grad_norm': 0.40855658054351807, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2094213962554932, 'eval_runtime': 7.7218, 'eval_samples_per_second': 129.503, 'eval_steps_per_second': 8.159, 'epoch': 0.08}
{'loss': 1.5141, 'grad_norm': 0.4161183834075928, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1647628545761108, 'eval_runtime': 7.7624, 'eval_samples_per_second': 128.825, 'eval_steps_per_second': 8.116, 'epoch': 0.12}
{'loss': 1.5302, 'grad_norm': 0.3293558657169342, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.154779076576233, 'eval_runtime': 7.7936, 'eval_samples_per_second': 128.311, 'eval_steps_per_second': 8.084, 'epoch': 0.16}
{'loss': 1.5452, 'grad_norm': 0.2649097740650177, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0694150924682617, 'eval_runtime': 7.8081, 'eval_samples_per_second': 128.071, 'eval_steps_per_second': 8.068, 'epoch': 0.2}
{'loss': 1.4919, 'grad_norm': 0.24917474389076233, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1075003147125244, 'eval_runtime': 7.8037, 'eval_samples_per_second': 128.145, 'eval_steps_per_second': 8.073, 'epoch': 0.24}
{'loss': 1.5463, 'grad_norm': 0.2636302411556244, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0568405389785767, 'eval_runtime': 7.8027, 'eval_samples_per_second': 128.161, 'eval_steps_per_second': 8.074, 'epoch': 0.28}
{'loss': 1.4518, 'grad_norm': 0.274480938911438, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0557260513305664, 'eval_runtime': 7.8279, 'eval_samples_per_second': 127.749, 'eval_steps_per_second': 8.048, 'epoch': 0.32}
{'loss': 1.4729, 'grad_norm': 0.2578987181186676, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0499895811080933, 'eval_runtime': 7.8294, 'eval_samples_per_second': 127.723, 'eval_steps_per_second': 8.047, 'epoch': 0.36}
{'loss': 1.4672, 'grad_norm': 0.2862406373023987, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0576695203781128, 'eval_runtime': 7.8071, 'eval_samples_per_second': 128.088, 'eval_steps_per_second': 8.07, 'epoch': 0.4}
{'loss': 1.4276, 'grad_norm': 0.31266793608665466, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.026434063911438, 'eval_runtime': 7.7968, 'eval_samples_per_second': 128.258, 'eval_steps_per_second': 8.08, 'epoch': 0.44}
{'loss': 1.446, 'grad_norm': 0.25654542446136475, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0230766534805298, 'eval_runtime': 7.7713, 'eval_samples_per_second': 128.678, 'eval_steps_per_second': 8.107, 'epoch': 0.48}
{'loss': 1.4679, 'grad_norm': 0.25436216592788696, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.050697922706604, 'eval_runtime': 7.78, 'eval_samples_per_second': 128.535, 'eval_steps_per_second': 8.098, 'epoch': 0.52}
{'loss': 1.3642, 'grad_norm': 0.31598150730133057, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0101126432418823, 'eval_runtime': 7.7762, 'eval_samples_per_second': 128.597, 'eval_steps_per_second': 8.102, 'epoch': 0.56}
{'loss': 1.4548, 'grad_norm': 0.30910196900367737, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.016783356666565, 'eval_runtime': 7.7542, 'eval_samples_per_second': 128.962, 'eval_steps_per_second': 8.125, 'epoch': 0.6}
{'loss': 1.4591, 'grad_norm': 0.2981853187084198, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0222660303115845, 'eval_runtime': 7.7608, 'eval_samples_per_second': 128.852, 'eval_steps_per_second': 8.118, 'epoch': 0.64}
{'loss': 1.4526, 'grad_norm': 0.29342493414878845, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0054330825805664, 'eval_runtime': 7.7613, 'eval_samples_per_second': 128.844, 'eval_steps_per_second': 8.117, 'epoch': 0.68}
{'loss': 1.4482, 'grad_norm': 0.26639482378959656, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0232974290847778, 'eval_runtime': 7.7563, 'eval_samples_per_second': 128.928, 'eval_steps_per_second': 8.122, 'epoch': 0.72}
{'loss': 1.4176, 'grad_norm': 0.3107680678367615, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.023247241973877, 'eval_runtime': 7.7921, 'eval_samples_per_second': 128.335, 'eval_steps_per_second': 8.085, 'epoch': 0.76}
{'loss': 1.4427, 'grad_norm': 0.29778847098350525, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0187747478485107, 'eval_runtime': 7.8107, 'eval_samples_per_second': 128.03, 'eval_steps_per_second': 8.066, 'epoch': 0.8}
{'loss': 1.4777, 'grad_norm': 0.31882548332214355, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0188876390457153, 'eval_runtime': 7.7579, 'eval_samples_per_second': 128.9, 'eval_steps_per_second': 8.121, 'epoch': 0.84}
{'loss': 1.4313, 'grad_norm': 0.28390079736709595, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0056604146957397, 'eval_runtime': 7.7696, 'eval_samples_per_second': 128.707, 'eval_steps_per_second': 8.109, 'epoch': 0.88}
{'loss': 1.44, 'grad_norm': 0.30581381916999817, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0059361457824707, 'eval_runtime': 7.7623, 'eval_samples_per_second': 128.827, 'eval_steps_per_second': 8.116, 'epoch': 0.92}
{'loss': 1.4124, 'grad_norm': 0.3350241482257843, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0083022117614746, 'eval_runtime': 7.7565, 'eval_samples_per_second': 128.924, 'eval_steps_per_second': 8.122, 'epoch': 0.96}
{'loss': 1.4528, 'grad_norm': 0.3525427579879761, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0064753293991089, 'eval_runtime': 7.752, 'eval_samples_per_second': 128.999, 'eval_steps_per_second': 8.127, 'epoch': 1.0}
{'train_runtime': 438.5467, 'train_samples_per_second': 22.793, 'train_steps_per_second': 1.425, 'train_loss': 1.553155401611328, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9103626012802124, 1.2094213962554932, 1.1647628545761108, 1.154779076576233, 1.0694150924682617, 1.1075003147125244, 1.0568405389785767, 1.0557260513305664, 1.0499895811080933, 1.0576695203781128, 1.026434063911438, 1.0230766534805298, 1.050697922706604, 1.0101126432418823, 1.016783356666565, 1.0222660303115845, 1.0054330825805664, 1.0232974290847778, 1.023247241973877, 1.0187747478485107, 1.0188876390457153, 1.0056604146957397, 1.0059361457824707, 1.0083022117614746, 1.0064753293991089], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9103626012802124, 1.2094213962554932, 1.1647628545761108, 1.154779076576233, 1.0694150924682617, 1.1075003147125244, 1.0568405389785767, 1.0557260513305664, 1.0499895811080933, 1.0576695203781128, 1.026434063911438, 1.0230766534805298, 1.050697922706604, 1.0101126432418823, 1.016783356666565, 1.0222660303115845, 1.0054330825805664, 1.0232974290847778, 1.023247241973877, 1.0187747478485107, 1.0188876390457153, 1.0056604146957397, 1.0059361457824707, 1.0083022117614746, 1.0064753293991089]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0938282012939453
current iteration best possible eval_loss (full train run):  -1.0064753293991089
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.3084 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.950719883065121
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.9504099035262088
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.9504087970075887
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.9502810292403927
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.9504099037462339
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0121, dtype=torch.float64), 0, 0, 0, tensor(0.0804, dtype=torch.float64), tensor(0.3317, dtype=torch.float64), tensor(0.5758, dtype=torch.float64), 25, 1, 1, 0, 1, 0, 36, 0.0988331548307494, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(8.6904e-18, dtype=torch.float64), tensor(6.4971e-18, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(6.0410e-20, dtype=torch.float64), tensor(1.8714e-19, dtype=torch.float64), tensor(9.5250e-19, dtype=torch.float64), tensor(0.0804, dtype=torch.float64), tensor(0.3317, dtype=torch.float64), tensor(0.5758, dtype=torch.float64), tensor(0.7660, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2842, dtype=torch.float64), tensor(0.9883, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.012
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.08
  mmlu: 0.332
  arc_challenge: 0.576

LoRA Parameters:
  lora_r: (36,)
  lora_dropout: (0.0988331548307494,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  36
lora dropout:  0.0988331548307494
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 28,569,600 || all params: 8,058,830,848 || trainable%: 0.3545
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7053, 'grad_norm': 1.138439416885376, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.207516074180603, 'eval_runtime': 6.9552, 'eval_samples_per_second': 143.777, 'eval_steps_per_second': 9.058, 'epoch': 0.04}
{'loss': 1.2195, 'grad_norm': 0.5494208931922913, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9197360277175903, 'eval_runtime': 6.9656, 'eval_samples_per_second': 143.564, 'eval_steps_per_second': 9.045, 'epoch': 0.08}
{'loss': 1.1383, 'grad_norm': 0.43088003993034363, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8894661068916321, 'eval_runtime': 6.9838, 'eval_samples_per_second': 143.189, 'eval_steps_per_second': 9.021, 'epoch': 0.12}
{'loss': 1.0737, 'grad_norm': 0.5876487493515015, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8829020857810974, 'eval_runtime': 6.9868, 'eval_samples_per_second': 143.126, 'eval_steps_per_second': 9.017, 'epoch': 0.16}
{'loss': 1.0522, 'grad_norm': 0.43189767003059387, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8832277059555054, 'eval_runtime': 7.0109, 'eval_samples_per_second': 142.634, 'eval_steps_per_second': 8.986, 'epoch': 0.2}
{'loss': 1.0385, 'grad_norm': 0.3675153851509094, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8765341639518738, 'eval_runtime': 7.0239, 'eval_samples_per_second': 142.371, 'eval_steps_per_second': 8.969, 'epoch': 0.24}
{'loss': 1.0572, 'grad_norm': 0.4446634352207184, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8785669207572937, 'eval_runtime': 7.0202, 'eval_samples_per_second': 142.447, 'eval_steps_per_second': 8.974, 'epoch': 0.28}
{'loss': 1.068, 'grad_norm': 0.42057153582572937, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8854618072509766, 'eval_runtime': 7.033, 'eval_samples_per_second': 142.186, 'eval_steps_per_second': 8.958, 'epoch': 0.32}
{'loss': 1.0142, 'grad_norm': 0.5868207812309265, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8843828439712524, 'eval_runtime': 7.027, 'eval_samples_per_second': 142.308, 'eval_steps_per_second': 8.965, 'epoch': 0.36}
{'loss': 0.9972, 'grad_norm': 0.43248072266578674, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.897257924079895, 'eval_runtime': 7.0291, 'eval_samples_per_second': 142.266, 'eval_steps_per_second': 8.963, 'epoch': 0.4}
{'loss': 0.9708, 'grad_norm': 0.4856530427932739, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9116828441619873, 'eval_runtime': 7.0442, 'eval_samples_per_second': 141.96, 'eval_steps_per_second': 8.943, 'epoch': 0.44}
{'loss': 0.9925, 'grad_norm': 0.6576420068740845, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9294288754463196, 'eval_runtime': 7.0429, 'eval_samples_per_second': 141.987, 'eval_steps_per_second': 8.945, 'epoch': 0.48}
{'loss': 0.926, 'grad_norm': 0.6127924919128418, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9241099953651428, 'eval_runtime': 7.0315, 'eval_samples_per_second': 142.217, 'eval_steps_per_second': 8.96, 'epoch': 0.52}
{'loss': 0.8821, 'grad_norm': 0.5660924911499023, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9763103127479553, 'eval_runtime': 7.0502, 'eval_samples_per_second': 141.84, 'eval_steps_per_second': 8.936, 'epoch': 0.56}
{'loss': 0.8924, 'grad_norm': 0.7737065553665161, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9826990365982056, 'eval_runtime': 7.0376, 'eval_samples_per_second': 142.094, 'eval_steps_per_second': 8.952, 'epoch': 0.6}
{'loss': 0.8841, 'grad_norm': 0.6356682777404785, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9843851327896118, 'eval_runtime': 7.0848, 'eval_samples_per_second': 141.148, 'eval_steps_per_second': 8.892, 'epoch': 0.64}
{'loss': 0.9309, 'grad_norm': 0.58702552318573, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9542276859283447, 'eval_runtime': 7.0739, 'eval_samples_per_second': 141.365, 'eval_steps_per_second': 8.906, 'epoch': 0.68}
{'loss': 0.833, 'grad_norm': 0.7509576082229614, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9929141402244568, 'eval_runtime': 7.0991, 'eval_samples_per_second': 140.862, 'eval_steps_per_second': 8.874, 'epoch': 0.72}
{'loss': 0.808, 'grad_norm': 0.6181496381759644, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0111525058746338, 'eval_runtime': 7.0712, 'eval_samples_per_second': 141.419, 'eval_steps_per_second': 8.909, 'epoch': 0.76}
{'loss': 0.8007, 'grad_norm': 0.5899245738983154, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.074229121208191, 'eval_runtime': 7.0518, 'eval_samples_per_second': 141.807, 'eval_steps_per_second': 8.934, 'epoch': 0.8}
{'loss': 0.7951, 'grad_norm': 0.638844907283783, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0393098592758179, 'eval_runtime': 7.0481, 'eval_samples_per_second': 141.883, 'eval_steps_per_second': 8.939, 'epoch': 0.84}
{'loss': 0.7372, 'grad_norm': 0.7683412432670593, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.082590937614441, 'eval_runtime': 7.0336, 'eval_samples_per_second': 142.175, 'eval_steps_per_second': 8.957, 'epoch': 0.88}
{'loss': 0.7577, 'grad_norm': 0.7076481580734253, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0666868686676025, 'eval_runtime': 7.0513, 'eval_samples_per_second': 141.818, 'eval_steps_per_second': 8.935, 'epoch': 0.92}
{'loss': 0.8127, 'grad_norm': 0.662765383720398, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0622390508651733, 'eval_runtime': 7.0495, 'eval_samples_per_second': 141.854, 'eval_steps_per_second': 8.937, 'epoch': 0.96}
{'loss': 0.7403, 'grad_norm': 0.6316246390342712, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0723865032196045, 'eval_runtime': 7.0312, 'eval_samples_per_second': 142.223, 'eval_steps_per_second': 8.96, 'epoch': 1.0}
{'train_runtime': 365.7349, 'train_samples_per_second': 27.337, 'train_steps_per_second': 1.709, 'train_loss': 1.0051057556152343, 'epoch': 1.0}
train_results:  {'eval_loss': [1.207516074180603, 0.9197360277175903, 0.8894661068916321, 0.8829020857810974, 0.8832277059555054, 0.8765341639518738, 0.8785669207572937, 0.8854618072509766, 0.8843828439712524, 0.897257924079895, 0.9116828441619873, 0.9294288754463196, 0.9241099953651428, 0.9763103127479553, 0.9826990365982056, 0.9843851327896118, 0.9542276859283447, 0.9929141402244568, 1.0111525058746338, 1.074229121208191, 1.0393098592758179, 1.082590937614441, 1.0666868686676025, 1.0622390508651733, 1.0723865032196045], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.207516074180603, 0.9197360277175903, 0.8894661068916321, 0.8829020857810974, 0.8832277059555054, 0.8765341639518738, 0.8785669207572937, 0.8854618072509766, 0.8843828439712524, 0.897257924079895, 0.9116828441619873, 0.9294288754463196, 0.9241099953651428, 0.9763103127479553, 0.9826990365982056, 0.9843851327896118, 0.9542276859283447, 0.9929141402244568, 1.0111525058746338, 1.074229121208191, 1.0393098592758179, 1.082590937614441, 1.0666868686676025, 1.0622390508651733, 1.0723865032196045]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760307312011719
current iteration best possible eval_loss (full train run):  -1.0723865032196045
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8055 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -0.963490133213937
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -0.9638110878147461
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -0.9634905839591097
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -0.9636693380833121
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -0.9634905829290006
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0328, dtype=torch.float64), 0, tensor(0.1078, dtype=torch.float64), tensor(0.1413, dtype=torch.float64), tensor(0.5237, dtype=torch.float64), 0, tensor(0.0400, dtype=torch.float64), tensor(0.1544, dtype=torch.float64), 0, 30, 1, 1, 1, 1, 1, 2, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0328, dtype=torch.float64), tensor(1.4148e-17, dtype=torch.float64), tensor(0.1078, dtype=torch.float64), tensor(0.1413, dtype=torch.float64), tensor(0.5237, dtype=torch.float64), tensor(5.7884e-18, dtype=torch.float64), tensor(0.0400, dtype=torch.float64), tensor(0.1544, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9272, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.033
  gsm8k: 0
  rowan_hellaswag: 0.108
  sciq: 0.141
  triviaqa: 0.524
  truthfulqa_gen: 0
  wikitext: 0.04
  mmlu: 0.154
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 4,116,480 || all params: 8,034,377,728 || trainable%: 0.0512
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.962, 'grad_norm': 4.579690456390381, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5173488855361938, 'eval_runtime': 8.0354, 'eval_samples_per_second': 124.45, 'eval_steps_per_second': 7.84, 'epoch': 0.04}
{'loss': 1.5322, 'grad_norm': 4.216945171356201, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2635647058486938, 'eval_runtime': 8.0478, 'eval_samples_per_second': 124.257, 'eval_steps_per_second': 7.828, 'epoch': 0.08}
{'loss': 1.3841, 'grad_norm': 3.9627203941345215, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2307586669921875, 'eval_runtime': 8.0373, 'eval_samples_per_second': 124.419, 'eval_steps_per_second': 7.838, 'epoch': 0.12}
{'loss': 1.3056, 'grad_norm': 2.260288953781128, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2865538597106934, 'eval_runtime': 8.0412, 'eval_samples_per_second': 124.36, 'eval_steps_per_second': 7.835, 'epoch': 0.16}
{'loss': 1.3294, 'grad_norm': 2.7404427528381348, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2485710382461548, 'eval_runtime': 8.0453, 'eval_samples_per_second': 124.296, 'eval_steps_per_second': 7.831, 'epoch': 0.2}
{'loss': 1.3259, 'grad_norm': 2.0676467418670654, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2444756031036377, 'eval_runtime': 8.07, 'eval_samples_per_second': 123.916, 'eval_steps_per_second': 7.807, 'epoch': 0.24}
{'loss': 1.3003, 'grad_norm': 2.288783073425293, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2579021453857422, 'eval_runtime': 8.0939, 'eval_samples_per_second': 123.549, 'eval_steps_per_second': 7.784, 'epoch': 0.28}
{'loss': 1.3371, 'grad_norm': 2.254210948944092, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2298017740249634, 'eval_runtime': 8.0514, 'eval_samples_per_second': 124.201, 'eval_steps_per_second': 7.825, 'epoch': 0.32}
{'loss': 1.291, 'grad_norm': 1.7136744260787964, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.273553729057312, 'eval_runtime': 8.0463, 'eval_samples_per_second': 124.281, 'eval_steps_per_second': 7.83, 'epoch': 0.36}
{'loss': 1.3083, 'grad_norm': 2.028874158859253, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2177375555038452, 'eval_runtime': 8.0206, 'eval_samples_per_second': 124.679, 'eval_steps_per_second': 7.855, 'epoch': 0.4}
{'loss': 1.2774, 'grad_norm': 2.601566791534424, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2703615427017212, 'eval_runtime': 8.0052, 'eval_samples_per_second': 124.918, 'eval_steps_per_second': 7.87, 'epoch': 0.44}
{'loss': 1.3703, 'grad_norm': 2.2080631256103516, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2491698265075684, 'eval_runtime': 8.0071, 'eval_samples_per_second': 124.889, 'eval_steps_per_second': 7.868, 'epoch': 0.48}
{'loss': 1.2479, 'grad_norm': 1.6872059106826782, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2982265949249268, 'eval_runtime': 8.0117, 'eval_samples_per_second': 124.818, 'eval_steps_per_second': 7.864, 'epoch': 0.52}
{'loss': 1.3324, 'grad_norm': 2.8874053955078125, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.268073320388794, 'eval_runtime': 8.0087, 'eval_samples_per_second': 124.864, 'eval_steps_per_second': 7.866, 'epoch': 0.56}
{'loss': 1.2702, 'grad_norm': 2.4587435722351074, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.281139850616455, 'eval_runtime': 8.0398, 'eval_samples_per_second': 124.382, 'eval_steps_per_second': 7.836, 'epoch': 0.6}
{'loss': 1.2271, 'grad_norm': 1.8058983087539673, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3250690698623657, 'eval_runtime': 8.0515, 'eval_samples_per_second': 124.2, 'eval_steps_per_second': 7.825, 'epoch': 0.64}
{'loss': 1.2506, 'grad_norm': 2.2487759590148926, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.325918436050415, 'eval_runtime': 8.0451, 'eval_samples_per_second': 124.299, 'eval_steps_per_second': 7.831, 'epoch': 0.68}
{'loss': 1.3091, 'grad_norm': 2.3820252418518066, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3357722759246826, 'eval_runtime': 8.0536, 'eval_samples_per_second': 124.168, 'eval_steps_per_second': 7.823, 'epoch': 0.72}
{'loss': 1.2983, 'grad_norm': 2.8666861057281494, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3185813426971436, 'eval_runtime': 8.0452, 'eval_samples_per_second': 124.298, 'eval_steps_per_second': 7.831, 'epoch': 0.76}
{'loss': 1.2756, 'grad_norm': 2.548440933227539, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3162399530410767, 'eval_runtime': 8.0542, 'eval_samples_per_second': 124.159, 'eval_steps_per_second': 7.822, 'epoch': 0.8}
{'loss': 1.2149, 'grad_norm': 1.893264889717102, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3168308734893799, 'eval_runtime': 8.0473, 'eval_samples_per_second': 124.265, 'eval_steps_per_second': 7.829, 'epoch': 0.84}
{'loss': 1.3385, 'grad_norm': 2.168120861053467, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.332710862159729, 'eval_runtime': 8.1122, 'eval_samples_per_second': 123.271, 'eval_steps_per_second': 7.766, 'epoch': 0.88}
{'loss': 1.2198, 'grad_norm': 1.6052348613739014, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3263413906097412, 'eval_runtime': 8.0511, 'eval_samples_per_second': 124.206, 'eval_steps_per_second': 7.825, 'epoch': 0.92}
{'loss': 1.225, 'grad_norm': 1.9162315130233765, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.326081395149231, 'eval_runtime': 8.0438, 'eval_samples_per_second': 124.319, 'eval_steps_per_second': 7.832, 'epoch': 0.96}
{'loss': 1.2128, 'grad_norm': 3.1254844665527344, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3263787031173706, 'eval_runtime': 8.0617, 'eval_samples_per_second': 124.043, 'eval_steps_per_second': 7.815, 'epoch': 1.0}
{'train_runtime': 445.3813, 'train_samples_per_second': 22.446, 'train_steps_per_second': 1.403, 'train_loss': 1.3658391845703124, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5173488855361938, 1.2635647058486938, 1.2307586669921875, 1.2865538597106934, 1.2485710382461548, 1.2444756031036377, 1.2579021453857422, 1.2298017740249634, 1.273553729057312, 1.2177375555038452, 1.2703615427017212, 1.2491698265075684, 1.2982265949249268, 1.268073320388794, 1.281139850616455, 1.3250690698623657, 1.325918436050415, 1.3357722759246826, 1.3185813426971436, 1.3162399530410767, 1.3168308734893799, 1.332710862159729, 1.3263413906097412, 1.326081395149231, 1.3263787031173706], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5173488855361938, 1.2635647058486938, 1.2307586669921875, 1.2865538597106934, 1.2485710382461548, 1.2444756031036377, 1.2579021453857422, 1.2298017740249634, 1.273553729057312, 1.2177375555038452, 1.2703615427017212, 1.2491698265075684, 1.2982265949249268, 1.268073320388794, 1.281139850616455, 1.3250690698623657, 1.325918436050415, 1.3357722759246826, 1.3185813426971436, 1.3162399530410767, 1.3168308734893799, 1.332710862159729, 1.3263413906097412, 1.326081395149231, 1.3263787031173706]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760270357131958
current iteration best possible eval_loss (full train run):  -1.3263787031173706
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1693 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -0.9746950631132019
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -0.9746596837757785
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -0.9746945048451642
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.9735881241033812
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -0.974691050282297
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.5291, dtype=torch.float64), tensor(0.0726, dtype=torch.float64), tensor(0.0197, dtype=torch.float64), tensor(0.3786, dtype=torch.float64), 0, 29, 1, 1, 0, 1, 1, 39, 1.1707085446020326e-18, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(7.5580e-18, dtype=torch.float64), tensor(1.2656e-17, dtype=torch.float64), tensor(3.5313e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5291, dtype=torch.float64), tensor(0.0726, dtype=torch.float64), tensor(0.0197, dtype=torch.float64), tensor(0.3786, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9150, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3025, dtype=torch.float64), tensor(1.1707e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.529
  truthfulqa_gen: 0.073
  wikitext: 0.02
  mmlu: 0.379
  arc_challenge: 0

LoRA Parameters:
  lora_r: (39,)
  lora_dropout: (1.1707085446020326e-18,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  39
lora dropout:  1.1707085446020326e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 56,749,056 || all params: 8,087,010,304 || trainable%: 0.7017
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.1211, 'grad_norm': 0.3874319791793823, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.979084014892578, 'eval_runtime': 7.8171, 'eval_samples_per_second': 127.925, 'eval_steps_per_second': 8.059, 'epoch': 0.04}
{'loss': 2.1458, 'grad_norm': 0.5828085541725159, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9821484088897705, 'eval_runtime': 7.8162, 'eval_samples_per_second': 127.939, 'eval_steps_per_second': 8.06, 'epoch': 0.08}
{'loss': 1.4912, 'grad_norm': 0.17210747301578522, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7117358446121216, 'eval_runtime': 7.8095, 'eval_samples_per_second': 128.049, 'eval_steps_per_second': 8.067, 'epoch': 0.12}
{'loss': 1.2587, 'grad_norm': 0.09071533381938934, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6708742380142212, 'eval_runtime': 7.8155, 'eval_samples_per_second': 127.951, 'eval_steps_per_second': 8.061, 'epoch': 0.16}
{'loss': 1.3152, 'grad_norm': 0.1196354553103447, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7308170795440674, 'eval_runtime': 7.8469, 'eval_samples_per_second': 127.439, 'eval_steps_per_second': 8.029, 'epoch': 0.2}
{'loss': 1.2326, 'grad_norm': 0.088504858314991, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7307277917861938, 'eval_runtime': 7.8498, 'eval_samples_per_second': 127.392, 'eval_steps_per_second': 8.026, 'epoch': 0.24}
{'loss': 1.1548, 'grad_norm': 0.1316073089838028, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7753833532333374, 'eval_runtime': 7.8478, 'eval_samples_per_second': 127.424, 'eval_steps_per_second': 8.028, 'epoch': 0.28}
{'loss': 1.1725, 'grad_norm': 0.10962396115064621, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7578160762786865, 'eval_runtime': 7.8562, 'eval_samples_per_second': 127.288, 'eval_steps_per_second': 8.019, 'epoch': 0.32}
{'loss': 1.1619, 'grad_norm': 0.08910263329744339, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7678920030593872, 'eval_runtime': 7.8476, 'eval_samples_per_second': 127.427, 'eval_steps_per_second': 8.028, 'epoch': 0.36}
{'loss': 1.2178, 'grad_norm': 0.11381039023399353, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.797008991241455, 'eval_runtime': 7.8542, 'eval_samples_per_second': 127.321, 'eval_steps_per_second': 8.021, 'epoch': 0.4}
{'loss': 1.1555, 'grad_norm': 0.08050572127103806, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7937678098678589, 'eval_runtime': 7.8574, 'eval_samples_per_second': 127.269, 'eval_steps_per_second': 8.018, 'epoch': 0.44}
{'loss': 1.1223, 'grad_norm': 0.11402124911546707, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8303693532943726, 'eval_runtime': 7.8514, 'eval_samples_per_second': 127.365, 'eval_steps_per_second': 8.024, 'epoch': 0.48}
{'loss': 1.189, 'grad_norm': 0.11724881827831268, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8191803693771362, 'eval_runtime': 7.8518, 'eval_samples_per_second': 127.359, 'eval_steps_per_second': 8.024, 'epoch': 0.52}
{'loss': 1.2122, 'grad_norm': 0.10807641595602036, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.785597324371338, 'eval_runtime': 7.8531, 'eval_samples_per_second': 127.338, 'eval_steps_per_second': 8.022, 'epoch': 0.56}
{'loss': 1.1225, 'grad_norm': 0.13505245745182037, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7823879718780518, 'eval_runtime': 7.8518, 'eval_samples_per_second': 127.359, 'eval_steps_per_second': 8.024, 'epoch': 0.6}
{'loss': 1.1444, 'grad_norm': 0.08746233582496643, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.814953088760376, 'eval_runtime': 7.8502, 'eval_samples_per_second': 127.386, 'eval_steps_per_second': 8.025, 'epoch': 0.64}
{'loss': 1.2131, 'grad_norm': 0.10001606494188309, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7908707857131958, 'eval_runtime': 7.8469, 'eval_samples_per_second': 127.439, 'eval_steps_per_second': 8.029, 'epoch': 0.68}
{'loss': 1.1574, 'grad_norm': 0.11025819927453995, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.798435091972351, 'eval_runtime': 7.8821, 'eval_samples_per_second': 126.87, 'eval_steps_per_second': 7.993, 'epoch': 0.72}
{'loss': 1.2003, 'grad_norm': 0.10154261440038681, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7942323684692383, 'eval_runtime': 7.8743, 'eval_samples_per_second': 126.995, 'eval_steps_per_second': 8.001, 'epoch': 0.76}
{'loss': 1.1557, 'grad_norm': 0.09681162983179092, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8202775716781616, 'eval_runtime': 7.883, 'eval_samples_per_second': 126.856, 'eval_steps_per_second': 7.992, 'epoch': 0.8}
{'loss': 1.2042, 'grad_norm': 0.08387915045022964, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8073149919509888, 'eval_runtime': 7.8759, 'eval_samples_per_second': 126.97, 'eval_steps_per_second': 7.999, 'epoch': 0.84}
{'loss': 1.1866, 'grad_norm': 0.1007954552769661, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.815545678138733, 'eval_runtime': 7.8933, 'eval_samples_per_second': 126.69, 'eval_steps_per_second': 7.981, 'epoch': 0.88}
{'loss': 1.1898, 'grad_norm': 0.09914615005254745, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8227795362472534, 'eval_runtime': 7.8801, 'eval_samples_per_second': 126.902, 'eval_steps_per_second': 7.995, 'epoch': 0.92}
{'loss': 1.1753, 'grad_norm': 0.11550244688987732, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8346405029296875, 'eval_runtime': 7.8816, 'eval_samples_per_second': 126.877, 'eval_steps_per_second': 7.993, 'epoch': 0.96}
{'loss': 1.1088, 'grad_norm': 0.14983142912387848, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8367071151733398, 'eval_runtime': 7.89, 'eval_samples_per_second': 126.742, 'eval_steps_per_second': 7.985, 'epoch': 1.0}
{'train_runtime': 414.7706, 'train_samples_per_second': 24.105, 'train_steps_per_second': 1.507, 'train_loss': 1.352337435913086, 'epoch': 1.0}
train_results:  {'eval_loss': [2.979084014892578, 1.9821484088897705, 1.7117358446121216, 1.6708742380142212, 1.7308170795440674, 1.7307277917861938, 1.7753833532333374, 1.7578160762786865, 1.7678920030593872, 1.797008991241455, 1.7937678098678589, 1.8303693532943726, 1.8191803693771362, 1.785597324371338, 1.7823879718780518, 1.814953088760376, 1.7908707857131958, 1.798435091972351, 1.7942323684692383, 1.8202775716781616, 1.8073149919509888, 1.815545678138733, 1.8227795362472534, 1.8346405029296875, 1.8367071151733398], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.979084014892578, 1.9821484088897705, 1.7117358446121216, 1.6708742380142212, 1.7308170795440674, 1.7307277917861938, 1.7753833532333374, 1.7578160762786865, 1.7678920030593872, 1.797008991241455, 1.7937678098678589, 1.8303693532943726, 1.8191803693771362, 1.785597324371338, 1.7823879718780518, 1.814953088760376, 1.7908707857131958, 1.798435091972351, 1.7942323684692383, 1.8202775716781616, 1.8073149919509888, 1.815545678138733, 1.8227795362472534, 1.8346405029296875, 1.8367071151733398]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.336314082145691
current iteration best possible eval_loss (full train run):  -1.8367071151733398
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.4220 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.9857924829961526
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.9867025405353753
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.9857994960917943
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.9860438322406244
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.9857957609446324
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0455, dtype=torch.float64), tensor(0.1386, dtype=torch.float64), tensor(0.1575, dtype=torch.float64), tensor(0.0363, dtype=torch.float64), tensor(0.1384, dtype=torch.float64), tensor(0.2139, dtype=torch.float64), tensor(0.0467, dtype=torch.float64), tensor(0.1072, dtype=torch.float64), tensor(0.1159, dtype=torch.float64), 26, 0, 1, 1, 0, 1, 2, 0.06729646361918733, 26.908586398326868, 0]
normalized proposed parameters for next round by BO: [tensor(0.0455, dtype=torch.float64), tensor(0.1386, dtype=torch.float64), tensor(0.1575, dtype=torch.float64), tensor(0.0363, dtype=torch.float64), tensor(0.1384, dtype=torch.float64), tensor(0.2139, dtype=torch.float64), tensor(0.0467, dtype=torch.float64), tensor(0.1072, dtype=torch.float64), tensor(0.1159, dtype=torch.float64), tensor(0.8221, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.6730, dtype=torch.float64), tensor(0.5606, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.045
  gsm8k: 0.139
  rowan_hellaswag: 0.157
  sciq: 0.036
  triviaqa: 0.138
  truthfulqa_gen: 0.214
  wikitext: 0.047
  mmlu: 0.107
  arc_challenge: 0.116

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.06729646361918733,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (26.908586398326868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  2
lora dropout:  0.06729646361918733
lora alpha:  26.908586398326868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,183,168 || all params: 8,032,444,416 || trainable%: 0.0272
length of training data:  9994
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1102, 'grad_norm': 5.321508407592773, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6427148580551147, 'eval_runtime': 7.0957, 'eval_samples_per_second': 140.93, 'eval_steps_per_second': 8.879, 'epoch': 0.04}
{'loss': 1.6018, 'grad_norm': 1.6565693616867065, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1069847345352173, 'eval_runtime': 7.0769, 'eval_samples_per_second': 141.304, 'eval_steps_per_second': 8.902, 'epoch': 0.08}
{'loss': 1.4475, 'grad_norm': 1.41703462600708, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0135471820831299, 'eval_runtime': 7.1266, 'eval_samples_per_second': 140.319, 'eval_steps_per_second': 8.84, 'epoch': 0.12}
{'loss': 1.3468, 'grad_norm': 1.1176872253417969, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.940025269985199, 'eval_runtime': 7.1093, 'eval_samples_per_second': 140.661, 'eval_steps_per_second': 8.862, 'epoch': 0.16}
{'loss': 1.2442, 'grad_norm': 1.2460601329803467, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.922090470790863, 'eval_runtime': 7.1253, 'eval_samples_per_second': 140.346, 'eval_steps_per_second': 8.842, 'epoch': 0.2}
{'loss': 1.2447, 'grad_norm': 1.0991084575653076, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9124214053153992, 'eval_runtime': 7.1868, 'eval_samples_per_second': 139.144, 'eval_steps_per_second': 8.766, 'epoch': 0.24}
{'loss': 1.215, 'grad_norm': 1.075929880142212, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9094818830490112, 'eval_runtime': 7.2332, 'eval_samples_per_second': 138.251, 'eval_steps_per_second': 8.71, 'epoch': 0.28}
{'loss': 1.2158, 'grad_norm': 0.9524815082550049, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9048800468444824, 'eval_runtime': 7.2509, 'eval_samples_per_second': 137.915, 'eval_steps_per_second': 8.689, 'epoch': 0.32}
{'loss': 1.185, 'grad_norm': 0.9598311185836792, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9061591029167175, 'eval_runtime': 7.2545, 'eval_samples_per_second': 137.846, 'eval_steps_per_second': 8.684, 'epoch': 0.36}
{'loss': 1.1273, 'grad_norm': 1.0304951667785645, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8977591395378113, 'eval_runtime': 7.1829, 'eval_samples_per_second': 139.22, 'eval_steps_per_second': 8.771, 'epoch': 0.4}
{'loss': 1.2137, 'grad_norm': 1.0714539289474487, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8993566632270813, 'eval_runtime': 7.1821, 'eval_samples_per_second': 139.234, 'eval_steps_per_second': 8.772, 'epoch': 0.44}
{'loss': 1.2242, 'grad_norm': 1.044653296470642, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8960266709327698, 'eval_runtime': 7.1923, 'eval_samples_per_second': 139.038, 'eval_steps_per_second': 8.759, 'epoch': 0.48}
{'loss': 1.1645, 'grad_norm': 1.043409824371338, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8953800201416016, 'eval_runtime': 7.1824, 'eval_samples_per_second': 139.23, 'eval_steps_per_second': 8.771, 'epoch': 0.52}
{'loss': 1.1616, 'grad_norm': 0.9491264224052429, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8920360803604126, 'eval_runtime': 7.197, 'eval_samples_per_second': 138.946, 'eval_steps_per_second': 8.754, 'epoch': 0.56}
{'loss': 1.1857, 'grad_norm': 1.3933755159378052, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8997157216072083, 'eval_runtime': 7.1955, 'eval_samples_per_second': 138.975, 'eval_steps_per_second': 8.755, 'epoch': 0.6}
{'loss': 1.1665, 'grad_norm': 0.9624735713005066, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8948692679405212, 'eval_runtime': 7.1886, 'eval_samples_per_second': 139.109, 'eval_steps_per_second': 8.764, 'epoch': 0.64}
{'loss': 1.1684, 'grad_norm': 0.9673340916633606, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8980345726013184, 'eval_runtime': 7.1744, 'eval_samples_per_second': 139.385, 'eval_steps_per_second': 8.781, 'epoch': 0.68}
{'loss': 1.2145, 'grad_norm': 1.1164826154708862, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9040772914886475, 'eval_runtime': 7.1469, 'eval_samples_per_second': 139.921, 'eval_steps_per_second': 8.815, 'epoch': 0.72}
{'loss': 1.1707, 'grad_norm': 1.359758973121643, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.890178382396698, 'eval_runtime': 7.1411, 'eval_samples_per_second': 140.034, 'eval_steps_per_second': 8.822, 'epoch': 0.76}
{'loss': 1.1418, 'grad_norm': 1.2557222843170166, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8942068815231323, 'eval_runtime': 7.1378, 'eval_samples_per_second': 140.099, 'eval_steps_per_second': 8.826, 'epoch': 0.8}
{'loss': 1.1139, 'grad_norm': 0.9903282523155212, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8943190574645996, 'eval_runtime': 7.1277, 'eval_samples_per_second': 140.298, 'eval_steps_per_second': 8.839, 'epoch': 0.84}
{'loss': 1.1199, 'grad_norm': 1.253714680671692, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8947413563728333, 'eval_runtime': 7.1286, 'eval_samples_per_second': 140.281, 'eval_steps_per_second': 8.838, 'epoch': 0.88}
{'loss': 1.1104, 'grad_norm': 1.4443247318267822, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8990383148193359, 'eval_runtime': 7.1343, 'eval_samples_per_second': 140.168, 'eval_steps_per_second': 8.831, 'epoch': 0.92}
{'loss': 1.1105, 'grad_norm': 1.0545603036880493, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8967809081077576, 'eval_runtime': 7.1314, 'eval_samples_per_second': 140.225, 'eval_steps_per_second': 8.834, 'epoch': 0.96}
{'loss': 1.1231, 'grad_norm': 1.456489086151123, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8950235843658447, 'eval_runtime': 7.15, 'eval_samples_per_second': 139.861, 'eval_steps_per_second': 8.811, 'epoch': 1.0}
{'train_runtime': 383.9624, 'train_samples_per_second': 26.029, 'train_steps_per_second': 1.628, 'train_loss': 1.2851120635986328, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6427148580551147, 1.1069847345352173, 1.0135471820831299, 0.940025269985199, 0.922090470790863, 0.9124214053153992, 0.9094818830490112, 0.9048800468444824, 0.9061591029167175, 0.8977591395378113, 0.8993566632270813, 0.8960266709327698, 0.8953800201416016, 0.8920360803604126, 0.8997157216072083, 0.8948692679405212, 0.8980345726013184, 0.9040772914886475, 0.890178382396698, 0.8942068815231323, 0.8943190574645996, 0.8947413563728333, 0.8990383148193359, 0.8967809081077576, 0.8950235843658447], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6427148580551147, 1.1069847345352173, 1.0135471820831299, 0.940025269985199, 0.922090470790863, 0.9124214053153992, 0.9094818830490112, 0.9048800468444824, 0.9061591029167175, 0.8977591395378113, 0.8993566632270813, 0.8960266709327698, 0.8953800201416016, 0.8920360803604126, 0.8997157216072083, 0.8948692679405212, 0.8980345726013184, 0.9040772914886475, 0.890178382396698, 0.8942068815231323, 0.8943190574645996, 0.8947413563728333, 0.8990383148193359, 0.8967809081077576, 0.8950235843658447]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760316848754883
current iteration best possible eval_loss (full train run):  -0.8950235843658447
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.2411 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -1.0644281881484297
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -1.0617499657347127
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -1.0648561795660745
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -1.0694828946940993
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -1.0647586305788934
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1013, dtype=torch.float64), 0, tensor(0.0590, dtype=torch.float64), tensor(0.1927, dtype=torch.float64), 0, tensor(0.1596, dtype=torch.float64), tensor(0.0534, dtype=torch.float64), tensor(0.3067, dtype=torch.float64), tensor(0.1272, dtype=torch.float64), 32, 0, 0, 0, 0, 1, 14, 0.06210926807233158, 16.707467952982384, 1]
normalized proposed parameters for next round by BO: [tensor(0.1013, dtype=torch.float64), tensor(5.6249e-18, dtype=torch.float64), tensor(0.0590, dtype=torch.float64), tensor(0.1927, dtype=torch.float64), tensor(3.4927e-18, dtype=torch.float64), tensor(0.1596, dtype=torch.float64), tensor(0.0534, dtype=torch.float64), tensor(0.3067, dtype=torch.float64), tensor(0.1272, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1060, dtype=torch.float64), tensor(0.6211, dtype=torch.float64), tensor(0.3481, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.101
  gsm8k: 0
  rowan_hellaswag: 0.059
  sciq: 0.193
  triviaqa: 0
  truthfulqa_gen: 0.16
  wikitext: 0.053
  mmlu: 0.307
  arc_challenge: 0.127

LoRA Parameters:
  lora_r: (14,)
  lora_dropout: (0.06210926807233158,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (16.707467952982384,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  14
lora dropout:  0.06210926807233158
lora alpha:  16.707467952982384
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 8,257,536 || all params: 8,038,518,784 || trainable%: 0.1027
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5674, 'grad_norm': 0.6422452926635742, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1730880737304688, 'eval_runtime': 6.6822, 'eval_samples_per_second': 149.65, 'eval_steps_per_second': 9.428, 'epoch': 0.04}
{'loss': 1.9291, 'grad_norm': 0.5794038772583008, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2367581129074097, 'eval_runtime': 6.709, 'eval_samples_per_second': 149.053, 'eval_steps_per_second': 9.39, 'epoch': 0.08}
{'loss': 1.4944, 'grad_norm': 0.4022986888885498, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1190968751907349, 'eval_runtime': 6.7576, 'eval_samples_per_second': 147.98, 'eval_steps_per_second': 9.323, 'epoch': 0.12}
{'loss': 1.3791, 'grad_norm': 0.42170941829681396, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.046857476234436, 'eval_runtime': 6.7465, 'eval_samples_per_second': 148.226, 'eval_steps_per_second': 9.338, 'epoch': 0.16}
{'loss': 1.3311, 'grad_norm': 0.37220466136932373, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9659814238548279, 'eval_runtime': 6.7331, 'eval_samples_per_second': 148.521, 'eval_steps_per_second': 9.357, 'epoch': 0.2}
{'loss': 1.2415, 'grad_norm': 0.3431466817855835, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9312761425971985, 'eval_runtime': 6.7339, 'eval_samples_per_second': 148.503, 'eval_steps_per_second': 9.356, 'epoch': 0.24}
{'loss': 1.2269, 'grad_norm': 0.357562780380249, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9062662124633789, 'eval_runtime': 6.7416, 'eval_samples_per_second': 148.332, 'eval_steps_per_second': 9.345, 'epoch': 0.28}
{'loss': 1.1409, 'grad_norm': 0.3460789918899536, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9030151963233948, 'eval_runtime': 6.7216, 'eval_samples_per_second': 148.774, 'eval_steps_per_second': 9.373, 'epoch': 0.32}
{'loss': 1.2133, 'grad_norm': 0.3388422131538391, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8977333307266235, 'eval_runtime': 6.7236, 'eval_samples_per_second': 148.731, 'eval_steps_per_second': 9.37, 'epoch': 0.36}
{'loss': 1.1444, 'grad_norm': 0.360409140586853, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8938583731651306, 'eval_runtime': 6.7279, 'eval_samples_per_second': 148.634, 'eval_steps_per_second': 9.364, 'epoch': 0.4}
{'loss': 1.1159, 'grad_norm': 0.3425752520561218, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8954001069068909, 'eval_runtime': 6.7212, 'eval_samples_per_second': 148.783, 'eval_steps_per_second': 9.373, 'epoch': 0.44}
{'loss': 1.1494, 'grad_norm': 0.3740246593952179, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8916665315628052, 'eval_runtime': 6.7186, 'eval_samples_per_second': 148.842, 'eval_steps_per_second': 9.377, 'epoch': 0.48}
{'loss': 1.1124, 'grad_norm': 0.4140332043170929, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8930484652519226, 'eval_runtime': 6.7208, 'eval_samples_per_second': 148.793, 'eval_steps_per_second': 9.374, 'epoch': 0.52}
{'loss': 1.1401, 'grad_norm': 0.482831746339798, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8891986012458801, 'eval_runtime': 6.7213, 'eval_samples_per_second': 148.78, 'eval_steps_per_second': 9.373, 'epoch': 0.56}
{'loss': 1.1875, 'grad_norm': 0.32948988676071167, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8884088397026062, 'eval_runtime': 6.7246, 'eval_samples_per_second': 148.708, 'eval_steps_per_second': 9.369, 'epoch': 0.6}
{'loss': 1.164, 'grad_norm': 0.42369595170021057, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8845644593238831, 'eval_runtime': 6.7277, 'eval_samples_per_second': 148.638, 'eval_steps_per_second': 9.364, 'epoch': 0.64}
{'loss': 1.1793, 'grad_norm': 0.32484880089759827, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8864598870277405, 'eval_runtime': 6.7204, 'eval_samples_per_second': 148.801, 'eval_steps_per_second': 9.374, 'epoch': 0.68}
{'loss': 1.187, 'grad_norm': 0.3406297564506531, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.885569155216217, 'eval_runtime': 6.731, 'eval_samples_per_second': 148.566, 'eval_steps_per_second': 9.36, 'epoch': 0.72}
{'loss': 1.1151, 'grad_norm': 0.3434699475765228, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8824067115783691, 'eval_runtime': 6.7218, 'eval_samples_per_second': 148.77, 'eval_steps_per_second': 9.373, 'epoch': 0.76}
{'loss': 1.1586, 'grad_norm': 0.35928836464881897, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8832874298095703, 'eval_runtime': 6.7163, 'eval_samples_per_second': 148.892, 'eval_steps_per_second': 9.38, 'epoch': 0.8}
{'loss': 1.1477, 'grad_norm': 0.4924432337284088, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8859270811080933, 'eval_runtime': 6.7227, 'eval_samples_per_second': 148.75, 'eval_steps_per_second': 9.371, 'epoch': 0.84}
{'loss': 1.1842, 'grad_norm': 0.38887181878089905, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8862206935882568, 'eval_runtime': 6.7473, 'eval_samples_per_second': 148.207, 'eval_steps_per_second': 9.337, 'epoch': 0.88}
{'loss': 1.1743, 'grad_norm': 0.43523356318473816, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8830704092979431, 'eval_runtime': 6.7685, 'eval_samples_per_second': 147.742, 'eval_steps_per_second': 9.308, 'epoch': 0.92}
{'loss': 1.1347, 'grad_norm': 0.38347938656806946, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8836039900779724, 'eval_runtime': 6.7536, 'eval_samples_per_second': 148.069, 'eval_steps_per_second': 9.328, 'epoch': 0.96}
{'loss': 1.1539, 'grad_norm': 0.4322410523891449, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8834145665168762, 'eval_runtime': 6.7794, 'eval_samples_per_second': 147.505, 'eval_steps_per_second': 9.293, 'epoch': 1.0}
{'train_runtime': 360.7252, 'train_samples_per_second': 27.711, 'train_steps_per_second': 1.733, 'train_loss': 1.3188917053222655, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1730880737304688, 1.2367581129074097, 1.1190968751907349, 1.046857476234436, 0.9659814238548279, 0.9312761425971985, 0.9062662124633789, 0.9030151963233948, 0.8977333307266235, 0.8938583731651306, 0.8954001069068909, 0.8916665315628052, 0.8930484652519226, 0.8891986012458801, 0.8884088397026062, 0.8845644593238831, 0.8864598870277405, 0.885569155216217, 0.8824067115783691, 0.8832874298095703, 0.8859270811080933, 0.8862206935882568, 0.8830704092979431, 0.8836039900779724, 0.8834145665168762], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1730880737304688, 1.2367581129074097, 1.1190968751907349, 1.046857476234436, 0.9659814238548279, 0.9312761425971985, 0.9062662124633789, 0.9030151963233948, 0.8977333307266235, 0.8938583731651306, 0.8954001069068909, 0.8916665315628052, 0.8930484652519226, 0.8891986012458801, 0.8884088397026062, 0.8845644593238831, 0.8864598870277405, 0.885569155216217, 0.8824067115783691, 0.8832874298095703, 0.8859270811080933, 0.8862206935882568, 0.8830704092979431, 0.8836039900779724, 0.8834145665168762]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760347843170166
current iteration best possible eval_loss (full train run):  -0.8834145665168762
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0766 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -0.9512721869466638
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -0.9512721869832224
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -0.951272186948408
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.951272186946643
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -0.9512721869466239
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.6216, dtype=torch.float64), 0, 0, tensor(0.3784, dtype=torch.float64), 0, 32, 0, 1, 1, 0, 1, 2, 0.09352039458758571, 39.967998831161495, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.9798e-17, dtype=torch.float64), tensor(1.0822e-16, dtype=torch.float64), tensor(0.6216, dtype=torch.float64), tensor(4.9885e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3784, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.9352, dtype=torch.float64), tensor(0.8327, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.622
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.378
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.09352039458758571,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (39.967998831161495,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  2
lora dropout:  0.09352039458758571
lora alpha:  39.967998831161495
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,686,976 || all params: 8,032,948,224 || trainable%: 0.0334
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8611, 'grad_norm': 3.560269594192505, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0453810691833496, 'eval_runtime': 7.3052, 'eval_samples_per_second': 136.889, 'eval_steps_per_second': 8.624, 'epoch': 0.04}
{'loss': 1.4269, 'grad_norm': 2.6971933841705322, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.0193793773651123, 'eval_runtime': 7.3193, 'eval_samples_per_second': 136.625, 'eval_steps_per_second': 8.607, 'epoch': 0.08}
{'loss': 1.2751, 'grad_norm': 3.464359998703003, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0060176849365234, 'eval_runtime': 7.3219, 'eval_samples_per_second': 136.577, 'eval_steps_per_second': 8.604, 'epoch': 0.12}
{'loss': 1.1773, 'grad_norm': 1.8663909435272217, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9207597970962524, 'eval_runtime': 7.3178, 'eval_samples_per_second': 136.652, 'eval_steps_per_second': 8.609, 'epoch': 0.16}
{'loss': 1.1545, 'grad_norm': 2.0495636463165283, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9557641744613647, 'eval_runtime': 7.3303, 'eval_samples_per_second': 136.42, 'eval_steps_per_second': 8.594, 'epoch': 0.2}
{'loss': 1.1559, 'grad_norm': 1.6860214471817017, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9141777753829956, 'eval_runtime': 7.318, 'eval_samples_per_second': 136.649, 'eval_steps_per_second': 8.609, 'epoch': 0.24}
{'loss': 1.1598, 'grad_norm': 1.6219052076339722, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9057475328445435, 'eval_runtime': 7.3234, 'eval_samples_per_second': 136.548, 'eval_steps_per_second': 8.603, 'epoch': 0.28}
{'loss': 1.1579, 'grad_norm': 2.1361806392669678, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.910003900527954, 'eval_runtime': 7.3222, 'eval_samples_per_second': 136.571, 'eval_steps_per_second': 8.604, 'epoch': 0.32}
{'loss': 1.1551, 'grad_norm': 2.3784937858581543, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9010827541351318, 'eval_runtime': 7.3257, 'eval_samples_per_second': 136.506, 'eval_steps_per_second': 8.6, 'epoch': 0.36}
{'loss': 1.1548, 'grad_norm': 1.6613953113555908, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9393967390060425, 'eval_runtime': 7.3651, 'eval_samples_per_second': 135.775, 'eval_steps_per_second': 8.554, 'epoch': 0.4}
{'loss': 1.1104, 'grad_norm': 1.5621978044509888, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9462387561798096, 'eval_runtime': 7.3666, 'eval_samples_per_second': 135.748, 'eval_steps_per_second': 8.552, 'epoch': 0.44}
{'loss': 1.1536, 'grad_norm': 1.687292456626892, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9376652240753174, 'eval_runtime': 7.3519, 'eval_samples_per_second': 136.02, 'eval_steps_per_second': 8.569, 'epoch': 0.48}
{'loss': 1.1329, 'grad_norm': 1.7683018445968628, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9594812393188477, 'eval_runtime': 7.3576, 'eval_samples_per_second': 135.914, 'eval_steps_per_second': 8.563, 'epoch': 0.52}
{'loss': 1.0821, 'grad_norm': 1.78403902053833, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9412341117858887, 'eval_runtime': 7.3421, 'eval_samples_per_second': 136.201, 'eval_steps_per_second': 8.581, 'epoch': 0.56}
{'loss': 1.1309, 'grad_norm': 1.5264153480529785, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9287885427474976, 'eval_runtime': 7.3377, 'eval_samples_per_second': 136.282, 'eval_steps_per_second': 8.586, 'epoch': 0.6}
{'loss': 1.1121, 'grad_norm': 2.0726757049560547, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9412083625793457, 'eval_runtime': 7.3269, 'eval_samples_per_second': 136.483, 'eval_steps_per_second': 8.598, 'epoch': 0.64}
{'loss': 1.125, 'grad_norm': 1.4158313274383545, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9295281171798706, 'eval_runtime': 7.4538, 'eval_samples_per_second': 134.16, 'eval_steps_per_second': 8.452, 'epoch': 0.68}
{'loss': 1.1215, 'grad_norm': 2.3249011039733887, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.933774709701538, 'eval_runtime': 7.3918, 'eval_samples_per_second': 135.285, 'eval_steps_per_second': 8.523, 'epoch': 0.72}
{'loss': 1.0899, 'grad_norm': 1.5430601835250854, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9215432405471802, 'eval_runtime': 7.3972, 'eval_samples_per_second': 135.186, 'eval_steps_per_second': 8.517, 'epoch': 0.76}
{'loss': 1.0418, 'grad_norm': 1.5241012573242188, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9253160953521729, 'eval_runtime': 7.4178, 'eval_samples_per_second': 134.811, 'eval_steps_per_second': 8.493, 'epoch': 0.8}
{'loss': 1.0687, 'grad_norm': 1.6898581981658936, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9319390058517456, 'eval_runtime': 7.4051, 'eval_samples_per_second': 135.041, 'eval_steps_per_second': 8.508, 'epoch': 0.84}
{'loss': 1.0808, 'grad_norm': 1.665284514427185, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9329721927642822, 'eval_runtime': 7.4228, 'eval_samples_per_second': 134.719, 'eval_steps_per_second': 8.487, 'epoch': 0.88}
{'loss': 1.0758, 'grad_norm': 1.4407234191894531, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9361565113067627, 'eval_runtime': 7.413, 'eval_samples_per_second': 134.899, 'eval_steps_per_second': 8.499, 'epoch': 0.92}
{'loss': 1.1301, 'grad_norm': 1.994770884513855, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9385805130004883, 'eval_runtime': 7.397, 'eval_samples_per_second': 135.189, 'eval_steps_per_second': 8.517, 'epoch': 0.96}
{'loss': 1.0727, 'grad_norm': 1.8033758401870728, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.937289834022522, 'eval_runtime': 7.3984, 'eval_samples_per_second': 135.165, 'eval_steps_per_second': 8.515, 'epoch': 1.0}
{'train_runtime': 386.6226, 'train_samples_per_second': 25.862, 'train_steps_per_second': 1.617, 'train_loss': 1.208267123413086, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0453810691833496, 2.0193793773651123, 2.0060176849365234, 1.9207597970962524, 1.9557641744613647, 1.9141777753829956, 1.9057475328445435, 1.910003900527954, 1.9010827541351318, 1.9393967390060425, 1.9462387561798096, 1.9376652240753174, 1.9594812393188477, 1.9412341117858887, 1.9287885427474976, 1.9412083625793457, 1.9295281171798706, 1.933774709701538, 1.9215432405471802, 1.9253160953521729, 1.9319390058517456, 1.9329721927642822, 1.9361565113067627, 1.9385805130004883, 1.937289834022522], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0453810691833496, 2.0193793773651123, 2.0060176849365234, 1.9207597970962524, 1.9557641744613647, 1.9141777753829956, 1.9057475328445435, 1.910003900527954, 1.9010827541351318, 1.9393967390060425, 1.9462387561798096, 1.9376652240753174, 1.9594812393188477, 1.9412341117858887, 1.9287885427474976, 1.9412083625793457, 1.9295281171798706, 1.933774709701538, 1.9215432405471802, 1.9253160953521729, 1.9319390058517456, 1.9329721927642822, 1.9361565113067627, 1.9385805130004883, 1.937289834022522]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760278701782227
current iteration best possible eval_loss (full train run):  -1.937289834022522
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0202 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -0.9943380362878941
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -0.9943380362795436
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -0.9943380362776656
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -0.9943380362792772
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -0.994338036279011
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.4833, dtype=torch.float64), 0, 0, tensor(0.5167, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 0, 1, 42, 0.0, 32.687050027960964, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.4326e-17, dtype=torch.float64), tensor(9.2848e-18, dtype=torch.float64), tensor(0.4833, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5167, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3296, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6810, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.483
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.517
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (42,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (32.687050027960964,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  42
lora dropout:  0.0
lora alpha:  32.687050027960964
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 67,436,544 || all params: 8,097,697,792 || trainable%: 0.8328
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2901, 'grad_norm': 1.078993558883667, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.436962842941284, 'eval_runtime': 7.6434, 'eval_samples_per_second': 130.832, 'eval_steps_per_second': 8.242, 'epoch': 0.04}
{'loss': 1.7701, 'grad_norm': 0.630509078502655, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.417731523513794, 'eval_runtime': 7.6372, 'eval_samples_per_second': 130.937, 'eval_steps_per_second': 8.249, 'epoch': 0.08}
{'loss': 1.7339, 'grad_norm': 0.5230944156646729, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.3816933631896973, 'eval_runtime': 7.6205, 'eval_samples_per_second': 131.225, 'eval_steps_per_second': 8.267, 'epoch': 0.12}
{'loss': 1.6823, 'grad_norm': 0.6762694120407104, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.339937925338745, 'eval_runtime': 7.5976, 'eval_samples_per_second': 131.621, 'eval_steps_per_second': 8.292, 'epoch': 0.16}
{'loss': 1.5521, 'grad_norm': 0.7067036628723145, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.2939095497131348, 'eval_runtime': 7.5935, 'eval_samples_per_second': 131.692, 'eval_steps_per_second': 8.297, 'epoch': 0.2}
{'loss': 1.5656, 'grad_norm': 0.5921355485916138, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.3141584396362305, 'eval_runtime': 7.6005, 'eval_samples_per_second': 131.571, 'eval_steps_per_second': 8.289, 'epoch': 0.24}
{'loss': 1.5629, 'grad_norm': 0.489930123090744, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.417052745819092, 'eval_runtime': 7.6073, 'eval_samples_per_second': 131.452, 'eval_steps_per_second': 8.281, 'epoch': 0.28}
{'loss': 1.4951, 'grad_norm': 0.5439581871032715, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.2707011699676514, 'eval_runtime': 7.6058, 'eval_samples_per_second': 131.478, 'eval_steps_per_second': 8.283, 'epoch': 0.32}
{'loss': 1.5049, 'grad_norm': 0.5666587948799133, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.2796976566314697, 'eval_runtime': 7.6084, 'eval_samples_per_second': 131.434, 'eval_steps_per_second': 8.28, 'epoch': 0.36}
{'loss': 1.5151, 'grad_norm': 0.5149495601654053, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.246046304702759, 'eval_runtime': 7.6152, 'eval_samples_per_second': 131.316, 'eval_steps_per_second': 8.273, 'epoch': 0.4}
{'loss': 1.4471, 'grad_norm': 0.4551422894001007, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.4000751972198486, 'eval_runtime': 7.6163, 'eval_samples_per_second': 131.298, 'eval_steps_per_second': 8.272, 'epoch': 0.44}
{'loss': 1.4945, 'grad_norm': 0.47960346937179565, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.3183255195617676, 'eval_runtime': 7.6569, 'eval_samples_per_second': 130.602, 'eval_steps_per_second': 8.228, 'epoch': 0.48}
{'loss': 1.3884, 'grad_norm': 0.5607141852378845, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.357325553894043, 'eval_runtime': 7.6468, 'eval_samples_per_second': 130.774, 'eval_steps_per_second': 8.239, 'epoch': 0.52}
{'loss': 1.371, 'grad_norm': 0.49138307571411133, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.408771276473999, 'eval_runtime': 7.6257, 'eval_samples_per_second': 131.136, 'eval_steps_per_second': 8.262, 'epoch': 0.56}
{'loss': 1.5322, 'grad_norm': 0.43159550428390503, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.35245418548584, 'eval_runtime': 7.6115, 'eval_samples_per_second': 131.38, 'eval_steps_per_second': 8.277, 'epoch': 0.6}
{'loss': 1.4168, 'grad_norm': 0.5477184057235718, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.310901165008545, 'eval_runtime': 7.6091, 'eval_samples_per_second': 131.422, 'eval_steps_per_second': 8.28, 'epoch': 0.64}
{'loss': 1.5314, 'grad_norm': 0.5523861646652222, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.324716329574585, 'eval_runtime': 7.6071, 'eval_samples_per_second': 131.456, 'eval_steps_per_second': 8.282, 'epoch': 0.68}
{'loss': 1.4013, 'grad_norm': 0.55121248960495, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.343153476715088, 'eval_runtime': 7.605, 'eval_samples_per_second': 131.492, 'eval_steps_per_second': 8.284, 'epoch': 0.72}
{'loss': 1.3417, 'grad_norm': 0.5930436253547668, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.3724160194396973, 'eval_runtime': 7.6934, 'eval_samples_per_second': 129.981, 'eval_steps_per_second': 8.189, 'epoch': 0.76}
{'loss': 1.4071, 'grad_norm': 0.5921927094459534, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.340385913848877, 'eval_runtime': 7.7185, 'eval_samples_per_second': 129.558, 'eval_steps_per_second': 8.162, 'epoch': 0.8}
{'loss': 1.4422, 'grad_norm': 0.5827844738960266, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.325404644012451, 'eval_runtime': 7.7518, 'eval_samples_per_second': 129.002, 'eval_steps_per_second': 8.127, 'epoch': 0.84}
{'loss': 1.3899, 'grad_norm': 0.5369573831558228, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.3519227504730225, 'eval_runtime': 7.7115, 'eval_samples_per_second': 129.677, 'eval_steps_per_second': 8.17, 'epoch': 0.88}
{'loss': 1.4124, 'grad_norm': 0.5327742695808411, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.341404676437378, 'eval_runtime': 7.7186, 'eval_samples_per_second': 129.557, 'eval_steps_per_second': 8.162, 'epoch': 0.92}
{'loss': 1.3341, 'grad_norm': 0.42784857749938965, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.3438003063201904, 'eval_runtime': 7.7028, 'eval_samples_per_second': 129.823, 'eval_steps_per_second': 8.179, 'epoch': 0.96}
{'loss': 1.4, 'grad_norm': 0.5366364121437073, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.3398358821868896, 'eval_runtime': 7.7048, 'eval_samples_per_second': 129.789, 'eval_steps_per_second': 8.177, 'epoch': 1.0}
{'train_runtime': 391.7691, 'train_samples_per_second': 25.523, 'train_steps_per_second': 1.595, 'train_loss': 1.5592825561523438, 'epoch': 1.0}
train_results:  {'eval_loss': [2.436962842941284, 2.417731523513794, 2.3816933631896973, 2.339937925338745, 2.2939095497131348, 2.3141584396362305, 2.417052745819092, 2.2707011699676514, 2.2796976566314697, 2.246046304702759, 2.4000751972198486, 2.3183255195617676, 2.357325553894043, 2.408771276473999, 2.35245418548584, 2.310901165008545, 2.324716329574585, 2.343153476715088, 2.3724160194396973, 2.340385913848877, 2.325404644012451, 2.3519227504730225, 2.341404676437378, 2.3438003063201904, 2.3398358821868896], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.436962842941284, 2.417731523513794, 2.3816933631896973, 2.339937925338745, 2.2939095497131348, 2.3141584396362305, 2.417052745819092, 2.2707011699676514, 2.2796976566314697, 2.246046304702759, 2.4000751972198486, 2.3183255195617676, 2.357325553894043, 2.408771276473999, 2.35245418548584, 2.310901165008545, 2.324716329574585, 2.343153476715088, 2.3724160194396973, 2.340385913848877, 2.325404644012451, 2.3519227504730225, 2.341404676437378, 2.3438003063201904, 2.3398358821868896]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.086806297302246
current iteration best possible eval_loss (full train run):  -2.3398358821868896
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2483 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -1.0277916441873676
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.9400285004883969
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -0.9920719016111177
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -1.031116698233649
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -1.0428764919341518
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.0780, dtype=torch.float64), 0, tensor(0.3016, dtype=torch.float64), 0, tensor(0.1651, dtype=torch.float64), tensor(0.4553, dtype=torch.float64), 28, 1, 0, 1, 1, 0, 48, 0.08090559198828912, 18.369354414440934, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(4.2660e-18, dtype=torch.float64), tensor(9.7412e-18, dtype=torch.float64), tensor(0.0780, dtype=torch.float64), tensor(1.1241e-19, dtype=torch.float64), tensor(0.3016, dtype=torch.float64), tensor(1.2573e-18, dtype=torch.float64), tensor(0.1651, dtype=torch.float64), tensor(0.4553, dtype=torch.float64), tensor(0.8771, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3770, dtype=torch.float64), tensor(0.8091, dtype=torch.float64), tensor(0.3827, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.078
  triviaqa: 0
  truthfulqa_gen: 0.302
  wikitext: 0
  mmlu: 0.165
  arc_challenge: 0.455

LoRA Parameters:
  lora_r: (48,)
  lora_dropout: (0.08090559198828912,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (18.369354414440934,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  48
lora dropout:  0.08090559198828912
lora alpha:  18.369354414440934
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 60,555,264 || all params: 8,090,816,512 || trainable%: 0.7484
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1144, 'grad_norm': 1.5869617462158203, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5053644180297852, 'eval_runtime': 7.2475, 'eval_samples_per_second': 137.978, 'eval_steps_per_second': 8.693, 'epoch': 0.04}
{'loss': 1.2225, 'grad_norm': 0.9123152494430542, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.018717646598816, 'eval_runtime': 7.2196, 'eval_samples_per_second': 138.512, 'eval_steps_per_second': 8.726, 'epoch': 0.08}
{'loss': 1.0565, 'grad_norm': 0.32978105545043945, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.952451765537262, 'eval_runtime': 7.2298, 'eval_samples_per_second': 138.317, 'eval_steps_per_second': 8.714, 'epoch': 0.12}
{'loss': 1.0044, 'grad_norm': 0.3340863287448883, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.922394871711731, 'eval_runtime': 7.2164, 'eval_samples_per_second': 138.574, 'eval_steps_per_second': 8.73, 'epoch': 0.16}
{'loss': 0.9678, 'grad_norm': 0.3670547604560852, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9066147208213806, 'eval_runtime': 7.2316, 'eval_samples_per_second': 138.282, 'eval_steps_per_second': 8.712, 'epoch': 0.2}
{'loss': 0.9021, 'grad_norm': 0.3638657331466675, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8869894742965698, 'eval_runtime': 7.221, 'eval_samples_per_second': 138.485, 'eval_steps_per_second': 8.725, 'epoch': 0.24}
{'loss': 0.8957, 'grad_norm': 0.26223281025886536, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8859926462173462, 'eval_runtime': 7.2293, 'eval_samples_per_second': 138.326, 'eval_steps_per_second': 8.715, 'epoch': 0.28}
{'loss': 0.8876, 'grad_norm': 0.2908957898616791, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8968760967254639, 'eval_runtime': 7.2354, 'eval_samples_per_second': 138.21, 'eval_steps_per_second': 8.707, 'epoch': 0.32}
{'loss': 0.8398, 'grad_norm': 0.3148142993450165, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8962709307670593, 'eval_runtime': 7.248, 'eval_samples_per_second': 137.97, 'eval_steps_per_second': 8.692, 'epoch': 0.36}
{'loss': 0.7695, 'grad_norm': 0.34765538573265076, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9077326059341431, 'eval_runtime': 7.2796, 'eval_samples_per_second': 137.37, 'eval_steps_per_second': 8.654, 'epoch': 0.4}
{'loss': 0.8027, 'grad_norm': 0.4220881462097168, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9167383909225464, 'eval_runtime': 7.2652, 'eval_samples_per_second': 137.643, 'eval_steps_per_second': 8.671, 'epoch': 0.44}
{'loss': 0.7989, 'grad_norm': 0.3988088071346283, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9446181654930115, 'eval_runtime': 7.2568, 'eval_samples_per_second': 137.801, 'eval_steps_per_second': 8.681, 'epoch': 0.48}
{'loss': 0.8081, 'grad_norm': 0.3430623710155487, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9282520413398743, 'eval_runtime': 7.2461, 'eval_samples_per_second': 138.004, 'eval_steps_per_second': 8.694, 'epoch': 0.52}
{'loss': 0.7592, 'grad_norm': 0.34747377038002014, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9712179899215698, 'eval_runtime': 7.2426, 'eval_samples_per_second': 138.073, 'eval_steps_per_second': 8.699, 'epoch': 0.56}
{'loss': 0.7546, 'grad_norm': 0.3550388514995575, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9280747175216675, 'eval_runtime': 7.3222, 'eval_samples_per_second': 136.571, 'eval_steps_per_second': 8.604, 'epoch': 0.6}
{'loss': 0.7196, 'grad_norm': 0.37723687291145325, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9741800427436829, 'eval_runtime': 7.3539, 'eval_samples_per_second': 135.982, 'eval_steps_per_second': 8.567, 'epoch': 0.64}
{'loss': 0.6996, 'grad_norm': 0.38078707456588745, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9631677269935608, 'eval_runtime': 7.3587, 'eval_samples_per_second': 135.894, 'eval_steps_per_second': 8.561, 'epoch': 0.68}
{'loss': 0.6665, 'grad_norm': 0.4166632294654846, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.972753643989563, 'eval_runtime': 7.3281, 'eval_samples_per_second': 136.461, 'eval_steps_per_second': 8.597, 'epoch': 0.72}
{'loss': 0.6966, 'grad_norm': 0.6419196128845215, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0089149475097656, 'eval_runtime': 7.3222, 'eval_samples_per_second': 136.571, 'eval_steps_per_second': 8.604, 'epoch': 0.76}
{'loss': 0.7124, 'grad_norm': 0.4265381395816803, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0094140768051147, 'eval_runtime': 7.3014, 'eval_samples_per_second': 136.959, 'eval_steps_per_second': 8.628, 'epoch': 0.8}
{'loss': 0.6685, 'grad_norm': 0.5034215450286865, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9959232211112976, 'eval_runtime': 7.2467, 'eval_samples_per_second': 137.994, 'eval_steps_per_second': 8.694, 'epoch': 0.84}
{'loss': 0.634, 'grad_norm': 0.5068097114562988, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0004595518112183, 'eval_runtime': 7.2778, 'eval_samples_per_second': 137.404, 'eval_steps_per_second': 8.656, 'epoch': 0.88}
{'loss': 0.5897, 'grad_norm': 0.5326311588287354, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0080666542053223, 'eval_runtime': 7.2857, 'eval_samples_per_second': 137.256, 'eval_steps_per_second': 8.647, 'epoch': 0.92}
{'loss': 0.6264, 'grad_norm': 0.465025395154953, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0189614295959473, 'eval_runtime': 7.2805, 'eval_samples_per_second': 137.353, 'eval_steps_per_second': 8.653, 'epoch': 0.96}
{'loss': 0.5677, 'grad_norm': 0.6126561164855957, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0056358575820923, 'eval_runtime': 7.2636, 'eval_samples_per_second': 137.673, 'eval_steps_per_second': 8.673, 'epoch': 1.0}
{'train_runtime': 368.6011, 'train_samples_per_second': 27.124, 'train_steps_per_second': 1.696, 'train_loss': 0.8865884674072265, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5053644180297852, 1.018717646598816, 0.952451765537262, 0.922394871711731, 0.9066147208213806, 0.8869894742965698, 0.8859926462173462, 0.8968760967254639, 0.8962709307670593, 0.9077326059341431, 0.9167383909225464, 0.9446181654930115, 0.9282520413398743, 0.9712179899215698, 0.9280747175216675, 0.9741800427436829, 0.9631677269935608, 0.972753643989563, 1.0089149475097656, 1.0094140768051147, 0.9959232211112976, 1.0004595518112183, 1.0080666542053223, 1.0189614295959473, 1.0056358575820923], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5053644180297852, 1.018717646598816, 0.952451765537262, 0.922394871711731, 0.9066147208213806, 0.8869894742965698, 0.8859926462173462, 0.8968760967254639, 0.8962709307670593, 0.9077326059341431, 0.9167383909225464, 0.9446181654930115, 0.9282520413398743, 0.9712179899215698, 0.9280747175216675, 0.9741800427436829, 0.9631677269935608, 0.972753643989563, 1.0089149475097656, 1.0094140768051147, 0.9959232211112976, 1.0004595518112183, 1.0080666542053223, 1.0189614295959473, 1.0056358575820923]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1429637670516968
current iteration best possible eval_loss (full train run):  -1.0056358575820923
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9139 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.9709141758400304
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.9876339030232223
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -1.038390150380293
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -1.0281257795926213
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -1.0390574992346893
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.6155, dtype=torch.float64), tensor(0.3793, dtype=torch.float64), 0, 0, 0, 0, 31, 1, 0, 1, 0, 0, 53, 0.1, 18.309192280807366, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(2.9927e-17, dtype=torch.float64), tensor(4.6957e-19, dtype=torch.float64), tensor(0.6155, dtype=torch.float64), tensor(0.3793, dtype=torch.float64), tensor(1.5588e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0052, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9719, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4160, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3814, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.615
  triviaqa: 0.379
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (53,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (18.309192280807366,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  53
lora dropout:  0.1
lora alpha:  18.309192280807366
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 43,743,232 || all params: 8,074,004,480 || trainable%: 0.5418
length of training data:  9946
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.1396, 'grad_norm': 0.9412757158279419, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.434983253479004, 'eval_runtime': 7.5418, 'eval_samples_per_second': 132.595, 'eval_steps_per_second': 8.353, 'epoch': 0.04}
{'loss': 1.5682, 'grad_norm': 0.3712747395038605, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.2394468784332275, 'eval_runtime': 7.0834, 'eval_samples_per_second': 141.174, 'eval_steps_per_second': 8.894, 'epoch': 0.08}
{'loss': 1.2587, 'grad_norm': 0.28099575638771057, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 2.2316854000091553, 'eval_runtime': 7.0623, 'eval_samples_per_second': 141.596, 'eval_steps_per_second': 8.921, 'epoch': 0.12}
{'loss': 1.1761, 'grad_norm': 0.29852184653282166, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 2.2007274627685547, 'eval_runtime': 7.0484, 'eval_samples_per_second': 141.877, 'eval_steps_per_second': 8.938, 'epoch': 0.16}
{'loss': 1.0497, 'grad_norm': 0.2631332278251648, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 2.1767327785491943, 'eval_runtime': 7.0453, 'eval_samples_per_second': 141.939, 'eval_steps_per_second': 8.942, 'epoch': 0.2}
{'loss': 0.9953, 'grad_norm': 0.1929880976676941, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 2.1621146202087402, 'eval_runtime': 7.0485, 'eval_samples_per_second': 141.875, 'eval_steps_per_second': 8.938, 'epoch': 0.24}
{'loss': 0.9922, 'grad_norm': 0.21354062855243683, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 2.173351526260376, 'eval_runtime': 7.0692, 'eval_samples_per_second': 141.458, 'eval_steps_per_second': 8.912, 'epoch': 0.28}
{'loss': 0.9694, 'grad_norm': 0.20924749970436096, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 2.1766507625579834, 'eval_runtime': 7.0785, 'eval_samples_per_second': 141.274, 'eval_steps_per_second': 8.9, 'epoch': 0.32}
{'loss': 0.9431, 'grad_norm': 0.22753477096557617, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 2.203108310699463, 'eval_runtime': 7.088, 'eval_samples_per_second': 141.083, 'eval_steps_per_second': 8.888, 'epoch': 0.36}
{'loss': 0.9324, 'grad_norm': 0.2310612052679062, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 2.183253526687622, 'eval_runtime': 7.0854, 'eval_samples_per_second': 141.136, 'eval_steps_per_second': 8.892, 'epoch': 0.4}
{'loss': 0.8979, 'grad_norm': 0.23578840494155884, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 2.181886911392212, 'eval_runtime': 7.0967, 'eval_samples_per_second': 140.91, 'eval_steps_per_second': 8.877, 'epoch': 0.44}
{'loss': 0.85, 'grad_norm': 0.27152568101882935, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 2.177615165710449, 'eval_runtime': 7.0811, 'eval_samples_per_second': 141.221, 'eval_steps_per_second': 8.897, 'epoch': 0.48}
{'loss': 0.843, 'grad_norm': 0.2985316216945648, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 2.155869722366333, 'eval_runtime': 7.0758, 'eval_samples_per_second': 141.326, 'eval_steps_per_second': 8.904, 'epoch': 0.52}
{'loss': 0.8179, 'grad_norm': 0.23769548535346985, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 2.137244701385498, 'eval_runtime': 7.09, 'eval_samples_per_second': 141.045, 'eval_steps_per_second': 8.886, 'epoch': 0.56}
{'loss': 0.8168, 'grad_norm': 0.22499626874923706, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 2.1584792137145996, 'eval_runtime': 7.0914, 'eval_samples_per_second': 141.015, 'eval_steps_per_second': 8.884, 'epoch': 0.6}
{'loss': 0.7832, 'grad_norm': 0.19766266644001007, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 2.1600630283355713, 'eval_runtime': 7.114, 'eval_samples_per_second': 140.568, 'eval_steps_per_second': 8.856, 'epoch': 0.64}
{'loss': 0.8157, 'grad_norm': 0.23358727991580963, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 2.1910250186920166, 'eval_runtime': 7.1051, 'eval_samples_per_second': 140.743, 'eval_steps_per_second': 8.867, 'epoch': 0.68}
{'loss': 0.7928, 'grad_norm': 0.25529080629348755, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 2.205845832824707, 'eval_runtime': 7.1319, 'eval_samples_per_second': 140.214, 'eval_steps_per_second': 8.833, 'epoch': 0.72}
{'loss': 0.7832, 'grad_norm': 0.22353166341781616, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 2.2408101558685303, 'eval_runtime': 7.1151, 'eval_samples_per_second': 140.547, 'eval_steps_per_second': 8.854, 'epoch': 0.76}
{'loss': 0.7993, 'grad_norm': 0.2379334419965744, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 2.233485698699951, 'eval_runtime': 7.1086, 'eval_samples_per_second': 140.675, 'eval_steps_per_second': 8.862, 'epoch': 0.8}
{'loss': 0.8017, 'grad_norm': 0.2512113153934479, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 2.231401205062866, 'eval_runtime': 7.0686, 'eval_samples_per_second': 141.47, 'eval_steps_per_second': 8.913, 'epoch': 0.84}
{'loss': 0.7861, 'grad_norm': 0.2395060956478119, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 2.2287559509277344, 'eval_runtime': 7.0702, 'eval_samples_per_second': 141.439, 'eval_steps_per_second': 8.911, 'epoch': 0.88}
{'loss': 0.8025, 'grad_norm': 0.28631219267845154, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 2.2260892391204834, 'eval_runtime': 7.0737, 'eval_samples_per_second': 141.369, 'eval_steps_per_second': 8.906, 'epoch': 0.92}
{'loss': 0.8041, 'grad_norm': 0.23937545716762543, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 2.23702073097229, 'eval_runtime': 7.0761, 'eval_samples_per_second': 141.32, 'eval_steps_per_second': 8.903, 'epoch': 0.96}
{'train_runtime': 247.3463, 'train_samples_per_second': 40.211, 'train_steps_per_second': 2.515, 'train_loss': 1.0499635218040737, 'epoch': 1.0}
train_results:  {'eval_loss': [2.434983253479004, 2.2394468784332275, 2.2316854000091553, 2.2007274627685547, 2.1767327785491943, 2.1621146202087402, 2.173351526260376, 2.1766507625579834, 2.203108310699463, 2.183253526687622, 2.181886911392212, 2.177615165710449, 2.155869722366333, 2.137244701385498, 2.1584792137145996, 2.1600630283355713, 2.1910250186920166, 2.205845832824707, 2.2408101558685303, 2.233485698699951, 2.231401205062866, 2.2287559509277344, 2.2260892391204834, 2.23702073097229], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.434983253479004, 2.2394468784332275, 2.2316854000091553, 2.2007274627685547, 2.1767327785491943, 2.1621146202087402, 2.173351526260376, 2.1766507625579834, 2.203108310699463, 2.183253526687622, 2.181886911392212, 2.177615165710449, 2.155869722366333, 2.137244701385498, 2.1584792137145996, 2.1600630283355713, 2.1910250186920166, 2.205845832824707, 2.2408101558685303, 2.233485698699951, 2.231401205062866, 2.2287559509277344, 2.2260892391204834, 2.23702073097229]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1943060159683228
current iteration best possible eval_loss (full train run):  -2.23702073097229
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6317 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7657198309898376, 0.9410857558250427, 0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.32434797286987305, 0.1894705891609192, 0.7398009896278381, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.25163692235946655, 0.6103376746177673, 0.06646472215652466, 0.1532604694366455]  ‚Üí  acq = -0.976458443078886
X = [0.9276310801506042, 0.01835566759109497, 0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.4864007830619812, 0.852687656879425, 0.4021565914154053, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7941768169403076, 0.03410828113555908, 0.9558417797088623, 0.6570152640342712]  ‚Üí  acq = -0.976458443078886
X = [0.3898862600326538, 0.1672157645225525, 0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8220118284225464, 0.09181463718414307, 0.7922564744949341, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8501660823822021, 0.8251820206642151, 0.8375023603439331, 0.5925764441490173]  ‚Üí  acq = -0.976458443078886
X = [0.7604454159736633, 0.9365105628967285, 0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.14734964072704315, 0.7609574198722839, 0.3722277879714966, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9713605642318726, 0.19831812381744385, 0.38370370864868164, 0.6161256432533264]  ‚Üí  acq = -0.976458443078886
X = [0.0232393741607666, 0.5331325531005859, 0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8101420402526855, 0.707656979560852, 0.6279451251029968, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.42141979932785034, 0.38733386993408203, 0.770684003829956, 0.287418007850647]  ‚Üí  acq = -0.976458443078886
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2079, dtype=torch.float64), 0, 0, 0, tensor(0.6271, dtype=torch.float64), 0, tensor(0.1650, dtype=torch.float64), 0, 1, 1, 0, 1, 1, 0, 2, 4.380176776841437e-18, 35.0701807902397, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2079, dtype=torch.float64), tensor(3.5361e-17, dtype=torch.float64), tensor(1.9675e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6271, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1650, dtype=torch.float64), tensor(2.2965e-16, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(4.3802e-17, dtype=torch.float64), tensor(0.7306, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.208
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.627
  wikitext: 0
  mmlu: 0.165
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (4.380176776841437e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (35.0701807902397,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  2
lora dropout:  4.380176776841437e-18
lora alpha:  35.0701807902397
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 90,112 || all params: 8,030,351,360 || trainable%: 0.0011
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.717, 'grad_norm': 7.845932483673096, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.2589776515960693, 'eval_runtime': 6.2848, 'eval_samples_per_second': 159.114, 'eval_steps_per_second': 10.024, 'epoch': 0.04}
{'loss': 2.5226, 'grad_norm': 14.935798645019531, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.3358867168426514, 'eval_runtime': 6.2666, 'eval_samples_per_second': 159.577, 'eval_steps_per_second': 10.053, 'epoch': 0.08}
{'loss': 1.7418, 'grad_norm': 4.2860002517700195, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1501238346099854, 'eval_runtime': 6.2763, 'eval_samples_per_second': 159.331, 'eval_steps_per_second': 10.038, 'epoch': 0.12}
{'loss': 1.5229, 'grad_norm': 4.78019380569458, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.026043176651001, 'eval_runtime': 6.3015, 'eval_samples_per_second': 158.693, 'eval_steps_per_second': 9.998, 'epoch': 0.16}
{'loss': 1.4349, 'grad_norm': 16.923913955688477, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.008913993835449, 'eval_runtime': 6.3137, 'eval_samples_per_second': 158.386, 'eval_steps_per_second': 9.978, 'epoch': 0.2}
{'loss': 1.4327, 'grad_norm': 2.9984982013702393, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9750207662582397, 'eval_runtime': 6.3145, 'eval_samples_per_second': 158.365, 'eval_steps_per_second': 9.977, 'epoch': 0.24}
{'loss': 1.3998, 'grad_norm': 3.164024829864502, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.996078610420227, 'eval_runtime': 6.3204, 'eval_samples_per_second': 158.219, 'eval_steps_per_second': 9.968, 'epoch': 0.28}
{'loss': 1.2958, 'grad_norm': 4.7898101806640625, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9216797351837158, 'eval_runtime': 6.3165, 'eval_samples_per_second': 158.316, 'eval_steps_per_second': 9.974, 'epoch': 0.32}
{'loss': 1.2308, 'grad_norm': 3.482750415802002, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.97822904586792, 'eval_runtime': 6.3357, 'eval_samples_per_second': 157.837, 'eval_steps_per_second': 9.944, 'epoch': 0.36}
{'loss': 1.2042, 'grad_norm': 2.7999119758605957, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9017398357391357, 'eval_runtime': 6.3411, 'eval_samples_per_second': 157.701, 'eval_steps_per_second': 9.935, 'epoch': 0.4}
{'loss': 1.1849, 'grad_norm': 3.4082067012786865, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9243170022964478, 'eval_runtime': 6.3123, 'eval_samples_per_second': 158.422, 'eval_steps_per_second': 9.981, 'epoch': 0.44}
{'loss': 1.1258, 'grad_norm': 6.2230305671691895, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9468079805374146, 'eval_runtime': 6.3148, 'eval_samples_per_second': 158.359, 'eval_steps_per_second': 9.977, 'epoch': 0.48}
{'loss': 1.1102, 'grad_norm': 3.9965384006500244, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9014639854431152, 'eval_runtime': 6.3123, 'eval_samples_per_second': 158.42, 'eval_steps_per_second': 9.98, 'epoch': 0.52}
{'loss': 1.0854, 'grad_norm': 4.127995491027832, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8884466886520386, 'eval_runtime': 6.3262, 'eval_samples_per_second': 158.073, 'eval_steps_per_second': 9.959, 'epoch': 0.56}
{'loss': 1.0968, 'grad_norm': 4.03494119644165, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9046025276184082, 'eval_runtime': 6.3225, 'eval_samples_per_second': 158.166, 'eval_steps_per_second': 9.964, 'epoch': 0.6}
{'loss': 1.1071, 'grad_norm': 2.636997699737549, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8937232494354248, 'eval_runtime': 6.3131, 'eval_samples_per_second': 158.402, 'eval_steps_per_second': 9.979, 'epoch': 0.64}
{'loss': 1.0477, 'grad_norm': 2.57580304145813, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9366620779037476, 'eval_runtime': 6.3278, 'eval_samples_per_second': 158.033, 'eval_steps_per_second': 9.956, 'epoch': 0.68}
{'loss': 1.0403, 'grad_norm': 3.138432264328003, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8883110284805298, 'eval_runtime': 6.3301, 'eval_samples_per_second': 157.975, 'eval_steps_per_second': 9.952, 'epoch': 0.72}
{'loss': 1.0953, 'grad_norm': 2.3964109420776367, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8554531335830688, 'eval_runtime': 6.3165, 'eval_samples_per_second': 158.316, 'eval_steps_per_second': 9.974, 'epoch': 0.76}
{'loss': 1.0585, 'grad_norm': 2.741137981414795, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8447102308273315, 'eval_runtime': 6.3366, 'eval_samples_per_second': 157.813, 'eval_steps_per_second': 9.942, 'epoch': 0.8}
{'loss': 1.0532, 'grad_norm': 2.648630142211914, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.852226972579956, 'eval_runtime': 6.3589, 'eval_samples_per_second': 157.26, 'eval_steps_per_second': 9.907, 'epoch': 0.84}
{'loss': 1.0861, 'grad_norm': 2.7026526927948, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8408267498016357, 'eval_runtime': 6.3763, 'eval_samples_per_second': 156.83, 'eval_steps_per_second': 9.88, 'epoch': 0.88}
{'loss': 1.0567, 'grad_norm': 2.730275869369507, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.844799280166626, 'eval_runtime': 6.3779, 'eval_samples_per_second': 156.791, 'eval_steps_per_second': 9.878, 'epoch': 0.92}
{'loss': 1.0387, 'grad_norm': 3.0793116092681885, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8448283672332764, 'eval_runtime': 6.3721, 'eval_samples_per_second': 156.933, 'eval_steps_per_second': 9.887, 'epoch': 0.96}
{'loss': 1.0708, 'grad_norm': 3.271358013153076, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8517695665359497, 'eval_runtime': 6.3822, 'eval_samples_per_second': 156.685, 'eval_steps_per_second': 9.871, 'epoch': 1.0}
{'train_runtime': 339.9577, 'train_samples_per_second': 29.412, 'train_steps_per_second': 1.838, 'train_loss': 1.350403158569336, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2589776515960693, 2.3358867168426514, 2.1501238346099854, 2.026043176651001, 2.008913993835449, 1.9750207662582397, 1.996078610420227, 1.9216797351837158, 1.97822904586792, 1.9017398357391357, 1.9243170022964478, 1.9468079805374146, 1.9014639854431152, 1.8884466886520386, 1.9046025276184082, 1.8937232494354248, 1.9366620779037476, 1.8883110284805298, 1.8554531335830688, 1.8447102308273315, 1.852226972579956, 1.8408267498016357, 1.844799280166626, 1.8448283672332764, 1.8517695665359497], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.2589776515960693, 2.3358867168426514, 2.1501238346099854, 2.026043176651001, 2.008913993835449, 1.9750207662582397, 1.996078610420227, 1.9216797351837158, 1.97822904586792, 1.9017398357391357, 1.9243170022964478, 1.9468079805374146, 1.9014639854431152, 1.8884466886520386, 1.9046025276184082, 1.8937232494354248, 1.9366620779037476, 1.8883110284805298, 1.8554531335830688, 1.8447102308273315, 1.852226972579956, 1.8408267498016357, 1.844799280166626, 1.8448283672332764, 1.8517695665359497]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0979005098342896
current iteration best possible eval_loss (full train run):  -1.8517695665359497
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.8623 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -0.9854816594261282
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -0.9854816594261282
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -0.9854816594261282
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -0.9854816594261282
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.9854816594261282
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.8105, dtype=torch.float64), 0, 0, 0, 0, tensor(0.1895, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 0, 2, 0.1, 32.56938978652289, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8105, dtype=torch.float64), tensor(9.7118e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.8569e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1895, dtype=torch.float64), tensor(1.9632e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6785, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.811
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.189
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (32.56938978652289,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  32.56938978652289
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,883,584 || all params: 8,033,144,832 || trainable%: 0.0359
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1998, 'grad_norm': 3.2319111824035645, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0030672550201416, 'eval_runtime': 7.541, 'eval_samples_per_second': 132.609, 'eval_steps_per_second': 8.354, 'epoch': 0.04}
{'loss': 1.8593, 'grad_norm': 1.5683324337005615, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7845568656921387, 'eval_runtime': 7.54, 'eval_samples_per_second': 132.627, 'eval_steps_per_second': 8.355, 'epoch': 0.08}
{'loss': 1.7173, 'grad_norm': 1.1080617904663086, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.82706880569458, 'eval_runtime': 7.5106, 'eval_samples_per_second': 133.146, 'eval_steps_per_second': 8.388, 'epoch': 0.12}
{'loss': 1.695, 'grad_norm': 1.5312907695770264, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.834893822669983, 'eval_runtime': 7.508, 'eval_samples_per_second': 133.192, 'eval_steps_per_second': 8.391, 'epoch': 0.16}
{'loss': 1.6841, 'grad_norm': 0.8657885193824768, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8316401243209839, 'eval_runtime': 7.5176, 'eval_samples_per_second': 133.021, 'eval_steps_per_second': 8.38, 'epoch': 0.2}
{'loss': 1.6563, 'grad_norm': 0.9761188626289368, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.815995693206787, 'eval_runtime': 7.49, 'eval_samples_per_second': 133.511, 'eval_steps_per_second': 8.411, 'epoch': 0.24}
{'loss': 1.6413, 'grad_norm': 0.9958743453025818, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8535507917404175, 'eval_runtime': 7.51, 'eval_samples_per_second': 133.156, 'eval_steps_per_second': 8.389, 'epoch': 0.28}
{'loss': 1.6487, 'grad_norm': 0.9515259861946106, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.82460355758667, 'eval_runtime': 7.5008, 'eval_samples_per_second': 133.318, 'eval_steps_per_second': 8.399, 'epoch': 0.32}
{'loss': 1.6697, 'grad_norm': 1.0727845430374146, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.822948932647705, 'eval_runtime': 7.5015, 'eval_samples_per_second': 133.306, 'eval_steps_per_second': 8.398, 'epoch': 0.36}
{'loss': 1.6503, 'grad_norm': 0.7925446033477783, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8287041187286377, 'eval_runtime': 7.4891, 'eval_samples_per_second': 133.527, 'eval_steps_per_second': 8.412, 'epoch': 0.4}
{'loss': 1.6741, 'grad_norm': 0.9561614394187927, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8702467679977417, 'eval_runtime': 7.4951, 'eval_samples_per_second': 133.42, 'eval_steps_per_second': 8.405, 'epoch': 0.44}
{'loss': 1.6783, 'grad_norm': 0.8994086980819702, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8498800992965698, 'eval_runtime': 7.5013, 'eval_samples_per_second': 133.309, 'eval_steps_per_second': 8.398, 'epoch': 0.48}
{'loss': 1.6687, 'grad_norm': 1.2333136796951294, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8411363363265991, 'eval_runtime': 7.5034, 'eval_samples_per_second': 133.273, 'eval_steps_per_second': 8.396, 'epoch': 0.52}
{'loss': 1.6289, 'grad_norm': 0.8523735404014587, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8340004682540894, 'eval_runtime': 7.5019, 'eval_samples_per_second': 133.299, 'eval_steps_per_second': 8.398, 'epoch': 0.56}
{'loss': 1.6752, 'grad_norm': 0.8677506446838379, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8645756244659424, 'eval_runtime': 7.5076, 'eval_samples_per_second': 133.197, 'eval_steps_per_second': 8.391, 'epoch': 0.6}
{'loss': 1.6408, 'grad_norm': 2.757568597793579, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8573538064956665, 'eval_runtime': 7.5117, 'eval_samples_per_second': 133.126, 'eval_steps_per_second': 8.387, 'epoch': 0.64}
{'loss': 1.6353, 'grad_norm': 0.9478392004966736, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.856732964515686, 'eval_runtime': 7.4895, 'eval_samples_per_second': 133.52, 'eval_steps_per_second': 8.412, 'epoch': 0.68}
{'loss': 1.6414, 'grad_norm': 0.8368449807167053, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.839546799659729, 'eval_runtime': 7.5136, 'eval_samples_per_second': 133.092, 'eval_steps_per_second': 8.385, 'epoch': 0.72}
{'loss': 1.6202, 'grad_norm': 0.9341052770614624, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.857259750366211, 'eval_runtime': 7.5437, 'eval_samples_per_second': 132.562, 'eval_steps_per_second': 8.351, 'epoch': 0.76}
{'loss': 1.6348, 'grad_norm': 0.8845346570014954, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8423000574111938, 'eval_runtime': 7.4964, 'eval_samples_per_second': 133.397, 'eval_steps_per_second': 8.404, 'epoch': 0.8}
{'loss': 1.6209, 'grad_norm': 0.8697093725204468, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8530374765396118, 'eval_runtime': 7.4907, 'eval_samples_per_second': 133.499, 'eval_steps_per_second': 8.41, 'epoch': 0.84}
{'loss': 1.6024, 'grad_norm': 0.8260287642478943, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.845541000366211, 'eval_runtime': 7.4998, 'eval_samples_per_second': 133.338, 'eval_steps_per_second': 8.4, 'epoch': 0.88}
{'loss': 1.613, 'grad_norm': 0.8504674434661865, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8433269262313843, 'eval_runtime': 7.4911, 'eval_samples_per_second': 133.492, 'eval_steps_per_second': 8.41, 'epoch': 0.92}
{'loss': 1.6215, 'grad_norm': 0.7816725373268127, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8437739610671997, 'eval_runtime': 7.4843, 'eval_samples_per_second': 133.613, 'eval_steps_per_second': 8.418, 'epoch': 0.96}
{'loss': 1.6086, 'grad_norm': 0.8963656425476074, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.844222068786621, 'eval_runtime': 7.4844, 'eval_samples_per_second': 133.611, 'eval_steps_per_second': 8.418, 'epoch': 1.0}
{'train_runtime': 434.9089, 'train_samples_per_second': 22.991, 'train_steps_per_second': 1.437, 'train_loss': 1.7194311889648437, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0030672550201416, 1.7845568656921387, 1.82706880569458, 1.834893822669983, 1.8316401243209839, 1.815995693206787, 1.8535507917404175, 1.82460355758667, 1.822948932647705, 1.8287041187286377, 1.8702467679977417, 1.8498800992965698, 1.8411363363265991, 1.8340004682540894, 1.8645756244659424, 1.8573538064956665, 1.856732964515686, 1.839546799659729, 1.857259750366211, 1.8423000574111938, 1.8530374765396118, 1.845541000366211, 1.8433269262313843, 1.8437739610671997, 1.844222068786621], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0030672550201416, 1.7845568656921387, 1.82706880569458, 1.834893822669983, 1.8316401243209839, 1.815995693206787, 1.8535507917404175, 1.82460355758667, 1.822948932647705, 1.8287041187286377, 1.8702467679977417, 1.8498800992965698, 1.8411363363265991, 1.8340004682540894, 1.8645756244659424, 1.8573538064956665, 1.856732964515686, 1.839546799659729, 1.857259750366211, 1.8423000574111938, 1.8530374765396118, 1.845541000366211, 1.8433269262313843, 1.8437739610671997, 1.844222068786621]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760297775268555
current iteration best possible eval_loss (full train run):  -1.844222068786621
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896, -1.0760297775268555]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.4165 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -1.0960139144891392
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -1.016884631751825
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -1.0395401086594833
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -1.0150671169768417
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -1.2791840637253808
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4696, dtype=torch.float64), 0, tensor(0.1760, dtype=torch.float64), tensor(0.0422, dtype=torch.float64), tensor(0.0139, dtype=torch.float64), 0, tensor(0.2984, dtype=torch.float64), 0, 26, 1, 1, 1, 0, 0, 2, 0.07654287829186253, 27.67600135034964, 1]
normalized proposed parameters for next round by BO: [tensor(1.5991e-18, dtype=torch.float64), tensor(0.4696, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1760, dtype=torch.float64), tensor(0.0422, dtype=torch.float64), tensor(0.0139, dtype=torch.float64), tensor(3.0954e-18, dtype=torch.float64), tensor(0.2984, dtype=torch.float64), tensor(3.8998e-18, dtype=torch.float64), tensor(0.8204, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.7654, dtype=torch.float64), tensor(0.5766, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.47
  rowan_hellaswag: 0
  sciq: 0.176
  triviaqa: 0.042
  truthfulqa_gen: 0.014
  wikitext: 0
  mmlu: 0.298
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.07654287829186253,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (27.67600135034964,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  2
lora dropout:  0.07654287829186253
lora alpha:  27.67600135034964
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,650,688 || all params: 8,031,911,936 || trainable%: 0.0206
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.3685, 'grad_norm': 2.3689112663269043, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0522468090057373, 'eval_runtime': 6.9946, 'eval_samples_per_second': 142.967, 'eval_steps_per_second': 9.007, 'epoch': 0.04}
{'loss': 1.2492, 'grad_norm': 1.2755674123764038, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.05092453956604, 'eval_runtime': 6.987, 'eval_samples_per_second': 143.123, 'eval_steps_per_second': 9.017, 'epoch': 0.08}
{'loss': 1.1646, 'grad_norm': 1.3342311382293701, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.962780475616455, 'eval_runtime': 7.0126, 'eval_samples_per_second': 142.601, 'eval_steps_per_second': 8.984, 'epoch': 0.12}
{'loss': 1.0788, 'grad_norm': 1.1603772640228271, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.946948766708374, 'eval_runtime': 6.9976, 'eval_samples_per_second': 142.906, 'eval_steps_per_second': 9.003, 'epoch': 0.16}
{'loss': 1.0932, 'grad_norm': 1.211512565612793, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9319024085998535, 'eval_runtime': 7.0148, 'eval_samples_per_second': 142.556, 'eval_steps_per_second': 8.981, 'epoch': 0.2}
{'loss': 1.0272, 'grad_norm': 0.9920589923858643, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.871433973312378, 'eval_runtime': 7.0103, 'eval_samples_per_second': 142.648, 'eval_steps_per_second': 8.987, 'epoch': 0.24}
{'loss': 1.0555, 'grad_norm': 0.8426457643508911, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9048808813095093, 'eval_runtime': 7.0068, 'eval_samples_per_second': 142.719, 'eval_steps_per_second': 8.991, 'epoch': 0.28}
{'loss': 1.0021, 'grad_norm': 1.287357211112976, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8997231721878052, 'eval_runtime': 6.9789, 'eval_samples_per_second': 143.289, 'eval_steps_per_second': 9.027, 'epoch': 0.32}
{'loss': 1.0383, 'grad_norm': 1.0409899950027466, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8173660039901733, 'eval_runtime': 6.9637, 'eval_samples_per_second': 143.602, 'eval_steps_per_second': 9.047, 'epoch': 0.36}
{'loss': 0.9891, 'grad_norm': 0.9216551184654236, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8646233081817627, 'eval_runtime': 6.9668, 'eval_samples_per_second': 143.538, 'eval_steps_per_second': 9.043, 'epoch': 0.4}
{'loss': 0.9426, 'grad_norm': 0.8572248220443726, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8316099643707275, 'eval_runtime': 6.9694, 'eval_samples_per_second': 143.484, 'eval_steps_per_second': 9.039, 'epoch': 0.44}
{'loss': 0.93, 'grad_norm': 0.8819599747657776, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9525164365768433, 'eval_runtime': 6.9758, 'eval_samples_per_second': 143.353, 'eval_steps_per_second': 9.031, 'epoch': 0.48}
{'loss': 0.9291, 'grad_norm': 0.904787540435791, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8940043449401855, 'eval_runtime': 6.9708, 'eval_samples_per_second': 143.455, 'eval_steps_per_second': 9.038, 'epoch': 0.52}
{'loss': 0.9422, 'grad_norm': 0.8743293881416321, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8888052701950073, 'eval_runtime': 6.9648, 'eval_samples_per_second': 143.579, 'eval_steps_per_second': 9.045, 'epoch': 0.56}
{'loss': 0.9582, 'grad_norm': 1.0303040742874146, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8292473554611206, 'eval_runtime': 6.9659, 'eval_samples_per_second': 143.556, 'eval_steps_per_second': 9.044, 'epoch': 0.6}
{'loss': 0.9913, 'grad_norm': 0.9577027559280396, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9055126905441284, 'eval_runtime': 6.9744, 'eval_samples_per_second': 143.381, 'eval_steps_per_second': 9.033, 'epoch': 0.64}
{'loss': 0.9103, 'grad_norm': 0.8679103255271912, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.913345217704773, 'eval_runtime': 6.9805, 'eval_samples_per_second': 143.257, 'eval_steps_per_second': 9.025, 'epoch': 0.68}
{'loss': 0.9153, 'grad_norm': 0.9690583348274231, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9224551916122437, 'eval_runtime': 6.9919, 'eval_samples_per_second': 143.023, 'eval_steps_per_second': 9.01, 'epoch': 0.72}
{'loss': 0.9416, 'grad_norm': 0.9504247307777405, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9253076314926147, 'eval_runtime': 6.9839, 'eval_samples_per_second': 143.186, 'eval_steps_per_second': 9.021, 'epoch': 0.76}
{'loss': 0.9183, 'grad_norm': 0.9955810308456421, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.902082085609436, 'eval_runtime': 6.984, 'eval_samples_per_second': 143.184, 'eval_steps_per_second': 9.021, 'epoch': 0.8}
{'loss': 0.9552, 'grad_norm': 0.9787005186080933, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.891736626625061, 'eval_runtime': 6.9772, 'eval_samples_per_second': 143.323, 'eval_steps_per_second': 9.029, 'epoch': 0.84}
{'loss': 0.9677, 'grad_norm': 0.9215174913406372, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9060156345367432, 'eval_runtime': 7.0472, 'eval_samples_per_second': 141.9, 'eval_steps_per_second': 8.94, 'epoch': 0.88}
{'loss': 0.9124, 'grad_norm': 1.4439061880111694, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9157450199127197, 'eval_runtime': 7.0511, 'eval_samples_per_second': 141.823, 'eval_steps_per_second': 8.935, 'epoch': 0.92}
{'loss': 0.9106, 'grad_norm': 0.8731346130371094, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9123525619506836, 'eval_runtime': 7.0351, 'eval_samples_per_second': 142.144, 'eval_steps_per_second': 8.955, 'epoch': 0.96}
{'loss': 0.9084, 'grad_norm': 1.0189687013626099, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9136713743209839, 'eval_runtime': 7.0315, 'eval_samples_per_second': 142.218, 'eval_steps_per_second': 8.96, 'epoch': 1.0}
{'train_runtime': 395.6686, 'train_samples_per_second': 25.266, 'train_steps_per_second': 1.58, 'train_loss': 1.0439941833496094, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0522468090057373, 2.05092453956604, 1.962780475616455, 1.946948766708374, 1.9319024085998535, 1.871433973312378, 1.9048808813095093, 1.8997231721878052, 1.8173660039901733, 1.8646233081817627, 1.8316099643707275, 1.9525164365768433, 1.8940043449401855, 1.8888052701950073, 1.8292473554611206, 1.9055126905441284, 1.913345217704773, 1.9224551916122437, 1.9253076314926147, 1.902082085609436, 1.891736626625061, 1.9060156345367432, 1.9157450199127197, 1.9123525619506836, 1.9136713743209839], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0522468090057373, 2.05092453956604, 1.962780475616455, 1.946948766708374, 1.9319024085998535, 1.871433973312378, 1.9048808813095093, 1.8997231721878052, 1.8173660039901733, 1.8646233081817627, 1.8316099643707275, 1.9525164365768433, 1.8940043449401855, 1.8888052701950073, 1.8292473554611206, 1.9055126905441284, 1.913345217704773, 1.9224551916122437, 1.9253076314926147, 1.902082085609436, 1.891736626625061, 1.9060156345367432, 1.9157450199127197, 1.9123525619506836, 1.9136713743209839]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760324001312256
current iteration best possible eval_loss (full train run):  -1.9136713743209839
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896, -1.0760297775268555, -1.0760324001312256]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1732 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -0.9860991271886645
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -0.9860991271886645
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -0.9860991271886645
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -0.9860991271886645
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -0.9860991271886645
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.2482, dtype=torch.float64), 0, tensor(0.7518, dtype=torch.float64), 32, 1, 0, 1, 1, 0, 14, 0.1, 33.14159187462579, 1]
normalized proposed parameters for next round by BO: [tensor(2.0430e-18, dtype=torch.float64), tensor(1.2268e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5645e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2482, dtype=torch.float64), tensor(9.2400e-19, dtype=torch.float64), tensor(0.7518, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1058, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6904, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.248
  mmlu: 0
  arc_challenge: 0.752

LoRA Parameters:
  lora_r: (14,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (33.14159187462579,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  14
lora dropout:  0.1
lora alpha:  33.14159187462579
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 20,185,088 || all params: 8,050,446,336 || trainable%: 0.2507
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6557, 'grad_norm': 1.0425467491149902, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.0911314487457275, 'eval_runtime': 7.4224, 'eval_samples_per_second': 134.727, 'eval_steps_per_second': 8.488, 'epoch': 0.04}
{'loss': 1.1962, 'grad_norm': 0.7370846271514893, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.8921472430229187, 'eval_runtime': 7.4194, 'eval_samples_per_second': 134.783, 'eval_steps_per_second': 8.491, 'epoch': 0.08}
{'loss': 0.9636, 'grad_norm': 0.7594239115715027, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8758756518363953, 'eval_runtime': 7.4324, 'eval_samples_per_second': 134.545, 'eval_steps_per_second': 8.476, 'epoch': 0.12}
{'loss': 1.0136, 'grad_norm': 0.7573881149291992, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8910752534866333, 'eval_runtime': 7.4892, 'eval_samples_per_second': 133.525, 'eval_steps_per_second': 8.412, 'epoch': 0.16}
{'loss': 0.8999, 'grad_norm': 0.7060225009918213, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8905609846115112, 'eval_runtime': 7.501, 'eval_samples_per_second': 133.315, 'eval_steps_per_second': 8.399, 'epoch': 0.2}
{'loss': 0.8006, 'grad_norm': 0.8663959503173828, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9262100458145142, 'eval_runtime': 7.5033, 'eval_samples_per_second': 133.275, 'eval_steps_per_second': 8.396, 'epoch': 0.24}
{'loss': 0.7132, 'grad_norm': 0.8097053170204163, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9313920736312866, 'eval_runtime': 7.4737, 'eval_samples_per_second': 133.802, 'eval_steps_per_second': 8.43, 'epoch': 0.28}
{'loss': 0.804, 'grad_norm': 0.8473263382911682, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9632823467254639, 'eval_runtime': 7.4782, 'eval_samples_per_second': 133.722, 'eval_steps_per_second': 8.424, 'epoch': 0.32}
{'loss': 0.6896, 'grad_norm': 1.0021237134933472, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9817586541175842, 'eval_runtime': 7.4771, 'eval_samples_per_second': 133.742, 'eval_steps_per_second': 8.426, 'epoch': 0.36}
{'loss': 0.6601, 'grad_norm': 1.1250194311141968, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9867991805076599, 'eval_runtime': 7.4701, 'eval_samples_per_second': 133.866, 'eval_steps_per_second': 8.434, 'epoch': 0.4}
{'loss': 0.6579, 'grad_norm': 0.8485544919967651, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0079339742660522, 'eval_runtime': 7.4664, 'eval_samples_per_second': 133.933, 'eval_steps_per_second': 8.438, 'epoch': 0.44}
{'loss': 0.5784, 'grad_norm': 0.9834070801734924, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.035333275794983, 'eval_runtime': 7.4686, 'eval_samples_per_second': 133.893, 'eval_steps_per_second': 8.435, 'epoch': 0.48}
{'loss': 0.6463, 'grad_norm': 0.7451013326644897, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0457043647766113, 'eval_runtime': 7.4659, 'eval_samples_per_second': 133.943, 'eval_steps_per_second': 8.438, 'epoch': 0.52}
{'loss': 0.5968, 'grad_norm': 0.7799468636512756, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0535216331481934, 'eval_runtime': 7.4663, 'eval_samples_per_second': 133.936, 'eval_steps_per_second': 8.438, 'epoch': 0.56}
{'loss': 0.4824, 'grad_norm': 1.0851126909255981, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0706977844238281, 'eval_runtime': 7.4701, 'eval_samples_per_second': 133.868, 'eval_steps_per_second': 8.434, 'epoch': 0.6}
{'loss': 0.4862, 'grad_norm': 0.7092361450195312, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1521492004394531, 'eval_runtime': 7.4719, 'eval_samples_per_second': 133.834, 'eval_steps_per_second': 8.432, 'epoch': 0.64}
{'loss': 0.4969, 'grad_norm': 0.8251635432243347, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0730247497558594, 'eval_runtime': 7.4698, 'eval_samples_per_second': 133.872, 'eval_steps_per_second': 8.434, 'epoch': 0.68}
{'loss': 0.4847, 'grad_norm': 0.9129394292831421, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1418267488479614, 'eval_runtime': 7.4734, 'eval_samples_per_second': 133.807, 'eval_steps_per_second': 8.43, 'epoch': 0.72}
{'loss': 0.5107, 'grad_norm': 0.8805413246154785, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.154295563697815, 'eval_runtime': 7.5, 'eval_samples_per_second': 133.333, 'eval_steps_per_second': 8.4, 'epoch': 0.76}
{'loss': 0.482, 'grad_norm': 0.4593411386013031, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.202457070350647, 'eval_runtime': 7.5055, 'eval_samples_per_second': 133.236, 'eval_steps_per_second': 8.394, 'epoch': 0.8}
{'loss': 0.544, 'grad_norm': 0.873571515083313, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1728583574295044, 'eval_runtime': 7.4976, 'eval_samples_per_second': 133.376, 'eval_steps_per_second': 8.403, 'epoch': 0.84}
{'loss': 0.4526, 'grad_norm': 0.6854473352432251, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.223943829536438, 'eval_runtime': 7.4905, 'eval_samples_per_second': 133.503, 'eval_steps_per_second': 8.411, 'epoch': 0.88}
{'loss': 0.4364, 'grad_norm': 0.6980345845222473, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2052241563796997, 'eval_runtime': 7.4733, 'eval_samples_per_second': 133.81, 'eval_steps_per_second': 8.43, 'epoch': 0.92}
{'loss': 0.404, 'grad_norm': 0.688274085521698, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2227928638458252, 'eval_runtime': 7.5097, 'eval_samples_per_second': 133.161, 'eval_steps_per_second': 8.389, 'epoch': 0.96}
{'loss': 0.4367, 'grad_norm': 0.38047856092453003, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2271220684051514, 'eval_runtime': 7.4928, 'eval_samples_per_second': 133.461, 'eval_steps_per_second': 8.408, 'epoch': 1.0}
{'train_runtime': 383.3416, 'train_samples_per_second': 26.084, 'train_steps_per_second': 1.63, 'train_loss': 0.7236969955444336, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0911314487457275, 0.8921472430229187, 0.8758756518363953, 0.8910752534866333, 0.8905609846115112, 0.9262100458145142, 0.9313920736312866, 0.9632823467254639, 0.9817586541175842, 0.9867991805076599, 1.0079339742660522, 1.035333275794983, 1.0457043647766113, 1.0535216331481934, 1.0706977844238281, 1.1521492004394531, 1.0730247497558594, 1.1418267488479614, 1.154295563697815, 1.202457070350647, 1.1728583574295044, 1.223943829536438, 1.2052241563796997, 1.2227928638458252, 1.2271220684051514], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.0911314487457275, 0.8921472430229187, 0.8758756518363953, 0.8910752534866333, 0.8905609846115112, 0.9262100458145142, 0.9313920736312866, 0.9632823467254639, 0.9817586541175842, 0.9867991805076599, 1.0079339742660522, 1.035333275794983, 1.0457043647766113, 1.0535216331481934, 1.0706977844238281, 1.1521492004394531, 1.0730247497558594, 1.1418267488479614, 1.154295563697815, 1.202457070350647, 1.1728583574295044, 1.223943829536438, 1.2052241563796997, 1.2227928638458252, 1.2271220684051514]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760294198989868
current iteration best possible eval_loss (full train run):  -1.2271220684051514
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896, -1.0760297775268555, -1.0760324001312256, -1.0760294198989868]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4580 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -1.1183432927156347
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -1.0351302540033394
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -1.0475565692528648
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -1.100624136528025
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -1.038954548593472
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1619, dtype=torch.float64), 0, 0, 0, tensor(0.8286, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 0, 57, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.6664e-17, dtype=torch.float64), tensor(0.1619, dtype=torch.float64), tensor(6.0250e-18, dtype=torch.float64), tensor(1.5846e-17, dtype=torch.float64), tensor(0.0095, dtype=torch.float64), tensor(0.8286, dtype=torch.float64), tensor(6.1699e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4437, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.162
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.829
  arc_challenge: 0

LoRA Parameters:
  lora_r: (57,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  57
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 82,182,144 || all params: 8,112,443,392 || trainable%: 1.0130
length of training data:  9905
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4972, 'grad_norm': 0.7385448813438416, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8243135213851929, 'eval_runtime': 7.8321, 'eval_samples_per_second': 127.68, 'eval_steps_per_second': 8.044, 'epoch': 0.04}
{'loss': 1.3373, 'grad_norm': 0.6071732044219971, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.804197072982788, 'eval_runtime': 7.8121, 'eval_samples_per_second': 128.006, 'eval_steps_per_second': 8.064, 'epoch': 0.08}
{'loss': 1.1996, 'grad_norm': 0.37730351090431213, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.8133090734481812, 'eval_runtime': 7.8104, 'eval_samples_per_second': 128.034, 'eval_steps_per_second': 8.066, 'epoch': 0.12}
{'loss': 1.1647, 'grad_norm': 0.3615550398826599, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.7691164016723633, 'eval_runtime': 7.8651, 'eval_samples_per_second': 127.144, 'eval_steps_per_second': 8.01, 'epoch': 0.16}
{'loss': 1.1983, 'grad_norm': 0.49495452642440796, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 1.7513781785964966, 'eval_runtime': 7.8701, 'eval_samples_per_second': 127.063, 'eval_steps_per_second': 8.005, 'epoch': 0.2}
{'loss': 1.1786, 'grad_norm': 0.3392593562602997, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 1.780413031578064, 'eval_runtime': 7.8683, 'eval_samples_per_second': 127.092, 'eval_steps_per_second': 8.007, 'epoch': 0.24}
{'loss': 1.1744, 'grad_norm': 0.33639681339263916, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 1.7957895994186401, 'eval_runtime': 7.8931, 'eval_samples_per_second': 126.693, 'eval_steps_per_second': 7.982, 'epoch': 0.28}
{'loss': 1.1664, 'grad_norm': 0.3505040109157562, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 1.8441579341888428, 'eval_runtime': 7.8524, 'eval_samples_per_second': 127.349, 'eval_steps_per_second': 8.023, 'epoch': 0.32}
{'loss': 1.1565, 'grad_norm': 0.3754795491695404, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 1.8633627891540527, 'eval_runtime': 7.83, 'eval_samples_per_second': 127.714, 'eval_steps_per_second': 8.046, 'epoch': 0.36}
{'loss': 1.1627, 'grad_norm': 0.3819180428981781, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 1.8661845922470093, 'eval_runtime': 7.8495, 'eval_samples_per_second': 127.396, 'eval_steps_per_second': 8.026, 'epoch': 0.4}
{'loss': 1.1272, 'grad_norm': 0.3314037322998047, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 1.8168269395828247, 'eval_runtime': 7.8368, 'eval_samples_per_second': 127.602, 'eval_steps_per_second': 8.039, 'epoch': 0.44}
{'loss': 1.1307, 'grad_norm': 0.36415961384773254, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 1.8259423971176147, 'eval_runtime': 7.8348, 'eval_samples_per_second': 127.635, 'eval_steps_per_second': 8.041, 'epoch': 0.48}
{'loss': 1.0943, 'grad_norm': 0.36287063360214233, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 1.8358139991760254, 'eval_runtime': 7.8298, 'eval_samples_per_second': 127.717, 'eval_steps_per_second': 8.046, 'epoch': 0.52}
{'loss': 1.1425, 'grad_norm': 0.34515219926834106, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 1.8205527067184448, 'eval_runtime': 7.8337, 'eval_samples_per_second': 127.653, 'eval_steps_per_second': 8.042, 'epoch': 0.56}
{'loss': 1.0792, 'grad_norm': 0.381609171628952, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 1.792827844619751, 'eval_runtime': 7.8272, 'eval_samples_per_second': 127.759, 'eval_steps_per_second': 8.049, 'epoch': 0.6}
{'loss': 1.0641, 'grad_norm': 0.554173469543457, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 1.8077418804168701, 'eval_runtime': 7.8252, 'eval_samples_per_second': 127.792, 'eval_steps_per_second': 8.051, 'epoch': 0.65}
{'loss': 1.0115, 'grad_norm': 0.38417235016822815, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 1.799005150794983, 'eval_runtime': 7.8319, 'eval_samples_per_second': 127.683, 'eval_steps_per_second': 8.044, 'epoch': 0.69}
{'loss': 1.0658, 'grad_norm': 0.6492875814437866, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 1.798996925354004, 'eval_runtime': 7.8295, 'eval_samples_per_second': 127.722, 'eval_steps_per_second': 8.046, 'epoch': 0.73}
{'loss': 1.0904, 'grad_norm': 0.6765519380569458, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 1.7787621021270752, 'eval_runtime': 7.8231, 'eval_samples_per_second': 127.827, 'eval_steps_per_second': 8.053, 'epoch': 0.77}
{'loss': 1.0837, 'grad_norm': 0.3862026631832123, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 1.8018120527267456, 'eval_runtime': 7.8338, 'eval_samples_per_second': 127.651, 'eval_steps_per_second': 8.042, 'epoch': 0.81}
{'loss': 1.038, 'grad_norm': 0.3996872007846832, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 1.7989370822906494, 'eval_runtime': 7.9082, 'eval_samples_per_second': 126.45, 'eval_steps_per_second': 7.966, 'epoch': 0.85}
{'loss': 1.038, 'grad_norm': 0.45442160964012146, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 1.8059496879577637, 'eval_runtime': 7.9106, 'eval_samples_per_second': 126.413, 'eval_steps_per_second': 7.964, 'epoch': 0.89}
{'loss': 1.0797, 'grad_norm': 0.45547592639923096, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 1.805572509765625, 'eval_runtime': 7.9166, 'eval_samples_per_second': 126.316, 'eval_steps_per_second': 7.958, 'epoch': 0.93}
{'loss': 1.0488, 'grad_norm': 0.474153608083725, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 1.8080371618270874, 'eval_runtime': 7.8972, 'eval_samples_per_second': 126.627, 'eval_steps_per_second': 7.977, 'epoch': 0.97}
{'train_runtime': 435.9352, 'train_samples_per_second': 22.721, 'train_steps_per_second': 1.422, 'train_loss': 1.1756924229283487, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8243135213851929, 1.804197072982788, 1.8133090734481812, 1.7691164016723633, 1.7513781785964966, 1.780413031578064, 1.7957895994186401, 1.8441579341888428, 1.8633627891540527, 1.8661845922470093, 1.8168269395828247, 1.8259423971176147, 1.8358139991760254, 1.8205527067184448, 1.792827844619751, 1.8077418804168701, 1.799005150794983, 1.798996925354004, 1.7787621021270752, 1.8018120527267456, 1.7989370822906494, 1.8059496879577637, 1.805572509765625, 1.8080371618270874], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.8243135213851929, 1.804197072982788, 1.8133090734481812, 1.7691164016723633, 1.7513781785964966, 1.780413031578064, 1.7957895994186401, 1.8441579341888428, 1.8633627891540527, 1.8661845922470093, 1.8168269395828247, 1.8259423971176147, 1.8358139991760254, 1.8205527067184448, 1.792827844619751, 1.8077418804168701, 1.799005150794983, 1.798996925354004, 1.7787621021270752, 1.8018120527267456, 1.7989370822906494, 1.8059496879577637, 1.805572509765625, 1.8080371618270874]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0860702991485596
current iteration best possible eval_loss (full train run):  -1.8080371618270874
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896, -1.0760297775268555, -1.0760324001312256, -1.0760294198989868, -1.0860702991485596]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0215 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.08295202255249023, 0.8953140377998352, 0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.36218807101249695, 0.6549691557884216, 0.8173236846923828, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3347404897212982, 0.355441689491272, 0.4367692470550537, 0.45111382007598877]  ‚Üí  acq = -1.011732903544223
X = [0.35591208934783936, 0.098782479763031, 0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.36877870559692383, 0.8672244548797607, 0.5090312361717224, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9806126356124878, 0.5262150168418884, 0.617688775062561, 0.313107967376709]  ‚Üí  acq = -1.011732903544223
X = [0.2322162389755249, 0.03715479373931885, 0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.40351834893226624, 0.9447525143623352, 0.40425533056259155, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9491793513298035, 0.21524584293365479, 0.22934073209762573, 0.15074169635772705]  ‚Üí  acq = -1.011732903544223
X = [0.5550482869148254, 0.7731989026069641, 0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6225687265396118, 0.3806919455528259, 0.9421022534370422, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7150893807411194, 0.5437468886375427, 0.6237008571624756, 0.9093855619430542]  ‚Üí  acq = -1.011732903544223
X = [0.7380771636962891, 0.9890440702438354, 0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.4263235926628113, 0.9630946516990662, 0.6111319065093994, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.834472119808197, 0.3034810423851013, 0.033137477934360504, 0.012760698795318604]  ‚Üí  acq = -1.011732903544223
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0257, dtype=torch.float64), 0, tensor(0.0435, dtype=torch.float64), 0, tensor(0.3846, dtype=torch.float64), tensor(0.4736, dtype=torch.float64), tensor(0.0138, dtype=torch.float64), 0, tensor(0.0530, dtype=torch.float64), 32, 0, 1, 1, 0, 1, 2, 0.04791774723784668, 43.77483255843523, 0]
normalized proposed parameters for next round by BO: [tensor(0.0257, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0435, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3846, dtype=torch.float64), tensor(0.4736, dtype=torch.float64), tensor(0.0138, dtype=torch.float64), tensor(0.0059, dtype=torch.float64), tensor(0.0530, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.4792, dtype=torch.float64), tensor(0.9120, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.026
  gsm8k: 0
  rowan_hellaswag: 0.043
  sciq: 0
  triviaqa: 0.385
  truthfulqa_gen: 0.474
  wikitext: 0.014
  mmlu: 0
  arc_challenge: 0.053

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.04791774723784668,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (43.77483255843523,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  2
lora dropout:  0.04791774723784668
lora alpha:  43.77483255843523
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,686,976 || all params: 8,032,948,224 || trainable%: 0.0334
length of training data:  9938
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2011, 'grad_norm': 6.139817237854004, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4880881309509277, 'eval_runtime': 7.2988, 'eval_samples_per_second': 137.01, 'eval_steps_per_second': 8.632, 'epoch': 0.04}
{'loss': 1.3612, 'grad_norm': 2.489708185195923, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0929768085479736, 'eval_runtime': 7.309, 'eval_samples_per_second': 136.817, 'eval_steps_per_second': 8.619, 'epoch': 0.08}
{'loss': 1.1291, 'grad_norm': 3.2897748947143555, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 0.9564079642295837, 'eval_runtime': 7.2776, 'eval_samples_per_second': 137.408, 'eval_steps_per_second': 8.657, 'epoch': 0.12}
{'loss': 1.0011, 'grad_norm': 2.18548321723938, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 0.946715235710144, 'eval_runtime': 7.3017, 'eval_samples_per_second': 136.955, 'eval_steps_per_second': 8.628, 'epoch': 0.16}
{'loss': 0.9067, 'grad_norm': 2.1407039165496826, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 0.9411309361457825, 'eval_runtime': 7.3027, 'eval_samples_per_second': 136.935, 'eval_steps_per_second': 8.627, 'epoch': 0.2}
{'loss': 0.9127, 'grad_norm': 2.429155111312866, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 0.9347501397132874, 'eval_runtime': 7.3109, 'eval_samples_per_second': 136.782, 'eval_steps_per_second': 8.617, 'epoch': 0.24}
{'loss': 0.899, 'grad_norm': 1.843058705329895, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 0.9263084530830383, 'eval_runtime': 7.3328, 'eval_samples_per_second': 136.373, 'eval_steps_per_second': 8.591, 'epoch': 0.28}
{'loss': 0.8749, 'grad_norm': 2.3984081745147705, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 0.9305544495582581, 'eval_runtime': 7.3303, 'eval_samples_per_second': 136.421, 'eval_steps_per_second': 8.594, 'epoch': 0.32}
{'loss': 0.8186, 'grad_norm': 1.6432404518127441, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 0.9247286915779114, 'eval_runtime': 7.3285, 'eval_samples_per_second': 136.454, 'eval_steps_per_second': 8.597, 'epoch': 0.36}
{'loss': 0.8314, 'grad_norm': 1.6320406198501587, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 0.9144677519798279, 'eval_runtime': 7.327, 'eval_samples_per_second': 136.481, 'eval_steps_per_second': 8.598, 'epoch': 0.4}
{'loss': 0.8262, 'grad_norm': 1.4256192445755005, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 0.9273276329040527, 'eval_runtime': 7.3304, 'eval_samples_per_second': 136.418, 'eval_steps_per_second': 8.594, 'epoch': 0.44}
{'loss': 0.7555, 'grad_norm': 2.186965227127075, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 0.9129481911659241, 'eval_runtime': 7.3301, 'eval_samples_per_second': 136.423, 'eval_steps_per_second': 8.595, 'epoch': 0.48}
{'loss': 0.7666, 'grad_norm': 1.5724552869796753, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 0.9185514450073242, 'eval_runtime': 7.3367, 'eval_samples_per_second': 136.302, 'eval_steps_per_second': 8.587, 'epoch': 0.52}
{'loss': 0.7937, 'grad_norm': 1.632585883140564, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 0.9176806211471558, 'eval_runtime': 7.3214, 'eval_samples_per_second': 136.586, 'eval_steps_per_second': 8.605, 'epoch': 0.56}
{'loss': 0.7642, 'grad_norm': 2.5990512371063232, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 0.9125344157218933, 'eval_runtime': 7.3354, 'eval_samples_per_second': 136.325, 'eval_steps_per_second': 8.588, 'epoch': 0.6}
{'loss': 0.7806, 'grad_norm': 1.926627278327942, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 0.905305027961731, 'eval_runtime': 7.327, 'eval_samples_per_second': 136.481, 'eval_steps_per_second': 8.598, 'epoch': 0.64}
{'loss': 0.8183, 'grad_norm': 2.4720571041107178, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 0.9132179021835327, 'eval_runtime': 7.3612, 'eval_samples_per_second': 135.847, 'eval_steps_per_second': 8.558, 'epoch': 0.68}
{'loss': 0.6935, 'grad_norm': 1.7222423553466797, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 0.9050979018211365, 'eval_runtime': 7.3995, 'eval_samples_per_second': 135.145, 'eval_steps_per_second': 8.514, 'epoch': 0.72}
{'loss': 0.7025, 'grad_norm': 2.2827727794647217, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 0.9057376384735107, 'eval_runtime': 7.3748, 'eval_samples_per_second': 135.597, 'eval_steps_per_second': 8.543, 'epoch': 0.76}
{'loss': 0.8113, 'grad_norm': 1.9246971607208252, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.9099364280700684, 'eval_runtime': 7.3735, 'eval_samples_per_second': 135.62, 'eval_steps_per_second': 8.544, 'epoch': 0.8}
{'loss': 0.7045, 'grad_norm': 1.743462324142456, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 0.903714656829834, 'eval_runtime': 7.4076, 'eval_samples_per_second': 134.997, 'eval_steps_per_second': 8.505, 'epoch': 0.84}
{'loss': 0.7529, 'grad_norm': 1.774932861328125, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.9006075263023376, 'eval_runtime': 7.3825, 'eval_samples_per_second': 135.455, 'eval_steps_per_second': 8.534, 'epoch': 0.88}
{'loss': 0.8042, 'grad_norm': 1.593174695968628, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.9024023413658142, 'eval_runtime': 7.343, 'eval_samples_per_second': 136.183, 'eval_steps_per_second': 8.58, 'epoch': 0.92}
{'loss': 0.723, 'grad_norm': 2.018012285232544, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.9008257389068604, 'eval_runtime': 7.3475, 'eval_samples_per_second': 136.102, 'eval_steps_per_second': 8.574, 'epoch': 0.96}
{'train_runtime': 340.4214, 'train_samples_per_second': 29.193, 'train_steps_per_second': 1.827, 'train_loss': 0.9348027361167589, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4880881309509277, 1.0929768085479736, 0.9564079642295837, 0.946715235710144, 0.9411309361457825, 0.9347501397132874, 0.9263084530830383, 0.9305544495582581, 0.9247286915779114, 0.9144677519798279, 0.9273276329040527, 0.9129481911659241, 0.9185514450073242, 0.9176806211471558, 0.9125344157218933, 0.905305027961731, 0.9132179021835327, 0.9050979018211365, 0.9057376384735107, 0.9099364280700684, 0.903714656829834, 0.9006075263023376, 0.9024023413658142, 0.9008257389068604], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.4880881309509277, 1.0929768085479736, 0.9564079642295837, 0.946715235710144, 0.9411309361457825, 0.9347501397132874, 0.9263084530830383, 0.9305544495582581, 0.9247286915779114, 0.9144677519798279, 0.9273276329040527, 0.9129481911659241, 0.9185514450073242, 0.9176806211471558, 0.9125344157218933, 0.905305027961731, 0.9132179021835327, 0.9050979018211365, 0.9057376384735107, 0.9099364280700684, 0.903714656829834, 0.9006075263023376, 0.9024023413658142, 0.9008257389068604]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760269165039062
current iteration best possible eval_loss (full train run):  -0.9008257389068604
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896, -1.0760297775268555, -1.0760324001312256, -1.0760294198989868, -1.0860702991485596, -1.0760269165039062]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.5612 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.038794755935668945, 0.5530740022659302, 0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.32418566942214966, 0.03446310758590698, 0.8077076077461243, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5978026986122131, 0.24681228399276733, 0.3197188079357147, 0.43591630458831787]  ‚Üí  acq = -1.0069655036882972
X = [0.06298112869262695, 0.4111078381538391, 0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.2800779640674591, 0.1305062174797058, 0.7458388209342957, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2502773702144623, 0.26675117015838623, 0.721631646156311, 0.1087694764137268]  ‚Üí  acq = -0.9919777270912025
X = [0.7721713781356812, 0.36252284049987793, 0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8363065719604492, 0.2568463683128357, 0.6493399143218994, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7879849672317505, 0.044145405292510986, 0.17305481433868408, 0.9866487383842468]  ‚Üí  acq = -1.0309994754875045
X = [0.5755511522293091, 0.49270403385162354, 0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2550579309463501, 0.3171401023864746, 0.8667199611663818, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6549527049064636, 0.34602898359298706, 0.037224020808935165, 0.7405623197555542]  ‚Üí  acq = -1.076721031468758
X = [0.7848239541053772, 0.7650286555290222, 0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.2828661799430847, 0.7089584469795227, 0.5923287272453308, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.9000691771507263, 0.3606852889060974, 0.7490195035934448, 0.591159462928772]  ‚Üí  acq = -0.9864021307664241
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1198, dtype=torch.float64), tensor(0.3442, dtype=torch.float64), 0, 0, tensor(0.5360, dtype=torch.float64), 0, 2, 0, 1, 0, 1, 1, 51, 0.1, 27.240180682844368, 1]
normalized proposed parameters for next round by BO: [tensor(1.8303e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1198, dtype=torch.float64), tensor(0.3442, dtype=torch.float64), tensor(3.4566e-18, dtype=torch.float64), tensor(9.8953e-18, dtype=torch.float64), tensor(0.5360, dtype=torch.float64), tensor(1.5469e-18, dtype=torch.float64), tensor(0.0654, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4007, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5675, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.12
  triviaqa: 0.344
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.536
  arc_challenge: 0

LoRA Parameters:
  lora_r: (51,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (2,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (27.240180682844368,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  2
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  51
lora dropout:  0.1
lora alpha:  27.240180682844368
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 4,282,368 || all params: 8,034,543,616 || trainable%: 0.0533
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6318, 'grad_norm': 3.796759843826294, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.6958701610565186, 'eval_runtime': 6.3904, 'eval_samples_per_second': 156.484, 'eval_steps_per_second': 9.858, 'epoch': 0.04}
{'loss': 2.0471, 'grad_norm': 0.9867148995399475, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.903469443321228, 'eval_runtime': 6.3917, 'eval_samples_per_second': 156.452, 'eval_steps_per_second': 9.857, 'epoch': 0.08}
{'loss': 1.5216, 'grad_norm': 1.1957249641418457, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.758485198020935, 'eval_runtime': 6.407, 'eval_samples_per_second': 156.079, 'eval_steps_per_second': 9.833, 'epoch': 0.12}
{'loss': 1.3912, 'grad_norm': 1.1303249597549438, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.75737726688385, 'eval_runtime': 6.4028, 'eval_samples_per_second': 156.183, 'eval_steps_per_second': 9.84, 'epoch': 0.16}
{'loss': 1.3536, 'grad_norm': 0.6453469395637512, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7488986253738403, 'eval_runtime': 6.3935, 'eval_samples_per_second': 156.408, 'eval_steps_per_second': 9.854, 'epoch': 0.2}
{'loss': 1.3786, 'grad_norm': 0.9519771337509155, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.729967474937439, 'eval_runtime': 6.3971, 'eval_samples_per_second': 156.321, 'eval_steps_per_second': 9.848, 'epoch': 0.24}
{'loss': 1.2833, 'grad_norm': 0.5631465911865234, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7329598665237427, 'eval_runtime': 6.379, 'eval_samples_per_second': 156.764, 'eval_steps_per_second': 9.876, 'epoch': 0.28}
{'loss': 1.2603, 'grad_norm': 0.4003693461418152, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7382934093475342, 'eval_runtime': 6.3717, 'eval_samples_per_second': 156.945, 'eval_steps_per_second': 9.888, 'epoch': 0.32}
{'loss': 1.2448, 'grad_norm': 0.8793152570724487, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7261240482330322, 'eval_runtime': 6.3855, 'eval_samples_per_second': 156.604, 'eval_steps_per_second': 9.866, 'epoch': 0.36}
{'loss': 1.2006, 'grad_norm': 0.3772229552268982, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.722583293914795, 'eval_runtime': 6.3702, 'eval_samples_per_second': 156.981, 'eval_steps_per_second': 9.89, 'epoch': 0.4}
{'loss': 1.2222, 'grad_norm': 0.4237900674343109, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7371045351028442, 'eval_runtime': 6.3711, 'eval_samples_per_second': 156.958, 'eval_steps_per_second': 9.888, 'epoch': 0.44}
{'loss': 1.1842, 'grad_norm': 0.4378536641597748, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6963603496551514, 'eval_runtime': 6.3787, 'eval_samples_per_second': 156.772, 'eval_steps_per_second': 9.877, 'epoch': 0.48}
{'loss': 1.1553, 'grad_norm': 0.38741162419319153, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7272804975509644, 'eval_runtime': 6.3677, 'eval_samples_per_second': 157.043, 'eval_steps_per_second': 9.894, 'epoch': 0.52}
{'loss': 1.1789, 'grad_norm': 0.5390116572380066, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7256667613983154, 'eval_runtime': 6.3831, 'eval_samples_per_second': 156.664, 'eval_steps_per_second': 9.87, 'epoch': 0.56}
{'loss': 1.2062, 'grad_norm': 0.4359818696975708, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7143996953964233, 'eval_runtime': 6.3658, 'eval_samples_per_second': 157.089, 'eval_steps_per_second': 9.897, 'epoch': 0.6}
{'loss': 1.1838, 'grad_norm': 0.2548901438713074, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7249618768692017, 'eval_runtime': 6.3664, 'eval_samples_per_second': 157.075, 'eval_steps_per_second': 9.896, 'epoch': 0.64}
{'loss': 1.1933, 'grad_norm': 0.3812347948551178, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7304203510284424, 'eval_runtime': 6.3777, 'eval_samples_per_second': 156.797, 'eval_steps_per_second': 9.878, 'epoch': 0.68}
{'loss': 1.1866, 'grad_norm': 0.3587527573108673, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7173148393630981, 'eval_runtime': 6.3687, 'eval_samples_per_second': 157.018, 'eval_steps_per_second': 9.892, 'epoch': 0.72}
{'loss': 1.179, 'grad_norm': 0.41491127014160156, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7334916591644287, 'eval_runtime': 6.372, 'eval_samples_per_second': 156.936, 'eval_steps_per_second': 9.887, 'epoch': 0.76}
{'loss': 1.1811, 'grad_norm': 0.31736814975738525, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7369639873504639, 'eval_runtime': 6.3663, 'eval_samples_per_second': 157.076, 'eval_steps_per_second': 9.896, 'epoch': 0.8}
{'loss': 1.2119, 'grad_norm': 0.2801690399646759, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.72784423828125, 'eval_runtime': 6.3641, 'eval_samples_per_second': 157.131, 'eval_steps_per_second': 9.899, 'epoch': 0.84}
{'loss': 1.1293, 'grad_norm': 0.4086771011352539, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7386749982833862, 'eval_runtime': 6.4128, 'eval_samples_per_second': 155.938, 'eval_steps_per_second': 9.824, 'epoch': 0.88}
{'loss': 1.1925, 'grad_norm': 0.43486878275871277, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.744436502456665, 'eval_runtime': 6.3979, 'eval_samples_per_second': 156.301, 'eval_steps_per_second': 9.847, 'epoch': 0.92}
{'loss': 1.2271, 'grad_norm': 0.3655549883842468, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7542626857757568, 'eval_runtime': 6.3993, 'eval_samples_per_second': 156.266, 'eval_steps_per_second': 9.845, 'epoch': 0.96}
{'loss': 1.1797, 'grad_norm': 0.38193392753601074, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7540123462677002, 'eval_runtime': 6.4068, 'eval_samples_per_second': 156.085, 'eval_steps_per_second': 9.833, 'epoch': 1.0}
{'train_runtime': 339.0491, 'train_samples_per_second': 29.491, 'train_steps_per_second': 1.843, 'train_loss': 1.3649595367431642, 'epoch': 1.0}
train_results:  {'eval_loss': [2.6958701610565186, 1.903469443321228, 1.758485198020935, 1.75737726688385, 1.7488986253738403, 1.729967474937439, 1.7329598665237427, 1.7382934093475342, 1.7261240482330322, 1.722583293914795, 1.7371045351028442, 1.6963603496551514, 1.7272804975509644, 1.7256667613983154, 1.7143996953964233, 1.7249618768692017, 1.7304203510284424, 1.7173148393630981, 1.7334916591644287, 1.7369639873504639, 1.72784423828125, 1.7386749982833862, 1.744436502456665, 1.7542626857757568, 1.7540123462677002], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.6958701610565186, 1.903469443321228, 1.758485198020935, 1.75737726688385, 1.7488986253738403, 1.729967474937439, 1.7329598665237427, 1.7382934093475342, 1.7261240482330322, 1.722583293914795, 1.7371045351028442, 1.6963603496551514, 1.7272804975509644, 1.7256667613983154, 1.7143996953964233, 1.7249618768692017, 1.7304203510284424, 1.7173148393630981, 1.7334916591644287, 1.7369639873504639, 1.72784423828125, 1.7386749982833862, 1.744436502456665, 1.7542626857757568, 1.7540123462677002]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.313380241394043
current iteration best possible eval_loss (full train run):  -1.7540123462677002
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896, -1.0760297775268555, -1.0760324001312256, -1.0760294198989868, -1.0860702991485596, -1.0760269165039062, -1.313380241394043]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4828 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -1.0168273839207018
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -1.0168273839207018
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -1.0168273839207018
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -1.0168273839207018
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -1.0168273839207018
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.3498, dtype=torch.float64), 0, tensor(0.2220, dtype=torch.float64), 0, tensor(0.4281, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 6, 0.0, 47.33400256826737, 1]
normalized proposed parameters for next round by BO: [tensor(6.6860e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3498, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2220, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4281, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.2944e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0438, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9861, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.35
  sciq: 0
  triviaqa: 0.222
  truthfulqa_gen: 0
  wikitext: 0.428
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (6,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.33400256826737,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  6
lora dropout:  0.0
lora alpha:  47.33400256826737
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 12,189,696 || all params: 8,042,450,944 || trainable%: 0.1516
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0161, 'grad_norm': 2.5157811641693115, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.258434772491455, 'eval_runtime': 7.9368, 'eval_samples_per_second': 125.995, 'eval_steps_per_second': 7.938, 'epoch': 0.04}
{'loss': 1.9131, 'grad_norm': 2.60927414894104, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.4333231449127197, 'eval_runtime': 7.9023, 'eval_samples_per_second': 126.546, 'eval_steps_per_second': 7.972, 'epoch': 0.08}
{'loss': 1.8493, 'grad_norm': 2.7132434844970703, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.311929225921631, 'eval_runtime': 7.9382, 'eval_samples_per_second': 125.973, 'eval_steps_per_second': 7.936, 'epoch': 0.12}
{'loss': 1.8475, 'grad_norm': 1.8648157119750977, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.2678768634796143, 'eval_runtime': 7.9413, 'eval_samples_per_second': 125.924, 'eval_steps_per_second': 7.933, 'epoch': 0.16}
{'loss': 1.7741, 'grad_norm': 1.9479354619979858, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.2349212169647217, 'eval_runtime': 7.9328, 'eval_samples_per_second': 126.06, 'eval_steps_per_second': 7.942, 'epoch': 0.2}
{'loss': 1.7974, 'grad_norm': 2.9537575244903564, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.2811810970306396, 'eval_runtime': 7.9334, 'eval_samples_per_second': 126.05, 'eval_steps_per_second': 7.941, 'epoch': 0.24}
{'loss': 1.7631, 'grad_norm': 1.7860902547836304, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.360121965408325, 'eval_runtime': 7.9605, 'eval_samples_per_second': 125.62, 'eval_steps_per_second': 7.914, 'epoch': 0.28}
{'loss': 1.7933, 'grad_norm': 0.9959511160850525, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.3530869483947754, 'eval_runtime': 7.9522, 'eval_samples_per_second': 125.751, 'eval_steps_per_second': 7.922, 'epoch': 0.32}
{'loss': 1.7575, 'grad_norm': 1.2810410261154175, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.4034643173217773, 'eval_runtime': 8.0017, 'eval_samples_per_second': 124.974, 'eval_steps_per_second': 7.873, 'epoch': 0.36}
{'loss': 1.7317, 'grad_norm': 1.644871473312378, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.3779923915863037, 'eval_runtime': 8.038, 'eval_samples_per_second': 124.409, 'eval_steps_per_second': 7.838, 'epoch': 0.4}
{'loss': 1.8022, 'grad_norm': 1.258037805557251, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.4163246154785156, 'eval_runtime': 8.0271, 'eval_samples_per_second': 124.578, 'eval_steps_per_second': 7.848, 'epoch': 0.44}
{'loss': 1.744, 'grad_norm': 1.0427285432815552, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.368391752243042, 'eval_runtime': 8.0228, 'eval_samples_per_second': 124.645, 'eval_steps_per_second': 7.853, 'epoch': 0.48}
{'loss': 1.7499, 'grad_norm': 1.36707603931427, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.308271884918213, 'eval_runtime': 8.0198, 'eval_samples_per_second': 124.692, 'eval_steps_per_second': 7.856, 'epoch': 0.52}
{'loss': 1.7417, 'grad_norm': 1.201526165008545, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.3528130054473877, 'eval_runtime': 8.0168, 'eval_samples_per_second': 124.738, 'eval_steps_per_second': 7.858, 'epoch': 0.56}
{'loss': 1.7565, 'grad_norm': 1.2865889072418213, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.3550515174865723, 'eval_runtime': 8.0095, 'eval_samples_per_second': 124.851, 'eval_steps_per_second': 7.866, 'epoch': 0.6}
{'loss': 1.7564, 'grad_norm': 2.2125377655029297, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.4243791103363037, 'eval_runtime': 7.9548, 'eval_samples_per_second': 125.71, 'eval_steps_per_second': 7.92, 'epoch': 0.64}
{'loss': 1.7593, 'grad_norm': 1.2918728590011597, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.3848860263824463, 'eval_runtime': 7.923, 'eval_samples_per_second': 126.214, 'eval_steps_per_second': 7.951, 'epoch': 0.68}
{'loss': 1.7858, 'grad_norm': 1.3224238157272339, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.3765251636505127, 'eval_runtime': 7.9229, 'eval_samples_per_second': 126.216, 'eval_steps_per_second': 7.952, 'epoch': 0.72}
{'loss': 1.7138, 'grad_norm': 1.0641109943389893, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.405292510986328, 'eval_runtime': 7.9227, 'eval_samples_per_second': 126.219, 'eval_steps_per_second': 7.952, 'epoch': 0.76}
{'loss': 1.6986, 'grad_norm': 2.864219903945923, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.432234287261963, 'eval_runtime': 7.9299, 'eval_samples_per_second': 126.105, 'eval_steps_per_second': 7.945, 'epoch': 0.8}
{'loss': 1.6705, 'grad_norm': 1.0849612951278687, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.4370551109313965, 'eval_runtime': 7.9257, 'eval_samples_per_second': 126.171, 'eval_steps_per_second': 7.949, 'epoch': 0.84}
{'loss': 1.7444, 'grad_norm': 1.434691309928894, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.459476947784424, 'eval_runtime': 7.9272, 'eval_samples_per_second': 126.148, 'eval_steps_per_second': 7.947, 'epoch': 0.88}
{'loss': 1.7251, 'grad_norm': 1.4085173606872559, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.451949119567871, 'eval_runtime': 7.9272, 'eval_samples_per_second': 126.148, 'eval_steps_per_second': 7.947, 'epoch': 0.92}
{'loss': 1.6878, 'grad_norm': 1.5498558282852173, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.4548351764678955, 'eval_runtime': 7.9353, 'eval_samples_per_second': 126.02, 'eval_steps_per_second': 7.939, 'epoch': 0.96}
{'loss': 1.6903, 'grad_norm': 1.3934651613235474, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.4629452228546143, 'eval_runtime': 7.9408, 'eval_samples_per_second': 125.931, 'eval_steps_per_second': 7.934, 'epoch': 1.0}
{'train_runtime': 451.005, 'train_samples_per_second': 22.17, 'train_steps_per_second': 1.386, 'train_loss': 1.8107688659667969, 'epoch': 1.0}
train_results:  {'eval_loss': [2.258434772491455, 2.4333231449127197, 2.311929225921631, 2.2678768634796143, 2.2349212169647217, 2.2811810970306396, 2.360121965408325, 2.3530869483947754, 2.4034643173217773, 2.3779923915863037, 2.4163246154785156, 2.368391752243042, 2.308271884918213, 2.3528130054473877, 2.3550515174865723, 2.4243791103363037, 2.3848860263824463, 2.3765251636505127, 2.405292510986328, 2.432234287261963, 2.4370551109313965, 2.459476947784424, 2.451949119567871, 2.4548351764678955, 2.4629452228546143], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.258434772491455, 2.4333231449127197, 2.311929225921631, 2.2678768634796143, 2.2349212169647217, 2.2811810970306396, 2.360121965408325, 2.3530869483947754, 2.4034643173217773, 2.3779923915863037, 2.4163246154785156, 2.368391752243042, 2.308271884918213, 2.3528130054473877, 2.3550515174865723, 2.4243791103363037, 2.3848860263824463, 2.3765251636505127, 2.405292510986328, 2.432234287261963, 2.4370551109313965, 2.459476947784424, 2.451949119567871, 2.4548351764678955, 2.4629452228546143]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760276317596436
current iteration best possible eval_loss (full train run):  -2.4629452228546143
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896, -1.0760297775268555, -1.0760324001312256, -1.0760294198989868, -1.0860702991485596, -1.0760269165039062, -1.313380241394043, -1.0760276317596436]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.7762 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.0292631156816585
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.0292631156816585
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.0292631156816585
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.0292631156816585
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.0292631156816585
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7723, dtype=torch.float64), 0, 0, 0, tensor(0.2277, dtype=torch.float64), 0, 0, 0, 0, 32, 1, 1, 1, 1, 1, 34, 0.0, 39.39488683659286, 1]
normalized proposed parameters for next round by BO: [tensor(0.7723, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.5416e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2277, dtype=torch.float64), tensor(3.5946e-18, dtype=torch.float64), tensor(1.3500e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2685, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8207, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.772
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.228
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (34,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (39.39488683659286,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  34
lora dropout:  0.0
lora alpha:  39.39488683659286
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 74,645,504 || all params: 8,104,906,752 || trainable%: 0.9210
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8184, 'grad_norm': 0.8208296298980713, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6325339078903198, 'eval_runtime': 8.2685, 'eval_samples_per_second': 120.94, 'eval_steps_per_second': 7.619, 'epoch': 0.04}
{'loss': 0.9982, 'grad_norm': 0.5214242339134216, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6294206380844116, 'eval_runtime': 8.231, 'eval_samples_per_second': 121.492, 'eval_steps_per_second': 7.654, 'epoch': 0.08}
{'loss': 0.912, 'grad_norm': 0.4631560444831848, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7107211351394653, 'eval_runtime': 8.2371, 'eval_samples_per_second': 121.402, 'eval_steps_per_second': 7.648, 'epoch': 0.12}
{'loss': 0.8751, 'grad_norm': 0.38623929023742676, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6266374588012695, 'eval_runtime': 8.1995, 'eval_samples_per_second': 121.959, 'eval_steps_per_second': 7.683, 'epoch': 0.16}
{'loss': 0.868, 'grad_norm': 0.41341087222099304, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6093238592147827, 'eval_runtime': 8.1918, 'eval_samples_per_second': 122.073, 'eval_steps_per_second': 7.691, 'epoch': 0.2}
{'loss': 0.8438, 'grad_norm': 0.3951266407966614, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6201828718185425, 'eval_runtime': 8.1909, 'eval_samples_per_second': 122.087, 'eval_steps_per_second': 7.691, 'epoch': 0.24}
{'loss': 0.8692, 'grad_norm': 0.436356782913208, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.642429232597351, 'eval_runtime': 8.1658, 'eval_samples_per_second': 122.461, 'eval_steps_per_second': 7.715, 'epoch': 0.28}
{'loss': 0.8388, 'grad_norm': 0.400024950504303, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6405848264694214, 'eval_runtime': 8.1703, 'eval_samples_per_second': 122.395, 'eval_steps_per_second': 7.711, 'epoch': 0.32}
{'loss': 0.8445, 'grad_norm': 0.37931904196739197, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6723227500915527, 'eval_runtime': 8.1872, 'eval_samples_per_second': 122.142, 'eval_steps_per_second': 7.695, 'epoch': 0.36}
{'loss': 0.8236, 'grad_norm': 0.4400075078010559, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6755059957504272, 'eval_runtime': 8.2373, 'eval_samples_per_second': 121.399, 'eval_steps_per_second': 7.648, 'epoch': 0.4}
{'loss': 0.8392, 'grad_norm': 0.35977885127067566, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6393311023712158, 'eval_runtime': 8.22, 'eval_samples_per_second': 121.654, 'eval_steps_per_second': 7.664, 'epoch': 0.44}
{'loss': 0.8122, 'grad_norm': 0.45128798484802246, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6427057981491089, 'eval_runtime': 8.2124, 'eval_samples_per_second': 121.767, 'eval_steps_per_second': 7.671, 'epoch': 0.48}
{'loss': 0.7907, 'grad_norm': 0.380840003490448, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6420966386795044, 'eval_runtime': 8.1756, 'eval_samples_per_second': 122.315, 'eval_steps_per_second': 7.706, 'epoch': 0.52}
{'loss': 0.7928, 'grad_norm': 0.3565060496330261, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.728542447090149, 'eval_runtime': 8.1803, 'eval_samples_per_second': 122.245, 'eval_steps_per_second': 7.701, 'epoch': 0.56}
{'loss': 0.7867, 'grad_norm': 0.45065397024154663, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6957693099975586, 'eval_runtime': 8.1738, 'eval_samples_per_second': 122.341, 'eval_steps_per_second': 7.708, 'epoch': 0.6}
{'loss': 0.791, 'grad_norm': 0.4868330657482147, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6640487909317017, 'eval_runtime': 8.1743, 'eval_samples_per_second': 122.334, 'eval_steps_per_second': 7.707, 'epoch': 0.64}
{'loss': 0.7815, 'grad_norm': 0.371122807264328, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6621227264404297, 'eval_runtime': 8.1951, 'eval_samples_per_second': 122.024, 'eval_steps_per_second': 7.688, 'epoch': 0.68}
{'loss': 0.7633, 'grad_norm': 0.4247453212738037, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6677026748657227, 'eval_runtime': 8.1851, 'eval_samples_per_second': 122.173, 'eval_steps_per_second': 7.697, 'epoch': 0.72}
{'loss': 0.7459, 'grad_norm': 0.4257393777370453, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6438220739364624, 'eval_runtime': 8.1904, 'eval_samples_per_second': 122.094, 'eval_steps_per_second': 7.692, 'epoch': 0.76}
{'loss': 0.7481, 'grad_norm': 0.44821926951408386, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.671133041381836, 'eval_runtime': 8.1894, 'eval_samples_per_second': 122.109, 'eval_steps_per_second': 7.693, 'epoch': 0.8}
{'loss': 0.7504, 'grad_norm': 0.4189333915710449, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6779230833053589, 'eval_runtime': 8.1765, 'eval_samples_per_second': 122.301, 'eval_steps_per_second': 7.705, 'epoch': 0.84}
{'loss': 0.7425, 'grad_norm': 0.41318395733833313, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7102504968643188, 'eval_runtime': 8.2014, 'eval_samples_per_second': 121.931, 'eval_steps_per_second': 7.682, 'epoch': 0.88}
{'loss': 0.7485, 'grad_norm': 0.451759397983551, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6921396255493164, 'eval_runtime': 8.2469, 'eval_samples_per_second': 121.257, 'eval_steps_per_second': 7.639, 'epoch': 0.92}
{'loss': 0.7356, 'grad_norm': 0.41511255502700806, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7041666507720947, 'eval_runtime': 8.2261, 'eval_samples_per_second': 121.565, 'eval_steps_per_second': 7.659, 'epoch': 0.96}
{'loss': 0.7334, 'grad_norm': 0.4710316061973572, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7101943492889404, 'eval_runtime': 8.2096, 'eval_samples_per_second': 121.808, 'eval_steps_per_second': 7.674, 'epoch': 1.0}
{'train_runtime': 340.8897, 'train_samples_per_second': 29.332, 'train_steps_per_second': 1.833, 'train_loss': 0.8901405975341797, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6325339078903198, 1.6294206380844116, 1.7107211351394653, 1.6266374588012695, 1.6093238592147827, 1.6201828718185425, 1.642429232597351, 1.6405848264694214, 1.6723227500915527, 1.6755059957504272, 1.6393311023712158, 1.6427057981491089, 1.6420966386795044, 1.728542447090149, 1.6957693099975586, 1.6640487909317017, 1.6621227264404297, 1.6677026748657227, 1.6438220739364624, 1.671133041381836, 1.6779230833053589, 1.7102504968643188, 1.6921396255493164, 1.7041666507720947, 1.7101943492889404], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6325339078903198, 1.6294206380844116, 1.7107211351394653, 1.6266374588012695, 1.6093238592147827, 1.6201828718185425, 1.642429232597351, 1.6405848264694214, 1.6723227500915527, 1.6755059957504272, 1.6393311023712158, 1.6427057981491089, 1.6420966386795044, 1.728542447090149, 1.6957693099975586, 1.6640487909317017, 1.6621227264404297, 1.6677026748657227, 1.6438220739364624, 1.671133041381836, 1.6779230833053589, 1.7102504968643188, 1.6921396255493164, 1.7041666507720947, 1.7101943492889404]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760283470153809
current iteration best possible eval_loss (full train run):  -1.7101943492889404
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896, -1.0760297775268555, -1.0760324001312256, -1.0760294198989868, -1.0860702991485596, -1.0760269165039062, -1.313380241394043, -1.0760276317596436, -1.0760283470153809]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.7593 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -1.0222942499602605
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -1.0222942499602605
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -1.0222942499602605
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -1.0222942499602605
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -1.0222942499602605
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.4739, dtype=torch.float64), tensor(0.5222, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 1, 2, 0.0, 37.734750709860755, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0439e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0039, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4739, dtype=torch.float64), tensor(0.5222, dtype=torch.float64), tensor(1.7659e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7861, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.474
  mmlu: 0.522
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (37.734750709860755,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  37.734750709860755
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 4,063,232 || all params: 8,034,324,480 || trainable%: 0.0506
length of training data:  9960
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5855, 'grad_norm': 2.996631383895874, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.98043954372406, 'eval_runtime': 7.9776, 'eval_samples_per_second': 125.351, 'eval_steps_per_second': 7.897, 'epoch': 0.04}
{'loss': 1.6705, 'grad_norm': 3.4247617721557617, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9459906816482544, 'eval_runtime': 7.9586, 'eval_samples_per_second': 125.651, 'eval_steps_per_second': 7.916, 'epoch': 0.08}
{'loss': 1.6894, 'grad_norm': 5.024908065795898, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.863906979560852, 'eval_runtime': 7.9639, 'eval_samples_per_second': 125.567, 'eval_steps_per_second': 7.911, 'epoch': 0.12}
{'loss': 1.6168, 'grad_norm': 2.6487951278686523, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.946035385131836, 'eval_runtime': 7.9745, 'eval_samples_per_second': 125.4, 'eval_steps_per_second': 7.9, 'epoch': 0.16}
{'loss': 1.5443, 'grad_norm': 2.0619685649871826, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.9622106552124023, 'eval_runtime': 7.9685, 'eval_samples_per_second': 125.494, 'eval_steps_per_second': 7.906, 'epoch': 0.2}
{'loss': 1.5695, 'grad_norm': 2.407348871231079, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.9301683902740479, 'eval_runtime': 7.9773, 'eval_samples_per_second': 125.355, 'eval_steps_per_second': 7.897, 'epoch': 0.24}
{'loss': 1.5153, 'grad_norm': 4.2930803298950195, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.9708629846572876, 'eval_runtime': 7.9623, 'eval_samples_per_second': 125.592, 'eval_steps_per_second': 7.912, 'epoch': 0.28}
{'loss': 1.5196, 'grad_norm': 2.198115110397339, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.9079862833023071, 'eval_runtime': 7.9334, 'eval_samples_per_second': 126.049, 'eval_steps_per_second': 7.941, 'epoch': 0.32}
{'loss': 1.4441, 'grad_norm': 1.8777284622192383, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.93121337890625, 'eval_runtime': 7.9381, 'eval_samples_per_second': 125.975, 'eval_steps_per_second': 7.936, 'epoch': 0.36}
{'loss': 1.4403, 'grad_norm': 2.259446144104004, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.9053503274917603, 'eval_runtime': 7.9367, 'eval_samples_per_second': 125.997, 'eval_steps_per_second': 7.938, 'epoch': 0.4}
{'loss': 1.5808, 'grad_norm': 2.479682207107544, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.9472341537475586, 'eval_runtime': 7.9254, 'eval_samples_per_second': 126.176, 'eval_steps_per_second': 7.949, 'epoch': 0.44}
{'loss': 1.5085, 'grad_norm': 2.983779191970825, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.91929292678833, 'eval_runtime': 7.923, 'eval_samples_per_second': 126.214, 'eval_steps_per_second': 7.952, 'epoch': 0.48}
{'loss': 1.458, 'grad_norm': 1.6318128108978271, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.9275012016296387, 'eval_runtime': 7.9272, 'eval_samples_per_second': 126.148, 'eval_steps_per_second': 7.947, 'epoch': 0.52}
{'loss': 1.5334, 'grad_norm': 2.4355244636535645, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.990877389907837, 'eval_runtime': 7.921, 'eval_samples_per_second': 126.246, 'eval_steps_per_second': 7.953, 'epoch': 0.56}
{'loss': 1.4908, 'grad_norm': 2.5282232761383057, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.9673084020614624, 'eval_runtime': 7.9231, 'eval_samples_per_second': 126.214, 'eval_steps_per_second': 7.951, 'epoch': 0.6}
{'loss': 1.4994, 'grad_norm': 2.171712636947632, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 1.971719741821289, 'eval_runtime': 7.9228, 'eval_samples_per_second': 126.218, 'eval_steps_per_second': 7.952, 'epoch': 0.64}
{'loss': 1.5253, 'grad_norm': 2.073920488357544, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 1.9871174097061157, 'eval_runtime': 8.0425, 'eval_samples_per_second': 124.339, 'eval_steps_per_second': 7.833, 'epoch': 0.68}
{'loss': 1.4928, 'grad_norm': 2.007181167602539, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 1.978440761566162, 'eval_runtime': 8.0291, 'eval_samples_per_second': 124.547, 'eval_steps_per_second': 7.846, 'epoch': 0.72}
{'loss': 1.4689, 'grad_norm': 2.0201687812805176, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 1.9912959337234497, 'eval_runtime': 7.9983, 'eval_samples_per_second': 125.026, 'eval_steps_per_second': 7.877, 'epoch': 0.76}
{'loss': 1.5363, 'grad_norm': 2.650463581085205, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 1.968036413192749, 'eval_runtime': 7.965, 'eval_samples_per_second': 125.55, 'eval_steps_per_second': 7.91, 'epoch': 0.8}
{'loss': 1.4509, 'grad_norm': 2.149474620819092, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 1.9739359617233276, 'eval_runtime': 7.9649, 'eval_samples_per_second': 125.551, 'eval_steps_per_second': 7.91, 'epoch': 0.84}
{'loss': 1.4436, 'grad_norm': 2.5784502029418945, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 1.9706984758377075, 'eval_runtime': 7.9648, 'eval_samples_per_second': 125.553, 'eval_steps_per_second': 7.91, 'epoch': 0.88}
{'loss': 1.4802, 'grad_norm': 2.9576709270477295, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 1.9717046022415161, 'eval_runtime': 7.9666, 'eval_samples_per_second': 125.524, 'eval_steps_per_second': 7.908, 'epoch': 0.92}
{'loss': 1.4385, 'grad_norm': 1.7765673398971558, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 1.9719598293304443, 'eval_runtime': 7.961, 'eval_samples_per_second': 125.613, 'eval_steps_per_second': 7.914, 'epoch': 0.96}
{'train_runtime': 430.5548, 'train_samples_per_second': 23.133, 'train_steps_per_second': 1.447, 'train_loss': 1.5566950594441276, 'epoch': 1.0}
train_results:  {'eval_loss': [1.98043954372406, 1.9459906816482544, 1.863906979560852, 1.946035385131836, 1.9622106552124023, 1.9301683902740479, 1.9708629846572876, 1.9079862833023071, 1.93121337890625, 1.9053503274917603, 1.9472341537475586, 1.91929292678833, 1.9275012016296387, 1.990877389907837, 1.9673084020614624, 1.971719741821289, 1.9871174097061157, 1.978440761566162, 1.9912959337234497, 1.968036413192749, 1.9739359617233276, 1.9706984758377075, 1.9717046022415161, 1.9719598293304443], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.98043954372406, 1.9459906816482544, 1.863906979560852, 1.946035385131836, 1.9622106552124023, 1.9301683902740479, 1.9708629846572876, 1.9079862833023071, 1.93121337890625, 1.9053503274917603, 1.9472341537475586, 1.91929292678833, 1.9275012016296387, 1.990877389907837, 1.9673084020614624, 1.971719741821289, 1.9871174097061157, 1.978440761566162, 1.9912959337234497, 1.968036413192749, 1.9739359617233276, 1.9706984758377075, 1.9717046022415161, 1.9719598293304443]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.07602858543396
current iteration best possible eval_loss (full train run):  -1.9719598293304443
max eval_loss so far:  -0.8768962621688843
BO observations:  [-1.0760282278060913, -1.076029658317566, -1.0760287046432495, -1.0760300159454346, -1.0760294198989868, -1.816397786140442, -1.0760276317596436, -1.0746192932128906, -1.4944487810134888, -1.0830825567245483, -1.0938282012939453, -1.0760307312011719, -1.0760270357131958, -1.336314082145691, -1.0760316848754883, -1.0760347843170166, -1.0760278701782227, -1.086806297302246, -1.1429637670516968, -1.1943060159683228, -1.0979005098342896, -1.0760297775268555, -1.0760324001312256, -1.0760294198989868, -1.0860702991485596, -1.0760269165039062, -1.313380241394043, -1.0760276317596436, -1.0760283470153809, -1.07602858543396]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.9743 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3357044458389282, 0.4876623749732971, 0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7566351294517517, 0.44088155031204224, 0.21595293283462524, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7294671535491943, 0.9934723973274231, 0.20285475254058838, 0.606965959072113]  ‚Üí  acq = -1.0379871052318173
X = [0.7454677820205688, 0.4424137473106384, 0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8704531192779541, 0.009343624114990234, 0.477742075920105, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7311053276062012, 0.7334456443786621, 0.34133514761924744, 0.4159647822380066]  ‚Üí  acq = -1.040238750130207
X = [0.6070147752761841, 0.050814270973205566, 0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.06286248564720154, 0.38677525520324707, 0.3574179410934448, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5247937440872192, 0.7738797068595886, 0.6983144283294678, 0.9456675052642822]  ‚Üí  acq = -1.089850439910167
X = [0.05165296792984009, 0.43891578912734985, 0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.34409016370773315, 0.6168457269668579, 0.23861968517303467, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.4990995526313782, 0.5673343539237976, 0.3437628448009491, 0.6652377843856812]  ‚Üí  acq = -1.0440753878413063
X = [0.05172693729400635, 0.2110685110092163, 0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.34519943594932556, 0.5801001787185669, 0.783306360244751, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9125829935073853, 0.5967274308204651, 0.9407432079315186, 0.06500035524368286]  ‚Üí  acq = -1.0412760688222857
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.8545, dtype=torch.float64), 0, 0, tensor(0.1455, dtype=torch.float64), 0, 10, 1, 1, 1, 1, 1, 2, 0.03952501201147892, 26.9308203195817, 0]
normalized proposed parameters for next round by BO: [tensor(3.1839e-18, dtype=torch.float64), tensor(1.8607e-17, dtype=torch.float64), tensor(2.8315e-18, dtype=torch.float64), tensor(1.5169e-18, dtype=torch.float64), tensor(0.8545, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9810e-18, dtype=torch.float64), tensor(0.1455, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3018, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.3953, dtype=torch.float64), tensor(0.5611, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-0.9691433906555176, -0.8967759013175964, -0.8967759013175964, -0.8955367207527161, -0.8860146403312683, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
arc_challenge
evaluation dataset:
data domain:  arc_challenge  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/arc_challenge/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 17, 1, 0, 1, 1, 0, 59, 0.00445603671922955, 41, 0]
Checking history sample input_X_between_0_1:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 0.53125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4609375, 0.044560367192295496, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3072175979614258
Checking history sample input_X:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 22, 1, 1, 1, 0, 0, 61, 0.08406775506720655, 5, 1]
Checking history sample input_X_between_0_1:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 0.6875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4765625, 0.8406775506720655, 0.10416666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1613417863845825
Checking history sample input_X:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 15, 1, 1, 1, 1, 0, 24, 0.05173325143341287, 28, 0]
Checking history sample input_X_between_0_1:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 0.46875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1875, 0.5173325143341286, 0.5833333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1443150043487549
Checking history sample input_X:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 1, 0, 1, 1, 0, 0, 114, 0.021033555465481826, 23, 1]
Checking history sample input_X_between_0_1:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 0.03125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.890625, 0.21033555465481824, 0.4791666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.3097282648086548
Checking history sample input_X:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 27, 1, 1, 1, 1, 1, 70, 0.005963494977880357, 46, 1]
Checking history sample input_X_between_0_1:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 0.05963494977880357, 0.9583333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9743626117706299
Checking history sample input_X:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 12, 1, 0, 1, 0, 0, 3, 0.017954470577765235, 12, 1]
Checking history sample input_X_between_0_1:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0234375, 0.17954470577765233, 0.25, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2079665660858154
Checking history sample input_X:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 18, 1, 0, 0, 0, 1, 5, 0.028541773579282805, 35, 0]
Checking history sample input_X_between_0_1:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 0.5625, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0390625, 0.28541773579282803, 0.7291666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0695475339889526
Checking history sample input_X:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 24, 0, 1, 0, 1, 0, 55, 0.08420649661865921, 22, 0]
Checking history sample input_X_between_0_1:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 0.75, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4296875, 0.8420649661865921, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0792094469070435
Checking history sample input_X:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 30, 1, 0, 1, 1, 0, 5, 0.09059974254780842, 29, 1]
Checking history sample input_X_between_0_1:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 0.9375, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0390625, 0.9059974254780842, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9744297862052917
Checking history sample input_X:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 20, 1, 1, 0, 1, 0, 27, 0.07188580123073206, 22, 0]
Checking history sample input_X_between_0_1:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 0.625, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2109375, 0.7188580123073205, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0874465703964233
Checking history sample input_X:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 8, 0, 0, 0, 1, 1, 31, 0.06288146497812035, 13, 1]
Checking history sample input_X_between_0_1:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2421875, 0.6288146497812034, 0.2708333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0237352848052979
Checking history sample input_X:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 32, 1, 1, 0, 1, 1, 114, 0.08252011949389138, 13, 0]
Checking history sample input_X_between_0_1:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.890625, 0.8252011949389138, 0.2708333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1076443195343018
Checking history sample input_X:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 11, 0, 1, 0, 0, 1, 100, 0.09203832421630076, 16, 1]
Checking history sample input_X_between_0_1:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 0.34375, 0.0, 1.0, 0.0, 0.0, 1.0, 0.78125, 0.9203832421630076, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.139199137687683
Checking history sample input_X:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 31, 1, 0, 1, 1, 1, 60, 0.008025447056787238, 26, 0]
Checking history sample input_X_between_0_1:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 0.96875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.46875, 0.08025447056787237, 0.5416666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2780427932739258
Checking history sample input_X:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 7, 0, 0, 1, 0, 0, 10, 0.06890300584266877, 3, 1]
Checking history sample input_X_between_0_1:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.078125, 0.6890300584266876, 0.0625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.5037378072738647
Checking history sample input_X:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 1, 1, 1, 0, 1, 0, 28, 0.0879909943648135, 16, 0]
Checking history sample input_X_between_0_1:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 0.03125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.21875, 0.879909943648135, 0.3333333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1370042562484741
Checking history sample input_X:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 1, 1, 0, 0, 1, 0, 47, 0.04979780998318395, 37, 0]
Checking history sample input_X_between_0_1:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 0.03125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3671875, 0.4979780998318395, 0.7708333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1642110347747803
Checking history sample input_X:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 30, 1, 0, 1, 0, 0, 86, 0.08085387005284786, 6, 1]
Checking history sample input_X_between_0_1:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 0.9375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.671875, 0.8085387005284785, 0.125, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1513490676879883
Checking history sample input_X:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 31, 0, 1, 1, 1, 1, 84, 0.015776955723827136, 41, 1]
Checking history sample input_X_between_0_1:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 0.96875, 0.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.15776955723827135, 0.8541666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0676783323287964
Checking history sample input_X:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 24, 0, 1, 1, 0, 1, 1, 0.07052967196513893, 22, 0]
Checking history sample input_X_between_0_1:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0078125, 0.7052967196513893, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -0.999065637588501
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8521 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.804328978061676, 0.2572243809700012, 0.35221558809280396, 0.055126965045928955, 0.8987093567848206, 0.865005373954773, 0.039726436138153076, 0.05059099197387695, 0.5459287762641907, 0.4507504105567932, 0.34477442502975464, 0.9984433650970459, 0.15108340978622437, 0.47017592191696167, 0.6401899456977844, 0.38202905654907227, 0.20270711183547974, 0.23371651768684387, 0.20799332857131958]  ‚Üí  acq = -0.7600743590996957
X = [0.22017920017242432, 0.08907556533813477, 0.37584930658340454, 0.9214252233505249, 0.4411628842353821, 0.9225553274154663, 0.08883661031723022, 0.43351733684539795, 0.41293054819107056, 0.7513986229896545, 0.9407190680503845, 0.19978350400924683, 0.7026312947273254, 0.7440809607505798, 0.010994017124176025, 0.33230990171432495, 0.7683675289154053, 0.5885322093963623, 0.5799259543418884]  ‚Üí  acq = -0.7600743592263752
X = [0.998962938785553, 0.6421431303024292, 0.45917171239852905, 0.2907344698905945, 0.42916905879974365, 0.5740932822227478, 0.8816664814949036, 0.6616891622543335, 0.589227020740509, 0.1619473397731781, 0.20203393697738647, 0.186711847782135, 0.9803569912910461, 0.8158959150314331, 0.4016662836074829, 0.6222302913665771, 0.19885820150375366, 0.7004542350769043, 0.8197498917579651]  ‚Üí  acq = -0.7600731391609346
X = [0.5537665486335754, 0.0695311427116394, 0.5686753392219543, 0.9620440602302551, 0.8745047450065613, 0.6767317652702332, 0.35727596282958984, 0.045126140117645264, 0.9644590616226196, 0.8661837577819824, 0.7016618847846985, 0.5097336769104004, 0.4259495139122009, 0.6334235668182373, 0.7822281718254089, 0.46566280722618103, 0.9261835217475891, 0.5518358945846558, 0.14757120609283447]  ‚Üí  acq = -0.7600743580699971
X = [0.18574166297912598, 0.7744396924972534, 0.553118884563446, 0.1432158350944519, 0.869378387928009, 0.23807036876678467, 0.38970625400543213, 0.48977190256118774, 0.5650233030319214, 0.29035109281539917, 0.3970392346382141, 0.22151917219161987, 0.8236859440803528, 0.8826838731765747, 0.8178805708885193, 0.12144239246845245, 0.7264899611473083, 0.4576621949672699, 0.2829708456993103]  ‚Üí  acq = -0.760252228778101
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.3874, dtype=torch.float64), 0, 0, tensor(0.1519, dtype=torch.float64), tensor(0.0575, dtype=torch.float64), tensor(0.1494, dtype=torch.float64), 0, 0, tensor(0.2538, dtype=torch.float64), 28, 0, 0, 0, 1, 1, 2, 0.1, 41.064686754527166, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.3874, dtype=torch.float64), tensor(1.9036e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1519, dtype=torch.float64), tensor(0.0575, dtype=torch.float64), tensor(0.1494, dtype=torch.float64), tensor(1.5896e-17, dtype=torch.float64), tensor(1.8031e-17, dtype=torch.float64), tensor(0.2538, dtype=torch.float64), tensor(0.8749, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8555, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.387
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.152
  triviaqa: 0.057
  truthfulqa_gen: 0.149
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.254

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (41.064686754527166,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  41.064686754527166
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,064,384 || all params: 8,032,325,632 || trainable%: 0.0257
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 3.1005, 'grad_norm': 5.906256675720215, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2392023801803589, 'eval_runtime': 7.4343, 'eval_samples_per_second': 134.512, 'eval_steps_per_second': 8.474, 'epoch': 0.04}
{'loss': 1.1295, 'grad_norm': 4.009129524230957, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9763044118881226, 'eval_runtime': 6.8806, 'eval_samples_per_second': 145.336, 'eval_steps_per_second': 9.156, 'epoch': 0.08}
{'loss': 0.9775, 'grad_norm': 1.9796279668807983, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9050737619400024, 'eval_runtime': 6.9021, 'eval_samples_per_second': 144.884, 'eval_steps_per_second': 9.128, 'epoch': 0.12}
{'loss': 0.9098, 'grad_norm': 1.747287392616272, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8867849111557007, 'eval_runtime': 6.9139, 'eval_samples_per_second': 144.635, 'eval_steps_per_second': 9.112, 'epoch': 0.16}
{'loss': 0.8327, 'grad_norm': 1.3786741495132446, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8930476307868958, 'eval_runtime': 6.9249, 'eval_samples_per_second': 144.406, 'eval_steps_per_second': 9.098, 'epoch': 0.2}
{'loss': 0.8646, 'grad_norm': 1.5573742389678955, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8782660365104675, 'eval_runtime': 6.9067, 'eval_samples_per_second': 144.788, 'eval_steps_per_second': 9.122, 'epoch': 0.24}
{'loss': 0.8332, 'grad_norm': 1.2231409549713135, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8834968209266663, 'eval_runtime': 6.8739, 'eval_samples_per_second': 145.478, 'eval_steps_per_second': 9.165, 'epoch': 0.28}
{'loss': 0.8277, 'grad_norm': 1.336715817451477, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8855417370796204, 'eval_runtime': 6.8793, 'eval_samples_per_second': 145.364, 'eval_steps_per_second': 9.158, 'epoch': 0.32}
{'loss': 0.7939, 'grad_norm': 1.382572054862976, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8897955417633057, 'eval_runtime': 6.8591, 'eval_samples_per_second': 145.791, 'eval_steps_per_second': 9.185, 'epoch': 0.36}
{'loss': 0.7996, 'grad_norm': 1.7221460342407227, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9107999205589294, 'eval_runtime': 6.8625, 'eval_samples_per_second': 145.72, 'eval_steps_per_second': 9.18, 'epoch': 0.4}
{'loss': 0.7988, 'grad_norm': 1.6738719940185547, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8960902094841003, 'eval_runtime': 6.8472, 'eval_samples_per_second': 146.046, 'eval_steps_per_second': 9.201, 'epoch': 0.44}
{'loss': 0.7754, 'grad_norm': 1.606101393699646, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9112016558647156, 'eval_runtime': 6.8505, 'eval_samples_per_second': 145.975, 'eval_steps_per_second': 9.196, 'epoch': 0.48}
{'loss': 0.7788, 'grad_norm': 1.3193551301956177, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8931325078010559, 'eval_runtime': 6.8566, 'eval_samples_per_second': 145.845, 'eval_steps_per_second': 9.188, 'epoch': 0.52}
{'loss': 0.7564, 'grad_norm': 1.7556453943252563, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9077271819114685, 'eval_runtime': 6.8512, 'eval_samples_per_second': 145.96, 'eval_steps_per_second': 9.195, 'epoch': 0.56}
{'loss': 0.7399, 'grad_norm': 1.490064263343811, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9208702445030212, 'eval_runtime': 6.8531, 'eval_samples_per_second': 145.92, 'eval_steps_per_second': 9.193, 'epoch': 0.6}
{'loss': 0.7281, 'grad_norm': 1.8723584413528442, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9347840547561646, 'eval_runtime': 6.8634, 'eval_samples_per_second': 145.701, 'eval_steps_per_second': 9.179, 'epoch': 0.64}
{'loss': 0.7545, 'grad_norm': 1.6341301202774048, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9106138348579407, 'eval_runtime': 6.8647, 'eval_samples_per_second': 145.673, 'eval_steps_per_second': 9.177, 'epoch': 0.68}
{'loss': 0.7159, 'grad_norm': 2.5681838989257812, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9312716126441956, 'eval_runtime': 6.8723, 'eval_samples_per_second': 145.511, 'eval_steps_per_second': 9.167, 'epoch': 0.72}
{'loss': 0.7282, 'grad_norm': 2.063337564468384, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9353755116462708, 'eval_runtime': 6.8529, 'eval_samples_per_second': 145.923, 'eval_steps_per_second': 9.193, 'epoch': 0.76}
{'loss': 0.7277, 'grad_norm': 2.2153995037078857, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.932441771030426, 'eval_runtime': 6.8576, 'eval_samples_per_second': 145.825, 'eval_steps_per_second': 9.187, 'epoch': 0.8}
{'loss': 0.7026, 'grad_norm': 1.6715431213378906, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9462592601776123, 'eval_runtime': 6.8621, 'eval_samples_per_second': 145.729, 'eval_steps_per_second': 9.181, 'epoch': 0.84}
{'loss': 0.6957, 'grad_norm': 2.386091947555542, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.932301938533783, 'eval_runtime': 6.8667, 'eval_samples_per_second': 145.631, 'eval_steps_per_second': 9.175, 'epoch': 0.88}
{'loss': 0.7297, 'grad_norm': 1.984342098236084, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.943880021572113, 'eval_runtime': 6.8568, 'eval_samples_per_second': 145.841, 'eval_steps_per_second': 9.188, 'epoch': 0.92}
{'loss': 0.6825, 'grad_norm': 1.5035945177078247, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.944968044757843, 'eval_runtime': 6.86, 'eval_samples_per_second': 145.772, 'eval_steps_per_second': 9.184, 'epoch': 0.96}
{'loss': 0.713, 'grad_norm': 1.8566498756408691, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9445481896400452, 'eval_runtime': 6.8648, 'eval_samples_per_second': 145.67, 'eval_steps_per_second': 9.177, 'epoch': 1.0}
{'train_runtime': 315.3862, 'train_samples_per_second': 31.698, 'train_steps_per_second': 1.982, 'train_loss': 0.8838532348632813, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2392023801803589, 0.9763044118881226, 0.9050737619400024, 0.8867849111557007, 0.8930476307868958, 0.8782660365104675, 0.8834968209266663, 0.8855417370796204, 0.8897955417633057, 0.9107999205589294, 0.8960902094841003, 0.9112016558647156, 0.8931325078010559, 0.9077271819114685, 0.9208702445030212, 0.9347840547561646, 0.9106138348579407, 0.9312716126441956, 0.9353755116462708, 0.932441771030426, 0.9462592601776123, 0.932301938533783, 0.943880021572113, 0.944968044757843, 0.9445481896400452], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2392023801803589, 0.9763044118881226, 0.9050737619400024, 0.8867849111557007, 0.8930476307868958, 0.8782660365104675, 0.8834968209266663, 0.8855417370796204, 0.8897955417633057, 0.9107999205589294, 0.8960902094841003, 0.9112016558647156, 0.8931325078010559, 0.9077271819114685, 0.9208702445030212, 0.9347840547561646, 0.9106138348579407, 0.9312716126441956, 0.9353755116462708, 0.932441771030426, 0.9462592601776123, 0.932301938533783, 0.943880021572113, 0.944968044757843, 0.9445481896400452]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760282278060913
current iteration best possible eval_loss (full train run):  -0.9445481896400452
max eval_loss so far:  -0.9445481896400452
BO observations:  [-1.0760282278060913]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.7722 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.8569920058015276
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.8572813438380198
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.857053766858326
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.8570605714759383
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.855987689519695
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4200, dtype=torch.float64), 0, 0, tensor(0.3059, dtype=torch.float64), tensor(0.0319, dtype=torch.float64), 0, 0, tensor(0.2339, dtype=torch.float64), 25, 0, 0, 0, 1, 1, 2, 0.1, 42.93653053811343, 1]
normalized proposed parameters for next round by BO: [tensor(1.5229e-18, dtype=torch.float64), tensor(0.4200, dtype=torch.float64), tensor(0.0084, dtype=torch.float64), tensor(4.3694e-18, dtype=torch.float64), tensor(0.3059, dtype=torch.float64), tensor(0.0319, dtype=torch.float64), tensor(6.0667e-19, dtype=torch.float64), tensor(5.7134e-19, dtype=torch.float64), tensor(0.2339, dtype=torch.float64), tensor(0.7830, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8945, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.42
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.306
  truthfulqa_gen: 0.032
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.234

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (42.93653053811343,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  42.93653053811343
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,843,200 || all params: 8,032,104,448 || trainable%: 0.0229
length of training data:  9913
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.3257, 'grad_norm': 5.387716770172119, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3890776634216309, 'eval_runtime': 6.7201, 'eval_samples_per_second': 148.808, 'eval_steps_per_second': 9.375, 'epoch': 0.04}
{'loss': 1.1079, 'grad_norm': 2.974156618118286, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9742896556854248, 'eval_runtime': 6.7319, 'eval_samples_per_second': 148.546, 'eval_steps_per_second': 9.358, 'epoch': 0.08}
{'loss': 0.9564, 'grad_norm': 1.6180295944213867, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 0.9383403658866882, 'eval_runtime': 6.7569, 'eval_samples_per_second': 147.997, 'eval_steps_per_second': 9.324, 'epoch': 0.12}
{'loss': 0.8973, 'grad_norm': 1.8460440635681152, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.8949340581893921, 'eval_runtime': 6.7649, 'eval_samples_per_second': 147.823, 'eval_steps_per_second': 9.313, 'epoch': 0.16}
{'loss': 0.893, 'grad_norm': 1.2651618719100952, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.8791809678077698, 'eval_runtime': 6.7525, 'eval_samples_per_second': 148.093, 'eval_steps_per_second': 9.33, 'epoch': 0.2}
{'loss': 0.8718, 'grad_norm': 1.2157702445983887, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.8776472210884094, 'eval_runtime': 6.7642, 'eval_samples_per_second': 147.837, 'eval_steps_per_second': 9.314, 'epoch': 0.24}
{'loss': 0.8631, 'grad_norm': 1.4210710525512695, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.8783763647079468, 'eval_runtime': 6.7636, 'eval_samples_per_second': 147.849, 'eval_steps_per_second': 9.315, 'epoch': 0.28}
{'loss': 0.8462, 'grad_norm': 1.1301554441452026, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.8849316835403442, 'eval_runtime': 6.7661, 'eval_samples_per_second': 147.797, 'eval_steps_per_second': 9.311, 'epoch': 0.32}
{'loss': 0.8639, 'grad_norm': 1.1108770370483398, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.8764601349830627, 'eval_runtime': 6.7646, 'eval_samples_per_second': 147.827, 'eval_steps_per_second': 9.313, 'epoch': 0.36}
{'loss': 0.8491, 'grad_norm': 1.0787104368209839, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.8818644881248474, 'eval_runtime': 6.7727, 'eval_samples_per_second': 147.652, 'eval_steps_per_second': 9.302, 'epoch': 0.4}
{'loss': 0.8435, 'grad_norm': 1.343459963798523, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.9214473366737366, 'eval_runtime': 6.7754, 'eval_samples_per_second': 147.593, 'eval_steps_per_second': 9.298, 'epoch': 0.44}
{'loss': 0.844, 'grad_norm': 1.2020305395126343, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.8888022303581238, 'eval_runtime': 6.7665, 'eval_samples_per_second': 147.787, 'eval_steps_per_second': 9.311, 'epoch': 0.48}
{'loss': 0.8044, 'grad_norm': 1.3462514877319336, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.8954575657844543, 'eval_runtime': 6.7778, 'eval_samples_per_second': 147.541, 'eval_steps_per_second': 9.295, 'epoch': 0.52}
{'loss': 0.8027, 'grad_norm': 1.3957208395004272, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.889985978603363, 'eval_runtime': 6.7705, 'eval_samples_per_second': 147.699, 'eval_steps_per_second': 9.305, 'epoch': 0.56}
{'loss': 0.7761, 'grad_norm': 1.3371226787567139, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.9124865531921387, 'eval_runtime': 6.7748, 'eval_samples_per_second': 147.605, 'eval_steps_per_second': 9.299, 'epoch': 0.6}
{'loss': 0.8081, 'grad_norm': 1.8701937198638916, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.9041829705238342, 'eval_runtime': 6.7669, 'eval_samples_per_second': 147.777, 'eval_steps_per_second': 9.31, 'epoch': 0.65}
{'loss': 0.8151, 'grad_norm': 1.2142300605773926, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.9041794538497925, 'eval_runtime': 6.772, 'eval_samples_per_second': 147.668, 'eval_steps_per_second': 9.303, 'epoch': 0.69}
{'loss': 0.7944, 'grad_norm': 1.9201442003250122, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.9150054454803467, 'eval_runtime': 6.7651, 'eval_samples_per_second': 147.818, 'eval_steps_per_second': 9.313, 'epoch': 0.73}
{'loss': 0.7956, 'grad_norm': 1.371394157409668, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.9179884195327759, 'eval_runtime': 6.7681, 'eval_samples_per_second': 147.751, 'eval_steps_per_second': 9.308, 'epoch': 0.77}
{'loss': 0.757, 'grad_norm': 1.6932932138442993, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.9293354749679565, 'eval_runtime': 6.7686, 'eval_samples_per_second': 147.741, 'eval_steps_per_second': 9.308, 'epoch': 0.81}
{'loss': 0.7856, 'grad_norm': 1.5764250755310059, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.9190632700920105, 'eval_runtime': 6.8455, 'eval_samples_per_second': 146.082, 'eval_steps_per_second': 9.203, 'epoch': 0.85}
{'loss': 0.768, 'grad_norm': 1.4388186931610107, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.9216352701187134, 'eval_runtime': 6.8537, 'eval_samples_per_second': 145.907, 'eval_steps_per_second': 9.192, 'epoch': 0.89}
{'loss': 0.7586, 'grad_norm': 1.4664641618728638, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.9287848472595215, 'eval_runtime': 6.8825, 'eval_samples_per_second': 145.296, 'eval_steps_per_second': 9.154, 'epoch': 0.93}
{'loss': 0.7776, 'grad_norm': 1.5540673732757568, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.931367814540863, 'eval_runtime': 6.8632, 'eval_samples_per_second': 145.704, 'eval_steps_per_second': 9.179, 'epoch': 0.97}
{'train_runtime': 378.4944, 'train_samples_per_second': 26.191, 'train_steps_per_second': 1.638, 'train_loss': 0.8956697156352381, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3890776634216309, 0.9742896556854248, 0.9383403658866882, 0.8949340581893921, 0.8791809678077698, 0.8776472210884094, 0.8783763647079468, 0.8849316835403442, 0.8764601349830627, 0.8818644881248474, 0.9214473366737366, 0.8888022303581238, 0.8954575657844543, 0.889985978603363, 0.9124865531921387, 0.9041829705238342, 0.9041794538497925, 0.9150054454803467, 0.9179884195327759, 0.9293354749679565, 0.9190632700920105, 0.9216352701187134, 0.9287848472595215, 0.931367814540863], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.3890776634216309, 0.9742896556854248, 0.9383403658866882, 0.8949340581893921, 0.8791809678077698, 0.8776472210884094, 0.8783763647079468, 0.8849316835403442, 0.8764601349830627, 0.8818644881248474, 0.9214473366737366, 0.8888022303581238, 0.8954575657844543, 0.889985978603363, 0.9124865531921387, 0.9041829705238342, 0.9041794538497925, 0.9150054454803467, 0.9179884195327759, 0.9293354749679565, 0.9190632700920105, 0.9216352701187134, 0.9287848472595215, 0.931367814540863]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760291814804077
current iteration best possible eval_loss (full train run):  -0.931367814540863
max eval_loss so far:  -0.931367814540863
BO observations:  [-1.0760282278060913, -1.0760291814804077]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 18.5659 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5931183099746704, 0.48910677433013916, 0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.43514037132263184, 0.4408547878265381, 0.28891366720199585, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7229896187782288, 0.5257965922355652, 0.7089747190475464, 0.6451549530029297]  ‚Üí  acq = -0.897747332024073
X = [0.8045198321342468, 0.5732041597366333, 0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5429293513298035, 0.6168438792228699, 0.6032367944717407, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.042392924427986145, 0.2894328832626343, 0.9503340721130371, 0.8085587620735168]  ‚Üí  acq = -0.9024533668104678
X = [0.18200689554214478, 0.36882972717285156, 0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.7035383582115173, 0.5326343774795532, 0.1735246777534485, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.2245919555425644, 0.050669074058532715, 0.8647359609603882, 0.040459275245666504]  ‚Üí  acq = -0.9024571722619756
X = [0.6313091516494751, 0.9872360825538635, 0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.09140855073928833, 0.0616917610168457, 0.16755545139312744, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.26512572169303894, 0.2105121612548828, 0.8316733837127686, 0.5374197959899902]  ‚Üí  acq = -0.9024533668356322
X = [0.04147899150848389, 0.4950082302093506, 0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6635695695877075, 0.1939259171485901, 0.9576328992843628, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7788231372833252, 0.13258826732635498, 0.11039420962333679, 0.1842997670173645]  ‚Üí  acq = -0.9024533668344313
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3578, dtype=torch.float64), 0, 0, tensor(0.0136, dtype=torch.float64), tensor(0.5918, dtype=torch.float64), tensor(0.0368, dtype=torch.float64), 0, 0, 0, 30, 1, 0, 0, 0, 1, 2, 0.09986953930008159, 40.78741890582863, 0]
normalized proposed parameters for next round by BO: [tensor(0.3578, dtype=torch.float64), tensor(2.1137e-18, dtype=torch.float64), tensor(1.0057e-17, dtype=torch.float64), tensor(0.0136, dtype=torch.float64), tensor(0.5918, dtype=torch.float64), tensor(0.0368, dtype=torch.float64), tensor(1.1351e-17, dtype=torch.float64), tensor(8.4397e-18, dtype=torch.float64), tensor(1.3486e-18, dtype=torch.float64), tensor(0.9357, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.9987, dtype=torch.float64), tensor(0.8497, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.358
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.014
  triviaqa: 0.592
  truthfulqa_gen: 0.037
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.09986953930008159,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (40.78741890582863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  2
lora dropout:  0.09986953930008159
lora alpha:  40.78741890582863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,597,440 || all params: 8,031,858,688 || trainable%: 0.0199
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.7043, 'grad_norm': 4.8343729972839355, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8891319036483765, 'eval_runtime': 6.9471, 'eval_samples_per_second': 143.946, 'eval_steps_per_second': 9.069, 'epoch': 0.04}
{'loss': 1.4618, 'grad_norm': 3.680785894393921, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.661047101020813, 'eval_runtime': 6.7175, 'eval_samples_per_second': 148.864, 'eval_steps_per_second': 9.378, 'epoch': 0.08}
{'loss': 1.1452, 'grad_norm': 1.3840361833572388, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5356553792953491, 'eval_runtime': 6.7188, 'eval_samples_per_second': 148.836, 'eval_steps_per_second': 9.377, 'epoch': 0.12}
{'loss': 0.9791, 'grad_norm': 1.141502022743225, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.598110318183899, 'eval_runtime': 6.7109, 'eval_samples_per_second': 149.012, 'eval_steps_per_second': 9.388, 'epoch': 0.16}
{'loss': 0.9484, 'grad_norm': 1.1338039636611938, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5818679332733154, 'eval_runtime': 6.7127, 'eval_samples_per_second': 148.972, 'eval_steps_per_second': 9.385, 'epoch': 0.2}
{'loss': 0.9413, 'grad_norm': 1.2051191329956055, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5052446126937866, 'eval_runtime': 6.7202, 'eval_samples_per_second': 148.805, 'eval_steps_per_second': 9.375, 'epoch': 0.24}
{'loss': 0.9255, 'grad_norm': 1.1786785125732422, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4864665269851685, 'eval_runtime': 6.7254, 'eval_samples_per_second': 148.691, 'eval_steps_per_second': 9.368, 'epoch': 0.28}
{'loss': 0.918, 'grad_norm': 1.0885682106018066, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5257946252822876, 'eval_runtime': 6.7084, 'eval_samples_per_second': 149.066, 'eval_steps_per_second': 9.391, 'epoch': 0.32}
{'loss': 0.9355, 'grad_norm': 1.08489990234375, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.524652123451233, 'eval_runtime': 6.6939, 'eval_samples_per_second': 149.39, 'eval_steps_per_second': 9.412, 'epoch': 0.36}
{'loss': 0.9142, 'grad_norm': 1.0371583700180054, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5525981187820435, 'eval_runtime': 6.6893, 'eval_samples_per_second': 149.493, 'eval_steps_per_second': 9.418, 'epoch': 0.4}
{'loss': 0.9037, 'grad_norm': 1.1328212022781372, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6060882806777954, 'eval_runtime': 6.6928, 'eval_samples_per_second': 149.414, 'eval_steps_per_second': 9.413, 'epoch': 0.44}
{'loss': 0.9141, 'grad_norm': 1.056217908859253, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6330714225769043, 'eval_runtime': 6.6872, 'eval_samples_per_second': 149.54, 'eval_steps_per_second': 9.421, 'epoch': 0.48}
{'loss': 0.9012, 'grad_norm': 1.110984444618225, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5878596305847168, 'eval_runtime': 6.6937, 'eval_samples_per_second': 149.394, 'eval_steps_per_second': 9.412, 'epoch': 0.52}
{'loss': 0.9137, 'grad_norm': 1.1933873891830444, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5589054822921753, 'eval_runtime': 6.6954, 'eval_samples_per_second': 149.355, 'eval_steps_per_second': 9.409, 'epoch': 0.56}
{'loss': 0.9169, 'grad_norm': 1.2160440683364868, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.553062915802002, 'eval_runtime': 6.6869, 'eval_samples_per_second': 149.547, 'eval_steps_per_second': 9.421, 'epoch': 0.6}
{'loss': 0.9185, 'grad_norm': 1.1889969110488892, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5188796520233154, 'eval_runtime': 6.684, 'eval_samples_per_second': 149.611, 'eval_steps_per_second': 9.425, 'epoch': 0.64}
{'loss': 0.9004, 'grad_norm': 1.2758456468582153, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5329731702804565, 'eval_runtime': 6.6913, 'eval_samples_per_second': 149.449, 'eval_steps_per_second': 9.415, 'epoch': 0.68}
{'loss': 0.9092, 'grad_norm': 1.1133310794830322, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5458850860595703, 'eval_runtime': 6.6969, 'eval_samples_per_second': 149.324, 'eval_steps_per_second': 9.407, 'epoch': 0.72}
{'loss': 0.9098, 'grad_norm': 1.011205792427063, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5391148328781128, 'eval_runtime': 6.6884, 'eval_samples_per_second': 149.512, 'eval_steps_per_second': 9.419, 'epoch': 0.76}
{'loss': 0.8755, 'grad_norm': 1.1361262798309326, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5454014539718628, 'eval_runtime': 6.6771, 'eval_samples_per_second': 149.766, 'eval_steps_per_second': 9.435, 'epoch': 0.8}
{'loss': 0.889, 'grad_norm': 1.1542234420776367, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5474541187286377, 'eval_runtime': 6.6518, 'eval_samples_per_second': 150.335, 'eval_steps_per_second': 9.471, 'epoch': 0.84}
{'loss': 0.8811, 'grad_norm': 1.1510893106460571, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.547232747077942, 'eval_runtime': 6.65, 'eval_samples_per_second': 150.376, 'eval_steps_per_second': 9.474, 'epoch': 0.88}
{'loss': 0.8748, 'grad_norm': 1.0956552028656006, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5539569854736328, 'eval_runtime': 6.6464, 'eval_samples_per_second': 150.457, 'eval_steps_per_second': 9.479, 'epoch': 0.92}
{'loss': 0.8764, 'grad_norm': 1.189331293106079, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5564852952957153, 'eval_runtime': 6.6499, 'eval_samples_per_second': 150.378, 'eval_steps_per_second': 9.474, 'epoch': 0.96}
{'loss': 0.8671, 'grad_norm': 1.1262449026107788, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5604076385498047, 'eval_runtime': 6.6605, 'eval_samples_per_second': 150.14, 'eval_steps_per_second': 9.459, 'epoch': 1.0}
{'train_runtime': 272.2251, 'train_samples_per_second': 36.727, 'train_steps_per_second': 2.296, 'train_loss': 1.052989358520508, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8891319036483765, 1.661047101020813, 1.5356553792953491, 1.598110318183899, 1.5818679332733154, 1.5052446126937866, 1.4864665269851685, 1.5257946252822876, 1.524652123451233, 1.5525981187820435, 1.6060882806777954, 1.6330714225769043, 1.5878596305847168, 1.5589054822921753, 1.553062915802002, 1.5188796520233154, 1.5329731702804565, 1.5458850860595703, 1.5391148328781128, 1.5454014539718628, 1.5474541187286377, 1.547232747077942, 1.5539569854736328, 1.5564852952957153, 1.5604076385498047], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8891319036483765, 1.661047101020813, 1.5356553792953491, 1.598110318183899, 1.5818679332733154, 1.5052446126937866, 1.4864665269851685, 1.5257946252822876, 1.524652123451233, 1.5525981187820435, 1.6060882806777954, 1.6330714225769043, 1.5878596305847168, 1.5589054822921753, 1.553062915802002, 1.5188796520233154, 1.5329731702804565, 1.5458850860595703, 1.5391148328781128, 1.5454014539718628, 1.5474541187286377, 1.547232747077942, 1.5539569854736328, 1.5564852952957153, 1.5604076385498047]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.07602858543396
current iteration best possible eval_loss (full train run):  -1.5604076385498047
max eval_loss so far:  -0.931367814540863
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.5690 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.932369647909194
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.9324080613837709
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.9324078023971083
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.9324027132316128
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.932407802400232
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.5319, dtype=torch.float64), 0, tensor(0.0598, dtype=torch.float64), 0, tensor(0.2371, dtype=torch.float64), tensor(0.1632, dtype=torch.float64), 25, 0, 1, 1, 0, 1, 2, 2.602085213965211e-19, 36.805254135402095, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(9.9921e-18, dtype=torch.float64), tensor(0.0059, dtype=torch.float64), tensor(0.5319, dtype=torch.float64), tensor(0.0021, dtype=torch.float64), tensor(0.0598, dtype=torch.float64), tensor(4.4482e-18, dtype=torch.float64), tensor(0.2371, dtype=torch.float64), tensor(0.1632, dtype=torch.float64), tensor(0.7807, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(2.6021e-18, dtype=torch.float64), tensor(0.7668, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.532
  triviaqa: 0
  truthfulqa_gen: 0.06
  wikitext: 0
  mmlu: 0.237
  arc_challenge: 0.163

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (2.602085213965211e-19,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (36.805254135402095,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  2
lora dropout:  2.602085213965211e-19
lora alpha:  36.805254135402095
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,099,200 || all params: 8,032,360,448 || trainable%: 0.0261
length of training data:  9917
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9192, 'grad_norm': 5.430542945861816, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.363762617111206, 'eval_runtime': 6.8122, 'eval_samples_per_second': 146.796, 'eval_steps_per_second': 9.248, 'epoch': 0.04}
{'loss': 1.2903, 'grad_norm': 1.983628273010254, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0421602725982666, 'eval_runtime': 6.8115, 'eval_samples_per_second': 146.811, 'eval_steps_per_second': 9.249, 'epoch': 0.08}
{'loss': 1.1074, 'grad_norm': 1.6103732585906982, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 0.9365296959877014, 'eval_runtime': 6.831, 'eval_samples_per_second': 146.392, 'eval_steps_per_second': 9.223, 'epoch': 0.12}
{'loss': 1.0027, 'grad_norm': 1.6085317134857178, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.9169301986694336, 'eval_runtime': 6.8524, 'eval_samples_per_second': 145.935, 'eval_steps_per_second': 9.194, 'epoch': 0.16}
{'loss': 0.9546, 'grad_norm': 1.3664517402648926, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.8990747332572937, 'eval_runtime': 6.8651, 'eval_samples_per_second': 145.664, 'eval_steps_per_second': 9.177, 'epoch': 0.2}
{'loss': 0.9594, 'grad_norm': 1.2620341777801514, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.8908528089523315, 'eval_runtime': 6.8613, 'eval_samples_per_second': 145.746, 'eval_steps_per_second': 9.182, 'epoch': 0.24}
{'loss': 1.0074, 'grad_norm': 1.3717988729476929, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.8917092680931091, 'eval_runtime': 6.8889, 'eval_samples_per_second': 145.162, 'eval_steps_per_second': 9.145, 'epoch': 0.28}
{'loss': 0.9792, 'grad_norm': 1.317829966545105, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.8865598440170288, 'eval_runtime': 6.8957, 'eval_samples_per_second': 145.018, 'eval_steps_per_second': 9.136, 'epoch': 0.32}
{'loss': 0.9173, 'grad_norm': 1.7039004564285278, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.9002794027328491, 'eval_runtime': 6.8744, 'eval_samples_per_second': 145.467, 'eval_steps_per_second': 9.164, 'epoch': 0.36}
{'loss': 0.9456, 'grad_norm': 1.3538799285888672, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.886711835861206, 'eval_runtime': 6.9067, 'eval_samples_per_second': 144.787, 'eval_steps_per_second': 9.122, 'epoch': 0.4}
{'loss': 0.9167, 'grad_norm': 1.3075145483016968, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.8837145566940308, 'eval_runtime': 6.8817, 'eval_samples_per_second': 145.312, 'eval_steps_per_second': 9.155, 'epoch': 0.44}
{'loss': 0.9438, 'grad_norm': 1.3995698690414429, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.8892143368721008, 'eval_runtime': 6.8761, 'eval_samples_per_second': 145.43, 'eval_steps_per_second': 9.162, 'epoch': 0.48}
{'loss': 0.9393, 'grad_norm': 1.2553149461746216, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.8876403570175171, 'eval_runtime': 6.8771, 'eval_samples_per_second': 145.41, 'eval_steps_per_second': 9.161, 'epoch': 0.52}
{'loss': 0.9595, 'grad_norm': 1.6119279861450195, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.8920266032218933, 'eval_runtime': 6.8895, 'eval_samples_per_second': 145.148, 'eval_steps_per_second': 9.144, 'epoch': 0.56}
{'loss': 0.9444, 'grad_norm': 1.470666527748108, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.8936771154403687, 'eval_runtime': 6.869, 'eval_samples_per_second': 145.581, 'eval_steps_per_second': 9.172, 'epoch': 0.6}
{'loss': 0.9664, 'grad_norm': 1.4835485219955444, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.8923451900482178, 'eval_runtime': 6.8694, 'eval_samples_per_second': 145.574, 'eval_steps_per_second': 9.171, 'epoch': 0.65}
{'loss': 0.8914, 'grad_norm': 1.4564526081085205, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.893843412399292, 'eval_runtime': 6.8606, 'eval_samples_per_second': 145.759, 'eval_steps_per_second': 9.183, 'epoch': 0.69}
{'loss': 0.872, 'grad_norm': 1.352185606956482, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.8918139934539795, 'eval_runtime': 6.8752, 'eval_samples_per_second': 145.45, 'eval_steps_per_second': 9.163, 'epoch': 0.73}
{'loss': 0.8448, 'grad_norm': 1.4550955295562744, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.8989319801330566, 'eval_runtime': 6.8739, 'eval_samples_per_second': 145.478, 'eval_steps_per_second': 9.165, 'epoch': 0.77}
{'loss': 0.9426, 'grad_norm': 1.389467477798462, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.890824556350708, 'eval_runtime': 6.8395, 'eval_samples_per_second': 146.209, 'eval_steps_per_second': 9.211, 'epoch': 0.81}
{'loss': 0.9461, 'grad_norm': 1.2609202861785889, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.8908121585845947, 'eval_runtime': 6.8376, 'eval_samples_per_second': 146.251, 'eval_steps_per_second': 9.214, 'epoch': 0.85}
{'loss': 0.8648, 'grad_norm': 1.5711395740509033, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.8992467522621155, 'eval_runtime': 6.8435, 'eval_samples_per_second': 146.123, 'eval_steps_per_second': 9.206, 'epoch': 0.89}
{'loss': 0.9202, 'grad_norm': 1.6276389360427856, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.9024765491485596, 'eval_runtime': 6.8491, 'eval_samples_per_second': 146.004, 'eval_steps_per_second': 9.198, 'epoch': 0.93}
{'loss': 0.845, 'grad_norm': 1.5565091371536255, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.9022233486175537, 'eval_runtime': 6.8416, 'eval_samples_per_second': 146.165, 'eval_steps_per_second': 9.208, 'epoch': 0.97}
{'train_runtime': 330.0249, 'train_samples_per_second': 30.049, 'train_steps_per_second': 1.879, 'train_loss': 1.0304026942099294, 'epoch': 1.0}
train_results:  {'eval_loss': [1.363762617111206, 1.0421602725982666, 0.9365296959877014, 0.9169301986694336, 0.8990747332572937, 0.8908528089523315, 0.8917092680931091, 0.8865598440170288, 0.9002794027328491, 0.886711835861206, 0.8837145566940308, 0.8892143368721008, 0.8876403570175171, 0.8920266032218933, 0.8936771154403687, 0.8923451900482178, 0.893843412399292, 0.8918139934539795, 0.8989319801330566, 0.890824556350708, 0.8908121585845947, 0.8992467522621155, 0.9024765491485596, 0.9022233486175537], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.363762617111206, 1.0421602725982666, 0.9365296959877014, 0.9169301986694336, 0.8990747332572937, 0.8908528089523315, 0.8917092680931091, 0.8865598440170288, 0.9002794027328491, 0.886711835861206, 0.8837145566940308, 0.8892143368721008, 0.8876403570175171, 0.8920266032218933, 0.8936771154403687, 0.8923451900482178, 0.893843412399292, 0.8918139934539795, 0.8989319801330566, 0.890824556350708, 0.8908121585845947, 0.8992467522621155, 0.9024765491485596, 0.9022233486175537]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760301351547241
current iteration best possible eval_loss (full train run):  -0.9022233486175537
max eval_loss so far:  -0.9022233486175537
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5502 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5662546753883362, 0.8651972413063049, 0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6881975531578064, 0.07535994052886963, 0.38411790132522583, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8597249984741211, 0.2481454610824585, 0.4851251542568207, 0.9228593707084656]  ‚Üí  acq = -0.9525394253640622
X = [0.36866605281829834, 0.9746328592300415, 0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.10368144512176514, 0.6828930377960205, 0.015926599502563477, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8190843462944031, 0.06521856784820557, 0.7280534505844116, 0.8793015480041504]  ‚Üí  acq = -0.9525394122611516
X = [0.43594950437545776, 0.018241524696350098, 0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.23293758928775787, 0.9870834350585938, 0.3120540976524353, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.0616411417722702, 0.7074243426322937, 0.4130009710788727, 0.3545602560043335]  ‚Üí  acq = -0.952237956747186
X = [0.21759581565856934, 0.7204521298408508, 0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9501417279243469, 0.84186190366745, 0.104533851146698, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.23558637499809265, 0.2996382713317871, 0.04963042959570885, 0.6170431971549988]  ‚Üí  acq = -0.9525394122630798
X = [0.2948945164680481, 0.971221923828125, 0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9141191244125366, 0.7222160696983337, 0.033279359340667725, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.831488847732544, 0.14262902736663818, 0.3206331431865692, 0.8077713251113892]  ‚Üí  acq = -0.9525394122626815
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0804, dtype=torch.float64), 0, tensor(0.0661, dtype=torch.float64), 0, tensor(0.0177, dtype=torch.float64), tensor(0.0469, dtype=torch.float64), 0, tensor(0.6105, dtype=torch.float64), tensor(0.1785, dtype=torch.float64), 27, 0, 1, 0, 1, 0, 2, 0.1, 37.986024420392496, 1]
normalized proposed parameters for next round by BO: [tensor(0.0804, dtype=torch.float64), tensor(2.0299e-17, dtype=torch.float64), tensor(0.0661, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0177, dtype=torch.float64), tensor(0.0469, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6105, dtype=torch.float64), tensor(0.1785, dtype=torch.float64), tensor(0.8335, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7914, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.08
  gsm8k: 0
  rowan_hellaswag: 0.066
  sciq: 0
  triviaqa: 0.018
  truthfulqa_gen: 0.047
  wikitext: 0
  mmlu: 0.61
  arc_challenge: 0.178

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (37.986024420392496,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  37.986024420392496
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,271,808 || all params: 8,031,533,056 || trainable%: 0.0158
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8531, 'grad_norm': 5.0099616050720215, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4557738304138184, 'eval_runtime': 6.6434, 'eval_samples_per_second': 150.525, 'eval_steps_per_second': 9.483, 'epoch': 0.04}
{'loss': 1.5087, 'grad_norm': 5.495967864990234, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0313560962677002, 'eval_runtime': 6.6574, 'eval_samples_per_second': 150.208, 'eval_steps_per_second': 9.463, 'epoch': 0.08}
{'loss': 1.3238, 'grad_norm': 2.018840789794922, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9727814197540283, 'eval_runtime': 6.6286, 'eval_samples_per_second': 150.861, 'eval_steps_per_second': 9.504, 'epoch': 0.12}
{'loss': 1.3135, 'grad_norm': 1.684756875038147, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9305387735366821, 'eval_runtime': 6.6259, 'eval_samples_per_second': 150.924, 'eval_steps_per_second': 9.508, 'epoch': 0.16}
{'loss': 1.293, 'grad_norm': 2.308384895324707, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9066212177276611, 'eval_runtime': 6.6334, 'eval_samples_per_second': 150.753, 'eval_steps_per_second': 9.497, 'epoch': 0.2}
{'loss': 1.2344, 'grad_norm': 1.4176560640335083, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8830163478851318, 'eval_runtime': 6.6206, 'eval_samples_per_second': 151.044, 'eval_steps_per_second': 9.516, 'epoch': 0.24}
{'loss': 1.2029, 'grad_norm': 1.6134586334228516, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8786710500717163, 'eval_runtime': 6.6082, 'eval_samples_per_second': 151.328, 'eval_steps_per_second': 9.534, 'epoch': 0.28}
{'loss': 1.2607, 'grad_norm': 1.3585309982299805, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8847474455833435, 'eval_runtime': 6.6128, 'eval_samples_per_second': 151.222, 'eval_steps_per_second': 9.527, 'epoch': 0.32}
{'loss': 1.1921, 'grad_norm': 1.528164267539978, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8740893602371216, 'eval_runtime': 6.6124, 'eval_samples_per_second': 151.231, 'eval_steps_per_second': 9.528, 'epoch': 0.36}
{'loss': 1.2286, 'grad_norm': 1.250569462776184, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8772077560424805, 'eval_runtime': 6.6248, 'eval_samples_per_second': 150.948, 'eval_steps_per_second': 9.51, 'epoch': 0.4}
{'loss': 1.1712, 'grad_norm': 1.4421387910842896, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8682199716567993, 'eval_runtime': 6.6155, 'eval_samples_per_second': 151.159, 'eval_steps_per_second': 9.523, 'epoch': 0.44}
{'loss': 1.2075, 'grad_norm': 1.3973639011383057, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8692426681518555, 'eval_runtime': 6.6152, 'eval_samples_per_second': 151.168, 'eval_steps_per_second': 9.524, 'epoch': 0.48}
{'loss': 1.1502, 'grad_norm': 1.782845139503479, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8699301481246948, 'eval_runtime': 6.6142, 'eval_samples_per_second': 151.19, 'eval_steps_per_second': 9.525, 'epoch': 0.52}
{'loss': 1.2048, 'grad_norm': 2.0518152713775635, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8679914474487305, 'eval_runtime': 6.6126, 'eval_samples_per_second': 151.226, 'eval_steps_per_second': 9.527, 'epoch': 0.56}
{'loss': 1.1301, 'grad_norm': 1.306484580039978, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8723044991493225, 'eval_runtime': 6.6099, 'eval_samples_per_second': 151.288, 'eval_steps_per_second': 9.531, 'epoch': 0.6}
{'loss': 1.1942, 'grad_norm': 1.4151896238327026, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8710857629776001, 'eval_runtime': 6.6037, 'eval_samples_per_second': 151.431, 'eval_steps_per_second': 9.54, 'epoch': 0.64}
{'loss': 1.2021, 'grad_norm': 1.1214442253112793, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8652587532997131, 'eval_runtime': 6.6139, 'eval_samples_per_second': 151.197, 'eval_steps_per_second': 9.525, 'epoch': 0.68}
{'loss': 1.1489, 'grad_norm': 1.1462756395339966, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8653520345687866, 'eval_runtime': 6.6129, 'eval_samples_per_second': 151.219, 'eval_steps_per_second': 9.527, 'epoch': 0.72}
{'loss': 1.1759, 'grad_norm': 1.3161336183547974, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8677054643630981, 'eval_runtime': 6.6116, 'eval_samples_per_second': 151.248, 'eval_steps_per_second': 9.529, 'epoch': 0.76}
{'loss': 1.1584, 'grad_norm': 1.3720983266830444, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.864484965801239, 'eval_runtime': 6.6554, 'eval_samples_per_second': 150.255, 'eval_steps_per_second': 9.466, 'epoch': 0.8}
{'loss': 1.1785, 'grad_norm': 1.7049665451049805, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8626270294189453, 'eval_runtime': 6.6698, 'eval_samples_per_second': 149.929, 'eval_steps_per_second': 9.446, 'epoch': 0.84}
{'loss': 1.1972, 'grad_norm': 2.683779239654541, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.862146258354187, 'eval_runtime': 6.6613, 'eval_samples_per_second': 150.122, 'eval_steps_per_second': 9.458, 'epoch': 0.88}
{'loss': 1.1419, 'grad_norm': 1.4571583271026611, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8614833950996399, 'eval_runtime': 6.6587, 'eval_samples_per_second': 150.18, 'eval_steps_per_second': 9.461, 'epoch': 0.92}
{'loss': 1.1429, 'grad_norm': 1.466059684753418, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8609217405319214, 'eval_runtime': 6.6581, 'eval_samples_per_second': 150.192, 'eval_steps_per_second': 9.462, 'epoch': 0.96}
{'loss': 1.1776, 'grad_norm': 1.4011871814727783, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8613058924674988, 'eval_runtime': 6.6422, 'eval_samples_per_second': 150.554, 'eval_steps_per_second': 9.485, 'epoch': 1.0}
{'train_runtime': 380.0382, 'train_samples_per_second': 26.305, 'train_steps_per_second': 1.645, 'train_loss': 1.279675247192383, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4557738304138184, 1.0313560962677002, 0.9727814197540283, 0.9305387735366821, 0.9066212177276611, 0.8830163478851318, 0.8786710500717163, 0.8847474455833435, 0.8740893602371216, 0.8772077560424805, 0.8682199716567993, 0.8692426681518555, 0.8699301481246948, 0.8679914474487305, 0.8723044991493225, 0.8710857629776001, 0.8652587532997131, 0.8653520345687866, 0.8677054643630981, 0.864484965801239, 0.8626270294189453, 0.862146258354187, 0.8614833950996399, 0.8609217405319214, 0.8613058924674988], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4557738304138184, 1.0313560962677002, 0.9727814197540283, 0.9305387735366821, 0.9066212177276611, 0.8830163478851318, 0.8786710500717163, 0.8847474455833435, 0.8740893602371216, 0.8772077560424805, 0.8682199716567993, 0.8692426681518555, 0.8699301481246948, 0.8679914474487305, 0.8723044991493225, 0.8710857629776001, 0.8652587532997131, 0.8653520345687866, 0.8677054643630981, 0.864484965801239, 0.8626270294189453, 0.862146258354187, 0.8614833950996399, 0.8609217405319214, 0.8613058924674988]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760291814804077
current iteration best possible eval_loss (full train run):  -0.8613058924674988
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.7965 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.9659926498924081
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.9660684634026963
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.9660684634314552
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.9660600401376178
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.9660712702445405
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0274, dtype=torch.float64), tensor(0.1027, dtype=torch.float64), tensor(0.4635, dtype=torch.float64), 0, tensor(0.0549, dtype=torch.float64), tensor(0.2515, dtype=torch.float64), tensor(0.1000, dtype=torch.float64), 26, 0, 0, 0, 1, 1, 128, 0.0, 35.37523967898406, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(9.5059e-18, dtype=torch.float64), tensor(0.0274, dtype=torch.float64), tensor(0.1027, dtype=torch.float64), tensor(0.4635, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0549, dtype=torch.float64), tensor(0.2515, dtype=torch.float64), tensor(0.1000, dtype=torch.float64), tensor(0.8183, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7370, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.027
  sciq: 0.103
  triviaqa: 0.464
  truthfulqa_gen: 0
  wikitext: 0.055
  mmlu: 0.252
  arc_challenge: 0.1

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (35.37523967898406,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  35.37523967898406
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 122,683,392 || all params: 8,152,944,640 || trainable%: 1.5048
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.247, 'grad_norm': 0.8758637309074402, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7222219705581665, 'eval_runtime': 6.7751, 'eval_samples_per_second': 147.599, 'eval_steps_per_second': 9.299, 'epoch': 0.04}
{'loss': 1.5953, 'grad_norm': 0.3361109793186188, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0425256490707397, 'eval_runtime': 6.7482, 'eval_samples_per_second': 148.187, 'eval_steps_per_second': 9.336, 'epoch': 0.08}
{'loss': 1.3233, 'grad_norm': 0.45029357075691223, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9889101982116699, 'eval_runtime': 6.7572, 'eval_samples_per_second': 147.99, 'eval_steps_per_second': 9.323, 'epoch': 0.12}
{'loss': 1.2947, 'grad_norm': 0.37647566199302673, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9588288068771362, 'eval_runtime': 6.7726, 'eval_samples_per_second': 147.653, 'eval_steps_per_second': 9.302, 'epoch': 0.16}
{'loss': 1.1872, 'grad_norm': 0.32091495394706726, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9077346920967102, 'eval_runtime': 6.773, 'eval_samples_per_second': 147.645, 'eval_steps_per_second': 9.302, 'epoch': 0.2}
{'loss': 1.2234, 'grad_norm': 0.4050610363483429, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9062286615371704, 'eval_runtime': 6.7684, 'eval_samples_per_second': 147.745, 'eval_steps_per_second': 9.308, 'epoch': 0.24}
{'loss': 1.1871, 'grad_norm': 0.27472326159477234, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8936710953712463, 'eval_runtime': 6.8395, 'eval_samples_per_second': 146.209, 'eval_steps_per_second': 9.211, 'epoch': 0.28}
{'loss': 1.1844, 'grad_norm': 0.7882702946662903, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8886638283729553, 'eval_runtime': 6.8499, 'eval_samples_per_second': 145.988, 'eval_steps_per_second': 9.197, 'epoch': 0.32}
{'loss': 1.1473, 'grad_norm': 0.19535835087299347, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8876107335090637, 'eval_runtime': 6.8507, 'eval_samples_per_second': 145.971, 'eval_steps_per_second': 9.196, 'epoch': 0.36}
{'loss': 1.1624, 'grad_norm': 0.3874363899230957, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8814874887466431, 'eval_runtime': 6.808, 'eval_samples_per_second': 146.887, 'eval_steps_per_second': 9.254, 'epoch': 0.4}
{'loss': 1.1551, 'grad_norm': 0.22446252405643463, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8820242881774902, 'eval_runtime': 6.7948, 'eval_samples_per_second': 147.172, 'eval_steps_per_second': 9.272, 'epoch': 0.44}
{'loss': 1.2017, 'grad_norm': 0.23259390890598297, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8803292512893677, 'eval_runtime': 6.7951, 'eval_samples_per_second': 147.166, 'eval_steps_per_second': 9.271, 'epoch': 0.48}
{'loss': 1.2156, 'grad_norm': 0.30227187275886536, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8823267817497253, 'eval_runtime': 6.7922, 'eval_samples_per_second': 147.229, 'eval_steps_per_second': 9.275, 'epoch': 0.52}
{'loss': 1.1751, 'grad_norm': 0.3171936869621277, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8783793449401855, 'eval_runtime': 6.7846, 'eval_samples_per_second': 147.393, 'eval_steps_per_second': 9.286, 'epoch': 0.56}
{'loss': 1.1836, 'grad_norm': 0.19016103446483612, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8759242296218872, 'eval_runtime': 6.8009, 'eval_samples_per_second': 147.04, 'eval_steps_per_second': 9.264, 'epoch': 0.6}
{'loss': 1.1104, 'grad_norm': 0.2154378741979599, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8766480684280396, 'eval_runtime': 6.7909, 'eval_samples_per_second': 147.256, 'eval_steps_per_second': 9.277, 'epoch': 0.64}
{'loss': 1.1395, 'grad_norm': 0.1837041825056076, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.871397852897644, 'eval_runtime': 6.8073, 'eval_samples_per_second': 146.901, 'eval_steps_per_second': 9.255, 'epoch': 0.68}
{'loss': 1.1621, 'grad_norm': 0.2514249086380005, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.870914876461029, 'eval_runtime': 6.8102, 'eval_samples_per_second': 146.838, 'eval_steps_per_second': 9.251, 'epoch': 0.72}
{'loss': 1.0992, 'grad_norm': 0.23350749909877777, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8714005351066589, 'eval_runtime': 6.8284, 'eval_samples_per_second': 146.447, 'eval_steps_per_second': 9.226, 'epoch': 0.76}
{'loss': 1.1467, 'grad_norm': 0.22865965962409973, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8680467009544373, 'eval_runtime': 6.7946, 'eval_samples_per_second': 147.175, 'eval_steps_per_second': 9.272, 'epoch': 0.8}
{'loss': 1.1445, 'grad_norm': 0.24910959601402283, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8675711750984192, 'eval_runtime': 6.7861, 'eval_samples_per_second': 147.361, 'eval_steps_per_second': 9.284, 'epoch': 0.84}
{'loss': 1.1676, 'grad_norm': 0.20092763006687164, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8694212436676025, 'eval_runtime': 6.7658, 'eval_samples_per_second': 147.802, 'eval_steps_per_second': 9.312, 'epoch': 0.88}
{'loss': 1.1627, 'grad_norm': 0.20731805264949799, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8675779700279236, 'eval_runtime': 6.7504, 'eval_samples_per_second': 148.14, 'eval_steps_per_second': 9.333, 'epoch': 0.92}
{'loss': 1.1307, 'grad_norm': 0.21028655767440796, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8677365183830261, 'eval_runtime': 6.7471, 'eval_samples_per_second': 148.212, 'eval_steps_per_second': 9.337, 'epoch': 0.96}
{'loss': 1.1489, 'grad_norm': 0.2237820327281952, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.866891622543335, 'eval_runtime': 6.7606, 'eval_samples_per_second': 147.916, 'eval_steps_per_second': 9.319, 'epoch': 1.0}
{'train_runtime': 358.2929, 'train_samples_per_second': 27.905, 'train_steps_per_second': 1.744, 'train_loss': 1.2758273529052735, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7222219705581665, 1.0425256490707397, 0.9889101982116699, 0.9588288068771362, 0.9077346920967102, 0.9062286615371704, 0.8936710953712463, 0.8886638283729553, 0.8876107335090637, 0.8814874887466431, 0.8820242881774902, 0.8803292512893677, 0.8823267817497253, 0.8783793449401855, 0.8759242296218872, 0.8766480684280396, 0.871397852897644, 0.870914876461029, 0.8714005351066589, 0.8680467009544373, 0.8675711750984192, 0.8694212436676025, 0.8675779700279236, 0.8677365183830261, 0.866891622543335], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7222219705581665, 1.0425256490707397, 0.9889101982116699, 0.9588288068771362, 0.9077346920967102, 0.9062286615371704, 0.8936710953712463, 0.8886638283729553, 0.8876107335090637, 0.8814874887466431, 0.8820242881774902, 0.8803292512893677, 0.8823267817497253, 0.8783793449401855, 0.8759242296218872, 0.8766480684280396, 0.871397852897644, 0.870914876461029, 0.8714005351066589, 0.8680467009544373, 0.8675711750984192, 0.8694212436676025, 0.8675779700279236, 0.8677365183830261, 0.866891622543335]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.5365482568740845
current iteration best possible eval_loss (full train run):  -0.866891622543335
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5937 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.9702304753840585
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.9702304242570927
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.970566550906764
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.9705347722430564
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.9702304238714605
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0110, dtype=torch.float64), tensor(0.0515, dtype=torch.float64), tensor(0.0586, dtype=torch.float64), tensor(0.0869, dtype=torch.float64), tensor(0.1921, dtype=torch.float64), tensor(0.1298, dtype=torch.float64), tensor(0.4541, dtype=torch.float64), tensor(0.0158, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 32, 0.06463574873780613, 33.67592314067458, 0]
normalized proposed parameters for next round by BO: [tensor(2.7178e-18, dtype=torch.float64), tensor(0.0110, dtype=torch.float64), tensor(0.0515, dtype=torch.float64), tensor(0.0586, dtype=torch.float64), tensor(0.0869, dtype=torch.float64), tensor(0.1921, dtype=torch.float64), tensor(0.1298, dtype=torch.float64), tensor(0.4541, dtype=torch.float64), tensor(0.0158, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2512, dtype=torch.float64), tensor(0.6464, dtype=torch.float64), tensor(0.7016, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.011
  rowan_hellaswag: 0.052
  sciq: 0.059
  triviaqa: 0.087
  truthfulqa_gen: 0.192
  wikitext: 0.13
  mmlu: 0.454
  arc_challenge: 0.016

LoRA Parameters:
  lora_r: (32,)
  lora_dropout: (0.06463574873780613,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (33.67592314067458,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  32
lora dropout:  0.06463574873780613
lora alpha:  33.67592314067458
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 51,380,224 || all params: 8,081,641,472 || trainable%: 0.6358
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8177, 'grad_norm': 0.6572057604789734, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7243479490280151, 'eval_runtime': 7.1676, 'eval_samples_per_second': 139.517, 'eval_steps_per_second': 8.79, 'epoch': 0.04}
{'loss': 1.7015, 'grad_norm': 0.6168586015701294, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2406522035598755, 'eval_runtime': 7.1514, 'eval_samples_per_second': 139.833, 'eval_steps_per_second': 8.809, 'epoch': 0.08}
{'loss': 1.4132, 'grad_norm': 0.4834311306476593, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0684950351715088, 'eval_runtime': 7.1818, 'eval_samples_per_second': 139.241, 'eval_steps_per_second': 8.772, 'epoch': 0.12}
{'loss': 1.3763, 'grad_norm': 0.4876933693885803, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0766528844833374, 'eval_runtime': 7.1976, 'eval_samples_per_second': 138.934, 'eval_steps_per_second': 8.753, 'epoch': 0.16}
{'loss': 1.3037, 'grad_norm': 0.5558589100837708, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0137946605682373, 'eval_runtime': 7.2066, 'eval_samples_per_second': 138.762, 'eval_steps_per_second': 8.742, 'epoch': 0.2}
{'loss': 1.3211, 'grad_norm': 0.46840783953666687, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0321592092514038, 'eval_runtime': 7.2094, 'eval_samples_per_second': 138.707, 'eval_steps_per_second': 8.739, 'epoch': 0.24}
{'loss': 1.2798, 'grad_norm': 0.4688369631767273, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9953501224517822, 'eval_runtime': 7.2199, 'eval_samples_per_second': 138.506, 'eval_steps_per_second': 8.726, 'epoch': 0.28}
{'loss': 1.3077, 'grad_norm': 0.4412895739078522, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.989088237285614, 'eval_runtime': 7.2213, 'eval_samples_per_second': 138.48, 'eval_steps_per_second': 8.724, 'epoch': 0.32}
{'loss': 1.2461, 'grad_norm': 0.46453312039375305, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9810667634010315, 'eval_runtime': 7.2207, 'eval_samples_per_second': 138.491, 'eval_steps_per_second': 8.725, 'epoch': 0.36}
{'loss': 1.2523, 'grad_norm': 0.3869079351425171, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9785271286964417, 'eval_runtime': 7.2268, 'eval_samples_per_second': 138.374, 'eval_steps_per_second': 8.718, 'epoch': 0.4}
{'loss': 1.268, 'grad_norm': 0.4982191324234009, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9777060747146606, 'eval_runtime': 7.2428, 'eval_samples_per_second': 138.068, 'eval_steps_per_second': 8.698, 'epoch': 0.44}
{'loss': 1.2431, 'grad_norm': 0.5331504940986633, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.980690062046051, 'eval_runtime': 7.303, 'eval_samples_per_second': 136.931, 'eval_steps_per_second': 8.627, 'epoch': 0.48}
{'loss': 1.26, 'grad_norm': 0.5303576588630676, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9887986183166504, 'eval_runtime': 7.2738, 'eval_samples_per_second': 137.479, 'eval_steps_per_second': 8.661, 'epoch': 0.52}
{'loss': 1.198, 'grad_norm': 0.3840116560459137, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9465997219085693, 'eval_runtime': 7.263, 'eval_samples_per_second': 137.683, 'eval_steps_per_second': 8.674, 'epoch': 0.56}
{'loss': 1.2657, 'grad_norm': 0.417216420173645, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9764251708984375, 'eval_runtime': 7.2719, 'eval_samples_per_second': 137.516, 'eval_steps_per_second': 8.664, 'epoch': 0.6}
{'loss': 1.2575, 'grad_norm': 0.39815181493759155, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9749393463134766, 'eval_runtime': 7.2649, 'eval_samples_per_second': 137.649, 'eval_steps_per_second': 8.672, 'epoch': 0.64}
{'loss': 1.1963, 'grad_norm': 0.5471382737159729, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.953526496887207, 'eval_runtime': 7.2631, 'eval_samples_per_second': 137.682, 'eval_steps_per_second': 8.674, 'epoch': 0.68}
{'loss': 1.1871, 'grad_norm': 0.3824336528778076, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9494950175285339, 'eval_runtime': 7.2019, 'eval_samples_per_second': 138.853, 'eval_steps_per_second': 8.748, 'epoch': 0.72}
{'loss': 1.1224, 'grad_norm': 0.48184388875961304, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9448819160461426, 'eval_runtime': 7.2157, 'eval_samples_per_second': 138.588, 'eval_steps_per_second': 8.731, 'epoch': 0.76}
{'loss': 1.2004, 'grad_norm': 0.4589594304561615, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9555503129959106, 'eval_runtime': 7.2149, 'eval_samples_per_second': 138.603, 'eval_steps_per_second': 8.732, 'epoch': 0.8}
{'loss': 1.1666, 'grad_norm': 0.567609965801239, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9450829029083252, 'eval_runtime': 7.2108, 'eval_samples_per_second': 138.68, 'eval_steps_per_second': 8.737, 'epoch': 0.84}
{'loss': 1.2409, 'grad_norm': 0.39888447523117065, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9408715963363647, 'eval_runtime': 7.2102, 'eval_samples_per_second': 138.692, 'eval_steps_per_second': 8.738, 'epoch': 0.88}
{'loss': 1.2081, 'grad_norm': 0.47194722294807434, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9382078647613525, 'eval_runtime': 7.2158, 'eval_samples_per_second': 138.584, 'eval_steps_per_second': 8.731, 'epoch': 0.92}
{'loss': 1.2632, 'grad_norm': 0.5322412848472595, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9326345324516296, 'eval_runtime': 7.2137, 'eval_samples_per_second': 138.625, 'eval_steps_per_second': 8.733, 'epoch': 0.96}
{'loss': 1.093, 'grad_norm': 0.6225345730781555, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9328612089157104, 'eval_runtime': 7.2044, 'eval_samples_per_second': 138.804, 'eval_steps_per_second': 8.745, 'epoch': 1.0}
{'train_runtime': 412.516, 'train_samples_per_second': 24.234, 'train_steps_per_second': 1.515, 'train_loss': 1.3275878967285155, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7243479490280151, 1.2406522035598755, 1.0684950351715088, 1.0766528844833374, 1.0137946605682373, 1.0321592092514038, 0.9953501224517822, 0.989088237285614, 0.9810667634010315, 0.9785271286964417, 0.9777060747146606, 0.980690062046051, 0.9887986183166504, 0.9465997219085693, 0.9764251708984375, 0.9749393463134766, 0.953526496887207, 0.9494950175285339, 0.9448819160461426, 0.9555503129959106, 0.9450829029083252, 0.9408715963363647, 0.9382078647613525, 0.9326345324516296, 0.9328612089157104], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7243479490280151, 1.2406522035598755, 1.0684950351715088, 1.0766528844833374, 1.0137946605682373, 1.0321592092514038, 0.9953501224517822, 0.989088237285614, 0.9810667634010315, 0.9785271286964417, 0.9777060747146606, 0.980690062046051, 0.9887986183166504, 0.9465997219085693, 0.9764251708984375, 0.9749393463134766, 0.953526496887207, 0.9494950175285339, 0.9448819160461426, 0.9555503129959106, 0.9450829029083252, 0.9408715963363647, 0.9382078647613525, 0.9326345324516296, 0.9328612089157104]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760306119918823
current iteration best possible eval_loss (full train run):  -0.9328612089157104
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.3763 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.940037792740902
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.9436063019514586
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.9436064969547837
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.9437768927440148
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.9441849795085162
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1388, dtype=torch.float64), 0, 0, tensor(0.2258, dtype=torch.float64), 0, tensor(0.4439, dtype=torch.float64), tensor(0.1166, dtype=torch.float64), 0, tensor(0.0749, dtype=torch.float64), 32, 1, 1, 0, 1, 1, 36, 0.1, 18.21087644748635, 1]
normalized proposed parameters for next round by BO: [tensor(0.1388, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2258, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4439, dtype=torch.float64), tensor(0.1166, dtype=torch.float64), tensor(9.8370e-19, dtype=torch.float64), tensor(0.0749, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2793, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3794, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.139
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.226
  triviaqa: 0
  truthfulqa_gen: 0.444
  wikitext: 0.117
  mmlu: 0
  arc_challenge: 0.075

LoRA Parameters:
  lora_r: (36,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (18.21087644748635,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  36
lora dropout:  0.1
lora alpha:  18.21087644748635
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 57,802,752 || all params: 8,088,064,000 || trainable%: 0.7147
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5485, 'grad_norm': 1.194018840789795, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6212629079818726, 'eval_runtime': 7.4361, 'eval_samples_per_second': 134.48, 'eval_steps_per_second': 8.472, 'epoch': 0.04}
{'loss': 1.2526, 'grad_norm': 1.1399450302124023, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.973100483417511, 'eval_runtime': 7.425, 'eval_samples_per_second': 134.68, 'eval_steps_per_second': 8.485, 'epoch': 0.08}
{'loss': 1.0358, 'grad_norm': 0.3569926917552948, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9336269497871399, 'eval_runtime': 7.4248, 'eval_samples_per_second': 134.684, 'eval_steps_per_second': 8.485, 'epoch': 0.12}
{'loss': 0.9822, 'grad_norm': 0.3324233591556549, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9226719737052917, 'eval_runtime': 7.4111, 'eval_samples_per_second': 134.933, 'eval_steps_per_second': 8.501, 'epoch': 0.16}
{'loss': 0.9625, 'grad_norm': 0.37344253063201904, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9179210066795349, 'eval_runtime': 7.4025, 'eval_samples_per_second': 135.089, 'eval_steps_per_second': 8.511, 'epoch': 0.2}
{'loss': 0.9203, 'grad_norm': 0.36438512802124023, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9020500779151917, 'eval_runtime': 7.4105, 'eval_samples_per_second': 134.944, 'eval_steps_per_second': 8.501, 'epoch': 0.24}
{'loss': 0.8792, 'grad_norm': 0.3250652551651001, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8979409337043762, 'eval_runtime': 7.4225, 'eval_samples_per_second': 134.725, 'eval_steps_per_second': 8.488, 'epoch': 0.28}
{'loss': 0.8507, 'grad_norm': 0.40113380551338196, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8924285769462585, 'eval_runtime': 7.4518, 'eval_samples_per_second': 134.196, 'eval_steps_per_second': 8.454, 'epoch': 0.32}
{'loss': 0.8212, 'grad_norm': 0.37536463141441345, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8952599763870239, 'eval_runtime': 7.4472, 'eval_samples_per_second': 134.28, 'eval_steps_per_second': 8.46, 'epoch': 0.36}
{'loss': 0.8396, 'grad_norm': 0.5261558890342712, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8894439935684204, 'eval_runtime': 7.465, 'eval_samples_per_second': 133.959, 'eval_steps_per_second': 8.439, 'epoch': 0.4}
{'loss': 0.8476, 'grad_norm': 0.4608863592147827, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8893317580223083, 'eval_runtime': 7.5001, 'eval_samples_per_second': 133.331, 'eval_steps_per_second': 8.4, 'epoch': 0.44}
{'loss': 0.8312, 'grad_norm': 0.39705413579940796, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8870858550071716, 'eval_runtime': 7.4927, 'eval_samples_per_second': 133.463, 'eval_steps_per_second': 8.408, 'epoch': 0.48}
{'loss': 0.8017, 'grad_norm': 0.4119347631931305, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8854000568389893, 'eval_runtime': 7.5202, 'eval_samples_per_second': 132.976, 'eval_steps_per_second': 8.377, 'epoch': 0.52}
{'loss': 0.7581, 'grad_norm': 0.39524179697036743, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8832328915596008, 'eval_runtime': 7.4904, 'eval_samples_per_second': 133.504, 'eval_steps_per_second': 8.411, 'epoch': 0.56}
{'loss': 0.7791, 'grad_norm': 0.4860522449016571, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8870425820350647, 'eval_runtime': 7.4942, 'eval_samples_per_second': 133.437, 'eval_steps_per_second': 8.407, 'epoch': 0.6}
{'loss': 0.7581, 'grad_norm': 0.3843495547771454, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8831650018692017, 'eval_runtime': 7.4952, 'eval_samples_per_second': 133.418, 'eval_steps_per_second': 8.405, 'epoch': 0.64}
{'loss': 0.7219, 'grad_norm': 0.37071773409843445, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.883906364440918, 'eval_runtime': 7.4918, 'eval_samples_per_second': 133.479, 'eval_steps_per_second': 8.409, 'epoch': 0.68}
{'loss': 0.7596, 'grad_norm': 0.32832473516464233, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8777841925621033, 'eval_runtime': 7.4695, 'eval_samples_per_second': 133.878, 'eval_steps_per_second': 8.434, 'epoch': 0.72}
{'loss': 0.7906, 'grad_norm': 0.3655881881713867, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8785687685012817, 'eval_runtime': 7.4583, 'eval_samples_per_second': 134.078, 'eval_steps_per_second': 8.447, 'epoch': 0.76}
{'loss': 0.7593, 'grad_norm': 0.35609006881713867, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8799530863761902, 'eval_runtime': 7.452, 'eval_samples_per_second': 134.192, 'eval_steps_per_second': 8.454, 'epoch': 0.8}
{'loss': 0.72, 'grad_norm': 0.42770007252693176, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8779548406600952, 'eval_runtime': 7.4341, 'eval_samples_per_second': 134.516, 'eval_steps_per_second': 8.474, 'epoch': 0.84}
{'loss': 0.7112, 'grad_norm': 0.514580249786377, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8739115595817566, 'eval_runtime': 7.4387, 'eval_samples_per_second': 134.432, 'eval_steps_per_second': 8.469, 'epoch': 0.88}
{'loss': 0.7168, 'grad_norm': 0.3284558057785034, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8735939860343933, 'eval_runtime': 7.481, 'eval_samples_per_second': 133.672, 'eval_steps_per_second': 8.421, 'epoch': 0.92}
{'loss': 0.6181, 'grad_norm': 0.33696249127388, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8711535930633545, 'eval_runtime': 7.4974, 'eval_samples_per_second': 133.379, 'eval_steps_per_second': 8.403, 'epoch': 0.96}
{'loss': 0.6059, 'grad_norm': 0.33345529437065125, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8712518215179443, 'eval_runtime': 7.4914, 'eval_samples_per_second': 133.486, 'eval_steps_per_second': 8.41, 'epoch': 1.0}
{'train_runtime': 355.2591, 'train_samples_per_second': 28.14, 'train_steps_per_second': 1.759, 'train_loss': 0.9308779983520508, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6212629079818726, 0.973100483417511, 0.9336269497871399, 0.9226719737052917, 0.9179210066795349, 0.9020500779151917, 0.8979409337043762, 0.8924285769462585, 0.8952599763870239, 0.8894439935684204, 0.8893317580223083, 0.8870858550071716, 0.8854000568389893, 0.8832328915596008, 0.8870425820350647, 0.8831650018692017, 0.883906364440918, 0.8777841925621033, 0.8785687685012817, 0.8799530863761902, 0.8779548406600952, 0.8739115595817566, 0.8735939860343933, 0.8711535930633545, 0.8712518215179443], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6212629079818726, 0.973100483417511, 0.9336269497871399, 0.9226719737052917, 0.9179210066795349, 0.9020500779151917, 0.8979409337043762, 0.8924285769462585, 0.8952599763870239, 0.8894439935684204, 0.8893317580223083, 0.8870858550071716, 0.8854000568389893, 0.8832328915596008, 0.8870425820350647, 0.8831650018692017, 0.883906364440918, 0.8777841925621033, 0.8785687685012817, 0.8799530863761902, 0.8779548406600952, 0.8739115595817566, 0.8735939860343933, 0.8711535930633545, 0.8712518215179443]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0960626602172852
current iteration best possible eval_loss (full train run):  -0.8712518215179443
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.4484 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.9864580346971336
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.9864592614202313
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.9864580353542859
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.9864581121142839
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.9864580450840027
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2324, dtype=torch.float64), tensor(0.1257, dtype=torch.float64), tensor(0.0606, dtype=torch.float64), tensor(0.2766, dtype=torch.float64), tensor(0.0684, dtype=torch.float64), 0, tensor(0.0250, dtype=torch.float64), 0, tensor(0.2112, dtype=torch.float64), 15, 1, 1, 0, 1, 1, 49, 0.038666622481449545, 39.44065566511992, 1]
normalized proposed parameters for next round by BO: [tensor(0.2324, dtype=torch.float64), tensor(0.1257, dtype=torch.float64), tensor(0.0606, dtype=torch.float64), tensor(0.2766, dtype=torch.float64), tensor(0.0684, dtype=torch.float64), tensor(2.5693e-19, dtype=torch.float64), tensor(0.0250, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2112, dtype=torch.float64), tensor(0.4668, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3848, dtype=torch.float64), tensor(0.3867, dtype=torch.float64), tensor(0.8217, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.232
  gsm8k: 0.126
  rowan_hellaswag: 0.061
  sciq: 0.277
  triviaqa: 0.068
  truthfulqa_gen: 0
  wikitext: 0.025
  mmlu: 0
  arc_challenge: 0.211

LoRA Parameters:
  lora_r: (49,)
  lora_dropout: (0.038666622481449545,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (39.44065566511992,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  49
lora dropout:  0.038666622481449545
lora alpha:  39.44065566511992
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 36,879,360 || all params: 8,067,140,608 || trainable%: 0.4572
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0023, 'grad_norm': 1.5550726652145386, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.55314040184021, 'eval_runtime': 6.9029, 'eval_samples_per_second': 144.867, 'eval_steps_per_second': 9.127, 'epoch': 0.04}
{'loss': 1.3706, 'grad_norm': 0.74849933385849, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0137773752212524, 'eval_runtime': 6.929, 'eval_samples_per_second': 144.321, 'eval_steps_per_second': 9.092, 'epoch': 0.08}
{'loss': 1.0795, 'grad_norm': 0.5820985436439514, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9497131109237671, 'eval_runtime': 6.8855, 'eval_samples_per_second': 145.232, 'eval_steps_per_second': 9.15, 'epoch': 0.12}
{'loss': 1.0627, 'grad_norm': 0.41732949018478394, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9002231955528259, 'eval_runtime': 6.9325, 'eval_samples_per_second': 144.247, 'eval_steps_per_second': 9.088, 'epoch': 0.16}
{'loss': 1.0643, 'grad_norm': 0.37733301520347595, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8954331278800964, 'eval_runtime': 6.9282, 'eval_samples_per_second': 144.338, 'eval_steps_per_second': 9.093, 'epoch': 0.2}
{'loss': 1.0311, 'grad_norm': 0.4525474011898041, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8878071308135986, 'eval_runtime': 6.9468, 'eval_samples_per_second': 143.951, 'eval_steps_per_second': 9.069, 'epoch': 0.24}
{'loss': 1.013, 'grad_norm': 0.4732869267463684, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8813387155532837, 'eval_runtime': 6.9493, 'eval_samples_per_second': 143.9, 'eval_steps_per_second': 9.066, 'epoch': 0.28}
{'loss': 1.0457, 'grad_norm': 0.3619444966316223, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8796164393424988, 'eval_runtime': 6.9339, 'eval_samples_per_second': 144.219, 'eval_steps_per_second': 9.086, 'epoch': 0.32}
{'loss': 1.0105, 'grad_norm': 0.3429761230945587, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8822929859161377, 'eval_runtime': 6.9419, 'eval_samples_per_second': 144.053, 'eval_steps_per_second': 9.075, 'epoch': 0.36}
{'loss': 1.0239, 'grad_norm': 0.34120604395866394, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8782821297645569, 'eval_runtime': 6.9417, 'eval_samples_per_second': 144.057, 'eval_steps_per_second': 9.076, 'epoch': 0.4}
{'loss': 0.942, 'grad_norm': 0.3563477694988251, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8764705061912537, 'eval_runtime': 6.9217, 'eval_samples_per_second': 144.474, 'eval_steps_per_second': 9.102, 'epoch': 0.44}
{'loss': 1.018, 'grad_norm': 0.47178733348846436, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8669753074645996, 'eval_runtime': 6.9306, 'eval_samples_per_second': 144.288, 'eval_steps_per_second': 9.09, 'epoch': 0.48}
{'loss': 0.9781, 'grad_norm': 0.29290521144866943, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8691248893737793, 'eval_runtime': 6.9208, 'eval_samples_per_second': 144.493, 'eval_steps_per_second': 9.103, 'epoch': 0.52}
{'loss': 0.9481, 'grad_norm': 0.3107364773750305, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8670751452445984, 'eval_runtime': 6.937, 'eval_samples_per_second': 144.155, 'eval_steps_per_second': 9.082, 'epoch': 0.56}
{'loss': 0.992, 'grad_norm': 0.3258976638317108, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8729545474052429, 'eval_runtime': 6.9081, 'eval_samples_per_second': 144.759, 'eval_steps_per_second': 9.12, 'epoch': 0.6}
{'loss': 1.0015, 'grad_norm': 0.3366554379463196, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.869307816028595, 'eval_runtime': 6.8785, 'eval_samples_per_second': 145.381, 'eval_steps_per_second': 9.159, 'epoch': 0.64}
{'loss': 0.9593, 'grad_norm': 0.28703421354293823, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8739994168281555, 'eval_runtime': 6.8718, 'eval_samples_per_second': 145.523, 'eval_steps_per_second': 9.168, 'epoch': 0.68}
{'loss': 0.9979, 'grad_norm': 0.3734184205532074, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8739380836486816, 'eval_runtime': 6.8455, 'eval_samples_per_second': 146.081, 'eval_steps_per_second': 9.203, 'epoch': 0.72}
{'loss': 0.9614, 'grad_norm': 0.38642966747283936, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8811151385307312, 'eval_runtime': 6.8445, 'eval_samples_per_second': 146.103, 'eval_steps_per_second': 9.205, 'epoch': 0.76}
{'loss': 0.9818, 'grad_norm': 0.39910024404525757, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8796929717063904, 'eval_runtime': 6.849, 'eval_samples_per_second': 146.006, 'eval_steps_per_second': 9.198, 'epoch': 0.8}
{'loss': 0.9524, 'grad_norm': 0.32214367389678955, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8809272646903992, 'eval_runtime': 6.8451, 'eval_samples_per_second': 146.091, 'eval_steps_per_second': 9.204, 'epoch': 0.84}
{'loss': 0.9109, 'grad_norm': 0.4775431752204895, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.886314332485199, 'eval_runtime': 6.8506, 'eval_samples_per_second': 145.972, 'eval_steps_per_second': 9.196, 'epoch': 0.88}
{'loss': 0.929, 'grad_norm': 0.359012633562088, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8809506893157959, 'eval_runtime': 6.8525, 'eval_samples_per_second': 145.931, 'eval_steps_per_second': 9.194, 'epoch': 0.92}
{'loss': 0.9213, 'grad_norm': 0.32427144050598145, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8797222375869751, 'eval_runtime': 6.8449, 'eval_samples_per_second': 146.094, 'eval_steps_per_second': 9.204, 'epoch': 0.96}
{'loss': 0.9475, 'grad_norm': 0.3675346374511719, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8800092339515686, 'eval_runtime': 6.8495, 'eval_samples_per_second': 145.997, 'eval_steps_per_second': 9.198, 'epoch': 1.0}
{'train_runtime': 386.6927, 'train_samples_per_second': 25.855, 'train_steps_per_second': 1.616, 'train_loss': 1.0857925201416017, 'epoch': 1.0}
train_results:  {'eval_loss': [1.55314040184021, 1.0137773752212524, 0.9497131109237671, 0.9002231955528259, 0.8954331278800964, 0.8878071308135986, 0.8813387155532837, 0.8796164393424988, 0.8822929859161377, 0.8782821297645569, 0.8764705061912537, 0.8669753074645996, 0.8691248893737793, 0.8670751452445984, 0.8729545474052429, 0.869307816028595, 0.8739994168281555, 0.8739380836486816, 0.8811151385307312, 0.8796929717063904, 0.8809272646903992, 0.886314332485199, 0.8809506893157959, 0.8797222375869751, 0.8800092339515686], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.55314040184021, 1.0137773752212524, 0.9497131109237671, 0.9002231955528259, 0.8954331278800964, 0.8878071308135986, 0.8813387155532837, 0.8796164393424988, 0.8822929859161377, 0.8782821297645569, 0.8764705061912537, 0.8669753074645996, 0.8691248893737793, 0.8670751452445984, 0.8729545474052429, 0.869307816028595, 0.8739994168281555, 0.8739380836486816, 0.8811151385307312, 0.8796929717063904, 0.8809272646903992, 0.886314332485199, 0.8809506893157959, 0.8797222375869751, 0.8800092339515686]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0969380140304565
current iteration best possible eval_loss (full train run):  -0.8800092339515686
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4521 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.989382003113317
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.9549697279050642
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.9916496755898916
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.9903688782016247
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.9971731278885935
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1993, dtype=torch.float64), tensor(0.0289, dtype=torch.float64), tensor(0.1002, dtype=torch.float64), tensor(0.1344, dtype=torch.float64), tensor(0.1355, dtype=torch.float64), 0, tensor(0.0549, dtype=torch.float64), tensor(0.2173, dtype=torch.float64), tensor(0.1295, dtype=torch.float64), 31, 0, 1, 1, 1, 0, 27, 0.06315241561496647, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.1993, dtype=torch.float64), tensor(0.0289, dtype=torch.float64), tensor(0.1002, dtype=torch.float64), tensor(0.1344, dtype=torch.float64), tensor(0.1355, dtype=torch.float64), tensor(3.4991e-18, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), tensor(0.2173, dtype=torch.float64), tensor(0.1295, dtype=torch.float64), tensor(0.9588, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2120, dtype=torch.float64), tensor(0.6315, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.199
  gsm8k: 0.029
  rowan_hellaswag: 0.1
  sciq: 0.134
  triviaqa: 0.135
  truthfulqa_gen: 0
  wikitext: 0.055
  mmlu: 0.217
  arc_challenge: 0.129

LoRA Parameters:
  lora_r: (27,)
  lora_dropout: (0.06315241561496647,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  27
lora dropout:  0.06315241561496647
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 35,140,608 || all params: 8,065,401,856 || trainable%: 0.4357
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8597, 'grad_norm': 1.2694069147109985, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1718418598175049, 'eval_runtime': 7.2957, 'eval_samples_per_second': 137.067, 'eval_steps_per_second': 8.635, 'epoch': 0.04}
{'loss': 1.4642, 'grad_norm': 2.6399173736572266, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.015418529510498, 'eval_runtime': 7.3344, 'eval_samples_per_second': 136.343, 'eval_steps_per_second': 8.59, 'epoch': 0.08}
{'loss': 1.2864, 'grad_norm': 0.6008956432342529, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9197888374328613, 'eval_runtime': 7.3387, 'eval_samples_per_second': 136.264, 'eval_steps_per_second': 8.585, 'epoch': 0.12}
{'loss': 1.2029, 'grad_norm': 0.9230091571807861, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8897552490234375, 'eval_runtime': 7.3966, 'eval_samples_per_second': 135.196, 'eval_steps_per_second': 8.517, 'epoch': 0.16}
{'loss': 1.2673, 'grad_norm': 0.4268884062767029, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8851248025894165, 'eval_runtime': 7.3983, 'eval_samples_per_second': 135.167, 'eval_steps_per_second': 8.516, 'epoch': 0.2}
{'loss': 1.1945, 'grad_norm': 0.4780915081501007, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8845693469047546, 'eval_runtime': 7.3858, 'eval_samples_per_second': 135.395, 'eval_steps_per_second': 8.53, 'epoch': 0.24}
{'loss': 1.1622, 'grad_norm': 0.8195988535881042, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8735719919204712, 'eval_runtime': 7.4137, 'eval_samples_per_second': 134.886, 'eval_steps_per_second': 8.498, 'epoch': 0.28}
{'loss': 1.2049, 'grad_norm': 0.4385472238063812, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8755487203598022, 'eval_runtime': 7.4242, 'eval_samples_per_second': 134.694, 'eval_steps_per_second': 8.486, 'epoch': 0.32}
{'loss': 1.2001, 'grad_norm': 0.6118754744529724, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8761828541755676, 'eval_runtime': 7.4159, 'eval_samples_per_second': 134.846, 'eval_steps_per_second': 8.495, 'epoch': 0.36}
{'loss': 1.1552, 'grad_norm': 0.5139079093933105, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8725428581237793, 'eval_runtime': 7.4097, 'eval_samples_per_second': 134.959, 'eval_steps_per_second': 8.502, 'epoch': 0.4}
{'loss': 1.2219, 'grad_norm': 0.5546918511390686, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8778948783874512, 'eval_runtime': 7.4048, 'eval_samples_per_second': 135.047, 'eval_steps_per_second': 8.508, 'epoch': 0.44}
{'loss': 1.184, 'grad_norm': 0.5821146368980408, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8675867915153503, 'eval_runtime': 7.4141, 'eval_samples_per_second': 134.878, 'eval_steps_per_second': 8.497, 'epoch': 0.48}
{'loss': 1.1725, 'grad_norm': 0.4573025405406952, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8693127632141113, 'eval_runtime': 7.4319, 'eval_samples_per_second': 134.555, 'eval_steps_per_second': 8.477, 'epoch': 0.52}
{'loss': 1.183, 'grad_norm': 0.5928147435188293, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8711280226707458, 'eval_runtime': 7.4142, 'eval_samples_per_second': 134.877, 'eval_steps_per_second': 8.497, 'epoch': 0.56}
{'loss': 1.1207, 'grad_norm': 0.4937686622142792, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8678006529808044, 'eval_runtime': 7.3915, 'eval_samples_per_second': 135.291, 'eval_steps_per_second': 8.523, 'epoch': 0.6}
{'loss': 1.1355, 'grad_norm': 0.5363906025886536, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8700438737869263, 'eval_runtime': 7.3627, 'eval_samples_per_second': 135.82, 'eval_steps_per_second': 8.557, 'epoch': 0.64}
{'loss': 1.113, 'grad_norm': 0.5522744655609131, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8752743005752563, 'eval_runtime': 7.3536, 'eval_samples_per_second': 135.987, 'eval_steps_per_second': 8.567, 'epoch': 0.68}
{'loss': 1.2121, 'grad_norm': 0.8126307725906372, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8717086911201477, 'eval_runtime': 7.3461, 'eval_samples_per_second': 136.126, 'eval_steps_per_second': 8.576, 'epoch': 0.72}
{'loss': 1.0995, 'grad_norm': 0.5330337285995483, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8704648613929749, 'eval_runtime': 7.3494, 'eval_samples_per_second': 136.066, 'eval_steps_per_second': 8.572, 'epoch': 0.76}
{'loss': 1.2025, 'grad_norm': 0.5589582324028015, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8647125363349915, 'eval_runtime': 7.3426, 'eval_samples_per_second': 136.192, 'eval_steps_per_second': 8.58, 'epoch': 0.8}
{'loss': 1.2063, 'grad_norm': 0.557193398475647, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8751686215400696, 'eval_runtime': 7.3406, 'eval_samples_per_second': 136.229, 'eval_steps_per_second': 8.582, 'epoch': 0.84}
{'loss': 1.1616, 'grad_norm': 0.43809279799461365, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8734021186828613, 'eval_runtime': 7.3326, 'eval_samples_per_second': 136.377, 'eval_steps_per_second': 8.592, 'epoch': 0.88}
{'loss': 1.173, 'grad_norm': 0.7123671770095825, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8723135590553284, 'eval_runtime': 7.3423, 'eval_samples_per_second': 136.196, 'eval_steps_per_second': 8.58, 'epoch': 0.92}
{'loss': 1.1333, 'grad_norm': 0.5431209206581116, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8718712329864502, 'eval_runtime': 7.3431, 'eval_samples_per_second': 136.183, 'eval_steps_per_second': 8.58, 'epoch': 0.96}
{'loss': 1.133, 'grad_norm': 0.5476787686347961, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8702529072761536, 'eval_runtime': 7.3257, 'eval_samples_per_second': 136.506, 'eval_steps_per_second': 8.6, 'epoch': 1.0}
{'train_runtime': 426.6879, 'train_samples_per_second': 23.427, 'train_steps_per_second': 1.465, 'train_loss': 1.2579696716308593, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1718418598175049, 1.015418529510498, 0.9197888374328613, 0.8897552490234375, 0.8851248025894165, 0.8845693469047546, 0.8735719919204712, 0.8755487203598022, 0.8761828541755676, 0.8725428581237793, 0.8778948783874512, 0.8675867915153503, 0.8693127632141113, 0.8711280226707458, 0.8678006529808044, 0.8700438737869263, 0.8752743005752563, 0.8717086911201477, 0.8704648613929749, 0.8647125363349915, 0.8751686215400696, 0.8734021186828613, 0.8723135590553284, 0.8718712329864502, 0.8702529072761536], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1718418598175049, 1.015418529510498, 0.9197888374328613, 0.8897552490234375, 0.8851248025894165, 0.8845693469047546, 0.8735719919204712, 0.8755487203598022, 0.8761828541755676, 0.8725428581237793, 0.8778948783874512, 0.8675867915153503, 0.8693127632141113, 0.8711280226707458, 0.8678006529808044, 0.8700438737869263, 0.8752743005752563, 0.8717086911201477, 0.8704648613929749, 0.8647125363349915, 0.8751686215400696, 0.8734021186828613, 0.8723135590553284, 0.8718712329864502, 0.8702529072761536]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760269165039062
current iteration best possible eval_loss (full train run):  -0.8702529072761536
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4591 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.9897188865312424
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.9900429750050332
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.9901377575439869
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.9897578375893796
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.9873239880930628
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4979, dtype=torch.float64), 0, 0, tensor(0.0696, dtype=torch.float64), tensor(0.2722, dtype=torch.float64), tensor(0.1603, dtype=torch.float64), 0, 0, 0, 21, 0, 0, 0, 1, 1, 2, 0.1, 23.700432068770276, 0]
normalized proposed parameters for next round by BO: [tensor(0.4979, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0696, dtype=torch.float64), tensor(0.2722, dtype=torch.float64), tensor(0.1603, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3617e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6540, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4938, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.498
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.07
  triviaqa: 0.272
  truthfulqa_gen: 0.16
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (23.700432068770276,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  23.700432068770276
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,548,288 || all params: 8,031,809,536 || trainable%: 0.0193
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.7865, 'grad_norm': 8.1485595703125, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8961098194122314, 'eval_runtime': 7.5705, 'eval_samples_per_second': 132.092, 'eval_steps_per_second': 8.322, 'epoch': 0.04}
{'loss': 1.3, 'grad_norm': 2.0929043292999268, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4932339191436768, 'eval_runtime': 6.7146, 'eval_samples_per_second': 148.929, 'eval_steps_per_second': 9.383, 'epoch': 0.08}
{'loss': 0.9836, 'grad_norm': 1.1239243745803833, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5874193906784058, 'eval_runtime': 6.7081, 'eval_samples_per_second': 149.074, 'eval_steps_per_second': 9.392, 'epoch': 0.12}
{'loss': 0.9306, 'grad_norm': 0.9839081168174744, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5442286729812622, 'eval_runtime': 6.7352, 'eval_samples_per_second': 148.473, 'eval_steps_per_second': 9.354, 'epoch': 0.16}
{'loss': 0.92, 'grad_norm': 0.9109010100364685, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6453759670257568, 'eval_runtime': 6.7186, 'eval_samples_per_second': 148.841, 'eval_steps_per_second': 9.377, 'epoch': 0.2}
{'loss': 0.8982, 'grad_norm': 0.9271882176399231, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6095644235610962, 'eval_runtime': 6.7356, 'eval_samples_per_second': 148.464, 'eval_steps_per_second': 9.353, 'epoch': 0.24}
{'loss': 0.9107, 'grad_norm': 1.0469125509262085, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6112102270126343, 'eval_runtime': 6.7095, 'eval_samples_per_second': 149.042, 'eval_steps_per_second': 9.39, 'epoch': 0.28}
{'loss': 0.8799, 'grad_norm': 1.0188688039779663, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.554418683052063, 'eval_runtime': 6.7292, 'eval_samples_per_second': 148.607, 'eval_steps_per_second': 9.362, 'epoch': 0.32}
{'loss': 0.8738, 'grad_norm': 1.0138047933578491, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6091355085372925, 'eval_runtime': 6.7117, 'eval_samples_per_second': 148.993, 'eval_steps_per_second': 9.387, 'epoch': 0.36}
{'loss': 0.8743, 'grad_norm': 0.9523528218269348, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6327509880065918, 'eval_runtime': 6.7129, 'eval_samples_per_second': 148.967, 'eval_steps_per_second': 9.385, 'epoch': 0.4}
{'loss': 0.8588, 'grad_norm': 1.0538190603256226, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5945961475372314, 'eval_runtime': 6.7142, 'eval_samples_per_second': 148.938, 'eval_steps_per_second': 9.383, 'epoch': 0.44}
{'loss': 0.8531, 'grad_norm': 1.0746501684188843, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6131761074066162, 'eval_runtime': 6.7251, 'eval_samples_per_second': 148.697, 'eval_steps_per_second': 9.368, 'epoch': 0.48}
{'loss': 0.86, 'grad_norm': 1.0775569677352905, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6129465103149414, 'eval_runtime': 6.7596, 'eval_samples_per_second': 147.938, 'eval_steps_per_second': 9.32, 'epoch': 0.52}
{'loss': 0.8443, 'grad_norm': 1.0400450229644775, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6038168668746948, 'eval_runtime': 6.7171, 'eval_samples_per_second': 148.873, 'eval_steps_per_second': 9.379, 'epoch': 0.56}
{'loss': 0.8655, 'grad_norm': 1.2750664949417114, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5868628025054932, 'eval_runtime': 6.7231, 'eval_samples_per_second': 148.742, 'eval_steps_per_second': 9.371, 'epoch': 0.6}
{'loss': 0.8361, 'grad_norm': 1.1666110754013062, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5875557661056519, 'eval_runtime': 6.7361, 'eval_samples_per_second': 148.453, 'eval_steps_per_second': 9.353, 'epoch': 0.64}
{'loss': 0.8295, 'grad_norm': 1.2181130647659302, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.600797414779663, 'eval_runtime': 6.7028, 'eval_samples_per_second': 149.191, 'eval_steps_per_second': 9.399, 'epoch': 0.68}
{'loss': 0.8393, 'grad_norm': 1.156093716621399, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5826005935668945, 'eval_runtime': 6.7013, 'eval_samples_per_second': 149.225, 'eval_steps_per_second': 9.401, 'epoch': 0.72}
{'loss': 0.8434, 'grad_norm': 1.2224035263061523, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5995738506317139, 'eval_runtime': 6.6984, 'eval_samples_per_second': 149.289, 'eval_steps_per_second': 9.405, 'epoch': 0.76}
{'loss': 0.8223, 'grad_norm': 1.0423939228057861, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6105881929397583, 'eval_runtime': 6.7021, 'eval_samples_per_second': 149.207, 'eval_steps_per_second': 9.4, 'epoch': 0.8}
{'loss': 0.827, 'grad_norm': 1.1253615617752075, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6153559684753418, 'eval_runtime': 6.6879, 'eval_samples_per_second': 149.523, 'eval_steps_per_second': 9.42, 'epoch': 0.84}
{'loss': 0.822, 'grad_norm': 1.3371517658233643, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6198674440383911, 'eval_runtime': 6.663, 'eval_samples_per_second': 150.084, 'eval_steps_per_second': 9.455, 'epoch': 0.88}
{'loss': 0.8145, 'grad_norm': 1.2504212856292725, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6233422756195068, 'eval_runtime': 6.6668, 'eval_samples_per_second': 149.997, 'eval_steps_per_second': 9.45, 'epoch': 0.92}
{'loss': 0.8313, 'grad_norm': 1.3037257194519043, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6259301900863647, 'eval_runtime': 6.6755, 'eval_samples_per_second': 149.802, 'eval_steps_per_second': 9.438, 'epoch': 0.96}
{'loss': 0.8283, 'grad_norm': 1.3033305406570435, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6289136409759521, 'eval_runtime': 6.7073, 'eval_samples_per_second': 149.09, 'eval_steps_per_second': 9.393, 'epoch': 1.0}
{'train_runtime': 261.9661, 'train_samples_per_second': 38.165, 'train_steps_per_second': 2.386, 'train_loss': 0.9973279754638672, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8961098194122314, 1.4932339191436768, 1.5874193906784058, 1.5442286729812622, 1.6453759670257568, 1.6095644235610962, 1.6112102270126343, 1.554418683052063, 1.6091355085372925, 1.6327509880065918, 1.5945961475372314, 1.6131761074066162, 1.6129465103149414, 1.6038168668746948, 1.5868628025054932, 1.5875557661056519, 1.600797414779663, 1.5826005935668945, 1.5995738506317139, 1.6105881929397583, 1.6153559684753418, 1.6198674440383911, 1.6233422756195068, 1.6259301900863647, 1.6289136409759521], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8961098194122314, 1.4932339191436768, 1.5874193906784058, 1.5442286729812622, 1.6453759670257568, 1.6095644235610962, 1.6112102270126343, 1.554418683052063, 1.6091355085372925, 1.6327509880065918, 1.5945961475372314, 1.6131761074066162, 1.6129465103149414, 1.6038168668746948, 1.5868628025054932, 1.5875557661056519, 1.600797414779663, 1.5826005935668945, 1.5995738506317139, 1.6105881929397583, 1.6153559684753418, 1.6198674440383911, 1.6233422756195068, 1.6259301900863647, 1.6289136409759521]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760340690612793
current iteration best possible eval_loss (full train run):  -1.6289136409759521
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.4853 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.9776038041799262
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.981719808256418
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.9744008416309802
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.9824807251603118
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.977604927832644
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.2338, dtype=torch.float64), 0, tensor(0.3105, dtype=torch.float64), tensor(0.0825, dtype=torch.float64), tensor(0.3669, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 2, 0.09426396919569208, 28.92147975554201, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(5.5263e-18, dtype=torch.float64), tensor(0.0063, dtype=torch.float64), tensor(3.8489e-18, dtype=torch.float64), tensor(0.2338, dtype=torch.float64), tensor(3.5096e-18, dtype=torch.float64), tensor(0.3105, dtype=torch.float64), tensor(0.0825, dtype=torch.float64), tensor(0.3669, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.9426, dtype=torch.float64), tensor(0.6025, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.234
  truthfulqa_gen: 0
  wikitext: 0.311
  mmlu: 0.082
  arc_challenge: 0.367

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.09426396919569208,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (28.92147975554201,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  2
lora dropout:  0.09426396919569208
lora alpha:  28.92147975554201
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,211,264 || all params: 8,033,472,512 || trainable%: 0.0400
length of training data:  9935
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8309, 'grad_norm': 3.1054420471191406, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2064290046691895, 'eval_runtime': 7.3652, 'eval_samples_per_second': 135.774, 'eval_steps_per_second': 8.554, 'epoch': 0.04}
{'loss': 1.4952, 'grad_norm': 2.6624996662139893, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0384236574172974, 'eval_runtime': 7.3709, 'eval_samples_per_second': 135.669, 'eval_steps_per_second': 8.547, 'epoch': 0.08}
{'loss': 1.3603, 'grad_norm': 2.19047212600708, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.9373480677604675, 'eval_runtime': 7.384, 'eval_samples_per_second': 135.428, 'eval_steps_per_second': 8.532, 'epoch': 0.12}
{'loss': 1.3438, 'grad_norm': 1.4936503171920776, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.887272834777832, 'eval_runtime': 7.3853, 'eval_samples_per_second': 135.404, 'eval_steps_per_second': 8.53, 'epoch': 0.16}
{'loss': 1.2475, 'grad_norm': 1.6979633569717407, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.8735730051994324, 'eval_runtime': 7.3626, 'eval_samples_per_second': 135.822, 'eval_steps_per_second': 8.557, 'epoch': 0.2}
{'loss': 1.2292, 'grad_norm': 1.5678668022155762, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.8748252987861633, 'eval_runtime': 7.411, 'eval_samples_per_second': 134.935, 'eval_steps_per_second': 8.501, 'epoch': 0.24}
{'loss': 1.1932, 'grad_norm': 1.3740272521972656, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.8827618360519409, 'eval_runtime': 7.362, 'eval_samples_per_second': 135.834, 'eval_steps_per_second': 8.558, 'epoch': 0.28}
{'loss': 1.1296, 'grad_norm': 1.7488027811050415, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.8838624954223633, 'eval_runtime': 7.3684, 'eval_samples_per_second': 135.715, 'eval_steps_per_second': 8.55, 'epoch': 0.32}
{'loss': 1.2089, 'grad_norm': 1.6372054815292358, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.8893552422523499, 'eval_runtime': 7.3784, 'eval_samples_per_second': 135.53, 'eval_steps_per_second': 8.538, 'epoch': 0.36}
{'loss': 1.143, 'grad_norm': 1.8358110189437866, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.8990902900695801, 'eval_runtime': 7.399, 'eval_samples_per_second': 135.153, 'eval_steps_per_second': 8.515, 'epoch': 0.4}
{'loss': 1.0451, 'grad_norm': 2.965998888015747, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.9268642067909241, 'eval_runtime': 7.3781, 'eval_samples_per_second': 135.536, 'eval_steps_per_second': 8.539, 'epoch': 0.44}
{'loss': 1.1779, 'grad_norm': 2.182293176651001, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.9202569723129272, 'eval_runtime': 7.3963, 'eval_samples_per_second': 135.203, 'eval_steps_per_second': 8.518, 'epoch': 0.48}
{'loss': 1.1011, 'grad_norm': 1.801092267036438, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.9242666363716125, 'eval_runtime': 7.3378, 'eval_samples_per_second': 136.281, 'eval_steps_per_second': 8.586, 'epoch': 0.52}
{'loss': 1.007, 'grad_norm': 1.5324456691741943, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.9449071884155273, 'eval_runtime': 7.3358, 'eval_samples_per_second': 136.317, 'eval_steps_per_second': 8.588, 'epoch': 0.56}
{'loss': 1.0774, 'grad_norm': 2.4109861850738525, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.9439721703529358, 'eval_runtime': 7.3324, 'eval_samples_per_second': 136.382, 'eval_steps_per_second': 8.592, 'epoch': 0.6}
{'loss': 1.032, 'grad_norm': 1.9024485349655151, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.9485838413238525, 'eval_runtime': 7.3274, 'eval_samples_per_second': 136.473, 'eval_steps_per_second': 8.598, 'epoch': 0.64}
{'loss': 1.0842, 'grad_norm': 1.8048102855682373, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.9534709453582764, 'eval_runtime': 7.2961, 'eval_samples_per_second': 137.06, 'eval_steps_per_second': 8.635, 'epoch': 0.68}
{'loss': 1.0053, 'grad_norm': 2.9756572246551514, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.9815818071365356, 'eval_runtime': 7.2894, 'eval_samples_per_second': 137.185, 'eval_steps_per_second': 8.643, 'epoch': 0.72}
{'loss': 0.9621, 'grad_norm': 2.204078435897827, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.966231644153595, 'eval_runtime': 7.2992, 'eval_samples_per_second': 137.001, 'eval_steps_per_second': 8.631, 'epoch': 0.76}
{'loss': 0.9854, 'grad_norm': 1.797611951828003, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.9988815188407898, 'eval_runtime': 7.3058, 'eval_samples_per_second': 136.878, 'eval_steps_per_second': 8.623, 'epoch': 0.81}
{'loss': 1.0004, 'grad_norm': 2.032019853591919, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.0092169046401978, 'eval_runtime': 7.3026, 'eval_samples_per_second': 136.938, 'eval_steps_per_second': 8.627, 'epoch': 0.85}
{'loss': 1.0418, 'grad_norm': 1.7586865425109863, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.00045645236969, 'eval_runtime': 7.3097, 'eval_samples_per_second': 136.804, 'eval_steps_per_second': 8.619, 'epoch': 0.89}
{'loss': 0.9725, 'grad_norm': 2.5682122707366943, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.0265376567840576, 'eval_runtime': 7.3038, 'eval_samples_per_second': 136.915, 'eval_steps_per_second': 8.626, 'epoch': 0.93}
{'loss': 0.9273, 'grad_norm': 2.9227066040039062, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.0225918292999268, 'eval_runtime': 7.3049, 'eval_samples_per_second': 136.895, 'eval_steps_per_second': 8.624, 'epoch': 0.97}
{'train_runtime': 385.5793, 'train_samples_per_second': 25.766, 'train_steps_per_second': 1.611, 'train_loss': 1.1843927325065968, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2064290046691895, 1.0384236574172974, 0.9373480677604675, 0.887272834777832, 0.8735730051994324, 0.8748252987861633, 0.8827618360519409, 0.8838624954223633, 0.8893552422523499, 0.8990902900695801, 0.9268642067909241, 0.9202569723129272, 0.9242666363716125, 0.9449071884155273, 0.9439721703529358, 0.9485838413238525, 0.9534709453582764, 0.9815818071365356, 0.966231644153595, 0.9988815188407898, 1.0092169046401978, 1.00045645236969, 1.0265376567840576, 1.0225918292999268], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.2064290046691895, 1.0384236574172974, 0.9373480677604675, 0.887272834777832, 0.8735730051994324, 0.8748252987861633, 0.8827618360519409, 0.8838624954223633, 0.8893552422523499, 0.8990902900695801, 0.9268642067909241, 0.9202569723129272, 0.9242666363716125, 0.9449071884155273, 0.9439721703529358, 0.9485838413238525, 0.9534709453582764, 0.9815818071365356, 0.966231644153595, 0.9988815188407898, 1.0092169046401978, 1.00045645236969, 1.0265376567840576, 1.0225918292999268]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760291814804077
current iteration best possible eval_loss (full train run):  -1.0225918292999268
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.5006 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1795889139175415, 0.5639668703079224, 0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.6092051267623901, 0.8282220363616943, 0.8343875408172607, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.22647850215435028, 0.034817516803741455, 0.2369377166032791, 0.8301550149917603]  ‚Üí  acq = -0.9863608291979998
X = [0.3627225160598755, 0.4275026321411133, 0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.20957553386688232, 0.29544466733932495, 0.45958811044692993, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7996509075164795, 0.16252559423446655, 0.23583632707595825, 0.46233898401260376]  ‚Üí  acq = -0.9863630108289307
X = [0.3497363328933716, 0.9076562523841858, 0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.451727032661438, 0.6221595406532288, 0.021270394325256348, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.990592360496521, 0.6705264449119568, 0.5292245149612427, 0.4145408272743225]  ‚Üí  acq = -0.9854187036475367
X = [0.8545095920562744, 0.7290428876876831, 0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.1896294802427292, 0.13708847761154175, 0.2295888066291809, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.42133286595344543, 0.9889391660690308, 0.27955883741378784, 0.647262454032898]  ‚Üí  acq = -0.9863658066038776
X = [0.25802141427993774, 0.15090978145599365, 0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.16085723042488098, 0.13615381717681885, 0.3176853060722351, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7902160882949829, 0.012897372245788574, 0.5332701206207275, 0.154333233833313]  ‚Üí  acq = -0.9863608275760352
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1920, dtype=torch.float64), 0, tensor(0.0999, dtype=torch.float64), tensor(0.0241, dtype=torch.float64), 0, tensor(0.0401, dtype=torch.float64), tensor(0.1408, dtype=torch.float64), tensor(0.5031, dtype=torch.float64), 0, 32, 0, 1, 1, 0, 1, 64, 0.03642429903759677, 40.880266460227844, 0]
normalized proposed parameters for next round by BO: [tensor(0.1920, dtype=torch.float64), tensor(1.7002e-19, dtype=torch.float64), tensor(0.0999, dtype=torch.float64), tensor(0.0241, dtype=torch.float64), tensor(2.0856e-19, dtype=torch.float64), tensor(0.0401, dtype=torch.float64), tensor(0.1408, dtype=torch.float64), tensor(0.5031, dtype=torch.float64), tensor(9.0743e-20, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5026, dtype=torch.float64), tensor(0.3642, dtype=torch.float64), tensor(0.8517, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.192
  gsm8k: 0
  rowan_hellaswag: 0.1
  sciq: 0.024
  triviaqa: 0
  truthfulqa_gen: 0.04
  wikitext: 0.141
  mmlu: 0.503
  arc_challenge: 0

LoRA Parameters:
  lora_r: (64,)
  lora_dropout: (0.03642429903759677,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (40.880266460227844,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  64
lora dropout:  0.03642429903759677
lora alpha:  40.880266460227844
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 85,983,232 || all params: 8,116,244,480 || trainable%: 1.0594
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8438, 'grad_norm': 1.2062721252441406, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4044837951660156, 'eval_runtime': 6.9545, 'eval_samples_per_second': 143.792, 'eval_steps_per_second': 9.059, 'epoch': 0.04}
{'loss': 1.6912, 'grad_norm': 0.511435329914093, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.348489761352539, 'eval_runtime': 6.9413, 'eval_samples_per_second': 144.065, 'eval_steps_per_second': 9.076, 'epoch': 0.08}
{'loss': 1.5077, 'grad_norm': 0.32204294204711914, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2682236433029175, 'eval_runtime': 6.9535, 'eval_samples_per_second': 143.813, 'eval_steps_per_second': 9.06, 'epoch': 0.12}
{'loss': 1.4072, 'grad_norm': 0.3327876031398773, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2102537155151367, 'eval_runtime': 6.9475, 'eval_samples_per_second': 143.937, 'eval_steps_per_second': 9.068, 'epoch': 0.16}
{'loss': 1.3348, 'grad_norm': 0.27030107378959656, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1770087480545044, 'eval_runtime': 6.9681, 'eval_samples_per_second': 143.51, 'eval_steps_per_second': 9.041, 'epoch': 0.2}
{'loss': 1.323, 'grad_norm': 0.31422993540763855, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2186399698257446, 'eval_runtime': 6.9608, 'eval_samples_per_second': 143.662, 'eval_steps_per_second': 9.051, 'epoch': 0.24}
{'loss': 1.3608, 'grad_norm': 0.27178701758384705, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2253730297088623, 'eval_runtime': 6.9966, 'eval_samples_per_second': 142.926, 'eval_steps_per_second': 9.004, 'epoch': 0.28}
{'loss': 1.3617, 'grad_norm': 0.3044211268424988, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.189910650253296, 'eval_runtime': 7.0048, 'eval_samples_per_second': 142.76, 'eval_steps_per_second': 8.994, 'epoch': 0.32}
{'loss': 1.3086, 'grad_norm': 0.32196739315986633, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2291209697723389, 'eval_runtime': 6.9937, 'eval_samples_per_second': 142.986, 'eval_steps_per_second': 9.008, 'epoch': 0.36}
{'loss': 1.348, 'grad_norm': 0.4693010747432709, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2081620693206787, 'eval_runtime': 6.9886, 'eval_samples_per_second': 143.09, 'eval_steps_per_second': 9.015, 'epoch': 0.4}
{'loss': 1.3084, 'grad_norm': 0.32958468794822693, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2194526195526123, 'eval_runtime': 6.9832, 'eval_samples_per_second': 143.2, 'eval_steps_per_second': 9.022, 'epoch': 0.44}
{'loss': 1.2713, 'grad_norm': 0.27287864685058594, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2183103561401367, 'eval_runtime': 6.9883, 'eval_samples_per_second': 143.097, 'eval_steps_per_second': 9.015, 'epoch': 0.48}
{'loss': 1.2902, 'grad_norm': 0.3425680696964264, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.23988938331604, 'eval_runtime': 6.9909, 'eval_samples_per_second': 143.044, 'eval_steps_per_second': 9.012, 'epoch': 0.52}
{'loss': 1.33, 'grad_norm': 0.2656116485595703, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2358856201171875, 'eval_runtime': 6.9814, 'eval_samples_per_second': 143.239, 'eval_steps_per_second': 9.024, 'epoch': 0.56}
{'loss': 1.3041, 'grad_norm': 0.44941720366477966, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.244494915008545, 'eval_runtime': 6.9864, 'eval_samples_per_second': 143.135, 'eval_steps_per_second': 9.017, 'epoch': 0.6}
{'loss': 1.3504, 'grad_norm': 0.30061987042427063, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.205364465713501, 'eval_runtime': 6.9724, 'eval_samples_per_second': 143.422, 'eval_steps_per_second': 9.036, 'epoch': 0.64}
{'loss': 1.3563, 'grad_norm': 0.4386518895626068, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2350482940673828, 'eval_runtime': 6.9593, 'eval_samples_per_second': 143.693, 'eval_steps_per_second': 9.053, 'epoch': 0.68}
{'loss': 1.2364, 'grad_norm': 0.30320385098457336, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2303422689437866, 'eval_runtime': 6.9568, 'eval_samples_per_second': 143.744, 'eval_steps_per_second': 9.056, 'epoch': 0.72}
{'loss': 1.2663, 'grad_norm': 0.2899511456489563, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.242376446723938, 'eval_runtime': 6.9433, 'eval_samples_per_second': 144.025, 'eval_steps_per_second': 9.074, 'epoch': 0.76}
{'loss': 1.2419, 'grad_norm': 0.3063189387321472, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2414883375167847, 'eval_runtime': 6.9564, 'eval_samples_per_second': 143.753, 'eval_steps_per_second': 9.056, 'epoch': 0.8}
{'loss': 1.2628, 'grad_norm': 0.29286298155784607, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2407124042510986, 'eval_runtime': 7.0093, 'eval_samples_per_second': 142.667, 'eval_steps_per_second': 8.988, 'epoch': 0.84}
{'loss': 1.2486, 'grad_norm': 0.3396894931793213, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2343405485153198, 'eval_runtime': 6.9787, 'eval_samples_per_second': 143.294, 'eval_steps_per_second': 9.028, 'epoch': 0.88}
{'loss': 1.3101, 'grad_norm': 0.2633073329925537, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2375898361206055, 'eval_runtime': 6.9983, 'eval_samples_per_second': 142.891, 'eval_steps_per_second': 9.002, 'epoch': 0.92}
{'loss': 1.2731, 'grad_norm': 0.314912885427475, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2394261360168457, 'eval_runtime': 6.9946, 'eval_samples_per_second': 142.968, 'eval_steps_per_second': 9.007, 'epoch': 0.96}
{'loss': 1.3047, 'grad_norm': 0.3114279508590698, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.238518238067627, 'eval_runtime': 6.9877, 'eval_samples_per_second': 143.109, 'eval_steps_per_second': 9.016, 'epoch': 1.0}
{'train_runtime': 400.5512, 'train_samples_per_second': 24.958, 'train_steps_per_second': 1.56, 'train_loss': 1.3936489593505859, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4044837951660156, 1.348489761352539, 1.2682236433029175, 1.2102537155151367, 1.1770087480545044, 1.2186399698257446, 1.2253730297088623, 1.189910650253296, 1.2291209697723389, 1.2081620693206787, 1.2194526195526123, 1.2183103561401367, 1.23988938331604, 1.2358856201171875, 1.244494915008545, 1.205364465713501, 1.2350482940673828, 1.2303422689437866, 1.242376446723938, 1.2414883375167847, 1.2407124042510986, 1.2343405485153198, 1.2375898361206055, 1.2394261360168457, 1.238518238067627], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4044837951660156, 1.348489761352539, 1.2682236433029175, 1.2102537155151367, 1.1770087480545044, 1.2186399698257446, 1.2253730297088623, 1.189910650253296, 1.2291209697723389, 1.2081620693206787, 1.2194526195526123, 1.2183103561401367, 1.23988938331604, 1.2358856201171875, 1.244494915008545, 1.205364465713501, 1.2350482940673828, 1.2303422689437866, 1.242376446723938, 1.2414883375167847, 1.2407124042510986, 1.2343405485153198, 1.2375898361206055, 1.2394261360168457, 1.238518238067627]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1062551736831665
current iteration best possible eval_loss (full train run):  -1.238518238067627
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.1244 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -1.0123349509275847
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -1.0123349509275847
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -1.0123349509275847
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -1.0123349509275847
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -1.0123349509275847
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4624, dtype=torch.float64), tensor(0.4740, dtype=torch.float64), tensor(0.0637, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 24, 0, 1, 1, 1, 1, 128, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.4624, dtype=torch.float64), tensor(0.4740, dtype=torch.float64), tensor(0.0637, dtype=torch.float64), tensor(5.5707e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.2326e-17, dtype=torch.float64), tensor(3.1050e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7439, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.462
  gsm8k: 0.474
  rowan_hellaswag: 0.064
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 185,597,952 || all params: 8,215,859,200 || trainable%: 2.2590
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.2728, 'grad_norm': 0.49386072158813477, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5294843912124634, 'eval_runtime': 7.1921, 'eval_samples_per_second': 139.041, 'eval_steps_per_second': 8.76, 'epoch': 0.04}
{'loss': 1.1791, 'grad_norm': 0.36805200576782227, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.523872971534729, 'eval_runtime': 7.1292, 'eval_samples_per_second': 140.267, 'eval_steps_per_second': 8.837, 'epoch': 0.08}
{'loss': 1.0136, 'grad_norm': 0.2360481470823288, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5580785274505615, 'eval_runtime': 7.1333, 'eval_samples_per_second': 140.187, 'eval_steps_per_second': 8.832, 'epoch': 0.12}
{'loss': 1.0444, 'grad_norm': 0.24119806289672852, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4259319305419922, 'eval_runtime': 7.1692, 'eval_samples_per_second': 139.485, 'eval_steps_per_second': 8.788, 'epoch': 0.16}
{'loss': 0.9647, 'grad_norm': 0.22897346317768097, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4514001607894897, 'eval_runtime': 7.1533, 'eval_samples_per_second': 139.797, 'eval_steps_per_second': 8.807, 'epoch': 0.2}
{'loss': 0.9333, 'grad_norm': 0.22522257268428802, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.468706488609314, 'eval_runtime': 7.1613, 'eval_samples_per_second': 139.639, 'eval_steps_per_second': 8.797, 'epoch': 0.24}
{'loss': 0.9167, 'grad_norm': 0.22610923647880554, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4865018129348755, 'eval_runtime': 7.1612, 'eval_samples_per_second': 139.642, 'eval_steps_per_second': 8.797, 'epoch': 0.28}
{'loss': 0.9313, 'grad_norm': 0.19127069413661957, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.407049298286438, 'eval_runtime': 7.1677, 'eval_samples_per_second': 139.515, 'eval_steps_per_second': 8.789, 'epoch': 0.32}
{'loss': 0.9324, 'grad_norm': 0.20818696916103363, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4641069173812866, 'eval_runtime': 7.1693, 'eval_samples_per_second': 139.484, 'eval_steps_per_second': 8.788, 'epoch': 0.36}
{'loss': 0.9271, 'grad_norm': 0.2197701632976532, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4801409244537354, 'eval_runtime': 7.1815, 'eval_samples_per_second': 139.246, 'eval_steps_per_second': 8.773, 'epoch': 0.4}
{'loss': 0.9319, 'grad_norm': 0.20538900792598724, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4466909170150757, 'eval_runtime': 7.1755, 'eval_samples_per_second': 139.362, 'eval_steps_per_second': 8.78, 'epoch': 0.44}
{'loss': 0.8911, 'grad_norm': 0.18199537694454193, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.46303129196167, 'eval_runtime': 7.1751, 'eval_samples_per_second': 139.372, 'eval_steps_per_second': 8.78, 'epoch': 0.48}
{'loss': 0.8878, 'grad_norm': 0.2026665210723877, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.465278148651123, 'eval_runtime': 7.1949, 'eval_samples_per_second': 138.986, 'eval_steps_per_second': 8.756, 'epoch': 0.52}
{'loss': 0.9102, 'grad_norm': 0.1889171153306961, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5068013668060303, 'eval_runtime': 7.1728, 'eval_samples_per_second': 139.415, 'eval_steps_per_second': 8.783, 'epoch': 0.56}
{'loss': 0.8653, 'grad_norm': 0.1755000352859497, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5277951955795288, 'eval_runtime': 7.1797, 'eval_samples_per_second': 139.281, 'eval_steps_per_second': 8.775, 'epoch': 0.6}
{'loss': 0.9361, 'grad_norm': 0.2070038765668869, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5098694562911987, 'eval_runtime': 7.1592, 'eval_samples_per_second': 139.681, 'eval_steps_per_second': 8.8, 'epoch': 0.64}
{'loss': 0.9033, 'grad_norm': 0.2020740509033203, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4945720434188843, 'eval_runtime': 7.1486, 'eval_samples_per_second': 139.888, 'eval_steps_per_second': 8.813, 'epoch': 0.68}
{'loss': 0.8768, 'grad_norm': 0.1984514594078064, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.554416298866272, 'eval_runtime': 7.1531, 'eval_samples_per_second': 139.799, 'eval_steps_per_second': 8.807, 'epoch': 0.72}
{'loss': 0.8587, 'grad_norm': 0.19371876120567322, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5004608631134033, 'eval_runtime': 7.1764, 'eval_samples_per_second': 139.346, 'eval_steps_per_second': 8.779, 'epoch': 0.76}
{'loss': 0.8554, 'grad_norm': 0.21116992831230164, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.542752981185913, 'eval_runtime': 7.1852, 'eval_samples_per_second': 139.176, 'eval_steps_per_second': 8.768, 'epoch': 0.8}
{'loss': 0.888, 'grad_norm': 0.18585172295570374, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5065302848815918, 'eval_runtime': 7.1899, 'eval_samples_per_second': 139.083, 'eval_steps_per_second': 8.762, 'epoch': 0.84}
{'loss': 0.8777, 'grad_norm': 0.18663597106933594, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5244462490081787, 'eval_runtime': 7.1543, 'eval_samples_per_second': 139.776, 'eval_steps_per_second': 8.806, 'epoch': 0.88}
{'loss': 0.8767, 'grad_norm': 0.2158004194498062, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5351697206497192, 'eval_runtime': 7.1348, 'eval_samples_per_second': 140.158, 'eval_steps_per_second': 8.83, 'epoch': 0.92}
{'loss': 0.8753, 'grad_norm': 0.20382088422775269, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5461701154708862, 'eval_runtime': 7.1393, 'eval_samples_per_second': 140.071, 'eval_steps_per_second': 8.824, 'epoch': 0.96}
{'loss': 0.8353, 'grad_norm': 0.21578405797481537, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5413161516189575, 'eval_runtime': 7.1292, 'eval_samples_per_second': 140.269, 'eval_steps_per_second': 8.837, 'epoch': 1.0}
{'train_runtime': 409.5479, 'train_samples_per_second': 24.412, 'train_steps_per_second': 1.526, 'train_loss': 0.9753986389160156, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5294843912124634, 1.523872971534729, 1.5580785274505615, 1.4259319305419922, 1.4514001607894897, 1.468706488609314, 1.4865018129348755, 1.407049298286438, 1.4641069173812866, 1.4801409244537354, 1.4466909170150757, 1.46303129196167, 1.465278148651123, 1.5068013668060303, 1.5277951955795288, 1.5098694562911987, 1.4945720434188843, 1.554416298866272, 1.5004608631134033, 1.542752981185913, 1.5065302848815918, 1.5244462490081787, 1.5351697206497192, 1.5461701154708862, 1.5413161516189575], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5294843912124634, 1.523872971534729, 1.5580785274505615, 1.4259319305419922, 1.4514001607894897, 1.468706488609314, 1.4865018129348755, 1.407049298286438, 1.4641069173812866, 1.4801409244537354, 1.4466909170150757, 1.46303129196167, 1.465278148651123, 1.5068013668060303, 1.5277951955795288, 1.5098694562911987, 1.4945720434188843, 1.554416298866272, 1.5004608631134033, 1.542752981185913, 1.5065302848815918, 1.5244462490081787, 1.5351697206497192, 1.5461701154708862, 1.5413161516189575]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.4758241176605225
current iteration best possible eval_loss (full train run):  -1.5413161516189575
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.4732 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.996934935337231
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -1.0587563816224974
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.9740486221814073
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -1.009513129575322
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.9965035701751219
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.8058, dtype=torch.float64), tensor(0.0174, dtype=torch.float64), 0, 0, 0, tensor(0.0562, dtype=torch.float64), 0, tensor(0.1206, dtype=torch.float64), 32, 1, 1, 0, 1, 1, 2, 0.023462185471183682, 23.9319938619329, 1]
normalized proposed parameters for next round by BO: [tensor(3.0944e-17, dtype=torch.float64), tensor(0.8058, dtype=torch.float64), tensor(0.0174, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.4820e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0562, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1206, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.2346, dtype=torch.float64), tensor(0.4986, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.806
  rowan_hellaswag: 0.017
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.056
  mmlu: 0
  arc_challenge: 0.121

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.023462185471183682,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (23.9319938619329,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  2
lora dropout:  0.023462185471183682
lora alpha:  23.9319938619329
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,211,264 || all params: 8,033,472,512 || trainable%: 0.0400
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.0052, 'grad_norm': 3.3468539714813232, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6656428575515747, 'eval_runtime': 7.3683, 'eval_samples_per_second': 135.717, 'eval_steps_per_second': 8.55, 'epoch': 0.04}
{'loss': 1.0672, 'grad_norm': 1.2772578001022339, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9310975074768066, 'eval_runtime': 7.3636, 'eval_samples_per_second': 135.803, 'eval_steps_per_second': 8.556, 'epoch': 0.08}
{'loss': 0.9219, 'grad_norm': 0.9002858400344849, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9038504362106323, 'eval_runtime': 7.3814, 'eval_samples_per_second': 135.476, 'eval_steps_per_second': 8.535, 'epoch': 0.12}
{'loss': 0.9157, 'grad_norm': 0.9490084052085876, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8841864466667175, 'eval_runtime': 7.3895, 'eval_samples_per_second': 135.327, 'eval_steps_per_second': 8.526, 'epoch': 0.16}
{'loss': 0.8656, 'grad_norm': 0.9055902361869812, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8825774192810059, 'eval_runtime': 7.3999, 'eval_samples_per_second': 135.137, 'eval_steps_per_second': 8.514, 'epoch': 0.2}
{'loss': 0.8951, 'grad_norm': 0.8386166095733643, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8895342946052551, 'eval_runtime': 7.4127, 'eval_samples_per_second': 134.904, 'eval_steps_per_second': 8.499, 'epoch': 0.24}
{'loss': 0.8876, 'grad_norm': 0.8851929306983948, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8818090558052063, 'eval_runtime': 7.4135, 'eval_samples_per_second': 134.888, 'eval_steps_per_second': 8.498, 'epoch': 0.28}
{'loss': 0.8593, 'grad_norm': 0.8213877081871033, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8798500299453735, 'eval_runtime': 7.4206, 'eval_samples_per_second': 134.76, 'eval_steps_per_second': 8.49, 'epoch': 0.32}
{'loss': 0.8558, 'grad_norm': 0.9166712164878845, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8800817728042603, 'eval_runtime': 7.4681, 'eval_samples_per_second': 133.903, 'eval_steps_per_second': 8.436, 'epoch': 0.36}
{'loss': 0.8227, 'grad_norm': 0.7622936367988586, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8748268485069275, 'eval_runtime': 7.4912, 'eval_samples_per_second': 133.49, 'eval_steps_per_second': 8.41, 'epoch': 0.4}
{'loss': 0.8588, 'grad_norm': 1.1618859767913818, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.882286548614502, 'eval_runtime': 7.456, 'eval_samples_per_second': 134.119, 'eval_steps_per_second': 8.45, 'epoch': 0.44}
{'loss': 0.8595, 'grad_norm': 0.9283486604690552, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8703062534332275, 'eval_runtime': 7.4352, 'eval_samples_per_second': 134.495, 'eval_steps_per_second': 8.473, 'epoch': 0.48}
{'loss': 0.8367, 'grad_norm': 0.8564273715019226, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8782657384872437, 'eval_runtime': 7.4391, 'eval_samples_per_second': 134.424, 'eval_steps_per_second': 8.469, 'epoch': 0.52}
{'loss': 0.8044, 'grad_norm': 0.8808335065841675, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8724355697631836, 'eval_runtime': 7.4311, 'eval_samples_per_second': 134.57, 'eval_steps_per_second': 8.478, 'epoch': 0.56}
{'loss': 0.8305, 'grad_norm': 1.030326008796692, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8797528743743896, 'eval_runtime': 7.438, 'eval_samples_per_second': 134.445, 'eval_steps_per_second': 8.47, 'epoch': 0.6}
{'loss': 0.7938, 'grad_norm': 0.8518844246864319, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8754337430000305, 'eval_runtime': 7.4396, 'eval_samples_per_second': 134.416, 'eval_steps_per_second': 8.468, 'epoch': 0.64}
{'loss': 0.8175, 'grad_norm': 1.045166254043579, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8798731565475464, 'eval_runtime': 7.4418, 'eval_samples_per_second': 134.376, 'eval_steps_per_second': 8.466, 'epoch': 0.68}
{'loss': 0.8037, 'grad_norm': 0.9477978348731995, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8809736967086792, 'eval_runtime': 7.4456, 'eval_samples_per_second': 134.307, 'eval_steps_per_second': 8.461, 'epoch': 0.72}
{'loss': 0.8068, 'grad_norm': 1.104728102684021, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.885800302028656, 'eval_runtime': 7.4426, 'eval_samples_per_second': 134.361, 'eval_steps_per_second': 8.465, 'epoch': 0.76}
{'loss': 0.772, 'grad_norm': 1.0022929906845093, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8851176500320435, 'eval_runtime': 7.443, 'eval_samples_per_second': 134.354, 'eval_steps_per_second': 8.464, 'epoch': 0.8}
{'loss': 0.8123, 'grad_norm': 0.9915125370025635, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8817629218101501, 'eval_runtime': 7.4362, 'eval_samples_per_second': 134.478, 'eval_steps_per_second': 8.472, 'epoch': 0.84}
{'loss': 0.8254, 'grad_norm': 1.0530771017074585, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8798463344573975, 'eval_runtime': 7.3942, 'eval_samples_per_second': 135.241, 'eval_steps_per_second': 8.52, 'epoch': 0.88}
{'loss': 0.8235, 'grad_norm': 0.9579891562461853, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.878317654132843, 'eval_runtime': 7.4026, 'eval_samples_per_second': 135.087, 'eval_steps_per_second': 8.51, 'epoch': 0.92}
{'loss': 0.8057, 'grad_norm': 1.0177689790725708, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8780019879341125, 'eval_runtime': 7.4066, 'eval_samples_per_second': 135.014, 'eval_steps_per_second': 8.506, 'epoch': 0.96}
{'loss': 0.7902, 'grad_norm': 0.9045102596282959, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8793078660964966, 'eval_runtime': 7.4005, 'eval_samples_per_second': 135.126, 'eval_steps_per_second': 8.513, 'epoch': 1.0}
{'train_runtime': 440.5186, 'train_samples_per_second': 22.696, 'train_steps_per_second': 1.419, 'train_loss': 0.8934768493652344, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6656428575515747, 0.9310975074768066, 0.9038504362106323, 0.8841864466667175, 0.8825774192810059, 0.8895342946052551, 0.8818090558052063, 0.8798500299453735, 0.8800817728042603, 0.8748268485069275, 0.882286548614502, 0.8703062534332275, 0.8782657384872437, 0.8724355697631836, 0.8797528743743896, 0.8754337430000305, 0.8798731565475464, 0.8809736967086792, 0.885800302028656, 0.8851176500320435, 0.8817629218101501, 0.8798463344573975, 0.878317654132843, 0.8780019879341125, 0.8793078660964966], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6656428575515747, 0.9310975074768066, 0.9038504362106323, 0.8841864466667175, 0.8825774192810059, 0.8895342946052551, 0.8818090558052063, 0.8798500299453735, 0.8800817728042603, 0.8748268485069275, 0.882286548614502, 0.8703062534332275, 0.8782657384872437, 0.8724355697631836, 0.8797528743743896, 0.8754337430000305, 0.8798731565475464, 0.8809736967086792, 0.885800302028656, 0.8851176500320435, 0.8817629218101501, 0.8798463344573975, 0.878317654132843, 0.8780019879341125, 0.8793078660964966]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760295391082764
current iteration best possible eval_loss (full train run):  -0.8793078660964966
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.5252 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -1.0033960593083533
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.9882385832856834
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -1.0033955531057774
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -1.0033955584521115
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -1.008593227176902
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1666, dtype=torch.float64), tensor(0.1625, dtype=torch.float64), tensor(0.0478, dtype=torch.float64), tensor(0.3086, dtype=torch.float64), tensor(0.0987, dtype=torch.float64), 0, 0, tensor(0.0667, dtype=torch.float64), tensor(0.1491, dtype=torch.float64), 27, 1, 1, 0, 1, 1, 76, 5.843653068837864e-21, 42.764537355649836, 1]
normalized proposed parameters for next round by BO: [tensor(0.1666, dtype=torch.float64), tensor(0.1625, dtype=torch.float64), tensor(0.0478, dtype=torch.float64), tensor(0.3086, dtype=torch.float64), tensor(0.0987, dtype=torch.float64), tensor(3.7517e-19, dtype=torch.float64), tensor(7.9659e-20, dtype=torch.float64), tensor(0.0667, dtype=torch.float64), tensor(0.1491, dtype=torch.float64), tensor(0.8579, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5950, dtype=torch.float64), tensor(5.8437e-20, dtype=torch.float64), tensor(0.8909, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.167
  gsm8k: 0.162
  rowan_hellaswag: 0.048
  sciq: 0.309
  triviaqa: 0.099
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.067
  arc_challenge: 0.149

LoRA Parameters:
  lora_r: (76,)
  lora_dropout: (5.843653068837864e-21,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (42.764537355649836,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  76
lora dropout:  5.843653068837864e-21
lora alpha:  42.764537355649836
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 102,961,152 || all params: 8,133,222,400 || trainable%: 1.2659
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7659, 'grad_norm': 1.0507248640060425, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2491978406906128, 'eval_runtime': 7.3176, 'eval_samples_per_second': 136.656, 'eval_steps_per_second': 8.609, 'epoch': 0.04}
{'loss': 1.2651, 'grad_norm': 0.6623484492301941, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0101675987243652, 'eval_runtime': 7.303, 'eval_samples_per_second': 136.929, 'eval_steps_per_second': 8.627, 'epoch': 0.08}
{'loss': 1.0914, 'grad_norm': 0.3284844160079956, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9467183947563171, 'eval_runtime': 7.3561, 'eval_samples_per_second': 135.942, 'eval_steps_per_second': 8.564, 'epoch': 0.12}
{'loss': 0.977, 'grad_norm': 0.3867626190185547, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9092718362808228, 'eval_runtime': 7.3894, 'eval_samples_per_second': 135.329, 'eval_steps_per_second': 8.526, 'epoch': 0.16}
{'loss': 1.0451, 'grad_norm': 0.32791709899902344, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8909697532653809, 'eval_runtime': 7.401, 'eval_samples_per_second': 135.116, 'eval_steps_per_second': 8.512, 'epoch': 0.2}
{'loss': 0.9813, 'grad_norm': 0.40456250309944153, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8930208683013916, 'eval_runtime': 7.3975, 'eval_samples_per_second': 135.181, 'eval_steps_per_second': 8.516, 'epoch': 0.24}
{'loss': 1.0108, 'grad_norm': 0.290267676115036, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8827208280563354, 'eval_runtime': 7.3713, 'eval_samples_per_second': 135.661, 'eval_steps_per_second': 8.547, 'epoch': 0.28}
{'loss': 0.9661, 'grad_norm': 0.30561932921409607, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8799833059310913, 'eval_runtime': 7.3406, 'eval_samples_per_second': 136.229, 'eval_steps_per_second': 8.582, 'epoch': 0.32}
{'loss': 0.9737, 'grad_norm': 0.4273153841495514, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8737947940826416, 'eval_runtime': 7.3683, 'eval_samples_per_second': 135.717, 'eval_steps_per_second': 8.55, 'epoch': 0.36}
{'loss': 0.9685, 'grad_norm': 0.33058828115463257, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8845623731613159, 'eval_runtime': 7.3295, 'eval_samples_per_second': 136.435, 'eval_steps_per_second': 8.595, 'epoch': 0.4}
{'loss': 0.9477, 'grad_norm': 0.3585076332092285, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.876317024230957, 'eval_runtime': 7.3303, 'eval_samples_per_second': 136.419, 'eval_steps_per_second': 8.594, 'epoch': 0.44}
{'loss': 0.9915, 'grad_norm': 0.3728957772254944, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8705298900604248, 'eval_runtime': 7.3238, 'eval_samples_per_second': 136.541, 'eval_steps_per_second': 8.602, 'epoch': 0.48}
{'loss': 0.9867, 'grad_norm': 0.28144267201423645, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8729050159454346, 'eval_runtime': 7.3532, 'eval_samples_per_second': 135.996, 'eval_steps_per_second': 8.568, 'epoch': 0.52}
{'loss': 0.9245, 'grad_norm': 0.3202056586742401, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8794191479682922, 'eval_runtime': 7.3298, 'eval_samples_per_second': 136.429, 'eval_steps_per_second': 8.595, 'epoch': 0.56}
{'loss': 0.9687, 'grad_norm': 0.2964402437210083, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8758560419082642, 'eval_runtime': 7.3379, 'eval_samples_per_second': 136.279, 'eval_steps_per_second': 8.586, 'epoch': 0.6}
{'loss': 0.9933, 'grad_norm': 0.29184287786483765, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8694607019424438, 'eval_runtime': 7.3137, 'eval_samples_per_second': 136.73, 'eval_steps_per_second': 8.614, 'epoch': 0.64}
{'loss': 0.9744, 'grad_norm': 0.2824913263320923, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8676943778991699, 'eval_runtime': 7.2855, 'eval_samples_per_second': 137.26, 'eval_steps_per_second': 8.647, 'epoch': 0.68}
{'loss': 0.9652, 'grad_norm': 0.33005839586257935, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8752885460853577, 'eval_runtime': 7.2988, 'eval_samples_per_second': 137.01, 'eval_steps_per_second': 8.632, 'epoch': 0.72}
{'loss': 0.9848, 'grad_norm': 0.5349186062812805, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8714874386787415, 'eval_runtime': 7.2839, 'eval_samples_per_second': 137.29, 'eval_steps_per_second': 8.649, 'epoch': 0.76}
{'loss': 0.8878, 'grad_norm': 0.2978610694408417, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8698980808258057, 'eval_runtime': 7.296, 'eval_samples_per_second': 137.062, 'eval_steps_per_second': 8.635, 'epoch': 0.8}
{'loss': 0.9393, 'grad_norm': 0.30352506041526794, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.874987781047821, 'eval_runtime': 7.2958, 'eval_samples_per_second': 137.066, 'eval_steps_per_second': 8.635, 'epoch': 0.84}
{'loss': 0.928, 'grad_norm': 0.2688136398792267, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8714911937713623, 'eval_runtime': 7.3113, 'eval_samples_per_second': 136.774, 'eval_steps_per_second': 8.617, 'epoch': 0.88}
{'loss': 0.9075, 'grad_norm': 0.2747138738632202, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8719207644462585, 'eval_runtime': 7.3049, 'eval_samples_per_second': 136.895, 'eval_steps_per_second': 8.624, 'epoch': 0.92}
{'loss': 0.9261, 'grad_norm': 0.2872786819934845, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8770415186882019, 'eval_runtime': 7.3182, 'eval_samples_per_second': 136.645, 'eval_steps_per_second': 8.609, 'epoch': 0.96}
{'loss': 0.9077, 'grad_norm': 0.32170167565345764, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8765324354171753, 'eval_runtime': 7.3762, 'eval_samples_per_second': 135.571, 'eval_steps_per_second': 8.541, 'epoch': 1.0}
{'train_runtime': 420.1665, 'train_samples_per_second': 23.791, 'train_steps_per_second': 1.488, 'train_loss': 1.0511161193847656, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2491978406906128, 1.0101675987243652, 0.9467183947563171, 0.9092718362808228, 0.8909697532653809, 0.8930208683013916, 0.8827208280563354, 0.8799833059310913, 0.8737947940826416, 0.8845623731613159, 0.876317024230957, 0.8705298900604248, 0.8729050159454346, 0.8794191479682922, 0.8758560419082642, 0.8694607019424438, 0.8676943778991699, 0.8752885460853577, 0.8714874386787415, 0.8698980808258057, 0.874987781047821, 0.8714911937713623, 0.8719207644462585, 0.8770415186882019, 0.8765324354171753], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2491978406906128, 1.0101675987243652, 0.9467183947563171, 0.9092718362808228, 0.8909697532653809, 0.8930208683013916, 0.8827208280563354, 0.8799833059310913, 0.8737947940826416, 0.8845623731613159, 0.876317024230957, 0.8705298900604248, 0.8729050159454346, 0.8794191479682922, 0.8758560419082642, 0.8694607019424438, 0.8676943778991699, 0.8752885460853577, 0.8714874386787415, 0.8698980808258057, 0.874987781047821, 0.8714911937713623, 0.8719207644462585, 0.8770415186882019, 0.8765324354171753]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1282360553741455
current iteration best possible eval_loss (full train run):  -0.8765324354171753
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.6651 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -1.005856541807694
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -1.000880755143164
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -1.0058565418896583
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.9991419742969212
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -1.0058648387490006
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3549, dtype=torch.float64), 0, tensor(0.0974, dtype=torch.float64), tensor(0.1440, dtype=torch.float64), 0, tensor(0.3658, dtype=torch.float64), tensor(0.0378, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 1, 1, 38, 0.04025098889622258, 21.57581731130951, 1]
normalized proposed parameters for next round by BO: [tensor(0.3549, dtype=torch.float64), tensor(1.0457e-18, dtype=torch.float64), tensor(0.0974, dtype=torch.float64), tensor(0.1440, dtype=torch.float64), tensor(2.1626e-19, dtype=torch.float64), tensor(0.3658, dtype=torch.float64), tensor(0.0378, dtype=torch.float64), tensor(2.2796e-18, dtype=torch.float64), tensor(4.8583e-19, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2972, dtype=torch.float64), tensor(0.4025, dtype=torch.float64), tensor(0.4495, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.355
  gsm8k: 0
  rowan_hellaswag: 0.097
  sciq: 0.144
  triviaqa: 0
  truthfulqa_gen: 0.366
  wikitext: 0.038
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (38,)
  lora_dropout: (0.04025098889622258,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (21.57581731130951,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  38
lora dropout:  0.04025098889622258
lora alpha:  21.57581731130951
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 83,427,328 || all params: 8,113,688,576 || trainable%: 1.0282
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2955, 'grad_norm': 0.9768659472465515, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.602768063545227, 'eval_runtime': 7.9211, 'eval_samples_per_second': 126.245, 'eval_steps_per_second': 7.953, 'epoch': 0.04}
{'loss': 1.3428, 'grad_norm': 0.6428978443145752, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4705536365509033, 'eval_runtime': 7.9184, 'eval_samples_per_second': 126.288, 'eval_steps_per_second': 7.956, 'epoch': 0.08}
{'loss': 1.1732, 'grad_norm': 0.3621494472026825, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4796555042266846, 'eval_runtime': 7.9285, 'eval_samples_per_second': 126.127, 'eval_steps_per_second': 7.946, 'epoch': 0.12}
{'loss': 1.0397, 'grad_norm': 0.289920836687088, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5336228609085083, 'eval_runtime': 7.9319, 'eval_samples_per_second': 126.074, 'eval_steps_per_second': 7.943, 'epoch': 0.16}
{'loss': 1.0685, 'grad_norm': 0.2905907928943634, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.538676142692566, 'eval_runtime': 7.9416, 'eval_samples_per_second': 125.919, 'eval_steps_per_second': 7.933, 'epoch': 0.2}
{'loss': 1.1468, 'grad_norm': 0.25108858942985535, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4771218299865723, 'eval_runtime': 7.9393, 'eval_samples_per_second': 125.956, 'eval_steps_per_second': 7.935, 'epoch': 0.24}
{'loss': 1.0638, 'grad_norm': 0.3618824779987335, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4986186027526855, 'eval_runtime': 7.9319, 'eval_samples_per_second': 126.074, 'eval_steps_per_second': 7.943, 'epoch': 0.28}
{'loss': 0.9932, 'grad_norm': 0.4478868842124939, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.524787187576294, 'eval_runtime': 7.9432, 'eval_samples_per_second': 125.893, 'eval_steps_per_second': 7.931, 'epoch': 0.32}
{'loss': 1.0576, 'grad_norm': 0.3429819941520691, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5281999111175537, 'eval_runtime': 7.936, 'eval_samples_per_second': 126.007, 'eval_steps_per_second': 7.938, 'epoch': 0.36}
{'loss': 1.0424, 'grad_norm': 0.3426743149757385, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5820255279541016, 'eval_runtime': 7.9253, 'eval_samples_per_second': 126.178, 'eval_steps_per_second': 7.949, 'epoch': 0.4}
{'loss': 1.0084, 'grad_norm': 0.3229657709598541, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.52188241481781, 'eval_runtime': 7.9335, 'eval_samples_per_second': 126.048, 'eval_steps_per_second': 7.941, 'epoch': 0.44}
{'loss': 0.944, 'grad_norm': 0.30955037474632263, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.51116943359375, 'eval_runtime': 7.9017, 'eval_samples_per_second': 126.556, 'eval_steps_per_second': 7.973, 'epoch': 0.48}
{'loss': 0.8902, 'grad_norm': 0.32147490978240967, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4857425689697266, 'eval_runtime': 7.899, 'eval_samples_per_second': 126.598, 'eval_steps_per_second': 7.976, 'epoch': 0.52}
{'loss': 0.9146, 'grad_norm': 0.3276616632938385, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5674026012420654, 'eval_runtime': 7.9114, 'eval_samples_per_second': 126.401, 'eval_steps_per_second': 7.963, 'epoch': 0.56}
{'loss': 0.9713, 'grad_norm': 0.28472408652305603, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5759402513504028, 'eval_runtime': 7.9064, 'eval_samples_per_second': 126.48, 'eval_steps_per_second': 7.968, 'epoch': 0.6}
{'loss': 0.9708, 'grad_norm': 0.3883781135082245, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5590800046920776, 'eval_runtime': 7.9723, 'eval_samples_per_second': 125.434, 'eval_steps_per_second': 7.902, 'epoch': 0.64}
{'loss': 0.9105, 'grad_norm': 0.2930983603000641, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5537710189819336, 'eval_runtime': 8.0037, 'eval_samples_per_second': 124.942, 'eval_steps_per_second': 7.871, 'epoch': 0.68}
{'loss': 0.9198, 'grad_norm': 0.3281511664390564, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.552187204360962, 'eval_runtime': 7.9503, 'eval_samples_per_second': 125.781, 'eval_steps_per_second': 7.924, 'epoch': 0.72}
{'loss': 0.8912, 'grad_norm': 0.30930081009864807, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5436863899230957, 'eval_runtime': 7.9248, 'eval_samples_per_second': 126.186, 'eval_steps_per_second': 7.95, 'epoch': 0.76}
{'loss': 0.9251, 'grad_norm': 0.34694477915763855, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.557309627532959, 'eval_runtime': 7.9323, 'eval_samples_per_second': 126.066, 'eval_steps_per_second': 7.942, 'epoch': 0.8}
{'loss': 0.9219, 'grad_norm': 0.30072304606437683, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5519651174545288, 'eval_runtime': 7.9201, 'eval_samples_per_second': 126.261, 'eval_steps_per_second': 7.954, 'epoch': 0.84}
{'loss': 0.8485, 'grad_norm': 0.37386929988861084, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5653773546218872, 'eval_runtime': 7.9199, 'eval_samples_per_second': 126.263, 'eval_steps_per_second': 7.955, 'epoch': 0.88}
{'loss': 0.8734, 'grad_norm': 0.27585628628730774, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5554226636886597, 'eval_runtime': 7.9051, 'eval_samples_per_second': 126.501, 'eval_steps_per_second': 7.97, 'epoch': 0.92}
{'loss': 0.9291, 'grad_norm': 0.2602578401565552, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5526378154754639, 'eval_runtime': 7.8976, 'eval_samples_per_second': 126.621, 'eval_steps_per_second': 7.977, 'epoch': 0.96}
{'loss': 0.9934, 'grad_norm': 0.3138674795627594, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5532459020614624, 'eval_runtime': 7.8869, 'eval_samples_per_second': 126.792, 'eval_steps_per_second': 7.988, 'epoch': 1.0}
{'train_runtime': 435.4326, 'train_samples_per_second': 22.963, 'train_steps_per_second': 1.435, 'train_loss': 1.0854284942626953, 'epoch': 1.0}
train_results:  {'eval_loss': [1.602768063545227, 1.4705536365509033, 1.4796555042266846, 1.5336228609085083, 1.538676142692566, 1.4771218299865723, 1.4986186027526855, 1.524787187576294, 1.5281999111175537, 1.5820255279541016, 1.52188241481781, 1.51116943359375, 1.4857425689697266, 1.5674026012420654, 1.5759402513504028, 1.5590800046920776, 1.5537710189819336, 1.552187204360962, 1.5436863899230957, 1.557309627532959, 1.5519651174545288, 1.5653773546218872, 1.5554226636886597, 1.5526378154754639, 1.5532459020614624], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.602768063545227, 1.4705536365509033, 1.4796555042266846, 1.5336228609085083, 1.538676142692566, 1.4771218299865723, 1.4986186027526855, 1.524787187576294, 1.5281999111175537, 1.5820255279541016, 1.52188241481781, 1.51116943359375, 1.4857425689697266, 1.5674026012420654, 1.5759402513504028, 1.5590800046920776, 1.5537710189819336, 1.552187204360962, 1.5436863899230957, 1.557309627532959, 1.5519651174545288, 1.5653773546218872, 1.5554226636886597, 1.5526378154754639, 1.5532459020614624]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.094757318496704
current iteration best possible eval_loss (full train run):  -1.5532459020614624
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.3494 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -1.013683546830693
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -1.0136835469366727
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -1.0136835471608223
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -1.0140513639329976
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -1.0136835469520065
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0169, dtype=torch.float64), 0, tensor(0.1293, dtype=torch.float64), tensor(0.4948, dtype=torch.float64), 0, tensor(0.3590, dtype=torch.float64), 0, 32, 1, 1, 0, 1, 1, 2, 0.1, 30.326431703428444, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0169, dtype=torch.float64), tensor(1.0862e-18, dtype=torch.float64), tensor(0.1293, dtype=torch.float64), tensor(0.4948, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3590, dtype=torch.float64), tensor(7.1595e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6318, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.017
  sciq: 0
  triviaqa: 0.129
  truthfulqa_gen: 0.495
  wikitext: 0
  mmlu: 0.359
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (30.326431703428444,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  30.326431703428444
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,211,264 || all params: 8,033,472,512 || trainable%: 0.0400
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9717, 'grad_norm': 5.008581638336182, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8110116720199585, 'eval_runtime': 7.4238, 'eval_samples_per_second': 134.702, 'eval_steps_per_second': 8.486, 'epoch': 0.04}
{'loss': 1.317, 'grad_norm': 1.917490005493164, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7487542629241943, 'eval_runtime': 7.3921, 'eval_samples_per_second': 135.28, 'eval_steps_per_second': 8.523, 'epoch': 0.08}
{'loss': 1.1758, 'grad_norm': 1.9383381605148315, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7245689630508423, 'eval_runtime': 7.4173, 'eval_samples_per_second': 134.82, 'eval_steps_per_second': 8.494, 'epoch': 0.12}
{'loss': 1.1046, 'grad_norm': 1.5674028396606445, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7271323204040527, 'eval_runtime': 7.3953, 'eval_samples_per_second': 135.221, 'eval_steps_per_second': 8.519, 'epoch': 0.16}
{'loss': 1.0241, 'grad_norm': 2.163954019546509, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7707090377807617, 'eval_runtime': 7.3928, 'eval_samples_per_second': 135.266, 'eval_steps_per_second': 8.522, 'epoch': 0.2}
{'loss': 0.9875, 'grad_norm': 1.50955069065094, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7599389553070068, 'eval_runtime': 7.3936, 'eval_samples_per_second': 135.252, 'eval_steps_per_second': 8.521, 'epoch': 0.24}
{'loss': 1.0229, 'grad_norm': 1.2373871803283691, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7502583265304565, 'eval_runtime': 7.3983, 'eval_samples_per_second': 135.166, 'eval_steps_per_second': 8.515, 'epoch': 0.28}
{'loss': 0.9524, 'grad_norm': 1.7451231479644775, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7589744329452515, 'eval_runtime': 7.3986, 'eval_samples_per_second': 135.16, 'eval_steps_per_second': 8.515, 'epoch': 0.32}
{'loss': 0.9412, 'grad_norm': 1.8177273273468018, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8155914545059204, 'eval_runtime': 7.4017, 'eval_samples_per_second': 135.105, 'eval_steps_per_second': 8.512, 'epoch': 0.36}
{'loss': 0.9214, 'grad_norm': 1.8159784078598022, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7845393419265747, 'eval_runtime': 7.4338, 'eval_samples_per_second': 134.52, 'eval_steps_per_second': 8.475, 'epoch': 0.4}
{'loss': 0.9553, 'grad_norm': 1.4584113359451294, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8009159564971924, 'eval_runtime': 7.514, 'eval_samples_per_second': 133.085, 'eval_steps_per_second': 8.384, 'epoch': 0.44}
{'loss': 0.9307, 'grad_norm': 1.33445143699646, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8000797033309937, 'eval_runtime': 7.4874, 'eval_samples_per_second': 133.557, 'eval_steps_per_second': 8.414, 'epoch': 0.48}
{'loss': 0.9051, 'grad_norm': 1.9644064903259277, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8017399311065674, 'eval_runtime': 7.4998, 'eval_samples_per_second': 133.337, 'eval_steps_per_second': 8.4, 'epoch': 0.52}
{'loss': 0.8963, 'grad_norm': 1.7814347743988037, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7912724018096924, 'eval_runtime': 7.5093, 'eval_samples_per_second': 133.168, 'eval_steps_per_second': 8.39, 'epoch': 0.56}
{'loss': 0.881, 'grad_norm': 2.930087089538574, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8076592683792114, 'eval_runtime': 7.5096, 'eval_samples_per_second': 133.162, 'eval_steps_per_second': 8.389, 'epoch': 0.6}
{'loss': 0.867, 'grad_norm': 1.4181245565414429, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8289496898651123, 'eval_runtime': 7.5039, 'eval_samples_per_second': 133.264, 'eval_steps_per_second': 8.396, 'epoch': 0.64}
{'loss': 0.8937, 'grad_norm': 1.3599799871444702, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8322499990463257, 'eval_runtime': 7.5195, 'eval_samples_per_second': 132.988, 'eval_steps_per_second': 8.378, 'epoch': 0.68}
{'loss': 0.8399, 'grad_norm': 1.8526259660720825, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8272099494934082, 'eval_runtime': 7.5281, 'eval_samples_per_second': 132.835, 'eval_steps_per_second': 8.369, 'epoch': 0.72}
{'loss': 0.8375, 'grad_norm': 1.4545971155166626, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.852321743965149, 'eval_runtime': 7.5115, 'eval_samples_per_second': 133.129, 'eval_steps_per_second': 8.387, 'epoch': 0.76}
{'loss': 0.8385, 'grad_norm': 1.3398488759994507, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8475605249404907, 'eval_runtime': 7.4542, 'eval_samples_per_second': 134.153, 'eval_steps_per_second': 8.452, 'epoch': 0.8}
{'loss': 0.855, 'grad_norm': 1.1311094760894775, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8518503904342651, 'eval_runtime': 7.4456, 'eval_samples_per_second': 134.308, 'eval_steps_per_second': 8.461, 'epoch': 0.84}
{'loss': 0.8234, 'grad_norm': 1.5943472385406494, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8478237390518188, 'eval_runtime': 7.449, 'eval_samples_per_second': 134.246, 'eval_steps_per_second': 8.458, 'epoch': 0.88}
{'loss': 0.8691, 'grad_norm': 2.2785558700561523, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8411355018615723, 'eval_runtime': 7.4508, 'eval_samples_per_second': 134.214, 'eval_steps_per_second': 8.456, 'epoch': 0.92}
{'loss': 0.8039, 'grad_norm': 1.3686109781265259, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.843822956085205, 'eval_runtime': 7.4458, 'eval_samples_per_second': 134.304, 'eval_steps_per_second': 8.461, 'epoch': 0.96}
{'loss': 0.8588, 'grad_norm': 1.5295922756195068, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8429033756256104, 'eval_runtime': 7.4417, 'eval_samples_per_second': 134.379, 'eval_steps_per_second': 8.466, 'epoch': 1.0}
{'train_runtime': 410.1511, 'train_samples_per_second': 24.379, 'train_steps_per_second': 1.524, 'train_loss': 1.0189456115722657, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8110116720199585, 1.7487542629241943, 1.7245689630508423, 1.7271323204040527, 1.7707090377807617, 1.7599389553070068, 1.7502583265304565, 1.7589744329452515, 1.8155914545059204, 1.7845393419265747, 1.8009159564971924, 1.8000797033309937, 1.8017399311065674, 1.7912724018096924, 1.8076592683792114, 1.8289496898651123, 1.8322499990463257, 1.8272099494934082, 1.852321743965149, 1.8475605249404907, 1.8518503904342651, 1.8478237390518188, 1.8411355018615723, 1.843822956085205, 1.8429033756256104], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8110116720199585, 1.7487542629241943, 1.7245689630508423, 1.7271323204040527, 1.7707090377807617, 1.7599389553070068, 1.7502583265304565, 1.7589744329452515, 1.8155914545059204, 1.7845393419265747, 1.8009159564971924, 1.8000797033309937, 1.8017399311065674, 1.7912724018096924, 1.8076592683792114, 1.8289496898651123, 1.8322499990463257, 1.8272099494934082, 1.852321743965149, 1.8475605249404907, 1.8518503904342651, 1.8478237390518188, 1.8411355018615723, 1.843822956085205, 1.8429033756256104]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760290622711182
current iteration best possible eval_loss (full train run):  -1.8429033756256104
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.2417 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -1.022540264647076
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -1.0538013354358593
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -1.0287519944450707
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -1.0225402642440409
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -1.017621491046256
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0110, dtype=torch.float64), tensor(0.0151, dtype=torch.float64), tensor(0.0156, dtype=torch.float64), tensor(0.0645, dtype=torch.float64), 0, tensor(0.1075, dtype=torch.float64), tensor(0.1056, dtype=torch.float64), tensor(0.6404, dtype=torch.float64), tensor(0.0331, dtype=torch.float64), 32, 0, 1, 0, 0, 1, 64, 0.008183115268381932, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0110, dtype=torch.float64), tensor(0.0151, dtype=torch.float64), tensor(0.0156, dtype=torch.float64), tensor(0.0645, dtype=torch.float64), tensor(0.0072, dtype=torch.float64), tensor(0.1075, dtype=torch.float64), tensor(0.1056, dtype=torch.float64), tensor(0.6404, dtype=torch.float64), tensor(0.0331, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5008, dtype=torch.float64), tensor(0.0818, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.011
  gsm8k: 0.015
  rowan_hellaswag: 0.016
  sciq: 0.065
  triviaqa: 0
  truthfulqa_gen: 0.107
  wikitext: 0.106
  mmlu: 0.64
  arc_challenge: 0.033

LoRA Parameters:
  lora_r: (64,)
  lora_dropout: (0.008183115268381932,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  64
lora dropout:  0.008183115268381932
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 48,234,496 || all params: 8,078,495,744 || trainable%: 0.5971
length of training data:  9923
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7684, 'grad_norm': 0.5044042468070984, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6558222770690918, 'eval_runtime': 6.5584, 'eval_samples_per_second': 152.476, 'eval_steps_per_second': 9.606, 'epoch': 0.04}
{'loss': 1.59, 'grad_norm': 0.49993541836738586, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.148315191268921, 'eval_runtime': 6.5214, 'eval_samples_per_second': 153.342, 'eval_steps_per_second': 9.661, 'epoch': 0.08}
{'loss': 1.3824, 'grad_norm': 0.40760377049446106, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.068908452987671, 'eval_runtime': 6.5476, 'eval_samples_per_second': 152.728, 'eval_steps_per_second': 9.622, 'epoch': 0.12}
{'loss': 1.402, 'grad_norm': 0.5754135251045227, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.0236523151397705, 'eval_runtime': 6.5616, 'eval_samples_per_second': 152.401, 'eval_steps_per_second': 9.601, 'epoch': 0.16}
{'loss': 1.207, 'grad_norm': 0.43583473563194275, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.0337235927581787, 'eval_runtime': 6.5718, 'eval_samples_per_second': 152.165, 'eval_steps_per_second': 9.586, 'epoch': 0.2}
{'loss': 1.258, 'grad_norm': 0.2884506285190582, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.0022917985916138, 'eval_runtime': 6.5819, 'eval_samples_per_second': 151.931, 'eval_steps_per_second': 9.572, 'epoch': 0.24}
{'loss': 1.2471, 'grad_norm': 0.29295504093170166, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.9810050129890442, 'eval_runtime': 6.6454, 'eval_samples_per_second': 150.479, 'eval_steps_per_second': 9.48, 'epoch': 0.28}
{'loss': 1.2586, 'grad_norm': 0.2938648462295532, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.9550911784172058, 'eval_runtime': 6.619, 'eval_samples_per_second': 151.08, 'eval_steps_per_second': 9.518, 'epoch': 0.32}
{'loss': 1.24, 'grad_norm': 0.41149696707725525, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.9640517234802246, 'eval_runtime': 6.6108, 'eval_samples_per_second': 151.269, 'eval_steps_per_second': 9.53, 'epoch': 0.36}
{'loss': 1.1864, 'grad_norm': 0.34685277938842773, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.9502068758010864, 'eval_runtime': 6.6021, 'eval_samples_per_second': 151.466, 'eval_steps_per_second': 9.542, 'epoch': 0.4}
{'loss': 1.2573, 'grad_norm': 0.3070107698440552, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.9319565296173096, 'eval_runtime': 6.5997, 'eval_samples_per_second': 151.523, 'eval_steps_per_second': 9.546, 'epoch': 0.44}
{'loss': 1.2206, 'grad_norm': 0.3671973943710327, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.9300230741500854, 'eval_runtime': 6.6059, 'eval_samples_per_second': 151.38, 'eval_steps_per_second': 9.537, 'epoch': 0.48}
{'loss': 1.2436, 'grad_norm': 0.39060065150260925, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.9181044101715088, 'eval_runtime': 6.61, 'eval_samples_per_second': 151.286, 'eval_steps_per_second': 9.531, 'epoch': 0.52}
{'loss': 1.2257, 'grad_norm': 0.35327205061912537, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.9280710816383362, 'eval_runtime': 6.6117, 'eval_samples_per_second': 151.247, 'eval_steps_per_second': 9.529, 'epoch': 0.56}
{'loss': 1.2052, 'grad_norm': 0.2880522310733795, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.9193644523620605, 'eval_runtime': 6.6075, 'eval_samples_per_second': 151.343, 'eval_steps_per_second': 9.535, 'epoch': 0.6}
{'loss': 1.1569, 'grad_norm': 0.45598939061164856, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.9189627766609192, 'eval_runtime': 6.6163, 'eval_samples_per_second': 151.142, 'eval_steps_per_second': 9.522, 'epoch': 0.64}
{'loss': 1.18, 'grad_norm': 0.37888145446777344, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.9053645133972168, 'eval_runtime': 6.6056, 'eval_samples_per_second': 151.388, 'eval_steps_per_second': 9.537, 'epoch': 0.68}
{'loss': 1.2246, 'grad_norm': 0.34770262241363525, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.9076482057571411, 'eval_runtime': 6.6016, 'eval_samples_per_second': 151.478, 'eval_steps_per_second': 9.543, 'epoch': 0.72}
{'loss': 1.158, 'grad_norm': 0.29908090829849243, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.901395320892334, 'eval_runtime': 6.62, 'eval_samples_per_second': 151.058, 'eval_steps_per_second': 9.517, 'epoch': 0.76}
{'loss': 1.2097, 'grad_norm': 0.32411476969718933, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.9034320116043091, 'eval_runtime': 6.6176, 'eval_samples_per_second': 151.113, 'eval_steps_per_second': 9.52, 'epoch': 0.81}
{'loss': 1.2047, 'grad_norm': 0.3530706465244293, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.9043537974357605, 'eval_runtime': 6.6124, 'eval_samples_per_second': 151.231, 'eval_steps_per_second': 9.528, 'epoch': 0.85}
{'loss': 1.148, 'grad_norm': 0.34292054176330566, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.9000949263572693, 'eval_runtime': 6.6124, 'eval_samples_per_second': 151.232, 'eval_steps_per_second': 9.528, 'epoch': 0.89}
{'loss': 1.1677, 'grad_norm': 0.2698267102241516, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.899904727935791, 'eval_runtime': 6.6083, 'eval_samples_per_second': 151.324, 'eval_steps_per_second': 9.533, 'epoch': 0.93}
{'loss': 1.1792, 'grad_norm': 0.3274078071117401, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.8991209864616394, 'eval_runtime': 6.6069, 'eval_samples_per_second': 151.356, 'eval_steps_per_second': 9.535, 'epoch': 0.97}
{'train_runtime': 367.4605, 'train_samples_per_second': 27.004, 'train_steps_per_second': 1.69, 'train_loss': 1.299504475125176, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6558222770690918, 1.148315191268921, 1.068908452987671, 1.0236523151397705, 1.0337235927581787, 1.0022917985916138, 0.9810050129890442, 0.9550911784172058, 0.9640517234802246, 0.9502068758010864, 0.9319565296173096, 0.9300230741500854, 0.9181044101715088, 0.9280710816383362, 0.9193644523620605, 0.9189627766609192, 0.9053645133972168, 0.9076482057571411, 0.901395320892334, 0.9034320116043091, 0.9043537974357605, 0.9000949263572693, 0.899904727935791, 0.8991209864616394], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.6558222770690918, 1.148315191268921, 1.068908452987671, 1.0236523151397705, 1.0337235927581787, 1.0022917985916138, 0.9810050129890442, 0.9550911784172058, 0.9640517234802246, 0.9502068758010864, 0.9319565296173096, 0.9300230741500854, 0.9181044101715088, 0.9280710816383362, 0.9193644523620605, 0.9189627766609192, 0.9053645133972168, 0.9076482057571411, 0.901395320892334, 0.9034320116043091, 0.9043537974357605, 0.9000949263572693, 0.899904727935791, 0.8991209864616394]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0961191654205322
current iteration best possible eval_loss (full train run):  -0.8991209864616394
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6998 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.769386351108551, 0.48460155725479126, 0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.871427059173584, 0.9379879832267761, 0.9101236462593079, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.43461495637893677, 0.05005854368209839, 0.09836432337760925, 0.9400144815444946]  ‚Üí  acq = -1.0851864569558374
X = [0.9526360034942627, 0.8012327551841736, 0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5360714197158813, 0.12353461980819702, 0.3528776168823242, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5302047729492188, 0.31215590238571167, 0.6163817644119263, 0.9380749464035034]  ‚Üí  acq = -1.070112731931699
X = [0.34730666875839233, 0.06246674060821533, 0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3383713960647583, 0.12947320938110352, 0.50956130027771, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.48614978790283203, 0.43715083599090576, 0.6119240522384644, 0.47161221504211426]  ‚Üí  acq = -1.0603828837386695
X = [0.45309585332870483, 0.0678371787071228, 0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7435163855552673, 0.12291663885116577, 0.4224488139152527, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.23402123153209686, 0.37334394454956055, 0.5466699600219727, 0.06654441356658936]  ‚Üí  acq = -1.062137453271728
X = [0.18636363744735718, 0.6823987364768982, 0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9869434833526611, 0.8804963827133179, 0.9389960169792175, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.33730313181877136, 0.484236478805542, 0.44418343901634216, 0.7705504298210144]  ‚Üí  acq = -1.0447776714371921
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2815, dtype=torch.float64), 0, tensor(0.0267, dtype=torch.float64), tensor(0.0632, dtype=torch.float64), tensor(0.1367, dtype=torch.float64), 0, tensor(0.0868, dtype=torch.float64), tensor(0.4051, dtype=torch.float64), 0, 17, 0, 0, 1, 1, 1, 57, 0.025708305609674333, 19.784518906257205, 1]
normalized proposed parameters for next round by BO: [tensor(0.2815, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0267, dtype=torch.float64), tensor(0.0632, dtype=torch.float64), tensor(0.1367, dtype=torch.float64), tensor(1.8999e-18, dtype=torch.float64), tensor(0.0868, dtype=torch.float64), tensor(0.4051, dtype=torch.float64), tensor(1.5969e-19, dtype=torch.float64), tensor(0.5400, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4457, dtype=torch.float64), tensor(0.2571, dtype=torch.float64), tensor(0.4122, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.281
  gsm8k: 0
  rowan_hellaswag: 0.027
  sciq: 0.063
  triviaqa: 0.137
  truthfulqa_gen: 0
  wikitext: 0.087
  mmlu: 0.405
  arc_challenge: 0

LoRA Parameters:
  lora_r: (57,)
  lora_dropout: (0.025708305609674333,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (19.784518906257205,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  57
lora dropout:  0.025708305609674333
lora alpha:  19.784518906257205
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,581,824 || all params: 8,083,843,072 || trainable%: 0.6628
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2829, 'grad_norm': 1.6280102729797363, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8401018381118774, 'eval_runtime': 6.9487, 'eval_samples_per_second': 143.913, 'eval_steps_per_second': 9.067, 'epoch': 0.04}
{'loss': 1.6732, 'grad_norm': 0.7781836986541748, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2575130462646484, 'eval_runtime': 6.9457, 'eval_samples_per_second': 143.975, 'eval_steps_per_second': 9.07, 'epoch': 0.08}
{'loss': 1.4638, 'grad_norm': 0.31880098581314087, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.256690263748169, 'eval_runtime': 6.9843, 'eval_samples_per_second': 143.178, 'eval_steps_per_second': 9.02, 'epoch': 0.12}
{'loss': 1.402, 'grad_norm': 0.28463754057884216, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2761257886886597, 'eval_runtime': 7.0098, 'eval_samples_per_second': 142.657, 'eval_steps_per_second': 8.987, 'epoch': 0.16}
{'loss': 1.2628, 'grad_norm': 0.29216429591178894, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.243337869644165, 'eval_runtime': 6.9894, 'eval_samples_per_second': 143.073, 'eval_steps_per_second': 9.014, 'epoch': 0.2}
{'loss': 1.2051, 'grad_norm': 0.26006418466567993, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2769420146942139, 'eval_runtime': 6.9912, 'eval_samples_per_second': 143.036, 'eval_steps_per_second': 9.011, 'epoch': 0.24}
{'loss': 1.2007, 'grad_norm': 0.23650145530700684, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2390663623809814, 'eval_runtime': 6.9987, 'eval_samples_per_second': 142.884, 'eval_steps_per_second': 9.002, 'epoch': 0.28}
{'loss': 1.2283, 'grad_norm': 0.25867170095443726, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.266404151916504, 'eval_runtime': 6.982, 'eval_samples_per_second': 143.226, 'eval_steps_per_second': 9.023, 'epoch': 0.32}
{'loss': 1.1997, 'grad_norm': 0.323914498090744, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2247378826141357, 'eval_runtime': 6.9808, 'eval_samples_per_second': 143.25, 'eval_steps_per_second': 9.025, 'epoch': 0.36}
{'loss': 1.2119, 'grad_norm': 0.2602898180484772, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2395788431167603, 'eval_runtime': 6.9875, 'eval_samples_per_second': 143.113, 'eval_steps_per_second': 9.016, 'epoch': 0.4}
{'loss': 1.229, 'grad_norm': 0.29374226927757263, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2368239164352417, 'eval_runtime': 7.0091, 'eval_samples_per_second': 142.672, 'eval_steps_per_second': 8.988, 'epoch': 0.44}
{'loss': 1.1955, 'grad_norm': 0.2894144356250763, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2270756959915161, 'eval_runtime': 7.0283, 'eval_samples_per_second': 142.282, 'eval_steps_per_second': 8.964, 'epoch': 0.48}
{'loss': 1.2331, 'grad_norm': 0.2893061637878418, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2513744831085205, 'eval_runtime': 7.0096, 'eval_samples_per_second': 142.661, 'eval_steps_per_second': 8.988, 'epoch': 0.52}
{'loss': 1.2772, 'grad_norm': 0.40666311979293823, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2458055019378662, 'eval_runtime': 6.9965, 'eval_samples_per_second': 142.928, 'eval_steps_per_second': 9.004, 'epoch': 0.56}
{'loss': 1.2208, 'grad_norm': 0.22958619892597198, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2484772205352783, 'eval_runtime': 6.9856, 'eval_samples_per_second': 143.151, 'eval_steps_per_second': 9.019, 'epoch': 0.6}
{'loss': 1.1923, 'grad_norm': 0.3153325915336609, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.242382287979126, 'eval_runtime': 6.9792, 'eval_samples_per_second': 143.282, 'eval_steps_per_second': 9.027, 'epoch': 0.64}
{'loss': 1.2536, 'grad_norm': 0.2539463937282562, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2885454893112183, 'eval_runtime': 6.9719, 'eval_samples_per_second': 143.434, 'eval_steps_per_second': 9.036, 'epoch': 0.68}
{'loss': 1.1027, 'grad_norm': 0.2707890272140503, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.273662805557251, 'eval_runtime': 6.9731, 'eval_samples_per_second': 143.408, 'eval_steps_per_second': 9.035, 'epoch': 0.72}
{'loss': 1.2558, 'grad_norm': 0.21351653337478638, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2830783128738403, 'eval_runtime': 6.9669, 'eval_samples_per_second': 143.536, 'eval_steps_per_second': 9.043, 'epoch': 0.76}
{'loss': 1.168, 'grad_norm': 0.29700329899787903, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.291294813156128, 'eval_runtime': 6.9735, 'eval_samples_per_second': 143.399, 'eval_steps_per_second': 9.034, 'epoch': 0.8}
{'loss': 1.1755, 'grad_norm': 0.3364020884037018, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2742245197296143, 'eval_runtime': 6.9639, 'eval_samples_per_second': 143.598, 'eval_steps_per_second': 9.047, 'epoch': 0.84}
{'loss': 1.1803, 'grad_norm': 0.22763752937316895, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2979656457901, 'eval_runtime': 6.9732, 'eval_samples_per_second': 143.406, 'eval_steps_per_second': 9.035, 'epoch': 0.88}
{'loss': 1.2656, 'grad_norm': 0.2573021948337555, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.306564211845398, 'eval_runtime': 6.9834, 'eval_samples_per_second': 143.197, 'eval_steps_per_second': 9.021, 'epoch': 0.92}
{'loss': 1.1557, 'grad_norm': 0.2518802583217621, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.302340030670166, 'eval_runtime': 6.9976, 'eval_samples_per_second': 142.907, 'eval_steps_per_second': 9.003, 'epoch': 0.96}
{'loss': 1.2094, 'grad_norm': 0.2579422891139984, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3009350299835205, 'eval_runtime': 7.0672, 'eval_samples_per_second': 141.499, 'eval_steps_per_second': 8.914, 'epoch': 1.0}
{'train_runtime': 383.9281, 'train_samples_per_second': 26.036, 'train_steps_per_second': 1.628, 'train_loss': 1.3297886016845704, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8401018381118774, 1.2575130462646484, 1.256690263748169, 1.2761257886886597, 1.243337869644165, 1.2769420146942139, 1.2390663623809814, 1.266404151916504, 1.2247378826141357, 1.2395788431167603, 1.2368239164352417, 1.2270756959915161, 1.2513744831085205, 1.2458055019378662, 1.2484772205352783, 1.242382287979126, 1.2885454893112183, 1.273662805557251, 1.2830783128738403, 1.291294813156128, 1.2742245197296143, 1.2979656457901, 1.306564211845398, 1.302340030670166, 1.3009350299835205], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8401018381118774, 1.2575130462646484, 1.256690263748169, 1.2761257886886597, 1.243337869644165, 1.2769420146942139, 1.2390663623809814, 1.266404151916504, 1.2247378826141357, 1.2395788431167603, 1.2368239164352417, 1.2270756959915161, 1.2513744831085205, 1.2458055019378662, 1.2484772205352783, 1.242382287979126, 1.2885454893112183, 1.273662805557251, 1.2830783128738403, 1.291294813156128, 1.2742245197296143, 1.2979656457901, 1.306564211845398, 1.302340030670166, 1.3009350299835205]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2706166505813599
current iteration best possible eval_loss (full train run):  -1.3009350299835205
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.8054 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.9998458815285907
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -1.0481177094735605
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -1.04638277091749
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -1.0481176906691079
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -1.0461861760465991
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0632, dtype=torch.float64), 0, tensor(0.0101, dtype=torch.float64), tensor(0.1501, dtype=torch.float64), 0, tensor(0.3111, dtype=torch.float64), tensor(0.4568, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 62, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.6893e-17, dtype=torch.float64), tensor(0.0087, dtype=torch.float64), tensor(0.0632, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0101, dtype=torch.float64), tensor(0.1501, dtype=torch.float64), tensor(1.1550e-18, dtype=torch.float64), tensor(0.3111, dtype=torch.float64), tensor(0.4568, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4850, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.063
  sciq: 0
  triviaqa: 0.01
  truthfulqa_gen: 0.15
  wikitext: 0
  mmlu: 0.311
  arc_challenge: 0.457

LoRA Parameters:
  lora_r: (62,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  62
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 136,118,272 || all params: 8,166,379,520 || trainable%: 1.6668
length of training data:  9910
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4673, 'grad_norm': 0.7409027218818665, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.0255544185638428, 'eval_runtime': 7.9698, 'eval_samples_per_second': 125.474, 'eval_steps_per_second': 7.905, 'epoch': 0.04}
{'loss': 1.2125, 'grad_norm': 0.47829505801200867, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.8841302394866943, 'eval_runtime': 7.9262, 'eval_samples_per_second': 126.164, 'eval_steps_per_second': 7.948, 'epoch': 0.08}
{'loss': 1.0979, 'grad_norm': 0.3746393024921417, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 0.8756278157234192, 'eval_runtime': 7.9342, 'eval_samples_per_second': 126.036, 'eval_steps_per_second': 7.94, 'epoch': 0.12}
{'loss': 1.0318, 'grad_norm': 0.3260754942893982, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.8715118169784546, 'eval_runtime': 7.9416, 'eval_samples_per_second': 125.919, 'eval_steps_per_second': 7.933, 'epoch': 0.16}
{'loss': 1.0563, 'grad_norm': 0.3614920675754547, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.8728731870651245, 'eval_runtime': 7.9172, 'eval_samples_per_second': 126.308, 'eval_steps_per_second': 7.957, 'epoch': 0.2}
{'loss': 1.0047, 'grad_norm': 0.3566255569458008, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.8869315385818481, 'eval_runtime': 7.9208, 'eval_samples_per_second': 126.25, 'eval_steps_per_second': 7.954, 'epoch': 0.24}
{'loss': 1.0218, 'grad_norm': 0.36573484539985657, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.909280002117157, 'eval_runtime': 7.9278, 'eval_samples_per_second': 126.138, 'eval_steps_per_second': 7.947, 'epoch': 0.28}
{'loss': 0.9587, 'grad_norm': 0.41756537556648254, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.9120016098022461, 'eval_runtime': 7.9251, 'eval_samples_per_second': 126.181, 'eval_steps_per_second': 7.949, 'epoch': 0.32}
{'loss': 1.0132, 'grad_norm': 0.38916146755218506, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.9236037731170654, 'eval_runtime': 7.9262, 'eval_samples_per_second': 126.164, 'eval_steps_per_second': 7.948, 'epoch': 0.36}
{'loss': 0.9397, 'grad_norm': 0.3813134729862213, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.9031362533569336, 'eval_runtime': 7.9526, 'eval_samples_per_second': 125.745, 'eval_steps_per_second': 7.922, 'epoch': 0.4}
{'loss': 0.9467, 'grad_norm': 0.40403223037719727, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.9190281629562378, 'eval_runtime': 7.9437, 'eval_samples_per_second': 125.887, 'eval_steps_per_second': 7.931, 'epoch': 0.44}
{'loss': 0.8718, 'grad_norm': 0.419637531042099, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.9278388023376465, 'eval_runtime': 7.9632, 'eval_samples_per_second': 125.578, 'eval_steps_per_second': 7.911, 'epoch': 0.48}
{'loss': 0.8628, 'grad_norm': 0.45072799921035767, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.9353489875793457, 'eval_runtime': 7.9738, 'eval_samples_per_second': 125.411, 'eval_steps_per_second': 7.901, 'epoch': 0.52}
{'loss': 0.8683, 'grad_norm': 0.38488179445266724, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.9489840865135193, 'eval_runtime': 7.9947, 'eval_samples_per_second': 125.082, 'eval_steps_per_second': 7.88, 'epoch': 0.56}
{'loss': 0.7865, 'grad_norm': 0.5233588218688965, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.9913082122802734, 'eval_runtime': 7.992, 'eval_samples_per_second': 125.125, 'eval_steps_per_second': 7.883, 'epoch': 0.6}
{'loss': 0.8132, 'grad_norm': 0.5231039524078369, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.9844403266906738, 'eval_runtime': 7.9759, 'eval_samples_per_second': 125.378, 'eval_steps_per_second': 7.899, 'epoch': 0.65}
{'loss': 0.828, 'grad_norm': 0.6356731653213501, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.9642646908760071, 'eval_runtime': 7.9825, 'eval_samples_per_second': 125.274, 'eval_steps_per_second': 7.892, 'epoch': 0.69}
{'loss': 0.7438, 'grad_norm': 0.5617734789848328, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 1.0328317880630493, 'eval_runtime': 8.0148, 'eval_samples_per_second': 124.769, 'eval_steps_per_second': 7.86, 'epoch': 0.73}
{'loss': 0.8015, 'grad_norm': 0.4092061519622803, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 1.0203129053115845, 'eval_runtime': 7.9709, 'eval_samples_per_second': 125.456, 'eval_steps_per_second': 7.904, 'epoch': 0.77}
{'loss': 0.7633, 'grad_norm': 0.3419538736343384, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 1.0155034065246582, 'eval_runtime': 7.9504, 'eval_samples_per_second': 125.78, 'eval_steps_per_second': 7.924, 'epoch': 0.81}
{'loss': 0.6844, 'grad_norm': 0.4297431707382202, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 1.0207476615905762, 'eval_runtime': 7.9269, 'eval_samples_per_second': 126.153, 'eval_steps_per_second': 7.948, 'epoch': 0.85}
{'loss': 0.7112, 'grad_norm': 0.375944584608078, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 1.032183051109314, 'eval_runtime': 7.9248, 'eval_samples_per_second': 126.186, 'eval_steps_per_second': 7.95, 'epoch': 0.89}
{'loss': 0.7168, 'grad_norm': 0.4417015314102173, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 1.053357720375061, 'eval_runtime': 7.9282, 'eval_samples_per_second': 126.132, 'eval_steps_per_second': 7.946, 'epoch': 0.93}
{'loss': 0.6626, 'grad_norm': 0.4162421226501465, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 1.0455620288848877, 'eval_runtime': 7.9296, 'eval_samples_per_second': 126.109, 'eval_steps_per_second': 7.945, 'epoch': 0.97}
{'train_runtime': 435.1632, 'train_samples_per_second': 22.773, 'train_steps_per_second': 1.425, 'train_loss': 0.9475361377962174, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0255544185638428, 0.8841302394866943, 0.8756278157234192, 0.8715118169784546, 0.8728731870651245, 0.8869315385818481, 0.909280002117157, 0.9120016098022461, 0.9236037731170654, 0.9031362533569336, 0.9190281629562378, 0.9278388023376465, 0.9353489875793457, 0.9489840865135193, 0.9913082122802734, 0.9844403266906738, 0.9642646908760071, 1.0328317880630493, 1.0203129053115845, 1.0155034065246582, 1.0207476615905762, 1.032183051109314, 1.053357720375061, 1.0455620288848877], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.0255544185638428, 0.8841302394866943, 0.8756278157234192, 0.8715118169784546, 0.8728731870651245, 0.8869315385818481, 0.909280002117157, 0.9120016098022461, 0.9236037731170654, 0.9031362533569336, 0.9190281629562378, 0.9278388023376465, 0.9353489875793457, 0.9489840865135193, 0.9913082122802734, 0.9844403266906738, 0.9642646908760071, 1.0328317880630493, 1.0203129053115845, 1.0155034065246582, 1.0207476615905762, 1.032183051109314, 1.053357720375061, 1.0455620288848877]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0890300273895264
current iteration best possible eval_loss (full train run):  -1.0455620288848877
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.4941 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6823287010192871, 0.1862812042236328, 0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8836188316345215, 0.9964625239372253, 0.7934750914573669, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.33229848742485046, 0.9432199001312256, 0.3315914273262024, 0.7924636006355286]  ‚Üí  acq = -1.023540555553244
X = [0.6971794366836548, 0.0029845833778381348, 0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9298456907272339, 0.9158058762550354, 0.6235672831535339, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5776915550231934, 0.40536046028137207, 0.8601958751678467, 0.07649117708206177]  ‚Üí  acq = -1.0341055550204357
X = [0.7746965289115906, 0.45509761571884155, 0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.08178042620420456, 0.07738137245178223, 0.8753153681755066, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7854319214820862, 0.9972329139709473, 0.8323233127593994, 0.5278875827789307]  ‚Üí  acq = -1.0233175624378743
X = [0.6284062266349792, 0.25792115926742554, 0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.653922438621521, 0.4825098514556885, 0.12023776769638062, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.750758707523346, 0.8871928453445435, 0.8711596727371216, 0.8900622725486755]  ‚Üí  acq = -1.016586565667795
X = [0.7428956627845764, 0.7939770817756653, 0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.10134205967187881, 0.695570707321167, 0.8957255482673645, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2861731946468353, 0.8315107822418213, 0.7387340068817139, 0.4935872554779053]  ‚Üí  acq = -0.9882402187827629
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0316, dtype=torch.float64), tensor(0.0112, dtype=torch.float64), 0, 0, tensor(0.2940, dtype=torch.float64), tensor(0.0539, dtype=torch.float64), tensor(0.0149, dtype=torch.float64), tensor(0.3923, dtype=torch.float64), tensor(0.2022, dtype=torch.float64), 29, 1, 1, 1, 1, 0, 75, 0.07175446461892204, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0316, dtype=torch.float64), tensor(0.0112, dtype=torch.float64), tensor(3.1156e-18, dtype=torch.float64), tensor(1.1216e-17, dtype=torch.float64), tensor(0.2940, dtype=torch.float64), tensor(0.0539, dtype=torch.float64), tensor(0.0149, dtype=torch.float64), tensor(0.3923, dtype=torch.float64), tensor(0.2022, dtype=torch.float64), tensor(0.9215, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5821, dtype=torch.float64), tensor(0.7175, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.032
  gsm8k: 0.011
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.294
  truthfulqa_gen: 0.054
  wikitext: 0.015
  mmlu: 0.392
  arc_challenge: 0.202

LoRA Parameters:
  lora_r: (75,)
  lora_dropout: (0.07175446461892204,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  75
lora dropout:  0.07175446461892204
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,132,800 || all params: 8,139,394,048 || trainable%: 1.3408
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6984, 'grad_norm': 0.7581809163093567, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1479564905166626, 'eval_runtime': 7.6322, 'eval_samples_per_second': 131.025, 'eval_steps_per_second': 8.255, 'epoch': 0.04}
{'loss': 1.3504, 'grad_norm': 1.1065330505371094, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9932399988174438, 'eval_runtime': 7.6317, 'eval_samples_per_second': 131.033, 'eval_steps_per_second': 8.255, 'epoch': 0.08}
{'loss': 1.1894, 'grad_norm': 0.43372103571891785, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9227972030639648, 'eval_runtime': 7.7097, 'eval_samples_per_second': 129.706, 'eval_steps_per_second': 8.171, 'epoch': 0.12}
{'loss': 1.1208, 'grad_norm': 0.6595720052719116, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8953794836997986, 'eval_runtime': 7.7365, 'eval_samples_per_second': 129.257, 'eval_steps_per_second': 8.143, 'epoch': 0.16}
{'loss': 1.0495, 'grad_norm': 0.37489941716194153, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8823785781860352, 'eval_runtime': 7.7257, 'eval_samples_per_second': 129.438, 'eval_steps_per_second': 8.155, 'epoch': 0.2}
{'loss': 1.1344, 'grad_norm': 0.30588701367378235, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8738176822662354, 'eval_runtime': 7.7085, 'eval_samples_per_second': 129.727, 'eval_steps_per_second': 8.173, 'epoch': 0.24}
{'loss': 1.0888, 'grad_norm': 0.361613392829895, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8716034889221191, 'eval_runtime': 7.7226, 'eval_samples_per_second': 129.49, 'eval_steps_per_second': 8.158, 'epoch': 0.28}
{'loss': 1.101, 'grad_norm': 0.33372047543525696, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.877875566482544, 'eval_runtime': 7.6816, 'eval_samples_per_second': 130.18, 'eval_steps_per_second': 8.201, 'epoch': 0.32}
{'loss': 1.034, 'grad_norm': 0.2968560457229614, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8742614984512329, 'eval_runtime': 7.6754, 'eval_samples_per_second': 130.287, 'eval_steps_per_second': 8.208, 'epoch': 0.36}
{'loss': 1.0282, 'grad_norm': 0.3110611140727997, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8670432567596436, 'eval_runtime': 7.672, 'eval_samples_per_second': 130.343, 'eval_steps_per_second': 8.212, 'epoch': 0.4}
{'loss': 1.0671, 'grad_norm': 0.5984731912612915, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.881240963935852, 'eval_runtime': 7.6681, 'eval_samples_per_second': 130.41, 'eval_steps_per_second': 8.216, 'epoch': 0.44}
{'loss': 1.0325, 'grad_norm': 0.3854214549064636, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8853766918182373, 'eval_runtime': 7.6731, 'eval_samples_per_second': 130.325, 'eval_steps_per_second': 8.21, 'epoch': 0.48}
{'loss': 1.0329, 'grad_norm': 0.3197261691093445, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8841044902801514, 'eval_runtime': 7.6747, 'eval_samples_per_second': 130.299, 'eval_steps_per_second': 8.209, 'epoch': 0.52}
{'loss': 1.0563, 'grad_norm': 0.30376771092414856, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.87493896484375, 'eval_runtime': 7.7395, 'eval_samples_per_second': 129.208, 'eval_steps_per_second': 8.14, 'epoch': 0.56}
{'loss': 0.9701, 'grad_norm': 0.3188377022743225, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8978336453437805, 'eval_runtime': 7.6888, 'eval_samples_per_second': 130.059, 'eval_steps_per_second': 8.194, 'epoch': 0.6}
{'loss': 1.0525, 'grad_norm': 0.34723973274230957, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8869842290878296, 'eval_runtime': 7.6863, 'eval_samples_per_second': 130.102, 'eval_steps_per_second': 8.196, 'epoch': 0.64}
{'loss': 1.0338, 'grad_norm': 0.5379406809806824, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8845592141151428, 'eval_runtime': 7.6834, 'eval_samples_per_second': 130.151, 'eval_steps_per_second': 8.2, 'epoch': 0.68}
{'loss': 0.967, 'grad_norm': 0.3208406865596771, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9040144085884094, 'eval_runtime': 7.6772, 'eval_samples_per_second': 130.257, 'eval_steps_per_second': 8.206, 'epoch': 0.72}
{'loss': 1.0204, 'grad_norm': 0.3868655264377594, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8975750803947449, 'eval_runtime': 7.6684, 'eval_samples_per_second': 130.406, 'eval_steps_per_second': 8.216, 'epoch': 0.76}
{'loss': 1.019, 'grad_norm': 0.5266178846359253, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8811732530593872, 'eval_runtime': 7.6788, 'eval_samples_per_second': 130.229, 'eval_steps_per_second': 8.204, 'epoch': 0.8}
{'loss': 0.9856, 'grad_norm': 0.371052622795105, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9033001661300659, 'eval_runtime': 7.6749, 'eval_samples_per_second': 130.294, 'eval_steps_per_second': 8.209, 'epoch': 0.84}
{'loss': 0.9495, 'grad_norm': 0.37272852659225464, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8975417017936707, 'eval_runtime': 7.6702, 'eval_samples_per_second': 130.375, 'eval_steps_per_second': 8.214, 'epoch': 0.88}
{'loss': 0.9789, 'grad_norm': 0.36036065220832825, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9128593802452087, 'eval_runtime': 7.7501, 'eval_samples_per_second': 129.031, 'eval_steps_per_second': 8.129, 'epoch': 0.92}
{'loss': 0.9834, 'grad_norm': 0.365660160779953, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9004649519920349, 'eval_runtime': 7.727, 'eval_samples_per_second': 129.416, 'eval_steps_per_second': 8.153, 'epoch': 0.96}
{'loss': 1.0141, 'grad_norm': 0.3935202956199646, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9019807577133179, 'eval_runtime': 7.7138, 'eval_samples_per_second': 129.638, 'eval_steps_per_second': 8.167, 'epoch': 1.0}
{'train_runtime': 426.9953, 'train_samples_per_second': 23.412, 'train_steps_per_second': 1.464, 'train_loss': 1.1183240997314454, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1479564905166626, 0.9932399988174438, 0.9227972030639648, 0.8953794836997986, 0.8823785781860352, 0.8738176822662354, 0.8716034889221191, 0.877875566482544, 0.8742614984512329, 0.8670432567596436, 0.881240963935852, 0.8853766918182373, 0.8841044902801514, 0.87493896484375, 0.8978336453437805, 0.8869842290878296, 0.8845592141151428, 0.9040144085884094, 0.8975750803947449, 0.8811732530593872, 0.9033001661300659, 0.8975417017936707, 0.9128593802452087, 0.9004649519920349, 0.9019807577133179], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1479564905166626, 0.9932399988174438, 0.9227972030639648, 0.8953794836997986, 0.8823785781860352, 0.8738176822662354, 0.8716034889221191, 0.877875566482544, 0.8742614984512329, 0.8670432567596436, 0.881240963935852, 0.8853766918182373, 0.8841044902801514, 0.87493896484375, 0.8978336453437805, 0.8869842290878296, 0.8845592141151428, 0.9040144085884094, 0.8975750803947449, 0.8811732530593872, 0.9033001661300659, 0.8975417017936707, 0.9128593802452087, 0.9004649519920349, 0.9019807577133179]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1126946210861206
current iteration best possible eval_loss (full train run):  -0.9019807577133179
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264, -1.1126946210861206]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.0211 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -1.057806221533443
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -1.0648927402065023
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -1.0574710073032247
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -1.0578062206644645
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -1.0520737185374065
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1029, dtype=torch.float64), tensor(0.0600, dtype=torch.float64), tensor(0.0639, dtype=torch.float64), tensor(0.1205, dtype=torch.float64), tensor(0.2910, dtype=torch.float64), tensor(0.0430, dtype=torch.float64), tensor(0.0791, dtype=torch.float64), tensor(0.1535, dtype=torch.float64), tensor(0.0860, dtype=torch.float64), 28, 1, 1, 1, 1, 1, 4, 0.0537529405463556, 33.91372098544754, 1]
normalized proposed parameters for next round by BO: [tensor(0.1029, dtype=torch.float64), tensor(0.0600, dtype=torch.float64), tensor(0.0639, dtype=torch.float64), tensor(0.1205, dtype=torch.float64), tensor(0.2910, dtype=torch.float64), tensor(0.0430, dtype=torch.float64), tensor(0.0791, dtype=torch.float64), tensor(0.1535, dtype=torch.float64), tensor(0.0860, dtype=torch.float64), tensor(0.8866, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0347, dtype=torch.float64), tensor(0.5375, dtype=torch.float64), tensor(0.7065, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.103
  gsm8k: 0.06
  rowan_hellaswag: 0.064
  sciq: 0.121
  triviaqa: 0.291
  truthfulqa_gen: 0.043
  wikitext: 0.079
  mmlu: 0.154
  arc_challenge: 0.086

LoRA Parameters:
  lora_r: (4,)
  lora_dropout: (0.0537529405463556,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (33.91372098544754,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  4
lora dropout:  0.0537529405463556
lora alpha:  33.91372098544754
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 7,684,096 || all params: 8,037,945,344 || trainable%: 0.0956
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8874, 'grad_norm': 3.4717514514923096, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1940997838974, 'eval_runtime': 7.6405, 'eval_samples_per_second': 130.881, 'eval_steps_per_second': 8.246, 'epoch': 0.04}
{'loss': 1.5014, 'grad_norm': 3.1474480628967285, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.016646146774292, 'eval_runtime': 7.6512, 'eval_samples_per_second': 130.698, 'eval_steps_per_second': 8.234, 'epoch': 0.08}
{'loss': 1.3085, 'grad_norm': 1.5682297945022583, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9747618436813354, 'eval_runtime': 7.6399, 'eval_samples_per_second': 130.892, 'eval_steps_per_second': 8.246, 'epoch': 0.12}
{'loss': 1.3963, 'grad_norm': 1.702085256576538, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9202859997749329, 'eval_runtime': 7.6503, 'eval_samples_per_second': 130.714, 'eval_steps_per_second': 8.235, 'epoch': 0.16}
{'loss': 1.2372, 'grad_norm': 1.925721287727356, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9097875356674194, 'eval_runtime': 7.6362, 'eval_samples_per_second': 130.956, 'eval_steps_per_second': 8.25, 'epoch': 0.2}
{'loss': 1.2014, 'grad_norm': 1.1196866035461426, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9027293920516968, 'eval_runtime': 7.6815, 'eval_samples_per_second': 130.182, 'eval_steps_per_second': 8.201, 'epoch': 0.24}
{'loss': 1.1938, 'grad_norm': 1.1229327917099, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8966997265815735, 'eval_runtime': 7.6645, 'eval_samples_per_second': 130.471, 'eval_steps_per_second': 8.22, 'epoch': 0.28}
{'loss': 1.2221, 'grad_norm': 1.131220817565918, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8925468921661377, 'eval_runtime': 7.665, 'eval_samples_per_second': 130.463, 'eval_steps_per_second': 8.219, 'epoch': 0.32}
{'loss': 1.2023, 'grad_norm': 0.9864037036895752, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8940086960792542, 'eval_runtime': 7.6842, 'eval_samples_per_second': 130.136, 'eval_steps_per_second': 8.199, 'epoch': 0.36}
{'loss': 1.1029, 'grad_norm': 1.094483733177185, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8891772627830505, 'eval_runtime': 7.6714, 'eval_samples_per_second': 130.355, 'eval_steps_per_second': 8.212, 'epoch': 0.4}
{'loss': 1.1756, 'grad_norm': 1.21645987033844, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8891043066978455, 'eval_runtime': 7.6883, 'eval_samples_per_second': 130.068, 'eval_steps_per_second': 8.194, 'epoch': 0.44}
{'loss': 1.2029, 'grad_norm': 0.9698838591575623, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8848630785942078, 'eval_runtime': 7.6765, 'eval_samples_per_second': 130.268, 'eval_steps_per_second': 8.207, 'epoch': 0.48}
{'loss': 1.2399, 'grad_norm': 1.2298191785812378, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8859047889709473, 'eval_runtime': 7.6296, 'eval_samples_per_second': 131.069, 'eval_steps_per_second': 8.257, 'epoch': 0.52}
{'loss': 1.1402, 'grad_norm': 0.9592411518096924, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8802905082702637, 'eval_runtime': 7.6362, 'eval_samples_per_second': 130.956, 'eval_steps_per_second': 8.25, 'epoch': 0.56}
{'loss': 1.1586, 'grad_norm': 1.022560477256775, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8871750831604004, 'eval_runtime': 7.668, 'eval_samples_per_second': 130.412, 'eval_steps_per_second': 8.216, 'epoch': 0.6}
{'loss': 1.1169, 'grad_norm': 1.0936777591705322, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.881211519241333, 'eval_runtime': 7.6647, 'eval_samples_per_second': 130.468, 'eval_steps_per_second': 8.22, 'epoch': 0.64}
{'loss': 1.0961, 'grad_norm': 1.4420641660690308, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8766177892684937, 'eval_runtime': 7.6645, 'eval_samples_per_second': 130.472, 'eval_steps_per_second': 8.22, 'epoch': 0.68}
{'loss': 1.2013, 'grad_norm': 1.1348403692245483, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8794024586677551, 'eval_runtime': 7.6729, 'eval_samples_per_second': 130.329, 'eval_steps_per_second': 8.211, 'epoch': 0.72}
{'loss': 1.1127, 'grad_norm': 1.2373697757720947, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8789384365081787, 'eval_runtime': 7.6654, 'eval_samples_per_second': 130.456, 'eval_steps_per_second': 8.219, 'epoch': 0.76}
{'loss': 1.058, 'grad_norm': 1.2388153076171875, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8815588355064392, 'eval_runtime': 7.6596, 'eval_samples_per_second': 130.555, 'eval_steps_per_second': 8.225, 'epoch': 0.8}
{'loss': 1.1087, 'grad_norm': 1.0250861644744873, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8765822649002075, 'eval_runtime': 7.6646, 'eval_samples_per_second': 130.47, 'eval_steps_per_second': 8.22, 'epoch': 0.84}
{'loss': 1.1017, 'grad_norm': 1.1872594356536865, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8721925616264343, 'eval_runtime': 7.696, 'eval_samples_per_second': 129.938, 'eval_steps_per_second': 8.186, 'epoch': 0.88}
{'loss': 1.1638, 'grad_norm': 0.9304195642471313, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8718876242637634, 'eval_runtime': 7.664, 'eval_samples_per_second': 130.481, 'eval_steps_per_second': 8.22, 'epoch': 0.92}
{'loss': 1.0641, 'grad_norm': 1.3112196922302246, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8720554113388062, 'eval_runtime': 7.6925, 'eval_samples_per_second': 129.998, 'eval_steps_per_second': 8.19, 'epoch': 0.96}
{'loss': 1.202, 'grad_norm': 1.0226967334747314, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8724844455718994, 'eval_runtime': 7.6776, 'eval_samples_per_second': 130.249, 'eval_steps_per_second': 8.206, 'epoch': 1.0}
{'train_runtime': 435.3813, 'train_samples_per_second': 22.961, 'train_steps_per_second': 1.436, 'train_loss': 1.2558251678466796, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1940997838974, 1.016646146774292, 0.9747618436813354, 0.9202859997749329, 0.9097875356674194, 0.9027293920516968, 0.8966997265815735, 0.8925468921661377, 0.8940086960792542, 0.8891772627830505, 0.8891043066978455, 0.8848630785942078, 0.8859047889709473, 0.8802905082702637, 0.8871750831604004, 0.881211519241333, 0.8766177892684937, 0.8794024586677551, 0.8789384365081787, 0.8815588355064392, 0.8765822649002075, 0.8721925616264343, 0.8718876242637634, 0.8720554113388062, 0.8724844455718994], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1940997838974, 1.016646146774292, 0.9747618436813354, 0.9202859997749329, 0.9097875356674194, 0.9027293920516968, 0.8966997265815735, 0.8925468921661377, 0.8940086960792542, 0.8891772627830505, 0.8891043066978455, 0.8848630785942078, 0.8859047889709473, 0.8802905082702637, 0.8871750831604004, 0.881211519241333, 0.8766177892684937, 0.8794024586677551, 0.8789384365081787, 0.8815588355064392, 0.8765822649002075, 0.8721925616264343, 0.8718876242637634, 0.8720554113388062, 0.8724844455718994]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760297775268555
current iteration best possible eval_loss (full train run):  -0.8724844455718994
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264, -1.1126946210861206, -1.0760297775268555]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.7236 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -1.051253214894346
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -1.0720648061515612
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -1.0498889651716143
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -1.0695992026007282
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -1.0657258693157061
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0441, dtype=torch.float64), tensor(0.0701, dtype=torch.float64), tensor(0.0415, dtype=torch.float64), tensor(0.2807, dtype=torch.float64), tensor(0.2825, dtype=torch.float64), tensor(0.2449, dtype=torch.float64), 0, 0, tensor(0.0362, dtype=torch.float64), 17, 0, 0, 0, 1, 1, 15, 0.08969804477173335, 26.152475781819035, 1]
normalized proposed parameters for next round by BO: [tensor(0.0441, dtype=torch.float64), tensor(0.0701, dtype=torch.float64), tensor(0.0415, dtype=torch.float64), tensor(0.2807, dtype=torch.float64), tensor(0.2825, dtype=torch.float64), tensor(0.2449, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0362, dtype=torch.float64), tensor(0.5250, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1158, dtype=torch.float64), tensor(0.8970, dtype=torch.float64), tensor(0.5448, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.044
  gsm8k: 0.07
  rowan_hellaswag: 0.041
  sciq: 0.281
  triviaqa: 0.283
  truthfulqa_gen: 0.245
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.036

LoRA Parameters:
  lora_r: (15,)
  lora_dropout: (0.08969804477173335,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (26.152475781819035,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  15
lora dropout:  0.08969804477173335
lora alpha:  26.152475781819035
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 9,400,320 || all params: 8,039,661,568 || trainable%: 0.1169
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6904, 'grad_norm': 3.4038522243499756, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.130790948867798, 'eval_runtime': 6.6441, 'eval_samples_per_second': 150.51, 'eval_steps_per_second': 9.482, 'epoch': 0.04}
{'loss': 1.5607, 'grad_norm': 2.1277096271514893, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1544241905212402, 'eval_runtime': 6.5885, 'eval_samples_per_second': 151.779, 'eval_steps_per_second': 9.562, 'epoch': 0.08}
{'loss': 1.2086, 'grad_norm': 1.1524468660354614, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0563714504241943, 'eval_runtime': 6.6028, 'eval_samples_per_second': 151.452, 'eval_steps_per_second': 9.541, 'epoch': 0.12}
{'loss': 1.0121, 'grad_norm': 0.7982044219970703, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9849317669868469, 'eval_runtime': 6.6011, 'eval_samples_per_second': 151.49, 'eval_steps_per_second': 9.544, 'epoch': 0.16}
{'loss': 1.0646, 'grad_norm': 0.8222554922103882, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9659010171890259, 'eval_runtime': 6.6232, 'eval_samples_per_second': 150.985, 'eval_steps_per_second': 9.512, 'epoch': 0.2}
{'loss': 0.9937, 'grad_norm': 0.5820105075836182, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9580126404762268, 'eval_runtime': 6.6181, 'eval_samples_per_second': 151.101, 'eval_steps_per_second': 9.519, 'epoch': 0.24}
{'loss': 0.9875, 'grad_norm': 0.6912475824356079, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9399599432945251, 'eval_runtime': 6.6136, 'eval_samples_per_second': 151.203, 'eval_steps_per_second': 9.526, 'epoch': 0.28}
{'loss': 1.034, 'grad_norm': 0.5170173048973083, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9191908240318298, 'eval_runtime': 6.6299, 'eval_samples_per_second': 150.832, 'eval_steps_per_second': 9.502, 'epoch': 0.32}
{'loss': 0.923, 'grad_norm': 0.4939078986644745, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9079457521438599, 'eval_runtime': 6.6235, 'eval_samples_per_second': 150.978, 'eval_steps_per_second': 9.512, 'epoch': 0.36}
{'loss': 0.9578, 'grad_norm': 0.5565173625946045, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9080859422683716, 'eval_runtime': 6.6195, 'eval_samples_per_second': 151.069, 'eval_steps_per_second': 9.517, 'epoch': 0.4}
{'loss': 0.9606, 'grad_norm': 0.728074312210083, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9255997538566589, 'eval_runtime': 6.6304, 'eval_samples_per_second': 150.82, 'eval_steps_per_second': 9.502, 'epoch': 0.44}
{'loss': 0.9286, 'grad_norm': 0.5070275068283081, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9081568121910095, 'eval_runtime': 6.6707, 'eval_samples_per_second': 149.909, 'eval_steps_per_second': 9.444, 'epoch': 0.48}
{'loss': 0.9509, 'grad_norm': 0.45973917841911316, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9189875721931458, 'eval_runtime': 6.6847, 'eval_samples_per_second': 149.596, 'eval_steps_per_second': 9.425, 'epoch': 0.52}
{'loss': 0.8907, 'grad_norm': 0.694843053817749, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8983098268508911, 'eval_runtime': 6.6617, 'eval_samples_per_second': 150.113, 'eval_steps_per_second': 9.457, 'epoch': 0.56}
{'loss': 0.9497, 'grad_norm': 0.5603772401809692, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9110619425773621, 'eval_runtime': 6.6615, 'eval_samples_per_second': 150.117, 'eval_steps_per_second': 9.457, 'epoch': 0.6}
{'loss': 0.9105, 'grad_norm': 0.47712528705596924, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.899870753288269, 'eval_runtime': 6.6424, 'eval_samples_per_second': 150.548, 'eval_steps_per_second': 9.485, 'epoch': 0.64}
{'loss': 0.9103, 'grad_norm': 0.6753812432289124, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8994415402412415, 'eval_runtime': 6.6324, 'eval_samples_per_second': 150.774, 'eval_steps_per_second': 9.499, 'epoch': 0.68}
{'loss': 0.8882, 'grad_norm': 0.5973948836326599, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8946081399917603, 'eval_runtime': 6.6198, 'eval_samples_per_second': 151.062, 'eval_steps_per_second': 9.517, 'epoch': 0.72}
{'loss': 0.9035, 'grad_norm': 0.560019850730896, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8941928744316101, 'eval_runtime': 6.6211, 'eval_samples_per_second': 151.031, 'eval_steps_per_second': 9.515, 'epoch': 0.76}
{'loss': 0.9285, 'grad_norm': 0.6161580681800842, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8919498324394226, 'eval_runtime': 6.6254, 'eval_samples_per_second': 150.933, 'eval_steps_per_second': 9.509, 'epoch': 0.8}
{'loss': 0.9013, 'grad_norm': 0.5418281555175781, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8953202962875366, 'eval_runtime': 6.6329, 'eval_samples_per_second': 150.763, 'eval_steps_per_second': 9.498, 'epoch': 0.84}
{'loss': 0.9652, 'grad_norm': 0.7893795371055603, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8929749727249146, 'eval_runtime': 6.632, 'eval_samples_per_second': 150.783, 'eval_steps_per_second': 9.499, 'epoch': 0.88}
{'loss': 0.8618, 'grad_norm': 0.5447719097137451, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8927135467529297, 'eval_runtime': 6.6295, 'eval_samples_per_second': 150.842, 'eval_steps_per_second': 9.503, 'epoch': 0.92}
{'loss': 0.8563, 'grad_norm': 0.616222620010376, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8885997533798218, 'eval_runtime': 6.6462, 'eval_samples_per_second': 150.461, 'eval_steps_per_second': 9.479, 'epoch': 0.96}
{'loss': 0.8836, 'grad_norm': 0.5814850926399231, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8885278701782227, 'eval_runtime': 6.6216, 'eval_samples_per_second': 151.021, 'eval_steps_per_second': 9.514, 'epoch': 1.0}
{'train_runtime': 346.3796, 'train_samples_per_second': 28.861, 'train_steps_per_second': 1.804, 'train_loss': 1.0848891296386718, 'epoch': 1.0}
train_results:  {'eval_loss': [2.130790948867798, 1.1544241905212402, 1.0563714504241943, 0.9849317669868469, 0.9659010171890259, 0.9580126404762268, 0.9399599432945251, 0.9191908240318298, 0.9079457521438599, 0.9080859422683716, 0.9255997538566589, 0.9081568121910095, 0.9189875721931458, 0.8983098268508911, 0.9110619425773621, 0.899870753288269, 0.8994415402412415, 0.8946081399917603, 0.8941928744316101, 0.8919498324394226, 0.8953202962875366, 0.8929749727249146, 0.8927135467529297, 0.8885997533798218, 0.8885278701782227], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.130790948867798, 1.1544241905212402, 1.0563714504241943, 0.9849317669868469, 0.9659010171890259, 0.9580126404762268, 0.9399599432945251, 0.9191908240318298, 0.9079457521438599, 0.9080859422683716, 0.9255997538566589, 0.9081568121910095, 0.9189875721931458, 0.8983098268508911, 0.9110619425773621, 0.899870753288269, 0.8994415402412415, 0.8946081399917603, 0.8941928744316101, 0.8919498324394226, 0.8953202962875366, 0.8929749727249146, 0.8927135467529297, 0.8885997533798218, 0.8885278701782227]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760350227355957
current iteration best possible eval_loss (full train run):  -0.8885278701782227
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264, -1.1126946210861206, -1.0760297775268555, -1.0760350227355957]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.9210 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -1.0512801446362534
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -1.0512773441784187
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -1.0512773441830168
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -1.0553615172538116
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -1.0520625080154467
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0512, dtype=torch.float64), tensor(0.3199, dtype=torch.float64), tensor(0.0379, dtype=torch.float64), 0, tensor(0.2497, dtype=torch.float64), tensor(0.3414, dtype=torch.float64), 0, 17, 1, 1, 1, 1, 1, 65, 0.020581795293459597, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(6.4293e-18, dtype=torch.float64), tensor(5.3243e-18, dtype=torch.float64), tensor(0.0512, dtype=torch.float64), tensor(0.3199, dtype=torch.float64), tensor(0.0379, dtype=torch.float64), tensor(2.2956e-18, dtype=torch.float64), tensor(0.2497, dtype=torch.float64), tensor(0.3414, dtype=torch.float64), tensor(5.6463e-18, dtype=torch.float64), tensor(0.5402, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5108, dtype=torch.float64), tensor(0.2058, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.051
  sciq: 0.32
  triviaqa: 0.038
  truthfulqa_gen: 0
  wikitext: 0.25
  mmlu: 0.341
  arc_challenge: 0

LoRA Parameters:
  lora_r: (65,)
  lora_dropout: (0.020581795293459597,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  65
lora dropout:  0.020581795293459597
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,811,840 || all params: 8,106,073,088 || trainable%: 0.9352
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9492, 'grad_norm': 0.7688075304031372, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9206736087799072, 'eval_runtime': 7.3086, 'eval_samples_per_second': 136.825, 'eval_steps_per_second': 8.62, 'epoch': 0.04}
{'loss': 1.7596, 'grad_norm': 1.0785582065582275, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9444568157196045, 'eval_runtime': 7.2916, 'eval_samples_per_second': 137.143, 'eval_steps_per_second': 8.64, 'epoch': 0.08}
{'loss': 1.5468, 'grad_norm': 0.5283246040344238, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8899548053741455, 'eval_runtime': 7.302, 'eval_samples_per_second': 136.948, 'eval_steps_per_second': 8.628, 'epoch': 0.12}
{'loss': 1.4915, 'grad_norm': 0.5419716835021973, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7737009525299072, 'eval_runtime': 7.3346, 'eval_samples_per_second': 136.341, 'eval_steps_per_second': 8.589, 'epoch': 0.16}
{'loss': 1.4114, 'grad_norm': 0.782288670539856, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8057522773742676, 'eval_runtime': 7.3467, 'eval_samples_per_second': 136.116, 'eval_steps_per_second': 8.575, 'epoch': 0.2}
{'loss': 1.4732, 'grad_norm': 0.5176038146018982, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7875268459320068, 'eval_runtime': 7.3467, 'eval_samples_per_second': 136.116, 'eval_steps_per_second': 8.575, 'epoch': 0.24}
{'loss': 1.4024, 'grad_norm': 0.48595574498176575, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8184937238693237, 'eval_runtime': 7.3147, 'eval_samples_per_second': 136.712, 'eval_steps_per_second': 8.613, 'epoch': 0.28}
{'loss': 1.3781, 'grad_norm': 0.5483453869819641, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8212193250656128, 'eval_runtime': 7.3128, 'eval_samples_per_second': 136.747, 'eval_steps_per_second': 8.615, 'epoch': 0.32}
{'loss': 1.4237, 'grad_norm': 0.5514647364616394, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.81412935256958, 'eval_runtime': 7.2834, 'eval_samples_per_second': 137.298, 'eval_steps_per_second': 8.65, 'epoch': 0.36}
{'loss': 1.4228, 'grad_norm': 0.5344206094741821, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8549175262451172, 'eval_runtime': 7.2938, 'eval_samples_per_second': 137.103, 'eval_steps_per_second': 8.638, 'epoch': 0.4}
{'loss': 1.3889, 'grad_norm': 0.5586082339286804, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9070112705230713, 'eval_runtime': 7.2808, 'eval_samples_per_second': 137.348, 'eval_steps_per_second': 8.653, 'epoch': 0.44}
{'loss': 1.3813, 'grad_norm': 0.5235105752944946, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8121860027313232, 'eval_runtime': 7.2884, 'eval_samples_per_second': 137.204, 'eval_steps_per_second': 8.644, 'epoch': 0.48}
{'loss': 1.3866, 'grad_norm': 0.553977370262146, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8205790519714355, 'eval_runtime': 7.3818, 'eval_samples_per_second': 135.469, 'eval_steps_per_second': 8.535, 'epoch': 0.52}
{'loss': 1.3288, 'grad_norm': 0.45671260356903076, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8791943788528442, 'eval_runtime': 7.3748, 'eval_samples_per_second': 135.597, 'eval_steps_per_second': 8.543, 'epoch': 0.56}
{'loss': 1.3404, 'grad_norm': 0.42547911405563354, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8582677841186523, 'eval_runtime': 7.3844, 'eval_samples_per_second': 135.421, 'eval_steps_per_second': 8.532, 'epoch': 0.6}
{'loss': 1.2906, 'grad_norm': 0.4900095760822296, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8599672317504883, 'eval_runtime': 7.4039, 'eval_samples_per_second': 135.064, 'eval_steps_per_second': 8.509, 'epoch': 0.64}
{'loss': 1.3708, 'grad_norm': 0.43815791606903076, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8478999137878418, 'eval_runtime': 7.3245, 'eval_samples_per_second': 136.527, 'eval_steps_per_second': 8.601, 'epoch': 0.68}
{'loss': 1.3623, 'grad_norm': 0.4015393853187561, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8347587585449219, 'eval_runtime': 7.3173, 'eval_samples_per_second': 136.662, 'eval_steps_per_second': 8.61, 'epoch': 0.72}
{'loss': 1.363, 'grad_norm': 0.5743339657783508, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.839133858680725, 'eval_runtime': 7.3229, 'eval_samples_per_second': 136.558, 'eval_steps_per_second': 8.603, 'epoch': 0.76}
{'loss': 1.3935, 'grad_norm': 0.6819879412651062, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8454303741455078, 'eval_runtime': 7.2989, 'eval_samples_per_second': 137.008, 'eval_steps_per_second': 8.631, 'epoch': 0.8}
{'loss': 1.2606, 'grad_norm': 0.9158839583396912, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8412892818450928, 'eval_runtime': 7.2866, 'eval_samples_per_second': 137.238, 'eval_steps_per_second': 8.646, 'epoch': 0.84}
{'loss': 1.2806, 'grad_norm': 0.43671828508377075, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8786921501159668, 'eval_runtime': 7.3287, 'eval_samples_per_second': 136.449, 'eval_steps_per_second': 8.596, 'epoch': 0.88}
{'loss': 1.3959, 'grad_norm': 0.38231292366981506, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8676496744155884, 'eval_runtime': 7.3362, 'eval_samples_per_second': 136.31, 'eval_steps_per_second': 8.588, 'epoch': 0.92}
{'loss': 1.3702, 'grad_norm': 0.4310486614704132, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8699904680252075, 'eval_runtime': 7.3455, 'eval_samples_per_second': 136.138, 'eval_steps_per_second': 8.577, 'epoch': 0.96}
{'loss': 1.315, 'grad_norm': 0.5977156758308411, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8662980794906616, 'eval_runtime': 7.3193, 'eval_samples_per_second': 136.624, 'eval_steps_per_second': 8.607, 'epoch': 1.0}
{'train_runtime': 418.9026, 'train_samples_per_second': 23.865, 'train_steps_per_second': 1.492, 'train_loss': 1.4594917877197267, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9206736087799072, 1.9444568157196045, 1.8899548053741455, 1.7737009525299072, 1.8057522773742676, 1.7875268459320068, 1.8184937238693237, 1.8212193250656128, 1.81412935256958, 1.8549175262451172, 1.9070112705230713, 1.8121860027313232, 1.8205790519714355, 1.8791943788528442, 1.8582677841186523, 1.8599672317504883, 1.8478999137878418, 1.8347587585449219, 1.839133858680725, 1.8454303741455078, 1.8412892818450928, 1.8786921501159668, 1.8676496744155884, 1.8699904680252075, 1.8662980794906616], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9206736087799072, 1.9444568157196045, 1.8899548053741455, 1.7737009525299072, 1.8057522773742676, 1.7875268459320068, 1.8184937238693237, 1.8212193250656128, 1.81412935256958, 1.8549175262451172, 1.9070112705230713, 1.8121860027313232, 1.8205790519714355, 1.8791943788528442, 1.8582677841186523, 1.8599672317504883, 1.8478999137878418, 1.8347587585449219, 1.839133858680725, 1.8454303741455078, 1.8412892818450928, 1.8786921501159668, 1.8676496744155884, 1.8699904680252075, 1.8662980794906616]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1118274927139282
current iteration best possible eval_loss (full train run):  -1.8662980794906616
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264, -1.1126946210861206, -1.0760297775268555, -1.0760350227355957, -1.1118274927139282]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.2353 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -1.0505143704762874
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -1.0507962422299522
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -1.0504169846741163
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -1.0505143677337052
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -1.0505143677337052
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0369, dtype=torch.float64), tensor(0.0908, dtype=torch.float64), 0, 0, tensor(0.0482, dtype=torch.float64), 0, 0, tensor(0.7323, dtype=torch.float64), tensor(0.0919, dtype=torch.float64), 30, 1, 1, 1, 1, 1, 2, 0.0800325811833833, 30.662493058598848, 1]
normalized proposed parameters for next round by BO: [tensor(0.0369, dtype=torch.float64), tensor(0.0908, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.9858e-18, dtype=torch.float64), tensor(0.0482, dtype=torch.float64), tensor(4.4995e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7323, dtype=torch.float64), tensor(0.0919, dtype=torch.float64), tensor(0.9220, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.8003, dtype=torch.float64), tensor(0.6388, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.037
  gsm8k: 0.091
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.048
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.732
  arc_challenge: 0.092

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0800325811833833,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (30.662493058598848,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  2
lora dropout:  0.0800325811833833
lora alpha:  30.662493058598848
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 4,116,480 || all params: 8,034,377,728 || trainable%: 0.0512
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4469, 'grad_norm': 3.150810480117798, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3150014877319336, 'eval_runtime': 7.7374, 'eval_samples_per_second': 129.242, 'eval_steps_per_second': 8.142, 'epoch': 0.04}
{'loss': 1.3418, 'grad_norm': 3.2740445137023926, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.021489143371582, 'eval_runtime': 7.7449, 'eval_samples_per_second': 129.117, 'eval_steps_per_second': 8.134, 'epoch': 0.08}
{'loss': 1.2452, 'grad_norm': 1.778440237045288, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9681833386421204, 'eval_runtime': 7.7506, 'eval_samples_per_second': 129.022, 'eval_steps_per_second': 8.128, 'epoch': 0.12}
{'loss': 1.205, 'grad_norm': 1.6663798093795776, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9378940463066101, 'eval_runtime': 7.7545, 'eval_samples_per_second': 128.958, 'eval_steps_per_second': 8.124, 'epoch': 0.16}
{'loss': 1.1166, 'grad_norm': 1.4428459405899048, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9081504940986633, 'eval_runtime': 7.7589, 'eval_samples_per_second': 128.884, 'eval_steps_per_second': 8.12, 'epoch': 0.2}
{'loss': 1.1476, 'grad_norm': 1.494217872619629, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9026703834533691, 'eval_runtime': 7.7877, 'eval_samples_per_second': 128.407, 'eval_steps_per_second': 8.09, 'epoch': 0.24}
{'loss': 1.1456, 'grad_norm': 1.3285155296325684, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8976163268089294, 'eval_runtime': 7.7992, 'eval_samples_per_second': 128.219, 'eval_steps_per_second': 8.078, 'epoch': 0.28}
{'loss': 1.1068, 'grad_norm': 1.126760482788086, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8919781446456909, 'eval_runtime': 7.7777, 'eval_samples_per_second': 128.573, 'eval_steps_per_second': 8.1, 'epoch': 0.32}
{'loss': 1.1356, 'grad_norm': 1.2361681461334229, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8952023983001709, 'eval_runtime': 7.7597, 'eval_samples_per_second': 128.87, 'eval_steps_per_second': 8.119, 'epoch': 0.36}
{'loss': 1.1114, 'grad_norm': 2.574765205383301, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8965457677841187, 'eval_runtime': 7.7132, 'eval_samples_per_second': 129.647, 'eval_steps_per_second': 8.168, 'epoch': 0.4}
{'loss': 1.1179, 'grad_norm': 1.2713794708251953, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8924229145050049, 'eval_runtime': 7.7351, 'eval_samples_per_second': 129.281, 'eval_steps_per_second': 8.145, 'epoch': 0.44}
{'loss': 1.0955, 'grad_norm': 2.406757354736328, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8936154246330261, 'eval_runtime': 7.7478, 'eval_samples_per_second': 129.068, 'eval_steps_per_second': 8.131, 'epoch': 0.48}
{'loss': 1.1217, 'grad_norm': 1.248127818107605, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8927866220474243, 'eval_runtime': 7.7315, 'eval_samples_per_second': 129.342, 'eval_steps_per_second': 8.149, 'epoch': 0.52}
{'loss': 1.0875, 'grad_norm': 1.202134609222412, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8862003087997437, 'eval_runtime': 7.741, 'eval_samples_per_second': 129.183, 'eval_steps_per_second': 8.139, 'epoch': 0.56}
{'loss': 1.1017, 'grad_norm': 1.5549566745758057, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8930492997169495, 'eval_runtime': 7.7835, 'eval_samples_per_second': 128.477, 'eval_steps_per_second': 8.094, 'epoch': 0.6}
{'loss': 1.0677, 'grad_norm': 1.2604857683181763, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8804442286491394, 'eval_runtime': 7.7543, 'eval_samples_per_second': 128.96, 'eval_steps_per_second': 8.125, 'epoch': 0.64}
{'loss': 1.0492, 'grad_norm': 1.356632113456726, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8865893483161926, 'eval_runtime': 7.7398, 'eval_samples_per_second': 129.203, 'eval_steps_per_second': 8.14, 'epoch': 0.68}
{'loss': 1.0771, 'grad_norm': 1.196028470993042, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8874661922454834, 'eval_runtime': 7.7322, 'eval_samples_per_second': 129.329, 'eval_steps_per_second': 8.148, 'epoch': 0.72}
{'loss': 1.0257, 'grad_norm': 1.4117721319198608, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8846234083175659, 'eval_runtime': 7.7394, 'eval_samples_per_second': 129.208, 'eval_steps_per_second': 8.14, 'epoch': 0.76}
{'loss': 1.0437, 'grad_norm': 1.2927775382995605, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8845145106315613, 'eval_runtime': 7.7413, 'eval_samples_per_second': 129.177, 'eval_steps_per_second': 8.138, 'epoch': 0.8}
{'loss': 1.1046, 'grad_norm': 1.1886705160140991, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.883277952671051, 'eval_runtime': 7.7264, 'eval_samples_per_second': 129.426, 'eval_steps_per_second': 8.154, 'epoch': 0.84}
{'loss': 1.1002, 'grad_norm': 1.438450574874878, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.881770133972168, 'eval_runtime': 7.7232, 'eval_samples_per_second': 129.48, 'eval_steps_per_second': 8.157, 'epoch': 0.88}
{'loss': 1.0851, 'grad_norm': 1.8717501163482666, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8835443258285522, 'eval_runtime': 7.715, 'eval_samples_per_second': 129.618, 'eval_steps_per_second': 8.166, 'epoch': 0.92}
{'loss': 1.0461, 'grad_norm': 1.2507388591766357, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8808218836784363, 'eval_runtime': 7.7431, 'eval_samples_per_second': 129.147, 'eval_steps_per_second': 8.136, 'epoch': 0.96}
{'loss': 1.0603, 'grad_norm': 1.9761170148849487, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8807923793792725, 'eval_runtime': 7.765, 'eval_samples_per_second': 128.783, 'eval_steps_per_second': 8.113, 'epoch': 1.0}
{'train_runtime': 456.8353, 'train_samples_per_second': 21.883, 'train_steps_per_second': 1.368, 'train_loss': 1.1674606719970704, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3150014877319336, 1.021489143371582, 0.9681833386421204, 0.9378940463066101, 0.9081504940986633, 0.9026703834533691, 0.8976163268089294, 0.8919781446456909, 0.8952023983001709, 0.8965457677841187, 0.8924229145050049, 0.8936154246330261, 0.8927866220474243, 0.8862003087997437, 0.8930492997169495, 0.8804442286491394, 0.8865893483161926, 0.8874661922454834, 0.8846234083175659, 0.8845145106315613, 0.883277952671051, 0.881770133972168, 0.8835443258285522, 0.8808218836784363, 0.8807923793792725], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3150014877319336, 1.021489143371582, 0.9681833386421204, 0.9378940463066101, 0.9081504940986633, 0.9026703834533691, 0.8976163268089294, 0.8919781446456909, 0.8952023983001709, 0.8965457677841187, 0.8924229145050049, 0.8936154246330261, 0.8927866220474243, 0.8862003087997437, 0.8930492997169495, 0.8804442286491394, 0.8865893483161926, 0.8874661922454834, 0.8846234083175659, 0.8845145106315613, 0.883277952671051, 0.881770133972168, 0.8835443258285522, 0.8808218836784363, 0.8807923793792725]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760294198989868
current iteration best possible eval_loss (full train run):  -0.8807923793792725
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264, -1.1126946210861206, -1.0760297775268555, -1.0760350227355957, -1.1118274927139282, -1.0760294198989868]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.8450 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -1.093911515336376
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -1.0916978285132628
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -1.0827614483582986
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -1.0802018521180263
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -1.0933019900411167
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.5577, dtype=torch.float64), 0, 0, tensor(0.1311, dtype=torch.float64), 0, tensor(0.2547, dtype=torch.float64), tensor(0.0544, dtype=torch.float64), 0, 0, 17, 0, 0, 0, 1, 1, 18, 0.1, 30.46929222731287, 1]
normalized proposed parameters for next round by BO: [tensor(0.5577, dtype=torch.float64), tensor(1.4172e-16, dtype=torch.float64), tensor(4.0773e-17, dtype=torch.float64), tensor(0.1311, dtype=torch.float64), tensor(0.0021, dtype=torch.float64), tensor(0.2547, dtype=torch.float64), tensor(0.0544, dtype=torch.float64), tensor(1.2984e-17, dtype=torch.float64), tensor(4.2088e-20, dtype=torch.float64), tensor(0.5178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1433, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6348, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.558
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.131
  triviaqa: 0
  truthfulqa_gen: 0.255
  wikitext: 0.054
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (18,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (17,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (30.46929222731287,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  17
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  18
lora dropout:  0.1
lora alpha:  30.46929222731287
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 11,280,384 || all params: 8,041,541,632 || trainable%: 0.1403
length of training data:  9977
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6902, 'grad_norm': 5.0867919921875, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9275617599487305, 'eval_runtime': 6.4917, 'eval_samples_per_second': 154.042, 'eval_steps_per_second': 9.705, 'epoch': 0.04}
{'loss': 1.3616, 'grad_norm': 1.6269444227218628, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6613643169403076, 'eval_runtime': 6.5018, 'eval_samples_per_second': 153.803, 'eval_steps_per_second': 9.69, 'epoch': 0.08}
{'loss': 1.0957, 'grad_norm': 0.7116215825080872, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.619760513305664, 'eval_runtime': 6.5205, 'eval_samples_per_second': 153.364, 'eval_steps_per_second': 9.662, 'epoch': 0.12}
{'loss': 1.017, 'grad_norm': 0.5992395877838135, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.5387885570526123, 'eval_runtime': 6.4936, 'eval_samples_per_second': 153.998, 'eval_steps_per_second': 9.702, 'epoch': 0.16}
{'loss': 0.9004, 'grad_norm': 0.4433937966823578, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.5543917417526245, 'eval_runtime': 6.5139, 'eval_samples_per_second': 153.518, 'eval_steps_per_second': 9.672, 'epoch': 0.2}
{'loss': 0.9735, 'grad_norm': 0.5363435745239258, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 1.5872026681900024, 'eval_runtime': 6.5131, 'eval_samples_per_second': 153.537, 'eval_steps_per_second': 9.673, 'epoch': 0.24}
{'loss': 0.9265, 'grad_norm': 0.5813937187194824, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 1.5515329837799072, 'eval_runtime': 6.5169, 'eval_samples_per_second': 153.446, 'eval_steps_per_second': 9.667, 'epoch': 0.28}
{'loss': 0.9109, 'grad_norm': 0.48744532465934753, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 1.674012303352356, 'eval_runtime': 6.544, 'eval_samples_per_second': 152.813, 'eval_steps_per_second': 9.627, 'epoch': 0.32}
{'loss': 0.8681, 'grad_norm': 0.5847836136817932, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 1.590222954750061, 'eval_runtime': 6.5493, 'eval_samples_per_second': 152.688, 'eval_steps_per_second': 9.619, 'epoch': 0.36}
{'loss': 0.8805, 'grad_norm': 0.5795878171920776, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 1.643949031829834, 'eval_runtime': 6.5497, 'eval_samples_per_second': 152.678, 'eval_steps_per_second': 9.619, 'epoch': 0.4}
{'loss': 0.8544, 'grad_norm': 0.6506380438804626, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 1.6174551248550415, 'eval_runtime': 6.5581, 'eval_samples_per_second': 152.483, 'eval_steps_per_second': 9.606, 'epoch': 0.44}
{'loss': 0.8529, 'grad_norm': 0.5117736458778381, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 1.628921627998352, 'eval_runtime': 6.5489, 'eval_samples_per_second': 152.698, 'eval_steps_per_second': 9.62, 'epoch': 0.48}
{'loss': 0.8545, 'grad_norm': 0.5976154804229736, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 1.6666814088821411, 'eval_runtime': 6.5572, 'eval_samples_per_second': 152.504, 'eval_steps_per_second': 9.608, 'epoch': 0.52}
{'loss': 0.8837, 'grad_norm': 0.4567616879940033, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 1.6857925653457642, 'eval_runtime': 6.5726, 'eval_samples_per_second': 152.148, 'eval_steps_per_second': 9.585, 'epoch': 0.56}
{'loss': 0.8543, 'grad_norm': 0.4921683669090271, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 1.6966052055358887, 'eval_runtime': 6.5741, 'eval_samples_per_second': 152.111, 'eval_steps_per_second': 9.583, 'epoch': 0.6}
{'loss': 0.8321, 'grad_norm': 0.5036912560462952, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 1.7046757936477661, 'eval_runtime': 6.5572, 'eval_samples_per_second': 152.504, 'eval_steps_per_second': 9.608, 'epoch': 0.64}
{'loss': 0.8173, 'grad_norm': 0.4782334268093109, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 1.705774188041687, 'eval_runtime': 6.5541, 'eval_samples_per_second': 152.576, 'eval_steps_per_second': 9.612, 'epoch': 0.68}
{'loss': 0.841, 'grad_norm': 0.5058599710464478, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 1.7015022039413452, 'eval_runtime': 6.5684, 'eval_samples_per_second': 152.243, 'eval_steps_per_second': 9.591, 'epoch': 0.72}
{'loss': 0.8043, 'grad_norm': 0.5257002711296082, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 1.7227976322174072, 'eval_runtime': 6.5696, 'eval_samples_per_second': 152.217, 'eval_steps_per_second': 9.59, 'epoch': 0.76}
{'loss': 0.8342, 'grad_norm': 0.4870411157608032, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 1.7581998109817505, 'eval_runtime': 6.5564, 'eval_samples_per_second': 152.523, 'eval_steps_per_second': 9.609, 'epoch': 0.8}
{'loss': 0.8511, 'grad_norm': 0.6616734862327576, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 1.7688349485397339, 'eval_runtime': 6.5473, 'eval_samples_per_second': 152.734, 'eval_steps_per_second': 9.622, 'epoch': 0.84}
{'loss': 0.8015, 'grad_norm': 0.5460437536239624, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 1.773971438407898, 'eval_runtime': 6.5407, 'eval_samples_per_second': 152.889, 'eval_steps_per_second': 9.632, 'epoch': 0.88}
{'loss': 0.8297, 'grad_norm': 0.7891958355903625, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 1.7471996545791626, 'eval_runtime': 6.5625, 'eval_samples_per_second': 152.382, 'eval_steps_per_second': 9.6, 'epoch': 0.92}
{'loss': 0.7807, 'grad_norm': 0.6016865372657776, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 1.7670634984970093, 'eval_runtime': 6.5882, 'eval_samples_per_second': 151.786, 'eval_steps_per_second': 9.563, 'epoch': 0.96}
{'train_runtime': 277.094, 'train_samples_per_second': 36.006, 'train_steps_per_second': 2.252, 'train_loss': 1.0048449711921887, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9275617599487305, 1.6613643169403076, 1.619760513305664, 1.5387885570526123, 1.5543917417526245, 1.5872026681900024, 1.5515329837799072, 1.674012303352356, 1.590222954750061, 1.643949031829834, 1.6174551248550415, 1.628921627998352, 1.6666814088821411, 1.6857925653457642, 1.6966052055358887, 1.7046757936477661, 1.705774188041687, 1.7015022039413452, 1.7227976322174072, 1.7581998109817505, 1.7688349485397339, 1.773971438407898, 1.7471996545791626, 1.7670634984970093], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9275617599487305, 1.6613643169403076, 1.619760513305664, 1.5387885570526123, 1.5543917417526245, 1.5872026681900024, 1.5515329837799072, 1.674012303352356, 1.590222954750061, 1.643949031829834, 1.6174551248550415, 1.628921627998352, 1.6666814088821411, 1.6857925653457642, 1.6966052055358887, 1.7046757936477661, 1.705774188041687, 1.7015022039413452, 1.7227976322174072, 1.7581998109817505, 1.7688349485397339, 1.773971438407898, 1.7471996545791626, 1.7670634984970093]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.076034426689148
current iteration best possible eval_loss (full train run):  -1.7670634984970093
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264, -1.1126946210861206, -1.0760297775268555, -1.0760350227355957, -1.1118274927139282, -1.0760294198989868, -1.076034426689148]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5310 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.647038459777832, 0.3565073609352112, 0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.6441194415092468, 0.6647308468818665, 0.9432829022407532, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.47025445103645325, 0.5915921330451965, 0.563302755355835, 0.236403226852417]  ‚Üí  acq = -1.1000560064928238
X = [0.7882768511772156, 0.0865660309791565, 0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.06684881448745728, 0.476356565952301, 0.49730604887008667, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.06370062381029129, 0.8247895836830139, 0.642969012260437, 0.2869639992713928]  ‚Üí  acq = -1.0966451125186412
X = [0.9537772536277771, 0.5254051685333252, 0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5333559513092041, 0.6058185696601868, 0.9840016961097717, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.07915887981653214, 0.6141131520271301, 0.8602699041366577, 0.6692355275154114]  ‚Üí  acq = -1.1086778925894467
X = [0.10701495409011841, 0.8236895203590393, 0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6551105976104736, 0.23645484447479248, 0.007582306861877441, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5136890411376953, 0.0011958479881286621, 0.378928005695343, 0.21825557947158813]  ‚Üí  acq = -1.0445023747594553
X = [0.37967562675476074, 0.21945631504058838, 0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4969014823436737, 0.5960436463356018, 0.011648118495941162, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.918420135974884, 0.3473445177078247, 0.7110291719436646, 0.30779868364334106]  ‚Üí  acq = -1.1254338638139372
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0519, dtype=torch.float64), tensor(0.5063, dtype=torch.float64), tensor(0.3896, dtype=torch.float64), 0, tensor(0.0521, dtype=torch.float64), 0, 0, 32, 0, 0, 0, 1, 1, 10, 0.06203172808859353, 20.910474027095127, 1]
normalized proposed parameters for next round by BO: [tensor(4.2032e-18, dtype=torch.float64), tensor(4.2294e-18, dtype=torch.float64), tensor(0.0519, dtype=torch.float64), tensor(0.5063, dtype=torch.float64), tensor(0.3896, dtype=torch.float64), tensor(1.2486e-18, dtype=torch.float64), tensor(0.0521, dtype=torch.float64), tensor(2.9101e-18, dtype=torch.float64), tensor(2.1990e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0791, dtype=torch.float64), tensor(0.6203, dtype=torch.float64), tensor(0.4356, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.052
  sciq: 0.506
  triviaqa: 0.39
  truthfulqa_gen: 0
  wikitext: 0.052
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (10,)
  lora_dropout: (0.06203172808859353,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (20.910474027095127,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  10
lora dropout:  0.06203172808859353
lora alpha:  20.910474027095127
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 11,796,480 || all params: 8,042,057,728 || trainable%: 0.1467
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.7656, 'grad_norm': 3.7495474815368652, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.193601608276367, 'eval_runtime': 6.8906, 'eval_samples_per_second': 145.125, 'eval_steps_per_second': 9.143, 'epoch': 0.04}
{'loss': 1.5279, 'grad_norm': 1.5698025226593018, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.072061538696289, 'eval_runtime': 6.964, 'eval_samples_per_second': 143.596, 'eval_steps_per_second': 9.047, 'epoch': 0.08}
{'loss': 1.1411, 'grad_norm': 0.727925181388855, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1748979091644287, 'eval_runtime': 6.9528, 'eval_samples_per_second': 143.828, 'eval_steps_per_second': 9.061, 'epoch': 0.12}
{'loss': 1.1569, 'grad_norm': 0.6386365294456482, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1878414154052734, 'eval_runtime': 6.9671, 'eval_samples_per_second': 143.531, 'eval_steps_per_second': 9.042, 'epoch': 0.16}
{'loss': 1.0772, 'grad_norm': 0.7326671481132507, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.1162569522857666, 'eval_runtime': 6.9782, 'eval_samples_per_second': 143.304, 'eval_steps_per_second': 9.028, 'epoch': 0.2}
{'loss': 1.0943, 'grad_norm': 0.46616148948669434, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1865601539611816, 'eval_runtime': 6.9854, 'eval_samples_per_second': 143.156, 'eval_steps_per_second': 9.019, 'epoch': 0.24}
{'loss': 0.9978, 'grad_norm': 0.6736834049224854, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.155559539794922, 'eval_runtime': 6.99, 'eval_samples_per_second': 143.062, 'eval_steps_per_second': 9.013, 'epoch': 0.28}
{'loss': 1.0694, 'grad_norm': 0.6831260323524475, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.0976665019989014, 'eval_runtime': 7.0023, 'eval_samples_per_second': 142.81, 'eval_steps_per_second': 8.997, 'epoch': 0.32}
{'loss': 1.0027, 'grad_norm': 0.5587502717971802, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.189833879470825, 'eval_runtime': 7.0393, 'eval_samples_per_second': 142.06, 'eval_steps_per_second': 8.95, 'epoch': 0.36}
{'loss': 1.0745, 'grad_norm': 0.7454050779342651, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.258147716522217, 'eval_runtime': 7.0342, 'eval_samples_per_second': 142.162, 'eval_steps_per_second': 8.956, 'epoch': 0.4}
{'loss': 1.0143, 'grad_norm': 0.5529918670654297, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.2280561923980713, 'eval_runtime': 7.0733, 'eval_samples_per_second': 141.376, 'eval_steps_per_second': 8.907, 'epoch': 0.44}
{'loss': 1.0667, 'grad_norm': 0.5439055562019348, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.262179136276245, 'eval_runtime': 7.0605, 'eval_samples_per_second': 141.633, 'eval_steps_per_second': 8.923, 'epoch': 0.48}
{'loss': 1.0957, 'grad_norm': 0.7472999691963196, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.2377371788024902, 'eval_runtime': 7.0596, 'eval_samples_per_second': 141.651, 'eval_steps_per_second': 8.924, 'epoch': 0.52}
{'loss': 1.0731, 'grad_norm': 0.6063833832740784, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.2766475677490234, 'eval_runtime': 7.0537, 'eval_samples_per_second': 141.769, 'eval_steps_per_second': 8.931, 'epoch': 0.56}
{'loss': 1.0836, 'grad_norm': 0.918147623538971, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.2557575702667236, 'eval_runtime': 7.0205, 'eval_samples_per_second': 142.44, 'eval_steps_per_second': 8.974, 'epoch': 0.6}
{'loss': 1.0583, 'grad_norm': 0.62196946144104, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.311718225479126, 'eval_runtime': 7.021, 'eval_samples_per_second': 142.431, 'eval_steps_per_second': 8.973, 'epoch': 0.64}
{'loss': 1.048, 'grad_norm': 0.5283958315849304, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2729651927948, 'eval_runtime': 6.9819, 'eval_samples_per_second': 143.228, 'eval_steps_per_second': 9.023, 'epoch': 0.68}
{'loss': 0.9849, 'grad_norm': 0.6181944608688354, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.246938467025757, 'eval_runtime': 6.9954, 'eval_samples_per_second': 142.951, 'eval_steps_per_second': 9.006, 'epoch': 0.72}
{'loss': 1.025, 'grad_norm': 0.5870736241340637, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.2978055477142334, 'eval_runtime': 6.9777, 'eval_samples_per_second': 143.314, 'eval_steps_per_second': 9.029, 'epoch': 0.76}
{'loss': 0.9408, 'grad_norm': 0.7293800115585327, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.278916835784912, 'eval_runtime': 6.964, 'eval_samples_per_second': 143.597, 'eval_steps_per_second': 9.047, 'epoch': 0.8}
{'loss': 1.0269, 'grad_norm': 0.7786691784858704, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.2807388305664062, 'eval_runtime': 6.9587, 'eval_samples_per_second': 143.704, 'eval_steps_per_second': 9.053, 'epoch': 0.84}
{'loss': 1.0713, 'grad_norm': 0.6496931314468384, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.3144795894622803, 'eval_runtime': 6.9699, 'eval_samples_per_second': 143.475, 'eval_steps_per_second': 9.039, 'epoch': 0.88}
{'loss': 1.0497, 'grad_norm': 0.8337822556495667, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.339463949203491, 'eval_runtime': 6.9651, 'eval_samples_per_second': 143.574, 'eval_steps_per_second': 9.045, 'epoch': 0.92}
{'loss': 0.9882, 'grad_norm': 0.6465439796447754, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.3437883853912354, 'eval_runtime': 6.977, 'eval_samples_per_second': 143.328, 'eval_steps_per_second': 9.03, 'epoch': 0.96}
{'loss': 0.9914, 'grad_norm': 0.7165613174438477, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.343449592590332, 'eval_runtime': 6.9855, 'eval_samples_per_second': 143.154, 'eval_steps_per_second': 9.019, 'epoch': 1.0}
{'train_runtime': 339.2089, 'train_samples_per_second': 29.477, 'train_steps_per_second': 1.843, 'train_loss': 1.1770075378417968, 'epoch': 1.0}
train_results:  {'eval_loss': [2.193601608276367, 2.072061538696289, 2.1748979091644287, 2.1878414154052734, 2.1162569522857666, 2.1865601539611816, 2.155559539794922, 2.0976665019989014, 2.189833879470825, 2.258147716522217, 2.2280561923980713, 2.262179136276245, 2.2377371788024902, 2.2766475677490234, 2.2557575702667236, 2.311718225479126, 2.2729651927948, 2.246938467025757, 2.2978055477142334, 2.278916835784912, 2.2807388305664062, 2.3144795894622803, 2.339463949203491, 2.3437883853912354, 2.343449592590332], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.193601608276367, 2.072061538696289, 2.1748979091644287, 2.1878414154052734, 2.1162569522857666, 2.1865601539611816, 2.155559539794922, 2.0976665019989014, 2.189833879470825, 2.258147716522217, 2.2280561923980713, 2.262179136276245, 2.2377371788024902, 2.2766475677490234, 2.2557575702667236, 2.311718225479126, 2.2729651927948, 2.246938467025757, 2.2978055477142334, 2.278916835784912, 2.2807388305664062, 2.3144795894622803, 2.339463949203491, 2.3437883853912354, 2.343449592590332]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760318040847778
current iteration best possible eval_loss (full train run):  -2.343449592590332
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264, -1.1126946210861206, -1.0760297775268555, -1.0760350227355957, -1.1118274927139282, -1.0760294198989868, -1.076034426689148, -1.0760318040847778]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 27.9594 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.069575238947366
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.0695674492825262
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.0695704833524784
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.0696460867725013
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.0713668447842315
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0283, dtype=torch.float64), tensor(0.0818, dtype=torch.float64), tensor(0.0809, dtype=torch.float64), tensor(0.0871, dtype=torch.float64), tensor(0.2081, dtype=torch.float64), tensor(0.3372, dtype=torch.float64), tensor(0.0675, dtype=torch.float64), tensor(0.1091, dtype=torch.float64), 0, 32, 1, 1, 1, 1, 1, 64, 0.00417089390274739, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0283, dtype=torch.float64), tensor(0.0818, dtype=torch.float64), tensor(0.0809, dtype=torch.float64), tensor(0.0871, dtype=torch.float64), tensor(0.2081, dtype=torch.float64), tensor(0.3372, dtype=torch.float64), tensor(0.0675, dtype=torch.float64), tensor(0.1091, dtype=torch.float64), tensor(2.8718e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4970, dtype=torch.float64), tensor(0.0417, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.028
  gsm8k: 0.082
  rowan_hellaswag: 0.081
  sciq: 0.087
  triviaqa: 0.208
  truthfulqa_gen: 0.337
  wikitext: 0.067
  mmlu: 0.109
  arc_challenge: 0

LoRA Parameters:
  lora_r: (64,)
  lora_dropout: (0.00417089390274739,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  64
lora dropout:  0.00417089390274739
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 140,509,184 || all params: 8,170,770,432 || trainable%: 1.7197
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8097, 'grad_norm': 0.6952471733093262, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.584409236907959, 'eval_runtime': 7.7142, 'eval_samples_per_second': 129.631, 'eval_steps_per_second': 8.167, 'epoch': 0.04}
{'loss': 1.3334, 'grad_norm': 0.4748448133468628, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1628364324569702, 'eval_runtime': 7.7211, 'eval_samples_per_second': 129.516, 'eval_steps_per_second': 8.159, 'epoch': 0.08}
{'loss': 1.2156, 'grad_norm': 0.3798975646495819, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2057875394821167, 'eval_runtime': 7.6683, 'eval_samples_per_second': 130.406, 'eval_steps_per_second': 8.216, 'epoch': 0.12}
{'loss': 1.1798, 'grad_norm': 0.40534523129463196, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2416528463363647, 'eval_runtime': 7.6748, 'eval_samples_per_second': 130.296, 'eval_steps_per_second': 8.209, 'epoch': 0.16}
{'loss': 1.2111, 'grad_norm': 0.5124689936637878, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2501505613327026, 'eval_runtime': 7.6652, 'eval_samples_per_second': 130.46, 'eval_steps_per_second': 8.219, 'epoch': 0.2}
{'loss': 1.1812, 'grad_norm': 0.38892731070518494, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2591333389282227, 'eval_runtime': 7.69, 'eval_samples_per_second': 130.039, 'eval_steps_per_second': 8.192, 'epoch': 0.24}
{'loss': 1.1188, 'grad_norm': 0.3770252466201782, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2215648889541626, 'eval_runtime': 7.7036, 'eval_samples_per_second': 129.809, 'eval_steps_per_second': 8.178, 'epoch': 0.28}
{'loss': 1.0929, 'grad_norm': 0.6011120676994324, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2458161115646362, 'eval_runtime': 7.6921, 'eval_samples_per_second': 130.004, 'eval_steps_per_second': 8.19, 'epoch': 0.32}
{'loss': 1.1186, 'grad_norm': 0.7068628072738647, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2624019384384155, 'eval_runtime': 7.7001, 'eval_samples_per_second': 129.868, 'eval_steps_per_second': 8.182, 'epoch': 0.36}
{'loss': 1.0625, 'grad_norm': 0.423697829246521, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2352378368377686, 'eval_runtime': 7.7033, 'eval_samples_per_second': 129.815, 'eval_steps_per_second': 8.178, 'epoch': 0.4}
{'loss': 1.0998, 'grad_norm': 0.46049952507019043, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1986277103424072, 'eval_runtime': 7.7067, 'eval_samples_per_second': 129.758, 'eval_steps_per_second': 8.175, 'epoch': 0.44}
{'loss': 1.0895, 'grad_norm': 0.3572435677051544, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2401750087738037, 'eval_runtime': 7.7046, 'eval_samples_per_second': 129.793, 'eval_steps_per_second': 8.177, 'epoch': 0.48}
{'loss': 1.1242, 'grad_norm': 0.39459624886512756, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2196314334869385, 'eval_runtime': 7.705, 'eval_samples_per_second': 129.786, 'eval_steps_per_second': 8.177, 'epoch': 0.52}
{'loss': 1.0769, 'grad_norm': 0.3863130807876587, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2117807865142822, 'eval_runtime': 7.7343, 'eval_samples_per_second': 129.294, 'eval_steps_per_second': 8.145, 'epoch': 0.56}
{'loss': 1.0896, 'grad_norm': 0.4277289807796478, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2169963121414185, 'eval_runtime': 7.7361, 'eval_samples_per_second': 129.265, 'eval_steps_per_second': 8.144, 'epoch': 0.6}
{'loss': 1.0007, 'grad_norm': 0.39058762788772583, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.227320671081543, 'eval_runtime': 7.7426, 'eval_samples_per_second': 129.156, 'eval_steps_per_second': 8.137, 'epoch': 0.64}
{'loss': 1.0324, 'grad_norm': 0.43891769647598267, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2486963272094727, 'eval_runtime': 7.7443, 'eval_samples_per_second': 129.128, 'eval_steps_per_second': 8.135, 'epoch': 0.68}
{'loss': 1.0058, 'grad_norm': 0.36862003803253174, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2254250049591064, 'eval_runtime': 7.7303, 'eval_samples_per_second': 129.361, 'eval_steps_per_second': 8.15, 'epoch': 0.72}
{'loss': 1.008, 'grad_norm': 0.5616156458854675, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2167295217514038, 'eval_runtime': 7.7621, 'eval_samples_per_second': 128.832, 'eval_steps_per_second': 8.116, 'epoch': 0.76}
{'loss': 1.0353, 'grad_norm': 0.40708237886428833, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2465312480926514, 'eval_runtime': 7.7266, 'eval_samples_per_second': 129.423, 'eval_steps_per_second': 8.154, 'epoch': 0.8}
{'loss': 1.0395, 'grad_norm': 0.43899625539779663, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2604780197143555, 'eval_runtime': 7.6958, 'eval_samples_per_second': 129.941, 'eval_steps_per_second': 8.186, 'epoch': 0.84}
{'loss': 1.0373, 'grad_norm': 0.3776203691959381, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.258581280708313, 'eval_runtime': 7.7275, 'eval_samples_per_second': 129.407, 'eval_steps_per_second': 8.153, 'epoch': 0.88}
{'loss': 0.9418, 'grad_norm': 0.4754377603530884, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.254534125328064, 'eval_runtime': 7.7012, 'eval_samples_per_second': 129.85, 'eval_steps_per_second': 8.181, 'epoch': 0.92}
{'loss': 0.9812, 'grad_norm': 0.3956696391105652, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2565959692001343, 'eval_runtime': 7.7019, 'eval_samples_per_second': 129.838, 'eval_steps_per_second': 8.18, 'epoch': 0.96}
{'loss': 0.9417, 'grad_norm': 0.40584439039230347, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2569853067398071, 'eval_runtime': 7.7207, 'eval_samples_per_second': 129.523, 'eval_steps_per_second': 8.16, 'epoch': 1.0}
{'train_runtime': 440.2279, 'train_samples_per_second': 22.709, 'train_steps_per_second': 1.42, 'train_loss': 1.1530770965576171, 'epoch': 1.0}
train_results:  {'eval_loss': [1.584409236907959, 1.1628364324569702, 1.2057875394821167, 1.2416528463363647, 1.2501505613327026, 1.2591333389282227, 1.2215648889541626, 1.2458161115646362, 1.2624019384384155, 1.2352378368377686, 1.1986277103424072, 1.2401750087738037, 1.2196314334869385, 1.2117807865142822, 1.2169963121414185, 1.227320671081543, 1.2486963272094727, 1.2254250049591064, 1.2167295217514038, 1.2465312480926514, 1.2604780197143555, 1.258581280708313, 1.254534125328064, 1.2565959692001343, 1.2569853067398071], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.584409236907959, 1.1628364324569702, 1.2057875394821167, 1.2416528463363647, 1.2501505613327026, 1.2591333389282227, 1.2215648889541626, 1.2458161115646362, 1.2624019384384155, 1.2352378368377686, 1.1986277103424072, 1.2401750087738037, 1.2196314334869385, 1.2117807865142822, 1.2169963121414185, 1.227320671081543, 1.2486963272094727, 1.2254250049591064, 1.2167295217514038, 1.2465312480926514, 1.2604780197143555, 1.258581280708313, 1.254534125328064, 1.2565959692001343, 1.2569853067398071]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0936832427978516
current iteration best possible eval_loss (full train run):  -1.2569853067398071
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264, -1.1126946210861206, -1.0760297775268555, -1.0760350227355957, -1.1118274927139282, -1.0760294198989868, -1.076034426689148, -1.0760318040847778, -1.0936832427978516]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.1669 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -1.0764154324674027
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -1.072018893590419
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -1.0845104849413845
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -1.0734935731615705
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -1.0718461116894273
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1882, dtype=torch.float64), 0, tensor(0.0590, dtype=torch.float64), tensor(0.5257, dtype=torch.float64), 0, tensor(0.0447, dtype=torch.float64), 0, 0, tensor(0.1779, dtype=torch.float64), 11, 0, 0, 0, 1, 1, 31, 0.1, 26.723320999611325, 1]
normalized proposed parameters for next round by BO: [tensor(0.1882, dtype=torch.float64), tensor(1.0495e-18, dtype=torch.float64), tensor(0.0590, dtype=torch.float64), tensor(0.5257, dtype=torch.float64), tensor(0.0045, dtype=torch.float64), tensor(0.0447, dtype=torch.float64), tensor(2.3637e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1779, dtype=torch.float64), tensor(0.3578, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2383, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5567, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.188
  gsm8k: 0
  rowan_hellaswag: 0.059
  sciq: 0.526
  triviaqa: 0
  truthfulqa_gen: 0.045
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.178

LoRA Parameters:
  lora_r: (31,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (26.723320999611325,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  31
lora dropout:  0.1
lora alpha:  26.723320999611325
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 12,570,624 || all params: 8,042,831,872 || trainable%: 0.1563
length of training data:  9952
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9404, 'grad_norm': 2.695155143737793, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1137397289276123, 'eval_runtime': 6.3947, 'eval_samples_per_second': 156.38, 'eval_steps_per_second': 9.852, 'epoch': 0.04}
{'loss': 1.6362, 'grad_norm': 1.5022363662719727, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0863455533981323, 'eval_runtime': 6.4537, 'eval_samples_per_second': 154.949, 'eval_steps_per_second': 9.762, 'epoch': 0.08}
{'loss': 1.1864, 'grad_norm': 0.6730700731277466, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 0.9807896018028259, 'eval_runtime': 6.4299, 'eval_samples_per_second': 155.523, 'eval_steps_per_second': 9.798, 'epoch': 0.12}
{'loss': 1.0821, 'grad_norm': 0.6784793138504028, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 0.9319087862968445, 'eval_runtime': 6.4246, 'eval_samples_per_second': 155.651, 'eval_steps_per_second': 9.806, 'epoch': 0.16}
{'loss': 1.0385, 'grad_norm': 0.4856052100658417, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 0.9045079350471497, 'eval_runtime': 6.4295, 'eval_samples_per_second': 155.532, 'eval_steps_per_second': 9.799, 'epoch': 0.2}
{'loss': 0.977, 'grad_norm': 0.6048714518547058, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 0.890609860420227, 'eval_runtime': 6.439, 'eval_samples_per_second': 155.303, 'eval_steps_per_second': 9.784, 'epoch': 0.24}
{'loss': 1.0457, 'grad_norm': 0.41283825039863586, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 0.8885427117347717, 'eval_runtime': 6.4512, 'eval_samples_per_second': 155.01, 'eval_steps_per_second': 9.766, 'epoch': 0.28}
{'loss': 0.9998, 'grad_norm': 0.5916329026222229, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 0.8869023323059082, 'eval_runtime': 6.4557, 'eval_samples_per_second': 154.903, 'eval_steps_per_second': 9.759, 'epoch': 0.32}
{'loss': 0.9777, 'grad_norm': 0.3462410867214203, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 0.8859941959381104, 'eval_runtime': 6.4505, 'eval_samples_per_second': 155.026, 'eval_steps_per_second': 9.767, 'epoch': 0.36}
{'loss': 0.9295, 'grad_norm': 0.4445039927959442, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 0.8849772214889526, 'eval_runtime': 6.4882, 'eval_samples_per_second': 154.125, 'eval_steps_per_second': 9.71, 'epoch': 0.4}
{'loss': 0.9396, 'grad_norm': 0.33742716908454895, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 0.8769004344940186, 'eval_runtime': 6.4591, 'eval_samples_per_second': 154.821, 'eval_steps_per_second': 9.754, 'epoch': 0.44}
{'loss': 0.9593, 'grad_norm': 0.3746638894081116, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 0.8762438893318176, 'eval_runtime': 6.4592, 'eval_samples_per_second': 154.818, 'eval_steps_per_second': 9.754, 'epoch': 0.48}
{'loss': 0.9938, 'grad_norm': 0.34934350848197937, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 0.8743261098861694, 'eval_runtime': 6.4554, 'eval_samples_per_second': 154.909, 'eval_steps_per_second': 9.759, 'epoch': 0.52}
{'loss': 0.9811, 'grad_norm': 0.4019674062728882, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 0.8738327026367188, 'eval_runtime': 6.4627, 'eval_samples_per_second': 154.734, 'eval_steps_per_second': 9.748, 'epoch': 0.56}
{'loss': 1.0004, 'grad_norm': 0.34352514147758484, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 0.8773313164710999, 'eval_runtime': 6.4578, 'eval_samples_per_second': 154.853, 'eval_steps_per_second': 9.756, 'epoch': 0.6}
{'loss': 0.9318, 'grad_norm': 0.382282018661499, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 0.8685734272003174, 'eval_runtime': 6.5075, 'eval_samples_per_second': 153.67, 'eval_steps_per_second': 9.681, 'epoch': 0.64}
{'loss': 0.9952, 'grad_norm': 0.3223794102668762, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 0.8714160919189453, 'eval_runtime': 6.4809, 'eval_samples_per_second': 154.299, 'eval_steps_per_second': 9.721, 'epoch': 0.68}
{'loss': 0.9246, 'grad_norm': 0.3464389145374298, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 0.8707956075668335, 'eval_runtime': 6.5096, 'eval_samples_per_second': 153.618, 'eval_steps_per_second': 9.678, 'epoch': 0.72}
{'loss': 0.9766, 'grad_norm': 0.3997364640235901, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 0.8706203103065491, 'eval_runtime': 6.4829, 'eval_samples_per_second': 154.251, 'eval_steps_per_second': 9.718, 'epoch': 0.76}
{'loss': 0.9406, 'grad_norm': 0.37465062737464905, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.8695505857467651, 'eval_runtime': 6.5102, 'eval_samples_per_second': 153.604, 'eval_steps_per_second': 9.677, 'epoch': 0.8}
{'loss': 0.9097, 'grad_norm': 0.3908955156803131, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 0.8690245747566223, 'eval_runtime': 6.5059, 'eval_samples_per_second': 153.707, 'eval_steps_per_second': 9.684, 'epoch': 0.84}
{'loss': 0.8882, 'grad_norm': 0.5598915815353394, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.867518961429596, 'eval_runtime': 6.4993, 'eval_samples_per_second': 153.862, 'eval_steps_per_second': 9.693, 'epoch': 0.88}
{'loss': 0.9352, 'grad_norm': 0.3957485556602478, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.8661045432090759, 'eval_runtime': 6.4719, 'eval_samples_per_second': 154.514, 'eval_steps_per_second': 9.734, 'epoch': 0.92}
{'loss': 0.9162, 'grad_norm': 0.3799602687358856, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.8667968511581421, 'eval_runtime': 6.4975, 'eval_samples_per_second': 153.906, 'eval_steps_per_second': 9.696, 'epoch': 0.96}
{'train_runtime': 319.9775, 'train_samples_per_second': 31.102, 'train_steps_per_second': 1.944, 'train_loss': 1.1209588894123432, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1137397289276123, 1.0863455533981323, 0.9807896018028259, 0.9319087862968445, 0.9045079350471497, 0.890609860420227, 0.8885427117347717, 0.8869023323059082, 0.8859941959381104, 0.8849772214889526, 0.8769004344940186, 0.8762438893318176, 0.8743261098861694, 0.8738327026367188, 0.8773313164710999, 0.8685734272003174, 0.8714160919189453, 0.8707956075668335, 0.8706203103065491, 0.8695505857467651, 0.8690245747566223, 0.867518961429596, 0.8661045432090759, 0.8667968511581421], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.1137397289276123, 1.0863455533981323, 0.9807896018028259, 0.9319087862968445, 0.9045079350471497, 0.890609860420227, 0.8885427117347717, 0.8869023323059082, 0.8859941959381104, 0.8849772214889526, 0.8769004344940186, 0.8762438893318176, 0.8743261098861694, 0.8738327026367188, 0.8773313164710999, 0.8685734272003174, 0.8714160919189453, 0.8707956075668335, 0.8706203103065491, 0.8695505857467651, 0.8690245747566223, 0.867518961429596, 0.8661045432090759, 0.8667968511581421]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0924404859542847
current iteration best possible eval_loss (full train run):  -0.8667968511581421
max eval_loss so far:  -0.8613058924674988
BO observations:  [-1.0760282278060913, -1.0760291814804077, -1.07602858543396, -1.0760301351547241, -1.0760291814804077, -1.5365482568740845, -1.0760306119918823, -1.0960626602172852, -1.0969380140304565, -1.0760269165039062, -1.0760340690612793, -1.0760291814804077, -1.1062551736831665, -1.4758241176605225, -1.0760295391082764, -1.1282360553741455, -1.094757318496704, -1.0760290622711182, -1.0961191654205322, -1.2706166505813599, -1.0890300273895264, -1.1126946210861206, -1.0760297775268555, -1.0760350227355957, -1.1118274927139282, -1.0760294198989868, -1.076034426689148, -1.0760318040847778, -1.0936832427978516, -1.0924404859542847]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9218 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3357044458389282, 0.4876623749732971, 0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7566351294517517, 0.44088155031204224, 0.21595293283462524, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7294671535491943, 0.9934723973274231, 0.20285475254058838, 0.606965959072113]  ‚Üí  acq = -1.0845694893248539
X = [0.7454677820205688, 0.4424137473106384, 0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8704531192779541, 0.009343624114990234, 0.477742075920105, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7311053276062012, 0.7334456443786621, 0.34133514761924744, 0.4159647822380066]  ‚Üí  acq = -1.089967993272676
X = [0.6070147752761841, 0.050814270973205566, 0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.06286248564720154, 0.38677525520324707, 0.3574179410934448, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5247937440872192, 0.7738797068595886, 0.6983144283294678, 0.9456675052642822]  ‚Üí  acq = -1.0846816925825438
X = [0.05165296792984009, 0.43891578912734985, 0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.34409016370773315, 0.6168457269668579, 0.23861968517303467, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.4990995526313782, 0.5673343539237976, 0.3437628448009491, 0.6652377843856812]  ‚Üí  acq = -1.0981336552494656
X = [0.05172693729400635, 0.2110685110092163, 0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.34519943594932556, 0.5801001787185669, 0.783306360244751, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9125829935073853, 0.5967274308204651, 0.9407432079315186, 0.06500035524368286]  ‚Üí  acq = -1.0834765759324043
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3996, dtype=torch.float64), 0, tensor(0.0379, dtype=torch.float64), tensor(0.0991, dtype=torch.float64), tensor(0.1145, dtype=torch.float64), tensor(0.0600, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.1702, dtype=torch.float64), tensor(0.0905, dtype=torch.float64), 9, 0, 0, 0, 1, 1, 2, 0.056040671755051646, 30.28142599472625, 1]
normalized proposed parameters for next round by BO: [tensor(0.3996, dtype=torch.float64), tensor(8.2216e-19, dtype=torch.float64), tensor(0.0379, dtype=torch.float64), tensor(0.0991, dtype=torch.float64), tensor(0.1145, dtype=torch.float64), tensor(0.0600, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.1702, dtype=torch.float64), tensor(0.0905, dtype=torch.float64), tensor(0.2826, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.5604, dtype=torch.float64), tensor(0.6309, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-0.9445481896400452, -0.931367814540863, -0.931367814540863, -0.9022233486175537, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
arc_challenge
evaluation dataset:
data domain:  arc_challenge  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/arc_challenge/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 17, 1, 0, 1, 1, 0, 59, 0.00445603671922955, 41, 0]
Checking history sample input_X_between_0_1:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 0.53125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4609375, 0.044560367192295496, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3072175979614258
Checking history sample input_X:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 22, 1, 1, 1, 0, 0, 61, 0.08406775506720655, 5, 1]
Checking history sample input_X_between_0_1:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 0.6875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4765625, 0.8406775506720655, 0.10416666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1613417863845825
Checking history sample input_X:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 15, 1, 1, 1, 1, 0, 24, 0.05173325143341287, 28, 0]
Checking history sample input_X_between_0_1:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 0.46875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1875, 0.5173325143341286, 0.5833333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1443150043487549
Checking history sample input_X:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 1, 0, 1, 1, 0, 0, 114, 0.021033555465481826, 23, 1]
Checking history sample input_X_between_0_1:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 0.03125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.890625, 0.21033555465481824, 0.4791666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.3097282648086548
Checking history sample input_X:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 27, 1, 1, 1, 1, 1, 70, 0.005963494977880357, 46, 1]
Checking history sample input_X_between_0_1:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 0.05963494977880357, 0.9583333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9743626117706299
Checking history sample input_X:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 12, 1, 0, 1, 0, 0, 3, 0.017954470577765235, 12, 1]
Checking history sample input_X_between_0_1:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0234375, 0.17954470577765233, 0.25, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2079665660858154
Checking history sample input_X:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 18, 1, 0, 0, 0, 1, 5, 0.028541773579282805, 35, 0]
Checking history sample input_X_between_0_1:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 0.5625, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0390625, 0.28541773579282803, 0.7291666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0695475339889526
Checking history sample input_X:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 24, 0, 1, 0, 1, 0, 55, 0.08420649661865921, 22, 0]
Checking history sample input_X_between_0_1:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 0.75, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4296875, 0.8420649661865921, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0792094469070435
Checking history sample input_X:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 30, 1, 0, 1, 1, 0, 5, 0.09059974254780842, 29, 1]
Checking history sample input_X_between_0_1:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 0.9375, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0390625, 0.9059974254780842, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9744297862052917
Checking history sample input_X:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 20, 1, 1, 0, 1, 0, 27, 0.07188580123073206, 22, 0]
Checking history sample input_X_between_0_1:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 0.625, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2109375, 0.7188580123073205, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0874465703964233
Checking history sample input_X:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 8, 0, 0, 0, 1, 1, 31, 0.06288146497812035, 13, 1]
Checking history sample input_X_between_0_1:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2421875, 0.6288146497812034, 0.2708333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0237352848052979
Checking history sample input_X:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 32, 1, 1, 0, 1, 1, 114, 0.08252011949389138, 13, 0]
Checking history sample input_X_between_0_1:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.890625, 0.8252011949389138, 0.2708333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1076443195343018
Checking history sample input_X:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 11, 0, 1, 0, 0, 1, 100, 0.09203832421630076, 16, 1]
Checking history sample input_X_between_0_1:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 0.34375, 0.0, 1.0, 0.0, 0.0, 1.0, 0.78125, 0.9203832421630076, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.139199137687683
Checking history sample input_X:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 31, 1, 0, 1, 1, 1, 60, 0.008025447056787238, 26, 0]
Checking history sample input_X_between_0_1:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 0.96875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.46875, 0.08025447056787237, 0.5416666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2780427932739258
Checking history sample input_X:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 7, 0, 0, 1, 0, 0, 10, 0.06890300584266877, 3, 1]
Checking history sample input_X_between_0_1:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.078125, 0.6890300584266876, 0.0625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.5037378072738647
Checking history sample input_X:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 1, 1, 1, 0, 1, 0, 28, 0.0879909943648135, 16, 0]
Checking history sample input_X_between_0_1:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 0.03125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.21875, 0.879909943648135, 0.3333333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1370042562484741
Checking history sample input_X:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 1, 1, 0, 0, 1, 0, 47, 0.04979780998318395, 37, 0]
Checking history sample input_X_between_0_1:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 0.03125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3671875, 0.4979780998318395, 0.7708333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1642110347747803
Checking history sample input_X:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 30, 1, 0, 1, 0, 0, 86, 0.08085387005284786, 6, 1]
Checking history sample input_X_between_0_1:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 0.9375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.671875, 0.8085387005284785, 0.125, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1513490676879883
Checking history sample input_X:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 31, 0, 1, 1, 1, 1, 84, 0.015776955723827136, 41, 1]
Checking history sample input_X_between_0_1:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 0.96875, 0.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.15776955723827135, 0.8541666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0676783323287964
Checking history sample input_X:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 24, 0, 1, 1, 0, 1, 1, 0.07052967196513893, 22, 0]
Checking history sample input_X_between_0_1:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0078125, 0.7052967196513893, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -0.999065637588501
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3199 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5127438902854919, 0.2584598660469055, 0.46276015043258667, 0.45267802476882935, 0.5981081128120422, 0.6203485727310181, 0.8743338584899902, 0.36928802728652954, 0.19647294282913208, 0.9270771741867065, 0.717819094657898, 0.9776399731636047, 0.9971083402633667, 0.18407493829727173, 0.2820512056350708, 0.01808677427470684, 0.29640334844589233, 0.4291670024394989, 0.6168282628059387]  ‚Üí  acq = -0.7600733889146574
X = [0.35247862339019775, 0.9623137712478638, 0.025654077529907227, 0.884928286075592, 0.6278672814369202, 0.45189279317855835, 0.1758977174758911, 0.432364284992218, 0.82850182056427, 0.07936932146549225, 0.49627870321273804, 0.2101234793663025, 0.4305505156517029, 0.8011020421981812, 0.9251723885536194, 0.667538583278656, 0.5131639838218689, 0.08707836270332336, 0.7114359140396118]  ‚Üí  acq = -0.7598076633863495
X = [0.21349084377288818, 0.667143702507019, 0.6081439256668091, 0.886577844619751, 0.3928453326225281, 0.5573267340660095, 0.6028369069099426, 0.07905411720275879, 0.8902835845947266, 0.19963105022907257, 0.24321013689041138, 0.2364453673362732, 0.53624427318573, 0.9353040456771851, 0.2590593695640564, 0.7763976454734802, 0.877033531665802, 0.5041688680648804, 0.6502122282981873]  ‚Üí  acq = -0.7600742649736277
X = [0.19365960359573364, 0.3253079056739807, 0.5448922514915466, 0.2400447130203247, 0.23860079050064087, 0.8798542022705078, 0.7010204195976257, 0.7635314464569092, 0.7340132594108582, 0.10895384848117828, 0.39294445514678955, 0.8465108871459961, 0.42448890209198, 0.43526172637939453, 0.09955936670303345, 0.7349498271942139, 0.30732011795043945, 0.466825395822525, 0.026748716831207275]  ‚Üí  acq = -0.7600743592288626
X = [0.10293912887573242, 0.9647647142410278, 0.3700437545776367, 0.5887780785560608, 0.9643240571022034, 0.3416856527328491, 0.9319763779640198, 0.44624894857406616, 0.09912139177322388, 0.04862143099308014, 0.6237801909446716, 0.885686993598938, 0.08997660875320435, 0.19565868377685547, 0.9792602062225342, 0.470312237739563, 0.3973916172981262, 0.3434576690196991, 0.35659104585647583]  ‚Üí  acq = -0.7596533802019021
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.3903, dtype=torch.float64), 0, 0, tensor(0.1489, dtype=torch.float64), tensor(0.0577, dtype=torch.float64), tensor(0.1504, dtype=torch.float64), 0, 0, tensor(0.2527, dtype=torch.float64), 28, 0, 0, 0, 1, 1, 2, 0.1, 40.89587815205037, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.3903, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9797e-18, dtype=torch.float64), tensor(0.1489, dtype=torch.float64), tensor(0.0577, dtype=torch.float64), tensor(0.1504, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2527, dtype=torch.float64), tensor(0.8733, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8520, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.39
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.149
  triviaqa: 0.058
  truthfulqa_gen: 0.15
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.253

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (40.89587815205037,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  40.89587815205037
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,064,384 || all params: 8,032,325,632 || trainable%: 0.0257
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 3.1469, 'grad_norm': 6.263461589813232, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.276572346687317, 'eval_runtime': 7.3369, 'eval_samples_per_second': 136.297, 'eval_steps_per_second': 8.587, 'epoch': 0.04}
{'loss': 1.1452, 'grad_norm': 3.900524377822876, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9855964183807373, 'eval_runtime': 6.9339, 'eval_samples_per_second': 144.218, 'eval_steps_per_second': 9.086, 'epoch': 0.08}
{'loss': 0.9774, 'grad_norm': 2.1370270252227783, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9229776859283447, 'eval_runtime': 6.9277, 'eval_samples_per_second': 144.348, 'eval_steps_per_second': 9.094, 'epoch': 0.12}
{'loss': 0.9211, 'grad_norm': 1.9759713411331177, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8766210079193115, 'eval_runtime': 6.9332, 'eval_samples_per_second': 144.233, 'eval_steps_per_second': 9.087, 'epoch': 0.16}
{'loss': 0.8605, 'grad_norm': 1.3231340646743774, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8742086887359619, 'eval_runtime': 6.948, 'eval_samples_per_second': 143.926, 'eval_steps_per_second': 9.067, 'epoch': 0.2}
{'loss': 0.8462, 'grad_norm': 1.6374216079711914, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8806474804878235, 'eval_runtime': 6.9584, 'eval_samples_per_second': 143.71, 'eval_steps_per_second': 9.054, 'epoch': 0.24}
{'loss': 0.8195, 'grad_norm': 1.3449087142944336, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8722204566001892, 'eval_runtime': 6.9512, 'eval_samples_per_second': 143.86, 'eval_steps_per_second': 9.063, 'epoch': 0.28}
{'loss': 0.8165, 'grad_norm': 1.3305842876434326, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.877676248550415, 'eval_runtime': 6.9802, 'eval_samples_per_second': 143.262, 'eval_steps_per_second': 9.026, 'epoch': 0.32}
{'loss': 0.7978, 'grad_norm': 1.7419426441192627, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8834995031356812, 'eval_runtime': 6.9698, 'eval_samples_per_second': 143.476, 'eval_steps_per_second': 9.039, 'epoch': 0.36}
{'loss': 0.8135, 'grad_norm': 1.2993110418319702, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8806400299072266, 'eval_runtime': 6.9723, 'eval_samples_per_second': 143.425, 'eval_steps_per_second': 9.036, 'epoch': 0.4}
{'loss': 0.7901, 'grad_norm': 1.2144052982330322, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8825982809066772, 'eval_runtime': 7.0361, 'eval_samples_per_second': 142.125, 'eval_steps_per_second': 8.954, 'epoch': 0.44}
{'loss': 0.8021, 'grad_norm': 1.3332613706588745, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8847663402557373, 'eval_runtime': 7.0505, 'eval_samples_per_second': 141.834, 'eval_steps_per_second': 8.936, 'epoch': 0.48}
{'loss': 0.7551, 'grad_norm': 1.6653422117233276, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9033613204956055, 'eval_runtime': 7.0367, 'eval_samples_per_second': 142.112, 'eval_steps_per_second': 8.953, 'epoch': 0.52}
{'loss': 0.7702, 'grad_norm': 1.809010624885559, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.903340756893158, 'eval_runtime': 7.0481, 'eval_samples_per_second': 141.883, 'eval_steps_per_second': 8.939, 'epoch': 0.56}
{'loss': 0.7759, 'grad_norm': 1.544718623161316, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9003680348396301, 'eval_runtime': 7.0606, 'eval_samples_per_second': 141.631, 'eval_steps_per_second': 8.923, 'epoch': 0.6}
{'loss': 0.7269, 'grad_norm': 1.6213172674179077, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9149255156517029, 'eval_runtime': 7.0555, 'eval_samples_per_second': 141.733, 'eval_steps_per_second': 8.929, 'epoch': 0.64}
{'loss': 0.7496, 'grad_norm': 1.4423538446426392, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8992419242858887, 'eval_runtime': 7.0259, 'eval_samples_per_second': 142.33, 'eval_steps_per_second': 8.967, 'epoch': 0.68}
{'loss': 0.7152, 'grad_norm': 1.8250020742416382, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9297464489936829, 'eval_runtime': 7.009, 'eval_samples_per_second': 142.675, 'eval_steps_per_second': 8.988, 'epoch': 0.72}
{'loss': 0.7231, 'grad_norm': 1.7277792692184448, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9239477515220642, 'eval_runtime': 7.0202, 'eval_samples_per_second': 142.446, 'eval_steps_per_second': 8.974, 'epoch': 0.76}
{'loss': 0.7204, 'grad_norm': 1.6283625364303589, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9349382519721985, 'eval_runtime': 7.0298, 'eval_samples_per_second': 142.252, 'eval_steps_per_second': 8.962, 'epoch': 0.8}
{'loss': 0.6869, 'grad_norm': 1.7979248762130737, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9443042278289795, 'eval_runtime': 7.0029, 'eval_samples_per_second': 142.798, 'eval_steps_per_second': 8.996, 'epoch': 0.84}
{'loss': 0.6826, 'grad_norm': 1.9042606353759766, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9451746344566345, 'eval_runtime': 7.0205, 'eval_samples_per_second': 142.44, 'eval_steps_per_second': 8.974, 'epoch': 0.88}
{'loss': 0.7063, 'grad_norm': 2.03751802444458, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9331361055374146, 'eval_runtime': 7.029, 'eval_samples_per_second': 142.268, 'eval_steps_per_second': 8.963, 'epoch': 0.92}
{'loss': 0.7047, 'grad_norm': 1.6489170789718628, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9419864416122437, 'eval_runtime': 7.0148, 'eval_samples_per_second': 142.556, 'eval_steps_per_second': 8.981, 'epoch': 0.96}
{'loss': 0.708, 'grad_norm': 1.9478983879089355, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9422724843025208, 'eval_runtime': 7.0228, 'eval_samples_per_second': 142.392, 'eval_steps_per_second': 8.971, 'epoch': 1.0}
{'train_runtime': 317.2369, 'train_samples_per_second': 31.516, 'train_steps_per_second': 1.97, 'train_loss': 0.8864527465820312, 'epoch': 1.0}
train_results:  {'eval_loss': [1.276572346687317, 0.9855964183807373, 0.9229776859283447, 0.8766210079193115, 0.8742086887359619, 0.8806474804878235, 0.8722204566001892, 0.877676248550415, 0.8834995031356812, 0.8806400299072266, 0.8825982809066772, 0.8847663402557373, 0.9033613204956055, 0.903340756893158, 0.9003680348396301, 0.9149255156517029, 0.8992419242858887, 0.9297464489936829, 0.9239477515220642, 0.9349382519721985, 0.9443042278289795, 0.9451746344566345, 0.9331361055374146, 0.9419864416122437, 0.9422724843025208], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.276572346687317, 0.9855964183807373, 0.9229776859283447, 0.8766210079193115, 0.8742086887359619, 0.8806474804878235, 0.8722204566001892, 0.877676248550415, 0.8834995031356812, 0.8806400299072266, 0.8825982809066772, 0.8847663402557373, 0.9033613204956055, 0.903340756893158, 0.9003680348396301, 0.9149255156517029, 0.8992419242858887, 0.9297464489936829, 0.9239477515220642, 0.9349382519721985, 0.9443042278289795, 0.9451746344566345, 0.9331361055374146, 0.9419864416122437, 0.9422724843025208]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760282278060913
current iteration best possible eval_loss (full train run):  -0.9422724843025208
max eval_loss so far:  -0.9422724843025208
BO observations:  [-1.0760282278060913]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.1870 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.8564284162899862
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.8567388071010971
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.8564943247182195
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.8565016443534479
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.8553649326613673
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4231, dtype=torch.float64), 0, 0, tensor(0.3007, dtype=torch.float64), tensor(0.0319, dtype=torch.float64), 0, 0, tensor(0.2364, dtype=torch.float64), 25, 0, 1, 0, 1, 0, 2, 0.1, 42.946165120127574, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.4231, dtype=torch.float64), tensor(0.0079, dtype=torch.float64), tensor(3.5725e-18, dtype=torch.float64), tensor(0.3007, dtype=torch.float64), tensor(0.0319, dtype=torch.float64), tensor(5.0715e-18, dtype=torch.float64), tensor(9.1695e-18, dtype=torch.float64), tensor(0.2364, dtype=torch.float64), tensor(0.7814, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8947, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.423
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.301
  truthfulqa_gen: 0.032
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.236

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (42.946165120127574,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  42.946165120127574
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,177,600 || all params: 8,031,438,848 || trainable%: 0.0147
length of training data:  9919
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4168, 'grad_norm': 5.840363502502441, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5000441074371338, 'eval_runtime': 6.6465, 'eval_samples_per_second': 150.456, 'eval_steps_per_second': 9.479, 'epoch': 0.04}
{'loss': 1.0438, 'grad_norm': 1.8416399955749512, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.935445249080658, 'eval_runtime': 6.6726, 'eval_samples_per_second': 149.866, 'eval_steps_per_second': 9.442, 'epoch': 0.08}
{'loss': 0.8913, 'grad_norm': 1.5327303409576416, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 0.893282413482666, 'eval_runtime': 6.6422, 'eval_samples_per_second': 150.552, 'eval_steps_per_second': 9.485, 'epoch': 0.12}
{'loss': 0.9032, 'grad_norm': 1.425560474395752, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.8963949084281921, 'eval_runtime': 6.6443, 'eval_samples_per_second': 150.506, 'eval_steps_per_second': 9.482, 'epoch': 0.16}
{'loss': 0.8876, 'grad_norm': 1.3158124685287476, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.8927173018455505, 'eval_runtime': 6.6502, 'eval_samples_per_second': 150.372, 'eval_steps_per_second': 9.473, 'epoch': 0.2}
{'loss': 0.8881, 'grad_norm': 1.1235294342041016, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.8835208415985107, 'eval_runtime': 6.6522, 'eval_samples_per_second': 150.326, 'eval_steps_per_second': 9.471, 'epoch': 0.24}
{'loss': 0.8667, 'grad_norm': 1.2688584327697754, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.8758441209793091, 'eval_runtime': 6.6658, 'eval_samples_per_second': 150.019, 'eval_steps_per_second': 9.451, 'epoch': 0.28}
{'loss': 0.8703, 'grad_norm': 1.2646244764328003, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.8754940032958984, 'eval_runtime': 6.7442, 'eval_samples_per_second': 148.275, 'eval_steps_per_second': 9.341, 'epoch': 0.32}
{'loss': 0.879, 'grad_norm': 1.2439699172973633, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.8681321740150452, 'eval_runtime': 6.7265, 'eval_samples_per_second': 148.665, 'eval_steps_per_second': 9.366, 'epoch': 0.36}
{'loss': 0.8658, 'grad_norm': 1.1185086965560913, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.8724090456962585, 'eval_runtime': 6.7235, 'eval_samples_per_second': 148.732, 'eval_steps_per_second': 9.37, 'epoch': 0.4}
{'loss': 0.8655, 'grad_norm': 1.2242348194122314, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.8722414374351501, 'eval_runtime': 6.713, 'eval_samples_per_second': 148.965, 'eval_steps_per_second': 9.385, 'epoch': 0.44}
{'loss': 0.844, 'grad_norm': 1.2979347705841064, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.8733641505241394, 'eval_runtime': 6.7033, 'eval_samples_per_second': 149.179, 'eval_steps_per_second': 9.398, 'epoch': 0.48}
{'loss': 0.8506, 'grad_norm': 1.1797887086868286, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.873048722743988, 'eval_runtime': 6.7186, 'eval_samples_per_second': 148.84, 'eval_steps_per_second': 9.377, 'epoch': 0.52}
{'loss': 0.831, 'grad_norm': 1.22090744972229, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.8721874952316284, 'eval_runtime': 6.7168, 'eval_samples_per_second': 148.879, 'eval_steps_per_second': 9.379, 'epoch': 0.56}
{'loss': 0.8643, 'grad_norm': 1.1576552391052246, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.876042366027832, 'eval_runtime': 6.7468, 'eval_samples_per_second': 148.217, 'eval_steps_per_second': 9.338, 'epoch': 0.6}
{'loss': 0.8401, 'grad_norm': 1.2511643171310425, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.8743947744369507, 'eval_runtime': 6.7655, 'eval_samples_per_second': 147.81, 'eval_steps_per_second': 9.312, 'epoch': 0.65}
{'loss': 0.8238, 'grad_norm': 1.1461235284805298, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.8822213411331177, 'eval_runtime': 6.7388, 'eval_samples_per_second': 148.393, 'eval_steps_per_second': 9.349, 'epoch': 0.69}
{'loss': 0.8432, 'grad_norm': 1.2936668395996094, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.8767036199569702, 'eval_runtime': 6.7236, 'eval_samples_per_second': 148.73, 'eval_steps_per_second': 9.37, 'epoch': 0.73}
{'loss': 0.8174, 'grad_norm': 1.4213907718658447, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.8839505910873413, 'eval_runtime': 6.7231, 'eval_samples_per_second': 148.741, 'eval_steps_per_second': 9.371, 'epoch': 0.77}
{'loss': 0.8164, 'grad_norm': 1.373589277267456, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.8802760243415833, 'eval_runtime': 6.7405, 'eval_samples_per_second': 148.356, 'eval_steps_per_second': 9.346, 'epoch': 0.81}
{'loss': 0.8487, 'grad_norm': 1.5685744285583496, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.8755612969398499, 'eval_runtime': 6.7029, 'eval_samples_per_second': 149.189, 'eval_steps_per_second': 9.399, 'epoch': 0.85}
{'loss': 0.7995, 'grad_norm': 1.2484854459762573, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.874147891998291, 'eval_runtime': 6.7065, 'eval_samples_per_second': 149.108, 'eval_steps_per_second': 9.394, 'epoch': 0.89}
{'loss': 0.8052, 'grad_norm': 1.3023655414581299, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.8766499161720276, 'eval_runtime': 6.714, 'eval_samples_per_second': 148.942, 'eval_steps_per_second': 9.383, 'epoch': 0.93}
{'loss': 0.8128, 'grad_norm': 1.011700987815857, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.8778097629547119, 'eval_runtime': 6.7724, 'eval_samples_per_second': 147.659, 'eval_steps_per_second': 9.302, 'epoch': 0.97}
{'train_runtime': 351.0776, 'train_samples_per_second': 28.253, 'train_steps_per_second': 1.766, 'train_loss': 0.920464700268161, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5000441074371338, 0.935445249080658, 0.893282413482666, 0.8963949084281921, 0.8927173018455505, 0.8835208415985107, 0.8758441209793091, 0.8754940032958984, 0.8681321740150452, 0.8724090456962585, 0.8722414374351501, 0.8733641505241394, 0.873048722743988, 0.8721874952316284, 0.876042366027832, 0.8743947744369507, 0.8822213411331177, 0.8767036199569702, 0.8839505910873413, 0.8802760243415833, 0.8755612969398499, 0.874147891998291, 0.8766499161720276, 0.8778097629547119], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.5000441074371338, 0.935445249080658, 0.893282413482666, 0.8963949084281921, 0.8927173018455505, 0.8835208415985107, 0.8758441209793091, 0.8754940032958984, 0.8681321740150452, 0.8724090456962585, 0.8722414374351501, 0.8733641505241394, 0.873048722743988, 0.8721874952316284, 0.876042366027832, 0.8743947744369507, 0.8822213411331177, 0.8767036199569702, 0.8839505910873413, 0.8802760243415833, 0.8755612969398499, 0.874147891998291, 0.8766499161720276, 0.8778097629547119]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760295391082764
current iteration best possible eval_loss (full train run):  -0.8778097629547119
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.2214 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5931183099746704, 0.48910677433013916, 0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.43514037132263184, 0.4408547878265381, 0.28891366720199585, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7229896187782288, 0.5257965922355652, 0.7089747190475464, 0.6451549530029297]  ‚Üí  acq = -0.8971020097870572
X = [0.8045198321342468, 0.5732041597366333, 0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5429293513298035, 0.6168438792228699, 0.6032367944717407, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.042392924427986145, 0.2894328832626343, 0.9503340721130371, 0.8085587620735168]  ‚Üí  acq = -0.9021321846456178
X = [0.18200689554214478, 0.36882972717285156, 0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.7035383582115173, 0.5326343774795532, 0.1735246777534485, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.2245919555425644, 0.050669074058532715, 0.8647359609603882, 0.040459275245666504]  ‚Üí  acq = -0.9021363667425819
X = [0.6313091516494751, 0.9872360825538635, 0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.09140855073928833, 0.0616917610168457, 0.16755545139312744, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.26512572169303894, 0.2105121612548828, 0.8316733837127686, 0.5374197959899902]  ‚Üí  acq = -0.9021321846749999
X = [0.04147899150848389, 0.4950082302093506, 0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6635695695877075, 0.1939259171485901, 0.9576328992843628, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7788231372833252, 0.13258826732635498, 0.11039420962333679, 0.1842997670173645]  ‚Üí  acq = -0.9021321846735553
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3582, dtype=torch.float64), 0, 0, 0, tensor(0.5975, dtype=torch.float64), tensor(0.0368, dtype=torch.float64), 0, 0, 0, 30, 1, 0, 0, 0, 1, 2, 0.1, 40.801480682362154, 0]
normalized proposed parameters for next round by BO: [tensor(0.3582, dtype=torch.float64), tensor(3.6921e-18, dtype=torch.float64), tensor(1.2130e-18, dtype=torch.float64), tensor(0.0075, dtype=torch.float64), tensor(0.5975, dtype=torch.float64), tensor(0.0368, dtype=torch.float64), tensor(2.6205e-18, dtype=torch.float64), tensor(6.6862e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9365, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8500, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.358
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.598
  truthfulqa_gen: 0.037
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (40.801480682362154,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  40.801480682362154
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,597,440 || all params: 8,031,858,688 || trainable%: 0.0199
length of training data:  9924
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6627, 'grad_norm': 5.12481164932251, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9126211404800415, 'eval_runtime': 7.3366, 'eval_samples_per_second': 136.303, 'eval_steps_per_second': 8.587, 'epoch': 0.04}
{'loss': 1.4956, 'grad_norm': 1.7558543682098389, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.566805362701416, 'eval_runtime': 6.7718, 'eval_samples_per_second': 147.671, 'eval_steps_per_second': 9.303, 'epoch': 0.08}
{'loss': 1.1814, 'grad_norm': 1.894348382949829, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.476251244544983, 'eval_runtime': 6.7859, 'eval_samples_per_second': 147.364, 'eval_steps_per_second': 9.284, 'epoch': 0.12}
{'loss': 0.9962, 'grad_norm': 1.4554694890975952, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.42611825466156, 'eval_runtime': 6.7833, 'eval_samples_per_second': 147.42, 'eval_steps_per_second': 9.287, 'epoch': 0.16}
{'loss': 0.9297, 'grad_norm': 1.3722856044769287, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.464231014251709, 'eval_runtime': 6.7964, 'eval_samples_per_second': 147.138, 'eval_steps_per_second': 9.27, 'epoch': 0.2}
{'loss': 0.9215, 'grad_norm': 1.0897663831710815, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.4964202642440796, 'eval_runtime': 6.8069, 'eval_samples_per_second': 146.911, 'eval_steps_per_second': 9.255, 'epoch': 0.24}
{'loss': 0.9227, 'grad_norm': 1.0832983255386353, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.5106641054153442, 'eval_runtime': 6.7881, 'eval_samples_per_second': 147.317, 'eval_steps_per_second': 9.281, 'epoch': 0.28}
{'loss': 0.9019, 'grad_norm': 0.9643756151199341, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.464331865310669, 'eval_runtime': 6.7922, 'eval_samples_per_second': 147.228, 'eval_steps_per_second': 9.275, 'epoch': 0.32}
{'loss': 0.9071, 'grad_norm': 1.1151448488235474, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.532253623008728, 'eval_runtime': 6.7843, 'eval_samples_per_second': 147.399, 'eval_steps_per_second': 9.286, 'epoch': 0.36}
{'loss': 0.897, 'grad_norm': 1.1774030923843384, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.51189386844635, 'eval_runtime': 6.7781, 'eval_samples_per_second': 147.534, 'eval_steps_per_second': 9.295, 'epoch': 0.4}
{'loss': 0.9102, 'grad_norm': 1.1527621746063232, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.5388495922088623, 'eval_runtime': 6.7753, 'eval_samples_per_second': 147.595, 'eval_steps_per_second': 9.299, 'epoch': 0.44}
{'loss': 0.9282, 'grad_norm': 1.3177376985549927, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.5617417097091675, 'eval_runtime': 6.7797, 'eval_samples_per_second': 147.5, 'eval_steps_per_second': 9.292, 'epoch': 0.48}
{'loss': 0.8781, 'grad_norm': 1.2905746698379517, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.5407202243804932, 'eval_runtime': 6.7919, 'eval_samples_per_second': 147.235, 'eval_steps_per_second': 9.276, 'epoch': 0.52}
{'loss': 0.9002, 'grad_norm': 1.147979497909546, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.4908242225646973, 'eval_runtime': 6.7759, 'eval_samples_per_second': 147.581, 'eval_steps_per_second': 9.298, 'epoch': 0.56}
{'loss': 0.8844, 'grad_norm': 1.1995086669921875, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.4813674688339233, 'eval_runtime': 6.7801, 'eval_samples_per_second': 147.491, 'eval_steps_per_second': 9.292, 'epoch': 0.6}
{'loss': 0.8979, 'grad_norm': 1.1431151628494263, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.5120019912719727, 'eval_runtime': 6.7697, 'eval_samples_per_second': 147.717, 'eval_steps_per_second': 9.306, 'epoch': 0.64}
{'loss': 0.8831, 'grad_norm': 1.061496615409851, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.5377238988876343, 'eval_runtime': 6.7535, 'eval_samples_per_second': 148.071, 'eval_steps_per_second': 9.328, 'epoch': 0.68}
{'loss': 0.89, 'grad_norm': 1.2300983667373657, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.565982699394226, 'eval_runtime': 6.7459, 'eval_samples_per_second': 148.237, 'eval_steps_per_second': 9.339, 'epoch': 0.72}
{'loss': 0.898, 'grad_norm': 1.2474393844604492, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.5743869543075562, 'eval_runtime': 6.749, 'eval_samples_per_second': 148.17, 'eval_steps_per_second': 9.335, 'epoch': 0.76}
{'loss': 0.8782, 'grad_norm': 1.2332017421722412, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.53781259059906, 'eval_runtime': 6.7397, 'eval_samples_per_second': 148.375, 'eval_steps_per_second': 9.348, 'epoch': 0.81}
{'loss': 0.8817, 'grad_norm': 1.085602045059204, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.5510743856430054, 'eval_runtime': 6.741, 'eval_samples_per_second': 148.345, 'eval_steps_per_second': 9.346, 'epoch': 0.85}
{'loss': 0.8758, 'grad_norm': 1.3329585790634155, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.5637484788894653, 'eval_runtime': 6.7493, 'eval_samples_per_second': 148.163, 'eval_steps_per_second': 9.334, 'epoch': 0.89}
{'loss': 0.8988, 'grad_norm': 1.1675163507461548, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.557740569114685, 'eval_runtime': 6.7508, 'eval_samples_per_second': 148.131, 'eval_steps_per_second': 9.332, 'epoch': 0.93}
{'loss': 0.8762, 'grad_norm': 1.2030662298202515, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.5629523992538452, 'eval_runtime': 6.7554, 'eval_samples_per_second': 148.031, 'eval_steps_per_second': 9.326, 'epoch': 0.97}
{'train_runtime': 270.9736, 'train_samples_per_second': 36.623, 'train_steps_per_second': 2.292, 'train_loss': 1.048373426600163, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9126211404800415, 1.566805362701416, 1.476251244544983, 1.42611825466156, 1.464231014251709, 1.4964202642440796, 1.5106641054153442, 1.464331865310669, 1.532253623008728, 1.51189386844635, 1.5388495922088623, 1.5617417097091675, 1.5407202243804932, 1.4908242225646973, 1.4813674688339233, 1.5120019912719727, 1.5377238988876343, 1.565982699394226, 1.5743869543075562, 1.53781259059906, 1.5510743856430054, 1.5637484788894653, 1.557740569114685, 1.5629523992538452], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.9126211404800415, 1.566805362701416, 1.476251244544983, 1.42611825466156, 1.464231014251709, 1.4964202642440796, 1.5106641054153442, 1.464331865310669, 1.532253623008728, 1.51189386844635, 1.5388495922088623, 1.5617417097091675, 1.5407202243804932, 1.4908242225646973, 1.4813674688339233, 1.5120019912719727, 1.5377238988876343, 1.565982699394226, 1.5743869543075562, 1.53781259059906, 1.5510743856430054, 1.5637484788894653, 1.557740569114685, 1.5629523992538452]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.07602858543396
current iteration best possible eval_loss (full train run):  -1.5629523992538452
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7488 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.04070591926574707, 0.2670907974243164, 0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.11440835893154144, 0.8343492150306702, 0.6285832524299622, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5520853400230408, 0.8785960674285889, 0.7897642850875854, 0.10385686159133911]  ‚Üí  acq = -0.9326601390397884
X = [0.1669445037841797, 0.07206535339355469, 0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.29802340269088745, 0.02810537815093994, 0.37732040882110596, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4432501196861267, 0.8628382086753845, 0.2149072289466858, 0.16291123628616333]  ‚Üí  acq = -0.9326351210284836
X = [0.5642975568771362, 0.34905433654785156, 0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.45446842908859253, 0.9301089644432068, 0.8571122884750366, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9834365248680115, 0.5003925561904907, 0.9843506813049316, 0.11054939031600952]  ‚Üí  acq = -0.9326340328742925
X = [0.16956406831741333, 0.7232905030250549, 0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.11824709177017212, 0.728330671787262, 0.602367103099823, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4289284646511078, 0.0390055775642395, 0.09640960395336151, 0.9996876120567322]  ‚Üí  acq = -0.9326340328767909
X = [0.7488081455230713, 0.8432244062423706, 0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.5128886699676514, 0.7522366642951965, 0.6226845979690552, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.46897920966148376, 0.9671209454536438, 0.12299968302249908, 0.006412029266357422]  ‚Üí  acq = -0.9326341545936554
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.5442, dtype=torch.float64), 0, tensor(0.0597, dtype=torch.float64), 0, tensor(0.2246, dtype=torch.float64), tensor(0.1617, dtype=torch.float64), 25, 0, 0, 0, 1, 1, 2, 2.0803238346732112e-20, 36.91866136534859, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0060, dtype=torch.float64), tensor(0.5442, dtype=torch.float64), tensor(0.0037, dtype=torch.float64), tensor(0.0597, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2246, dtype=torch.float64), tensor(0.1617, dtype=torch.float64), tensor(0.7815, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(2.0803e-19, dtype=torch.float64), tensor(0.7691, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.544
  triviaqa: 0
  truthfulqa_gen: 0.06
  wikitext: 0
  mmlu: 0.225
  arc_challenge: 0.162

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (2.0803238346732112e-20,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (36.91866136534859,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  2.0803238346732112e-20
lora alpha:  36.91866136534859
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,843,200 || all params: 8,032,104,448 || trainable%: 0.0229
length of training data:  9901
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2086, 'grad_norm': 9.260223388671875, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5538299083709717, 'eval_runtime': 6.8281, 'eval_samples_per_second': 146.454, 'eval_steps_per_second': 9.227, 'epoch': 0.04}
{'loss': 1.2246, 'grad_norm': 3.164695978164673, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0259511470794678, 'eval_runtime': 6.8109, 'eval_samples_per_second': 146.824, 'eval_steps_per_second': 9.25, 'epoch': 0.08}
{'loss': 1.1453, 'grad_norm': 3.147122383117676, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 0.9237832427024841, 'eval_runtime': 6.827, 'eval_samples_per_second': 146.478, 'eval_steps_per_second': 9.228, 'epoch': 0.12}
{'loss': 1.0489, 'grad_norm': 2.3893849849700928, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 0.9075445532798767, 'eval_runtime': 6.8855, 'eval_samples_per_second': 145.233, 'eval_steps_per_second': 9.15, 'epoch': 0.16}
{'loss': 0.9818, 'grad_norm': 1.5903441905975342, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 0.8960616588592529, 'eval_runtime': 6.9381, 'eval_samples_per_second': 144.133, 'eval_steps_per_second': 9.08, 'epoch': 0.2}
{'loss': 1.014, 'grad_norm': 1.6537165641784668, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 0.8845174312591553, 'eval_runtime': 6.9714, 'eval_samples_per_second': 143.443, 'eval_steps_per_second': 9.037, 'epoch': 0.24}
{'loss': 0.9624, 'grad_norm': 2.3272430896759033, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 0.8866444230079651, 'eval_runtime': 6.9685, 'eval_samples_per_second': 143.502, 'eval_steps_per_second': 9.041, 'epoch': 0.28}
{'loss': 0.9356, 'grad_norm': 1.5319427251815796, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 0.8872806429862976, 'eval_runtime': 6.9506, 'eval_samples_per_second': 143.873, 'eval_steps_per_second': 9.064, 'epoch': 0.32}
{'loss': 0.9409, 'grad_norm': 1.9157283306121826, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 0.8790034651756287, 'eval_runtime': 6.9652, 'eval_samples_per_second': 143.571, 'eval_steps_per_second': 9.045, 'epoch': 0.36}
{'loss': 0.9452, 'grad_norm': 1.741811990737915, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 0.8798019886016846, 'eval_runtime': 6.9587, 'eval_samples_per_second': 143.706, 'eval_steps_per_second': 9.053, 'epoch': 0.4}
{'loss': 0.9049, 'grad_norm': 6.885422706604004, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 0.8749842643737793, 'eval_runtime': 6.9676, 'eval_samples_per_second': 143.521, 'eval_steps_per_second': 9.042, 'epoch': 0.44}
{'loss': 0.8943, 'grad_norm': 1.5573511123657227, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 0.8761942386627197, 'eval_runtime': 6.9827, 'eval_samples_per_second': 143.211, 'eval_steps_per_second': 9.022, 'epoch': 0.48}
{'loss': 0.9064, 'grad_norm': 1.5471091270446777, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 0.884472131729126, 'eval_runtime': 6.9734, 'eval_samples_per_second': 143.402, 'eval_steps_per_second': 9.034, 'epoch': 0.53}
{'loss': 0.957, 'grad_norm': 1.5040416717529297, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 0.8732339143753052, 'eval_runtime': 6.964, 'eval_samples_per_second': 143.595, 'eval_steps_per_second': 9.046, 'epoch': 0.57}
{'loss': 0.9281, 'grad_norm': 1.4057172536849976, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 0.8713146448135376, 'eval_runtime': 6.9536, 'eval_samples_per_second': 143.811, 'eval_steps_per_second': 9.06, 'epoch': 0.61}
{'loss': 0.927, 'grad_norm': 1.5047166347503662, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 0.873956561088562, 'eval_runtime': 6.9789, 'eval_samples_per_second': 143.289, 'eval_steps_per_second': 9.027, 'epoch': 0.65}
{'loss': 0.9146, 'grad_norm': 1.7664235830307007, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 0.8714959025382996, 'eval_runtime': 6.9671, 'eval_samples_per_second': 143.532, 'eval_steps_per_second': 9.043, 'epoch': 0.69}
{'loss': 0.9271, 'grad_norm': 2.051128625869751, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 0.8773059248924255, 'eval_runtime': 6.9582, 'eval_samples_per_second': 143.715, 'eval_steps_per_second': 9.054, 'epoch': 0.73}
{'loss': 0.8499, 'grad_norm': 1.4221378564834595, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 0.8824435472488403, 'eval_runtime': 6.9524, 'eval_samples_per_second': 143.835, 'eval_steps_per_second': 9.062, 'epoch': 0.77}
{'loss': 0.9089, 'grad_norm': 1.5547131299972534, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 0.8838468194007874, 'eval_runtime': 6.9634, 'eval_samples_per_second': 143.608, 'eval_steps_per_second': 9.047, 'epoch': 0.81}
{'loss': 0.9255, 'grad_norm': 1.8142036199569702, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 0.8907850384712219, 'eval_runtime': 6.9646, 'eval_samples_per_second': 143.583, 'eval_steps_per_second': 9.046, 'epoch': 0.85}
{'loss': 0.927, 'grad_norm': 1.43417227268219, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 0.8838936686515808, 'eval_runtime': 6.9819, 'eval_samples_per_second': 143.227, 'eval_steps_per_second': 9.023, 'epoch': 0.89}
{'loss': 0.8999, 'grad_norm': 1.5704829692840576, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 0.8880544304847717, 'eval_runtime': 6.9653, 'eval_samples_per_second': 143.57, 'eval_steps_per_second': 9.045, 'epoch': 0.93}
{'loss': 0.9273, 'grad_norm': 1.5496842861175537, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 0.885058581829071, 'eval_runtime': 6.9679, 'eval_samples_per_second': 143.515, 'eval_steps_per_second': 9.041, 'epoch': 0.97}
{'train_runtime': 344.386, 'train_samples_per_second': 28.75, 'train_steps_per_second': 1.797, 'train_loss': 1.0444239953800627, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5538299083709717, 1.0259511470794678, 0.9237832427024841, 0.9075445532798767, 0.8960616588592529, 0.8845174312591553, 0.8866444230079651, 0.8872806429862976, 0.8790034651756287, 0.8798019886016846, 0.8749842643737793, 0.8761942386627197, 0.884472131729126, 0.8732339143753052, 0.8713146448135376, 0.873956561088562, 0.8714959025382996, 0.8773059248924255, 0.8824435472488403, 0.8838468194007874, 0.8907850384712219, 0.8838936686515808, 0.8880544304847717, 0.885058581829071], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.5538299083709717, 1.0259511470794678, 0.9237832427024841, 0.9075445532798767, 0.8960616588592529, 0.8845174312591553, 0.8866444230079651, 0.8872806429862976, 0.8790034651756287, 0.8798019886016846, 0.8749842643737793, 0.8761942386627197, 0.884472131729126, 0.8732339143753052, 0.8713146448135376, 0.873956561088562, 0.8714959025382996, 0.8773059248924255, 0.8824435472488403, 0.8838468194007874, 0.8907850384712219, 0.8838936686515808, 0.8880544304847717, 0.885058581829071]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760300159454346
current iteration best possible eval_loss (full train run):  -0.885058581829071
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9622 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5662546753883362, 0.8651972413063049, 0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6881975531578064, 0.07535994052886963, 0.38411790132522583, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8597249984741211, 0.2481454610824585, 0.4851251542568207, 0.9228593707084656]  ‚Üí  acq = -0.9528281443785421
X = [0.36866605281829834, 0.9746328592300415, 0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.10368144512176514, 0.6828930377960205, 0.015926599502563477, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8190843462944031, 0.06521856784820557, 0.7280534505844116, 0.8793015480041504]  ‚Üí  acq = -0.952828129331889
X = [0.43594950437545776, 0.018241524696350098, 0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.23293758928775787, 0.9870834350585938, 0.3120540976524353, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.0616411417722702, 0.7074243426322937, 0.4130009710788727, 0.3545602560043335]  ‚Üí  acq = -0.9525025502103388
X = [0.21759581565856934, 0.7204521298408508, 0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9501417279243469, 0.84186190366745, 0.104533851146698, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.23558637499809265, 0.2996382713317871, 0.04963042959570885, 0.6170431971549988]  ‚Üí  acq = -0.9528281293342262
X = [0.2948945164680481, 0.971221923828125, 0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9141191244125366, 0.7222160696983337, 0.033279359340667725, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.831488847732544, 0.14262902736663818, 0.3206331431865692, 0.8077713251113892]  ‚Üí  acq = -0.9528281293338763
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0827, dtype=torch.float64), 0, tensor(0.0654, dtype=torch.float64), 0, tensor(0.0132, dtype=torch.float64), tensor(0.0481, dtype=torch.float64), 0, tensor(0.6121, dtype=torch.float64), tensor(0.1785, dtype=torch.float64), 27, 1, 0, 0, 0, 1, 2, 0.1, 37.93526160084096, 0]
normalized proposed parameters for next round by BO: [tensor(0.0827, dtype=torch.float64), tensor(9.5131e-19, dtype=torch.float64), tensor(0.0654, dtype=torch.float64), tensor(2.9256e-18, dtype=torch.float64), tensor(0.0132, dtype=torch.float64), tensor(0.0481, dtype=torch.float64), tensor(5.4791e-18, dtype=torch.float64), tensor(0.6121, dtype=torch.float64), tensor(0.1785, dtype=torch.float64), tensor(0.8341, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7903, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.083
  gsm8k: 0
  rowan_hellaswag: 0.065
  sciq: 0
  triviaqa: 0.013
  truthfulqa_gen: 0.048
  wikitext: 0
  mmlu: 0.612
  arc_challenge: 0.178

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (37.93526160084096,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  37.93526160084096
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,437,696 || all params: 8,031,698,944 || trainable%: 0.0179
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0195, 'grad_norm': 2.4355123043060303, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7868140935897827, 'eval_runtime': 6.6524, 'eval_samples_per_second': 150.322, 'eval_steps_per_second': 9.47, 'epoch': 0.04}
{'loss': 1.6618, 'grad_norm': 1.5190383195877075, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1185946464538574, 'eval_runtime': 6.6712, 'eval_samples_per_second': 149.898, 'eval_steps_per_second': 9.444, 'epoch': 0.08}
{'loss': 1.4081, 'grad_norm': 1.258721113204956, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0198477506637573, 'eval_runtime': 6.7152, 'eval_samples_per_second': 148.916, 'eval_steps_per_second': 9.382, 'epoch': 0.12}
{'loss': 1.2666, 'grad_norm': 1.0922138690948486, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.918765664100647, 'eval_runtime': 6.7201, 'eval_samples_per_second': 148.806, 'eval_steps_per_second': 9.375, 'epoch': 0.16}
{'loss': 1.2286, 'grad_norm': 1.1272437572479248, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8900645971298218, 'eval_runtime': 6.7378, 'eval_samples_per_second': 148.416, 'eval_steps_per_second': 9.35, 'epoch': 0.2}
{'loss': 1.2459, 'grad_norm': 0.9775094389915466, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8920671343803406, 'eval_runtime': 6.7314, 'eval_samples_per_second': 148.558, 'eval_steps_per_second': 9.359, 'epoch': 0.24}
{'loss': 1.1915, 'grad_norm': 1.0895992517471313, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8774616122245789, 'eval_runtime': 6.7291, 'eval_samples_per_second': 148.608, 'eval_steps_per_second': 9.362, 'epoch': 0.28}
{'loss': 1.2737, 'grad_norm': 1.0989649295806885, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8812842965126038, 'eval_runtime': 6.7307, 'eval_samples_per_second': 148.574, 'eval_steps_per_second': 9.36, 'epoch': 0.32}
{'loss': 1.2086, 'grad_norm': 1.0834850072860718, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8771874904632568, 'eval_runtime': 6.7319, 'eval_samples_per_second': 148.547, 'eval_steps_per_second': 9.358, 'epoch': 0.36}
{'loss': 1.1777, 'grad_norm': 1.3298107385635376, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8716074228286743, 'eval_runtime': 6.7348, 'eval_samples_per_second': 148.482, 'eval_steps_per_second': 9.354, 'epoch': 0.4}
{'loss': 1.1404, 'grad_norm': 1.1553605794906616, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8710954785346985, 'eval_runtime': 6.7382, 'eval_samples_per_second': 148.408, 'eval_steps_per_second': 9.35, 'epoch': 0.44}
{'loss': 1.2099, 'grad_norm': 1.0859583616256714, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8755553960800171, 'eval_runtime': 6.7358, 'eval_samples_per_second': 148.459, 'eval_steps_per_second': 9.353, 'epoch': 0.48}
{'loss': 1.1854, 'grad_norm': 1.0218344926834106, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8714122176170349, 'eval_runtime': 6.7297, 'eval_samples_per_second': 148.594, 'eval_steps_per_second': 9.361, 'epoch': 0.52}
{'loss': 1.1966, 'grad_norm': 1.1578099727630615, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8702117800712585, 'eval_runtime': 6.7153, 'eval_samples_per_second': 148.914, 'eval_steps_per_second': 9.382, 'epoch': 0.56}
{'loss': 1.1438, 'grad_norm': 1.1410804986953735, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8746553659439087, 'eval_runtime': 6.7063, 'eval_samples_per_second': 149.113, 'eval_steps_per_second': 9.394, 'epoch': 0.6}
{'loss': 1.1203, 'grad_norm': 1.0154805183410645, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8754042387008667, 'eval_runtime': 6.7045, 'eval_samples_per_second': 149.153, 'eval_steps_per_second': 9.397, 'epoch': 0.64}
{'loss': 1.158, 'grad_norm': 1.2513177394866943, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8721349835395813, 'eval_runtime': 6.7067, 'eval_samples_per_second': 149.105, 'eval_steps_per_second': 9.394, 'epoch': 0.68}
{'loss': 1.1587, 'grad_norm': 1.0202080011367798, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8746885061264038, 'eval_runtime': 6.7106, 'eval_samples_per_second': 149.018, 'eval_steps_per_second': 9.388, 'epoch': 0.72}
{'loss': 1.1899, 'grad_norm': 1.1228386163711548, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8743687272071838, 'eval_runtime': 6.7071, 'eval_samples_per_second': 149.095, 'eval_steps_per_second': 9.393, 'epoch': 0.76}
{'loss': 1.1832, 'grad_norm': 1.0917022228240967, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8782199621200562, 'eval_runtime': 6.7171, 'eval_samples_per_second': 148.874, 'eval_steps_per_second': 9.379, 'epoch': 0.8}
{'loss': 1.1749, 'grad_norm': 0.945590078830719, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8748637437820435, 'eval_runtime': 6.7004, 'eval_samples_per_second': 149.245, 'eval_steps_per_second': 9.402, 'epoch': 0.84}
{'loss': 1.1766, 'grad_norm': 1.1460597515106201, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8750706315040588, 'eval_runtime': 6.754, 'eval_samples_per_second': 148.061, 'eval_steps_per_second': 9.328, 'epoch': 0.88}
{'loss': 1.1624, 'grad_norm': 1.072408676147461, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8763031959533691, 'eval_runtime': 6.7483, 'eval_samples_per_second': 148.185, 'eval_steps_per_second': 9.336, 'epoch': 0.92}
{'loss': 1.1634, 'grad_norm': 1.1561942100524902, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8774847984313965, 'eval_runtime': 6.7568, 'eval_samples_per_second': 147.999, 'eval_steps_per_second': 9.324, 'epoch': 0.96}
{'loss': 1.1322, 'grad_norm': 1.4170610904693604, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8790983557701111, 'eval_runtime': 6.7291, 'eval_samples_per_second': 148.609, 'eval_steps_per_second': 9.362, 'epoch': 1.0}
{'train_runtime': 365.9972, 'train_samples_per_second': 27.314, 'train_steps_per_second': 1.708, 'train_loss': 1.2871105133056642, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7868140935897827, 1.1185946464538574, 1.0198477506637573, 0.918765664100647, 0.8900645971298218, 0.8920671343803406, 0.8774616122245789, 0.8812842965126038, 0.8771874904632568, 0.8716074228286743, 0.8710954785346985, 0.8755553960800171, 0.8714122176170349, 0.8702117800712585, 0.8746553659439087, 0.8754042387008667, 0.8721349835395813, 0.8746885061264038, 0.8743687272071838, 0.8782199621200562, 0.8748637437820435, 0.8750706315040588, 0.8763031959533691, 0.8774847984313965, 0.8790983557701111], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7868140935897827, 1.1185946464538574, 1.0198477506637573, 0.918765664100647, 0.8900645971298218, 0.8920671343803406, 0.8774616122245789, 0.8812842965126038, 0.8771874904632568, 0.8716074228286743, 0.8710954785346985, 0.8755553960800171, 0.8714122176170349, 0.8702117800712585, 0.8746553659439087, 0.8754042387008667, 0.8721349835395813, 0.8746885061264038, 0.8743687272071838, 0.8782199621200562, 0.8748637437820435, 0.8750706315040588, 0.8763031959533691, 0.8774847984313965, 0.8790983557701111]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.076029896736145
current iteration best possible eval_loss (full train run):  -0.8790983557701111
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4129 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.9538667787560775
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.9534770286960698
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.953477028695961
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.9534804197496525
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.9534865180755797
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1403, dtype=torch.float64), tensor(0.0224, dtype=torch.float64), tensor(0.0785, dtype=torch.float64), tensor(0.4259, dtype=torch.float64), tensor(0.0102, dtype=torch.float64), 0, tensor(0.2563, dtype=torch.float64), tensor(0.0663, dtype=torch.float64), 25, 1, 1, 1, 1, 0, 128, 0.08907714119048751, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.1403, dtype=torch.float64), tensor(0.0224, dtype=torch.float64), tensor(0.0785, dtype=torch.float64), tensor(0.4259, dtype=torch.float64), tensor(0.0102, dtype=torch.float64), tensor(1.5546e-19, dtype=torch.float64), tensor(0.2563, dtype=torch.float64), tensor(0.0663, dtype=torch.float64), tensor(0.7657, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8908, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.14
  rowan_hellaswag: 0.022
  sciq: 0.078
  triviaqa: 0.426
  truthfulqa_gen: 0.01
  wikitext: 0
  mmlu: 0.256
  arc_challenge: 0.066

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.08907714119048751,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.08907714119048751
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 160,563,200 || all params: 8,190,824,448 || trainable%: 1.9603
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8759, 'grad_norm': 0.29777956008911133, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.909883737564087, 'eval_runtime': 7.1185, 'eval_samples_per_second': 140.478, 'eval_steps_per_second': 8.85, 'epoch': 0.04}
{'loss': 2.1574, 'grad_norm': 0.42884016036987305, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7973285913467407, 'eval_runtime': 7.114, 'eval_samples_per_second': 140.569, 'eval_steps_per_second': 8.856, 'epoch': 0.08}
{'loss': 1.5831, 'grad_norm': 0.12166091054677963, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2994590997695923, 'eval_runtime': 7.1546, 'eval_samples_per_second': 139.771, 'eval_steps_per_second': 8.806, 'epoch': 0.12}
{'loss': 1.4608, 'grad_norm': 0.14885298907756805, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1369234323501587, 'eval_runtime': 7.183, 'eval_samples_per_second': 139.217, 'eval_steps_per_second': 8.771, 'epoch': 0.16}
{'loss': 1.4044, 'grad_norm': 0.11649730056524277, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0817211866378784, 'eval_runtime': 7.1967, 'eval_samples_per_second': 138.952, 'eval_steps_per_second': 8.754, 'epoch': 0.2}
{'loss': 1.2834, 'grad_norm': 0.05951299890875816, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0349262952804565, 'eval_runtime': 7.1823, 'eval_samples_per_second': 139.232, 'eval_steps_per_second': 8.772, 'epoch': 0.24}
{'loss': 1.2225, 'grad_norm': 0.07164435088634491, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9971651434898376, 'eval_runtime': 7.1892, 'eval_samples_per_second': 139.097, 'eval_steps_per_second': 8.763, 'epoch': 0.28}
{'loss': 1.1705, 'grad_norm': 0.0691700354218483, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9711861610412598, 'eval_runtime': 7.2008, 'eval_samples_per_second': 138.874, 'eval_steps_per_second': 8.749, 'epoch': 0.32}
{'loss': 1.2161, 'grad_norm': 0.07191725820302963, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9352174997329712, 'eval_runtime': 7.2245, 'eval_samples_per_second': 138.418, 'eval_steps_per_second': 8.72, 'epoch': 0.36}
{'loss': 1.1003, 'grad_norm': 0.06858699023723602, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9305214881896973, 'eval_runtime': 7.2188, 'eval_samples_per_second': 138.528, 'eval_steps_per_second': 8.727, 'epoch': 0.4}
{'loss': 1.165, 'grad_norm': 0.06365814805030823, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9260140657424927, 'eval_runtime': 7.205, 'eval_samples_per_second': 138.792, 'eval_steps_per_second': 8.744, 'epoch': 0.44}
{'loss': 1.0847, 'grad_norm': 0.055610742419958115, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9210091233253479, 'eval_runtime': 7.1996, 'eval_samples_per_second': 138.897, 'eval_steps_per_second': 8.751, 'epoch': 0.48}
{'loss': 1.1068, 'grad_norm': 0.05687817186117172, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9190527200698853, 'eval_runtime': 7.2158, 'eval_samples_per_second': 138.585, 'eval_steps_per_second': 8.731, 'epoch': 0.52}
{'loss': 1.1225, 'grad_norm': 0.05360911041498184, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9047142267227173, 'eval_runtime': 7.2482, 'eval_samples_per_second': 137.966, 'eval_steps_per_second': 8.692, 'epoch': 0.56}
{'loss': 1.1386, 'grad_norm': 0.078348308801651, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.914014995098114, 'eval_runtime': 7.2354, 'eval_samples_per_second': 138.209, 'eval_steps_per_second': 8.707, 'epoch': 0.6}
{'loss': 1.0376, 'grad_norm': 0.05910699814558029, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9080758094787598, 'eval_runtime': 7.2352, 'eval_samples_per_second': 138.213, 'eval_steps_per_second': 8.707, 'epoch': 0.64}
{'loss': 1.0924, 'grad_norm': 0.06357105821371078, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9051060676574707, 'eval_runtime': 7.2417, 'eval_samples_per_second': 138.089, 'eval_steps_per_second': 8.7, 'epoch': 0.68}
{'loss': 1.1074, 'grad_norm': 0.06237879395484924, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9004443883895874, 'eval_runtime': 7.2312, 'eval_samples_per_second': 138.29, 'eval_steps_per_second': 8.712, 'epoch': 0.72}
{'loss': 1.0797, 'grad_norm': 0.06512080132961273, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9045259952545166, 'eval_runtime': 7.2477, 'eval_samples_per_second': 137.975, 'eval_steps_per_second': 8.692, 'epoch': 0.76}
{'loss': 1.0831, 'grad_norm': 0.0606192871928215, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8996621370315552, 'eval_runtime': 7.2267, 'eval_samples_per_second': 138.376, 'eval_steps_per_second': 8.718, 'epoch': 0.8}
{'loss': 1.0973, 'grad_norm': 0.06267707049846649, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8957843780517578, 'eval_runtime': 7.1811, 'eval_samples_per_second': 139.254, 'eval_steps_per_second': 8.773, 'epoch': 0.84}
{'loss': 1.0874, 'grad_norm': 0.06030203029513359, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8963764905929565, 'eval_runtime': 7.1823, 'eval_samples_per_second': 139.231, 'eval_steps_per_second': 8.772, 'epoch': 0.88}
{'loss': 1.0465, 'grad_norm': 0.051340945065021515, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8946743011474609, 'eval_runtime': 7.1809, 'eval_samples_per_second': 139.259, 'eval_steps_per_second': 8.773, 'epoch': 0.92}
{'loss': 1.0733, 'grad_norm': 0.06585617363452911, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8917519450187683, 'eval_runtime': 7.1726, 'eval_samples_per_second': 139.419, 'eval_steps_per_second': 8.783, 'epoch': 0.96}
{'loss': 1.055, 'grad_norm': 0.05884728953242302, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8922914862632751, 'eval_runtime': 7.1599, 'eval_samples_per_second': 139.667, 'eval_steps_per_second': 8.799, 'epoch': 1.0}
{'train_runtime': 407.6889, 'train_samples_per_second': 24.521, 'train_steps_per_second': 1.533, 'train_loss': 1.3140707061767578, 'epoch': 1.0}
train_results:  {'eval_loss': [2.909883737564087, 1.7973285913467407, 1.2994590997695923, 1.1369234323501587, 1.0817211866378784, 1.0349262952804565, 0.9971651434898376, 0.9711861610412598, 0.9352174997329712, 0.9305214881896973, 0.9260140657424927, 0.9210091233253479, 0.9190527200698853, 0.9047142267227173, 0.914014995098114, 0.9080758094787598, 0.9051060676574707, 0.9004443883895874, 0.9045259952545166, 0.8996621370315552, 0.8957843780517578, 0.8963764905929565, 0.8946743011474609, 0.8917519450187683, 0.8922914862632751], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.909883737564087, 1.7973285913467407, 1.2994590997695923, 1.1369234323501587, 1.0817211866378784, 1.0349262952804565, 0.9971651434898376, 0.9711861610412598, 0.9352174997329712, 0.9305214881896973, 0.9260140657424927, 0.9210091233253479, 0.9190527200698853, 0.9047142267227173, 0.914014995098114, 0.9080758094787598, 0.9051060676574707, 0.9004443883895874, 0.9045259952545166, 0.8996621370315552, 0.8957843780517578, 0.8963764905929565, 0.8946743011474609, 0.8917519450187683, 0.8922914862632751]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.9401277303695679
current iteration best possible eval_loss (full train run):  -0.8922914862632751
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8703 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.9158367194327153
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.9133225801598719
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.9454392703934846
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.9171684032516751
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.9201827980093669
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3817, dtype=torch.float64), tensor(0.3004, dtype=torch.float64), 0, tensor(0.3179, dtype=torch.float64), 0, 0, 32, 0, 1, 1, 0, 1, 2, 0.1, 29.427911075013554, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.8604e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3817, dtype=torch.float64), tensor(0.3004, dtype=torch.float64), tensor(8.3086e-18, dtype=torch.float64), tensor(0.3179, dtype=torch.float64), tensor(8.6918e-18, dtype=torch.float64), tensor(4.3338e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6131, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.382
  triviaqa: 0.3
  truthfulqa_gen: 0
  wikitext: 0.318
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (29.427911075013554,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  29.427911075013554
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,686,976 || all params: 8,032,948,224 || trainable%: 0.0334
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2873, 'grad_norm': 4.029311180114746, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.3842592239379883, 'eval_runtime': 7.1379, 'eval_samples_per_second': 140.097, 'eval_steps_per_second': 8.826, 'epoch': 0.04}
{'loss': 1.6923, 'grad_norm': 2.8914682865142822, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.376329183578491, 'eval_runtime': 7.1703, 'eval_samples_per_second': 139.465, 'eval_steps_per_second': 8.786, 'epoch': 0.08}
{'loss': 1.6009, 'grad_norm': 2.6482717990875244, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.264561176300049, 'eval_runtime': 7.2346, 'eval_samples_per_second': 138.224, 'eval_steps_per_second': 8.708, 'epoch': 0.12}
{'loss': 1.4974, 'grad_norm': 2.2426393032073975, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.152268886566162, 'eval_runtime': 7.2471, 'eval_samples_per_second': 137.986, 'eval_steps_per_second': 8.693, 'epoch': 0.16}
{'loss': 1.3029, 'grad_norm': 2.101475477218628, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.2174463272094727, 'eval_runtime': 7.2639, 'eval_samples_per_second': 137.668, 'eval_steps_per_second': 8.673, 'epoch': 0.2}
{'loss': 1.3304, 'grad_norm': 2.118637800216675, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1740193367004395, 'eval_runtime': 7.2423, 'eval_samples_per_second': 138.078, 'eval_steps_per_second': 8.699, 'epoch': 0.24}
{'loss': 1.358, 'grad_norm': 2.3551909923553467, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.1427226066589355, 'eval_runtime': 7.2481, 'eval_samples_per_second': 137.968, 'eval_steps_per_second': 8.692, 'epoch': 0.28}
{'loss': 1.2707, 'grad_norm': 1.7501131296157837, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.1588919162750244, 'eval_runtime': 7.2534, 'eval_samples_per_second': 137.866, 'eval_steps_per_second': 8.686, 'epoch': 0.32}
{'loss': 1.2632, 'grad_norm': 2.2114458084106445, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.1544368267059326, 'eval_runtime': 7.2599, 'eval_samples_per_second': 137.743, 'eval_steps_per_second': 8.678, 'epoch': 0.36}
{'loss': 1.2399, 'grad_norm': 2.1779048442840576, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.166494607925415, 'eval_runtime': 7.2849, 'eval_samples_per_second': 137.27, 'eval_steps_per_second': 8.648, 'epoch': 0.4}
{'loss': 1.2924, 'grad_norm': 1.9131025075912476, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.15606951713562, 'eval_runtime': 7.2593, 'eval_samples_per_second': 137.754, 'eval_steps_per_second': 8.679, 'epoch': 0.44}
{'loss': 1.3889, 'grad_norm': 2.016000270843506, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.174731731414795, 'eval_runtime': 7.2543, 'eval_samples_per_second': 137.85, 'eval_steps_per_second': 8.685, 'epoch': 0.48}
{'loss': 1.1622, 'grad_norm': 1.8599032163619995, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.1515941619873047, 'eval_runtime': 7.2566, 'eval_samples_per_second': 137.806, 'eval_steps_per_second': 8.682, 'epoch': 0.52}
{'loss': 1.3036, 'grad_norm': 2.024181604385376, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.1905171871185303, 'eval_runtime': 7.1835, 'eval_samples_per_second': 139.209, 'eval_steps_per_second': 8.77, 'epoch': 0.56}
{'loss': 1.3782, 'grad_norm': 1.8190592527389526, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.2252275943756104, 'eval_runtime': 7.1935, 'eval_samples_per_second': 139.014, 'eval_steps_per_second': 8.758, 'epoch': 0.6}
{'loss': 1.1794, 'grad_norm': 1.7330400943756104, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.2295382022857666, 'eval_runtime': 7.1843, 'eval_samples_per_second': 139.192, 'eval_steps_per_second': 8.769, 'epoch': 0.64}
{'loss': 1.3051, 'grad_norm': 2.259504795074463, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2429912090301514, 'eval_runtime': 7.2036, 'eval_samples_per_second': 138.819, 'eval_steps_per_second': 8.746, 'epoch': 0.68}
{'loss': 1.2145, 'grad_norm': 1.6731770038604736, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.2263011932373047, 'eval_runtime': 7.1831, 'eval_samples_per_second': 139.215, 'eval_steps_per_second': 8.771, 'epoch': 0.72}
{'loss': 1.3108, 'grad_norm': 1.9458562135696411, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.2208573818206787, 'eval_runtime': 7.1856, 'eval_samples_per_second': 139.167, 'eval_steps_per_second': 8.768, 'epoch': 0.76}
{'loss': 1.2593, 'grad_norm': 2.057473659515381, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.230790138244629, 'eval_runtime': 7.1835, 'eval_samples_per_second': 139.208, 'eval_steps_per_second': 8.77, 'epoch': 0.8}
{'loss': 1.2568, 'grad_norm': 2.0194058418273926, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.2355005741119385, 'eval_runtime': 7.1739, 'eval_samples_per_second': 139.393, 'eval_steps_per_second': 8.782, 'epoch': 0.84}
{'loss': 1.2854, 'grad_norm': 1.8248671293258667, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.2328741550445557, 'eval_runtime': 7.1577, 'eval_samples_per_second': 139.711, 'eval_steps_per_second': 8.802, 'epoch': 0.88}
{'loss': 1.2456, 'grad_norm': 1.938232183456421, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.225304365158081, 'eval_runtime': 7.1751, 'eval_samples_per_second': 139.371, 'eval_steps_per_second': 8.78, 'epoch': 0.92}
{'loss': 1.2211, 'grad_norm': 1.7242270708084106, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.237264633178711, 'eval_runtime': 7.1612, 'eval_samples_per_second': 139.641, 'eval_steps_per_second': 8.797, 'epoch': 0.96}
{'loss': 1.2807, 'grad_norm': 2.2012906074523926, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.2360587120056152, 'eval_runtime': 7.1506, 'eval_samples_per_second': 139.848, 'eval_steps_per_second': 8.81, 'epoch': 1.0}
{'train_runtime': 350.6167, 'train_samples_per_second': 28.518, 'train_steps_per_second': 1.783, 'train_loss': 1.3970758331298827, 'epoch': 1.0}
train_results:  {'eval_loss': [2.3842592239379883, 2.376329183578491, 2.264561176300049, 2.152268886566162, 2.2174463272094727, 2.1740193367004395, 2.1427226066589355, 2.1588919162750244, 2.1544368267059326, 2.166494607925415, 2.15606951713562, 2.174731731414795, 2.1515941619873047, 2.1905171871185303, 2.2252275943756104, 2.2295382022857666, 2.2429912090301514, 2.2263011932373047, 2.2208573818206787, 2.230790138244629, 2.2355005741119385, 2.2328741550445557, 2.225304365158081, 2.237264633178711, 2.2360587120056152], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.3842592239379883, 2.376329183578491, 2.264561176300049, 2.152268886566162, 2.2174463272094727, 2.1740193367004395, 2.1427226066589355, 2.1588919162750244, 2.1544368267059326, 2.166494607925415, 2.15606951713562, 2.174731731414795, 2.1515941619873047, 2.1905171871185303, 2.2252275943756104, 2.2295382022857666, 2.2429912090301514, 2.2263011932373047, 2.2208573818206787, 2.230790138244629, 2.2355005741119385, 2.2328741550445557, 2.225304365158081, 2.237264633178711, 2.2360587120056152]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.076029658317566
current iteration best possible eval_loss (full train run):  -2.2360587120056152
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9568 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.9426792610976753
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.9527684300642558
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.952771014476103
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.9542161945932726
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.9758187978047336
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.2430, dtype=torch.float64), tensor(0.3752, dtype=torch.float64), 0, 0, tensor(0.3818, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 2, 0.1, 24.12140819364597, 1]
normalized proposed parameters for next round by BO: [tensor(2.2605e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2430, dtype=torch.float64), tensor(0.3752, dtype=torch.float64), tensor(4.9203e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3818, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5025, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.243
  truthfulqa_gen: 0.375
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.382

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (24.12140819364597,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  24.12140819364597
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,211,264 || all params: 8,033,472,512 || trainable%: 0.0400
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0661, 'grad_norm': 4.135115146636963, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2435579299926758, 'eval_runtime': 7.3849, 'eval_samples_per_second': 135.412, 'eval_steps_per_second': 8.531, 'epoch': 0.04}
{'loss': 1.1673, 'grad_norm': 1.5636860132217407, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0016942024230957, 'eval_runtime': 7.3987, 'eval_samples_per_second': 135.159, 'eval_steps_per_second': 8.515, 'epoch': 0.08}
{'loss': 0.9449, 'grad_norm': 1.501657247543335, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.896453857421875, 'eval_runtime': 7.4701, 'eval_samples_per_second': 133.866, 'eval_steps_per_second': 8.434, 'epoch': 0.12}
{'loss': 0.8228, 'grad_norm': 1.45048987865448, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8778297901153564, 'eval_runtime': 7.4419, 'eval_samples_per_second': 134.374, 'eval_steps_per_second': 8.466, 'epoch': 0.16}
{'loss': 0.7843, 'grad_norm': 1.40278959274292, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8818279504776001, 'eval_runtime': 7.5059, 'eval_samples_per_second': 133.228, 'eval_steps_per_second': 8.393, 'epoch': 0.2}
{'loss': 0.7574, 'grad_norm': 1.4452720880508423, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8927949070930481, 'eval_runtime': 7.5058, 'eval_samples_per_second': 133.231, 'eval_steps_per_second': 8.394, 'epoch': 0.24}
{'loss': 0.7253, 'grad_norm': 1.5870375633239746, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8853171467781067, 'eval_runtime': 7.5056, 'eval_samples_per_second': 133.233, 'eval_steps_per_second': 8.394, 'epoch': 0.28}
{'loss': 0.7179, 'grad_norm': 1.591766595840454, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8832451105117798, 'eval_runtime': 7.5005, 'eval_samples_per_second': 133.324, 'eval_steps_per_second': 8.399, 'epoch': 0.32}
{'loss': 0.6122, 'grad_norm': 1.5002799034118652, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.911746084690094, 'eval_runtime': 7.5083, 'eval_samples_per_second': 133.185, 'eval_steps_per_second': 8.391, 'epoch': 0.36}
{'loss': 0.643, 'grad_norm': 1.983701229095459, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9211934208869934, 'eval_runtime': 7.5143, 'eval_samples_per_second': 133.079, 'eval_steps_per_second': 8.384, 'epoch': 0.4}
{'loss': 0.6208, 'grad_norm': 1.4221912622451782, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9041714668273926, 'eval_runtime': 7.472, 'eval_samples_per_second': 133.832, 'eval_steps_per_second': 8.431, 'epoch': 0.44}
{'loss': 0.5689, 'grad_norm': 1.763777256011963, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9461884498596191, 'eval_runtime': 7.4504, 'eval_samples_per_second': 134.22, 'eval_steps_per_second': 8.456, 'epoch': 0.48}
{'loss': 0.5552, 'grad_norm': 1.872876763343811, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9409707188606262, 'eval_runtime': 7.4555, 'eval_samples_per_second': 134.129, 'eval_steps_per_second': 8.45, 'epoch': 0.52}
{'loss': 0.5405, 'grad_norm': 1.680761456489563, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9499263763427734, 'eval_runtime': 7.4477, 'eval_samples_per_second': 134.27, 'eval_steps_per_second': 8.459, 'epoch': 0.56}
{'loss': 0.527, 'grad_norm': 2.0661723613739014, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9795311093330383, 'eval_runtime': 7.4486, 'eval_samples_per_second': 134.253, 'eval_steps_per_second': 8.458, 'epoch': 0.6}
{'loss': 0.5169, 'grad_norm': 1.9168726205825806, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9460203647613525, 'eval_runtime': 7.4447, 'eval_samples_per_second': 134.324, 'eval_steps_per_second': 8.462, 'epoch': 0.64}
{'loss': 0.4772, 'grad_norm': 2.267512321472168, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9874317049980164, 'eval_runtime': 7.4447, 'eval_samples_per_second': 134.323, 'eval_steps_per_second': 8.462, 'epoch': 0.68}
{'loss': 0.5098, 'grad_norm': 1.9874333143234253, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9769600033760071, 'eval_runtime': 7.4379, 'eval_samples_per_second': 134.447, 'eval_steps_per_second': 8.47, 'epoch': 0.72}
{'loss': 0.4694, 'grad_norm': 2.168790102005005, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9940897226333618, 'eval_runtime': 7.416, 'eval_samples_per_second': 134.844, 'eval_steps_per_second': 8.495, 'epoch': 0.76}
{'loss': 0.4719, 'grad_norm': 2.4131808280944824, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9845134019851685, 'eval_runtime': 7.4108, 'eval_samples_per_second': 134.939, 'eval_steps_per_second': 8.501, 'epoch': 0.8}
{'loss': 0.4506, 'grad_norm': 2.2037270069122314, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0170460939407349, 'eval_runtime': 7.4274, 'eval_samples_per_second': 134.636, 'eval_steps_per_second': 8.482, 'epoch': 0.84}
{'loss': 0.4125, 'grad_norm': 2.7172493934631348, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.033760905265808, 'eval_runtime': 7.4468, 'eval_samples_per_second': 134.287, 'eval_steps_per_second': 8.46, 'epoch': 0.88}
{'loss': 0.4382, 'grad_norm': 2.1354169845581055, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0141328573226929, 'eval_runtime': 7.4601, 'eval_samples_per_second': 134.046, 'eval_steps_per_second': 8.445, 'epoch': 0.92}
{'loss': 0.4389, 'grad_norm': 2.2029078006744385, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0160465240478516, 'eval_runtime': 7.4579, 'eval_samples_per_second': 134.086, 'eval_steps_per_second': 8.447, 'epoch': 0.96}
{'loss': 0.3956, 'grad_norm': 2.5572335720062256, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0187774896621704, 'eval_runtime': 7.4525, 'eval_samples_per_second': 134.183, 'eval_steps_per_second': 8.454, 'epoch': 1.0}
{'train_runtime': 349.9205, 'train_samples_per_second': 28.575, 'train_steps_per_second': 1.786, 'train_loss': 0.7053804077148438, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2435579299926758, 1.0016942024230957, 0.896453857421875, 0.8778297901153564, 0.8818279504776001, 0.8927949070930481, 0.8853171467781067, 0.8832451105117798, 0.911746084690094, 0.9211934208869934, 0.9041714668273926, 0.9461884498596191, 0.9409707188606262, 0.9499263763427734, 0.9795311093330383, 0.9460203647613525, 0.9874317049980164, 0.9769600033760071, 0.9940897226333618, 0.9845134019851685, 1.0170460939407349, 1.033760905265808, 1.0141328573226929, 1.0160465240478516, 1.0187774896621704], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2435579299926758, 1.0016942024230957, 0.896453857421875, 0.8778297901153564, 0.8818279504776001, 0.8927949070930481, 0.8853171467781067, 0.8832451105117798, 0.911746084690094, 0.9211934208869934, 0.9041714668273926, 0.9461884498596191, 0.9409707188606262, 0.9499263763427734, 0.9795311093330383, 0.9460203647613525, 0.9874317049980164, 0.9769600033760071, 0.9940897226333618, 0.9845134019851685, 1.0170460939407349, 1.033760905265808, 1.0141328573226929, 1.0160465240478516, 1.0187774896621704]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760295391082764
current iteration best possible eval_loss (full train run):  -1.0187774896621704
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1046 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.9787769024330208
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.9832442547801923
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.9783369928287551
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.9793911875428417
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.9783713243276101
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.6024, dtype=torch.float64), 0, 0, tensor(0.1301, dtype=torch.float64), tensor(0.2675, dtype=torch.float64), 0, 0, 0, 0, 32, 1, 1, 0, 0, 1, 2, 0.1, 26.830970759432425, 0]
normalized proposed parameters for next round by BO: [tensor(0.6024, dtype=torch.float64), tensor(1.9880e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1301, dtype=torch.float64), tensor(0.2675, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.1148e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5590, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.602
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.13
  triviaqa: 0.267
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (26.830970759432425,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  26.830970759432425
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,031,616 || all params: 8,032,292,864 || trainable%: 0.0253
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5615, 'grad_norm': 4.8299736976623535, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7557271718978882, 'eval_runtime': 8.2083, 'eval_samples_per_second': 121.828, 'eval_steps_per_second': 7.675, 'epoch': 0.04}
{'loss': 1.3066, 'grad_norm': 1.7540229558944702, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6926130056381226, 'eval_runtime': 6.9564, 'eval_samples_per_second': 143.752, 'eval_steps_per_second': 9.056, 'epoch': 0.08}
{'loss': 1.0716, 'grad_norm': 1.353035807609558, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6166759729385376, 'eval_runtime': 6.9625, 'eval_samples_per_second': 143.626, 'eval_steps_per_second': 9.048, 'epoch': 0.12}
{'loss': 0.9366, 'grad_norm': 1.2913455963134766, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5691745281219482, 'eval_runtime': 6.9564, 'eval_samples_per_second': 143.753, 'eval_steps_per_second': 9.056, 'epoch': 0.16}
{'loss': 0.8956, 'grad_norm': 1.1488415002822876, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5679959058761597, 'eval_runtime': 6.9654, 'eval_samples_per_second': 143.567, 'eval_steps_per_second': 9.045, 'epoch': 0.2}
{'loss': 0.8776, 'grad_norm': 1.0134249925613403, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.56614351272583, 'eval_runtime': 6.9655, 'eval_samples_per_second': 143.565, 'eval_steps_per_second': 9.045, 'epoch': 0.24}
{'loss': 0.8552, 'grad_norm': 0.9182689785957336, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5415153503417969, 'eval_runtime': 7.0003, 'eval_samples_per_second': 142.85, 'eval_steps_per_second': 9.0, 'epoch': 0.28}
{'loss': 0.872, 'grad_norm': 0.9676930904388428, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5039304494857788, 'eval_runtime': 6.9964, 'eval_samples_per_second': 142.93, 'eval_steps_per_second': 9.005, 'epoch': 0.32}
{'loss': 0.8648, 'grad_norm': 1.1224761009216309, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.55225670337677, 'eval_runtime': 6.9926, 'eval_samples_per_second': 143.009, 'eval_steps_per_second': 9.01, 'epoch': 0.36}
{'loss': 0.8538, 'grad_norm': 0.9974619150161743, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4631990194320679, 'eval_runtime': 7.001, 'eval_samples_per_second': 142.836, 'eval_steps_per_second': 8.999, 'epoch': 0.4}
{'loss': 0.8578, 'grad_norm': 1.1017789840698242, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5418015718460083, 'eval_runtime': 7.0301, 'eval_samples_per_second': 142.245, 'eval_steps_per_second': 8.961, 'epoch': 0.44}
{'loss': 0.8415, 'grad_norm': 1.471381425857544, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5398277044296265, 'eval_runtime': 7.0272, 'eval_samples_per_second': 142.304, 'eval_steps_per_second': 8.965, 'epoch': 0.48}
{'loss': 0.8364, 'grad_norm': 0.978538453578949, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5460580587387085, 'eval_runtime': 7.0459, 'eval_samples_per_second': 141.927, 'eval_steps_per_second': 8.941, 'epoch': 0.52}
{'loss': 0.8483, 'grad_norm': 0.9974583387374878, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.546941876411438, 'eval_runtime': 7.0625, 'eval_samples_per_second': 141.593, 'eval_steps_per_second': 8.92, 'epoch': 0.56}
{'loss': 0.831, 'grad_norm': 1.0501686334609985, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5654845237731934, 'eval_runtime': 7.0398, 'eval_samples_per_second': 142.049, 'eval_steps_per_second': 8.949, 'epoch': 0.6}
{'loss': 0.8196, 'grad_norm': 1.1397615671157837, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.561489462852478, 'eval_runtime': 7.0674, 'eval_samples_per_second': 141.494, 'eval_steps_per_second': 8.914, 'epoch': 0.64}
{'loss': 0.8423, 'grad_norm': 1.0508276224136353, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.586908221244812, 'eval_runtime': 7.0685, 'eval_samples_per_second': 141.473, 'eval_steps_per_second': 8.913, 'epoch': 0.68}
{'loss': 0.8203, 'grad_norm': 1.250157117843628, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5576709508895874, 'eval_runtime': 7.0298, 'eval_samples_per_second': 142.252, 'eval_steps_per_second': 8.962, 'epoch': 0.72}
{'loss': 0.8275, 'grad_norm': 1.1490591764450073, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5577218532562256, 'eval_runtime': 7.0594, 'eval_samples_per_second': 141.654, 'eval_steps_per_second': 8.924, 'epoch': 0.76}
{'loss': 0.8368, 'grad_norm': 1.066298007965088, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.544551968574524, 'eval_runtime': 7.0278, 'eval_samples_per_second': 142.292, 'eval_steps_per_second': 8.964, 'epoch': 0.8}
{'loss': 0.8239, 'grad_norm': 1.0390028953552246, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5765657424926758, 'eval_runtime': 7.0312, 'eval_samples_per_second': 142.223, 'eval_steps_per_second': 8.96, 'epoch': 0.84}
{'loss': 0.7883, 'grad_norm': 1.2241451740264893, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5661438703536987, 'eval_runtime': 7.034, 'eval_samples_per_second': 142.167, 'eval_steps_per_second': 8.956, 'epoch': 0.88}
{'loss': 0.8213, 'grad_norm': 1.1597570180892944, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.564996600151062, 'eval_runtime': 7.053, 'eval_samples_per_second': 141.785, 'eval_steps_per_second': 8.932, 'epoch': 0.92}
{'loss': 0.8306, 'grad_norm': 1.061508297920227, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5631208419799805, 'eval_runtime': 7.0382, 'eval_samples_per_second': 142.082, 'eval_steps_per_second': 8.951, 'epoch': 0.96}
{'loss': 0.808, 'grad_norm': 1.355389952659607, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5669572353363037, 'eval_runtime': 7.0514, 'eval_samples_per_second': 141.816, 'eval_steps_per_second': 8.934, 'epoch': 1.0}
{'train_runtime': 294.2249, 'train_samples_per_second': 33.981, 'train_steps_per_second': 2.124, 'train_loss': 0.9811644622802734, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7557271718978882, 1.6926130056381226, 1.6166759729385376, 1.5691745281219482, 1.5679959058761597, 1.56614351272583, 1.5415153503417969, 1.5039304494857788, 1.55225670337677, 1.4631990194320679, 1.5418015718460083, 1.5398277044296265, 1.5460580587387085, 1.546941876411438, 1.5654845237731934, 1.561489462852478, 1.586908221244812, 1.5576709508895874, 1.5577218532562256, 1.544551968574524, 1.5765657424926758, 1.5661438703536987, 1.564996600151062, 1.5631208419799805, 1.5669572353363037], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7557271718978882, 1.6926130056381226, 1.6166759729385376, 1.5691745281219482, 1.5679959058761597, 1.56614351272583, 1.5415153503417969, 1.5039304494857788, 1.55225670337677, 1.4631990194320679, 1.5418015718460083, 1.5398277044296265, 1.5460580587387085, 1.546941876411438, 1.5654845237731934, 1.561489462852478, 1.586908221244812, 1.5576709508895874, 1.5577218532562256, 1.544551968574524, 1.5765657424926758, 1.5661438703536987, 1.564996600151062, 1.5631208419799805, 1.5669572353363037]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.076029658317566
current iteration best possible eval_loss (full train run):  -1.5669572353363037
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2077 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.9723185680044251
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.8831879741096449
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.9366679822845978
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.9684456483121957
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.9684923942060517
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1385, dtype=torch.float64), tensor(0.1117, dtype=torch.float64), tensor(0.3466, dtype=torch.float64), tensor(0.0430, dtype=torch.float64), tensor(0.2667, dtype=torch.float64), 0, tensor(0.0936, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 0, 2, 0.1, 16.63435458057141, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.1385, dtype=torch.float64), tensor(0.1117, dtype=torch.float64), tensor(0.3466, dtype=torch.float64), tensor(0.0430, dtype=torch.float64), tensor(0.2667, dtype=torch.float64), tensor(2.0685e-18, dtype=torch.float64), tensor(0.0936, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3465, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.138
  rowan_hellaswag: 0.112
  sciq: 0.347
  triviaqa: 0.043
  truthfulqa_gen: 0.267
  wikitext: 0
  mmlu: 0.094
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (16.63435458057141,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  16.63435458057141
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,883,584 || all params: 8,033,144,832 || trainable%: 0.0359
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3218, 'grad_norm': 7.7359185218811035, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0409607887268066, 'eval_runtime': 7.311, 'eval_samples_per_second': 136.779, 'eval_steps_per_second': 8.617, 'epoch': 0.04}
{'loss': 1.4931, 'grad_norm': 4.124582767486572, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7035797834396362, 'eval_runtime': 7.3043, 'eval_samples_per_second': 136.906, 'eval_steps_per_second': 8.625, 'epoch': 0.08}
{'loss': 1.2151, 'grad_norm': 1.0869454145431519, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8104168176651, 'eval_runtime': 7.3154, 'eval_samples_per_second': 136.697, 'eval_steps_per_second': 8.612, 'epoch': 0.12}
{'loss': 1.1786, 'grad_norm': 0.8939366340637207, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7340283393859863, 'eval_runtime': 7.3205, 'eval_samples_per_second': 136.602, 'eval_steps_per_second': 8.606, 'epoch': 0.16}
{'loss': 1.1657, 'grad_norm': 1.1010046005249023, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7468801736831665, 'eval_runtime': 7.3075, 'eval_samples_per_second': 136.845, 'eval_steps_per_second': 8.621, 'epoch': 0.2}
{'loss': 1.14, 'grad_norm': 0.8200181722640991, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.806965708732605, 'eval_runtime': 7.3199, 'eval_samples_per_second': 136.614, 'eval_steps_per_second': 8.607, 'epoch': 0.24}
{'loss': 1.1108, 'grad_norm': 1.0631953477859497, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8015239238739014, 'eval_runtime': 7.3497, 'eval_samples_per_second': 136.06, 'eval_steps_per_second': 8.572, 'epoch': 0.28}
{'loss': 1.1064, 'grad_norm': 0.9187343120574951, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.834636926651001, 'eval_runtime': 7.3794, 'eval_samples_per_second': 135.512, 'eval_steps_per_second': 8.537, 'epoch': 0.32}
{'loss': 1.0855, 'grad_norm': 0.8463777899742126, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8624950647354126, 'eval_runtime': 7.3599, 'eval_samples_per_second': 135.872, 'eval_steps_per_second': 8.56, 'epoch': 0.36}
{'loss': 1.0604, 'grad_norm': 0.9262695908546448, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8316264152526855, 'eval_runtime': 7.3648, 'eval_samples_per_second': 135.781, 'eval_steps_per_second': 8.554, 'epoch': 0.4}
{'loss': 1.096, 'grad_norm': 1.3262568712234497, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8367658853530884, 'eval_runtime': 7.3544, 'eval_samples_per_second': 135.973, 'eval_steps_per_second': 8.566, 'epoch': 0.44}
{'loss': 1.0799, 'grad_norm': 0.8338509202003479, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8486347198486328, 'eval_runtime': 7.3521, 'eval_samples_per_second': 136.016, 'eval_steps_per_second': 8.569, 'epoch': 0.48}
{'loss': 1.1109, 'grad_norm': 0.9221742153167725, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9088064432144165, 'eval_runtime': 7.3558, 'eval_samples_per_second': 135.947, 'eval_steps_per_second': 8.565, 'epoch': 0.52}
{'loss': 1.0202, 'grad_norm': 0.854053258895874, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8329582214355469, 'eval_runtime': 7.3594, 'eval_samples_per_second': 135.88, 'eval_steps_per_second': 8.56, 'epoch': 0.56}
{'loss': 1.0575, 'grad_norm': 1.080288290977478, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8289146423339844, 'eval_runtime': 7.3515, 'eval_samples_per_second': 136.026, 'eval_steps_per_second': 8.57, 'epoch': 0.6}
{'loss': 1.0466, 'grad_norm': 0.7872903347015381, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.797702670097351, 'eval_runtime': 7.3743, 'eval_samples_per_second': 135.605, 'eval_steps_per_second': 8.543, 'epoch': 0.64}
{'loss': 1.003, 'grad_norm': 1.052361249923706, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8039032220840454, 'eval_runtime': 7.39, 'eval_samples_per_second': 135.318, 'eval_steps_per_second': 8.525, 'epoch': 0.68}
{'loss': 1.0277, 'grad_norm': 1.169497013092041, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8314530849456787, 'eval_runtime': 7.3969, 'eval_samples_per_second': 135.191, 'eval_steps_per_second': 8.517, 'epoch': 0.72}
{'loss': 1.0178, 'grad_norm': 1.1623492240905762, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8502877950668335, 'eval_runtime': 7.3731, 'eval_samples_per_second': 135.628, 'eval_steps_per_second': 8.545, 'epoch': 0.76}
{'loss': 0.955, 'grad_norm': 0.9383617639541626, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8467193841934204, 'eval_runtime': 7.3785, 'eval_samples_per_second': 135.529, 'eval_steps_per_second': 8.538, 'epoch': 0.8}
{'loss': 1.0492, 'grad_norm': 1.19158935546875, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8510725498199463, 'eval_runtime': 7.3312, 'eval_samples_per_second': 136.403, 'eval_steps_per_second': 8.593, 'epoch': 0.84}
{'loss': 0.9135, 'grad_norm': 1.0106284618377686, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8546974658966064, 'eval_runtime': 7.312, 'eval_samples_per_second': 136.761, 'eval_steps_per_second': 8.616, 'epoch': 0.88}
{'loss': 1.0574, 'grad_norm': 0.9879500865936279, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8581571578979492, 'eval_runtime': 7.3189, 'eval_samples_per_second': 136.633, 'eval_steps_per_second': 8.608, 'epoch': 0.92}
{'loss': 0.9587, 'grad_norm': 1.0502890348434448, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.860103726387024, 'eval_runtime': 7.3495, 'eval_samples_per_second': 136.064, 'eval_steps_per_second': 8.572, 'epoch': 0.96}
{'loss': 1.0473, 'grad_norm': 0.9801333546638489, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8603237867355347, 'eval_runtime': 7.3737, 'eval_samples_per_second': 135.618, 'eval_steps_per_second': 8.544, 'epoch': 1.0}
{'train_runtime': 421.5245, 'train_samples_per_second': 23.716, 'train_steps_per_second': 1.483, 'train_loss': 1.172717498779297, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0409607887268066, 1.7035797834396362, 1.8104168176651, 1.7340283393859863, 1.7468801736831665, 1.806965708732605, 1.8015239238739014, 1.834636926651001, 1.8624950647354126, 1.8316264152526855, 1.8367658853530884, 1.8486347198486328, 1.9088064432144165, 1.8329582214355469, 1.8289146423339844, 1.797702670097351, 1.8039032220840454, 1.8314530849456787, 1.8502877950668335, 1.8467193841934204, 1.8510725498199463, 1.8546974658966064, 1.8581571578979492, 1.860103726387024, 1.8603237867355347], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0409607887268066, 1.7035797834396362, 1.8104168176651, 1.7340283393859863, 1.7468801736831665, 1.806965708732605, 1.8015239238739014, 1.834636926651001, 1.8624950647354126, 1.8316264152526855, 1.8367658853530884, 1.8486347198486328, 1.9088064432144165, 1.8329582214355469, 1.8289146423339844, 1.797702670097351, 1.8039032220840454, 1.8314530849456787, 1.8502877950668335, 1.8467193841934204, 1.8510725498199463, 1.8546974658966064, 1.8581571578979492, 1.860103726387024, 1.8603237867355347]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760319232940674
current iteration best possible eval_loss (full train run):  -1.8603237867355347
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0425 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.9450445108298806
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.9450445110031453
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.9450445106893799
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.9450445107234859
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.9450445107228624
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0453, dtype=torch.float64), 0, tensor(0.1571, dtype=torch.float64), tensor(0.2357, dtype=torch.float64), tensor(0.5619, dtype=torch.float64), 0, 0, 1, 1, 1, 1, 0, 1, 2, 1.0408340855860844e-18, 36.37024918190041, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.9393e-17, dtype=torch.float64), tensor(0.0453, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1571, dtype=torch.float64), tensor(0.2357, dtype=torch.float64), tensor(0.5619, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5317e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.0408e-17, dtype=torch.float64), tensor(0.7577, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.045
  sciq: 0
  triviaqa: 0.157
  truthfulqa_gen: 0.236
  wikitext: 0.562
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.0408340855860844e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (36.37024918190041,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  2
lora dropout:  1.0408340855860844e-18
lora alpha:  36.37024918190041
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 100,352 || all params: 8,030,361,600 || trainable%: 0.0012
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.872, 'grad_norm': 5.071885108947754, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.18705677986145, 'eval_runtime': 6.1364, 'eval_samples_per_second': 162.961, 'eval_steps_per_second': 10.267, 'epoch': 0.04}
{'loss': 3.1674, 'grad_norm': 12.01096248626709, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.3194997310638428, 'eval_runtime': 6.171, 'eval_samples_per_second': 162.047, 'eval_steps_per_second': 10.209, 'epoch': 0.08}
{'loss': 2.4755, 'grad_norm': 4.694841384887695, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.301636219024658, 'eval_runtime': 6.134, 'eval_samples_per_second': 163.026, 'eval_steps_per_second': 10.271, 'epoch': 0.12}
{'loss': 2.3555, 'grad_norm': 12.35777473449707, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.3793303966522217, 'eval_runtime': 6.1551, 'eval_samples_per_second': 162.467, 'eval_steps_per_second': 10.235, 'epoch': 0.16}
{'loss': 2.266, 'grad_norm': 10.502364158630371, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.4320759773254395, 'eval_runtime': 6.1658, 'eval_samples_per_second': 162.186, 'eval_steps_per_second': 10.218, 'epoch': 0.2}
{'loss': 2.1692, 'grad_norm': 5.026463985443115, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.4392154216766357, 'eval_runtime': 6.1765, 'eval_samples_per_second': 161.903, 'eval_steps_per_second': 10.2, 'epoch': 0.24}
{'loss': 2.1578, 'grad_norm': 4.021618366241455, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.248771905899048, 'eval_runtime': 6.1907, 'eval_samples_per_second': 161.531, 'eval_steps_per_second': 10.176, 'epoch': 0.28}
{'loss': 2.0192, 'grad_norm': 6.713804721832275, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.278548002243042, 'eval_runtime': 6.2132, 'eval_samples_per_second': 160.947, 'eval_steps_per_second': 10.14, 'epoch': 0.32}
{'loss': 2.0122, 'grad_norm': 4.745330810546875, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.276118516921997, 'eval_runtime': 6.1941, 'eval_samples_per_second': 161.443, 'eval_steps_per_second': 10.171, 'epoch': 0.36}
{'loss': 1.9915, 'grad_norm': 3.0757369995117188, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.2661049365997314, 'eval_runtime': 6.2318, 'eval_samples_per_second': 160.467, 'eval_steps_per_second': 10.109, 'epoch': 0.4}
{'loss': 1.9608, 'grad_norm': 3.3016443252563477, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.2397758960723877, 'eval_runtime': 6.1971, 'eval_samples_per_second': 161.366, 'eval_steps_per_second': 10.166, 'epoch': 0.44}
{'loss': 1.9351, 'grad_norm': 3.883366107940674, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.196204423904419, 'eval_runtime': 6.198, 'eval_samples_per_second': 161.342, 'eval_steps_per_second': 10.165, 'epoch': 0.48}
{'loss': 1.9477, 'grad_norm': 2.4722821712493896, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.1757631301879883, 'eval_runtime': 6.2467, 'eval_samples_per_second': 160.086, 'eval_steps_per_second': 10.085, 'epoch': 0.52}
{'loss': 1.9247, 'grad_norm': 3.279737949371338, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.2609095573425293, 'eval_runtime': 6.2178, 'eval_samples_per_second': 160.828, 'eval_steps_per_second': 10.132, 'epoch': 0.56}
{'loss': 1.9668, 'grad_norm': 4.799564361572266, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.243622303009033, 'eval_runtime': 6.2027, 'eval_samples_per_second': 161.22, 'eval_steps_per_second': 10.157, 'epoch': 0.6}
{'loss': 1.9072, 'grad_norm': 3.2435014247894287, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.22198486328125, 'eval_runtime': 6.1875, 'eval_samples_per_second': 161.616, 'eval_steps_per_second': 10.182, 'epoch': 0.64}
{'loss': 1.9394, 'grad_norm': 2.328035593032837, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.212808132171631, 'eval_runtime': 6.1912, 'eval_samples_per_second': 161.52, 'eval_steps_per_second': 10.176, 'epoch': 0.68}
{'loss': 1.8507, 'grad_norm': 3.9860594272613525, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.2314722537994385, 'eval_runtime': 6.1878, 'eval_samples_per_second': 161.607, 'eval_steps_per_second': 10.181, 'epoch': 0.72}
{'loss': 1.9161, 'grad_norm': 2.971160411834717, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.1975812911987305, 'eval_runtime': 6.1983, 'eval_samples_per_second': 161.336, 'eval_steps_per_second': 10.164, 'epoch': 0.76}
{'loss': 1.9068, 'grad_norm': 4.326536178588867, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.193056106567383, 'eval_runtime': 6.1908, 'eval_samples_per_second': 161.529, 'eval_steps_per_second': 10.176, 'epoch': 0.8}
{'loss': 1.8804, 'grad_norm': 5.4155354499816895, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.187126398086548, 'eval_runtime': 6.2008, 'eval_samples_per_second': 161.269, 'eval_steps_per_second': 10.16, 'epoch': 0.84}
{'loss': 1.858, 'grad_norm': 2.9659225940704346, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.190304756164551, 'eval_runtime': 6.2036, 'eval_samples_per_second': 161.196, 'eval_steps_per_second': 10.155, 'epoch': 0.88}
{'loss': 1.8743, 'grad_norm': 2.5404181480407715, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.2385857105255127, 'eval_runtime': 6.2051, 'eval_samples_per_second': 161.159, 'eval_steps_per_second': 10.153, 'epoch': 0.92}
{'loss': 1.8532, 'grad_norm': 3.1962828636169434, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.2282938957214355, 'eval_runtime': 6.1977, 'eval_samples_per_second': 161.35, 'eval_steps_per_second': 10.165, 'epoch': 0.96}
{'loss': 1.8863, 'grad_norm': 4.043027877807617, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.229483127593994, 'eval_runtime': 6.2015, 'eval_samples_per_second': 161.251, 'eval_steps_per_second': 10.159, 'epoch': 1.0}
{'train_runtime': 331.3612, 'train_samples_per_second': 30.176, 'train_steps_per_second': 1.886, 'train_loss': 2.1237516967773438, 'epoch': 1.0}
train_results:  {'eval_loss': [3.18705677986145, 2.3194997310638428, 2.301636219024658, 2.3793303966522217, 2.4320759773254395, 2.4392154216766357, 2.248771905899048, 2.278548002243042, 2.276118516921997, 2.2661049365997314, 2.2397758960723877, 2.196204423904419, 2.1757631301879883, 2.2609095573425293, 2.243622303009033, 2.22198486328125, 2.212808132171631, 2.2314722537994385, 2.1975812911987305, 2.193056106567383, 2.187126398086548, 2.190304756164551, 2.2385857105255127, 2.2282938957214355, 2.229483127593994], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.18705677986145, 2.3194997310638428, 2.301636219024658, 2.3793303966522217, 2.4320759773254395, 2.4392154216766357, 2.248771905899048, 2.278548002243042, 2.276118516921997, 2.2661049365997314, 2.2397758960723877, 2.196204423904419, 2.1757631301879883, 2.2609095573425293, 2.243622303009033, 2.22198486328125, 2.212808132171631, 2.2314722537994385, 2.1975812911987305, 2.193056106567383, 2.187126398086548, 2.190304756164551, 2.2385857105255127, 2.2282938957214355, 2.229483127593994]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0847843885421753
current iteration best possible eval_loss (full train run):  -2.229483127593994
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0364 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.9527419862697017
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.9570921644986983
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.952535829872541
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.9560248054298464
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.9527695931674729
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0417, dtype=torch.float64), tensor(0.3135, dtype=torch.float64), tensor(0.1425, dtype=torch.float64), tensor(0.3398, dtype=torch.float64), tensor(0.1625, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 0, 1, 2, 0.0, 28.235230717463594, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0417, dtype=torch.float64), tensor(0.3135, dtype=torch.float64), tensor(0.1425, dtype=torch.float64), tensor(0.3398, dtype=torch.float64), tensor(0.1625, dtype=torch.float64), tensor(1.3770e-17, dtype=torch.float64), tensor(2.0560e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5882, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.042
  sciq: 0.314
  triviaqa: 0.143
  truthfulqa_gen: 0.34
  wikitext: 0.162
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (28.235230717463594,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  28.235230717463594
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,211,264 || all params: 8,033,472,512 || trainable%: 0.0400
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3751, 'grad_norm': 3.518332004547119, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.213697671890259, 'eval_runtime': 7.3933, 'eval_samples_per_second': 135.258, 'eval_steps_per_second': 8.521, 'epoch': 0.04}
{'loss': 1.5783, 'grad_norm': 2.230377674102783, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.1932289600372314, 'eval_runtime': 7.3594, 'eval_samples_per_second': 135.881, 'eval_steps_per_second': 8.56, 'epoch': 0.08}
{'loss': 1.3658, 'grad_norm': 2.612468957901001, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.110872507095337, 'eval_runtime': 7.3712, 'eval_samples_per_second': 135.664, 'eval_steps_per_second': 8.547, 'epoch': 0.12}
{'loss': 1.1949, 'grad_norm': 1.97074294090271, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0475704669952393, 'eval_runtime': 7.3862, 'eval_samples_per_second': 135.387, 'eval_steps_per_second': 8.529, 'epoch': 0.16}
{'loss': 1.0854, 'grad_norm': 1.5905476808547974, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.1628754138946533, 'eval_runtime': 7.3828, 'eval_samples_per_second': 135.45, 'eval_steps_per_second': 8.533, 'epoch': 0.2}
{'loss': 1.125, 'grad_norm': 2.123591184616089, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.188789129257202, 'eval_runtime': 7.3717, 'eval_samples_per_second': 135.655, 'eval_steps_per_second': 8.546, 'epoch': 0.24}
{'loss': 1.0765, 'grad_norm': 1.6411874294281006, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.2155404090881348, 'eval_runtime': 7.3863, 'eval_samples_per_second': 135.386, 'eval_steps_per_second': 8.529, 'epoch': 0.28}
{'loss': 1.0854, 'grad_norm': 1.932089924812317, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.247194528579712, 'eval_runtime': 7.3838, 'eval_samples_per_second': 135.431, 'eval_steps_per_second': 8.532, 'epoch': 0.32}
{'loss': 1.1886, 'grad_norm': 2.445984363555908, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.3238444328308105, 'eval_runtime': 7.3733, 'eval_samples_per_second': 135.625, 'eval_steps_per_second': 8.544, 'epoch': 0.36}
{'loss': 1.0437, 'grad_norm': 1.416349172592163, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.18503999710083, 'eval_runtime': 7.3785, 'eval_samples_per_second': 135.529, 'eval_steps_per_second': 8.538, 'epoch': 0.4}
{'loss': 1.0517, 'grad_norm': 1.934615135192871, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.196948766708374, 'eval_runtime': 7.3823, 'eval_samples_per_second': 135.458, 'eval_steps_per_second': 8.534, 'epoch': 0.44}
{'loss': 1.0208, 'grad_norm': 1.2761801481246948, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.184330463409424, 'eval_runtime': 7.3914, 'eval_samples_per_second': 135.293, 'eval_steps_per_second': 8.523, 'epoch': 0.48}
{'loss': 1.0578, 'grad_norm': 1.9980908632278442, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.168112277984619, 'eval_runtime': 7.3932, 'eval_samples_per_second': 135.26, 'eval_steps_per_second': 8.521, 'epoch': 0.52}
{'loss': 0.9513, 'grad_norm': 1.7956697940826416, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.1905126571655273, 'eval_runtime': 7.4019, 'eval_samples_per_second': 135.1, 'eval_steps_per_second': 8.511, 'epoch': 0.56}
{'loss': 1.0325, 'grad_norm': 1.559203863143921, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.240077257156372, 'eval_runtime': 7.4688, 'eval_samples_per_second': 133.891, 'eval_steps_per_second': 8.435, 'epoch': 0.6}
{'loss': 1.0552, 'grad_norm': 2.069218873977661, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.2675046920776367, 'eval_runtime': 7.455, 'eval_samples_per_second': 134.138, 'eval_steps_per_second': 8.451, 'epoch': 0.64}
{'loss': 1.0986, 'grad_norm': 1.8818343877792358, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.1817123889923096, 'eval_runtime': 7.4543, 'eval_samples_per_second': 134.151, 'eval_steps_per_second': 8.452, 'epoch': 0.68}
{'loss': 0.8988, 'grad_norm': 2.200408697128296, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.205512523651123, 'eval_runtime': 7.4956, 'eval_samples_per_second': 133.411, 'eval_steps_per_second': 8.405, 'epoch': 0.72}
{'loss': 0.9157, 'grad_norm': 2.0193214416503906, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.251044511795044, 'eval_runtime': 7.4936, 'eval_samples_per_second': 133.446, 'eval_steps_per_second': 8.407, 'epoch': 0.76}
{'loss': 0.9789, 'grad_norm': 1.5481816530227661, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.2276647090911865, 'eval_runtime': 7.4899, 'eval_samples_per_second': 133.513, 'eval_steps_per_second': 8.411, 'epoch': 0.8}
{'loss': 0.9469, 'grad_norm': 1.6062430143356323, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.2617437839508057, 'eval_runtime': 7.4865, 'eval_samples_per_second': 133.574, 'eval_steps_per_second': 8.415, 'epoch': 0.84}
{'loss': 0.9203, 'grad_norm': 2.351116418838501, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.2546942234039307, 'eval_runtime': 7.5069, 'eval_samples_per_second': 133.212, 'eval_steps_per_second': 8.392, 'epoch': 0.88}
{'loss': 1.0072, 'grad_norm': 3.2426908016204834, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.261275291442871, 'eval_runtime': 7.4777, 'eval_samples_per_second': 133.731, 'eval_steps_per_second': 8.425, 'epoch': 0.92}
{'loss': 0.9499, 'grad_norm': 1.6618680953979492, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.272897958755493, 'eval_runtime': 7.5031, 'eval_samples_per_second': 133.279, 'eval_steps_per_second': 8.397, 'epoch': 0.96}
{'loss': 0.9108, 'grad_norm': 1.7814096212387085, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.276233673095703, 'eval_runtime': 7.4757, 'eval_samples_per_second': 133.766, 'eval_steps_per_second': 8.427, 'epoch': 1.0}
{'train_runtime': 365.5004, 'train_samples_per_second': 27.354, 'train_steps_per_second': 1.71, 'train_loss': 1.1565950256347657, 'epoch': 1.0}
train_results:  {'eval_loss': [2.213697671890259, 2.1932289600372314, 2.110872507095337, 2.0475704669952393, 2.1628754138946533, 2.188789129257202, 2.2155404090881348, 2.247194528579712, 2.3238444328308105, 2.18503999710083, 2.196948766708374, 2.184330463409424, 2.168112277984619, 2.1905126571655273, 2.240077257156372, 2.2675046920776367, 2.1817123889923096, 2.205512523651123, 2.251044511795044, 2.2276647090911865, 2.2617437839508057, 2.2546942234039307, 2.261275291442871, 2.272897958755493, 2.276233673095703], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.213697671890259, 2.1932289600372314, 2.110872507095337, 2.0475704669952393, 2.1628754138946533, 2.188789129257202, 2.2155404090881348, 2.247194528579712, 2.3238444328308105, 2.18503999710083, 2.196948766708374, 2.184330463409424, 2.168112277984619, 2.1905126571655273, 2.240077257156372, 2.2675046920776367, 2.1817123889923096, 2.205512523651123, 2.251044511795044, 2.2276647090911865, 2.2617437839508057, 2.2546942234039307, 2.261275291442871, 2.272897958755493, 2.276233673095703]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.076029896736145
current iteration best possible eval_loss (full train run):  -2.276233673095703
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0952 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -1.0824748281803684
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -1.0512055938444684
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -1.0592600380797041
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -1.0836371206324225
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -1.0494855540397219
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0832, dtype=torch.float64), tensor(0.2648, dtype=torch.float64), tensor(0.0286, dtype=torch.float64), tensor(0.0911, dtype=torch.float64), tensor(0.0145, dtype=torch.float64), tensor(0.0570, dtype=torch.float64), 0, tensor(0.1566, dtype=torch.float64), tensor(0.3042, dtype=torch.float64), 32, 1, 0, 0, 1, 1, 81, 0.1, 26.759630103845495, 1]
normalized proposed parameters for next round by BO: [tensor(0.0832, dtype=torch.float64), tensor(0.2648, dtype=torch.float64), tensor(0.0286, dtype=torch.float64), tensor(0.0911, dtype=torch.float64), tensor(0.0145, dtype=torch.float64), tensor(0.0570, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1566, dtype=torch.float64), tensor(0.3042, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6311, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5575, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.083
  gsm8k: 0.265
  rowan_hellaswag: 0.029
  sciq: 0.091
  triviaqa: 0.015
  truthfulqa_gen: 0.057
  wikitext: 0
  mmlu: 0.157
  arc_challenge: 0.304

LoRA Parameters:
  lora_r: (81,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (26.759630103845495,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  81
lora dropout:  0.1
lora alpha:  26.759630103845495
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 116,785,152 || all params: 8,147,046,400 || trainable%: 1.4335
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6889, 'grad_norm': 0.5219120979309082, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3533223867416382, 'eval_runtime': 7.7403, 'eval_samples_per_second': 129.195, 'eval_steps_per_second': 8.139, 'epoch': 0.04}
{'loss': 1.2033, 'grad_norm': 0.5742539763450623, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9355378746986389, 'eval_runtime': 7.7547, 'eval_samples_per_second': 128.954, 'eval_steps_per_second': 8.124, 'epoch': 0.08}
{'loss': 1.0705, 'grad_norm': 0.2126602828502655, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8859383463859558, 'eval_runtime': 7.7562, 'eval_samples_per_second': 128.928, 'eval_steps_per_second': 8.122, 'epoch': 0.12}
{'loss': 1.0181, 'grad_norm': 0.18945837020874023, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8821160793304443, 'eval_runtime': 7.7664, 'eval_samples_per_second': 128.759, 'eval_steps_per_second': 8.112, 'epoch': 0.16}
{'loss': 0.9824, 'grad_norm': 0.1695553958415985, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8604379296302795, 'eval_runtime': 7.763, 'eval_samples_per_second': 128.816, 'eval_steps_per_second': 8.115, 'epoch': 0.2}
{'loss': 0.999, 'grad_norm': 0.17098331451416016, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8618556261062622, 'eval_runtime': 7.7668, 'eval_samples_per_second': 128.753, 'eval_steps_per_second': 8.111, 'epoch': 0.24}
{'loss': 0.9802, 'grad_norm': 0.16277861595153809, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8647841811180115, 'eval_runtime': 7.751, 'eval_samples_per_second': 129.016, 'eval_steps_per_second': 8.128, 'epoch': 0.28}
{'loss': 0.9063, 'grad_norm': 0.16444465517997742, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8615475296974182, 'eval_runtime': 7.7339, 'eval_samples_per_second': 129.3, 'eval_steps_per_second': 8.146, 'epoch': 0.32}
{'loss': 0.9748, 'grad_norm': 0.22720254957675934, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8605321049690247, 'eval_runtime': 7.7296, 'eval_samples_per_second': 129.373, 'eval_steps_per_second': 8.15, 'epoch': 0.36}
{'loss': 0.9471, 'grad_norm': 0.1990620195865631, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.866342306137085, 'eval_runtime': 7.7325, 'eval_samples_per_second': 129.324, 'eval_steps_per_second': 8.147, 'epoch': 0.4}
{'loss': 0.9415, 'grad_norm': 0.19368506968021393, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8685428500175476, 'eval_runtime': 7.7356, 'eval_samples_per_second': 129.272, 'eval_steps_per_second': 8.144, 'epoch': 0.44}
{'loss': 0.947, 'grad_norm': 0.23965899646282196, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8924117684364319, 'eval_runtime': 7.7299, 'eval_samples_per_second': 129.367, 'eval_steps_per_second': 8.15, 'epoch': 0.48}
{'loss': 0.9097, 'grad_norm': 0.20406638085842133, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8696893453598022, 'eval_runtime': 7.7318, 'eval_samples_per_second': 129.335, 'eval_steps_per_second': 8.148, 'epoch': 0.52}
{'loss': 0.911, 'grad_norm': 0.19700701534748077, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.876567006111145, 'eval_runtime': 7.7592, 'eval_samples_per_second': 128.88, 'eval_steps_per_second': 8.119, 'epoch': 0.56}
{'loss': 0.9101, 'grad_norm': 0.20923598110675812, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8932263255119324, 'eval_runtime': 7.787, 'eval_samples_per_second': 128.418, 'eval_steps_per_second': 8.09, 'epoch': 0.6}
{'loss': 0.8714, 'grad_norm': 0.20789191126823425, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8900760412216187, 'eval_runtime': 7.7708, 'eval_samples_per_second': 128.687, 'eval_steps_per_second': 8.107, 'epoch': 0.64}
{'loss': 0.8698, 'grad_norm': 0.3288622498512268, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8996540307998657, 'eval_runtime': 7.7697, 'eval_samples_per_second': 128.705, 'eval_steps_per_second': 8.108, 'epoch': 0.68}
{'loss': 0.8603, 'grad_norm': 0.23015433549880981, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.91483074426651, 'eval_runtime': 7.7689, 'eval_samples_per_second': 128.718, 'eval_steps_per_second': 8.109, 'epoch': 0.72}
{'loss': 0.9018, 'grad_norm': 0.27607461810112, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9434128403663635, 'eval_runtime': 7.7675, 'eval_samples_per_second': 128.741, 'eval_steps_per_second': 8.111, 'epoch': 0.76}
{'loss': 0.8786, 'grad_norm': 0.2149098515510559, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9159150123596191, 'eval_runtime': 7.7688, 'eval_samples_per_second': 128.721, 'eval_steps_per_second': 8.109, 'epoch': 0.8}
{'loss': 0.9041, 'grad_norm': 0.19646210968494415, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.959419846534729, 'eval_runtime': 7.7676, 'eval_samples_per_second': 128.739, 'eval_steps_per_second': 8.111, 'epoch': 0.84}
{'loss': 0.8391, 'grad_norm': 0.20113712549209595, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9348393082618713, 'eval_runtime': 7.7703, 'eval_samples_per_second': 128.695, 'eval_steps_per_second': 8.108, 'epoch': 0.88}
{'loss': 0.8872, 'grad_norm': 0.2854529321193695, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9548191428184509, 'eval_runtime': 7.8346, 'eval_samples_per_second': 127.639, 'eval_steps_per_second': 8.041, 'epoch': 0.92}
{'loss': 0.8224, 'grad_norm': 0.38052091002464294, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9605553150177002, 'eval_runtime': 7.8832, 'eval_samples_per_second': 126.852, 'eval_steps_per_second': 7.992, 'epoch': 0.96}
{'loss': 0.8771, 'grad_norm': 0.2934156060218811, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.949447751045227, 'eval_runtime': 7.8431, 'eval_samples_per_second': 127.501, 'eval_steps_per_second': 8.033, 'epoch': 1.0}
{'train_runtime': 446.5842, 'train_samples_per_second': 22.383, 'train_steps_per_second': 1.4, 'train_loss': 1.004068536376953, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3533223867416382, 0.9355378746986389, 0.8859383463859558, 0.8821160793304443, 0.8604379296302795, 0.8618556261062622, 0.8647841811180115, 0.8615475296974182, 0.8605321049690247, 0.866342306137085, 0.8685428500175476, 0.8924117684364319, 0.8696893453598022, 0.876567006111145, 0.8932263255119324, 0.8900760412216187, 0.8996540307998657, 0.91483074426651, 0.9434128403663635, 0.9159150123596191, 0.959419846534729, 0.9348393082618713, 0.9548191428184509, 0.9605553150177002, 0.949447751045227], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3533223867416382, 0.9355378746986389, 0.8859383463859558, 0.8821160793304443, 0.8604379296302795, 0.8618556261062622, 0.8647841811180115, 0.8615475296974182, 0.8605321049690247, 0.866342306137085, 0.8685428500175476, 0.8924117684364319, 0.8696893453598022, 0.876567006111145, 0.8932263255119324, 0.8900760412216187, 0.8996540307998657, 0.91483074426651, 0.9434128403663635, 0.9159150123596191, 0.959419846534729, 0.9348393082618713, 0.9548191428184509, 0.9605553150177002, 0.949447751045227]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.260966420173645
current iteration best possible eval_loss (full train run):  -0.949447751045227
max eval_loss so far:  -0.8778097629547119
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8738 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -1.0076330436545247
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -1.0706392559866285
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -1.1345448761802175
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.9337334642181969
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -1.004784716829569
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0499, dtype=torch.float64), 0, tensor(0.1189, dtype=torch.float64), tensor(0.2666, dtype=torch.float64), tensor(0.1469, dtype=torch.float64), 0, tensor(0.0956, dtype=torch.float64), tensor(0.2189, dtype=torch.float64), tensor(0.1032, dtype=torch.float64), 32, 1, 0, 1, 1, 0, 2, 0.06983877058490391, 46.74842015120022, 1]
normalized proposed parameters for next round by BO: [tensor(0.0499, dtype=torch.float64), tensor(3.1608e-18, dtype=torch.float64), tensor(0.1189, dtype=torch.float64), tensor(0.2666, dtype=torch.float64), tensor(0.1469, dtype=torch.float64), tensor(2.6500e-18, dtype=torch.float64), tensor(0.0956, dtype=torch.float64), tensor(0.2189, dtype=torch.float64), tensor(0.1032, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.6984, dtype=torch.float64), tensor(0.9739, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.05
  gsm8k: 0
  rowan_hellaswag: 0.119
  sciq: 0.267
  triviaqa: 0.147
  truthfulqa_gen: 0
  wikitext: 0.096
  mmlu: 0.219
  arc_challenge: 0.103

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.06983877058490391,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (46.74842015120022,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  2
lora dropout:  0.06983877058490391
lora alpha:  46.74842015120022
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,883,584 || all params: 8,033,144,832 || trainable%: 0.0359
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9666, 'grad_norm': 5.540704727172852, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3017873764038086, 'eval_runtime': 7.3088, 'eval_samples_per_second': 136.821, 'eval_steps_per_second': 8.62, 'epoch': 0.04}
{'loss': 1.5299, 'grad_norm': 3.9003961086273193, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9465417861938477, 'eval_runtime': 7.2976, 'eval_samples_per_second': 137.032, 'eval_steps_per_second': 8.633, 'epoch': 0.08}
{'loss': 1.4038, 'grad_norm': 1.9935626983642578, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9163437485694885, 'eval_runtime': 7.3095, 'eval_samples_per_second': 136.809, 'eval_steps_per_second': 8.619, 'epoch': 0.12}
{'loss': 1.2952, 'grad_norm': 1.7444180250167847, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8995633125305176, 'eval_runtime': 7.3319, 'eval_samples_per_second': 136.391, 'eval_steps_per_second': 8.593, 'epoch': 0.16}
{'loss': 1.3622, 'grad_norm': 2.5270094871520996, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8963578939437866, 'eval_runtime': 7.3315, 'eval_samples_per_second': 136.398, 'eval_steps_per_second': 8.593, 'epoch': 0.2}
{'loss': 1.3288, 'grad_norm': 1.5711463689804077, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.889540433883667, 'eval_runtime': 7.3318, 'eval_samples_per_second': 136.393, 'eval_steps_per_second': 8.593, 'epoch': 0.24}
{'loss': 1.3525, 'grad_norm': 1.9485188722610474, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8847270607948303, 'eval_runtime': 7.3213, 'eval_samples_per_second': 136.588, 'eval_steps_per_second': 8.605, 'epoch': 0.28}
{'loss': 1.3192, 'grad_norm': 1.878221869468689, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8819228410720825, 'eval_runtime': 7.2997, 'eval_samples_per_second': 136.993, 'eval_steps_per_second': 8.631, 'epoch': 0.32}
{'loss': 1.272, 'grad_norm': 3.9916765689849854, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8827252984046936, 'eval_runtime': 7.3065, 'eval_samples_per_second': 136.865, 'eval_steps_per_second': 8.623, 'epoch': 0.36}
{'loss': 1.2863, 'grad_norm': 1.9675626754760742, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8813459277153015, 'eval_runtime': 7.3184, 'eval_samples_per_second': 136.642, 'eval_steps_per_second': 8.608, 'epoch': 0.4}
{'loss': 1.305, 'grad_norm': 3.3733112812042236, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8799944519996643, 'eval_runtime': 7.342, 'eval_samples_per_second': 136.203, 'eval_steps_per_second': 8.581, 'epoch': 0.44}
{'loss': 1.2689, 'grad_norm': 1.9128843545913696, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8775712251663208, 'eval_runtime': 7.3463, 'eval_samples_per_second': 136.123, 'eval_steps_per_second': 8.576, 'epoch': 0.48}
{'loss': 1.2381, 'grad_norm': 1.625781774520874, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8787325620651245, 'eval_runtime': 7.3394, 'eval_samples_per_second': 136.251, 'eval_steps_per_second': 8.584, 'epoch': 0.52}
{'loss': 1.2232, 'grad_norm': 2.3350272178649902, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8718985319137573, 'eval_runtime': 7.3452, 'eval_samples_per_second': 136.144, 'eval_steps_per_second': 8.577, 'epoch': 0.56}
{'loss': 1.3225, 'grad_norm': 2.14715838432312, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8762627840042114, 'eval_runtime': 7.3529, 'eval_samples_per_second': 136.001, 'eval_steps_per_second': 8.568, 'epoch': 0.6}
{'loss': 1.3226, 'grad_norm': 1.7699755430221558, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8695346713066101, 'eval_runtime': 7.3504, 'eval_samples_per_second': 136.048, 'eval_steps_per_second': 8.571, 'epoch': 0.64}
{'loss': 1.2934, 'grad_norm': 2.15816330909729, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8691688776016235, 'eval_runtime': 7.3375, 'eval_samples_per_second': 136.287, 'eval_steps_per_second': 8.586, 'epoch': 0.68}
{'loss': 1.3249, 'grad_norm': 1.8072988986968994, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8741821050643921, 'eval_runtime': 7.3427, 'eval_samples_per_second': 136.19, 'eval_steps_per_second': 8.58, 'epoch': 0.72}
{'loss': 1.2457, 'grad_norm': 1.789584994316101, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8689705729484558, 'eval_runtime': 7.3428, 'eval_samples_per_second': 136.188, 'eval_steps_per_second': 8.58, 'epoch': 0.76}
{'loss': 1.2251, 'grad_norm': 1.4132601022720337, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8687148690223694, 'eval_runtime': 7.4096, 'eval_samples_per_second': 134.96, 'eval_steps_per_second': 8.502, 'epoch': 0.8}
{'loss': 1.2728, 'grad_norm': 4.4384074211120605, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8695444464683533, 'eval_runtime': 7.3979, 'eval_samples_per_second': 135.174, 'eval_steps_per_second': 8.516, 'epoch': 0.84}
{'loss': 1.2394, 'grad_norm': 1.5249993801116943, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8693683743476868, 'eval_runtime': 7.3821, 'eval_samples_per_second': 135.463, 'eval_steps_per_second': 8.534, 'epoch': 0.88}
{'loss': 1.2933, 'grad_norm': 1.9925265312194824, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8676791191101074, 'eval_runtime': 7.3363, 'eval_samples_per_second': 136.308, 'eval_steps_per_second': 8.587, 'epoch': 0.92}
{'loss': 1.266, 'grad_norm': 2.0629594326019287, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8669024109840393, 'eval_runtime': 7.3466, 'eval_samples_per_second': 136.117, 'eval_steps_per_second': 8.575, 'epoch': 0.96}
{'loss': 1.3039, 'grad_norm': 2.8057384490966797, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8668954968452454, 'eval_runtime': 7.3955, 'eval_samples_per_second': 135.218, 'eval_steps_per_second': 8.519, 'epoch': 1.0}
{'train_runtime': 417.7671, 'train_samples_per_second': 23.927, 'train_steps_per_second': 1.496, 'train_loss': 1.3704610443115235, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3017873764038086, 0.9465417861938477, 0.9163437485694885, 0.8995633125305176, 0.8963578939437866, 0.889540433883667, 0.8847270607948303, 0.8819228410720825, 0.8827252984046936, 0.8813459277153015, 0.8799944519996643, 0.8775712251663208, 0.8787325620651245, 0.8718985319137573, 0.8762627840042114, 0.8695346713066101, 0.8691688776016235, 0.8741821050643921, 0.8689705729484558, 0.8687148690223694, 0.8695444464683533, 0.8693683743476868, 0.8676791191101074, 0.8669024109840393, 0.8668954968452454], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3017873764038086, 0.9465417861938477, 0.9163437485694885, 0.8995633125305176, 0.8963578939437866, 0.889540433883667, 0.8847270607948303, 0.8819228410720825, 0.8827252984046936, 0.8813459277153015, 0.8799944519996643, 0.8775712251663208, 0.8787325620651245, 0.8718985319137573, 0.8762627840042114, 0.8695346713066101, 0.8691688776016235, 0.8741821050643921, 0.8689705729484558, 0.8687148690223694, 0.8695444464683533, 0.8693683743476868, 0.8676791191101074, 0.8669024109840393, 0.8668954968452454]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760266780853271
current iteration best possible eval_loss (full train run):  -0.8668954968452454
max eval_loss so far:  -0.8668954968452454
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.8282 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -1.0145065185342386
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -1.0997015730996007
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -1.0174247313204767
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -1.0006822381148983
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -1.0317773250041473
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0467, dtype=torch.float64), tensor(0.1243, dtype=torch.float64), tensor(0.2000, dtype=torch.float64), tensor(0.5758, dtype=torch.float64), 0, 0, 0, tensor(0.0532, dtype=torch.float64), 32, 0, 0, 0, 0, 1, 2, 0.1, 25.93363337382777, 0]
normalized proposed parameters for next round by BO: [tensor(2.2257e-18, dtype=torch.float64), tensor(0.0467, dtype=torch.float64), tensor(0.1243, dtype=torch.float64), tensor(0.2000, dtype=torch.float64), tensor(0.5758, dtype=torch.float64), tensor(2.1583e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0532, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5403, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.047
  rowan_hellaswag: 0.124
  sciq: 0.2
  triviaqa: 0.576
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.053

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (25.93363337382777,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  25.93363337382777
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,179,648 || all params: 8,031,440,896 || trainable%: 0.0147
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.887, 'grad_norm': 2.174349546432495, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.273192882537842, 'eval_runtime': 6.5369, 'eval_samples_per_second': 152.979, 'eval_steps_per_second': 9.638, 'epoch': 0.04}
{'loss': 1.9492, 'grad_norm': 1.4627410173416138, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1921603679656982, 'eval_runtime': 6.5749, 'eval_samples_per_second': 152.094, 'eval_steps_per_second': 9.582, 'epoch': 0.08}
{'loss': 1.4836, 'grad_norm': 1.6089943647384644, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0666331052780151, 'eval_runtime': 6.552, 'eval_samples_per_second': 152.625, 'eval_steps_per_second': 9.615, 'epoch': 0.12}
{'loss': 1.2721, 'grad_norm': 1.0968374013900757, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9481871128082275, 'eval_runtime': 6.5698, 'eval_samples_per_second': 152.212, 'eval_steps_per_second': 9.589, 'epoch': 0.16}
{'loss': 1.2823, 'grad_norm': 1.1786879301071167, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9208027720451355, 'eval_runtime': 6.5736, 'eval_samples_per_second': 152.124, 'eval_steps_per_second': 9.584, 'epoch': 0.2}
{'loss': 1.2343, 'grad_norm': 0.8479729294776917, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9133526682853699, 'eval_runtime': 6.5729, 'eval_samples_per_second': 152.14, 'eval_steps_per_second': 9.585, 'epoch': 0.24}
{'loss': 1.156, 'grad_norm': 1.0054495334625244, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9029480814933777, 'eval_runtime': 6.5904, 'eval_samples_per_second': 151.735, 'eval_steps_per_second': 9.559, 'epoch': 0.28}
{'loss': 1.1988, 'grad_norm': 0.9639353156089783, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8977681398391724, 'eval_runtime': 6.5837, 'eval_samples_per_second': 151.891, 'eval_steps_per_second': 9.569, 'epoch': 0.32}
{'loss': 1.196, 'grad_norm': 0.9281045794487, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8925927877426147, 'eval_runtime': 6.5923, 'eval_samples_per_second': 151.693, 'eval_steps_per_second': 9.557, 'epoch': 0.36}
{'loss': 1.1774, 'grad_norm': 0.9311702847480774, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.890899658203125, 'eval_runtime': 6.5918, 'eval_samples_per_second': 151.704, 'eval_steps_per_second': 9.557, 'epoch': 0.4}
{'loss': 1.1551, 'grad_norm': 1.0536413192749023, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8965919017791748, 'eval_runtime': 6.602, 'eval_samples_per_second': 151.469, 'eval_steps_per_second': 9.543, 'epoch': 0.44}
{'loss': 1.2417, 'grad_norm': 0.7797768712043762, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8869809508323669, 'eval_runtime': 6.6063, 'eval_samples_per_second': 151.37, 'eval_steps_per_second': 9.536, 'epoch': 0.48}
{'loss': 1.1469, 'grad_norm': 0.9231496453285217, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8963627815246582, 'eval_runtime': 6.5973, 'eval_samples_per_second': 151.578, 'eval_steps_per_second': 9.549, 'epoch': 0.52}
{'loss': 1.1591, 'grad_norm': 0.7868724465370178, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.882889449596405, 'eval_runtime': 6.5827, 'eval_samples_per_second': 151.914, 'eval_steps_per_second': 9.571, 'epoch': 0.56}
{'loss': 1.2596, 'grad_norm': 1.0528564453125, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.884681761264801, 'eval_runtime': 6.5802, 'eval_samples_per_second': 151.971, 'eval_steps_per_second': 9.574, 'epoch': 0.6}
{'loss': 1.1521, 'grad_norm': 0.797548234462738, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8841386437416077, 'eval_runtime': 6.5707, 'eval_samples_per_second': 152.192, 'eval_steps_per_second': 9.588, 'epoch': 0.64}
{'loss': 1.1284, 'grad_norm': 1.5958069562911987, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8803693652153015, 'eval_runtime': 6.5578, 'eval_samples_per_second': 152.489, 'eval_steps_per_second': 9.607, 'epoch': 0.68}
{'loss': 1.1583, 'grad_norm': 0.8872809410095215, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8795467615127563, 'eval_runtime': 6.5472, 'eval_samples_per_second': 152.738, 'eval_steps_per_second': 9.622, 'epoch': 0.72}
{'loss': 1.1902, 'grad_norm': 1.0044931173324585, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8763357400894165, 'eval_runtime': 6.5521, 'eval_samples_per_second': 152.624, 'eval_steps_per_second': 9.615, 'epoch': 0.76}
{'loss': 1.1519, 'grad_norm': 0.9391055107116699, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8778380155563354, 'eval_runtime': 6.5541, 'eval_samples_per_second': 152.576, 'eval_steps_per_second': 9.612, 'epoch': 0.8}
{'loss': 1.156, 'grad_norm': 0.8029831051826477, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8782035708427429, 'eval_runtime': 6.5596, 'eval_samples_per_second': 152.448, 'eval_steps_per_second': 9.604, 'epoch': 0.84}
{'loss': 1.2102, 'grad_norm': 0.8881248831748962, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8784125447273254, 'eval_runtime': 6.5578, 'eval_samples_per_second': 152.491, 'eval_steps_per_second': 9.607, 'epoch': 0.88}
{'loss': 1.0963, 'grad_norm': 0.9691522121429443, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8752293586730957, 'eval_runtime': 6.567, 'eval_samples_per_second': 152.276, 'eval_steps_per_second': 9.593, 'epoch': 0.92}
{'loss': 1.0926, 'grad_norm': 0.9910566806793213, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8744478225708008, 'eval_runtime': 6.563, 'eval_samples_per_second': 152.369, 'eval_steps_per_second': 9.599, 'epoch': 0.96}
{'loss': 1.1192, 'grad_norm': 0.8527343273162842, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8745988011360168, 'eval_runtime': 6.5499, 'eval_samples_per_second': 152.675, 'eval_steps_per_second': 9.619, 'epoch': 1.0}
{'train_runtime': 354.632, 'train_samples_per_second': 28.19, 'train_steps_per_second': 1.762, 'train_loss': 1.3301714477539062, 'epoch': 1.0}
train_results:  {'eval_loss': [2.273192882537842, 1.1921603679656982, 1.0666331052780151, 0.9481871128082275, 0.9208027720451355, 0.9133526682853699, 0.9029480814933777, 0.8977681398391724, 0.8925927877426147, 0.890899658203125, 0.8965919017791748, 0.8869809508323669, 0.8963627815246582, 0.882889449596405, 0.884681761264801, 0.8841386437416077, 0.8803693652153015, 0.8795467615127563, 0.8763357400894165, 0.8778380155563354, 0.8782035708427429, 0.8784125447273254, 0.8752293586730957, 0.8744478225708008, 0.8745988011360168], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.273192882537842, 1.1921603679656982, 1.0666331052780151, 0.9481871128082275, 0.9208027720451355, 0.9133526682853699, 0.9029480814933777, 0.8977681398391724, 0.8925927877426147, 0.890899658203125, 0.8965919017791748, 0.8869809508323669, 0.8963627815246582, 0.882889449596405, 0.884681761264801, 0.8841386437416077, 0.8803693652153015, 0.8795467615127563, 0.8763357400894165, 0.8778380155563354, 0.8782035708427429, 0.8784125447273254, 0.8752293586730957, 0.8744478225708008, 0.8745988011360168]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.076029896736145
current iteration best possible eval_loss (full train run):  -0.8745988011360168
max eval_loss so far:  -0.8668954968452454
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1284 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -1.039797228741469
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -1.0769478706198221
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -1.02639453286524
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -1.0323921726593355
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -1.0021129588061917
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0539, dtype=torch.float64), tensor(0.4320, dtype=torch.float64), tensor(0.2675, dtype=torch.float64), 0, 0, 0, tensor(0.2466, dtype=torch.float64), 32, 0, 1, 0, 0, 1, 2, 0.1, 21.21436604583375, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(6.4177e-17, dtype=torch.float64), tensor(0.0539, dtype=torch.float64), tensor(0.4320, dtype=torch.float64), tensor(0.2675, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.2115e-17, dtype=torch.float64), tensor(0.2466, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4420, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.054
  sciq: 0.432
  triviaqa: 0.267
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.247

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (21.21436604583375,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  21.21436604583375
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,507,328 || all params: 8,031,768,576 || trainable%: 0.0188
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.665, 'grad_norm': 5.597570419311523, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8467514514923096, 'eval_runtime': 6.6789, 'eval_samples_per_second': 149.724, 'eval_steps_per_second': 9.433, 'epoch': 0.04}
{'loss': 1.4788, 'grad_norm': 1.6744228601455688, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0645619630813599, 'eval_runtime': 6.6843, 'eval_samples_per_second': 149.604, 'eval_steps_per_second': 9.425, 'epoch': 0.08}
{'loss': 1.1973, 'grad_norm': 1.956289529800415, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9660099148750305, 'eval_runtime': 6.6693, 'eval_samples_per_second': 149.94, 'eval_steps_per_second': 9.446, 'epoch': 0.12}
{'loss': 1.0686, 'grad_norm': 1.5554425716400146, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8851785063743591, 'eval_runtime': 6.681, 'eval_samples_per_second': 149.679, 'eval_steps_per_second': 9.43, 'epoch': 0.16}
{'loss': 0.9618, 'grad_norm': 1.3549528121948242, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8679478764533997, 'eval_runtime': 6.6881, 'eval_samples_per_second': 149.518, 'eval_steps_per_second': 9.42, 'epoch': 0.2}
{'loss': 0.982, 'grad_norm': 1.3165638446807861, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8683849573135376, 'eval_runtime': 6.7154, 'eval_samples_per_second': 148.912, 'eval_steps_per_second': 9.381, 'epoch': 0.24}
{'loss': 0.9417, 'grad_norm': 1.2175363302230835, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8698737621307373, 'eval_runtime': 6.7278, 'eval_samples_per_second': 148.637, 'eval_steps_per_second': 9.364, 'epoch': 0.28}
{'loss': 0.9396, 'grad_norm': 1.3396893739700317, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8710398077964783, 'eval_runtime': 6.7666, 'eval_samples_per_second': 147.784, 'eval_steps_per_second': 9.31, 'epoch': 0.32}
{'loss': 1.0238, 'grad_norm': 1.1308143138885498, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8673811554908752, 'eval_runtime': 6.7863, 'eval_samples_per_second': 147.356, 'eval_steps_per_second': 9.283, 'epoch': 0.36}
{'loss': 0.9266, 'grad_norm': 1.4106303453445435, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8692666292190552, 'eval_runtime': 6.7776, 'eval_samples_per_second': 147.545, 'eval_steps_per_second': 9.295, 'epoch': 0.4}
{'loss': 0.9207, 'grad_norm': 1.2417210340499878, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8708769679069519, 'eval_runtime': 6.7735, 'eval_samples_per_second': 147.635, 'eval_steps_per_second': 9.301, 'epoch': 0.44}
{'loss': 0.9448, 'grad_norm': 1.0831494331359863, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.866040825843811, 'eval_runtime': 6.7824, 'eval_samples_per_second': 147.44, 'eval_steps_per_second': 9.289, 'epoch': 0.48}
{'loss': 0.9537, 'grad_norm': 1.516135334968567, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8886530995368958, 'eval_runtime': 6.8113, 'eval_samples_per_second': 146.816, 'eval_steps_per_second': 9.249, 'epoch': 0.52}
{'loss': 0.9224, 'grad_norm': 1.612371563911438, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8873498439788818, 'eval_runtime': 6.8186, 'eval_samples_per_second': 146.657, 'eval_steps_per_second': 9.239, 'epoch': 0.56}
{'loss': 0.966, 'grad_norm': 1.0483665466308594, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8807057738304138, 'eval_runtime': 6.8174, 'eval_samples_per_second': 146.683, 'eval_steps_per_second': 9.241, 'epoch': 0.6}
{'loss': 0.8639, 'grad_norm': 1.39475417137146, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8930267691612244, 'eval_runtime': 6.8019, 'eval_samples_per_second': 147.018, 'eval_steps_per_second': 9.262, 'epoch': 0.64}
{'loss': 0.9182, 'grad_norm': 1.2722322940826416, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9152938723564148, 'eval_runtime': 6.825, 'eval_samples_per_second': 146.521, 'eval_steps_per_second': 9.231, 'epoch': 0.68}
{'loss': 0.8306, 'grad_norm': 1.6422910690307617, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9111595749855042, 'eval_runtime': 6.826, 'eval_samples_per_second': 146.499, 'eval_steps_per_second': 9.229, 'epoch': 0.72}
{'loss': 0.9277, 'grad_norm': 1.331248164176941, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9121443629264832, 'eval_runtime': 6.8391, 'eval_samples_per_second': 146.217, 'eval_steps_per_second': 9.212, 'epoch': 0.76}
{'loss': 0.9119, 'grad_norm': 1.4235773086547852, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9050796031951904, 'eval_runtime': 6.7712, 'eval_samples_per_second': 147.685, 'eval_steps_per_second': 9.304, 'epoch': 0.8}
{'loss': 0.8413, 'grad_norm': 2.000734329223633, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9350864291191101, 'eval_runtime': 6.7832, 'eval_samples_per_second': 147.422, 'eval_steps_per_second': 9.288, 'epoch': 0.84}
{'loss': 0.865, 'grad_norm': 1.6053557395935059, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9205836057662964, 'eval_runtime': 6.7907, 'eval_samples_per_second': 147.261, 'eval_steps_per_second': 9.277, 'epoch': 0.88}
{'loss': 0.8666, 'grad_norm': 2.0071513652801514, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9217432141304016, 'eval_runtime': 6.7752, 'eval_samples_per_second': 147.598, 'eval_steps_per_second': 9.299, 'epoch': 0.92}
{'loss': 0.8501, 'grad_norm': 1.4751100540161133, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9190440773963928, 'eval_runtime': 6.7736, 'eval_samples_per_second': 147.631, 'eval_steps_per_second': 9.301, 'epoch': 0.96}
{'loss': 0.8449, 'grad_norm': 1.5167758464813232, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9219691157341003, 'eval_runtime': 6.7908, 'eval_samples_per_second': 147.258, 'eval_steps_per_second': 9.277, 'epoch': 1.0}
{'train_runtime': 342.8512, 'train_samples_per_second': 29.161, 'train_steps_per_second': 1.823, 'train_loss': 1.0645110107421876, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8467514514923096, 1.0645619630813599, 0.9660099148750305, 0.8851785063743591, 0.8679478764533997, 0.8683849573135376, 0.8698737621307373, 0.8710398077964783, 0.8673811554908752, 0.8692666292190552, 0.8708769679069519, 0.866040825843811, 0.8886530995368958, 0.8873498439788818, 0.8807057738304138, 0.8930267691612244, 0.9152938723564148, 0.9111595749855042, 0.9121443629264832, 0.9050796031951904, 0.9350864291191101, 0.9205836057662964, 0.9217432141304016, 0.9190440773963928, 0.9219691157341003], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8467514514923096, 1.0645619630813599, 0.9660099148750305, 0.8851785063743591, 0.8679478764533997, 0.8683849573135376, 0.8698737621307373, 0.8710398077964783, 0.8673811554908752, 0.8692666292190552, 0.8708769679069519, 0.866040825843811, 0.8886530995368958, 0.8873498439788818, 0.8807057738304138, 0.8930267691612244, 0.9152938723564148, 0.9111595749855042, 0.9121443629264832, 0.9050796031951904, 0.9350864291191101, 0.9205836057662964, 0.9217432141304016, 0.9190440773963928, 0.9219691157341003]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760301351547241
current iteration best possible eval_loss (full train run):  -0.9219691157341003
max eval_loss so far:  -0.8668954968452454
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.9063 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -1.0326483306045753
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -1.122407603207856
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -1.0504696567937817
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -1.165934883441905
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -1.0383957511859694
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1301, dtype=torch.float64), 0, 0, tensor(0.0653, dtype=torch.float64), tensor(0.1208, dtype=torch.float64), 0, 0, tensor(0.6158, dtype=torch.float64), tensor(0.0680, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 2, 0.1, 18.800699763962648, 1]
normalized proposed parameters for next round by BO: [tensor(0.1301, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0653, dtype=torch.float64), tensor(0.1208, dtype=torch.float64), tensor(3.4137e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6158, dtype=torch.float64), tensor(0.0680, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3917, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.13
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.065
  triviaqa: 0.121
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.616
  arc_challenge: 0.068

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (18.800699763962648,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  18.800699763962648
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 4,390,912 || all params: 8,034,652,160 || trainable%: 0.0546
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7765, 'grad_norm': 4.068066596984863, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4369523525238037, 'eval_runtime': 8.007, 'eval_samples_per_second': 124.89, 'eval_steps_per_second': 7.868, 'epoch': 0.04}
{'loss': 1.2935, 'grad_norm': 2.4361698627471924, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9671168327331543, 'eval_runtime': 8.0071, 'eval_samples_per_second': 124.889, 'eval_steps_per_second': 7.868, 'epoch': 0.08}
{'loss': 1.2014, 'grad_norm': 1.2251864671707153, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9882060289382935, 'eval_runtime': 7.9817, 'eval_samples_per_second': 125.287, 'eval_steps_per_second': 7.893, 'epoch': 0.12}
{'loss': 1.1662, 'grad_norm': 1.2351195812225342, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9381256103515625, 'eval_runtime': 7.975, 'eval_samples_per_second': 125.391, 'eval_steps_per_second': 7.9, 'epoch': 0.16}
{'loss': 1.1481, 'grad_norm': 1.0631701946258545, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9273662567138672, 'eval_runtime': 7.9682, 'eval_samples_per_second': 125.499, 'eval_steps_per_second': 7.906, 'epoch': 0.2}
{'loss': 1.169, 'grad_norm': 1.7792253494262695, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9209622740745544, 'eval_runtime': 7.963, 'eval_samples_per_second': 125.581, 'eval_steps_per_second': 7.912, 'epoch': 0.24}
{'loss': 1.1514, 'grad_norm': 1.0303032398223877, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8918471932411194, 'eval_runtime': 7.9676, 'eval_samples_per_second': 125.508, 'eval_steps_per_second': 7.907, 'epoch': 0.28}
{'loss': 1.1308, 'grad_norm': 0.9894149899482727, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8925576210021973, 'eval_runtime': 7.9571, 'eval_samples_per_second': 125.673, 'eval_steps_per_second': 7.917, 'epoch': 0.32}
{'loss': 1.1082, 'grad_norm': 0.9584043622016907, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8816463351249695, 'eval_runtime': 8.0385, 'eval_samples_per_second': 124.401, 'eval_steps_per_second': 7.837, 'epoch': 0.36}
{'loss': 1.1174, 'grad_norm': 3.026494264602661, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8829003572463989, 'eval_runtime': 7.9922, 'eval_samples_per_second': 125.121, 'eval_steps_per_second': 7.883, 'epoch': 0.4}
{'loss': 1.1781, 'grad_norm': 1.2189515829086304, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.883449375629425, 'eval_runtime': 7.9898, 'eval_samples_per_second': 125.159, 'eval_steps_per_second': 7.885, 'epoch': 0.44}
{'loss': 1.1037, 'grad_norm': 0.9237631559371948, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8794921636581421, 'eval_runtime': 7.9888, 'eval_samples_per_second': 125.176, 'eval_steps_per_second': 7.886, 'epoch': 0.48}
{'loss': 1.1271, 'grad_norm': 1.2441989183425903, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8806506395339966, 'eval_runtime': 7.9843, 'eval_samples_per_second': 125.246, 'eval_steps_per_second': 7.89, 'epoch': 0.52}
{'loss': 1.075, 'grad_norm': 1.133736252784729, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8746950626373291, 'eval_runtime': 7.9806, 'eval_samples_per_second': 125.304, 'eval_steps_per_second': 7.894, 'epoch': 0.56}
{'loss': 1.074, 'grad_norm': 1.0558916330337524, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8806540369987488, 'eval_runtime': 7.9832, 'eval_samples_per_second': 125.263, 'eval_steps_per_second': 7.892, 'epoch': 0.6}
{'loss': 1.0672, 'grad_norm': 1.131426453590393, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8733161687850952, 'eval_runtime': 7.9885, 'eval_samples_per_second': 125.18, 'eval_steps_per_second': 7.886, 'epoch': 0.64}
{'loss': 1.0705, 'grad_norm': 1.3470596075057983, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8766903281211853, 'eval_runtime': 7.98, 'eval_samples_per_second': 125.313, 'eval_steps_per_second': 7.895, 'epoch': 0.68}
{'loss': 1.0597, 'grad_norm': 1.046615719795227, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8732227683067322, 'eval_runtime': 7.9823, 'eval_samples_per_second': 125.277, 'eval_steps_per_second': 7.892, 'epoch': 0.72}
{'loss': 1.1222, 'grad_norm': 1.071273684501648, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8750061988830566, 'eval_runtime': 7.9824, 'eval_samples_per_second': 125.276, 'eval_steps_per_second': 7.892, 'epoch': 0.76}
{'loss': 1.1138, 'grad_norm': 1.0644805431365967, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8760241270065308, 'eval_runtime': 7.986, 'eval_samples_per_second': 125.22, 'eval_steps_per_second': 7.889, 'epoch': 0.8}
{'loss': 1.1158, 'grad_norm': 1.1467293500900269, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8724648356437683, 'eval_runtime': 7.9778, 'eval_samples_per_second': 125.348, 'eval_steps_per_second': 7.897, 'epoch': 0.84}
{'loss': 1.0313, 'grad_norm': 1.0461680889129639, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8712189793586731, 'eval_runtime': 7.9453, 'eval_samples_per_second': 125.86, 'eval_steps_per_second': 7.929, 'epoch': 0.88}
{'loss': 1.0487, 'grad_norm': 1.1329529285430908, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8727300763130188, 'eval_runtime': 7.9708, 'eval_samples_per_second': 125.457, 'eval_steps_per_second': 7.904, 'epoch': 0.92}
{'loss': 1.0276, 'grad_norm': 1.1500288248062134, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8705995678901672, 'eval_runtime': 7.998, 'eval_samples_per_second': 125.031, 'eval_steps_per_second': 7.877, 'epoch': 0.96}
{'loss': 1.1101, 'grad_norm': 1.0908275842666626, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8699289560317993, 'eval_runtime': 8.0443, 'eval_samples_per_second': 124.312, 'eval_steps_per_second': 7.832, 'epoch': 1.0}
{'train_runtime': 455.8595, 'train_samples_per_second': 21.93, 'train_steps_per_second': 1.371, 'train_loss': 1.183493685913086, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4369523525238037, 0.9671168327331543, 0.9882060289382935, 0.9381256103515625, 0.9273662567138672, 0.9209622740745544, 0.8918471932411194, 0.8925576210021973, 0.8816463351249695, 0.8829003572463989, 0.883449375629425, 0.8794921636581421, 0.8806506395339966, 0.8746950626373291, 0.8806540369987488, 0.8733161687850952, 0.8766903281211853, 0.8732227683067322, 0.8750061988830566, 0.8760241270065308, 0.8724648356437683, 0.8712189793586731, 0.8727300763130188, 0.8705995678901672, 0.8699289560317993], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4369523525238037, 0.9671168327331543, 0.9882060289382935, 0.9381256103515625, 0.9273662567138672, 0.9209622740745544, 0.8918471932411194, 0.8925576210021973, 0.8816463351249695, 0.8829003572463989, 0.883449375629425, 0.8794921636581421, 0.8806506395339966, 0.8746950626373291, 0.8806540369987488, 0.8733161687850952, 0.8766903281211853, 0.8732227683067322, 0.8750061988830566, 0.8760241270065308, 0.8724648356437683, 0.8712189793586731, 0.8727300763130188, 0.8705995678901672, 0.8699289560317993]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760302543640137
current iteration best possible eval_loss (full train run):  -0.8699289560317993
max eval_loss so far:  -0.8668954968452454
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3439 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -1.0639900695583795
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -1.0556798695583662
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -1.0364378728125752
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -1.0343531039724996
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -1.04054001556564
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2555, dtype=torch.float64), tensor(0.0587, dtype=torch.float64), 0, tensor(0.2063, dtype=torch.float64), tensor(0.1052, dtype=torch.float64), 0, tensor(0.1057, dtype=torch.float64), tensor(0.1740, dtype=torch.float64), tensor(0.0946, dtype=torch.float64), 32, 1, 0, 1, 1, 0, 128, 0.0975092467419507, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.2555, dtype=torch.float64), tensor(0.0587, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2063, dtype=torch.float64), tensor(0.1052, dtype=torch.float64), tensor(3.7787e-18, dtype=torch.float64), tensor(0.1057, dtype=torch.float64), tensor(0.1740, dtype=torch.float64), tensor(0.0946, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9751, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.255
  gsm8k: 0.059
  rowan_hellaswag: 0
  sciq: 0.206
  triviaqa: 0.105
  truthfulqa_gen: 0
  wikitext: 0.106
  mmlu: 0.174
  arc_challenge: 0.095

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0975092467419507,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.0975092467419507
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7957, 'grad_norm': 0.6934417486190796, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1786692142486572, 'eval_runtime': 7.2252, 'eval_samples_per_second': 138.404, 'eval_steps_per_second': 8.719, 'epoch': 0.04}
{'loss': 1.3263, 'grad_norm': 0.3945107161998749, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9616950750350952, 'eval_runtime': 7.237, 'eval_samples_per_second': 138.179, 'eval_steps_per_second': 8.705, 'epoch': 0.08}
{'loss': 1.1422, 'grad_norm': 0.2990378737449646, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9154272675514221, 'eval_runtime': 7.2837, 'eval_samples_per_second': 137.293, 'eval_steps_per_second': 8.649, 'epoch': 0.12}
{'loss': 1.1181, 'grad_norm': 0.26717710494995117, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8959711790084839, 'eval_runtime': 7.2695, 'eval_samples_per_second': 137.562, 'eval_steps_per_second': 8.666, 'epoch': 0.16}
{'loss': 1.0764, 'grad_norm': 0.24043075740337372, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8860985040664673, 'eval_runtime': 7.3095, 'eval_samples_per_second': 136.807, 'eval_steps_per_second': 8.619, 'epoch': 0.2}
{'loss': 1.0893, 'grad_norm': 0.20855258405208588, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8899925351142883, 'eval_runtime': 7.3234, 'eval_samples_per_second': 136.55, 'eval_steps_per_second': 8.603, 'epoch': 0.24}
{'loss': 1.042, 'grad_norm': 0.26754191517829895, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8817419409751892, 'eval_runtime': 7.3518, 'eval_samples_per_second': 136.02, 'eval_steps_per_second': 8.569, 'epoch': 0.28}
{'loss': 1.0601, 'grad_norm': 0.22878727316856384, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8793471455574036, 'eval_runtime': 7.362, 'eval_samples_per_second': 135.833, 'eval_steps_per_second': 8.557, 'epoch': 0.32}
{'loss': 1.1058, 'grad_norm': 0.22511281073093414, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8755301237106323, 'eval_runtime': 7.3313, 'eval_samples_per_second': 136.402, 'eval_steps_per_second': 8.593, 'epoch': 0.36}
{'loss': 1.0347, 'grad_norm': 0.23948831856250763, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.873844563961029, 'eval_runtime': 7.3501, 'eval_samples_per_second': 136.052, 'eval_steps_per_second': 8.571, 'epoch': 0.4}
{'loss': 1.0957, 'grad_norm': 0.25597792863845825, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.872485339641571, 'eval_runtime': 7.3246, 'eval_samples_per_second': 136.527, 'eval_steps_per_second': 8.601, 'epoch': 0.44}
{'loss': 1.0093, 'grad_norm': 0.27337655425071716, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8683053851127625, 'eval_runtime': 7.2952, 'eval_samples_per_second': 137.076, 'eval_steps_per_second': 8.636, 'epoch': 0.48}
{'loss': 1.0219, 'grad_norm': 0.21407732367515564, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8664089441299438, 'eval_runtime': 7.2878, 'eval_samples_per_second': 137.216, 'eval_steps_per_second': 8.645, 'epoch': 0.52}
{'loss': 1.0381, 'grad_norm': 0.27598920464515686, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8667609095573425, 'eval_runtime': 7.289, 'eval_samples_per_second': 137.193, 'eval_steps_per_second': 8.643, 'epoch': 0.56}
{'loss': 1.0458, 'grad_norm': 0.2542065680027008, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8695657849311829, 'eval_runtime': 7.2949, 'eval_samples_per_second': 137.082, 'eval_steps_per_second': 8.636, 'epoch': 0.6}
{'loss': 1.0437, 'grad_norm': 0.2849912643432617, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8643211722373962, 'eval_runtime': 7.2823, 'eval_samples_per_second': 137.319, 'eval_steps_per_second': 8.651, 'epoch': 0.64}
{'loss': 0.9696, 'grad_norm': 0.22204247117042542, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8614367246627808, 'eval_runtime': 7.2916, 'eval_samples_per_second': 137.143, 'eval_steps_per_second': 8.64, 'epoch': 0.68}
{'loss': 1.0374, 'grad_norm': 0.2753080725669861, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8631306886672974, 'eval_runtime': 7.2862, 'eval_samples_per_second': 137.245, 'eval_steps_per_second': 8.646, 'epoch': 0.72}
{'loss': 1.0241, 'grad_norm': 0.2690238356590271, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8625630140304565, 'eval_runtime': 7.2709, 'eval_samples_per_second': 137.534, 'eval_steps_per_second': 8.665, 'epoch': 0.76}
{'loss': 1.0216, 'grad_norm': 0.2395065575838089, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8656700253486633, 'eval_runtime': 7.2673, 'eval_samples_per_second': 137.602, 'eval_steps_per_second': 8.669, 'epoch': 0.8}
{'loss': 1.0468, 'grad_norm': 0.32074472308158875, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8662354350090027, 'eval_runtime': 7.2479, 'eval_samples_per_second': 137.971, 'eval_steps_per_second': 8.692, 'epoch': 0.84}
{'loss': 1.0213, 'grad_norm': 0.22516296803951263, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8615479469299316, 'eval_runtime': 7.2556, 'eval_samples_per_second': 137.825, 'eval_steps_per_second': 8.683, 'epoch': 0.88}
{'loss': 1.0257, 'grad_norm': 0.24892112612724304, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8618332743644714, 'eval_runtime': 7.2621, 'eval_samples_per_second': 137.701, 'eval_steps_per_second': 8.675, 'epoch': 0.92}
{'loss': 1.0344, 'grad_norm': 0.28057995438575745, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.861046552658081, 'eval_runtime': 7.2533, 'eval_samples_per_second': 137.869, 'eval_steps_per_second': 8.686, 'epoch': 0.96}
{'loss': 1.0398, 'grad_norm': 0.3252721130847931, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8608999252319336, 'eval_runtime': 7.2386, 'eval_samples_per_second': 138.149, 'eval_steps_per_second': 8.703, 'epoch': 1.0}
{'train_runtime': 390.9875, 'train_samples_per_second': 25.566, 'train_steps_per_second': 1.599, 'train_loss': 1.1306435607910157, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1786692142486572, 0.9616950750350952, 0.9154272675514221, 0.8959711790084839, 0.8860985040664673, 0.8899925351142883, 0.8817419409751892, 0.8793471455574036, 0.8755301237106323, 0.873844563961029, 0.872485339641571, 0.8683053851127625, 0.8664089441299438, 0.8667609095573425, 0.8695657849311829, 0.8643211722373962, 0.8614367246627808, 0.8631306886672974, 0.8625630140304565, 0.8656700253486633, 0.8662354350090027, 0.8615479469299316, 0.8618332743644714, 0.861046552658081, 0.8608999252319336], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1786692142486572, 0.9616950750350952, 0.9154272675514221, 0.8959711790084839, 0.8860985040664673, 0.8899925351142883, 0.8817419409751892, 0.8793471455574036, 0.8755301237106323, 0.873844563961029, 0.872485339641571, 0.8683053851127625, 0.8664089441299438, 0.8667609095573425, 0.8695657849311829, 0.8643211722373962, 0.8614367246627808, 0.8631306886672974, 0.8625630140304565, 0.8656700253486633, 0.8662354350090027, 0.8615479469299316, 0.8618332743644714, 0.861046552658081, 0.8608999252319336]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.4149739742279053
current iteration best possible eval_loss (full train run):  -0.8608999252319336
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7870 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -1.1091102737568534
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -1.0189025371732467
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -1.0407716002267353
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -1.0958198259755834
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -1.0174278940113295
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.5492, dtype=torch.float64), 0, 0, 0, tensor(0.1032, dtype=torch.float64), 0, 0, tensor(0.3465, dtype=torch.float64), 0, 31, 0, 1, 1, 1, 1, 5, 0.001989938248713737, 42.40935210380895, 1]
normalized proposed parameters for next round by BO: [tensor(0.5492, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.6385e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1032, dtype=torch.float64), tensor(0.0010, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3465, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9840, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0390, dtype=torch.float64), tensor(0.0199, dtype=torch.float64), tensor(0.8835, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.549
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.103
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.347
  arc_challenge: 0

LoRA Parameters:
  lora_r: (5,)
  lora_dropout: (0.001989938248713737,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (42.40935210380895,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  5
lora dropout:  0.001989938248713737
lora alpha:  42.40935210380895
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 9,364,480 || all params: 8,039,625,728 || trainable%: 0.1165
length of training data:  9989
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7168, 'grad_norm': 2.6756887435913086, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2898253202438354, 'eval_runtime': 7.8605, 'eval_samples_per_second': 127.218, 'eval_steps_per_second': 8.015, 'epoch': 0.04}
{'loss': 1.2374, 'grad_norm': 2.040635824203491, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.400623083114624, 'eval_runtime': 7.8529, 'eval_samples_per_second': 127.342, 'eval_steps_per_second': 8.023, 'epoch': 0.08}
{'loss': 1.1287, 'grad_norm': 1.5594757795333862, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3401113748550415, 'eval_runtime': 7.8513, 'eval_samples_per_second': 127.367, 'eval_steps_per_second': 8.024, 'epoch': 0.12}
{'loss': 1.1013, 'grad_norm': 1.3801913261413574, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3175759315490723, 'eval_runtime': 7.861, 'eval_samples_per_second': 127.21, 'eval_steps_per_second': 8.014, 'epoch': 0.16}
{'loss': 1.0891, 'grad_norm': 1.09067702293396, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2350962162017822, 'eval_runtime': 7.8723, 'eval_samples_per_second': 127.028, 'eval_steps_per_second': 8.003, 'epoch': 0.2}
{'loss': 1.0446, 'grad_norm': 2.310638189315796, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2384480237960815, 'eval_runtime': 7.8997, 'eval_samples_per_second': 126.588, 'eval_steps_per_second': 7.975, 'epoch': 0.24}
{'loss': 1.0279, 'grad_norm': 0.8876395225524902, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2876216173171997, 'eval_runtime': 7.9177, 'eval_samples_per_second': 126.3, 'eval_steps_per_second': 7.957, 'epoch': 0.28}
{'loss': 1.0136, 'grad_norm': 0.9223029613494873, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.278211236000061, 'eval_runtime': 7.9377, 'eval_samples_per_second': 125.981, 'eval_steps_per_second': 7.937, 'epoch': 0.32}
{'loss': 1.0449, 'grad_norm': 1.1787062883377075, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3061615228652954, 'eval_runtime': 7.9653, 'eval_samples_per_second': 125.544, 'eval_steps_per_second': 7.909, 'epoch': 0.36}
{'loss': 0.9659, 'grad_norm': 1.0086860656738281, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2985886335372925, 'eval_runtime': 7.9894, 'eval_samples_per_second': 125.166, 'eval_steps_per_second': 7.885, 'epoch': 0.4}
{'loss': 0.9959, 'grad_norm': 1.109296202659607, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2836874723434448, 'eval_runtime': 7.9809, 'eval_samples_per_second': 125.299, 'eval_steps_per_second': 7.894, 'epoch': 0.44}
{'loss': 0.9787, 'grad_norm': 1.0567728281021118, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.314245343208313, 'eval_runtime': 7.9868, 'eval_samples_per_second': 125.206, 'eval_steps_per_second': 7.888, 'epoch': 0.48}
{'loss': 1.0046, 'grad_norm': 1.9316636323928833, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.28178071975708, 'eval_runtime': 7.9502, 'eval_samples_per_second': 125.783, 'eval_steps_per_second': 7.924, 'epoch': 0.52}
{'loss': 0.9909, 'grad_norm': 0.9589663743972778, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3083549737930298, 'eval_runtime': 7.9161, 'eval_samples_per_second': 126.325, 'eval_steps_per_second': 7.958, 'epoch': 0.56}
{'loss': 0.9846, 'grad_norm': 1.4512312412261963, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2850353717803955, 'eval_runtime': 7.9001, 'eval_samples_per_second': 126.58, 'eval_steps_per_second': 7.975, 'epoch': 0.6}
{'loss': 0.9632, 'grad_norm': 0.8822333216667175, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.286862850189209, 'eval_runtime': 7.8994, 'eval_samples_per_second': 126.592, 'eval_steps_per_second': 7.975, 'epoch': 0.64}
{'loss': 0.9686, 'grad_norm': 0.9865711331367493, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.301344633102417, 'eval_runtime': 7.8999, 'eval_samples_per_second': 126.583, 'eval_steps_per_second': 7.975, 'epoch': 0.68}
{'loss': 0.9214, 'grad_norm': 1.7712011337280273, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3142377138137817, 'eval_runtime': 7.8662, 'eval_samples_per_second': 127.126, 'eval_steps_per_second': 8.009, 'epoch': 0.72}
{'loss': 0.9665, 'grad_norm': 0.9915795922279358, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.312225103378296, 'eval_runtime': 7.8983, 'eval_samples_per_second': 126.61, 'eval_steps_per_second': 7.976, 'epoch': 0.76}
{'loss': 0.9537, 'grad_norm': 1.1338527202606201, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3155754804611206, 'eval_runtime': 7.8823, 'eval_samples_per_second': 126.867, 'eval_steps_per_second': 7.993, 'epoch': 0.8}
{'loss': 0.9782, 'grad_norm': 0.9359028935432434, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.312056064605713, 'eval_runtime': 7.8774, 'eval_samples_per_second': 126.945, 'eval_steps_per_second': 7.998, 'epoch': 0.84}
{'loss': 0.9623, 'grad_norm': 0.9832417368888855, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3307033777236938, 'eval_runtime': 7.8733, 'eval_samples_per_second': 127.011, 'eval_steps_per_second': 8.002, 'epoch': 0.88}
{'loss': 0.9168, 'grad_norm': 1.1972821950912476, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3224958181381226, 'eval_runtime': 7.9041, 'eval_samples_per_second': 126.517, 'eval_steps_per_second': 7.971, 'epoch': 0.92}
{'loss': 0.9327, 'grad_norm': 0.9046595692634583, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3227072954177856, 'eval_runtime': 7.8879, 'eval_samples_per_second': 126.776, 'eval_steps_per_second': 7.987, 'epoch': 0.96}
{'loss': 0.9232, 'grad_norm': 1.9074541330337524, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3233754634857178, 'eval_runtime': 7.8904, 'eval_samples_per_second': 126.737, 'eval_steps_per_second': 7.984, 'epoch': 1.0}
{'train_runtime': 430.8477, 'train_samples_per_second': 23.185, 'train_steps_per_second': 1.451, 'train_loss': 1.072455648803711, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2898253202438354, 1.400623083114624, 1.3401113748550415, 1.3175759315490723, 1.2350962162017822, 1.2384480237960815, 1.2876216173171997, 1.278211236000061, 1.3061615228652954, 1.2985886335372925, 1.2836874723434448, 1.314245343208313, 1.28178071975708, 1.3083549737930298, 1.2850353717803955, 1.286862850189209, 1.301344633102417, 1.3142377138137817, 1.312225103378296, 1.3155754804611206, 1.312056064605713, 1.3307033777236938, 1.3224958181381226, 1.3227072954177856, 1.3233754634857178], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2898253202438354, 1.400623083114624, 1.3401113748550415, 1.3175759315490723, 1.2350962162017822, 1.2384480237960815, 1.2876216173171997, 1.278211236000061, 1.3061615228652954, 1.2985886335372925, 1.2836874723434448, 1.314245343208313, 1.28178071975708, 1.3083549737930298, 1.2850353717803955, 1.286862850189209, 1.301344633102417, 1.3142377138137817, 1.312225103378296, 1.3155754804611206, 1.312056064605713, 1.3307033777236938, 1.3224958181381226, 1.3227072954177856, 1.3233754634857178]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760271549224854
current iteration best possible eval_loss (full train run):  -1.3233754634857178
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4280 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -1.0352893083067054
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -1.0718010956253872
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -1.0594939720371068
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -1.0526943632816015
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -1.003127855851788
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1760, dtype=torch.float64), tensor(0.0337, dtype=torch.float64), tensor(0.1092, dtype=torch.float64), 0, tensor(0.1428, dtype=torch.float64), tensor(0.3161, dtype=torch.float64), tensor(0.1557, dtype=torch.float64), tensor(0.0665, dtype=torch.float64), 29, 0, 1, 1, 0, 1, 51, 1.7347234759768077e-19, 7.69229659836283, 1]
normalized proposed parameters for next round by BO: [tensor(3.7074e-17, dtype=torch.float64), tensor(0.1760, dtype=torch.float64), tensor(0.0337, dtype=torch.float64), tensor(0.1092, dtype=torch.float64), tensor(1.3391e-17, dtype=torch.float64), tensor(0.1428, dtype=torch.float64), tensor(0.3161, dtype=torch.float64), tensor(0.1557, dtype=torch.float64), tensor(0.0665, dtype=torch.float64), tensor(0.9137, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3986, dtype=torch.float64), tensor(1.7347e-18, dtype=torch.float64), tensor(0.1603, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.176
  rowan_hellaswag: 0.034
  sciq: 0.109
  triviaqa: 0
  truthfulqa_gen: 0.143
  wikitext: 0.316
  mmlu: 0.156
  arc_challenge: 0.067

LoRA Parameters:
  lora_r: (51,)
  lora_dropout: (1.7347234759768077e-19,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (7.69229659836283,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  51
lora dropout:  1.7347234759768077e-19
lora alpha:  7.69229659836283
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 62,094,336 || all params: 8,092,355,584 || trainable%: 0.7673
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0719, 'grad_norm': 0.20736272633075714, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2291486263275146, 'eval_runtime': 7.2809, 'eval_samples_per_second': 137.346, 'eval_steps_per_second': 8.653, 'epoch': 0.04}
{'loss': 1.9841, 'grad_norm': 0.20295968651771545, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2303528785705566, 'eval_runtime': 7.2771, 'eval_samples_per_second': 137.418, 'eval_steps_per_second': 8.657, 'epoch': 0.08}
{'loss': 1.5992, 'grad_norm': 0.19795311987400055, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1355026960372925, 'eval_runtime': 7.2812, 'eval_samples_per_second': 137.341, 'eval_steps_per_second': 8.652, 'epoch': 0.12}
{'loss': 1.6021, 'grad_norm': 0.24800577759742737, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1133955717086792, 'eval_runtime': 7.2842, 'eval_samples_per_second': 137.284, 'eval_steps_per_second': 8.649, 'epoch': 0.16}
{'loss': 1.4249, 'grad_norm': 0.2050735056400299, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.079277515411377, 'eval_runtime': 7.2765, 'eval_samples_per_second': 137.429, 'eval_steps_per_second': 8.658, 'epoch': 0.2}
{'loss': 1.4937, 'grad_norm': 0.15765468776226044, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0689384937286377, 'eval_runtime': 7.2934, 'eval_samples_per_second': 137.11, 'eval_steps_per_second': 8.638, 'epoch': 0.24}
{'loss': 1.4267, 'grad_norm': 0.14769676327705383, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0123318433761597, 'eval_runtime': 7.2769, 'eval_samples_per_second': 137.42, 'eval_steps_per_second': 8.657, 'epoch': 0.28}
{'loss': 1.3495, 'grad_norm': 0.2497275471687317, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.005863070487976, 'eval_runtime': 7.2872, 'eval_samples_per_second': 137.226, 'eval_steps_per_second': 8.645, 'epoch': 0.32}
{'loss': 1.3555, 'grad_norm': 0.21261177957057953, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9884070158004761, 'eval_runtime': 7.2882, 'eval_samples_per_second': 137.209, 'eval_steps_per_second': 8.644, 'epoch': 0.36}
{'loss': 1.3501, 'grad_norm': 0.20231257379055023, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9917548894882202, 'eval_runtime': 7.2921, 'eval_samples_per_second': 137.134, 'eval_steps_per_second': 8.639, 'epoch': 0.4}
{'loss': 1.4238, 'grad_norm': 0.2097512185573578, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9837182760238647, 'eval_runtime': 7.2946, 'eval_samples_per_second': 137.088, 'eval_steps_per_second': 8.637, 'epoch': 0.44}
{'loss': 1.3288, 'grad_norm': 0.17354139685630798, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9719980359077454, 'eval_runtime': 7.3533, 'eval_samples_per_second': 135.992, 'eval_steps_per_second': 8.568, 'epoch': 0.48}
{'loss': 1.3301, 'grad_norm': 0.1721799522638321, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.973720371723175, 'eval_runtime': 7.3495, 'eval_samples_per_second': 136.064, 'eval_steps_per_second': 8.572, 'epoch': 0.52}
{'loss': 1.3679, 'grad_norm': 0.2602393925189972, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9614638686180115, 'eval_runtime': 7.391, 'eval_samples_per_second': 135.3, 'eval_steps_per_second': 8.524, 'epoch': 0.56}
{'loss': 1.3416, 'grad_norm': 0.18791843950748444, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9625645279884338, 'eval_runtime': 7.3847, 'eval_samples_per_second': 135.415, 'eval_steps_per_second': 8.531, 'epoch': 0.6}
{'loss': 1.274, 'grad_norm': 0.21099551022052765, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.948136031627655, 'eval_runtime': 7.3939, 'eval_samples_per_second': 135.247, 'eval_steps_per_second': 8.521, 'epoch': 0.64}
{'loss': 1.3306, 'grad_norm': 0.15698258578777313, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.946610689163208, 'eval_runtime': 7.3916, 'eval_samples_per_second': 135.288, 'eval_steps_per_second': 8.523, 'epoch': 0.68}
{'loss': 1.2787, 'grad_norm': 0.18914932012557983, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9473418593406677, 'eval_runtime': 7.4018, 'eval_samples_per_second': 135.103, 'eval_steps_per_second': 8.511, 'epoch': 0.72}
{'loss': 1.276, 'grad_norm': 0.19645868241786957, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.939536988735199, 'eval_runtime': 7.3909, 'eval_samples_per_second': 135.302, 'eval_steps_per_second': 8.524, 'epoch': 0.76}
{'loss': 1.2853, 'grad_norm': 0.16184300184249878, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.942137598991394, 'eval_runtime': 7.3849, 'eval_samples_per_second': 135.411, 'eval_steps_per_second': 8.531, 'epoch': 0.8}
{'loss': 1.3133, 'grad_norm': 0.18126608431339264, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9374586939811707, 'eval_runtime': 7.4199, 'eval_samples_per_second': 134.773, 'eval_steps_per_second': 8.491, 'epoch': 0.84}
{'loss': 1.3487, 'grad_norm': 0.1849302351474762, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9324092864990234, 'eval_runtime': 7.4185, 'eval_samples_per_second': 134.798, 'eval_steps_per_second': 8.492, 'epoch': 0.88}
{'loss': 1.3615, 'grad_norm': 0.18170113861560822, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9300876259803772, 'eval_runtime': 7.3384, 'eval_samples_per_second': 136.27, 'eval_steps_per_second': 8.585, 'epoch': 0.92}
{'loss': 1.3273, 'grad_norm': 0.17098011076450348, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9298275113105774, 'eval_runtime': 7.3291, 'eval_samples_per_second': 136.442, 'eval_steps_per_second': 8.596, 'epoch': 0.96}
{'loss': 1.2425, 'grad_norm': 0.19213369488716125, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9297061562538147, 'eval_runtime': 7.3246, 'eval_samples_per_second': 136.526, 'eval_steps_per_second': 8.601, 'epoch': 1.0}
{'train_runtime': 416.4481, 'train_samples_per_second': 24.003, 'train_steps_per_second': 1.501, 'train_loss': 1.459512600708008, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2291486263275146, 1.2303528785705566, 1.1355026960372925, 1.1133955717086792, 1.079277515411377, 1.0689384937286377, 1.0123318433761597, 1.005863070487976, 0.9884070158004761, 0.9917548894882202, 0.9837182760238647, 0.9719980359077454, 0.973720371723175, 0.9614638686180115, 0.9625645279884338, 0.948136031627655, 0.946610689163208, 0.9473418593406677, 0.939536988735199, 0.942137598991394, 0.9374586939811707, 0.9324092864990234, 0.9300876259803772, 0.9298275113105774, 0.9297061562538147], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.2291486263275146, 1.2303528785705566, 1.1355026960372925, 1.1133955717086792, 1.079277515411377, 1.0689384937286377, 1.0123318433761597, 1.005863070487976, 0.9884070158004761, 0.9917548894882202, 0.9837182760238647, 0.9719980359077454, 0.973720371723175, 0.9614638686180115, 0.9625645279884338, 0.948136031627655, 0.946610689163208, 0.9473418593406677, 0.939536988735199, 0.942137598991394, 0.9374586939811707, 0.9324092864990234, 0.9300876259803772, 0.9298275113105774, 0.9297061562538147]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2985050678253174
current iteration best possible eval_loss (full train run):  -0.9297061562538147
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9260 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.9595236186163727
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -1.058892081888402
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.9808790553159215
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -1.0595109017112456
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -1.058173945044609
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4425, dtype=torch.float64), tensor(0.0114, dtype=torch.float64), 0, tensor(0.4813, dtype=torch.float64), 0, 0, 0, tensor(0.0648, dtype=torch.float64), 32, 1, 1, 0, 1, 1, 2, 0.07255222982946861, 30.458116050975235, 0]
normalized proposed parameters for next round by BO: [tensor(5.2522e-18, dtype=torch.float64), tensor(0.4425, dtype=torch.float64), tensor(0.0114, dtype=torch.float64), tensor(6.7632e-18, dtype=torch.float64), tensor(0.4813, dtype=torch.float64), tensor(1.0984e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.8708e-18, dtype=torch.float64), tensor(0.0648, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.7255, dtype=torch.float64), tensor(0.6345, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.442
  rowan_hellaswag: 0.011
  sciq: 0
  triviaqa: 0.481
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.065

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.07255222982946861,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (30.458116050975235,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  2
lora dropout:  0.07255222982946861
lora alpha:  30.458116050975235
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,211,264 || all params: 8,033,472,512 || trainable%: 0.0400
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.3734, 'grad_norm': 3.6539833545684814, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7294676303863525, 'eval_runtime': 7.5054, 'eval_samples_per_second': 133.238, 'eval_steps_per_second': 8.394, 'epoch': 0.04}
{'loss': 1.0588, 'grad_norm': 2.1818764209747314, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9685347080230713, 'eval_runtime': 7.4813, 'eval_samples_per_second': 133.667, 'eval_steps_per_second': 8.421, 'epoch': 0.08}
{'loss': 0.9216, 'grad_norm': 1.5474210977554321, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9471461176872253, 'eval_runtime': 7.4867, 'eval_samples_per_second': 133.57, 'eval_steps_per_second': 8.415, 'epoch': 0.12}
{'loss': 0.9598, 'grad_norm': 1.1251708269119263, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9006476998329163, 'eval_runtime': 7.4837, 'eval_samples_per_second': 133.624, 'eval_steps_per_second': 8.418, 'epoch': 0.16}
{'loss': 0.9025, 'grad_norm': 1.1308711767196655, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8913745284080505, 'eval_runtime': 7.4932, 'eval_samples_per_second': 133.455, 'eval_steps_per_second': 8.408, 'epoch': 0.2}
{'loss': 0.9118, 'grad_norm': 1.1824558973312378, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.89412522315979, 'eval_runtime': 7.494, 'eval_samples_per_second': 133.439, 'eval_steps_per_second': 8.407, 'epoch': 0.24}
{'loss': 0.924, 'grad_norm': 1.060612678527832, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9094048738479614, 'eval_runtime': 7.5015, 'eval_samples_per_second': 133.306, 'eval_steps_per_second': 8.398, 'epoch': 0.28}
{'loss': 0.8778, 'grad_norm': 1.0521526336669922, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8897502422332764, 'eval_runtime': 7.5077, 'eval_samples_per_second': 133.196, 'eval_steps_per_second': 8.391, 'epoch': 0.32}
{'loss': 0.876, 'grad_norm': 1.120491862297058, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8896342515945435, 'eval_runtime': 7.5084, 'eval_samples_per_second': 133.184, 'eval_steps_per_second': 8.391, 'epoch': 0.36}
{'loss': 0.8762, 'grad_norm': 1.0622985363006592, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8900733590126038, 'eval_runtime': 7.5011, 'eval_samples_per_second': 133.314, 'eval_steps_per_second': 8.399, 'epoch': 0.4}
{'loss': 0.8675, 'grad_norm': 1.0255253314971924, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8950306177139282, 'eval_runtime': 7.5113, 'eval_samples_per_second': 133.133, 'eval_steps_per_second': 8.387, 'epoch': 0.44}
{'loss': 0.8435, 'grad_norm': 1.111473798751831, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.882559061050415, 'eval_runtime': 7.5033, 'eval_samples_per_second': 133.274, 'eval_steps_per_second': 8.396, 'epoch': 0.48}
{'loss': 0.8461, 'grad_norm': 1.4062985181808472, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.89106684923172, 'eval_runtime': 7.4999, 'eval_samples_per_second': 133.335, 'eval_steps_per_second': 8.4, 'epoch': 0.52}
{'loss': 0.8599, 'grad_norm': 1.1132766008377075, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8740935325622559, 'eval_runtime': 7.5001, 'eval_samples_per_second': 133.331, 'eval_steps_per_second': 8.4, 'epoch': 0.56}
{'loss': 0.8759, 'grad_norm': 1.386035680770874, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8867446184158325, 'eval_runtime': 7.4984, 'eval_samples_per_second': 133.362, 'eval_steps_per_second': 8.402, 'epoch': 0.6}
{'loss': 0.8471, 'grad_norm': 1.122747540473938, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8858643770217896, 'eval_runtime': 7.5092, 'eval_samples_per_second': 133.169, 'eval_steps_per_second': 8.39, 'epoch': 0.64}
{'loss': 0.8565, 'grad_norm': 1.008396863937378, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8767035007476807, 'eval_runtime': 7.4973, 'eval_samples_per_second': 133.381, 'eval_steps_per_second': 8.403, 'epoch': 0.68}
{'loss': 0.8384, 'grad_norm': 1.2496092319488525, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8813995122909546, 'eval_runtime': 7.5018, 'eval_samples_per_second': 133.302, 'eval_steps_per_second': 8.398, 'epoch': 0.72}
{'loss': 0.8639, 'grad_norm': 1.0351322889328003, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8807121515274048, 'eval_runtime': 7.5086, 'eval_samples_per_second': 133.18, 'eval_steps_per_second': 8.39, 'epoch': 0.76}
{'loss': 0.87, 'grad_norm': 1.1259948015213013, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.879599392414093, 'eval_runtime': 7.5022, 'eval_samples_per_second': 133.294, 'eval_steps_per_second': 8.398, 'epoch': 0.8}
{'loss': 0.8522, 'grad_norm': 0.9774422645568848, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8746913075447083, 'eval_runtime': 7.5021, 'eval_samples_per_second': 133.295, 'eval_steps_per_second': 8.398, 'epoch': 0.84}
{'loss': 0.845, 'grad_norm': 1.2696571350097656, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.873657763004303, 'eval_runtime': 7.5505, 'eval_samples_per_second': 132.442, 'eval_steps_per_second': 8.344, 'epoch': 0.88}
{'loss': 0.8366, 'grad_norm': 1.2753177881240845, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8733376264572144, 'eval_runtime': 7.607, 'eval_samples_per_second': 131.458, 'eval_steps_per_second': 8.282, 'epoch': 0.92}
{'loss': 0.8658, 'grad_norm': 1.1453948020935059, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.873932957649231, 'eval_runtime': 7.621, 'eval_samples_per_second': 131.216, 'eval_steps_per_second': 8.267, 'epoch': 0.96}
{'loss': 0.8383, 'grad_norm': 1.1603773832321167, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8737995624542236, 'eval_runtime': 7.6353, 'eval_samples_per_second': 130.97, 'eval_steps_per_second': 8.251, 'epoch': 1.0}
{'train_runtime': 434.7323, 'train_samples_per_second': 22.998, 'train_steps_per_second': 1.438, 'train_loss': 0.939543701171875, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7294676303863525, 0.9685347080230713, 0.9471461176872253, 0.9006476998329163, 0.8913745284080505, 0.89412522315979, 0.9094048738479614, 0.8897502422332764, 0.8896342515945435, 0.8900733590126038, 0.8950306177139282, 0.882559061050415, 0.89106684923172, 0.8740935325622559, 0.8867446184158325, 0.8858643770217896, 0.8767035007476807, 0.8813995122909546, 0.8807121515274048, 0.879599392414093, 0.8746913075447083, 0.873657763004303, 0.8733376264572144, 0.873932957649231, 0.8737995624542236], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7294676303863525, 0.9685347080230713, 0.9471461176872253, 0.9006476998329163, 0.8913745284080505, 0.89412522315979, 0.9094048738479614, 0.8897502422332764, 0.8896342515945435, 0.8900733590126038, 0.8950306177139282, 0.882559061050415, 0.89106684923172, 0.8740935325622559, 0.8867446184158325, 0.8858643770217896, 0.8767035007476807, 0.8813995122909546, 0.8807121515274048, 0.879599392414093, 0.8746913075447083, 0.873657763004303, 0.8733376264572144, 0.873932957649231, 0.8737995624542236]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760290622711182
current iteration best possible eval_loss (full train run):  -0.8737995624542236
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5758 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -1.0234025653907706
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -1.0356788403837793
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -1.0553824525984674
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -1.062454427937831
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -1.0963465124964316
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1466, dtype=torch.float64), 0, tensor(0.3184, dtype=torch.float64), tensor(0.0953, dtype=torch.float64), 0, tensor(0.3630, dtype=torch.float64), tensor(0.0767, dtype=torch.float64), 32, 0, 1, 1, 0, 1, 2, 0.1, 30.769741866916824, 1]
normalized proposed parameters for next round by BO: [tensor(5.8653e-17, dtype=torch.float64), tensor(1.3032e-18, dtype=torch.float64), tensor(0.1466, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3184, dtype=torch.float64), tensor(0.0953, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3630, dtype=torch.float64), tensor(0.0767, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6410, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.147
  sciq: 0
  triviaqa: 0.318
  truthfulqa_gen: 0.095
  wikitext: 0
  mmlu: 0.363
  arc_challenge: 0.077

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (30.769741866916824,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  30.769741866916824
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,686,976 || all params: 8,032,948,224 || trainable%: 0.0334
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0211, 'grad_norm': 3.613832473754883, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5474016666412354, 'eval_runtime': 7.1338, 'eval_samples_per_second': 140.178, 'eval_steps_per_second': 8.831, 'epoch': 0.04}
{'loss': 1.6525, 'grad_norm': 2.3201167583465576, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.081588625907898, 'eval_runtime': 7.1022, 'eval_samples_per_second': 140.802, 'eval_steps_per_second': 8.871, 'epoch': 0.08}
{'loss': 1.4496, 'grad_norm': 1.491241693496704, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.987438440322876, 'eval_runtime': 7.1137, 'eval_samples_per_second': 140.573, 'eval_steps_per_second': 8.856, 'epoch': 0.12}
{'loss': 1.3367, 'grad_norm': 1.2812464237213135, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9583613872528076, 'eval_runtime': 7.1782, 'eval_samples_per_second': 139.311, 'eval_steps_per_second': 8.777, 'epoch': 0.16}
{'loss': 1.3486, 'grad_norm': 1.2638556957244873, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.938793957233429, 'eval_runtime': 7.1918, 'eval_samples_per_second': 139.047, 'eval_steps_per_second': 8.76, 'epoch': 0.2}
{'loss': 1.3249, 'grad_norm': 1.369925856590271, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9197542071342468, 'eval_runtime': 7.1867, 'eval_samples_per_second': 139.145, 'eval_steps_per_second': 8.766, 'epoch': 0.24}
{'loss': 1.2828, 'grad_norm': 1.3444844484329224, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8922756314277649, 'eval_runtime': 7.1881, 'eval_samples_per_second': 139.119, 'eval_steps_per_second': 8.764, 'epoch': 0.28}
{'loss': 1.3289, 'grad_norm': 1.7224094867706299, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8895124793052673, 'eval_runtime': 7.1844, 'eval_samples_per_second': 139.19, 'eval_steps_per_second': 8.769, 'epoch': 0.32}
{'loss': 1.3078, 'grad_norm': 1.1442856788635254, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8836488127708435, 'eval_runtime': 7.199, 'eval_samples_per_second': 138.909, 'eval_steps_per_second': 8.751, 'epoch': 0.36}
{'loss': 1.2738, 'grad_norm': 1.2481509447097778, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8820174932479858, 'eval_runtime': 7.1859, 'eval_samples_per_second': 139.162, 'eval_steps_per_second': 8.767, 'epoch': 0.4}
{'loss': 1.2635, 'grad_norm': 1.5529865026474, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8886663913726807, 'eval_runtime': 7.1947, 'eval_samples_per_second': 138.99, 'eval_steps_per_second': 8.756, 'epoch': 0.44}
{'loss': 1.3083, 'grad_norm': 1.2199740409851074, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8845970630645752, 'eval_runtime': 7.197, 'eval_samples_per_second': 138.947, 'eval_steps_per_second': 8.754, 'epoch': 0.48}
{'loss': 1.2761, 'grad_norm': 1.4106096029281616, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8870216012001038, 'eval_runtime': 7.1891, 'eval_samples_per_second': 139.1, 'eval_steps_per_second': 8.763, 'epoch': 0.52}
{'loss': 1.2841, 'grad_norm': 1.1622300148010254, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8776159286499023, 'eval_runtime': 7.1552, 'eval_samples_per_second': 139.759, 'eval_steps_per_second': 8.805, 'epoch': 0.56}
{'loss': 1.3203, 'grad_norm': 1.2076208591461182, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8820251822471619, 'eval_runtime': 7.1514, 'eval_samples_per_second': 139.833, 'eval_steps_per_second': 8.809, 'epoch': 0.6}
{'loss': 1.2834, 'grad_norm': 1.0794519186019897, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8756698966026306, 'eval_runtime': 7.1937, 'eval_samples_per_second': 139.011, 'eval_steps_per_second': 8.758, 'epoch': 0.64}
{'loss': 1.238, 'grad_norm': 1.3158793449401855, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8739848136901855, 'eval_runtime': 7.2497, 'eval_samples_per_second': 137.937, 'eval_steps_per_second': 8.69, 'epoch': 0.68}
{'loss': 1.2746, 'grad_norm': 1.03805410861969, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8734229803085327, 'eval_runtime': 7.2636, 'eval_samples_per_second': 137.672, 'eval_steps_per_second': 8.673, 'epoch': 0.72}
{'loss': 1.2984, 'grad_norm': 1.4956412315368652, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.875502347946167, 'eval_runtime': 7.2699, 'eval_samples_per_second': 137.553, 'eval_steps_per_second': 8.666, 'epoch': 0.76}
{'loss': 1.2817, 'grad_norm': 1.2294844388961792, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8734821081161499, 'eval_runtime': 7.245, 'eval_samples_per_second': 138.026, 'eval_steps_per_second': 8.696, 'epoch': 0.8}
{'loss': 1.2531, 'grad_norm': 1.4866386651992798, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8727198839187622, 'eval_runtime': 7.2601, 'eval_samples_per_second': 137.74, 'eval_steps_per_second': 8.678, 'epoch': 0.84}
{'loss': 1.2657, 'grad_norm': 1.3750996589660645, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8734341859817505, 'eval_runtime': 7.2727, 'eval_samples_per_second': 137.501, 'eval_steps_per_second': 8.663, 'epoch': 0.88}
{'loss': 1.244, 'grad_norm': 1.0235060453414917, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8718001246452332, 'eval_runtime': 7.2439, 'eval_samples_per_second': 138.047, 'eval_steps_per_second': 8.697, 'epoch': 0.92}
{'loss': 1.2318, 'grad_norm': 1.2663917541503906, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8691291213035583, 'eval_runtime': 7.1931, 'eval_samples_per_second': 139.023, 'eval_steps_per_second': 8.758, 'epoch': 0.96}
{'loss': 1.2116, 'grad_norm': 1.1932456493377686, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.869259774684906, 'eval_runtime': 7.1906, 'eval_samples_per_second': 139.071, 'eval_steps_per_second': 8.761, 'epoch': 1.0}
{'train_runtime': 410.0082, 'train_samples_per_second': 24.382, 'train_steps_per_second': 1.524, 'train_loss': 1.3744564178466796, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5474016666412354, 1.081588625907898, 0.987438440322876, 0.9583613872528076, 0.938793957233429, 0.9197542071342468, 0.8922756314277649, 0.8895124793052673, 0.8836488127708435, 0.8820174932479858, 0.8886663913726807, 0.8845970630645752, 0.8870216012001038, 0.8776159286499023, 0.8820251822471619, 0.8756698966026306, 0.8739848136901855, 0.8734229803085327, 0.875502347946167, 0.8734821081161499, 0.8727198839187622, 0.8734341859817505, 0.8718001246452332, 0.8691291213035583, 0.869259774684906], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5474016666412354, 1.081588625907898, 0.987438440322876, 0.9583613872528076, 0.938793957233429, 0.9197542071342468, 0.8922756314277649, 0.8895124793052673, 0.8836488127708435, 0.8820174932479858, 0.8886663913726807, 0.8845970630645752, 0.8870216012001038, 0.8776159286499023, 0.8820251822471619, 0.8756698966026306, 0.8739848136901855, 0.8734229803085327, 0.875502347946167, 0.8734821081161499, 0.8727198839187622, 0.8734341859817505, 0.8718001246452332, 0.8691291213035583, 0.869259774684906]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.07602858543396
current iteration best possible eval_loss (full train run):  -0.869259774684906
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182, -1.07602858543396]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4608 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -1.0694313266411304
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -1.068042828710909
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -1.069861803118119
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -1.069482920190403
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -1.1326230216278554
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0340, dtype=torch.float64), tensor(0.0965, dtype=torch.float64), tensor(0.0759, dtype=torch.float64), tensor(0.6757, dtype=torch.float64), 0, tensor(0.0878, dtype=torch.float64), tensor(0.0301, dtype=torch.float64), 32, 0, 0, 1, 0, 1, 2, 0.0538785603495793, 36.404007104378394, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0340, dtype=torch.float64), tensor(0.0965, dtype=torch.float64), tensor(0.0759, dtype=torch.float64), tensor(0.6757, dtype=torch.float64), tensor(7.6367e-18, dtype=torch.float64), tensor(0.0878, dtype=torch.float64), tensor(0.0301, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.5388, dtype=torch.float64), tensor(0.7584, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.034
  sciq: 0.097
  triviaqa: 0.076
  truthfulqa_gen: 0.676
  wikitext: 0
  mmlu: 0.088
  arc_challenge: 0.03

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0538785603495793,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (36.404007104378394,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.0538785603495793
lora alpha:  36.404007104378394
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,359,296 || all params: 8,032,620,544 || trainable%: 0.0294
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1606, 'grad_norm': 6.479968547821045, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.849241018295288, 'eval_runtime': 6.9368, 'eval_samples_per_second': 144.158, 'eval_steps_per_second': 9.082, 'epoch': 0.04}
{'loss': 1.4162, 'grad_norm': 2.873657464981079, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1348872184753418, 'eval_runtime': 6.9501, 'eval_samples_per_second': 143.882, 'eval_steps_per_second': 9.065, 'epoch': 0.08}
{'loss': 1.0747, 'grad_norm': 1.9614588022232056, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0018155574798584, 'eval_runtime': 6.9571, 'eval_samples_per_second': 143.738, 'eval_steps_per_second': 9.055, 'epoch': 0.12}
{'loss': 0.9192, 'grad_norm': 2.0209975242614746, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.966491162776947, 'eval_runtime': 6.9633, 'eval_samples_per_second': 143.61, 'eval_steps_per_second': 9.047, 'epoch': 0.16}
{'loss': 0.8403, 'grad_norm': 1.4871231317520142, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9756388068199158, 'eval_runtime': 6.964, 'eval_samples_per_second': 143.595, 'eval_steps_per_second': 9.047, 'epoch': 0.2}
{'loss': 0.7533, 'grad_norm': 1.8115564584732056, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9427725076675415, 'eval_runtime': 6.9717, 'eval_samples_per_second': 143.438, 'eval_steps_per_second': 9.037, 'epoch': 0.24}
{'loss': 0.7987, 'grad_norm': 2.091465473175049, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9398998022079468, 'eval_runtime': 6.9562, 'eval_samples_per_second': 143.756, 'eval_steps_per_second': 9.057, 'epoch': 0.28}
{'loss': 0.7158, 'grad_norm': 1.7686350345611572, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9376046061515808, 'eval_runtime': 6.9494, 'eval_samples_per_second': 143.897, 'eval_steps_per_second': 9.065, 'epoch': 0.32}
{'loss': 0.7699, 'grad_norm': 1.4367594718933105, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9476789832115173, 'eval_runtime': 6.9651, 'eval_samples_per_second': 143.573, 'eval_steps_per_second': 9.045, 'epoch': 0.36}
{'loss': 0.6457, 'grad_norm': 1.5166518688201904, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9267377853393555, 'eval_runtime': 6.9578, 'eval_samples_per_second': 143.723, 'eval_steps_per_second': 9.055, 'epoch': 0.4}
{'loss': 0.7222, 'grad_norm': 1.298537015914917, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9456075429916382, 'eval_runtime': 6.9606, 'eval_samples_per_second': 143.665, 'eval_steps_per_second': 9.051, 'epoch': 0.44}
{'loss': 0.6989, 'grad_norm': 1.3167041540145874, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9207314252853394, 'eval_runtime': 6.9629, 'eval_samples_per_second': 143.619, 'eval_steps_per_second': 9.048, 'epoch': 0.48}
{'loss': 0.647, 'grad_norm': 1.7399675846099854, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9242934584617615, 'eval_runtime': 6.9548, 'eval_samples_per_second': 143.786, 'eval_steps_per_second': 9.059, 'epoch': 0.52}
{'loss': 0.6488, 'grad_norm': 1.2702709436416626, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9068922400474548, 'eval_runtime': 6.9568, 'eval_samples_per_second': 143.745, 'eval_steps_per_second': 9.056, 'epoch': 0.56}
{'loss': 0.606, 'grad_norm': 1.635886311531067, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9181031584739685, 'eval_runtime': 6.9526, 'eval_samples_per_second': 143.83, 'eval_steps_per_second': 9.061, 'epoch': 0.6}
{'loss': 0.6474, 'grad_norm': 1.2732571363449097, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9047740697860718, 'eval_runtime': 6.9487, 'eval_samples_per_second': 143.912, 'eval_steps_per_second': 9.066, 'epoch': 0.64}
{'loss': 0.6974, 'grad_norm': 1.2837772369384766, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9085813164710999, 'eval_runtime': 6.9614, 'eval_samples_per_second': 143.648, 'eval_steps_per_second': 9.05, 'epoch': 0.68}
{'loss': 0.5838, 'grad_norm': 2.1760313510894775, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9003833532333374, 'eval_runtime': 6.96, 'eval_samples_per_second': 143.677, 'eval_steps_per_second': 9.052, 'epoch': 0.72}
{'loss': 0.5798, 'grad_norm': 1.4979667663574219, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9030993580818176, 'eval_runtime': 6.9636, 'eval_samples_per_second': 143.603, 'eval_steps_per_second': 9.047, 'epoch': 0.76}
{'loss': 0.6082, 'grad_norm': 1.0250462293624878, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8994393348693848, 'eval_runtime': 6.9673, 'eval_samples_per_second': 143.527, 'eval_steps_per_second': 9.042, 'epoch': 0.8}
{'loss': 0.6611, 'grad_norm': 1.30160391330719, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8951388001441956, 'eval_runtime': 6.9659, 'eval_samples_per_second': 143.557, 'eval_steps_per_second': 9.044, 'epoch': 0.84}
{'loss': 0.6025, 'grad_norm': 1.1075831651687622, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8911777138710022, 'eval_runtime': 6.9752, 'eval_samples_per_second': 143.366, 'eval_steps_per_second': 9.032, 'epoch': 0.88}
{'loss': 0.6503, 'grad_norm': 1.0386667251586914, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.888407826423645, 'eval_runtime': 6.989, 'eval_samples_per_second': 143.081, 'eval_steps_per_second': 9.014, 'epoch': 0.92}
{'loss': 0.6183, 'grad_norm': 1.2161351442337036, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8955478668212891, 'eval_runtime': 7.0351, 'eval_samples_per_second': 142.145, 'eval_steps_per_second': 8.955, 'epoch': 0.96}
{'loss': 0.5908, 'grad_norm': 1.414511799812317, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8957942724227905, 'eval_runtime': 7.0493, 'eval_samples_per_second': 141.857, 'eval_steps_per_second': 8.937, 'epoch': 1.0}
{'train_runtime': 340.6242, 'train_samples_per_second': 29.352, 'train_steps_per_second': 1.835, 'train_loss': 0.8262848937988281, 'epoch': 1.0}
train_results:  {'eval_loss': [1.849241018295288, 1.1348872184753418, 1.0018155574798584, 0.966491162776947, 0.9756388068199158, 0.9427725076675415, 0.9398998022079468, 0.9376046061515808, 0.9476789832115173, 0.9267377853393555, 0.9456075429916382, 0.9207314252853394, 0.9242934584617615, 0.9068922400474548, 0.9181031584739685, 0.9047740697860718, 0.9085813164710999, 0.9003833532333374, 0.9030993580818176, 0.8994393348693848, 0.8951388001441956, 0.8911777138710022, 0.888407826423645, 0.8955478668212891, 0.8957942724227905], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.849241018295288, 1.1348872184753418, 1.0018155574798584, 0.966491162776947, 0.9756388068199158, 0.9427725076675415, 0.9398998022079468, 0.9376046061515808, 0.9476789832115173, 0.9267377853393555, 0.9456075429916382, 0.9207314252853394, 0.9242934584617615, 0.9068922400474548, 0.9181031584739685, 0.9047740697860718, 0.9085813164710999, 0.9003833532333374, 0.9030993580818176, 0.8994393348693848, 0.8951388001441956, 0.8911777138710022, 0.888407826423645, 0.8955478668212891, 0.8957942724227905]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760284662246704
current iteration best possible eval_loss (full train run):  -0.8957942724227905
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182, -1.07602858543396, -1.0760284662246704]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8663 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -1.0596400699418054
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -1.064813578517107
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -1.0552122047687154
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -1.1220455040329882
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -1.0728381890988397
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0110, dtype=torch.float64), 0, 0, tensor(0.0574, dtype=torch.float64), tensor(0.0568, dtype=torch.float64), tensor(0.4046, dtype=torch.float64), tensor(0.4702, dtype=torch.float64), 32, 0, 1, 1, 1, 1, 16, 0.011452236897953084, 44.000244057598046, 1]
normalized proposed parameters for next round by BO: [tensor(1.2089e-17, dtype=torch.float64), tensor(1.0047e-17, dtype=torch.float64), tensor(0.0110, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.8815e-19, dtype=torch.float64), tensor(0.0574, dtype=torch.float64), tensor(0.0568, dtype=torch.float64), tensor(0.4046, dtype=torch.float64), tensor(0.4702, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1239, dtype=torch.float64), tensor(0.1145, dtype=torch.float64), tensor(0.9167, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.011
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.057
  wikitext: 0.057
  mmlu: 0.405
  arc_challenge: 0.47

LoRA Parameters:
  lora_r: (16,)
  lora_dropout: (0.011452236897953084,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (44.000244057598046,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  16
lora dropout:  0.011452236897953084
lora alpha:  44.000244057598046
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 30,932,992 || all params: 8,061,194,240 || trainable%: 0.3837
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4338, 'grad_norm': 1.1442186832427979, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.038663625717163, 'eval_runtime': 7.568, 'eval_samples_per_second': 132.134, 'eval_steps_per_second': 8.324, 'epoch': 0.04}
{'loss': 1.1589, 'grad_norm': 1.2025750875473022, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.8900257349014282, 'eval_runtime': 7.5676, 'eval_samples_per_second': 132.142, 'eval_steps_per_second': 8.325, 'epoch': 0.08}
{'loss': 1.1699, 'grad_norm': 0.7686294317245483, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8657494783401489, 'eval_runtime': 7.5681, 'eval_samples_per_second': 132.133, 'eval_steps_per_second': 8.324, 'epoch': 0.12}
{'loss': 1.0414, 'grad_norm': 0.6132582426071167, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8666046261787415, 'eval_runtime': 7.5807, 'eval_samples_per_second': 131.914, 'eval_steps_per_second': 8.311, 'epoch': 0.16}
{'loss': 1.0633, 'grad_norm': 0.8975153565406799, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8721294403076172, 'eval_runtime': 7.5696, 'eval_samples_per_second': 132.108, 'eval_steps_per_second': 8.323, 'epoch': 0.2}
{'loss': 0.9851, 'grad_norm': 0.7064630389213562, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8753355741500854, 'eval_runtime': 7.5729, 'eval_samples_per_second': 132.05, 'eval_steps_per_second': 8.319, 'epoch': 0.24}
{'loss': 0.9867, 'grad_norm': 1.5842952728271484, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8897777199745178, 'eval_runtime': 7.5663, 'eval_samples_per_second': 132.166, 'eval_steps_per_second': 8.326, 'epoch': 0.28}
{'loss': 0.9879, 'grad_norm': 0.7399908304214478, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8841930627822876, 'eval_runtime': 7.5762, 'eval_samples_per_second': 131.991, 'eval_steps_per_second': 8.315, 'epoch': 0.32}
{'loss': 0.9145, 'grad_norm': 0.7958348393440247, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.913450300693512, 'eval_runtime': 7.5835, 'eval_samples_per_second': 131.866, 'eval_steps_per_second': 8.308, 'epoch': 0.36}
{'loss': 0.9165, 'grad_norm': 0.8581545948982239, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9232407212257385, 'eval_runtime': 7.5739, 'eval_samples_per_second': 132.032, 'eval_steps_per_second': 8.318, 'epoch': 0.4}
{'loss': 0.9333, 'grad_norm': 0.8102193474769592, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9043062925338745, 'eval_runtime': 7.573, 'eval_samples_per_second': 132.048, 'eval_steps_per_second': 8.319, 'epoch': 0.44}
{'loss': 0.9451, 'grad_norm': 0.6945362687110901, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9239256381988525, 'eval_runtime': 7.574, 'eval_samples_per_second': 132.03, 'eval_steps_per_second': 8.318, 'epoch': 0.48}
{'loss': 0.8407, 'grad_norm': 0.8011119365692139, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9738306999206543, 'eval_runtime': 7.5694, 'eval_samples_per_second': 132.111, 'eval_steps_per_second': 8.323, 'epoch': 0.52}
{'loss': 0.7645, 'grad_norm': 0.7720235586166382, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9421188235282898, 'eval_runtime': 7.5336, 'eval_samples_per_second': 132.739, 'eval_steps_per_second': 8.363, 'epoch': 0.56}
{'loss': 0.8234, 'grad_norm': 0.8453106880187988, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9399763345718384, 'eval_runtime': 7.5324, 'eval_samples_per_second': 132.76, 'eval_steps_per_second': 8.364, 'epoch': 0.6}
{'loss': 0.8559, 'grad_norm': 1.0007898807525635, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9579700827598572, 'eval_runtime': 7.5386, 'eval_samples_per_second': 132.651, 'eval_steps_per_second': 8.357, 'epoch': 0.64}
{'loss': 0.7702, 'grad_norm': 0.8945276737213135, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.980766236782074, 'eval_runtime': 7.5421, 'eval_samples_per_second': 132.589, 'eval_steps_per_second': 8.353, 'epoch': 0.68}
{'loss': 0.8003, 'grad_norm': 0.8248352408409119, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9788591265678406, 'eval_runtime': 7.5395, 'eval_samples_per_second': 132.634, 'eval_steps_per_second': 8.356, 'epoch': 0.72}
{'loss': 0.7859, 'grad_norm': 0.9611949324607849, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0242300033569336, 'eval_runtime': 7.5412, 'eval_samples_per_second': 132.605, 'eval_steps_per_second': 8.354, 'epoch': 0.76}
{'loss': 0.7591, 'grad_norm': 0.9043338894844055, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0519660711288452, 'eval_runtime': 7.538, 'eval_samples_per_second': 132.661, 'eval_steps_per_second': 8.358, 'epoch': 0.8}
{'loss': 0.7376, 'grad_norm': 0.885330080986023, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0759267807006836, 'eval_runtime': 7.5869, 'eval_samples_per_second': 131.807, 'eval_steps_per_second': 8.304, 'epoch': 0.84}
{'loss': 0.7397, 'grad_norm': 0.9395961761474609, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0407569408416748, 'eval_runtime': 7.6096, 'eval_samples_per_second': 131.412, 'eval_steps_per_second': 8.279, 'epoch': 0.88}
{'loss': 0.7321, 'grad_norm': 0.818901002407074, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.069854497909546, 'eval_runtime': 7.5766, 'eval_samples_per_second': 131.985, 'eval_steps_per_second': 8.315, 'epoch': 0.92}
{'loss': 0.6882, 'grad_norm': 0.7665913701057434, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0749071836471558, 'eval_runtime': 7.5674, 'eval_samples_per_second': 132.146, 'eval_steps_per_second': 8.325, 'epoch': 0.96}
{'loss': 0.7616, 'grad_norm': 0.76937335729599, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0808058977127075, 'eval_runtime': 7.5633, 'eval_samples_per_second': 132.217, 'eval_steps_per_second': 8.33, 'epoch': 1.0}
{'train_runtime': 421.9505, 'train_samples_per_second': 23.695, 'train_steps_per_second': 1.481, 'train_loss': 0.943834228515625, 'epoch': 1.0}
train_results:  {'eval_loss': [1.038663625717163, 0.8900257349014282, 0.8657494783401489, 0.8666046261787415, 0.8721294403076172, 0.8753355741500854, 0.8897777199745178, 0.8841930627822876, 0.913450300693512, 0.9232407212257385, 0.9043062925338745, 0.9239256381988525, 0.9738306999206543, 0.9421188235282898, 0.9399763345718384, 0.9579700827598572, 0.980766236782074, 0.9788591265678406, 1.0242300033569336, 1.0519660711288452, 1.0759267807006836, 1.0407569408416748, 1.069854497909546, 1.0749071836471558, 1.0808058977127075], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.038663625717163, 0.8900257349014282, 0.8657494783401489, 0.8666046261787415, 0.8721294403076172, 0.8753355741500854, 0.8897777199745178, 0.8841930627822876, 0.913450300693512, 0.9232407212257385, 0.9043062925338745, 0.9239256381988525, 0.9738306999206543, 0.9421188235282898, 0.9399763345718384, 0.9579700827598572, 0.980766236782074, 0.9788591265678406, 1.0242300033569336, 1.0519660711288452, 1.0759267807006836, 1.0407569408416748, 1.069854497909546, 1.0749071836471558, 1.0808058977127075]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0760270357131958
current iteration best possible eval_loss (full train run):  -1.0808058977127075
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182, -1.07602858543396, -1.0760284662246704, -1.0760270357131958]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2963 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -1.1588547978001342
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -1.1025714160789037
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -1.1140063709711474
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -1.0833742186996929
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -1.053492098978523
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3202, dtype=torch.float64), 0, 0, tensor(0.1760, dtype=torch.float64), 0, tensor(0.4231, dtype=torch.float64), tensor(0.0576, dtype=torch.float64), tensor(0.0231, dtype=torch.float64), 0, 32, 0, 1, 1, 1, 1, 50, 0.1, 12.312887206142818, 1]
normalized proposed parameters for next round by BO: [tensor(0.3202, dtype=torch.float64), tensor(8.4678e-18, dtype=torch.float64), tensor(8.9925e-18, dtype=torch.float64), tensor(0.1760, dtype=torch.float64), tensor(1.4377e-17, dtype=torch.float64), tensor(0.4231, dtype=torch.float64), tensor(0.0576, dtype=torch.float64), tensor(0.0231, dtype=torch.float64), tensor(5.9153e-19, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3878, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2565, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.32
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.176
  triviaqa: 0
  truthfulqa_gen: 0.423
  wikitext: 0.058
  mmlu: 0.023
  arc_challenge: 0

LoRA Parameters:
  lora_r: (50,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (12.312887206142818,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  50
lora dropout:  0.1
lora alpha:  12.312887206142818
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,665,600 || all params: 8,126,926,848 || trainable%: 1.1894
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6212, 'grad_norm': 1.2305101156234741, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8390889167785645, 'eval_runtime': 7.7247, 'eval_samples_per_second': 129.454, 'eval_steps_per_second': 8.156, 'epoch': 0.04}
{'loss': 1.2586, 'grad_norm': 0.286465585231781, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2343608140945435, 'eval_runtime': 7.7175, 'eval_samples_per_second': 129.575, 'eval_steps_per_second': 8.163, 'epoch': 0.08}
{'loss': 1.0024, 'grad_norm': 0.2674833834171295, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2577636241912842, 'eval_runtime': 7.7145, 'eval_samples_per_second': 129.626, 'eval_steps_per_second': 8.166, 'epoch': 0.12}
{'loss': 0.9334, 'grad_norm': 0.19839081168174744, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2503594160079956, 'eval_runtime': 7.7181, 'eval_samples_per_second': 129.565, 'eval_steps_per_second': 8.163, 'epoch': 0.16}
{'loss': 0.8901, 'grad_norm': 0.21532957255840302, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2713569402694702, 'eval_runtime': 7.7261, 'eval_samples_per_second': 129.432, 'eval_steps_per_second': 8.154, 'epoch': 0.2}
{'loss': 0.8247, 'grad_norm': 0.2213781177997589, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.253930926322937, 'eval_runtime': 7.7292, 'eval_samples_per_second': 129.379, 'eval_steps_per_second': 8.151, 'epoch': 0.24}
{'loss': 0.8685, 'grad_norm': 0.2339891791343689, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3095309734344482, 'eval_runtime': 7.7329, 'eval_samples_per_second': 129.318, 'eval_steps_per_second': 8.147, 'epoch': 0.28}
{'loss': 0.8847, 'grad_norm': 0.2820257246494293, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2459224462509155, 'eval_runtime': 7.7827, 'eval_samples_per_second': 128.49, 'eval_steps_per_second': 8.095, 'epoch': 0.32}
{'loss': 0.819, 'grad_norm': 0.24884358048439026, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2558842897415161, 'eval_runtime': 7.7802, 'eval_samples_per_second': 128.531, 'eval_steps_per_second': 8.097, 'epoch': 0.36}
{'loss': 0.8787, 'grad_norm': 0.2382214367389679, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.266250729560852, 'eval_runtime': 7.7828, 'eval_samples_per_second': 128.489, 'eval_steps_per_second': 8.095, 'epoch': 0.4}
{'loss': 0.8572, 'grad_norm': 0.23683269321918488, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.247085690498352, 'eval_runtime': 7.7854, 'eval_samples_per_second': 128.445, 'eval_steps_per_second': 8.092, 'epoch': 0.44}
{'loss': 0.7818, 'grad_norm': 0.27553752064704895, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.25028395652771, 'eval_runtime': 7.8096, 'eval_samples_per_second': 128.047, 'eval_steps_per_second': 8.067, 'epoch': 0.48}
{'loss': 0.7664, 'grad_norm': 0.30968743562698364, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2741519212722778, 'eval_runtime': 7.7931, 'eval_samples_per_second': 128.319, 'eval_steps_per_second': 8.084, 'epoch': 0.52}
{'loss': 0.7676, 'grad_norm': 0.2610218822956085, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.269481897354126, 'eval_runtime': 7.7878, 'eval_samples_per_second': 128.406, 'eval_steps_per_second': 8.09, 'epoch': 0.56}
{'loss': 0.7486, 'grad_norm': 0.2466762661933899, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2787004709243774, 'eval_runtime': 7.8003, 'eval_samples_per_second': 128.201, 'eval_steps_per_second': 8.077, 'epoch': 0.6}
{'loss': 0.7871, 'grad_norm': 0.3435797393321991, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2966758012771606, 'eval_runtime': 7.8048, 'eval_samples_per_second': 128.126, 'eval_steps_per_second': 8.072, 'epoch': 0.64}
{'loss': 0.7468, 'grad_norm': 0.3333715796470642, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.277172327041626, 'eval_runtime': 7.7898, 'eval_samples_per_second': 128.373, 'eval_steps_per_second': 8.088, 'epoch': 0.68}
{'loss': 0.6984, 'grad_norm': 0.23308289051055908, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2653093338012695, 'eval_runtime': 7.7918, 'eval_samples_per_second': 128.34, 'eval_steps_per_second': 8.085, 'epoch': 0.72}
{'loss': 0.6981, 'grad_norm': 0.2869022786617279, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2434200048446655, 'eval_runtime': 7.7868, 'eval_samples_per_second': 128.422, 'eval_steps_per_second': 8.091, 'epoch': 0.76}
{'loss': 0.7008, 'grad_norm': 0.2817854881286621, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2512413263320923, 'eval_runtime': 7.7863, 'eval_samples_per_second': 128.43, 'eval_steps_per_second': 8.091, 'epoch': 0.8}
{'loss': 0.7411, 'grad_norm': 0.20877684652805328, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2443523406982422, 'eval_runtime': 7.7833, 'eval_samples_per_second': 128.48, 'eval_steps_per_second': 8.094, 'epoch': 0.84}
{'loss': 0.6798, 'grad_norm': 0.304050087928772, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.237646222114563, 'eval_runtime': 7.7552, 'eval_samples_per_second': 128.946, 'eval_steps_per_second': 8.124, 'epoch': 0.88}
{'loss': 0.7041, 'grad_norm': 0.2503994107246399, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2420125007629395, 'eval_runtime': 7.7506, 'eval_samples_per_second': 129.023, 'eval_steps_per_second': 8.128, 'epoch': 0.92}
{'loss': 0.6981, 'grad_norm': 0.22335509955883026, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2427279949188232, 'eval_runtime': 7.7488, 'eval_samples_per_second': 129.052, 'eval_steps_per_second': 8.13, 'epoch': 0.96}
{'loss': 0.6842, 'grad_norm': 0.23757384717464447, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2452932596206665, 'eval_runtime': 7.7513, 'eval_samples_per_second': 129.01, 'eval_steps_per_second': 8.128, 'epoch': 1.0}
{'train_runtime': 354.1358, 'train_samples_per_second': 28.232, 'train_steps_per_second': 1.765, 'train_loss': 0.9216647033691406, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8390889167785645, 1.2343608140945435, 1.2577636241912842, 1.2503594160079956, 1.2713569402694702, 1.253930926322937, 1.3095309734344482, 1.2459224462509155, 1.2558842897415161, 1.266250729560852, 1.247085690498352, 1.25028395652771, 1.2741519212722778, 1.269481897354126, 1.2787004709243774, 1.2966758012771606, 1.277172327041626, 1.2653093338012695, 1.2434200048446655, 1.2512413263320923, 1.2443523406982422, 1.237646222114563, 1.2420125007629395, 1.2427279949188232, 1.2452932596206665], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8390889167785645, 1.2343608140945435, 1.2577636241912842, 1.2503594160079956, 1.2713569402694702, 1.253930926322937, 1.3095309734344482, 1.2459224462509155, 1.2558842897415161, 1.266250729560852, 1.247085690498352, 1.25028395652771, 1.2741519212722778, 1.269481897354126, 1.2787004709243774, 1.2966758012771606, 1.277172327041626, 1.2653093338012695, 1.2434200048446655, 1.2512413263320923, 1.2443523406982422, 1.237646222114563, 1.2420125007629395, 1.2427279949188232, 1.2452932596206665]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1965289115905762
current iteration best possible eval_loss (full train run):  -1.2452932596206665
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182, -1.07602858543396, -1.0760284662246704, -1.0760270357131958, -1.1965289115905762]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0072 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -1.0601906746420422
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -1.0675691450910558
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -1.035436731976529
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -1.0561791680895118
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -1.056530645521424
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0106, dtype=torch.float64), tensor(0.3347, dtype=torch.float64), tensor(0.1253, dtype=torch.float64), 0, 0, 0, tensor(0.5294, dtype=torch.float64), 32, 0, 0, 0, 0, 1, 108, 0.003930346859619269, 42.6064912403332, 1]
normalized proposed parameters for next round by BO: [tensor(2.2161e-18, dtype=torch.float64), tensor(5.9659e-18, dtype=torch.float64), tensor(0.0106, dtype=torch.float64), tensor(0.3347, dtype=torch.float64), tensor(0.1253, dtype=torch.float64), tensor(7.4080e-19, dtype=torch.float64), tensor(2.6575e-18, dtype=torch.float64), tensor(2.1086e-18, dtype=torch.float64), tensor(0.5294, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8444, dtype=torch.float64), tensor(0.0393, dtype=torch.float64), tensor(0.8876, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.011
  sciq: 0.335
  triviaqa: 0.125
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.529

LoRA Parameters:
  lora_r: (108,)
  lora_dropout: (0.003930346859619269,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (42.6064912403332,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  108
lora dropout:  0.003930346859619269
lora alpha:  42.6064912403332
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 63,700,992 || all params: 8,093,962,240 || trainable%: 0.7870
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2518, 'grad_norm': 0.4987805187702179, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5359468460083008, 'eval_runtime': 6.5998, 'eval_samples_per_second': 151.519, 'eval_steps_per_second': 9.546, 'epoch': 0.04}
{'loss': 1.3487, 'grad_norm': 0.17720003426074982, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0565134286880493, 'eval_runtime': 6.6316, 'eval_samples_per_second': 150.793, 'eval_steps_per_second': 9.5, 'epoch': 0.08}
{'loss': 1.0723, 'grad_norm': 0.22444118559360504, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9755366444587708, 'eval_runtime': 6.6282, 'eval_samples_per_second': 150.87, 'eval_steps_per_second': 9.505, 'epoch': 0.12}
{'loss': 0.9529, 'grad_norm': 0.20902284979820251, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8794560432434082, 'eval_runtime': 6.6508, 'eval_samples_per_second': 150.359, 'eval_steps_per_second': 9.473, 'epoch': 0.16}
{'loss': 0.8385, 'grad_norm': 0.20913133025169373, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8567016124725342, 'eval_runtime': 6.6578, 'eval_samples_per_second': 150.199, 'eval_steps_per_second': 9.463, 'epoch': 0.2}
{'loss': 0.8172, 'grad_norm': 0.21446965634822845, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8606774210929871, 'eval_runtime': 6.6625, 'eval_samples_per_second': 150.094, 'eval_steps_per_second': 9.456, 'epoch': 0.24}
{'loss': 0.7824, 'grad_norm': 0.2479231357574463, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8688044548034668, 'eval_runtime': 6.647, 'eval_samples_per_second': 150.443, 'eval_steps_per_second': 9.478, 'epoch': 0.28}
{'loss': 0.755, 'grad_norm': 0.3084897994995117, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8926975727081299, 'eval_runtime': 6.6387, 'eval_samples_per_second': 150.631, 'eval_steps_per_second': 9.49, 'epoch': 0.32}
{'loss': 0.7287, 'grad_norm': 0.298679381608963, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8944777846336365, 'eval_runtime': 6.6436, 'eval_samples_per_second': 150.52, 'eval_steps_per_second': 9.483, 'epoch': 0.36}
{'loss': 0.7176, 'grad_norm': 0.37535610795021057, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9188597798347473, 'eval_runtime': 6.656, 'eval_samples_per_second': 150.24, 'eval_steps_per_second': 9.465, 'epoch': 0.4}
{'loss': 0.7156, 'grad_norm': 0.29083094000816345, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9172372817993164, 'eval_runtime': 6.645, 'eval_samples_per_second': 150.489, 'eval_steps_per_second': 9.481, 'epoch': 0.44}
{'loss': 0.6608, 'grad_norm': 0.347583532333374, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9896447658538818, 'eval_runtime': 6.6559, 'eval_samples_per_second': 150.242, 'eval_steps_per_second': 9.465, 'epoch': 0.48}
{'loss': 0.6515, 'grad_norm': 0.33756834268569946, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9556528329849243, 'eval_runtime': 6.6521, 'eval_samples_per_second': 150.329, 'eval_steps_per_second': 9.471, 'epoch': 0.52}
{'loss': 0.6112, 'grad_norm': 0.43368372321128845, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9577676057815552, 'eval_runtime': 6.6841, 'eval_samples_per_second': 149.609, 'eval_steps_per_second': 9.425, 'epoch': 0.56}
{'loss': 0.5688, 'grad_norm': 0.35335150361061096, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9618109464645386, 'eval_runtime': 6.7403, 'eval_samples_per_second': 148.361, 'eval_steps_per_second': 9.347, 'epoch': 0.6}
{'loss': 0.6493, 'grad_norm': 0.3541787266731262, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9594570398330688, 'eval_runtime': 6.7103, 'eval_samples_per_second': 149.024, 'eval_steps_per_second': 9.389, 'epoch': 0.64}
{'loss': 0.559, 'grad_norm': 0.3913976848125458, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.017432451248169, 'eval_runtime': 6.7006, 'eval_samples_per_second': 149.239, 'eval_steps_per_second': 9.402, 'epoch': 0.68}
{'loss': 0.5387, 'grad_norm': 0.42561420798301697, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0236852169036865, 'eval_runtime': 6.6962, 'eval_samples_per_second': 149.337, 'eval_steps_per_second': 9.408, 'epoch': 0.72}
{'loss': 0.5451, 'grad_norm': 0.3857091963291168, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9824140071868896, 'eval_runtime': 6.6963, 'eval_samples_per_second': 149.335, 'eval_steps_per_second': 9.408, 'epoch': 0.76}
{'loss': 0.5261, 'grad_norm': 0.5105224251747131, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0160295963287354, 'eval_runtime': 6.7086, 'eval_samples_per_second': 149.063, 'eval_steps_per_second': 9.391, 'epoch': 0.8}
{'loss': 0.525, 'grad_norm': 0.5518646836280823, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0465590953826904, 'eval_runtime': 6.7165, 'eval_samples_per_second': 148.887, 'eval_steps_per_second': 9.38, 'epoch': 0.84}
{'loss': 0.5013, 'grad_norm': 0.5378034710884094, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0665009021759033, 'eval_runtime': 6.7029, 'eval_samples_per_second': 149.188, 'eval_steps_per_second': 9.399, 'epoch': 0.88}
{'loss': 0.4792, 'grad_norm': 0.4955664873123169, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0703178644180298, 'eval_runtime': 6.6908, 'eval_samples_per_second': 149.458, 'eval_steps_per_second': 9.416, 'epoch': 0.92}
{'loss': 0.4724, 'grad_norm': 0.38726216554641724, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0691851377487183, 'eval_runtime': 6.6955, 'eval_samples_per_second': 149.353, 'eval_steps_per_second': 9.409, 'epoch': 0.96}
{'loss': 0.5308, 'grad_norm': 0.3686750531196594, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0699563026428223, 'eval_runtime': 6.6858, 'eval_samples_per_second': 149.571, 'eval_steps_per_second': 9.423, 'epoch': 1.0}
{'train_runtime': 316.2122, 'train_samples_per_second': 31.621, 'train_steps_per_second': 1.977, 'train_loss': 0.7919981246948242, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5359468460083008, 1.0565134286880493, 0.9755366444587708, 0.8794560432434082, 0.8567016124725342, 0.8606774210929871, 0.8688044548034668, 0.8926975727081299, 0.8944777846336365, 0.9188597798347473, 0.9172372817993164, 0.9896447658538818, 0.9556528329849243, 0.9577676057815552, 0.9618109464645386, 0.9594570398330688, 1.017432451248169, 1.0236852169036865, 0.9824140071868896, 1.0160295963287354, 1.0465590953826904, 1.0665009021759033, 1.0703178644180298, 1.0691851377487183, 1.0699563026428223], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5359468460083008, 1.0565134286880493, 0.9755366444587708, 0.8794560432434082, 0.8567016124725342, 0.8606774210929871, 0.8688044548034668, 0.8926975727081299, 0.8944777846336365, 0.9188597798347473, 0.9172372817993164, 0.9896447658538818, 0.9556528329849243, 0.9577676057815552, 0.9618109464645386, 0.9594570398330688, 1.017432451248169, 1.0236852169036865, 0.9824140071868896, 1.0160295963287354, 1.0465590953826904, 1.0665009021759033, 1.0703178644180298, 1.0691851377487183, 1.0699563026428223]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3180348873138428
current iteration best possible eval_loss (full train run):  -1.0699563026428223
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182, -1.07602858543396, -1.0760284662246704, -1.0760270357131958, -1.1965289115905762, -1.3180348873138428]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.4091 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -0.9984172214132754
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -0.9984172214132754
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -0.9984172214132754
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -0.9984172214132754
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -0.9984172214132754
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(1.0000, dtype=torch.float64), 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 4, 3.851859888774472e-35, 15.508628544336682, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1685e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.9978e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0330, dtype=torch.float64), tensor(3.8519e-34, dtype=torch.float64), tensor(0.3231, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 1.0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (4,)
  lora_dropout: (3.851859888774472e-35,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (15.508628544336682,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  4
lora dropout:  3.851859888774472e-35
lora alpha:  15.508628544336682
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 167,936 || all params: 8,030,429,184 || trainable%: 0.0021
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 5.4302, 'grad_norm': 4.8111348152160645, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.6751186847686768, 'eval_runtime': 6.2094, 'eval_samples_per_second': 161.045, 'eval_steps_per_second': 10.146, 'epoch': 0.04}
{'loss': 3.2495, 'grad_norm': 1.965160846710205, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.8024935722351074, 'eval_runtime': 6.1628, 'eval_samples_per_second': 162.264, 'eval_steps_per_second': 10.223, 'epoch': 0.08}
{'loss': 2.0408, 'grad_norm': 1.9060872793197632, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.628129005432129, 'eval_runtime': 6.1745, 'eval_samples_per_second': 161.957, 'eval_steps_per_second': 10.203, 'epoch': 0.12}
{'loss': 1.5819, 'grad_norm': 1.4298229217529297, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.4327964782714844, 'eval_runtime': 6.1792, 'eval_samples_per_second': 161.833, 'eval_steps_per_second': 10.195, 'epoch': 0.16}
{'loss': 1.318, 'grad_norm': 1.1207988262176514, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.3999500274658203, 'eval_runtime': 6.1583, 'eval_samples_per_second': 162.382, 'eval_steps_per_second': 10.23, 'epoch': 0.2}
{'loss': 1.2375, 'grad_norm': 1.052876353263855, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.3983099460601807, 'eval_runtime': 6.1621, 'eval_samples_per_second': 162.282, 'eval_steps_per_second': 10.224, 'epoch': 0.24}
{'loss': 1.1912, 'grad_norm': 1.1253491640090942, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.3635165691375732, 'eval_runtime': 6.1644, 'eval_samples_per_second': 162.221, 'eval_steps_per_second': 10.22, 'epoch': 0.28}
{'loss': 1.1746, 'grad_norm': 1.003872036933899, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.381751298904419, 'eval_runtime': 6.1698, 'eval_samples_per_second': 162.08, 'eval_steps_per_second': 10.211, 'epoch': 0.32}
{'loss': 1.2324, 'grad_norm': 0.9626046419143677, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.351372003555298, 'eval_runtime': 6.1743, 'eval_samples_per_second': 161.962, 'eval_steps_per_second': 10.204, 'epoch': 0.36}
{'loss': 1.1884, 'grad_norm': 1.0626670122146606, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.3587305545806885, 'eval_runtime': 6.165, 'eval_samples_per_second': 162.206, 'eval_steps_per_second': 10.219, 'epoch': 0.4}
{'loss': 1.1883, 'grad_norm': 1.1061683893203735, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.371994733810425, 'eval_runtime': 6.1589, 'eval_samples_per_second': 162.368, 'eval_steps_per_second': 10.229, 'epoch': 0.44}
{'loss': 1.164, 'grad_norm': 1.0086307525634766, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.352959156036377, 'eval_runtime': 6.169, 'eval_samples_per_second': 162.101, 'eval_steps_per_second': 10.212, 'epoch': 0.48}
{'loss': 1.1616, 'grad_norm': 1.0934977531433105, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.3672573566436768, 'eval_runtime': 6.1766, 'eval_samples_per_second': 161.901, 'eval_steps_per_second': 10.2, 'epoch': 0.52}
{'loss': 1.1634, 'grad_norm': 1.1033724546432495, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.360645055770874, 'eval_runtime': 6.1778, 'eval_samples_per_second': 161.869, 'eval_steps_per_second': 10.198, 'epoch': 0.56}
{'loss': 1.156, 'grad_norm': 1.107776403427124, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.3749892711639404, 'eval_runtime': 6.1693, 'eval_samples_per_second': 162.092, 'eval_steps_per_second': 10.212, 'epoch': 0.6}
{'loss': 1.1598, 'grad_norm': 1.021671175956726, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.3666863441467285, 'eval_runtime': 6.1747, 'eval_samples_per_second': 161.952, 'eval_steps_per_second': 10.203, 'epoch': 0.64}
{'loss': 1.1641, 'grad_norm': 1.0703226327896118, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.368546485900879, 'eval_runtime': 6.1754, 'eval_samples_per_second': 161.933, 'eval_steps_per_second': 10.202, 'epoch': 0.68}
{'loss': 1.1772, 'grad_norm': 1.122765302658081, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.3857531547546387, 'eval_runtime': 6.1827, 'eval_samples_per_second': 161.742, 'eval_steps_per_second': 10.19, 'epoch': 0.72}
{'loss': 1.1459, 'grad_norm': 0.9642939567565918, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.3919765949249268, 'eval_runtime': 6.1814, 'eval_samples_per_second': 161.776, 'eval_steps_per_second': 10.192, 'epoch': 0.76}
{'loss': 1.1626, 'grad_norm': 0.9268197417259216, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.385730743408203, 'eval_runtime': 6.1763, 'eval_samples_per_second': 161.909, 'eval_steps_per_second': 10.2, 'epoch': 0.8}
{'loss': 1.1285, 'grad_norm': 1.0106614828109741, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.3837666511535645, 'eval_runtime': 6.1783, 'eval_samples_per_second': 161.857, 'eval_steps_per_second': 10.197, 'epoch': 0.84}
{'loss': 1.1622, 'grad_norm': 0.9795370697975159, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.3801703453063965, 'eval_runtime': 6.1896, 'eval_samples_per_second': 161.561, 'eval_steps_per_second': 10.178, 'epoch': 0.88}
{'loss': 1.1534, 'grad_norm': 1.0771605968475342, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.3838775157928467, 'eval_runtime': 6.1821, 'eval_samples_per_second': 161.756, 'eval_steps_per_second': 10.191, 'epoch': 0.92}
{'loss': 1.1372, 'grad_norm': 0.9469927549362183, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.3831520080566406, 'eval_runtime': 6.1771, 'eval_samples_per_second': 161.889, 'eval_steps_per_second': 10.199, 'epoch': 0.96}
{'loss': 1.1674, 'grad_norm': 1.1368887424468994, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.383497714996338, 'eval_runtime': 6.185, 'eval_samples_per_second': 161.682, 'eval_steps_per_second': 10.186, 'epoch': 1.0}
{'train_runtime': 187.5348, 'train_samples_per_second': 53.318, 'train_steps_per_second': 3.333, 'train_loss': 1.4814513061523438, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6751186847686768, 2.8024935722351074, 2.628129005432129, 2.4327964782714844, 2.3999500274658203, 2.3983099460601807, 2.3635165691375732, 2.381751298904419, 2.351372003555298, 2.3587305545806885, 2.371994733810425, 2.352959156036377, 2.3672573566436768, 2.360645055770874, 2.3749892711639404, 2.3666863441467285, 2.368546485900879, 2.3857531547546387, 2.3919765949249268, 2.385730743408203, 2.3837666511535645, 2.3801703453063965, 2.3838775157928467, 2.3831520080566406, 2.383497714996338], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.6751186847686768, 2.8024935722351074, 2.628129005432129, 2.4327964782714844, 2.3999500274658203, 2.3983099460601807, 2.3635165691375732, 2.381751298904419, 2.351372003555298, 2.3587305545806885, 2.371994733810425, 2.352959156036377, 2.3672573566436768, 2.360645055770874, 2.3749892711639404, 2.3666863441467285, 2.368546485900879, 2.3857531547546387, 2.3919765949249268, 2.385730743408203, 2.3837666511535645, 2.3801703453063965, 2.3838775157928467, 2.3831520080566406, 2.383497714996338]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2186923027038574
current iteration best possible eval_loss (full train run):  -2.383497714996338
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182, -1.07602858543396, -1.0760284662246704, -1.0760270357131958, -1.1965289115905762, -1.3180348873138428, -1.2186923027038574]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.8340 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -1.018016324435055
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -1.0531703282904374
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -1.2423790044600982
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -1.0085535780887427
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -1.1230068243707385
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0804, dtype=torch.float64), tensor(0.1346, dtype=torch.float64), tensor(0.3502, dtype=torch.float64), 0, tensor(0.1146, dtype=torch.float64), tensor(0.0352, dtype=torch.float64), tensor(0.2850, dtype=torch.float64), 32, 0, 0, 1, 0, 1, 72, 0.005811624469743303, 36.19052627866368, 1]
normalized proposed parameters for next round by BO: [tensor(3.8819e-18, dtype=torch.float64), tensor(2.1640e-18, dtype=torch.float64), tensor(0.0804, dtype=torch.float64), tensor(0.1346, dtype=torch.float64), tensor(0.3502, dtype=torch.float64), tensor(1.8404e-18, dtype=torch.float64), tensor(0.1146, dtype=torch.float64), tensor(0.0352, dtype=torch.float64), tensor(0.2850, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5629, dtype=torch.float64), tensor(0.0581, dtype=torch.float64), tensor(0.7540, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.08
  sciq: 0.135
  triviaqa: 0.35
  truthfulqa_gen: 0
  wikitext: 0.115
  mmlu: 0.035
  arc_challenge: 0.285

LoRA Parameters:
  lora_r: (72,)
  lora_dropout: (0.005811624469743303,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (36.19052627866368,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  72
lora dropout:  0.005811624469743303
lora alpha:  36.19052627866368
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 84,934,656 || all params: 8,115,195,904 || trainable%: 1.0466
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1779, 'grad_norm': 0.7825585603713989, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3603595495224, 'eval_runtime': 6.9354, 'eval_samples_per_second': 144.187, 'eval_steps_per_second': 9.084, 'epoch': 0.04}
{'loss': 1.6493, 'grad_norm': 0.2461133897304535, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0478986501693726, 'eval_runtime': 6.9497, 'eval_samples_per_second': 143.89, 'eval_steps_per_second': 9.065, 'epoch': 0.08}
{'loss': 1.4227, 'grad_norm': 0.39299535751342773, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9644091725349426, 'eval_runtime': 6.9313, 'eval_samples_per_second': 144.274, 'eval_steps_per_second': 9.089, 'epoch': 0.12}
{'loss': 1.3107, 'grad_norm': 0.2553652822971344, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8967128992080688, 'eval_runtime': 6.945, 'eval_samples_per_second': 143.989, 'eval_steps_per_second': 9.071, 'epoch': 0.16}
{'loss': 1.208, 'grad_norm': 0.2470465451478958, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8618066310882568, 'eval_runtime': 6.9477, 'eval_samples_per_second': 143.932, 'eval_steps_per_second': 9.068, 'epoch': 0.2}
{'loss': 1.1413, 'grad_norm': 0.2532835900783539, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8643810153007507, 'eval_runtime': 6.9455, 'eval_samples_per_second': 143.978, 'eval_steps_per_second': 9.071, 'epoch': 0.24}
{'loss': 1.1934, 'grad_norm': 0.25879281759262085, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8637405633926392, 'eval_runtime': 6.959, 'eval_samples_per_second': 143.698, 'eval_steps_per_second': 9.053, 'epoch': 0.28}
{'loss': 1.1461, 'grad_norm': 0.2740539014339447, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8635517954826355, 'eval_runtime': 6.9507, 'eval_samples_per_second': 143.871, 'eval_steps_per_second': 9.064, 'epoch': 0.32}
{'loss': 1.0673, 'grad_norm': 0.31258636713027954, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8646836876869202, 'eval_runtime': 6.9645, 'eval_samples_per_second': 143.585, 'eval_steps_per_second': 9.046, 'epoch': 0.36}
{'loss': 1.185, 'grad_norm': 0.29426783323287964, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8822357058525085, 'eval_runtime': 6.9798, 'eval_samples_per_second': 143.271, 'eval_steps_per_second': 9.026, 'epoch': 0.4}
{'loss': 1.0741, 'grad_norm': 0.32457441091537476, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8807981014251709, 'eval_runtime': 6.9633, 'eval_samples_per_second': 143.61, 'eval_steps_per_second': 9.047, 'epoch': 0.44}
{'loss': 1.1429, 'grad_norm': 0.32185831665992737, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8812805414199829, 'eval_runtime': 6.959, 'eval_samples_per_second': 143.699, 'eval_steps_per_second': 9.053, 'epoch': 0.48}
{'loss': 1.0615, 'grad_norm': 0.27234819531440735, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8796759247779846, 'eval_runtime': 6.9663, 'eval_samples_per_second': 143.549, 'eval_steps_per_second': 9.044, 'epoch': 0.52}
{'loss': 1.1379, 'grad_norm': 0.33519184589385986, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8781506419181824, 'eval_runtime': 6.9635, 'eval_samples_per_second': 143.606, 'eval_steps_per_second': 9.047, 'epoch': 0.56}
{'loss': 1.1143, 'grad_norm': 0.3540964126586914, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9184283018112183, 'eval_runtime': 6.9638, 'eval_samples_per_second': 143.601, 'eval_steps_per_second': 9.047, 'epoch': 0.6}
{'loss': 1.0646, 'grad_norm': 0.32481083273887634, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8927130103111267, 'eval_runtime': 6.9624, 'eval_samples_per_second': 143.628, 'eval_steps_per_second': 9.049, 'epoch': 0.64}
{'loss': 1.0275, 'grad_norm': 0.287600040435791, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8882342576980591, 'eval_runtime': 6.9521, 'eval_samples_per_second': 143.841, 'eval_steps_per_second': 9.062, 'epoch': 0.68}
{'loss': 1.0032, 'grad_norm': 0.38290953636169434, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.919651985168457, 'eval_runtime': 6.9825, 'eval_samples_per_second': 143.215, 'eval_steps_per_second': 9.023, 'epoch': 0.72}
{'loss': 1.0626, 'grad_norm': 0.32153305411338806, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9263173937797546, 'eval_runtime': 6.9756, 'eval_samples_per_second': 143.357, 'eval_steps_per_second': 9.032, 'epoch': 0.76}
{'loss': 1.0542, 'grad_norm': 0.3834192156791687, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.943594217300415, 'eval_runtime': 6.9787, 'eval_samples_per_second': 143.292, 'eval_steps_per_second': 9.027, 'epoch': 0.8}
{'loss': 1.076, 'grad_norm': 0.3864978551864624, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9339380860328674, 'eval_runtime': 6.9798, 'eval_samples_per_second': 143.27, 'eval_steps_per_second': 9.026, 'epoch': 0.84}
{'loss': 1.0502, 'grad_norm': 0.3230821490287781, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9337339997291565, 'eval_runtime': 6.9798, 'eval_samples_per_second': 143.271, 'eval_steps_per_second': 9.026, 'epoch': 0.88}
{'loss': 1.0509, 'grad_norm': 0.47251564264297485, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9544175267219543, 'eval_runtime': 6.9833, 'eval_samples_per_second': 143.198, 'eval_steps_per_second': 9.021, 'epoch': 0.92}
{'loss': 1.0321, 'grad_norm': 0.3123341500759125, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9366717338562012, 'eval_runtime': 6.959, 'eval_samples_per_second': 143.699, 'eval_steps_per_second': 9.053, 'epoch': 0.96}
{'loss': 1.0224, 'grad_norm': 0.5458433628082275, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9497308135032654, 'eval_runtime': 6.9696, 'eval_samples_per_second': 143.48, 'eval_steps_per_second': 9.039, 'epoch': 1.0}
{'train_runtime': 373.9087, 'train_samples_per_second': 26.734, 'train_steps_per_second': 1.672, 'train_loss': 1.2190400665283203, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3603595495224, 1.0478986501693726, 0.9644091725349426, 0.8967128992080688, 0.8618066310882568, 0.8643810153007507, 0.8637405633926392, 0.8635517954826355, 0.8646836876869202, 0.8822357058525085, 0.8807981014251709, 0.8812805414199829, 0.8796759247779846, 0.8781506419181824, 0.9184283018112183, 0.8927130103111267, 0.8882342576980591, 0.919651985168457, 0.9263173937797546, 0.943594217300415, 0.9339380860328674, 0.9337339997291565, 0.9544175267219543, 0.9366717338562012, 0.9497308135032654], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3603595495224, 1.0478986501693726, 0.9644091725349426, 0.8967128992080688, 0.8618066310882568, 0.8643810153007507, 0.8637405633926392, 0.8635517954826355, 0.8646836876869202, 0.8822357058525085, 0.8807981014251709, 0.8812805414199829, 0.8796759247779846, 0.8781506419181824, 0.9184283018112183, 0.8927130103111267, 0.8882342576980591, 0.919651985168457, 0.9263173937797546, 0.943594217300415, 0.9339380860328674, 0.9337339997291565, 0.9544175267219543, 0.9366717338562012, 0.9497308135032654]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1285734176635742
current iteration best possible eval_loss (full train run):  -0.9497308135032654
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182, -1.07602858543396, -1.0760284662246704, -1.0760270357131958, -1.1965289115905762, -1.3180348873138428, -1.2186923027038574, -1.1285734176635742]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9089 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.0911850809866646
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.056530449199601
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.0592168309103898
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.070159889561478
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.0497306537207804
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1346, dtype=torch.float64), tensor(0.0983, dtype=torch.float64), 0, tensor(0.0943, dtype=torch.float64), tensor(0.4953, dtype=torch.float64), tensor(0.1776, dtype=torch.float64), 1, 1, 1, 1, 1, 0, 77, 0.1, 36.885809789645904, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(9.2559e-20, dtype=torch.float64), tensor(1.3989e-17, dtype=torch.float64), tensor(0.1346, dtype=torch.float64), tensor(0.0983, dtype=torch.float64), tensor(1.5392e-17, dtype=torch.float64), tensor(0.0943, dtype=torch.float64), tensor(0.4953, dtype=torch.float64), tensor(0.1776, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6024, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7685, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.135
  triviaqa: 0.098
  truthfulqa_gen: 0
  wikitext: 0.094
  mmlu: 0.495
  arc_challenge: 0.178

LoRA Parameters:
  lora_r: (77,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (36.885809789645904,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  77
lora dropout:  0.1
lora alpha:  36.885809789645904
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,863,552 || all params: 8,034,124,800 || trainable%: 0.0481
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6165, 'grad_norm': 3.812455177307129, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.941110372543335, 'eval_runtime': 6.156, 'eval_samples_per_second': 162.444, 'eval_steps_per_second': 10.234, 'epoch': 0.04}
{'loss': 2.4476, 'grad_norm': 1.1405528783798218, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.0747995376586914, 'eval_runtime': 6.1477, 'eval_samples_per_second': 162.661, 'eval_steps_per_second': 10.248, 'epoch': 0.08}
{'loss': 1.9383, 'grad_norm': 0.7076123356819153, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5749728679656982, 'eval_runtime': 6.1732, 'eval_samples_per_second': 161.99, 'eval_steps_per_second': 10.205, 'epoch': 0.12}
{'loss': 1.6599, 'grad_norm': 1.056503415107727, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2541706562042236, 'eval_runtime': 6.1926, 'eval_samples_per_second': 161.483, 'eval_steps_per_second': 10.173, 'epoch': 0.16}
{'loss': 1.5658, 'grad_norm': 0.5627061724662781, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1325995922088623, 'eval_runtime': 6.1996, 'eval_samples_per_second': 161.301, 'eval_steps_per_second': 10.162, 'epoch': 0.2}
{'loss': 1.516, 'grad_norm': 1.3858590126037598, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1139931678771973, 'eval_runtime': 6.1991, 'eval_samples_per_second': 161.314, 'eval_steps_per_second': 10.163, 'epoch': 0.24}
{'loss': 1.5019, 'grad_norm': 0.7969485521316528, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0882511138916016, 'eval_runtime': 6.218, 'eval_samples_per_second': 160.824, 'eval_steps_per_second': 10.132, 'epoch': 0.28}
{'loss': 1.4664, 'grad_norm': 0.6596311330795288, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0638588666915894, 'eval_runtime': 6.2198, 'eval_samples_per_second': 160.777, 'eval_steps_per_second': 10.129, 'epoch': 0.32}
{'loss': 1.4783, 'grad_norm': 0.8766291737556458, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.038577675819397, 'eval_runtime': 6.2214, 'eval_samples_per_second': 160.736, 'eval_steps_per_second': 10.126, 'epoch': 0.36}
{'loss': 1.4278, 'grad_norm': 0.689355731010437, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0387736558914185, 'eval_runtime': 6.2165, 'eval_samples_per_second': 160.862, 'eval_steps_per_second': 10.134, 'epoch': 0.4}
{'loss': 1.4049, 'grad_norm': 0.8271933197975159, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.040012240409851, 'eval_runtime': 6.2193, 'eval_samples_per_second': 160.79, 'eval_steps_per_second': 10.13, 'epoch': 0.44}
{'loss': 1.3184, 'grad_norm': 0.6176389455795288, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.019797921180725, 'eval_runtime': 6.2167, 'eval_samples_per_second': 160.856, 'eval_steps_per_second': 10.134, 'epoch': 0.48}
{'loss': 1.3518, 'grad_norm': 0.488045871257782, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0113126039505005, 'eval_runtime': 6.2254, 'eval_samples_per_second': 160.633, 'eval_steps_per_second': 10.12, 'epoch': 0.52}
{'loss': 1.4263, 'grad_norm': 0.3988455832004547, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0167404413223267, 'eval_runtime': 6.2236, 'eval_samples_per_second': 160.68, 'eval_steps_per_second': 10.123, 'epoch': 0.56}
{'loss': 1.305, 'grad_norm': 0.6615636348724365, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9971222877502441, 'eval_runtime': 6.2429, 'eval_samples_per_second': 160.182, 'eval_steps_per_second': 10.091, 'epoch': 0.6}
{'loss': 1.3771, 'grad_norm': 0.42999768257141113, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0003365278244019, 'eval_runtime': 6.2354, 'eval_samples_per_second': 160.375, 'eval_steps_per_second': 10.104, 'epoch': 0.64}
{'loss': 1.3372, 'grad_norm': 0.5531878471374512, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9940640926361084, 'eval_runtime': 6.2168, 'eval_samples_per_second': 160.855, 'eval_steps_per_second': 10.134, 'epoch': 0.68}
{'loss': 1.3797, 'grad_norm': 0.4724850654602051, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9924429059028625, 'eval_runtime': 6.2184, 'eval_samples_per_second': 160.812, 'eval_steps_per_second': 10.131, 'epoch': 0.72}
{'loss': 1.3101, 'grad_norm': 0.45538631081581116, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9910712838172913, 'eval_runtime': 6.2244, 'eval_samples_per_second': 160.659, 'eval_steps_per_second': 10.122, 'epoch': 0.76}
{'loss': 1.3445, 'grad_norm': 0.5267060399055481, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.990141749382019, 'eval_runtime': 6.2234, 'eval_samples_per_second': 160.685, 'eval_steps_per_second': 10.123, 'epoch': 0.8}
{'loss': 1.3426, 'grad_norm': 0.4531936049461365, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9882669448852539, 'eval_runtime': 6.2205, 'eval_samples_per_second': 160.758, 'eval_steps_per_second': 10.128, 'epoch': 0.84}
{'loss': 1.3095, 'grad_norm': 0.5948972702026367, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9860004186630249, 'eval_runtime': 6.2158, 'eval_samples_per_second': 160.879, 'eval_steps_per_second': 10.135, 'epoch': 0.88}
{'loss': 1.3581, 'grad_norm': 0.5772108435630798, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9859417080879211, 'eval_runtime': 6.2136, 'eval_samples_per_second': 160.937, 'eval_steps_per_second': 10.139, 'epoch': 0.92}
{'loss': 1.3279, 'grad_norm': 0.48865723609924316, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9832130074501038, 'eval_runtime': 6.2199, 'eval_samples_per_second': 160.774, 'eval_steps_per_second': 10.129, 'epoch': 0.96}
{'loss': 1.3295, 'grad_norm': 0.5546935796737671, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9827616810798645, 'eval_runtime': 6.2057, 'eval_samples_per_second': 161.142, 'eval_steps_per_second': 10.152, 'epoch': 1.0}
{'train_runtime': 336.3636, 'train_samples_per_second': 29.718, 'train_steps_per_second': 1.858, 'train_loss': 1.553643035888672, 'epoch': 1.0}
train_results:  {'eval_loss': [2.941110372543335, 2.0747995376586914, 1.5749728679656982, 1.2541706562042236, 1.1325995922088623, 1.1139931678771973, 1.0882511138916016, 1.0638588666915894, 1.038577675819397, 1.0387736558914185, 1.040012240409851, 1.019797921180725, 1.0113126039505005, 1.0167404413223267, 0.9971222877502441, 1.0003365278244019, 0.9940640926361084, 0.9924429059028625, 0.9910712838172913, 0.990141749382019, 0.9882669448852539, 0.9860004186630249, 0.9859417080879211, 0.9832130074501038, 0.9827616810798645], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.941110372543335, 2.0747995376586914, 1.5749728679656982, 1.2541706562042236, 1.1325995922088623, 1.1139931678771973, 1.0882511138916016, 1.0638588666915894, 1.038577675819397, 1.0387736558914185, 1.040012240409851, 1.019797921180725, 1.0113126039505005, 1.0167404413223267, 0.9971222877502441, 1.0003365278244019, 0.9940640926361084, 0.9924429059028625, 0.9910712838172913, 0.990141749382019, 0.9882669448852539, 0.9860004186630249, 0.9859417080879211, 0.9832130074501038, 0.9827616810798645]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.4350188970565796
current iteration best possible eval_loss (full train run):  -0.9827616810798645
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182, -1.07602858543396, -1.0760284662246704, -1.0760270357131958, -1.1965289115905762, -1.3180348873138428, -1.2186923027038574, -1.1285734176635742, -1.4350188970565796]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.2388 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -1.0368634516278672
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -1.0373006701475886
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -1.0324919566155324
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -1.0376851133471763
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -1.036380895733407
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0947, dtype=torch.float64), tensor(0.0289, dtype=torch.float64), 0, tensor(0.1232, dtype=torch.float64), tensor(0.3215, dtype=torch.float64), tensor(0.0847, dtype=torch.float64), tensor(0.0188, dtype=torch.float64), tensor(0.2607, dtype=torch.float64), tensor(0.0673, dtype=torch.float64), 29, 0, 0, 0, 1, 1, 28, 0.1, 21.73223790798518, 0]
normalized proposed parameters for next round by BO: [tensor(0.0947, dtype=torch.float64), tensor(0.0289, dtype=torch.float64), tensor(4.6286e-18, dtype=torch.float64), tensor(0.1232, dtype=torch.float64), tensor(0.3215, dtype=torch.float64), tensor(0.0847, dtype=torch.float64), tensor(0.0188, dtype=torch.float64), tensor(0.2607, dtype=torch.float64), tensor(0.0673, dtype=torch.float64), tensor(0.9038, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2187, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4528, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.095
  gsm8k: 0.029
  rowan_hellaswag: 0
  sciq: 0.123
  triviaqa: 0.321
  truthfulqa_gen: 0.085
  wikitext: 0.019
  mmlu: 0.261
  arc_challenge: 0.067

LoRA Parameters:
  lora_r: (28,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (21.73223790798518,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  28
lora dropout:  0.1
lora alpha:  21.73223790798518
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 29,933,568 || all params: 8,060,194,816 || trainable%: 0.3714
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3593, 'grad_norm': 1.4414396286010742, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8057717084884644, 'eval_runtime': 6.9451, 'eval_samples_per_second': 143.986, 'eval_steps_per_second': 9.071, 'epoch': 0.04}
{'loss': 1.452, 'grad_norm': 0.8745660185813904, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9987788200378418, 'eval_runtime': 6.9693, 'eval_samples_per_second': 143.485, 'eval_steps_per_second': 9.04, 'epoch': 0.08}
{'loss': 1.1996, 'grad_norm': 0.4029589295387268, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9684146642684937, 'eval_runtime': 6.9701, 'eval_samples_per_second': 143.471, 'eval_steps_per_second': 9.039, 'epoch': 0.12}
{'loss': 1.0982, 'grad_norm': 0.2825135588645935, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9824498891830444, 'eval_runtime': 6.9808, 'eval_samples_per_second': 143.251, 'eval_steps_per_second': 9.025, 'epoch': 0.16}
{'loss': 1.1479, 'grad_norm': 0.3312176465988159, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9226043224334717, 'eval_runtime': 7.0109, 'eval_samples_per_second': 142.635, 'eval_steps_per_second': 8.986, 'epoch': 0.2}
{'loss': 1.1054, 'grad_norm': 0.2921905815601349, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9343456029891968, 'eval_runtime': 7.0118, 'eval_samples_per_second': 142.616, 'eval_steps_per_second': 8.985, 'epoch': 0.24}
{'loss': 1.1252, 'grad_norm': 0.2828617990016937, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9285916090011597, 'eval_runtime': 7.0234, 'eval_samples_per_second': 142.382, 'eval_steps_per_second': 8.97, 'epoch': 0.28}
{'loss': 1.0878, 'grad_norm': 0.27016234397888184, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9125474095344543, 'eval_runtime': 7.0207, 'eval_samples_per_second': 142.436, 'eval_steps_per_second': 8.973, 'epoch': 0.32}
{'loss': 1.0792, 'grad_norm': 0.30717721581459045, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9218795895576477, 'eval_runtime': 7.0197, 'eval_samples_per_second': 142.456, 'eval_steps_per_second': 8.975, 'epoch': 0.36}
{'loss': 1.0655, 'grad_norm': 0.2834288775920868, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9068846106529236, 'eval_runtime': 6.9833, 'eval_samples_per_second': 143.199, 'eval_steps_per_second': 9.022, 'epoch': 0.4}
{'loss': 1.0361, 'grad_norm': 0.29745522141456604, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8977197408676147, 'eval_runtime': 6.9728, 'eval_samples_per_second': 143.415, 'eval_steps_per_second': 9.035, 'epoch': 0.44}
{'loss': 1.0404, 'grad_norm': 0.3028978109359741, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8870376944541931, 'eval_runtime': 6.9754, 'eval_samples_per_second': 143.361, 'eval_steps_per_second': 9.032, 'epoch': 0.48}
{'loss': 1.1003, 'grad_norm': 0.28160494565963745, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8886619806289673, 'eval_runtime': 6.9888, 'eval_samples_per_second': 143.086, 'eval_steps_per_second': 9.014, 'epoch': 0.52}
{'loss': 1.0463, 'grad_norm': 0.3175593316555023, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8807355761528015, 'eval_runtime': 7.0406, 'eval_samples_per_second': 142.034, 'eval_steps_per_second': 8.948, 'epoch': 0.56}
{'loss': 1.0229, 'grad_norm': 0.31114619970321655, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.88340824842453, 'eval_runtime': 7.0474, 'eval_samples_per_second': 141.896, 'eval_steps_per_second': 8.939, 'epoch': 0.6}
{'loss': 1.0523, 'grad_norm': 0.3338683843612671, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8768276572227478, 'eval_runtime': 7.0147, 'eval_samples_per_second': 142.557, 'eval_steps_per_second': 8.981, 'epoch': 0.64}
{'loss': 1.0533, 'grad_norm': 0.3213083744049072, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8769069910049438, 'eval_runtime': 7.0234, 'eval_samples_per_second': 142.38, 'eval_steps_per_second': 8.97, 'epoch': 0.68}
{'loss': 1.0307, 'grad_norm': 0.28494352102279663, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8765988349914551, 'eval_runtime': 7.0197, 'eval_samples_per_second': 142.456, 'eval_steps_per_second': 8.975, 'epoch': 0.72}
{'loss': 1.0308, 'grad_norm': 0.2934480607509613, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8754730224609375, 'eval_runtime': 7.0242, 'eval_samples_per_second': 142.365, 'eval_steps_per_second': 8.969, 'epoch': 0.76}
{'loss': 1.0641, 'grad_norm': 0.28452369570732117, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8741267323493958, 'eval_runtime': 7.0196, 'eval_samples_per_second': 142.459, 'eval_steps_per_second': 8.975, 'epoch': 0.8}
{'loss': 1.0586, 'grad_norm': 0.398067831993103, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8747231364250183, 'eval_runtime': 7.0183, 'eval_samples_per_second': 142.484, 'eval_steps_per_second': 8.976, 'epoch': 0.84}
{'loss': 1.0797, 'grad_norm': 0.30977362394332886, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8701967597007751, 'eval_runtime': 7.0199, 'eval_samples_per_second': 142.453, 'eval_steps_per_second': 8.975, 'epoch': 0.88}
{'loss': 0.998, 'grad_norm': 0.3202365040779114, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8705453872680664, 'eval_runtime': 7.0184, 'eval_samples_per_second': 142.484, 'eval_steps_per_second': 8.976, 'epoch': 0.92}
{'loss': 1.0656, 'grad_norm': 0.3148522675037384, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8710948824882507, 'eval_runtime': 7.0267, 'eval_samples_per_second': 142.314, 'eval_steps_per_second': 8.966, 'epoch': 0.96}
{'loss': 1.0366, 'grad_norm': 0.36975929141044617, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.870092511177063, 'eval_runtime': 7.0205, 'eval_samples_per_second': 142.441, 'eval_steps_per_second': 8.974, 'epoch': 1.0}
{'train_runtime': 360.6556, 'train_samples_per_second': 27.716, 'train_steps_per_second': 1.733, 'train_loss': 1.177436947631836, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8057717084884644, 0.9987788200378418, 0.9684146642684937, 0.9824498891830444, 0.9226043224334717, 0.9343456029891968, 0.9285916090011597, 0.9125474095344543, 0.9218795895576477, 0.9068846106529236, 0.8977197408676147, 0.8870376944541931, 0.8886619806289673, 0.8807355761528015, 0.88340824842453, 0.8768276572227478, 0.8769069910049438, 0.8765988349914551, 0.8754730224609375, 0.8741267323493958, 0.8747231364250183, 0.8701967597007751, 0.8705453872680664, 0.8710948824882507, 0.870092511177063], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8057717084884644, 0.9987788200378418, 0.9684146642684937, 0.9824498891830444, 0.9226043224334717, 0.9343456029891968, 0.9285916090011597, 0.9125474095344543, 0.9218795895576477, 0.9068846106529236, 0.8977197408676147, 0.8870376944541931, 0.8886619806289673, 0.8807355761528015, 0.88340824842453, 0.8768276572227478, 0.8769069910049438, 0.8765988349914551, 0.8754730224609375, 0.8741267323493958, 0.8747231364250183, 0.8701967597007751, 0.8705453872680664, 0.8710948824882507, 0.870092511177063]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0845947265625
current iteration best possible eval_loss (full train run):  -0.870092511177063
max eval_loss so far:  -0.8608999252319336
BO observations:  [-1.0760282278060913, -1.0760295391082764, -1.07602858543396, -1.0760300159454346, -1.076029896736145, -1.9401277303695679, -1.076029658317566, -1.0760295391082764, -1.076029658317566, -1.0760319232940674, -1.0847843885421753, -1.076029896736145, -1.260966420173645, -1.0760266780853271, -1.076029896736145, -1.0760301351547241, -1.0760302543640137, -1.4149739742279053, -1.0760271549224854, -1.2985050678253174, -1.0760290622711182, -1.07602858543396, -1.0760284662246704, -1.0760270357131958, -1.1965289115905762, -1.3180348873138428, -1.2186923027038574, -1.1285734176635742, -1.4350188970565796, -1.0845947265625]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9898 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -1.1095906285616004
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -1.1908500706669565
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -1.1680109106752157
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -1.3416909658699538
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -1.1611813635551096
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0616, dtype=torch.float64), tensor(0.1081, dtype=torch.float64), tensor(0.5066, dtype=torch.float64), 0, 0, tensor(0.2772, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), 25, 1, 1, 1, 1, 0, 2, 0.021569288267330882, 34.48300985290373, 1]
normalized proposed parameters for next round by BO: [tensor(4.3645e-17, dtype=torch.float64), tensor(2.3641e-17, dtype=torch.float64), tensor(0.0616, dtype=torch.float64), tensor(0.1081, dtype=torch.float64), tensor(0.5066, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0065, dtype=torch.float64), tensor(0.2772, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), tensor(0.7842, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.2157, dtype=torch.float64), tensor(0.7184, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-0.9422724843025208, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8668954968452454, -0.8668954968452454, -0.8668954968452454, -0.8668954968452454, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'arc_challenge', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_arc_challenge_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['arc_challenge'], 'weight': [1.0], 'random': [[-0.9691433906555176, -0.8967759013175964, -0.8967759013175964, -0.8955367207527161, -0.8860146403312683, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843, -0.8768962621688843], [-0.9445481896400452, -0.931367814540863, -0.931367814540863, -0.9022233486175537, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988, -0.8613058924674988], [-0.9422724843025208, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8778097629547119, -0.8668954968452454, -0.8668954968452454, -0.8668954968452454, -0.8668954968452454, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336, -0.8608999252319336]], 'random_full_inputs': [[[0.3909464900235631, 0, 0, 0.14696624401915492, 0.059086038718282864, 0.15011091074464294, 0, 0, 0.25289031649435617, 28, 0, 0, 0, 1, 1, 2, 0.1, 40.96875764355262, 1], [0, 0.4243987606311636, 0, 0, 0.29941511412645644, 0.03187805201492987, 0, 0, 0.23644189151039527, 25, 0, 1, 0, 1, 0, 2, 0.1, 42.8162850213463, 0], [0.3554573000069263, 0, 0, 0, 0.6005602423772796, 0.03657844963478663, 0, 0, 0, 30, 1, 0, 0, 0, 1, 2, 0.1, 40.80605172568954, 0], [0, 0, 0, 0.5417549057040603, 0, 0.05978936180128765, 0, 0.2244373093922077, 0.16225746796224322, 25, 0, 0, 0, 1, 1, 2, 4.33680868994202e-19, 36.95713489865037, 1], [0.0815336259357411, 0, 0.06581025761536434, 0, 0.015935590562680332, 0.04781184732525531, 0, 0.6099330886163828, 0.17897558994457613, 27, 0, 1, 0, 1, 0, 2, 0.1, 38.009221572600296, 0], [0, 0.3537708807665275, 0.055078561473414944, 0.03096738396674287, 0.16196288358205252, 0, 0.1266103415599504, 0, 0.2716099486513119, 26, 0, 1, 1, 0, 1, 112, 0.09999040184166648, 1.5106428402215637, 0], [0.07163258308319351, 0.21321902349764443, 0, 0, 0.1460108933174911, 0.11226892379560399, 0.10972769169974493, 0.26835719944068365, 0.07878368516563837, 32, 0, 0, 0, 1, 1, 2, 0.07796840064425789, 37.182492308655085, 1], [0.3993136596805303, 0, 0, 0, 0, 0.011994421043538778, 0.05672193468522459, 0.5319699845907062, 0, 24, 0, 1, 0, 1, 0, 2, 0.06364669255584127, 25.04444140863432, 1], [0, 0.10927320749308825, 0, 0.04518204306604927, 0.6682118045415459, 0.021091508187255973, 0, 0, 0.15624143671206076, 26, 0, 1, 0, 1, 0, 128, 0.0, 40.88092978266088, 1], [0.6622102528566289, 0, 0.0646228162912579, 0, 0, 0, 0, 0.26513620502405594, 0, 28, 1, 1, 0, 1, 0, 53, 0.1, 48.0, 0], [0, 0, 0.25287398281349965, 0, 0.37298479215211, 0, 0.06817024742117124, 0.2958935129671449, 0.010077464646074234, 28, 1, 1, 0, 1, 1, 35, 1.7347234759768084e-19, 22.213978851097316, 0], [0, 0, 0.012138102945194925, 0, 0, 0, 0.08035870259052806, 0.3317406486622849, 0.5757625458019922, 25, 1, 1, 0, 1, 0, 36, 0.0988331548307494, 48.0, 0], [0.03275962798378262, 0, 0.10777054727059027, 0.1413177467602954, 0.5237192041805339, 0, 0.03999221866051585, 0.15444065514428199, 0, 30, 1, 1, 1, 1, 1, 2, 0.1, 48.0, 1], [0, 0, 0, 0, 0.5291282190405019, 0.07256898907294525, 0.01967958744931507, 0.3786232044372378, 0, 29, 1, 1, 0, 1, 1, 39, 1.1707085446020326e-18, 1.4800000190734863, 0], [0.04546684370098815, 0.13857352241695636, 0.15748599874042093, 0.036349552168665714, 0.13836975931631473, 0.2138838820026857, 0.046746963158988734, 0.10724218251440099, 0.11588129598057874, 26, 0, 1, 1, 0, 1, 2, 0.06729646361918733, 26.908586398326868, 0], [0.10134683525122543, 0, 0.0589754035144989, 0.19272336185829608, 0, 0.15958399541320598, 0.053384292477111295, 0.3067397190754374, 0.12724639241022495, 32, 0, 0, 0, 0, 1, 14, 0.06210926807233158, 16.707467952982384, 1], [0, 0, 0, 0, 0.6216169261255485, 0, 0, 0.3783830738744514, 0, 32, 0, 1, 1, 0, 1, 2, 0.09352039458758571, 39.967998831161495, 1], [0, 0, 0, 0.48325910898674757, 0, 0, 0.5167408910132526, 0, 0, 32, 1, 1, 1, 0, 1, 42, 0.0, 32.687050027960964, 1], [0, 0, 0, 0.07797229271299673, 0, 0.3016251913060507, 0, 0.16507275824354162, 0.45532975773741086, 28, 1, 0, 1, 1, 0, 48, 0.08090559198828912, 18.369354414440934, 1], [0, 0, 0, 0.6154892985096668, 0.37929558103389416, 0, 0, 0, 0, 31, 1, 0, 1, 0, 0, 53, 0.1, 18.309192280807366, 1], [0, 0.20790012176281764, 0, 0, 0, 0.6271459427614986, 0, 0.1649539354756835, 0, 1, 1, 0, 1, 1, 0, 2, 4.380176776841437e-18, 35.0701807902397, 1], [0, 0, 0.8105180284063138, 0, 0, 0, 0, 0.1894819715936863, 0, 32, 1, 0, 1, 1, 0, 2, 0.1, 32.56938978652289, 1], [0, 0.46955449655694936, 0, 0.17600817063974686, 0.04219158771630027, 0.013881535447107548, 0, 0.298364209639896, 0, 26, 1, 1, 1, 0, 0, 2, 0.07654287829186253, 27.67600135034964, 1], [0, 0, 0, 0, 0, 0, 0.24822050730599854, 0, 0.7517794926940016, 32, 1, 0, 1, 1, 0, 14, 0.1, 33.14159187462579, 1], [0, 0, 0, 0.16190544371905377, 0, 0, 0, 0.8286017999767683, 0, 32, 1, 0, 1, 1, 0, 57, 0.1, 48.0, 1], [0.025666739333399503, 0, 0.04349061055253566, 0, 0.38460185730740953, 0.4736068356491252, 0.013750208212548246, 0, 0.05299151758014026, 32, 0, 1, 1, 0, 1, 2, 0.04791774723784668, 43.77483255843523, 0], [0, 0, 0, 0.11983101200806379, 0.34420495967247133, 0, 0, 0.535964028319465, 0, 2, 0, 1, 0, 1, 1, 51, 0.1, 27.240180682844368, 1], [0, 0, 0.34984992041853297, 0, 0.2220065292463785, 0, 0.4281435503350885, 0, 0, 32, 1, 0, 1, 1, 1, 6, 0.0, 47.33400256826737, 1], [0.7722921660242219, 0, 0, 0, 0.22770783397577823, 0, 0, 0, 0, 32, 1, 1, 1, 1, 1, 34, 0.0, 39.39488683659286, 1], [0, 0, 0, 0, 0, 0, 0.4739422674878507, 0.5221701010805496, 0, 32, 1, 0, 1, 1, 1, 2, 0.0, 37.734750709860755, 1]], [[0.38743387874312984, 0, 0, 0.15190953828736867, 0.057492610422878364, 0.14938322740763454, 0, 0, 0.2537807451389886, 28, 0, 0, 0, 1, 1, 2, 0.1, 41.064686754527166, 1], [0, 0.41998336010812365, 0, 0, 0.3058625206873876, 0.031850577279950255, 0, 0, 0.23385850164178543, 25, 0, 0, 0, 1, 1, 2, 0.1, 42.93653053811343, 1], [0.35782235234617993, 0, 0, 0.013648547468534989, 0.5917668223992738, 0.036762277786011245, 0, 0, 0, 30, 1, 0, 0, 0, 1, 2, 0.09986953930008159, 40.78741890582863, 0], [0, 0, 0, 0.531914165902789, 0, 0.05979948281337844, 0, 0.2370855416258375, 0.16316931073517035, 25, 0, 1, 1, 0, 1, 2, 2.602085213965211e-19, 36.805254135402095, 0], [0.08035048392236938, 0, 0.06612898044659714, 0, 0.01770439492352177, 0.04686636891024026, 0, 0.6104840567504513, 0.1784657150468202, 27, 0, 1, 0, 1, 0, 2, 0.1, 37.986024420392496, 1], [0, 0, 0.027417079006837455, 0.10265738784422296, 0.4635275969871546, 0, 0.05487187072073188, 0.2515167075957307, 0.10000935784532239, 26, 0, 0, 0, 1, 1, 128, 0.0, 35.37523967898406, 1], [0, 0.011043874058052329, 0.05152474291249127, 0.05858354672156937, 0.08693969647201592, 0.19210060597255416, 0.1298253852601685, 0.4541494702969747, 0.015832678306173773, 32, 1, 1, 1, 0, 1, 32, 0.06463574873780613, 33.67592314067458, 0], [0.13884279125178883, 0, 0, 0.22577676584403425, 0, 0.44389157764967435, 0.11658695528584086, 0, 0.07490190996866164, 32, 1, 1, 0, 1, 1, 36, 0.1, 18.21087644748635, 1], [0.232422117788006, 0.12572889001941928, 0.06057942105673095, 0.2766282382334375, 0.06842160530667761, 0, 0.025016520990693437, 0, 0.21120320660503528, 15, 1, 1, 0, 1, 1, 49, 0.038666622481449545, 39.44065566511992, 1], [0.19932827387545218, 0.028947725346350685, 0.10019911243477234, 0.1343826123680016, 0.13547174020439157, 0, 0.054912976479134366, 0.2173065761954756, 0.12945098309642153, 31, 0, 1, 1, 1, 0, 27, 0.06315241561496647, 48.0, 1], [0.4979360693214976, 0, 0, 0.06955405141941515, 0.27216267194817445, 0.16034720731091284, 0, 0, 0, 21, 0, 0, 0, 1, 1, 2, 0.1, 23.700432068770276, 0], [0, 0, 0, 0, 0.233800403643385, 0, 0.3105216021358312, 0.0824776840025715, 0.3668984875804275, 32, 1, 1, 1, 0, 1, 2, 0.09426396919569208, 28.92147975554201, 0], [0.19203664255293296, 0, 0.09994441534238696, 0.024052662299051755, 0, 0.04006591209893582, 0.14077247385613217, 0.5031278938505606, 0, 32, 0, 1, 1, 0, 1, 64, 0.03642429903759677, 40.880266460227844, 0], [0.46236560337357324, 0.4739744245596639, 0.06365997206676303, 0, 0, 0, 0, 0, 0, 24, 0, 1, 1, 1, 1, 128, 0.0, 48.0, 1], [0, 0.8058171965699287, 0.017367611535845852, 0, 0, 0, 0.05623791429471005, 0, 0.1205772775995155, 32, 1, 1, 0, 1, 1, 2, 0.023462185471183682, 23.9319938619329, 1], [0.1666292713750449, 0.16249492850275993, 0.047818372842805094, 0.30863338812072943, 0.09867818131686053, 0, 0, 0.06668343994537695, 0.1490624178964231, 27, 1, 1, 0, 1, 1, 76, 5.843653068837864e-21, 42.764537355649836, 1], [0.35492020242851136, 0, 0.0974164568049251, 0.14401062255575137, 0, 0.36583388997883437, 0.03781882823197779, 0, 0, 32, 1, 1, 1, 1, 1, 38, 0.04025098889622258, 21.57581731130951, 1], [0, 0, 0.01691279892600945, 0, 0.1293182502361023, 0.4947608937339757, 0, 0.3590080571039126, 0, 32, 1, 1, 0, 1, 1, 2, 0.1, 30.326431703428444, 1], [0.010995978681327455, 0.015084388841967512, 0.015619755196959958, 0.06452846436945921, 0, 0.10749478332062493, 0.10556752879239532, 0.6403576810663631, 0.033115266460964125, 32, 0, 1, 0, 0, 1, 64, 0.008183115268381932, 48.0, 0], [0.2814799731981487, 0, 0.026742678521773776, 0.06323452865340143, 0.13669631289391668, 0, 0.08675707128914804, 0.4050894354436115, 0, 17, 0, 0, 1, 1, 1, 57, 0.025708305609674333, 19.784518906257205, 1], [0, 0, 0.0631699895148161, 0, 0.010055286911925726, 0.15006907233681877, 0, 0.3111429343195046, 0.4568267924228002, 32, 1, 1, 1, 1, 1, 62, 0.0, 48.0, 1], [0.03162538266977205, 0.011155107393498034, 0, 0, 0.2939741166039937, 0.05386214412051893, 0.014856944561603896, 0.3923169221098232, 0.20220938254079024, 29, 1, 1, 1, 1, 0, 75, 0.07175446461892204, 48.0, 1], [0.1029359424342565, 0.060020786665153356, 0.06391114945262812, 0.1205267160625454, 0.29097008750139386, 0.04304742065720737, 0.07910208129937539, 0.15350097354524617, 0.0859848423821939, 28, 1, 1, 1, 1, 1, 4, 0.0537529405463556, 33.91372098544754, 1], [0.04406966939359226, 0.07006377500403003, 0.04147024197804107, 0.28072456274766, 0.2825183776587697, 0.24493230395603618, 0, 0, 0.03622106926187077, 17, 0, 0, 0, 1, 1, 15, 0.08969804477173335, 26.152475781819035, 1], [0, 0, 0.051163151851725354, 0.31988263480363494, 0.03788000942112947, 0, 0.2497174422555611, 0.341356761667949, 0, 17, 1, 1, 1, 1, 1, 65, 0.020581795293459597, 48.0, 1], [0.03686832452232903, 0.09078687784876982, 0, 0, 0.048165934727638963, 0, 0, 0.7323135144961237, 0.09186534840513845, 30, 1, 1, 1, 1, 1, 2, 0.0800325811833833, 30.662493058598848, 1], [0.5576628979675962, 0, 0, 0.1311382311613929, 0, 0.254719823352649, 0.05439755929175146, 0, 0, 17, 0, 0, 0, 1, 1, 18, 0.1, 30.46929222731287, 1], [0, 0, 0.05194052383845974, 0.506308691698982, 0.3896223847168099, 0, 0.05212839974574863, 0, 0, 32, 0, 0, 0, 1, 1, 10, 0.06203172808859353, 20.910474027095127, 1], [0.02831644518177981, 0.08181714390580282, 0.08091780554285202, 0.08714353986510494, 0.20806095781460637, 0.33716679398867666, 0.06746546207012814, 0.10911185163104936, 0, 32, 1, 1, 1, 1, 1, 64, 0.00417089390274739, 48.0, 1], [0.18822164288288615, 0, 0.0589919203667867, 0.5256714062297699, 0, 0.04474305349806849, 0, 0, 0.17787154970359775, 11, 0, 0, 0, 1, 1, 31, 0.1, 26.723320999611325, 1]], [[0.3903056644998564, 0, 0, 0.1489059269944273, 0.057659274993514135, 0.1504340025233281, 0, 0, 0.252695130988874, 28, 0, 0, 0, 1, 1, 2, 0.1, 40.89587815205037, 1], [0, 0.423122568139625, 0, 0, 0.3007441662496175, 0.031858012263778594, 0, 0, 0.23636553482676093, 25, 0, 1, 0, 1, 0, 2, 0.1, 42.946165120127574, 0], [0.35820298969450964, 0, 0, 0, 0.597527094323973, 0.03679932162193599, 0, 0, 0, 30, 1, 0, 0, 0, 1, 2, 0.1, 40.801480682362154, 0], [0, 0, 0, 0.5442302639977147, 0, 0.05969334597147774, 0, 0.22461055690822257, 0.16173252433160656, 25, 0, 0, 0, 1, 1, 2, 2.0803238346732112e-20, 36.91866136534859, 1], [0.08272906473405253, 0, 0.06543475257691657, 0, 0.013216016239307092, 0.04807219558954898, 0, 0.6120705537104902, 0.1784774171496846, 27, 1, 0, 0, 0, 1, 2, 0.1, 37.93526160084096, 0], [0, 0.14034921417044305, 0.022406242008456537, 0.07847617409753042, 0.42594992681805793, 0.010168839611334104, 0, 0.25630522911933007, 0.06634437417484802, 25, 1, 1, 1, 1, 0, 128, 0.08907714119048751, 1.4800000190734863, 1], [0, 0, 0, 0.3816947131875065, 0.3004042043363621, 0, 0.3179010824761313, 0, 0, 32, 0, 1, 1, 0, 1, 2, 0.1, 29.427911075013554, 1], [0, 0, 0, 0, 0.24300869171715653, 0.37515315985195163, 0, 0, 0.3818381484308921, 32, 1, 1, 1, 0, 1, 2, 0.1, 24.12140819364597, 1], [0.6024385824579679, 0, 0, 0.1300655536802675, 0.26749586386176466, 0, 0, 0, 0, 32, 1, 1, 0, 0, 1, 2, 0.1, 26.830970759432425, 0], [0, 0.13848396611122424, 0.11171362254246515, 0.34656087086433973, 0.042950222357026446, 0.2667363568805333, 0, 0.09355496124441119, 0, 32, 1, 0, 1, 1, 0, 2, 0.1, 16.63435458057141, 1], [0, 0, 0.04531700444291922, 0, 0.15706483276211736, 0.2357099786711085, 0.5619081841238549, 0, 0, 1, 1, 1, 1, 0, 1, 2, 1.0408340855860844e-18, 36.37024918190041, 1], [0, 0, 0.04170235921292702, 0.3135131551281557, 0.14252858367670518, 0.33977635268253986, 0.16247954929967226, 0, 0, 32, 1, 1, 1, 0, 1, 2, 0.0, 28.235230717463594, 1], [0.08320575412428517, 0.2648307467543272, 0.028574103530837402, 0.09114576952398401, 0.014501145109323835, 0.056999889342580844, 0, 0.15657433293428527, 0.3041682586803763, 32, 1, 0, 0, 1, 1, 81, 0.1, 26.759630103845495, 1], [0.049937834755429185, 0, 0.11888065025775536, 0.2665753748491506, 0.1468610520850839, 0, 0.0956317200313578, 0.2189228734055229, 0.10319049461570023, 32, 1, 0, 1, 1, 0, 2, 0.06983877058490391, 46.74842015120022, 1], [0, 0.04668741154256146, 0.12433007371984205, 0.19999744376883194, 0.5758305902653789, 0, 0, 0, 0.05315448070338582, 32, 0, 0, 0, 0, 1, 2, 0.1, 25.93363337382777, 0], [0, 0, 0.053916184916510956, 0.4319993764416861, 0.26746216477696283, 0, 0, 0, 0.24662227386484004, 32, 0, 1, 0, 0, 1, 2, 0.1, 21.21436604583375, 0], [0.13009354135382364, 0, 0, 0.0652852600605899, 0.12082250082327589, 0, 0, 0.6158067118002217, 0.06799198596208886, 32, 1, 1, 1, 1, 1, 2, 0.1, 18.800699763962648, 1], [0.25548986072692614, 0.05868596900223586, 0, 0.20633419253469626, 0.10516319072266248, 0, 0.10572700707318904, 0.17403329192561043, 0.09456648801467965, 32, 1, 0, 1, 1, 0, 128, 0.0975092467419507, 48.0, 1], [0.5492073707502501, 0, 0, 0, 0.1032423720022609, 0, 0, 0.34650827151604574, 0, 31, 0, 1, 1, 1, 1, 5, 0.001989938248713737, 42.40935210380895, 1], [0, 0.1759947541346064, 0.03365808889031197, 0.10922098301045542, 0, 0.14277086384555646, 0.3160813975208243, 0.1557283211061502, 0.06654559149209519, 29, 0, 1, 1, 0, 1, 51, 1.7347234759768077e-19, 7.69229659836283, 1], [0, 0.4424681355214227, 0.011379298130663702, 0, 0.48134366822043634, 0, 0, 0, 0.06480889812747713, 32, 1, 1, 0, 1, 1, 2, 0.07255222982946861, 30.458116050975235, 0], [0, 0, 0.1465747904823793, 0, 0.31844537582453986, 0.09531567281605835, 0, 0.3629872221631586, 0.07667693871386394, 32, 0, 1, 1, 0, 1, 2, 0.1, 30.769741866916824, 1], [0, 0, 0.034010414461726764, 0.09654388036035852, 0.07585840666065592, 0.6756705406969704, 0, 0.087810103560339, 0.03010665425994938, 32, 0, 0, 1, 0, 1, 2, 0.0538785603495793, 36.404007104378394, 0], [0, 0, 0.010978866994264551, 0, 0, 0.05744912534323048, 0.05680018443810788, 0.40460303013466903, 0.4701687930897281, 32, 0, 1, 1, 1, 1, 16, 0.011452236897953084, 44.000244057598046, 1], [0.32022871854108403, 0, 0, 0.17595600769900102, 0, 0.4231334676991652, 0.05762562086303304, 0.023056185197716714, 0, 32, 0, 1, 1, 1, 1, 50, 0.1, 12.312887206142818, 1], [0, 0, 0.010576524116221347, 0.33470001445863873, 0.12530795032593106, 0, 0, 0, 0.5294155110992089, 32, 0, 0, 0, 0, 1, 108, 0.003930346859619269, 42.6064912403332, 1], [0, 0, 0, 0, 0.9999999999999982, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 4, 3.851859888774472e-35, 15.508628544336682, 0], [0, 0, 0.08043332732832541, 0.134555281902639, 0.3501896286904793, 0, 0.11458814607847323, 0.03523921029769962, 0.2849944057023834, 32, 0, 0, 1, 0, 1, 72, 0.005811624469743303, 36.19052627866368, 1], [0, 0, 0, 0.13457903410128969, 0.09828108840351264, 0, 0.09428032583783842, 0.49529115253725475, 0.1775683991201044, 1, 1, 1, 1, 1, 0, 77, 0.1, 36.885809789645904, 1], [0.09473725796213506, 0.02893877643456468, 0, 0.12323190995408453, 0.3214734244300721, 0.08472892800222666, 0.018796131414842737, 0.26074705637838536, 0.0673465154236889, 29, 0, 0, 0, 1, 1, 28, 0.1, 21.73223790798518, 0]]], 'random_full_train_performance': [-0.9422724843025208, -0.8778097629547119, -1.5629523992538452, -0.885058581829071, -0.8790983557701111, -0.8922914862632751, -2.2360587120056152, -1.0187774896621704, -1.5669572353363037, -1.8603237867355347, -2.229483127593994, -2.276233673095703, -0.949447751045227, -0.8668954968452454, -0.8745988011360168, -0.9219691157341003, -0.8699289560317993, -0.8608999252319336, -1.3233754634857178, -0.9297061562538147, -0.8737995624542236, -0.869259774684906, -0.8957942724227905, -1.0808058977127075, -1.2452932596206665, -1.0699563026428223, -2.383497714996338, -0.9497308135032654, -0.9827616810798645, -0.870092511177063]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.055 MB of 0.055 MB uploadedwandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.055 MB of 0.055 MB uploadedwandb: - 0.091 MB of 0.097 MB uploaded (0.006 MB deduped)wandb: \ 0.932 MB of 1.291 MB uploaded (0.006 MB deduped)wandb: | 1.291 MB of 1.291 MB uploaded (0.006 MB deduped)wandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÖ
wandb:            eval/runtime ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñÑ
wandb: eval/samples_per_second ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñÖ
wandb:   eval/steps_per_second ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñÖ
wandb:             train/epoch ‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÖ‚ñá‚ñÉ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÑ‚ñá‚ñÉ‚ñá‚ñÉ‚ñá
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÖ‚ñá‚ñÉ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñà‚ñÖ‚ñà‚ñÖ‚ñá‚ñÉ‚ñá‚ñÉ‚ñá
wandb:         train/grad_norm ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÅ‚ñÑ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÖ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñá‚ñÉ‚ñà‚ñÉ‚ñá‚ñÉ‚ñà‚ñÉ‚ñá‚ñÇ‚ñà‚ñÉ‚ñÜ‚ñÅ‚ñá‚ñÉ‚ñÜ‚ñÅ‚ñá‚ñÉ‚ñÜ‚ñÅ‚ñá‚ñÇ‚ñÖ‚ñà‚ñÜ‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñÅ‚ñÖ‚ñà‚ñÜ‚ñÇ‚ñÖ‚ñÑ
wandb:              train/loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñà
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.87009
wandb:             eval/runtime 7.0205
wandb:  eval/samples_per_second 142.441
wandb:    eval/steps_per_second 8.974
wandb:               total_flos 9.481045816639488e+16
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 0.36976
wandb:      train/learning_rate 0.0
wandb:               train/loss 1.0366
wandb:               train_loss 1.17744
wandb:            train_runtime 360.6556
wandb: train_samples_per_second 27.716
wandb:   train_steps_per_second 1.733
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/8xamcowb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260101_054053-8xamcowb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

2026-01-01 23:33:01.500033: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-01 23:33:01.528589: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-01 23:33:01.528628: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-01 23:33:01.529601: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-01 23:33:01.534285: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-01 23:33:02.396185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'gsm8k', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_gsm8k_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['gsm8k']
evaluation tasks and weights:  {'gsm8k': (1.0, 'exact_match,strict-match')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/gsm8k/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 9, 1, 1, 0, 0, 1, 14, 0.03669660801939172, 9, 1]
Checking history sample input_X_between_0_1:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 0.28125, 1.0, 1.0, 0.0, 0.0, 1.0, 0.109375, 0.3669660801939172, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.7
Checking history sample input_X:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 4, 1, 1, 0, 1, 0, 45, 0.02996529708553877, 41, 0]
Checking history sample input_X_between_0_1:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 0.125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3515625, 0.29965297085538767, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 5, 0, 1, 1, 0, 0, 115, 0.08044623754842878, 26, 0]
Checking history sample input_X_between_0_1:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 0.15625, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8984375, 0.8044623754842878, 0.5416666666666666, 0.0]
Checking history sample performance at 625 steps:  0.73
Checking history sample input_X:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 30, 0, 1, 0, 1, 1, 121, 0.019265932894845208, 29, 1]
Checking history sample input_X_between_0_1:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 0.9375, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9453125, 0.19265932894845206, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 19, 0, 0, 1, 1, 1, 108, 0.09306302255978231, 35, 1]
Checking history sample input_X_between_0_1:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 0.59375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.84375, 0.930630225597823, 0.7291666666666666, 1.0]
Checking history sample performance at 625 steps:  0.62
Checking history sample input_X:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 14, 0, 0, 0, 1, 1, 44, 0.004820496247456452, 22, 1]
Checking history sample input_X_between_0_1:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 0.4375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.34375, 0.04820496247456452, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.63
Checking history sample input_X:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 24, 0, 1, 1, 0, 1, 79, 0.03164316476579598, 9, 1]
Checking history sample input_X_between_0_1:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6171875, 0.3164316476579597, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 29, 0, 1, 0, 1, 1, 23, 0.015858984905097274, 40, 0]
Checking history sample input_X_between_0_1:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 0.90625, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1796875, 0.15858984905097273, 0.8333333333333334, 0.0]
Checking history sample performance at 625 steps:  0.6
Checking history sample input_X:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 27, 0, 0, 1, 0, 0, 58, 0.02356583389888708, 4, 0]
Checking history sample input_X_between_0_1:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 0.84375, 0.0, 0.0, 1.0, 0.0, 0.0, 0.453125, 0.23565833898887079, 0.08333333333333333, 0.0]
Checking history sample performance at 625 steps:  0.65
Checking history sample input_X:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 18, 0, 1, 1, 0, 1, 102, 0.07944916714916157, 39, 0]
Checking history sample input_X_between_0_1:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 0.5625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.796875, 0.7944916714916157, 0.8125, 0.0]
Checking history sample performance at 625 steps:  0.62
Checking history sample input_X:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 16, 1, 0, 0, 0, 0, 57, 0.051177108415818844, 16, 1]
Checking history sample input_X_between_0_1:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4453125, 0.5117710841581884, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.71
Checking history sample input_X:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 21, 0, 1, 0, 0, 1, 80, 0.08438899074092726, 11, 1]
Checking history sample input_X_between_0_1:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 0.65625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.625, 0.8438899074092726, 0.22916666666666666, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 19, 0, 1, 1, 1, 0, 106, 0.021750066051679486, 16, 1]
Checking history sample input_X_between_0_1:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 0.59375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.828125, 0.21750066051679484, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 6, 1, 0, 0, 0, 1, 109, 0.0976089152670931, 17, 0]
Checking history sample input_X_between_0_1:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 0.1875, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8515625, 0.976089152670931, 0.3541666666666667, 0.0]
Checking history sample performance at 625 steps:  0.73
Checking history sample input_X:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 5, 0, 1, 1, 0, 1, 6, 0.045424701431822194, 31, 1]
Checking history sample input_X_between_0_1:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 0.15625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.046875, 0.4542470143182219, 0.6458333333333334, 1.0]
Checking history sample performance at 625 steps:  0.67
Checking history sample input_X:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 18, 1, 0, 0, 0, 0, 27, 0.050875722794158945, 27, 1]
Checking history sample input_X_between_0_1:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 0.5625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2109375, 0.5087572279415894, 0.5625, 1.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 25, 0, 0, 0, 1, 1, 5, 0.0932969995849881, 3, 0]
Checking history sample input_X_between_0_1:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 0.78125, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0390625, 0.932969995849881, 0.0625, 0.0]
Checking history sample performance at 625 steps:  0.65
Checking history sample input_X:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 14, 0, 0, 0, 1, 0, 14, 0.06234381073507801, 24, 1]
Checking history sample input_X_between_0_1:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 0.4375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.109375, 0.6234381073507801, 0.5, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 19, 1, 1, 0, 1, 0, 54, 0.0659080396027719, 16, 1]
Checking history sample input_X_between_0_1:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 0.59375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.421875, 0.659080396027719, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.67
Checking history sample input_X:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 30, 1, 1, 0, 1, 0, 113, 0.08743251828499332, 21, 0]
Checking history sample input_X_between_0_1:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 0.9375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8828125, 0.8743251828499332, 0.4375, 0.0]
Checking history sample performance at 625 steps:  0.63
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4419 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.04721724987030029, 0.9552534818649292, 0.7877234220504761, 0.9343103170394897, 0.7920901775360107, 0.016238808631896973, 0.5277969837188721, 0.6896137595176697, 0.851170539855957, 0.5446590781211853, 0.4593464732170105, 0.9650031924247742, 0.14034557342529297, 0.965531587600708, 0.9692542552947998, 0.25340819358825684, 0.351277232170105, 0.5913376808166504, 0.8002597093582153]  ‚Üí  acq = 0.8466325776282386
X = [0.5514346361160278, 0.3467305302619934, 0.9695500135421753, 0.15956681966781616, 0.3163951635360718, 0.9154331088066101, 0.5655574798583984, 0.14875507354736328, 0.9060488939285278, 0.9076350331306458, 0.9575125575065613, 0.7412155270576477, 0.7786166667938232, 0.858634352684021, 0.5375894904136658, 0.7437804937362671, 0.4195660352706909, 0.4245356619358063, 0.29956281185150146]  ‚Üí  acq = 0.8466261660116958
X = [0.22735458612442017, 0.21345436573028564, 0.2868765592575073, 0.10611122846603394, 0.1178324818611145, 0.6689758896827698, 0.44643378257751465, 0.7314204573631287, 0.44307100772857666, 0.141457200050354, 0.9654361605644226, 0.6157980561256409, 0.8275841474533081, 0.8103951811790466, 0.25714796781539917, 0.12356750667095184, 0.005941212177276611, 0.9863280057907104, 0.5831512212753296]  ‚Üí  acq = 0.8474564583325223
X = [0.037648797035217285, 0.8307607173919678, 0.550024151802063, 0.18017929792404175, 0.5817824006080627, 0.6496009826660156, 0.3821280598640442, 0.7011111974716187, 0.11027246713638306, 0.9743361473083496, 0.1613714098930359, 0.25635451078414917, 0.12883728742599487, 0.6904253959655762, 0.7460834383964539, 0.9972613453865051, 0.5962762832641602, 0.10387539118528366, 0.8506726622581482]  ‚Üí  acq = 0.8466325858472887
X = [0.05562150478363037, 0.6058835387229919, 0.30786025524139404, 0.39311569929122925, 0.2579893469810486, 0.916362464427948, 0.46542543172836304, 0.916629433631897, 0.5571898818016052, 0.04553040489554405, 0.9530133008956909, 0.23926013708114624, 0.034314751625061035, 0.062207937240600586, 0.5334619879722595, 0.8159095048904419, 0.5127643346786499, 0.5380191802978516, 0.5375372171401978]  ‚Üí  acq = 0.8466358938575302
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.0354, dtype=torch.float64), tensor(0.0284, dtype=torch.float64), tensor(0.1469, dtype=torch.float64), tensor(0.1130, dtype=torch.float64), tensor(0.2353, dtype=torch.float64), tensor(0.0995, dtype=torch.float64), tensor(0.2728, dtype=torch.float64), tensor(0.0687, dtype=torch.float64), 14, 1, 0, 1, 1, 1, 26, 0.06319152730770125, 19.453576244216457, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(1.7710e-19, dtype=torch.float64), tensor(0.0354, dtype=torch.float64), tensor(0.0284, dtype=torch.float64), tensor(0.1469, dtype=torch.float64), tensor(0.1130, dtype=torch.float64), tensor(0.2353, dtype=torch.float64), tensor(0.0995, dtype=torch.float64), tensor(0.2728, dtype=torch.float64), tensor(0.0687, dtype=torch.float64), tensor(0.4411, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2056, dtype=torch.float64), tensor(0.6319, dtype=torch.float64), tensor(0.4053, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.035
  rowan_hellaswag: 0.028
  sciq: 0.147
  triviaqa: 0.113
  truthfulqa_gen: 0.235
  wikitext: 0.1
  mmlu: 0.273
  arc_challenge: 0.069

LoRA Parameters:
  lora_r: (26,)
  lora_dropout: (0.06319152730770125,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (19.453576244216457,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  26
lora dropout:  0.06319152730770125
lora alpha:  19.453576244216457
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 23,109,632 || all params: 8,053,370,880 || trainable%: 0.2870
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260101_233441-9ejh3okx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/9ejh3okx
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:07<11:45,  7.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:25,  1.06it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:13<00:50,  1.64it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:15<00:35,  2.13it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:18<00:28,  2.33it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:23<00:29,  1.97it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:26<00:23,  2.15it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:31<00:22,  1.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:15,  2.23it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:36<00:11,  2.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:41<00:08,  2.12it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:45<00:05,  2.18it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:47<00:01,  2.44it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:47<00:00,  2.11it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.3061, 'grad_norm': 1.276941180229187, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.1401045322418213, 'eval_runtime': 8.6875, 'eval_samples_per_second': 114.993, 'eval_steps_per_second': 7.252, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:34,  3.99s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:26,  1.05it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:50,  1.63it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:36,  2.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:18<00:30,  2.19it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:22<00:29,  1.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:25<00:22,  2.22it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:29<00:19,  2.24it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:31<00:13,  2.58it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:33<00:09,  2.79it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:08,  2.26it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:41<00:04,  2.36it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:44<00:01,  2.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:44<00:00,  2.27it/s]
Evaluation performance at step 50: 0.75
{'loss': 1.6813, 'grad_norm': 0.42720335721969604, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.43772554397583, 'eval_runtime': 8.7583, 'eval_samples_per_second': 114.064, 'eval_steps_per_second': 7.193, 'epoch': 0.08}
{'loss': 1.4605, 'grad_norm': 0.39668336510658264, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3209645748138428, 'eval_runtime': 8.7867, 'eval_samples_per_second': 113.695, 'eval_steps_per_second': 7.17, 'epoch': 0.12}
{'loss': 1.3191, 'grad_norm': 0.31880560517311096, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2773579359054565, 'eval_runtime': 8.7527, 'eval_samples_per_second': 114.137, 'eval_steps_per_second': 7.198, 'epoch': 0.16}
{'loss': 1.3045, 'grad_norm': 0.3515155613422394, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2504081726074219, 'eval_runtime': 8.7187, 'eval_samples_per_second': 114.581, 'eval_steps_per_second': 7.226, 'epoch': 0.2}
{'loss': 1.259, 'grad_norm': 0.30616873502731323, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2322111129760742, 'eval_runtime': 8.7131, 'eval_samples_per_second': 114.655, 'eval_steps_per_second': 7.231, 'epoch': 0.24}
{'loss': 1.3096, 'grad_norm': 0.34329456090927124, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2179768085479736, 'eval_runtime': 8.7665, 'eval_samples_per_second': 113.957, 'eval_steps_per_second': 7.186, 'epoch': 0.28}
{'loss': 1.2695, 'grad_norm': 0.2625048756599426, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.204725980758667, 'eval_runtime': 8.7637, 'eval_samples_per_second': 113.994, 'eval_steps_per_second': 7.189, 'epoch': 0.32}
{'loss': 1.2325, 'grad_norm': 0.3524690866470337, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1928951740264893, 'eval_runtime': 8.7448, 'eval_samples_per_second': 114.239, 'eval_steps_per_second': 7.204, 'epoch': 0.36}
{'loss': 1.2424, 'grad_norm': 0.3536989688873291, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1822530031204224, 'eval_runtime': 8.7541, 'eval_samples_per_second': 114.118, 'eval_steps_per_second': 7.197, 'epoch': 0.4}
{'loss': 1.2229, 'grad_norm': 0.32039862871170044, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.170836329460144, 'eval_runtime': 8.7568, 'eval_samples_per_second': 114.083, 'eval_steps_per_second': 7.194, 'epoch': 0.44}
{'loss': 1.1756, 'grad_norm': 0.288835346698761, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1604533195495605, 'eval_runtime': 8.7554, 'eval_samples_per_second': 114.101, 'eval_steps_per_second': 7.196, 'epoch': 0.48}
{'loss': 1.144, 'grad_norm': 0.3009420335292816, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1533459424972534, 'eval_runtime': 8.7568, 'eval_samples_per_second': 114.083, 'eval_steps_per_second': 7.194, 'epoch': 0.52}
{'loss': 1.2594, 'grad_norm': 0.3797720968723297, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1424962282180786, 'eval_runtime': 8.7526, 'eval_samples_per_second': 114.137, 'eval_steps_per_second': 7.198, 'epoch': 0.56}
{'loss': 1.1802, 'grad_norm': 0.360757440328598, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1337584257125854, 'eval_runtime': 8.7645, 'eval_samples_per_second': 113.982, 'eval_steps_per_second': 7.188, 'epoch': 0.6}
{'loss': 1.1481, 'grad_norm': 0.3324506878852844, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1258714199066162, 'eval_runtime': 8.7517, 'eval_samples_per_second': 114.149, 'eval_steps_per_second': 7.199, 'epoch': 0.64}
{'loss': 1.1421, 'grad_norm': 0.33638453483581543, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1194758415222168, 'eval_runtime': 8.7348, 'eval_samples_per_second': 114.371, 'eval_steps_per_second': 7.213, 'epoch': 0.68}
{'loss': 1.2612, 'grad_norm': 0.37589728832244873, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1138989925384521, 'eval_runtime': 8.7065, 'eval_samples_per_second': 114.741, 'eval_steps_per_second': 7.236, 'epoch': 0.72}
{'loss': 1.2175, 'grad_norm': 0.28410056233406067, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1042063236236572, 'eval_runtime': 8.7071, 'eval_samples_per_second': 114.734, 'eval_steps_per_second': 7.235, 'epoch': 0.76}
{'loss': 1.1658, 'grad_norm': 0.3705412745475769, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0999040603637695, 'eval_runtime': 8.7049, 'eval_samples_per_second': 114.763, 'eval_steps_per_second': 7.237, 'epoch': 0.8}
{'loss': 1.218, 'grad_norm': 0.40326592326164246, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0931077003479004, 'eval_runtime': 8.6961, 'eval_samples_per_second': 114.88, 'eval_steps_per_second': 7.245, 'epoch': 0.84}
{'loss': 1.134, 'grad_norm': 0.43791452050209045, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0857282876968384, 'eval_runtime': 8.6954, 'eval_samples_per_second': 114.888, 'eval_steps_per_second': 7.245, 'epoch': 0.88}
{'loss': 1.1987, 'grad_norm': 0.40276890993118286, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0832120180130005, 'eval_runtime': 8.6997, 'eval_samples_per_second': 114.832, 'eval_steps_per_second': 7.242, 'epoch': 0.92}
{'loss': 1.0947, 'grad_norm': 0.294018417596817, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0802799463272095, 'eval_runtime': 8.7079, 'eval_samples_per_second': 114.723, 'eval_steps_per_second': 7.235, 'epoch': 0.96}
{'loss': 1.0789, 'grad_norm': 0.42248794436454773, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0791138410568237, 'eval_runtime': 8.7465, 'eval_samples_per_second': 114.218, 'eval_steps_per_second': 7.203, 'epoch': 1.0}
{'train_runtime': 513.9179, 'train_samples_per_second': 19.451, 'train_steps_per_second': 1.216, 'train_loss': 1.3210254180908203, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1401045322418213, 1.43772554397583, 1.3209645748138428, 1.2773579359054565, 1.2504081726074219, 1.2322111129760742, 1.2179768085479736, 1.204725980758667, 1.1928951740264893, 1.1822530031204224, 1.170836329460144, 1.1604533195495605, 1.1533459424972534, 1.1424962282180786, 1.1337584257125854, 1.1258714199066162, 1.1194758415222168, 1.1138989925384521, 1.1042063236236572, 1.0999040603637695, 1.0931077003479004, 1.0857282876968384, 1.0832120180130005, 1.0802799463272095, 1.0791138410568237], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:10<17:23, 10.54s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:19<01:19,  1.04it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:28<00:48,  1.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:32<00:26,  1.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:40<00:18,  1.93it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:48<00:09,  1.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:50<00:01,  2.61it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:50<00:00,  1.97it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.6226346492767334
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.7454999999999999
BO observations:  [0.6226346492767334]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4559 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 0.812995193348727
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 0.8129951933503804
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 0.8129951933502111
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 0.8129921641941442
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 0.8129954200773892
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4116, dtype=torch.float64), tensor(0.0498, dtype=torch.float64), 0, tensor(0.5011, dtype=torch.float64), 0, 0, tensor(0.0375, dtype=torch.float64), 0, 1, 1, 0, 0, 0, 0, 128, 0.05259140646327104, 43.68586075601523, 1]
normalized proposed parameters for next round by BO: [tensor(1.8767e-18, dtype=torch.float64), tensor(0.4116, dtype=torch.float64), tensor(0.0498, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5011, dtype=torch.float64), tensor(4.1138e-19, dtype=torch.float64), tensor(1.2608e-18, dtype=torch.float64), tensor(0.0375, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9977, dtype=torch.float64), tensor(0.5259, dtype=torch.float64), tensor(0.9101, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.412
  rowan_hellaswag: 0.05
  sciq: 0
  triviaqa: 0.501
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.037
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05259140646327104,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (43.68586075601523,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  128
lora dropout:  0.05259140646327104
lora alpha:  43.68586075601523
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,048,576 || all params: 8,031,309,824 || trainable%: 0.0131
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<04:57,  3.01s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.24it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.97it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:27,  2.47it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:23,  2.48it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.66it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.65it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.92it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.15it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:06,  3.13it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.35it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.61it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.85it/s]
Evaluation performance at step 25: 0.78
{'loss': 3.4961, 'grad_norm': 0.21146555244922638, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 3.4143683910369873, 'eval_runtime': 8.6183, 'eval_samples_per_second': 115.916, 'eval_steps_per_second': 7.31, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:36,  2.80s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:44,  2.05it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.56it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:25,  3.00it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  2.92it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.82it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.95it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  3.01it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.34it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:07,  3.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:06,  2.72it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.03it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.2825, 'grad_norm': 0.15026076138019562, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 3.051248073577881, 'eval_runtime': 8.6246, 'eval_samples_per_second': 115.831, 'eval_steps_per_second': 7.305, 'epoch': 0.08}
{'loss': 2.8938, 'grad_norm': 0.20497851073741913, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.8169078826904297, 'eval_runtime': 8.6309, 'eval_samples_per_second': 115.747, 'eval_steps_per_second': 7.299, 'epoch': 0.12}
{'loss': 2.8079, 'grad_norm': 0.25310778617858887, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.640352249145508, 'eval_runtime': 8.6508, 'eval_samples_per_second': 115.48, 'eval_steps_per_second': 7.283, 'epoch': 0.16}
{'loss': 2.6577, 'grad_norm': 0.547121524810791, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.549015760421753, 'eval_runtime': 8.6977, 'eval_samples_per_second': 114.858, 'eval_steps_per_second': 7.243, 'epoch': 0.2}
{'loss': 2.5003, 'grad_norm': 0.3555856943130493, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.4917759895324707, 'eval_runtime': 8.6964, 'eval_samples_per_second': 114.876, 'eval_steps_per_second': 7.244, 'epoch': 0.24}
{'loss': 2.4926, 'grad_norm': 0.11667201668024063, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.445347309112549, 'eval_runtime': 8.696, 'eval_samples_per_second': 114.88, 'eval_steps_per_second': 7.245, 'epoch': 0.28}
{'loss': 2.3957, 'grad_norm': 0.39462336897850037, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.416876792907715, 'eval_runtime': 8.6995, 'eval_samples_per_second': 114.834, 'eval_steps_per_second': 7.242, 'epoch': 0.32}
{'loss': 2.4176, 'grad_norm': 0.24413053691387177, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.389448404312134, 'eval_runtime': 8.7143, 'eval_samples_per_second': 114.64, 'eval_steps_per_second': 7.23, 'epoch': 0.36}
{'loss': 2.3505, 'grad_norm': 0.30411413311958313, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.3681161403656006, 'eval_runtime': 8.7028, 'eval_samples_per_second': 114.791, 'eval_steps_per_second': 7.239, 'epoch': 0.4}
{'loss': 2.3831, 'grad_norm': 0.3523505926132202, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.34653902053833, 'eval_runtime': 8.6825, 'eval_samples_per_second': 115.059, 'eval_steps_per_second': 7.256, 'epoch': 0.44}
{'loss': 2.2725, 'grad_norm': 1.0716357231140137, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.329123020172119, 'eval_runtime': 8.6672, 'eval_samples_per_second': 115.262, 'eval_steps_per_second': 7.269, 'epoch': 0.48}
{'loss': 2.3242, 'grad_norm': 1.1488609313964844, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.3111841678619385, 'eval_runtime': 8.7006, 'eval_samples_per_second': 114.82, 'eval_steps_per_second': 7.241, 'epoch': 0.52}
{'loss': 2.314, 'grad_norm': 0.3068329989910126, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.292019844055176, 'eval_runtime': 8.7912, 'eval_samples_per_second': 113.637, 'eval_steps_per_second': 7.166, 'epoch': 0.56}
{'loss': 2.2361, 'grad_norm': 0.31695663928985596, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.2744550704956055, 'eval_runtime': 8.7706, 'eval_samples_per_second': 113.903, 'eval_steps_per_second': 7.183, 'epoch': 0.6}
{'loss': 2.247, 'grad_norm': 0.2291402518749237, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.2601723670959473, 'eval_runtime': 8.7598, 'eval_samples_per_second': 114.044, 'eval_steps_per_second': 7.192, 'epoch': 0.64}
{'loss': 2.3104, 'grad_norm': 0.16487178206443787, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2485923767089844, 'eval_runtime': 8.7388, 'eval_samples_per_second': 114.318, 'eval_steps_per_second': 7.209, 'epoch': 0.68}
{'loss': 2.2471, 'grad_norm': 0.13550521433353424, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.239583730697632, 'eval_runtime': 8.7135, 'eval_samples_per_second': 114.65, 'eval_steps_per_second': 7.23, 'epoch': 0.72}
{'loss': 2.2201, 'grad_norm': 0.14896443486213684, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.232710838317871, 'eval_runtime': 8.7077, 'eval_samples_per_second': 114.726, 'eval_steps_per_second': 7.235, 'epoch': 0.76}
{'loss': 2.2102, 'grad_norm': 0.32557573914527893, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.227562189102173, 'eval_runtime': 8.6613, 'eval_samples_per_second': 115.34, 'eval_steps_per_second': 7.274, 'epoch': 0.8}
{'loss': 2.2853, 'grad_norm': 0.15800079703330994, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.222140073776245, 'eval_runtime': 8.6545, 'eval_samples_per_second': 115.431, 'eval_steps_per_second': 7.279, 'epoch': 0.84}
{'loss': 2.2461, 'grad_norm': 0.16351765394210815, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.2184762954711914, 'eval_runtime': 8.6649, 'eval_samples_per_second': 115.293, 'eval_steps_per_second': 7.271, 'epoch': 0.88}
{'loss': 2.197, 'grad_norm': 0.12237463891506195, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.215641498565674, 'eval_runtime': 8.6388, 'eval_samples_per_second': 115.641, 'eval_steps_per_second': 7.293, 'epoch': 0.92}
{'loss': 2.2702, 'grad_norm': 0.2865794897079468, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.2129580974578857, 'eval_runtime': 8.6557, 'eval_samples_per_second': 115.416, 'eval_steps_per_second': 7.278, 'epoch': 0.96}
{'loss': 2.2129, 'grad_norm': 0.1691332310438156, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.2124972343444824, 'eval_runtime': 8.654, 'eval_samples_per_second': 115.438, 'eval_steps_per_second': 7.28, 'epoch': 1.0}
{'train_runtime': 505.777, 'train_samples_per_second': 19.768, 'train_steps_per_second': 1.236, 'train_loss': 2.450844403076172, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4143683910369873, 3.051248073577881, 2.8169078826904297, 2.640352249145508, 2.549015760421753, 2.4917759895324707, 2.445347309112549, 2.416876792907715, 2.389448404312134, 2.3681161403656006, 2.34653902053833, 2.329123020172119, 2.3111841678619385, 2.292019844055176, 2.2744550704956055, 2.2601723670959473, 2.2485923767089844, 2.239583730697632, 2.232710838317871, 2.227562189102173, 2.222140073776245, 2.2184762954711914, 2.215641498565674, 2.2129580974578857, 2.2124972343444824], 'performance': [0.78, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<13:37,  8.26s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:11,  1.17it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:43,  1.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:31<00:27,  1.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:36<00:16,  2.16it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:44<00:08,  2.15it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:46<00:01,  2.88it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:46<00:00,  2.17it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  0.794015645980835
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.6226346492767334, 0.794015645980835]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8716 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 0.8056102497175526
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 0.8051119247006776
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 0.8056103297335516
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 0.805602233858527
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.8056317664267638
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3661, dtype=torch.float64), 0, 0, tensor(0.0410, dtype=torch.float64), tensor(0.0699, dtype=torch.float64), 0, tensor(0.1787, dtype=torch.float64), tensor(0.1742, dtype=torch.float64), tensor(0.1701, dtype=torch.float64), 32, 1, 0, 0, 0, 0, 7, 0.032816262115845915, 1.4800000190734866, 1]
normalized proposed parameters for next round by BO: [tensor(0.3661, dtype=torch.float64), tensor(7.4550e-19, dtype=torch.float64), tensor(2.7025e-18, dtype=torch.float64), tensor(0.0410, dtype=torch.float64), tensor(0.0699, dtype=torch.float64), tensor(4.1775e-18, dtype=torch.float64), tensor(0.1787, dtype=torch.float64), tensor(0.1742, dtype=torch.float64), tensor(0.1701, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0538, dtype=torch.float64), tensor(0.3282, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.366
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.041
  triviaqa: 0.07
  truthfulqa_gen: 0
  wikitext: 0.179
  mmlu: 0.174
  arc_challenge: 0.17

LoRA Parameters:
  lora_r: (7,)
  lora_dropout: (0.032816262115845915,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  7
lora dropout:  0.032816262115845915
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,835,008 || all params: 8,032,096,256 || trainable%: 0.0228
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:58,  4.23s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:24,  1.07it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:47,  1.74it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:33,  2.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:17<00:30,  2.18it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:21<00:25,  2.29it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:20,  2.50it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:17,  2.50it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:29<00:12,  2.76it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:31<00:09,  2.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:06,  3.07it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  3.25it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.48it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.64it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.2837, 'grad_norm': 0.4341158866882324, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 4.136192798614502, 'eval_runtime': 7.3597, 'eval_samples_per_second': 135.738, 'eval_steps_per_second': 8.56, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:08,  3.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:51,  1.78it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:34,  2.38it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:22,  2.93it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:24,  2.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:19,  2.55it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:16,  2.57it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  2.92it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:06,  3.02it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.20it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.43it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.91it/s]
Evaluation performance at step 50: 0.8
{'loss': 3.5436, 'grad_norm': 0.5969629883766174, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.8}
{'eval_loss': 2.9312562942504883, 'eval_runtime': 7.3451, 'eval_samples_per_second': 136.009, 'eval_steps_per_second': 8.577, 'epoch': 0.08}
{'loss': 2.5437, 'grad_norm': 0.19079937040805817, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.2516305446624756, 'eval_runtime': 7.373, 'eval_samples_per_second': 135.494, 'eval_steps_per_second': 8.545, 'epoch': 0.12}
{'loss': 2.0589, 'grad_norm': 0.16600747406482697, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8672994375228882, 'eval_runtime': 7.3686, 'eval_samples_per_second': 135.575, 'eval_steps_per_second': 8.55, 'epoch': 0.16}
{'loss': 1.7567, 'grad_norm': 0.23974187672138214, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7131792306900024, 'eval_runtime': 7.403, 'eval_samples_per_second': 134.945, 'eval_steps_per_second': 8.51, 'epoch': 0.2}
{'loss': 1.7071, 'grad_norm': 0.16394436359405518, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6821284294128418, 'eval_runtime': 7.4012, 'eval_samples_per_second': 134.977, 'eval_steps_per_second': 8.512, 'epoch': 0.24}
{'loss': 1.677, 'grad_norm': 0.12972582876682281, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.665148138999939, 'eval_runtime': 7.4258, 'eval_samples_per_second': 134.53, 'eval_steps_per_second': 8.484, 'epoch': 0.28}
{'loss': 1.6582, 'grad_norm': 0.1845100224018097, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.654815673828125, 'eval_runtime': 7.4319, 'eval_samples_per_second': 134.421, 'eval_steps_per_second': 8.477, 'epoch': 0.32}
{'loss': 1.6458, 'grad_norm': 0.1842564046382904, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6466193199157715, 'eval_runtime': 7.4342, 'eval_samples_per_second': 134.379, 'eval_steps_per_second': 8.474, 'epoch': 0.36}
{'loss': 1.6473, 'grad_norm': 0.2779902219772339, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6390843391418457, 'eval_runtime': 7.4415, 'eval_samples_per_second': 134.247, 'eval_steps_per_second': 8.466, 'epoch': 0.4}
{'loss': 1.6187, 'grad_norm': 0.16377313435077667, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6322287321090698, 'eval_runtime': 7.4342, 'eval_samples_per_second': 134.379, 'eval_steps_per_second': 8.474, 'epoch': 0.44}
{'loss': 1.6198, 'grad_norm': 0.18325458467006683, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6236943006515503, 'eval_runtime': 7.4324, 'eval_samples_per_second': 134.411, 'eval_steps_per_second': 8.476, 'epoch': 0.48}
{'loss': 1.6266, 'grad_norm': 0.2231682389974594, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6176601648330688, 'eval_runtime': 7.4394, 'eval_samples_per_second': 134.285, 'eval_steps_per_second': 8.468, 'epoch': 0.52}
{'loss': 1.6069, 'grad_norm': 0.14186258614063263, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6144996881484985, 'eval_runtime': 7.4389, 'eval_samples_per_second': 134.294, 'eval_steps_per_second': 8.469, 'epoch': 0.56}
{'loss': 1.5949, 'grad_norm': 0.1545274257659912, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6092625856399536, 'eval_runtime': 7.4539, 'eval_samples_per_second': 134.024, 'eval_steps_per_second': 8.452, 'epoch': 0.6}
{'loss': 1.5888, 'grad_norm': 0.19157809019088745, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6046267747879028, 'eval_runtime': 7.4381, 'eval_samples_per_second': 134.309, 'eval_steps_per_second': 8.47, 'epoch': 0.64}
{'loss': 1.6663, 'grad_norm': 0.1327745020389557, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.601379156112671, 'eval_runtime': 7.4208, 'eval_samples_per_second': 134.622, 'eval_steps_per_second': 8.49, 'epoch': 0.68}
{'loss': 1.5708, 'grad_norm': 0.15457920730113983, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5986337661743164, 'eval_runtime': 7.4246, 'eval_samples_per_second': 134.552, 'eval_steps_per_second': 8.485, 'epoch': 0.72}
{'loss': 1.6079, 'grad_norm': 0.21670833230018616, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5945709943771362, 'eval_runtime': 7.4251, 'eval_samples_per_second': 134.543, 'eval_steps_per_second': 8.485, 'epoch': 0.76}
{'loss': 1.6044, 'grad_norm': 0.1313955932855606, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5926358699798584, 'eval_runtime': 7.4126, 'eval_samples_per_second': 134.771, 'eval_steps_per_second': 8.499, 'epoch': 0.8}
{'loss': 1.6077, 'grad_norm': 0.22251006960868835, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.590330719947815, 'eval_runtime': 7.4091, 'eval_samples_per_second': 134.835, 'eval_steps_per_second': 8.503, 'epoch': 0.84}
{'loss': 1.5828, 'grad_norm': 0.1254204958677292, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.588713526725769, 'eval_runtime': 7.4065, 'eval_samples_per_second': 134.881, 'eval_steps_per_second': 8.506, 'epoch': 0.88}
{'loss': 1.5621, 'grad_norm': 0.15830454230308533, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.587392807006836, 'eval_runtime': 7.4046, 'eval_samples_per_second': 134.916, 'eval_steps_per_second': 8.508, 'epoch': 0.92}
{'loss': 1.6431, 'grad_norm': 0.20326672494411469, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5867046117782593, 'eval_runtime': 7.4158, 'eval_samples_per_second': 134.712, 'eval_steps_per_second': 8.495, 'epoch': 0.96}
{'loss': 1.6481, 'grad_norm': 0.1507934182882309, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5862849950790405, 'eval_runtime': 7.4077, 'eval_samples_per_second': 134.86, 'eval_steps_per_second': 8.505, 'epoch': 1.0}
{'train_runtime': 460.9493, 'train_samples_per_second': 21.688, 'train_steps_per_second': 1.356, 'train_loss': 1.8668501892089844, 'epoch': 1.0}
train_results:  {'eval_loss': [4.136192798614502, 2.9312562942504883, 2.2516305446624756, 1.8672994375228882, 1.7131792306900024, 1.6821284294128418, 1.665148138999939, 1.654815673828125, 1.6466193199157715, 1.6390843391418457, 1.6322287321090698, 1.6236943006515503, 1.6176601648330688, 1.6144996881484985, 1.6092625856399536, 1.6046267747879028, 1.601379156112671, 1.5986337661743164, 1.5945709943771362, 1.5926358699798584, 1.590330719947815, 1.588713526725769, 1.587392807006836, 1.5867046117782593, 1.5862849950790405], 'performance': [0.77, 0.8]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<16:21,  9.91s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:05,  1.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:38,  1.74it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:24,  2.11it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:36<00:16,  2.07it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:44<00:09,  2.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:45<00:01,  2.80it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:45<00:00,  2.18it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.8]
current iteration observed (possibly low-fid or predicted) performance:  0.6125386953353882
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.8085000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6325 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.7982865757598484
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 0.7982865757598484
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 0.7982865757598484
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 0.7982865757598484
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 0.7982865757598484
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1375, dtype=torch.float64), 0, tensor(0.1921, dtype=torch.float64), tensor(0.3121, dtype=torch.float64), 0, 0, 0, tensor(0.3583, dtype=torch.float64), 2, 1, 0, 0, 0, 0, 128, 0.016678297032101424, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.1375, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1921, dtype=torch.float64), tensor(0.3121, dtype=torch.float64), tensor(7.6571e-18, dtype=torch.float64), tensor(4.9357e-18, dtype=torch.float64), tensor(1.5671e-18, dtype=torch.float64), tensor(0.3583, dtype=torch.float64), tensor(0.0665, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1668, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.138
  rowan_hellaswag: 0
  sciq: 0.192
  triviaqa: 0.312
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.358

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.016678297032101424,)
  num_layers_to_apply: (2,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  2
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  128
lora dropout:  0.016678297032101424
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,097,152 || all params: 8,032,358,400 || trainable%: 0.0261
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:38,  4.02s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:53,  1.69it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:34,  2.41it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:25,  2.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:23,  2.87it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:20,  2.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.99it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:14,  2.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:11,  3.10it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.29it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:05,  3.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:30<00:03,  3.43it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:32<00:00,  3.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:32<00:00,  3.09it/s]
Evaluation performance at step 25: 0.78
{'loss': 4.2076, 'grad_norm': 0.34283068776130676, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 3.948375940322876, 'eval_runtime': 7.4158, 'eval_samples_per_second': 134.713, 'eval_steps_per_second': 8.495, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:37,  4.62s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<00:59,  1.52it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:38,  2.14it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:27,  2.69it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:22,  2.95it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:19,  3.07it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:16,  3.14it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:13,  3.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.41it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:07,  3.59it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:06,  2.76it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:04,  2.66it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.05it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.88it/s]
Evaluation performance at step 50: 0.77
{'loss': 3.66, 'grad_norm': 0.30585622787475586, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 3.3556480407714844, 'eval_runtime': 7.399, 'eval_samples_per_second': 135.018, 'eval_steps_per_second': 8.515, 'epoch': 0.08}
{'loss': 3.0528, 'grad_norm': 0.45673036575317383, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.90118145942688, 'eval_runtime': 7.4519, 'eval_samples_per_second': 134.059, 'eval_steps_per_second': 8.454, 'epoch': 0.12}
{'loss': 2.7562, 'grad_norm': 0.4057823717594147, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.65920352935791, 'eval_runtime': 7.4481, 'eval_samples_per_second': 134.127, 'eval_steps_per_second': 8.458, 'epoch': 0.16}
{'loss': 2.6133, 'grad_norm': 0.3674505949020386, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.495723009109497, 'eval_runtime': 7.4605, 'eval_samples_per_second': 133.905, 'eval_steps_per_second': 8.444, 'epoch': 0.2}
{'loss': 2.4339, 'grad_norm': 0.6900599002838135, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.366250514984131, 'eval_runtime': 7.4514, 'eval_samples_per_second': 134.069, 'eval_steps_per_second': 8.455, 'epoch': 0.24}
{'loss': 2.2545, 'grad_norm': 0.8606422543525696, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.2694079875946045, 'eval_runtime': 7.4564, 'eval_samples_per_second': 133.978, 'eval_steps_per_second': 8.449, 'epoch': 0.28}
{'loss': 2.2302, 'grad_norm': 1.3171451091766357, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.1768863201141357, 'eval_runtime': 7.4594, 'eval_samples_per_second': 133.925, 'eval_steps_per_second': 8.446, 'epoch': 0.32}
{'loss': 2.1311, 'grad_norm': 1.3628615140914917, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.1096320152282715, 'eval_runtime': 7.5451, 'eval_samples_per_second': 132.403, 'eval_steps_per_second': 8.35, 'epoch': 0.36}
{'loss': 2.0557, 'grad_norm': 0.8090840578079224, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.0491297245025635, 'eval_runtime': 7.5195, 'eval_samples_per_second': 132.855, 'eval_steps_per_second': 8.378, 'epoch': 0.4}
{'loss': 2.0049, 'grad_norm': 0.9670751094818115, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9920557737350464, 'eval_runtime': 7.531, 'eval_samples_per_second': 132.652, 'eval_steps_per_second': 8.365, 'epoch': 0.44}
{'loss': 1.9293, 'grad_norm': 0.8656917810440063, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9427554607391357, 'eval_runtime': 7.5472, 'eval_samples_per_second': 132.367, 'eval_steps_per_second': 8.347, 'epoch': 0.48}
{'loss': 1.9258, 'grad_norm': 0.90920090675354, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9097952842712402, 'eval_runtime': 7.5407, 'eval_samples_per_second': 132.481, 'eval_steps_per_second': 8.355, 'epoch': 0.52}
{'loss': 1.8884, 'grad_norm': 0.8596279621124268, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8850796222686768, 'eval_runtime': 7.5303, 'eval_samples_per_second': 132.664, 'eval_steps_per_second': 8.366, 'epoch': 0.56}
{'loss': 1.8152, 'grad_norm': 1.4876582622528076, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.867811679840088, 'eval_runtime': 7.521, 'eval_samples_per_second': 132.827, 'eval_steps_per_second': 8.377, 'epoch': 0.6}
{'loss': 1.8804, 'grad_norm': 1.3802944421768188, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8502787351608276, 'eval_runtime': 7.4958, 'eval_samples_per_second': 133.275, 'eval_steps_per_second': 8.405, 'epoch': 0.64}
{'loss': 1.8152, 'grad_norm': 0.6244180202484131, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8357319831848145, 'eval_runtime': 7.467, 'eval_samples_per_second': 133.789, 'eval_steps_per_second': 8.437, 'epoch': 0.68}
{'loss': 1.8341, 'grad_norm': 1.2035998106002808, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8228123188018799, 'eval_runtime': 7.466, 'eval_samples_per_second': 133.806, 'eval_steps_per_second': 8.438, 'epoch': 0.72}
{'loss': 1.8054, 'grad_norm': 0.7709125876426697, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8131922483444214, 'eval_runtime': 7.4657, 'eval_samples_per_second': 133.812, 'eval_steps_per_second': 8.439, 'epoch': 0.76}
{'loss': 1.7778, 'grad_norm': 0.5290899276733398, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8047531843185425, 'eval_runtime': 7.4777, 'eval_samples_per_second': 133.597, 'eval_steps_per_second': 8.425, 'epoch': 0.8}
{'loss': 1.7426, 'grad_norm': 0.8432836532592773, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7953007221221924, 'eval_runtime': 7.4747, 'eval_samples_per_second': 133.651, 'eval_steps_per_second': 8.428, 'epoch': 0.84}
{'loss': 1.7675, 'grad_norm': 0.4618798792362213, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.790455937385559, 'eval_runtime': 7.4781, 'eval_samples_per_second': 133.59, 'eval_steps_per_second': 8.425, 'epoch': 0.88}
{'loss': 1.7785, 'grad_norm': 0.583283543586731, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7868967056274414, 'eval_runtime': 7.4787, 'eval_samples_per_second': 133.58, 'eval_steps_per_second': 8.424, 'epoch': 0.92}
{'loss': 1.7527, 'grad_norm': 0.3283787965774536, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.784226417541504, 'eval_runtime': 7.4773, 'eval_samples_per_second': 133.605, 'eval_steps_per_second': 8.426, 'epoch': 0.96}
{'loss': 1.7488, 'grad_norm': 0.36901018023490906, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7832300662994385, 'eval_runtime': 7.4808, 'eval_samples_per_second': 133.542, 'eval_steps_per_second': 8.422, 'epoch': 1.0}
{'train_runtime': 448.8903, 'train_samples_per_second': 22.273, 'train_steps_per_second': 1.392, 'train_loss': 2.194473944091797, 'epoch': 1.0}
train_results:  {'eval_loss': [3.948375940322876, 3.3556480407714844, 2.90118145942688, 2.65920352935791, 2.495723009109497, 2.366250514984131, 2.2694079875946045, 2.1768863201141357, 2.1096320152282715, 2.0491297245025635, 1.9920557737350464, 1.9427554607391357, 1.9097952842712402, 1.8850796222686768, 1.867811679840088, 1.8502787351608276, 1.8357319831848145, 1.8228123188018799, 1.8131922483444214, 1.8047531843185425, 1.7953007221221924, 1.790455937385559, 1.7868967056274414, 1.784226417541504, 1.7832300662994385], 'performance': [0.78, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<13:46,  8.35s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:02,  1.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:41,  1.63it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:26,  1.94it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:15,  2.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:06,  2.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:00,  3.58it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.49it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  0.793671727180481
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6394 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 0.7879797940810422
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 0.7879797940810422
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 0.7879797940810422
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.7879797940810422
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.7879797940810422
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.7225, dtype=torch.float64), tensor(0.0260, dtype=torch.float64), tensor(0.1816, dtype=torch.float64), tensor(0.0699, dtype=torch.float64), 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 16, 0.08100208055194043, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.7225, dtype=torch.float64), tensor(0.0260, dtype=torch.float64), tensor(0.1816, dtype=torch.float64), tensor(0.0699, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.7568e-18, dtype=torch.float64), tensor(2.8869e-18, dtype=torch.float64), tensor(0.2479, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1227, dtype=torch.float64), tensor(0.8100, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.722
  rowan_hellaswag: 0.026
  sciq: 0.182
  triviaqa: 0.07
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (16,)
  lora_dropout: (0.08100208055194043,)
  num_layers_to_apply: (8,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  8
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  16
lora dropout:  0.08100208055194043
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,048,576 || all params: 8,031,309,824 || trainable%: 0.0131
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:54,  3.58s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:51,  1.76it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:34,  2.41it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.88it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:22,  3.00it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.90it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.98it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:14,  2.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.23it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:07,  3.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:05,  3.27it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:30<00:03,  3.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:31<00:00,  3.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:31<00:00,  3.14it/s]
Evaluation performance at step 25: 0.75
{'loss': 2.8894, 'grad_norm': 1.7142513990402222, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 2.4468791484832764, 'eval_runtime': 8.8217, 'eval_samples_per_second': 113.243, 'eval_steps_per_second': 7.141, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:59,  3.64s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:54,  1.67it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:35,  2.31it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:22,  2.95it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:20,  2.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.96it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:14,  3.01it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.26it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:07,  3.39it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.79it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.05it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.89it/s]
Evaluation performance at step 50: 0.71
{'loss': 2.0559, 'grad_norm': 0.8599750995635986, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.71}
{'eval_loss': 1.7446463108062744, 'eval_runtime': 8.7927, 'eval_samples_per_second': 113.618, 'eval_steps_per_second': 7.165, 'epoch': 0.08}
{'loss': 1.5947, 'grad_norm': 1.2191251516342163, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5052303075790405, 'eval_runtime': 8.834, 'eval_samples_per_second': 113.086, 'eval_steps_per_second': 7.132, 'epoch': 0.12}
{'loss': 1.4476, 'grad_norm': 1.933451771736145, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.366594672203064, 'eval_runtime': 8.9162, 'eval_samples_per_second': 112.043, 'eval_steps_per_second': 7.066, 'epoch': 0.16}
{'loss': 1.3122, 'grad_norm': 1.3385200500488281, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.31672203540802, 'eval_runtime': 8.942, 'eval_samples_per_second': 111.72, 'eval_steps_per_second': 7.045, 'epoch': 0.2}
{'loss': 1.2737, 'grad_norm': 0.9363726377487183, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2832576036453247, 'eval_runtime': 8.9312, 'eval_samples_per_second': 111.854, 'eval_steps_per_second': 7.054, 'epoch': 0.24}
{'loss': 1.2503, 'grad_norm': 0.9367574453353882, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.268375277519226, 'eval_runtime': 8.944, 'eval_samples_per_second': 111.695, 'eval_steps_per_second': 7.044, 'epoch': 0.28}
{'loss': 1.2651, 'grad_norm': 0.7252370715141296, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2562741041183472, 'eval_runtime': 8.9384, 'eval_samples_per_second': 111.765, 'eval_steps_per_second': 7.048, 'epoch': 0.32}
{'loss': 1.2827, 'grad_norm': 0.6050358414649963, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2461485862731934, 'eval_runtime': 8.9451, 'eval_samples_per_second': 111.681, 'eval_steps_per_second': 7.043, 'epoch': 0.36}
{'loss': 1.2211, 'grad_norm': 0.6253726482391357, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.242211937904358, 'eval_runtime': 8.9296, 'eval_samples_per_second': 111.876, 'eval_steps_per_second': 7.055, 'epoch': 0.4}
{'loss': 1.237, 'grad_norm': 1.005236268043518, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2322808504104614, 'eval_runtime': 8.9544, 'eval_samples_per_second': 111.565, 'eval_steps_per_second': 7.036, 'epoch': 0.44}
{'loss': 1.2124, 'grad_norm': 0.5936993360519409, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2288895845413208, 'eval_runtime': 8.9272, 'eval_samples_per_second': 111.906, 'eval_steps_per_second': 7.057, 'epoch': 0.48}
{'loss': 1.2214, 'grad_norm': 0.6931517124176025, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2256724834442139, 'eval_runtime': 8.9302, 'eval_samples_per_second': 111.868, 'eval_steps_per_second': 7.055, 'epoch': 0.52}
{'loss': 1.2369, 'grad_norm': 0.7078863382339478, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.219677209854126, 'eval_runtime': 8.9206, 'eval_samples_per_second': 111.988, 'eval_steps_per_second': 7.062, 'epoch': 0.56}
{'loss': 1.2289, 'grad_norm': 0.6749553084373474, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2163941860198975, 'eval_runtime': 8.9178, 'eval_samples_per_second': 112.023, 'eval_steps_per_second': 7.065, 'epoch': 0.6}
{'loss': 1.1936, 'grad_norm': 0.9482640027999878, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2142601013183594, 'eval_runtime': 8.9311, 'eval_samples_per_second': 111.856, 'eval_steps_per_second': 7.054, 'epoch': 0.64}
{'loss': 1.1731, 'grad_norm': 0.7025912404060364, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2127412557601929, 'eval_runtime': 8.9241, 'eval_samples_per_second': 111.945, 'eval_steps_per_second': 7.06, 'epoch': 0.68}
{'loss': 1.2375, 'grad_norm': 0.6347122192382812, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.208276629447937, 'eval_runtime': 8.9277, 'eval_samples_per_second': 111.899, 'eval_steps_per_second': 7.057, 'epoch': 0.72}
{'loss': 1.2172, 'grad_norm': 0.8090566396713257, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.207806944847107, 'eval_runtime': 8.9224, 'eval_samples_per_second': 111.966, 'eval_steps_per_second': 7.061, 'epoch': 0.76}
{'loss': 1.1903, 'grad_norm': 0.5595157146453857, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2057774066925049, 'eval_runtime': 8.9197, 'eval_samples_per_second': 111.999, 'eval_steps_per_second': 7.063, 'epoch': 0.8}
{'loss': 1.1879, 'grad_norm': 0.8715486526489258, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.203242301940918, 'eval_runtime': 8.9181, 'eval_samples_per_second': 112.019, 'eval_steps_per_second': 7.064, 'epoch': 0.84}
{'loss': 1.2221, 'grad_norm': 0.6867659091949463, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2017276287078857, 'eval_runtime': 8.9198, 'eval_samples_per_second': 111.998, 'eval_steps_per_second': 7.063, 'epoch': 0.88}
{'loss': 1.2258, 'grad_norm': 0.6840529441833496, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2007625102996826, 'eval_runtime': 8.9191, 'eval_samples_per_second': 112.007, 'eval_steps_per_second': 7.064, 'epoch': 0.92}
{'loss': 1.2025, 'grad_norm': 0.5324229598045349, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2004159688949585, 'eval_runtime': 8.9195, 'eval_samples_per_second': 112.002, 'eval_steps_per_second': 7.063, 'epoch': 0.96}
{'loss': 1.1997, 'grad_norm': 0.5764426589012146, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1989843845367432, 'eval_runtime': 8.9272, 'eval_samples_per_second': 111.905, 'eval_steps_per_second': 7.057, 'epoch': 1.0}
{'train_runtime': 514.7815, 'train_samples_per_second': 19.424, 'train_steps_per_second': 1.214, 'train_loss': 1.3511591186523437, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4468791484832764, 1.7446463108062744, 1.5052303075790405, 1.366594672203064, 1.31672203540802, 1.2832576036453247, 1.268375277519226, 1.2562741041183472, 1.2461485862731934, 1.242211937904358, 1.2322808504104614, 1.2288895845413208, 1.2256724834442139, 1.219677209854126, 1.2163941860198975, 1.2142601013183594, 1.2127412557601929, 1.208276629447937, 1.207806944847107, 1.2057774066925049, 1.203242301940918, 1.2017276287078857, 1.2007625102996826, 1.2004159688949585, 1.1989843845367432], 'performance': [0.75, 0.71]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:13,  9.23s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:17<01:14,  1.11it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:39,  1.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:26,  1.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:36<00:15,  2.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:39<00:07,  2.67it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:41<00:00,  3.53it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:41<00:00,  2.41it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.71]
current iteration observed (possibly low-fid or predicted) performance:  1.0166096687316895
current iteration best possible performance (full train run):  0.735
max performance so far:  0.8085000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4477 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 0.9290411635473653
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 0.9379189369974688
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 0.8885479672477632
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.9220528543365692
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 0.9002532751125667
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.8399, dtype=torch.float64), 0, tensor(0.1601, dtype=torch.float64), 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.8399, dtype=torch.float64), tensor(4.8719e-17, dtype=torch.float64), tensor(0.1601, dtype=torch.float64), tensor(7.1463e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.4233e-17, dtype=torch.float64), tensor(1.5525e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.84
  rowan_hellaswag: 0
  sciq: 0.16
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:02,  2.45s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.02it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.56it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.65it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.82it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.15it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.85it/s]
Evaluation performance at step 25: 0.77
{'loss': 2.8137, 'grad_norm': 1.3200165033340454, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.7786717414855957, 'eval_runtime': 8.8091, 'eval_samples_per_second': 113.406, 'eval_steps_per_second': 7.152, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:54,  2.98s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:47,  1.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.61it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.04it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.15it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:19,  2.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:18,  2.75it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.20it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.01it/s]
Evaluation performance at step 50: 0.77
{'loss': 2.6806, 'grad_norm': 0.9259567260742188, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 2.6081390380859375, 'eval_runtime': 8.8537, 'eval_samples_per_second': 112.834, 'eval_steps_per_second': 7.116, 'epoch': 0.08}
{'loss': 2.562, 'grad_norm': 1.240229606628418, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.4649593830108643, 'eval_runtime': 8.8235, 'eval_samples_per_second': 113.221, 'eval_steps_per_second': 7.14, 'epoch': 0.12}
{'loss': 2.3812, 'grad_norm': 1.0798907279968262, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.2591171264648438, 'eval_runtime': 8.8736, 'eval_samples_per_second': 112.581, 'eval_steps_per_second': 7.1, 'epoch': 0.16}
{'loss': 2.2527, 'grad_norm': 30.510753631591797, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.203002691268921, 'eval_runtime': 8.9021, 'eval_samples_per_second': 112.221, 'eval_steps_per_second': 7.077, 'epoch': 0.2}
{'loss': 2.1588, 'grad_norm': 3.9299609661102295, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.0910251140594482, 'eval_runtime': 8.915, 'eval_samples_per_second': 112.059, 'eval_steps_per_second': 7.067, 'epoch': 0.24}
{'loss': 2.0973, 'grad_norm': 2.7552237510681152, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.0630404949188232, 'eval_runtime': 8.8896, 'eval_samples_per_second': 112.379, 'eval_steps_per_second': 7.087, 'epoch': 0.28}
{'loss': 2.0601, 'grad_norm': 1.6395785808563232, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.0388987064361572, 'eval_runtime': 8.8957, 'eval_samples_per_second': 112.302, 'eval_steps_per_second': 7.082, 'epoch': 0.32}
{'loss': 2.0147, 'grad_norm': 1.7215174436569214, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.018275737762451, 'eval_runtime': 8.899, 'eval_samples_per_second': 112.26, 'eval_steps_per_second': 7.079, 'epoch': 0.36}
{'loss': 1.9944, 'grad_norm': 0.6097856760025024, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.99072265625, 'eval_runtime': 8.9036, 'eval_samples_per_second': 112.201, 'eval_steps_per_second': 7.076, 'epoch': 0.4}
{'loss': 2.0043, 'grad_norm': 0.9306309223175049, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.965386986732483, 'eval_runtime': 8.9046, 'eval_samples_per_second': 112.189, 'eval_steps_per_second': 7.075, 'epoch': 0.44}
{'loss': 1.9277, 'grad_norm': 0.4921332001686096, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.942058801651001, 'eval_runtime': 8.9008, 'eval_samples_per_second': 112.238, 'eval_steps_per_second': 7.078, 'epoch': 0.48}
{'loss': 1.9312, 'grad_norm': 0.5172944068908691, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.924067497253418, 'eval_runtime': 8.8902, 'eval_samples_per_second': 112.37, 'eval_steps_per_second': 7.086, 'epoch': 0.52}
{'loss': 1.8872, 'grad_norm': 0.3951864540576935, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9077343940734863, 'eval_runtime': 8.9009, 'eval_samples_per_second': 112.236, 'eval_steps_per_second': 7.078, 'epoch': 0.56}
{'loss': 1.9241, 'grad_norm': 0.8295713067054749, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8955179452896118, 'eval_runtime': 8.8943, 'eval_samples_per_second': 112.319, 'eval_steps_per_second': 7.083, 'epoch': 0.6}
{'loss': 1.8685, 'grad_norm': 0.571732223033905, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8827921152114868, 'eval_runtime': 8.9091, 'eval_samples_per_second': 112.132, 'eval_steps_per_second': 7.071, 'epoch': 0.64}
{'loss': 1.853, 'grad_norm': 0.6668766736984253, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8687827587127686, 'eval_runtime': 8.9134, 'eval_samples_per_second': 112.078, 'eval_steps_per_second': 7.068, 'epoch': 0.68}
{'loss': 1.8732, 'grad_norm': 0.529128909111023, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8508548736572266, 'eval_runtime': 8.9211, 'eval_samples_per_second': 111.982, 'eval_steps_per_second': 7.062, 'epoch': 0.72}
{'loss': 1.8513, 'grad_norm': 1.731766939163208, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8410897254943848, 'eval_runtime': 8.9022, 'eval_samples_per_second': 112.219, 'eval_steps_per_second': 7.077, 'epoch': 0.76}
{'loss': 1.8348, 'grad_norm': 0.809560239315033, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8324309587478638, 'eval_runtime': 8.9155, 'eval_samples_per_second': 112.052, 'eval_steps_per_second': 7.066, 'epoch': 0.8}
{'loss': 1.82, 'grad_norm': 0.9217108488082886, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8253817558288574, 'eval_runtime': 8.9095, 'eval_samples_per_second': 112.127, 'eval_steps_per_second': 7.071, 'epoch': 0.84}
{'loss': 1.8197, 'grad_norm': 0.788500964641571, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8205984830856323, 'eval_runtime': 8.9099, 'eval_samples_per_second': 112.123, 'eval_steps_per_second': 7.071, 'epoch': 0.88}
{'loss': 1.8261, 'grad_norm': 0.4588758945465088, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.816652536392212, 'eval_runtime': 8.9169, 'eval_samples_per_second': 112.034, 'eval_steps_per_second': 7.065, 'epoch': 0.92}
{'loss': 1.7855, 'grad_norm': 0.4758634567260742, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8140274286270142, 'eval_runtime': 8.9277, 'eval_samples_per_second': 111.9, 'eval_steps_per_second': 7.057, 'epoch': 0.96}
{'loss': 1.8088, 'grad_norm': 0.7751012444496155, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8133294582366943, 'eval_runtime': 8.9202, 'eval_samples_per_second': 111.993, 'eval_steps_per_second': 7.063, 'epoch': 1.0}
{'train_runtime': 513.347, 'train_samples_per_second': 19.478, 'train_steps_per_second': 1.217, 'train_loss': 2.0412457153320314, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7786717414855957, 2.6081390380859375, 2.4649593830108643, 2.2591171264648438, 2.203002691268921, 2.0910251140594482, 2.0630404949188232, 2.0388987064361572, 2.018275737762451, 1.99072265625, 1.965386986732483, 1.942058801651001, 1.924067497253418, 1.9077343940734863, 1.8955179452896118, 1.8827921152114868, 1.8687827587127686, 1.8508548736572266, 1.8410897254943848, 1.8324309587478638, 1.8253817558288574, 1.8205984830856323, 1.816652536392212, 1.8140274286270142, 1.8133294582366943], 'performance': [0.77, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:49,  9.59s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:17<01:13,  1.14it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:40,  1.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:25,  2.01it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:13,  2.51it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:06,  2.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.52it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  1.226782202720642
current iteration best possible performance (full train run):  0.756
max performance so far:  0.8085000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4561 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.2033481710762621
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 1.0856044369212163
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 1.028999216895445
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 1.0405997161091338
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 1.0048710315961251
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(1., dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 104, 1.1796119636642283e-17, 39.06070870990225, 1]
normalized proposed parameters for next round by BO: [tensor(8.2319e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.3338e-17, dtype=torch.float64), tensor(3.8899e-17, dtype=torch.float64), tensor(7.3593e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1953e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8152, dtype=torch.float64), tensor(1.1796e-16, dtype=torch.float64), tensor(0.8138, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 1.0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (104,)
  lora_dropout: (1.1796119636642283e-17,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (39.06070870990225,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  104
lora dropout:  1.1796119636642283e-17
lora alpha:  39.06070870990225
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 851,968 || all params: 8,031,113,216 || trainable%: 0.0106
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:05,  2.48s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:10,  1.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.00it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.54it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.77it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.87it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.78it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.02it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.17it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.37it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.64it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.97it/s]
Evaluation performance at step 25: 0.78
{'loss': 2.6833, 'grad_norm': 0.14407388865947723, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 2.611881732940674, 'eval_runtime': 8.8143, 'eval_samples_per_second': 113.451, 'eval_steps_per_second': 7.147, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:34,  2.77s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:44,  2.04it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.64it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  2.98it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:17,  2.98it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  3.03it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.35it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:07,  3.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:06,  2.72it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:32<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:32<00:00,  3.05it/s]
Evaluation performance at step 50: 0.77
{'loss': 2.4892, 'grad_norm': 0.12199919670820236, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 2.36360764503479, 'eval_runtime': 8.8238, 'eval_samples_per_second': 113.33, 'eval_steps_per_second': 7.14, 'epoch': 0.08}
{'loss': 2.2658, 'grad_norm': 0.3344161808490753, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1556737422943115, 'eval_runtime': 8.8442, 'eval_samples_per_second': 113.068, 'eval_steps_per_second': 7.123, 'epoch': 0.12}
{'loss': 2.1317, 'grad_norm': 0.2580518424510956, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0868606567382812, 'eval_runtime': 8.8824, 'eval_samples_per_second': 112.583, 'eval_steps_per_second': 7.093, 'epoch': 0.16}
{'loss': 2.0433, 'grad_norm': 0.2585112750530243, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0183284282684326, 'eval_runtime': 8.9009, 'eval_samples_per_second': 112.348, 'eval_steps_per_second': 7.078, 'epoch': 0.2}
{'loss': 1.9923, 'grad_norm': 0.1387178748846054, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9717769622802734, 'eval_runtime': 8.8978, 'eval_samples_per_second': 112.388, 'eval_steps_per_second': 7.08, 'epoch': 0.24}
{'loss': 1.9285, 'grad_norm': 0.28206485509872437, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9229135513305664, 'eval_runtime': 8.9006, 'eval_samples_per_second': 112.352, 'eval_steps_per_second': 7.078, 'epoch': 0.28}
{'loss': 1.8703, 'grad_norm': 0.24888969957828522, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8904343843460083, 'eval_runtime': 8.9257, 'eval_samples_per_second': 112.036, 'eval_steps_per_second': 7.058, 'epoch': 0.32}
{'loss': 1.8655, 'grad_norm': 0.2610151767730713, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.859438419342041, 'eval_runtime': 8.9139, 'eval_samples_per_second': 112.185, 'eval_steps_per_second': 7.068, 'epoch': 0.36}
{'loss': 1.8694, 'grad_norm': 0.1871768981218338, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8367451429367065, 'eval_runtime': 8.8929, 'eval_samples_per_second': 112.45, 'eval_steps_per_second': 7.084, 'epoch': 0.4}
{'loss': 1.8176, 'grad_norm': 0.24225188791751862, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8181545734405518, 'eval_runtime': 8.9038, 'eval_samples_per_second': 112.312, 'eval_steps_per_second': 7.076, 'epoch': 0.44}
{'loss': 1.8103, 'grad_norm': 0.30545929074287415, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8022392988204956, 'eval_runtime': 8.9015, 'eval_samples_per_second': 112.341, 'eval_steps_per_second': 7.077, 'epoch': 0.48}
{'loss': 1.8119, 'grad_norm': 0.5959149599075317, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.786651849746704, 'eval_runtime': 8.8886, 'eval_samples_per_second': 112.504, 'eval_steps_per_second': 7.088, 'epoch': 0.52}
{'loss': 1.7677, 'grad_norm': 0.14398646354675293, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7731010913848877, 'eval_runtime': 8.8833, 'eval_samples_per_second': 112.571, 'eval_steps_per_second': 7.092, 'epoch': 0.56}
{'loss': 1.7752, 'grad_norm': 0.7868009805679321, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.759202480316162, 'eval_runtime': 8.9014, 'eval_samples_per_second': 112.341, 'eval_steps_per_second': 7.078, 'epoch': 0.6}
{'loss': 1.7374, 'grad_norm': 0.12516137957572937, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7437506914138794, 'eval_runtime': 8.9038, 'eval_samples_per_second': 112.312, 'eval_steps_per_second': 7.076, 'epoch': 0.64}
{'loss': 1.7114, 'grad_norm': 0.6653991341590881, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7282150983810425, 'eval_runtime': 8.9044, 'eval_samples_per_second': 112.304, 'eval_steps_per_second': 7.075, 'epoch': 0.68}
{'loss': 1.7218, 'grad_norm': 1.9812980890274048, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.717625617980957, 'eval_runtime': 8.9231, 'eval_samples_per_second': 112.068, 'eval_steps_per_second': 7.06, 'epoch': 0.72}
{'loss': 1.6934, 'grad_norm': 0.7069364786148071, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.707855463027954, 'eval_runtime': 8.9174, 'eval_samples_per_second': 112.141, 'eval_steps_per_second': 7.065, 'epoch': 0.76}
{'loss': 1.6974, 'grad_norm': 0.26985079050064087, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.700317621231079, 'eval_runtime': 8.9208, 'eval_samples_per_second': 112.098, 'eval_steps_per_second': 7.062, 'epoch': 0.8}
{'loss': 1.7005, 'grad_norm': 0.32101744413375854, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6935508251190186, 'eval_runtime': 8.9131, 'eval_samples_per_second': 112.195, 'eval_steps_per_second': 7.068, 'epoch': 0.84}
{'loss': 1.7013, 'grad_norm': 0.13251246511936188, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6884703636169434, 'eval_runtime': 8.9085, 'eval_samples_per_second': 112.253, 'eval_steps_per_second': 7.072, 'epoch': 0.88}
{'loss': 1.681, 'grad_norm': 0.1428595930337906, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.684626817703247, 'eval_runtime': 8.9084, 'eval_samples_per_second': 112.253, 'eval_steps_per_second': 7.072, 'epoch': 0.92}
{'loss': 1.6758, 'grad_norm': 0.12250982969999313, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6821175813674927, 'eval_runtime': 8.9659, 'eval_samples_per_second': 111.534, 'eval_steps_per_second': 7.027, 'epoch': 0.96}
{'loss': 1.6835, 'grad_norm': 0.11489907652139664, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6814826726913452, 'eval_runtime': 8.9205, 'eval_samples_per_second': 112.101, 'eval_steps_per_second': 7.062, 'epoch': 1.0}
{'train_runtime': 519.6679, 'train_samples_per_second': 19.243, 'train_steps_per_second': 1.203, 'train_loss': 1.8850251403808593, 'epoch': 1.0}
train_results:  {'eval_loss': [2.611881732940674, 2.36360764503479, 2.1556737422943115, 2.0868606567382812, 2.0183284282684326, 1.9717769622802734, 1.9229135513305664, 1.8904343843460083, 1.859438419342041, 1.8367451429367065, 1.8181545734405518, 1.8022392988204956, 1.786651849746704, 1.7731010913848877, 1.759202480316162, 1.7437506914138794, 1.7282150983810425, 1.717625617980957, 1.707855463027954, 1.700317621231079, 1.6935508251190186, 1.6884703636169434, 1.684626817703247, 1.6821175813674927, 1.6814826726913452], 'performance': [0.78, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:49,  9.59s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:32<00:28,  1.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:37<00:15,  2.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:41<00:07,  2.67it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:43<00:00,  3.51it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:43<00:00,  2.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  0.7708524465560913
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9384 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 0.8913555308558241
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.9843310846805096
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.0159355276637188
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.9501802975863085
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.9475171457203155
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.8512, dtype=torch.float64), 0, tensor(0.1488, dtype=torch.float64), 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 2, 3.399847640995838e-17, 1.4800000190735003, 0]
normalized proposed parameters for next round by BO: [tensor(4.0023e-17, dtype=torch.float64), tensor(0.8512, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1488, dtype=torch.float64), tensor(3.0784e-16, dtype=torch.float64), tensor(2.0661e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.0382e-17, dtype=torch.float64), tensor(9.3927e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.3998e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.851
  rowan_hellaswag: 0
  sciq: 0.149
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.399847640995838e-17,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190735003,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  2
lora dropout:  3.399847640995838e-17
lora alpha:  1.4800000190735003
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 63,488 || all params: 8,030,324,736 || trainable%: 0.0008
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<04:58,  3.01s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:54,  1.66it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:36,  2.25it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:23,  2.84it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:20,  2.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:16,  3.04it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:16,  2.68it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  2.93it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:04,  2.69it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.81it/s]
Evaluation performance at step 25: 0.79
{'loss': 2.8229, 'grad_norm': 0.4797486662864685, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.79}
{'eval_loss': 2.763817310333252, 'eval_runtime': 8.8432, 'eval_samples_per_second': 112.968, 'eval_steps_per_second': 7.124, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:35,  3.99s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:44,  1.85it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.37it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:27,  2.46it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:23,  2.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.98it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.65it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:12,  2.90it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.14it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.81it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.04it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.67it/s]
Evaluation performance at step 50: 0.79
{'loss': 2.4967, 'grad_norm': 1.3319189548492432, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.79}
{'eval_loss': 1.957383632659912, 'eval_runtime': 8.8838, 'eval_samples_per_second': 112.452, 'eval_steps_per_second': 7.092, 'epoch': 0.08}
{'loss': 1.7138, 'grad_norm': 0.6447153687477112, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4621100425720215, 'eval_runtime': 8.8882, 'eval_samples_per_second': 112.396, 'eval_steps_per_second': 7.088, 'epoch': 0.12}
{'loss': 1.3327, 'grad_norm': 0.48926857113838196, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2657029628753662, 'eval_runtime': 8.9222, 'eval_samples_per_second': 111.967, 'eval_steps_per_second': 7.061, 'epoch': 0.16}
{'loss': 1.236, 'grad_norm': 0.39753258228302, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1667295694351196, 'eval_runtime': 8.9273, 'eval_samples_per_second': 111.904, 'eval_steps_per_second': 7.057, 'epoch': 0.2}
{'loss': 1.139, 'grad_norm': 0.3669671416282654, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1201800107955933, 'eval_runtime': 8.9539, 'eval_samples_per_second': 111.571, 'eval_steps_per_second': 7.036, 'epoch': 0.24}
{'loss': 1.0952, 'grad_norm': 0.3968919515609741, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0913828611373901, 'eval_runtime': 8.9528, 'eval_samples_per_second': 111.585, 'eval_steps_per_second': 7.037, 'epoch': 0.28}
{'loss': 1.0951, 'grad_norm': 0.33700963854789734, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0722172260284424, 'eval_runtime': 8.967, 'eval_samples_per_second': 111.409, 'eval_steps_per_second': 7.026, 'epoch': 0.32}
{'loss': 1.0561, 'grad_norm': 0.3240179419517517, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0577820539474487, 'eval_runtime': 8.9787, 'eval_samples_per_second': 111.263, 'eval_steps_per_second': 7.017, 'epoch': 0.36}
{'loss': 1.059, 'grad_norm': 0.27411407232284546, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.047637701034546, 'eval_runtime': 8.9723, 'eval_samples_per_second': 111.343, 'eval_steps_per_second': 7.022, 'epoch': 0.4}
{'loss': 1.0291, 'grad_norm': 0.3438289165496826, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0393683910369873, 'eval_runtime': 8.9846, 'eval_samples_per_second': 111.19, 'eval_steps_per_second': 7.012, 'epoch': 0.44}
{'loss': 1.0414, 'grad_norm': 0.3562113642692566, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.032749056816101, 'eval_runtime': 8.9762, 'eval_samples_per_second': 111.294, 'eval_steps_per_second': 7.019, 'epoch': 0.48}
{'loss': 1.0225, 'grad_norm': 0.28651583194732666, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0276745557785034, 'eval_runtime': 9.0088, 'eval_samples_per_second': 110.892, 'eval_steps_per_second': 6.993, 'epoch': 0.52}
{'loss': 1.0419, 'grad_norm': 0.36085766553878784, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0230869054794312, 'eval_runtime': 9.0271, 'eval_samples_per_second': 110.667, 'eval_steps_per_second': 6.979, 'epoch': 0.56}
{'loss': 1.0268, 'grad_norm': 0.29848939180374146, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0196130275726318, 'eval_runtime': 8.989, 'eval_samples_per_second': 111.136, 'eval_steps_per_second': 7.009, 'epoch': 0.6}
{'loss': 1.0056, 'grad_norm': 0.2506190240383148, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0166438817977905, 'eval_runtime': 9.0015, 'eval_samples_per_second': 110.981, 'eval_steps_per_second': 6.999, 'epoch': 0.64}
{'loss': 1.0211, 'grad_norm': 0.31658875942230225, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.013674020767212, 'eval_runtime': 8.9988, 'eval_samples_per_second': 111.014, 'eval_steps_per_second': 7.001, 'epoch': 0.68}
{'loss': 1.0008, 'grad_norm': 0.3698651194572449, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.011373519897461, 'eval_runtime': 8.9801, 'eval_samples_per_second': 111.246, 'eval_steps_per_second': 7.016, 'epoch': 0.72}
{'loss': 1.004, 'grad_norm': 0.32060569524765015, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0095916986465454, 'eval_runtime': 8.9909, 'eval_samples_per_second': 111.113, 'eval_steps_per_second': 7.007, 'epoch': 0.76}
{'loss': 1.0109, 'grad_norm': 0.29961708188056946, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0082610845565796, 'eval_runtime': 8.9907, 'eval_samples_per_second': 111.114, 'eval_steps_per_second': 7.007, 'epoch': 0.8}
{'loss': 1.015, 'grad_norm': 0.3072323203086853, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0069061517715454, 'eval_runtime': 8.9929, 'eval_samples_per_second': 111.087, 'eval_steps_per_second': 7.005, 'epoch': 0.84}
{'loss': 1.0004, 'grad_norm': 0.2618720233440399, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0059913396835327, 'eval_runtime': 8.9806, 'eval_samples_per_second': 111.239, 'eval_steps_per_second': 7.015, 'epoch': 0.88}
{'loss': 1.0022, 'grad_norm': 0.2571973502635956, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.005145788192749, 'eval_runtime': 8.9889, 'eval_samples_per_second': 111.137, 'eval_steps_per_second': 7.009, 'epoch': 0.92}
{'loss': 1.0124, 'grad_norm': 0.27918049693107605, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0047897100448608, 'eval_runtime': 8.9775, 'eval_samples_per_second': 111.278, 'eval_steps_per_second': 7.018, 'epoch': 0.96}
{'loss': 1.0072, 'grad_norm': 0.4811316728591919, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.004747748374939, 'eval_runtime': 8.9759, 'eval_samples_per_second': 111.298, 'eval_steps_per_second': 7.019, 'epoch': 1.0}
{'train_runtime': 431.8737, 'train_samples_per_second': 23.153, 'train_steps_per_second': 1.447, 'train_loss': 1.2114922119140625, 'epoch': 1.0}
train_results:  {'eval_loss': [2.763817310333252, 1.957383632659912, 1.4621100425720215, 1.2657029628753662, 1.1667295694351196, 1.1201800107955933, 1.0913828611373901, 1.0722172260284424, 1.0577820539474487, 1.047637701034546, 1.0393683910369873, 1.032749056816101, 1.0276745557785034, 1.0230869054794312, 1.0196130275726318, 1.0166438817977905, 1.013674020767212, 1.011373519897461, 1.0095916986465454, 1.0082610845565796, 1.0069061517715454, 1.0059913396835327, 1.005145788192749, 1.0047897100448608, 1.004747748374939], 'performance': [0.79, 0.79]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:52,  9.62s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.87it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.79, 0.79]
current iteration observed (possibly low-fid or predicted) performance:  0.6542402505874634
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4835 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.8185288852519188
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.9517505279265437
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 0.8957174660832243
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 0.9256819544011125
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 0.8950175743060225
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.7983, dtype=torch.float64), 0, tensor(0.2017, dtype=torch.float64), 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.7983, dtype=torch.float64), tensor(7.6787e-16, dtype=torch.float64), tensor(0.2017, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.4435e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.798
  rowan_hellaswag: 0
  sciq: 0.202
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 73,728 || all params: 8,030,334,976 || trainable%: 0.0009
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:35,  2.78s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:46,  1.97it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:33,  2.49it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:28,  2.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:27,  2.43it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:21,  2.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:16,  2.54it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:12,  2.88it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.60it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:04,  2.70it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:01,  2.96it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.77it/s]
Evaluation performance at step 25: 0.77
{'loss': 2.7393, 'grad_norm': 4.9331955909729, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.1438934803009033, 'eval_runtime': 8.7697, 'eval_samples_per_second': 113.915, 'eval_steps_per_second': 7.184, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:40,  5.26s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:29,  1.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<01:03,  1.30it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:19<00:51,  1.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:43,  1.54it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:29<00:36,  1.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:32<00:28,  1.77it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:37<00:24,  1.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:41<00:19,  1.77it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:46<00:15,  1.78it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:50<00:10,  1.79it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:54<00:06,  1.80it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:58<00:01,  1.90it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:58<00:00,  1.71it/s]
Evaluation performance at step 50: 0.65
{'loss': 1.7279, 'grad_norm': 3.093703031539917, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.65}
{'eval_loss': 1.4235605001449585, 'eval_runtime': 8.7527, 'eval_samples_per_second': 114.136, 'eval_steps_per_second': 7.198, 'epoch': 0.08}
{'loss': 1.2528, 'grad_norm': 2.011016607284546, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1065318584442139, 'eval_runtime': 8.761, 'eval_samples_per_second': 114.028, 'eval_steps_per_second': 7.191, 'epoch': 0.12}
{'loss': 1.0626, 'grad_norm': 2.1064958572387695, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.018660068511963, 'eval_runtime': 8.8032, 'eval_samples_per_second': 113.481, 'eval_steps_per_second': 7.156, 'epoch': 0.16}
{'loss': 1.0005, 'grad_norm': 1.7616738080978394, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9926895499229431, 'eval_runtime': 8.8315, 'eval_samples_per_second': 113.118, 'eval_steps_per_second': 7.134, 'epoch': 0.2}
{'loss': 0.9783, 'grad_norm': 1.3637429475784302, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9806974530220032, 'eval_runtime': 8.842, 'eval_samples_per_second': 112.984, 'eval_steps_per_second': 7.125, 'epoch': 0.24}
{'loss': 0.9894, 'grad_norm': 1.3564677238464355, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9726676940917969, 'eval_runtime': 8.8686, 'eval_samples_per_second': 112.645, 'eval_steps_per_second': 7.104, 'epoch': 0.28}
{'loss': 0.9829, 'grad_norm': 1.2695627212524414, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9656372666358948, 'eval_runtime': 8.8636, 'eval_samples_per_second': 112.709, 'eval_steps_per_second': 7.108, 'epoch': 0.32}
{'loss': 0.9621, 'grad_norm': 1.4085850715637207, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9601057171821594, 'eval_runtime': 8.8702, 'eval_samples_per_second': 112.624, 'eval_steps_per_second': 7.102, 'epoch': 0.36}
{'loss': 0.9757, 'grad_norm': 1.4149285554885864, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9566807746887207, 'eval_runtime': 8.8674, 'eval_samples_per_second': 112.66, 'eval_steps_per_second': 7.105, 'epoch': 0.4}
{'loss': 0.9678, 'grad_norm': 1.3158162832260132, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9523972272872925, 'eval_runtime': 8.874, 'eval_samples_per_second': 112.576, 'eval_steps_per_second': 7.099, 'epoch': 0.44}
{'loss': 0.96, 'grad_norm': 1.3727412223815918, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.949622392654419, 'eval_runtime': 8.8793, 'eval_samples_per_second': 112.508, 'eval_steps_per_second': 7.095, 'epoch': 0.48}
{'loss': 0.9417, 'grad_norm': 1.5269865989685059, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9475211501121521, 'eval_runtime': 8.8903, 'eval_samples_per_second': 112.37, 'eval_steps_per_second': 7.086, 'epoch': 0.52}
{'loss': 0.9536, 'grad_norm': 1.4728740453720093, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9451326131820679, 'eval_runtime': 8.878, 'eval_samples_per_second': 112.526, 'eval_steps_per_second': 7.096, 'epoch': 0.56}
{'loss': 0.9407, 'grad_norm': 1.4811320304870605, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9433053135871887, 'eval_runtime': 8.8806, 'eval_samples_per_second': 112.492, 'eval_steps_per_second': 7.094, 'epoch': 0.6}
{'loss': 0.9861, 'grad_norm': 1.2434831857681274, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9415850639343262, 'eval_runtime': 8.8762, 'eval_samples_per_second': 112.548, 'eval_steps_per_second': 7.098, 'epoch': 0.64}
{'loss': 0.963, 'grad_norm': 1.514063835144043, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9401317834854126, 'eval_runtime': 8.8798, 'eval_samples_per_second': 112.503, 'eval_steps_per_second': 7.095, 'epoch': 0.68}
{'loss': 0.9491, 'grad_norm': 1.1596604585647583, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9381989240646362, 'eval_runtime': 8.8672, 'eval_samples_per_second': 112.662, 'eval_steps_per_second': 7.105, 'epoch': 0.72}
{'loss': 0.9683, 'grad_norm': 1.3851088285446167, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9368526339530945, 'eval_runtime': 8.8843, 'eval_samples_per_second': 112.445, 'eval_steps_per_second': 7.091, 'epoch': 0.76}
{'loss': 0.9581, 'grad_norm': 1.200986385345459, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9355341792106628, 'eval_runtime': 8.8783, 'eval_samples_per_second': 112.521, 'eval_steps_per_second': 7.096, 'epoch': 0.8}
{'loss': 0.951, 'grad_norm': 1.3131778240203857, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9341259598731995, 'eval_runtime': 8.8897, 'eval_samples_per_second': 112.378, 'eval_steps_per_second': 7.087, 'epoch': 0.84}
{'loss': 0.9231, 'grad_norm': 1.6840739250183105, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9337129592895508, 'eval_runtime': 8.8918, 'eval_samples_per_second': 112.35, 'eval_steps_per_second': 7.085, 'epoch': 0.88}
{'loss': 0.9464, 'grad_norm': 1.1972637176513672, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9328719973564148, 'eval_runtime': 8.8771, 'eval_samples_per_second': 112.537, 'eval_steps_per_second': 7.097, 'epoch': 0.92}
{'loss': 0.9465, 'grad_norm': 1.1710126399993896, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9323781132698059, 'eval_runtime': 8.8949, 'eval_samples_per_second': 112.311, 'eval_steps_per_second': 7.083, 'epoch': 0.96}
{'loss': 0.9395, 'grad_norm': 1.4652841091156006, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9322612881660461, 'eval_runtime': 8.8986, 'eval_samples_per_second': 112.265, 'eval_steps_per_second': 7.08, 'epoch': 1.0}
{'train_runtime': 451.2789, 'train_samples_per_second': 22.157, 'train_steps_per_second': 1.385, 'train_loss': 1.0786553314208984, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1438934803009033, 1.4235605001449585, 1.1065318584442139, 1.018660068511963, 0.9926895499229431, 0.9806974530220032, 0.9726676940917969, 0.9656372666358948, 0.9601057171821594, 0.9566807746887207, 0.9523972272872925, 0.949622392654419, 0.9475211501121521, 0.9451326131820679, 0.9433053135871887, 0.9415850639343262, 0.9401317834854126, 0.9381989240646362, 0.9368526339530945, 0.9355341792106628, 0.9341259598731995, 0.9337129592895508, 0.9328719973564148, 0.9323781132698059, 0.9322612881660461], 'performance': [0.77, 0.65]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:52,  9.62s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.87it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.65]
current iteration observed (possibly low-fid or predicted) performance:  1.2025779485702515
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0330 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 0.9426343251636371
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.8417263045500933
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 0.9436144477448927
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.8253344655739147
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.9378799943258067
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.8266, dtype=torch.float64), 0, tensor(0.1734, dtype=torch.float64), 0, 0, 0, 0, 0, 18, 1, 0, 0, 0, 1, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(2.7839e-18, dtype=torch.float64), tensor(0.8266, dtype=torch.float64), tensor(1.2749e-18, dtype=torch.float64), tensor(0.1734, dtype=torch.float64), tensor(6.3184e-17, dtype=torch.float64), tensor(7.2182e-17, dtype=torch.float64), tensor(4.9942e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5496, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.827
  rowan_hellaswag: 0
  sciq: 0.173
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 958,464 || all params: 8,031,219,712 || trainable%: 0.0119
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:11,  5.57s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:35,  1.05s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:13<00:54,  1.53it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:16<00:40,  1.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:18<00:29,  2.29it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:23<00:29,  1.99it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:26<00:22,  2.22it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:29<00:18,  2.30it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:31<00:13,  2.67it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:34<00:09,  2.76it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:07,  2.57it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:42<00:04,  2.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:44<00:01,  2.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:44<00:00,  2.25it/s]
Evaluation performance at step 25: 0.7
{'loss': 2.2269, 'grad_norm': 1.8245692253112793, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.7}
{'eval_loss': 1.4574189186096191, 'eval_runtime': 9.3234, 'eval_samples_per_second': 107.15, 'eval_steps_per_second': 6.757, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:43,  4.08s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:02,  1.44it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.94it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:31,  2.39it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.74it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:23,  2.50it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:22,  2.28it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:18,  2.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:14,  2.44it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:09,  2.83it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:05,  3.17it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  3.12it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.18it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.65it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.189, 'grad_norm': 0.9080531001091003, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.0270237922668457, 'eval_runtime': 9.3645, 'eval_samples_per_second': 106.679, 'eval_steps_per_second': 6.728, 'epoch': 0.08}
{'loss': 0.9938, 'grad_norm': 1.003919243812561, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9488757848739624, 'eval_runtime': 9.3621, 'eval_samples_per_second': 106.707, 'eval_steps_per_second': 6.729, 'epoch': 0.12}
{'loss': 0.9131, 'grad_norm': 0.9789848923683167, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8691714406013489, 'eval_runtime': 9.3936, 'eval_samples_per_second': 106.349, 'eval_steps_per_second': 6.707, 'epoch': 0.16}
{'loss': 0.8668, 'grad_norm': 0.8304319977760315, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8493019342422485, 'eval_runtime': 9.4123, 'eval_samples_per_second': 106.137, 'eval_steps_per_second': 6.693, 'epoch': 0.2}
{'loss': 0.8453, 'grad_norm': 0.7941974997520447, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8410450220108032, 'eval_runtime': 9.4204, 'eval_samples_per_second': 106.046, 'eval_steps_per_second': 6.688, 'epoch': 0.24}
{'loss': 0.8407, 'grad_norm': 0.8094426989555359, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8320655822753906, 'eval_runtime': 9.4345, 'eval_samples_per_second': 105.888, 'eval_steps_per_second': 6.678, 'epoch': 0.28}
{'loss': 0.8169, 'grad_norm': 0.9258634448051453, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8249130249023438, 'eval_runtime': 9.4465, 'eval_samples_per_second': 105.754, 'eval_steps_per_second': 6.669, 'epoch': 0.32}
{'loss': 0.8433, 'grad_norm': 0.7706774473190308, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8156862258911133, 'eval_runtime': 9.4603, 'eval_samples_per_second': 105.599, 'eval_steps_per_second': 6.659, 'epoch': 0.36}
{'loss': 0.8218, 'grad_norm': 0.9527338147163391, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8092681169509888, 'eval_runtime': 9.4499, 'eval_samples_per_second': 105.715, 'eval_steps_per_second': 6.667, 'epoch': 0.4}
{'loss': 0.828, 'grad_norm': 0.9467993974685669, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8015545010566711, 'eval_runtime': 9.4677, 'eval_samples_per_second': 105.517, 'eval_steps_per_second': 6.654, 'epoch': 0.44}
{'loss': 0.8162, 'grad_norm': 0.9705109000205994, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7947996854782104, 'eval_runtime': 9.4887, 'eval_samples_per_second': 105.283, 'eval_steps_per_second': 6.639, 'epoch': 0.48}
{'loss': 0.7949, 'grad_norm': 0.8759738802909851, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.788769006729126, 'eval_runtime': 9.4597, 'eval_samples_per_second': 105.606, 'eval_steps_per_second': 6.66, 'epoch': 0.52}
{'loss': 0.8015, 'grad_norm': 0.9504059553146362, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7821080088615417, 'eval_runtime': 9.461, 'eval_samples_per_second': 105.591, 'eval_steps_per_second': 6.659, 'epoch': 0.56}
{'loss': 0.8182, 'grad_norm': 1.061726689338684, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7783648371696472, 'eval_runtime': 9.4488, 'eval_samples_per_second': 105.728, 'eval_steps_per_second': 6.668, 'epoch': 0.6}
{'loss': 0.8016, 'grad_norm': 0.9915981888771057, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7720654606819153, 'eval_runtime': 9.4633, 'eval_samples_per_second': 105.566, 'eval_steps_per_second': 6.657, 'epoch': 0.64}
{'loss': 0.7971, 'grad_norm': 0.960968554019928, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7673324942588806, 'eval_runtime': 9.4739, 'eval_samples_per_second': 105.448, 'eval_steps_per_second': 6.65, 'epoch': 0.68}
{'loss': 0.7821, 'grad_norm': 1.0720001459121704, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7599123120307922, 'eval_runtime': 9.4744, 'eval_samples_per_second': 105.442, 'eval_steps_per_second': 6.649, 'epoch': 0.72}
{'loss': 0.8, 'grad_norm': 1.1263842582702637, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7558938264846802, 'eval_runtime': 9.4662, 'eval_samples_per_second': 105.533, 'eval_steps_per_second': 6.655, 'epoch': 0.76}
{'loss': 0.8021, 'grad_norm': 0.9916879534721375, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7528038024902344, 'eval_runtime': 9.4593, 'eval_samples_per_second': 105.611, 'eval_steps_per_second': 6.66, 'epoch': 0.8}
{'loss': 0.7979, 'grad_norm': 1.0711420774459839, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7501605749130249, 'eval_runtime': 9.4563, 'eval_samples_per_second': 105.644, 'eval_steps_per_second': 6.662, 'epoch': 0.84}
{'loss': 0.7927, 'grad_norm': 1.1622971296310425, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7472283244132996, 'eval_runtime': 9.4529, 'eval_samples_per_second': 105.682, 'eval_steps_per_second': 6.665, 'epoch': 0.88}
{'loss': 0.7908, 'grad_norm': 1.1507437229156494, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7447925209999084, 'eval_runtime': 9.4637, 'eval_samples_per_second': 105.561, 'eval_steps_per_second': 6.657, 'epoch': 0.92}
{'loss': 0.7904, 'grad_norm': 1.0812348127365112, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7432185411453247, 'eval_runtime': 9.4436, 'eval_samples_per_second': 105.786, 'eval_steps_per_second': 6.671, 'epoch': 0.96}
{'loss': 0.7677, 'grad_norm': 1.2959545850753784, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7428839802742004, 'eval_runtime': 9.4564, 'eval_samples_per_second': 105.643, 'eval_steps_per_second': 6.662, 'epoch': 1.0}
{'train_runtime': 515.6127, 'train_samples_per_second': 19.392, 'train_steps_per_second': 1.212, 'train_loss': 0.8935475646972656, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4574189186096191, 1.0270237922668457, 0.9488757848739624, 0.8691714406013489, 0.8493019342422485, 0.8410450220108032, 0.8320655822753906, 0.8249130249023438, 0.8156862258911133, 0.8092681169509888, 0.8015545010566711, 0.7947996854782104, 0.788769006729126, 0.7821080088615417, 0.7783648371696472, 0.7720654606819153, 0.7673324942588806, 0.7599123120307922, 0.7558938264846802, 0.7528038024902344, 0.7501605749130249, 0.7472283244132996, 0.7447925209999084, 0.7432185411453247, 0.7428839802742004], 'performance': [0.7, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:07<11:35,  7.02s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:08,  1.21it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:40,  1.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:23,  2.20it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:31<00:13,  2.60it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:07,  2.50it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:41<00:00,  3.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:41<00:00,  2.43it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.7, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  0.9818884134292603
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8793 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.888576524394864
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.8864512203472945
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.9048711477254335
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.9047961677404427
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 0.8914125123433535
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.6375, dtype=torch.float64), tensor(0.1937, dtype=torch.float64), tensor(0.1688, dtype=torch.float64), 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.6375, dtype=torch.float64), tensor(0.1937, dtype=torch.float64), tensor(0.1688, dtype=torch.float64), tensor(1.8209e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.2690e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7179e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.638
  rowan_hellaswag: 0.194
  sciq: 0.169
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 36,864 || all params: 8,030,298,112 || trainable%: 0.0005
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:03,  2.46s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.01it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.54it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.62it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.78it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.88it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.20it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.65it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Evaluation performance at step 25: 0.8
{'loss': 3.0857, 'grad_norm': 3.96427321434021, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.8}
{'eval_loss': 2.7767863273620605, 'eval_runtime': 8.9146, 'eval_samples_per_second': 112.063, 'eval_steps_per_second': 7.067, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:45,  4.10s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:21,  1.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.79it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.51it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:23,  2.54it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:20,  2.47it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.63it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:11,  3.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.60it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.65it/s]
Evaluation performance at step 50: 0.79
{'loss': 2.3509, 'grad_norm': 8.98018741607666, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.79}
{'eval_loss': 2.065995454788208, 'eval_runtime': 8.865, 'eval_samples_per_second': 112.691, 'eval_steps_per_second': 7.107, 'epoch': 0.08}
{'loss': 1.881, 'grad_norm': 4.537182807922363, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7139697074890137, 'eval_runtime': 8.9163, 'eval_samples_per_second': 112.042, 'eval_steps_per_second': 7.066, 'epoch': 0.12}
{'loss': 1.694, 'grad_norm': 4.871440887451172, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5566294193267822, 'eval_runtime': 8.9577, 'eval_samples_per_second': 111.524, 'eval_steps_per_second': 7.033, 'epoch': 0.16}
{'loss': 1.5523, 'grad_norm': 6.569425106048584, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5011026859283447, 'eval_runtime': 8.9269, 'eval_samples_per_second': 111.909, 'eval_steps_per_second': 7.057, 'epoch': 0.2}
{'loss': 1.474, 'grad_norm': 6.0169243812561035, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.451925277709961, 'eval_runtime': 8.9414, 'eval_samples_per_second': 111.728, 'eval_steps_per_second': 7.046, 'epoch': 0.24}
{'loss': 1.4336, 'grad_norm': 4.793915271759033, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4230022430419922, 'eval_runtime': 8.9052, 'eval_samples_per_second': 112.182, 'eval_steps_per_second': 7.075, 'epoch': 0.28}
{'loss': 1.4217, 'grad_norm': 4.213408470153809, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4092048406600952, 'eval_runtime': 8.894, 'eval_samples_per_second': 112.323, 'eval_steps_per_second': 7.083, 'epoch': 0.32}
{'loss': 1.3682, 'grad_norm': 2.4939942359924316, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3963243961334229, 'eval_runtime': 8.9024, 'eval_samples_per_second': 112.217, 'eval_steps_per_second': 7.077, 'epoch': 0.36}
{'loss': 1.4169, 'grad_norm': 5.5810227394104, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3864052295684814, 'eval_runtime': 8.8943, 'eval_samples_per_second': 112.32, 'eval_steps_per_second': 7.083, 'epoch': 0.4}
{'loss': 1.3861, 'grad_norm': 4.379201889038086, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3744454383850098, 'eval_runtime': 8.9154, 'eval_samples_per_second': 112.053, 'eval_steps_per_second': 7.066, 'epoch': 0.44}
{'loss': 1.3755, 'grad_norm': 4.786240577697754, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.365805983543396, 'eval_runtime': 8.9707, 'eval_samples_per_second': 111.363, 'eval_steps_per_second': 7.023, 'epoch': 0.48}
{'loss': 1.3385, 'grad_norm': 6.215335845947266, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.348544716835022, 'eval_runtime': 8.9588, 'eval_samples_per_second': 111.511, 'eval_steps_per_second': 7.032, 'epoch': 0.52}
{'loss': 1.3744, 'grad_norm': 3.650628089904785, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3146405220031738, 'eval_runtime': 8.9532, 'eval_samples_per_second': 111.581, 'eval_steps_per_second': 7.037, 'epoch': 0.56}
{'loss': 1.2885, 'grad_norm': 3.735452890396118, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2891813516616821, 'eval_runtime': 8.9487, 'eval_samples_per_second': 111.636, 'eval_steps_per_second': 7.04, 'epoch': 0.6}
{'loss': 1.269, 'grad_norm': 5.223828315734863, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2870557308197021, 'eval_runtime': 8.9583, 'eval_samples_per_second': 111.516, 'eval_steps_per_second': 7.033, 'epoch': 0.64}
{'loss': 1.3034, 'grad_norm': 4.281225681304932, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2754731178283691, 'eval_runtime': 8.9652, 'eval_samples_per_second': 111.43, 'eval_steps_per_second': 7.027, 'epoch': 0.68}
{'loss': 1.2591, 'grad_norm': 3.2326862812042236, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2715990543365479, 'eval_runtime': 8.9717, 'eval_samples_per_second': 111.35, 'eval_steps_per_second': 7.022, 'epoch': 0.72}
{'loss': 1.2732, 'grad_norm': 4.008825302124023, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2642855644226074, 'eval_runtime': 8.9832, 'eval_samples_per_second': 111.208, 'eval_steps_per_second': 7.013, 'epoch': 0.76}
{'loss': 1.2524, 'grad_norm': 3.659749984741211, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2562508583068848, 'eval_runtime': 8.9723, 'eval_samples_per_second': 111.343, 'eval_steps_per_second': 7.022, 'epoch': 0.8}
{'loss': 1.2206, 'grad_norm': 3.208481550216675, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2478034496307373, 'eval_runtime': 8.9693, 'eval_samples_per_second': 111.38, 'eval_steps_per_second': 7.024, 'epoch': 0.84}
{'loss': 1.2533, 'grad_norm': 4.031037330627441, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2380645275115967, 'eval_runtime': 8.9855, 'eval_samples_per_second': 111.179, 'eval_steps_per_second': 7.011, 'epoch': 0.88}
{'loss': 1.2298, 'grad_norm': 3.3187172412872314, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2301087379455566, 'eval_runtime': 8.9871, 'eval_samples_per_second': 111.159, 'eval_steps_per_second': 7.01, 'epoch': 0.92}
{'loss': 1.2119, 'grad_norm': 2.667877674102783, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2281018495559692, 'eval_runtime': 8.9935, 'eval_samples_per_second': 111.08, 'eval_steps_per_second': 7.005, 'epoch': 0.96}
{'loss': 1.2704, 'grad_norm': 2.579286813735962, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2262885570526123, 'eval_runtime': 8.9982, 'eval_samples_per_second': 111.023, 'eval_steps_per_second': 7.001, 'epoch': 1.0}
{'train_runtime': 518.5476, 'train_samples_per_second': 19.283, 'train_steps_per_second': 1.205, 'train_loss': 1.4793755828857422, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7767863273620605, 2.065995454788208, 1.7139697074890137, 1.5566294193267822, 1.5011026859283447, 1.451925277709961, 1.4230022430419922, 1.4092048406600952, 1.3963243961334229, 1.3864052295684814, 1.3744454383850098, 1.365805983543396, 1.348544716835022, 1.3146405220031738, 1.2891813516616821, 1.2870557308197021, 1.2754731178283691, 1.2715990543365479, 1.2642855644226074, 1.2562508583068848, 1.2478034496307373, 1.2380645275115967, 1.2301087379455566, 1.2281018495559692, 1.2262885570526123], 'performance': [0.8, 0.79]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<10:47,  6.54s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:05,  1.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:20<00:34,  1.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:26<00:23,  2.21it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:29<00:12,  2.76it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:05,  3.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  4.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.82it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.8, 0.79]
current iteration observed (possibly low-fid or predicted) performance:  1.2089287042617798
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7457 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.8975159469189523
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.8606759117411238
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.8331949558429012
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.8967342338292312
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.8975270791806524
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5536, dtype=torch.float64), 0, tensor(0.1702, dtype=torch.float64), tensor(0.2762, dtype=torch.float64), 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 2.3330148220055246e-17, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(6.4326e-17, dtype=torch.float64), tensor(0.5536, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1702, dtype=torch.float64), tensor(0.2762, dtype=torch.float64), tensor(3.5719e-17, dtype=torch.float64), tensor(1.0605e-16, dtype=torch.float64), tensor(1.9660e-18, dtype=torch.float64), tensor(1.6959e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(2.3330e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.554
  rowan_hellaswag: 0
  sciq: 0.17
  triviaqa: 0.276
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (2.3330148220055246e-17,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  2
lora dropout:  2.3330148220055246e-17
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:02,  2.45s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.05it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:28,  2.59it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:27,  2.40it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:23,  2.49it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:19,  2.66it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.80it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.18it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:05,  3.26it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.96it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.1313, 'grad_norm': 4.103567600250244, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.7109947204589844, 'eval_runtime': 8.6097, 'eval_samples_per_second': 116.031, 'eval_steps_per_second': 7.317, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:35,  2.78s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:11,  1.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  1.99it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:27,  2.48it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.71it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.17it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:05,  3.27it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.46it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.91it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.2413, 'grad_norm': 3.577188491821289, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.8710013628005981, 'eval_runtime': 8.601, 'eval_samples_per_second': 116.149, 'eval_steps_per_second': 7.325, 'epoch': 0.08}
{'loss': 1.622, 'grad_norm': 2.989856719970703, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4724411964416504, 'eval_runtime': 8.5884, 'eval_samples_per_second': 116.319, 'eval_steps_per_second': 7.335, 'epoch': 0.12}
{'loss': 1.3933, 'grad_norm': 8.341293334960938, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3291161060333252, 'eval_runtime': 8.6206, 'eval_samples_per_second': 115.885, 'eval_steps_per_second': 7.308, 'epoch': 0.16}
{'loss': 1.2993, 'grad_norm': 5.069142818450928, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2596946954727173, 'eval_runtime': 8.6396, 'eval_samples_per_second': 115.63, 'eval_steps_per_second': 7.292, 'epoch': 0.2}
{'loss': 1.2229, 'grad_norm': 3.814483642578125, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2188016176223755, 'eval_runtime': 8.6512, 'eval_samples_per_second': 115.476, 'eval_steps_per_second': 7.282, 'epoch': 0.24}
{'loss': 1.2337, 'grad_norm': 3.7634012699127197, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2039388418197632, 'eval_runtime': 8.6617, 'eval_samples_per_second': 115.335, 'eval_steps_per_second': 7.273, 'epoch': 0.28}
{'loss': 1.189, 'grad_norm': 2.762850046157837, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1751163005828857, 'eval_runtime': 8.6773, 'eval_samples_per_second': 115.128, 'eval_steps_per_second': 7.26, 'epoch': 0.32}
{'loss': 1.164, 'grad_norm': 4.546972274780273, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.130609393119812, 'eval_runtime': 8.6687, 'eval_samples_per_second': 115.242, 'eval_steps_per_second': 7.268, 'epoch': 0.36}
{'loss': 1.1227, 'grad_norm': 3.8568153381347656, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.111914873123169, 'eval_runtime': 8.6649, 'eval_samples_per_second': 115.293, 'eval_steps_per_second': 7.271, 'epoch': 0.4}
{'loss': 1.1215, 'grad_norm': 6.125671863555908, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1028358936309814, 'eval_runtime': 8.6667, 'eval_samples_per_second': 115.269, 'eval_steps_per_second': 7.269, 'epoch': 0.44}
{'loss': 1.1404, 'grad_norm': 3.754754066467285, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0933480262756348, 'eval_runtime': 8.6561, 'eval_samples_per_second': 115.41, 'eval_steps_per_second': 7.278, 'epoch': 0.48}
{'loss': 1.0906, 'grad_norm': 3.2682433128356934, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0867305994033813, 'eval_runtime': 8.6648, 'eval_samples_per_second': 115.293, 'eval_steps_per_second': 7.271, 'epoch': 0.52}
{'loss': 1.087, 'grad_norm': 2.716141939163208, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.079402208328247, 'eval_runtime': 8.6765, 'eval_samples_per_second': 115.139, 'eval_steps_per_second': 7.261, 'epoch': 0.56}
{'loss': 1.0968, 'grad_norm': 2.566253900527954, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0744538307189941, 'eval_runtime': 8.677, 'eval_samples_per_second': 115.131, 'eval_steps_per_second': 7.261, 'epoch': 0.6}
{'loss': 1.096, 'grad_norm': 3.2678816318511963, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0694892406463623, 'eval_runtime': 8.6842, 'eval_samples_per_second': 115.036, 'eval_steps_per_second': 7.255, 'epoch': 0.64}
{'loss': 1.0791, 'grad_norm': 4.604219913482666, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0611345767974854, 'eval_runtime': 8.6778, 'eval_samples_per_second': 115.122, 'eval_steps_per_second': 7.26, 'epoch': 0.68}
{'loss': 1.0791, 'grad_norm': 2.8639402389526367, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.050225019454956, 'eval_runtime': 8.6811, 'eval_samples_per_second': 115.078, 'eval_steps_per_second': 7.257, 'epoch': 0.72}
{'loss': 1.0554, 'grad_norm': 3.2644639015197754, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0385446548461914, 'eval_runtime': 8.6858, 'eval_samples_per_second': 115.016, 'eval_steps_per_second': 7.253, 'epoch': 0.76}
{'loss': 1.0421, 'grad_norm': 2.582625150680542, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.026793122291565, 'eval_runtime': 8.6843, 'eval_samples_per_second': 115.035, 'eval_steps_per_second': 7.254, 'epoch': 0.8}
{'loss': 1.013, 'grad_norm': 2.4575629234313965, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0199943780899048, 'eval_runtime': 8.6988, 'eval_samples_per_second': 114.843, 'eval_steps_per_second': 7.242, 'epoch': 0.84}
{'loss': 1.0376, 'grad_norm': 3.1992640495300293, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0176199674606323, 'eval_runtime': 8.678, 'eval_samples_per_second': 115.119, 'eval_steps_per_second': 7.26, 'epoch': 0.88}
{'loss': 1.0309, 'grad_norm': 3.2529678344726562, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0153127908706665, 'eval_runtime': 8.6736, 'eval_samples_per_second': 115.177, 'eval_steps_per_second': 7.263, 'epoch': 0.92}
{'loss': 0.9872, 'grad_norm': 2.129134178161621, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0141440629959106, 'eval_runtime': 8.6647, 'eval_samples_per_second': 115.296, 'eval_steps_per_second': 7.271, 'epoch': 0.96}
{'loss': 1.0236, 'grad_norm': 2.8548338413238525, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.013035535812378, 'eval_runtime': 8.6647, 'eval_samples_per_second': 115.295, 'eval_steps_per_second': 7.271, 'epoch': 1.0}
{'train_runtime': 509.9892, 'train_samples_per_second': 19.606, 'train_steps_per_second': 1.226, 'train_loss': 1.2639947082519531, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7109947204589844, 1.8710013628005981, 1.4724411964416504, 1.3291161060333252, 1.2596946954727173, 1.2188016176223755, 1.2039388418197632, 1.1751163005828857, 1.130609393119812, 1.111914873123169, 1.1028358936309814, 1.0933480262756348, 1.0867305994033813, 1.079402208328247, 1.0744538307189941, 1.0694892406463623, 1.0611345767974854, 1.050225019454956, 1.0385446548461914, 1.026793122291565, 1.0199943780899048, 1.0176199674606323, 1.0153127908706665, 1.0141440629959106, 1.013035535812378], 'performance': [0.77, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<09:55,  6.02s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:44,  1.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:28,  2.32it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:05,  3.72it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:29<00:00,  4.73it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:29<00:00,  3.42it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  1.2165716886520386
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5138 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.8536054744203784
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.8358189051250556
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 0.8619939719501217
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.8892056224347712
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.8892056306402634
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.9705, dtype=torch.float64), 0, tensor(0.0295, dtype=torch.float64), 0, 6, 1, 0, 0, 0, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(1.7298e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9705, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0295, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1795, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.971
  wikitext: 0
  mmlu: 0.029
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (6,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  6
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 6,291,456 || all params: 8,036,552,704 || trainable%: 0.0783
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:05,  2.48s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.56it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.66it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.78it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.88it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.11it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Evaluation performance at step 25: 0.79
{'loss': 5.3112, 'grad_norm': 0.07721582055091858, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.79}
{'eval_loss': 5.3122663497924805, 'eval_runtime': 3.7673, 'eval_samples_per_second': 265.176, 'eval_steps_per_second': 16.723, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:37,  2.81s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:45,  2.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.63it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  3.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:18,  2.77it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:11,  3.07it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.61it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.96it/s]
Evaluation performance at step 50: 0.76
{'loss': 4.9573, 'grad_norm': 0.14563080668449402, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 4.468440055847168, 'eval_runtime': 3.7526, 'eval_samples_per_second': 266.218, 'eval_steps_per_second': 16.789, 'epoch': 0.08}
{'loss': 3.9838, 'grad_norm': 0.11664356291294098, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.6188836097717285, 'eval_runtime': 3.765, 'eval_samples_per_second': 265.341, 'eval_steps_per_second': 16.733, 'epoch': 0.12}
{'loss': 3.3559, 'grad_norm': 0.11441695690155029, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.1076066493988037, 'eval_runtime': 3.7819, 'eval_samples_per_second': 264.156, 'eval_steps_per_second': 16.658, 'epoch': 0.16}
{'loss': 2.9371, 'grad_norm': 0.10520840436220169, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.7287745475769043, 'eval_runtime': 3.771, 'eval_samples_per_second': 264.92, 'eval_steps_per_second': 16.707, 'epoch': 0.2}
{'loss': 2.554, 'grad_norm': 0.14106686413288116, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.385075807571411, 'eval_runtime': 3.7598, 'eval_samples_per_second': 265.703, 'eval_steps_per_second': 16.756, 'epoch': 0.24}
{'loss': 2.2007, 'grad_norm': 0.1267094761133194, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.0370709896087646, 'eval_runtime': 3.7849, 'eval_samples_per_second': 263.946, 'eval_steps_per_second': 16.645, 'epoch': 0.28}
{'loss': 1.9536, 'grad_norm': 0.11138510704040527, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8875386714935303, 'eval_runtime': 3.7733, 'eval_samples_per_second': 264.752, 'eval_steps_per_second': 16.696, 'epoch': 0.32}
{'loss': 1.8693, 'grad_norm': 0.0810154601931572, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8286176919937134, 'eval_runtime': 3.7896, 'eval_samples_per_second': 263.613, 'eval_steps_per_second': 16.624, 'epoch': 0.36}
{'loss': 1.8453, 'grad_norm': 0.13220061361789703, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7918592691421509, 'eval_runtime': 3.7786, 'eval_samples_per_second': 264.386, 'eval_steps_per_second': 16.673, 'epoch': 0.4}
{'loss': 1.7516, 'grad_norm': 0.11136951297521591, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7641117572784424, 'eval_runtime': 3.7725, 'eval_samples_per_second': 264.809, 'eval_steps_per_second': 16.7, 'epoch': 0.44}
{'loss': 1.7298, 'grad_norm': 0.09378567337989807, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7419205904006958, 'eval_runtime': 3.779, 'eval_samples_per_second': 264.353, 'eval_steps_per_second': 16.671, 'epoch': 0.48}
{'loss': 1.7178, 'grad_norm': 0.10273915529251099, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7229235172271729, 'eval_runtime': 3.7753, 'eval_samples_per_second': 264.616, 'eval_steps_per_second': 16.688, 'epoch': 0.52}
{'loss': 1.7034, 'grad_norm': 0.11246899515390396, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7037948369979858, 'eval_runtime': 3.7769, 'eval_samples_per_second': 264.499, 'eval_steps_per_second': 16.68, 'epoch': 0.56}
{'loss': 1.7098, 'grad_norm': 0.09228401631116867, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6877943277359009, 'eval_runtime': 3.8048, 'eval_samples_per_second': 262.56, 'eval_steps_per_second': 16.558, 'epoch': 0.6}
{'loss': 1.7121, 'grad_norm': 0.12091345340013504, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.675820231437683, 'eval_runtime': 3.801, 'eval_samples_per_second': 262.827, 'eval_steps_per_second': 16.575, 'epoch': 0.64}
{'loss': 1.6482, 'grad_norm': 0.1019250825047493, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6638528108596802, 'eval_runtime': 3.7813, 'eval_samples_per_second': 264.193, 'eval_steps_per_second': 16.661, 'epoch': 0.68}
{'loss': 1.6837, 'grad_norm': 0.05598142370581627, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6548696756362915, 'eval_runtime': 3.79, 'eval_samples_per_second': 263.59, 'eval_steps_per_second': 16.623, 'epoch': 0.72}
{'loss': 1.6586, 'grad_norm': 0.10069335997104645, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.645939826965332, 'eval_runtime': 3.7785, 'eval_samples_per_second': 264.394, 'eval_steps_per_second': 16.673, 'epoch': 0.76}
{'loss': 1.6406, 'grad_norm': 0.06720030307769775, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6401522159576416, 'eval_runtime': 3.788, 'eval_samples_per_second': 263.729, 'eval_steps_per_second': 16.632, 'epoch': 0.8}
{'loss': 1.6294, 'grad_norm': 0.11666771024465561, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6351171731948853, 'eval_runtime': 3.782, 'eval_samples_per_second': 264.148, 'eval_steps_per_second': 16.658, 'epoch': 0.84}
{'loss': 1.6596, 'grad_norm': 0.0922275111079216, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6308821439743042, 'eval_runtime': 3.814, 'eval_samples_per_second': 261.929, 'eval_steps_per_second': 16.518, 'epoch': 0.88}
{'loss': 1.607, 'grad_norm': 0.11178642511367798, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6275873184204102, 'eval_runtime': 3.812, 'eval_samples_per_second': 262.068, 'eval_steps_per_second': 16.527, 'epoch': 0.92}
{'loss': 1.6277, 'grad_norm': 0.08450686931610107, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6256104707717896, 'eval_runtime': 3.7978, 'eval_samples_per_second': 263.045, 'eval_steps_per_second': 16.588, 'epoch': 0.96}
{'loss': 1.5764, 'grad_norm': 0.09591436386108398, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.624928593635559, 'eval_runtime': 3.8026, 'eval_samples_per_second': 262.713, 'eval_steps_per_second': 16.568, 'epoch': 1.0}
{'train_runtime': 287.8194, 'train_samples_per_second': 34.741, 'train_steps_per_second': 2.172, 'train_loss': 2.2409565490722656, 'epoch': 1.0}
train_results:  {'eval_loss': [5.3122663497924805, 4.468440055847168, 3.6188836097717285, 3.1076066493988037, 2.7287745475769043, 2.385075807571411, 2.0370709896087646, 1.8875386714935303, 1.8286176919937134, 1.7918592691421509, 1.7641117572784424, 1.7419205904006958, 1.7229235172271729, 1.7037948369979858, 1.6877943277359009, 1.675820231437683, 1.6638528108596802, 1.6548696756362915, 1.645939826965332, 1.6401522159576416, 1.6351171731948853, 1.6308821439743042, 1.6275873184204102, 1.6256104707717896, 1.624928593635559], 'performance': [0.79, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:07<12:58,  7.87s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<00:58,  1.41it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:39,  1.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:25,  2.04it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:32<00:13,  2.54it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:35<00:06,  2.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.81it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.66it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.79, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  0.7168589234352112
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0843 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.7996679412012727
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.8660413241485624
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.8653049652304581
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.8619676959448193
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.8660416154487998
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3595, dtype=torch.float64), tensor(0.4691, dtype=torch.float64), 0, tensor(0.1714, dtype=torch.float64), 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 6.938893903907231e-19, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.3595, dtype=torch.float64), tensor(0.4691, dtype=torch.float64), tensor(1.5666e-18, dtype=torch.float64), tensor(0.1714, dtype=torch.float64), tensor(5.1797e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.3460e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(6.9389e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.36
  gsm8k: 0.469
  rowan_hellaswag: 0
  sciq: 0.171
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (6.938893903907231e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  6.938893903907231e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:54,  2.98s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.96it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.81it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:14,  2.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.13it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.32it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.34it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.94it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.4765, 'grad_norm': 1.4164835214614868, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.393073797225952, 'eval_runtime': 8.6284, 'eval_samples_per_second': 115.781, 'eval_steps_per_second': 7.301, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:33,  2.76s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:55,  1.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:34,  2.37it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:23,  2.83it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:21,  2.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:19,  2.64it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.12it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.91it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.3439, 'grad_norm': 0.6998577117919922, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 3.201413154602051, 'eval_runtime': 8.656, 'eval_samples_per_second': 115.412, 'eval_steps_per_second': 7.278, 'epoch': 0.08}
{'loss': 3.1625, 'grad_norm': 1.253347396850586, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.0520808696746826, 'eval_runtime': 8.6633, 'eval_samples_per_second': 115.314, 'eval_steps_per_second': 7.272, 'epoch': 0.12}
{'loss': 2.9379, 'grad_norm': 4.41891622543335, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.73412823677063, 'eval_runtime': 8.6751, 'eval_samples_per_second': 115.158, 'eval_steps_per_second': 7.262, 'epoch': 0.16}
{'loss': 2.7064, 'grad_norm': 1.4284664392471313, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.578756809234619, 'eval_runtime': 8.7145, 'eval_samples_per_second': 114.636, 'eval_steps_per_second': 7.229, 'epoch': 0.2}
{'loss': 2.523, 'grad_norm': 3.4845364093780518, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.499506711959839, 'eval_runtime': 8.7309, 'eval_samples_per_second': 114.421, 'eval_steps_per_second': 7.216, 'epoch': 0.24}
{'loss': 2.462, 'grad_norm': 1.9642446041107178, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.4591481685638428, 'eval_runtime': 8.7222, 'eval_samples_per_second': 114.536, 'eval_steps_per_second': 7.223, 'epoch': 0.28}
{'loss': 2.4461, 'grad_norm': 2.500225782394409, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.4220569133758545, 'eval_runtime': 8.7358, 'eval_samples_per_second': 114.357, 'eval_steps_per_second': 7.212, 'epoch': 0.32}
{'loss': 2.3724, 'grad_norm': 0.7999124526977539, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.3991191387176514, 'eval_runtime': 8.7175, 'eval_samples_per_second': 114.597, 'eval_steps_per_second': 7.227, 'epoch': 0.36}
{'loss': 2.3724, 'grad_norm': 1.036484956741333, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.374394178390503, 'eval_runtime': 8.7201, 'eval_samples_per_second': 114.563, 'eval_steps_per_second': 7.225, 'epoch': 0.4}
{'loss': 2.4144, 'grad_norm': 1.2007331848144531, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.34891676902771, 'eval_runtime': 8.7313, 'eval_samples_per_second': 114.416, 'eval_steps_per_second': 7.215, 'epoch': 0.44}
{'loss': 2.4231, 'grad_norm': 1.816733717918396, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.322707414627075, 'eval_runtime': 8.7297, 'eval_samples_per_second': 114.436, 'eval_steps_per_second': 7.217, 'epoch': 0.48}
{'loss': 2.3787, 'grad_norm': 2.0197348594665527, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.2992746829986572, 'eval_runtime': 8.7474, 'eval_samples_per_second': 114.205, 'eval_steps_per_second': 7.202, 'epoch': 0.52}
{'loss': 2.289, 'grad_norm': 1.4406194686889648, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.279120922088623, 'eval_runtime': 8.7493, 'eval_samples_per_second': 114.181, 'eval_steps_per_second': 7.201, 'epoch': 0.56}
{'loss': 2.2943, 'grad_norm': 2.6518731117248535, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.2596054077148438, 'eval_runtime': 8.7338, 'eval_samples_per_second': 114.383, 'eval_steps_per_second': 7.213, 'epoch': 0.6}
{'loss': 2.3062, 'grad_norm': 1.701940894126892, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.2346365451812744, 'eval_runtime': 8.747, 'eval_samples_per_second': 114.211, 'eval_steps_per_second': 7.203, 'epoch': 0.64}
{'loss': 2.2451, 'grad_norm': 1.9256541728973389, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2167558670043945, 'eval_runtime': 8.7888, 'eval_samples_per_second': 113.668, 'eval_steps_per_second': 7.168, 'epoch': 0.68}
{'loss': 2.239, 'grad_norm': 2.143252372741699, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.2019083499908447, 'eval_runtime': 8.72, 'eval_samples_per_second': 114.564, 'eval_steps_per_second': 7.225, 'epoch': 0.72}
{'loss': 2.2722, 'grad_norm': 1.9606554508209229, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.1896209716796875, 'eval_runtime': 8.7283, 'eval_samples_per_second': 114.455, 'eval_steps_per_second': 7.218, 'epoch': 0.76}
{'loss': 2.1795, 'grad_norm': 1.3189507722854614, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.1799659729003906, 'eval_runtime': 8.7172, 'eval_samples_per_second': 114.601, 'eval_steps_per_second': 7.227, 'epoch': 0.8}
{'loss': 2.2131, 'grad_norm': 3.6269757747650146, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.1735055446624756, 'eval_runtime': 8.7204, 'eval_samples_per_second': 114.559, 'eval_steps_per_second': 7.224, 'epoch': 0.84}
{'loss': 2.2479, 'grad_norm': 1.004847526550293, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.1679701805114746, 'eval_runtime': 8.7163, 'eval_samples_per_second': 114.613, 'eval_steps_per_second': 7.228, 'epoch': 0.88}
{'loss': 2.2394, 'grad_norm': 0.7675227522850037, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1641173362731934, 'eval_runtime': 8.7181, 'eval_samples_per_second': 114.589, 'eval_steps_per_second': 7.226, 'epoch': 0.92}
{'loss': 2.2057, 'grad_norm': 0.8353549242019653, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1612884998321533, 'eval_runtime': 8.7191, 'eval_samples_per_second': 114.576, 'eval_steps_per_second': 7.226, 'epoch': 0.96}
{'loss': 2.2118, 'grad_norm': 1.5086252689361572, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.160966157913208, 'eval_runtime': 8.7081, 'eval_samples_per_second': 114.72, 'eval_steps_per_second': 7.235, 'epoch': 1.0}
{'train_runtime': 508.7276, 'train_samples_per_second': 19.655, 'train_steps_per_second': 1.229, 'train_loss': 2.4784876037597656, 'epoch': 1.0}
train_results:  {'eval_loss': [3.393073797225952, 3.201413154602051, 3.0520808696746826, 2.73412823677063, 2.578756809234619, 2.499506711959839, 2.4591481685638428, 2.4220569133758545, 2.3991191387176514, 2.374394178390503, 2.34891676902771, 2.322707414627075, 2.2992746829986572, 2.279120922088623, 2.2596054077148438, 2.2346365451812744, 2.2167558670043945, 2.2019083499908447, 2.1896209716796875, 2.1799659729003906, 2.1735055446624756, 2.1679701805114746, 2.1641173362731934, 2.1612884998321533, 2.160966157913208], 'performance': [0.77, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<13:45,  8.34s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:11,  1.16it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:41,  1.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:23,  2.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:32<00:13,  2.53it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:06,  2.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.72it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.57it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2275041341781616
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2909 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.7896819890572782
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 0.8549761535679554
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.8602033878512197
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.8252497389506614
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.7966911849640043
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0592, dtype=torch.float64), tensor(0.3450, dtype=torch.float64), 0, tensor(0.1723, dtype=torch.float64), 0, tensor(0.4235, dtype=torch.float64), 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1.3877787807814456e-18, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0592, dtype=torch.float64), tensor(0.3450, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1723, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4235, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.6622e-17, dtype=torch.float64), tensor(8.8664e-18, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.3878e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.059
  gsm8k: 0.345
  rowan_hellaswag: 0
  sciq: 0.172
  triviaqa: 0
  truthfulqa_gen: 0.424
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.3877787807814456e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  1.3877787807814456e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:03,  2.46s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.02it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.55it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:23,  2.80it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.92it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.82it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.07it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.84it/s]
Evaluation performance at step 25: 0.78
{'loss': 3.8799, 'grad_norm': 2.226228713989258, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 3.675407886505127, 'eval_runtime': 8.4501, 'eval_samples_per_second': 118.224, 'eval_steps_per_second': 7.456, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:05,  2.48s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:10,  1.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.00it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.52it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.75it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.70it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:19,  2.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  2.93it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.13it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.52it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:04,  2.71it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.71it/s]
Evaluation performance at step 50: 0.77
{'loss': 3.6818, 'grad_norm': 1.1448348760604858, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 3.4414453506469727, 'eval_runtime': 8.4624, 'eval_samples_per_second': 118.051, 'eval_steps_per_second': 7.445, 'epoch': 0.08}
{'loss': 3.4558, 'grad_norm': 1.279639482498169, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.2079966068267822, 'eval_runtime': 8.5585, 'eval_samples_per_second': 116.725, 'eval_steps_per_second': 7.361, 'epoch': 0.12}
{'loss': 3.0246, 'grad_norm': 3.9370460510253906, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.8723673820495605, 'eval_runtime': 8.5814, 'eval_samples_per_second': 116.415, 'eval_steps_per_second': 7.341, 'epoch': 0.16}
{'loss': 2.9121, 'grad_norm': 1.9910122156143188, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.752568006515503, 'eval_runtime': 8.5692, 'eval_samples_per_second': 116.58, 'eval_steps_per_second': 7.352, 'epoch': 0.2}
{'loss': 2.7774, 'grad_norm': 3.5114147663116455, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.6876280307769775, 'eval_runtime': 8.5842, 'eval_samples_per_second': 116.377, 'eval_steps_per_second': 7.339, 'epoch': 0.24}
{'loss': 2.7634, 'grad_norm': 6.837685585021973, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.641561508178711, 'eval_runtime': 8.5677, 'eval_samples_per_second': 116.6, 'eval_steps_per_second': 7.353, 'epoch': 0.28}
{'loss': 2.6458, 'grad_norm': 2.0459818840026855, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.604671001434326, 'eval_runtime': 8.5995, 'eval_samples_per_second': 116.169, 'eval_steps_per_second': 7.326, 'epoch': 0.32}
{'loss': 2.6253, 'grad_norm': 2.2801826000213623, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.582941770553589, 'eval_runtime': 8.5802, 'eval_samples_per_second': 116.431, 'eval_steps_per_second': 7.343, 'epoch': 0.36}
{'loss': 2.6553, 'grad_norm': 7.0965752601623535, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.559056520462036, 'eval_runtime': 8.5824, 'eval_samples_per_second': 116.401, 'eval_steps_per_second': 7.341, 'epoch': 0.4}
{'loss': 2.5552, 'grad_norm': 4.666964054107666, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.5421690940856934, 'eval_runtime': 8.6002, 'eval_samples_per_second': 116.16, 'eval_steps_per_second': 7.325, 'epoch': 0.44}
{'loss': 2.566, 'grad_norm': 2.697057008743286, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.5190646648406982, 'eval_runtime': 8.6001, 'eval_samples_per_second': 116.162, 'eval_steps_per_second': 7.326, 'epoch': 0.48}
{'loss': 2.5083, 'grad_norm': 3.9864084720611572, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.5058603286743164, 'eval_runtime': 8.5933, 'eval_samples_per_second': 116.253, 'eval_steps_per_second': 7.331, 'epoch': 0.52}
{'loss': 2.4861, 'grad_norm': 2.144381523132324, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.4828739166259766, 'eval_runtime': 8.5546, 'eval_samples_per_second': 116.78, 'eval_steps_per_second': 7.364, 'epoch': 0.56}
{'loss': 2.5181, 'grad_norm': 3.3901684284210205, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.4495201110839844, 'eval_runtime': 8.5575, 'eval_samples_per_second': 116.74, 'eval_steps_per_second': 7.362, 'epoch': 0.6}
{'loss': 2.4135, 'grad_norm': 2.2122583389282227, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.4190568923950195, 'eval_runtime': 8.5484, 'eval_samples_per_second': 116.864, 'eval_steps_per_second': 7.37, 'epoch': 0.64}
{'loss': 2.4753, 'grad_norm': 5.449292182922363, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.399628162384033, 'eval_runtime': 8.5392, 'eval_samples_per_second': 116.989, 'eval_steps_per_second': 7.378, 'epoch': 0.68}
{'loss': 2.3365, 'grad_norm': 3.7954349517822266, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.385720729827881, 'eval_runtime': 8.5261, 'eval_samples_per_second': 117.17, 'eval_steps_per_second': 7.389, 'epoch': 0.72}
{'loss': 2.4157, 'grad_norm': 2.6950063705444336, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.3697781562805176, 'eval_runtime': 8.5366, 'eval_samples_per_second': 117.026, 'eval_steps_per_second': 7.38, 'epoch': 0.76}
{'loss': 2.4006, 'grad_norm': 2.748441457748413, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.357120990753174, 'eval_runtime': 8.541, 'eval_samples_per_second': 116.965, 'eval_steps_per_second': 7.376, 'epoch': 0.8}
{'loss': 2.3659, 'grad_norm': 3.155564069747925, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.343353509902954, 'eval_runtime': 8.5237, 'eval_samples_per_second': 117.202, 'eval_steps_per_second': 7.391, 'epoch': 0.84}
{'loss': 2.373, 'grad_norm': 11.365386009216309, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.3367958068847656, 'eval_runtime': 8.5321, 'eval_samples_per_second': 117.087, 'eval_steps_per_second': 7.384, 'epoch': 0.88}
{'loss': 2.412, 'grad_norm': 2.794011354446411, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.3295445442199707, 'eval_runtime': 8.535, 'eval_samples_per_second': 117.048, 'eval_steps_per_second': 7.381, 'epoch': 0.92}
{'loss': 2.3343, 'grad_norm': 1.8971055746078491, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.324394702911377, 'eval_runtime': 8.546, 'eval_samples_per_second': 116.897, 'eval_steps_per_second': 7.372, 'epoch': 0.96}
{'loss': 2.305, 'grad_norm': 5.056407928466797, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.3240153789520264, 'eval_runtime': 8.5369, 'eval_samples_per_second': 117.021, 'eval_steps_per_second': 7.38, 'epoch': 1.0}
{'train_runtime': 501.791, 'train_samples_per_second': 19.927, 'train_steps_per_second': 1.246, 'train_loss': 2.6754761962890625, 'epoch': 1.0}
train_results:  {'eval_loss': [3.675407886505127, 3.4414453506469727, 3.2079966068267822, 2.8723673820495605, 2.752568006515503, 2.6876280307769775, 2.641561508178711, 2.604671001434326, 2.582941770553589, 2.559056520462036, 2.5421690940856934, 2.5190646648406982, 2.5058603286743164, 2.4828739166259766, 2.4495201110839844, 2.4190568923950195, 2.399628162384033, 2.385720729827881, 2.3697781562805176, 2.357120990753174, 2.343353509902954, 2.3367958068847656, 2.3295445442199707, 2.324394702911377, 2.3240153789520264], 'performance': [0.78, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:48,  9.59s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:05,  1.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:41,  1.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:23,  2.14it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:14,  2.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:06,  2.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:00,  3.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.50it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  1.2271978855133057
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0655 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.812244414378643
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.8548800951430957
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.8548811328554267
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.8547545819109087
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.8136675486407574
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1570, dtype=torch.float64), tensor(0.3314, dtype=torch.float64), 0, tensor(0.1743, dtype=torch.float64), 0, 0, tensor(0.3373, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 1.3877787807814463e-18, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.1570, dtype=torch.float64), tensor(0.3314, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1743, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.3968e-16, dtype=torch.float64), tensor(0.3373, dtype=torch.float64), tensor(2.9840e-17, dtype=torch.float64), tensor(1.4661e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.3878e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.157
  gsm8k: 0.331
  rowan_hellaswag: 0
  sciq: 0.174
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.337
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.3877787807814463e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  1.3877787807814463e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:36,  2.79s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.97it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.52it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.64it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.79it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:14,  2.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.12it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.32it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.49it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.73it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.98it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.3994, 'grad_norm': 1.59727144241333, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.2975122928619385, 'eval_runtime': 8.568, 'eval_samples_per_second': 116.597, 'eval_steps_per_second': 7.353, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:35,  2.78s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:45,  1.99it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.61it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.03it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:14<00:18,  3.11it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:18,  2.81it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:11,  3.11it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:08,  3.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.61it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.00it/s]
Evaluation performance at step 50: 0.75
{'loss': 3.2635, 'grad_norm': 1.0985952615737915, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 3.1346395015716553, 'eval_runtime': 8.6289, 'eval_samples_per_second': 115.774, 'eval_steps_per_second': 7.301, 'epoch': 0.08}
{'loss': 3.1561, 'grad_norm': 1.006120204925537, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.952500820159912, 'eval_runtime': 8.6139, 'eval_samples_per_second': 115.976, 'eval_steps_per_second': 7.314, 'epoch': 0.12}
{'loss': 2.927, 'grad_norm': 1.1865216493606567, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.7183775901794434, 'eval_runtime': 8.6557, 'eval_samples_per_second': 115.416, 'eval_steps_per_second': 7.278, 'epoch': 0.16}
{'loss': 2.6416, 'grad_norm': 1.6799712181091309, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.605227470397949, 'eval_runtime': 8.6714, 'eval_samples_per_second': 115.206, 'eval_steps_per_second': 7.265, 'epoch': 0.2}
{'loss': 2.5568, 'grad_norm': 1.526775598526001, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.5556881427764893, 'eval_runtime': 8.6841, 'eval_samples_per_second': 115.038, 'eval_steps_per_second': 7.255, 'epoch': 0.24}
{'loss': 2.6765, 'grad_norm': 2.5129446983337402, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.5190136432647705, 'eval_runtime': 8.64, 'eval_samples_per_second': 115.625, 'eval_steps_per_second': 7.292, 'epoch': 0.28}
{'loss': 2.4966, 'grad_norm': 2.2289769649505615, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.4870645999908447, 'eval_runtime': 8.6384, 'eval_samples_per_second': 115.647, 'eval_steps_per_second': 7.293, 'epoch': 0.32}
{'loss': 2.5923, 'grad_norm': 0.6379137635231018, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.455065965652466, 'eval_runtime': 8.6194, 'eval_samples_per_second': 115.902, 'eval_steps_per_second': 7.309, 'epoch': 0.36}
{'loss': 2.4647, 'grad_norm': 1.4725558757781982, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.422210693359375, 'eval_runtime': 8.6291, 'eval_samples_per_second': 115.771, 'eval_steps_per_second': 7.301, 'epoch': 0.4}
{'loss': 2.4829, 'grad_norm': 1.3571828603744507, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.397590398788452, 'eval_runtime': 8.621, 'eval_samples_per_second': 115.88, 'eval_steps_per_second': 7.308, 'epoch': 0.44}
{'loss': 2.4879, 'grad_norm': 0.7271699905395508, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.370528221130371, 'eval_runtime': 8.6385, 'eval_samples_per_second': 115.645, 'eval_steps_per_second': 7.293, 'epoch': 0.48}
{'loss': 2.3663, 'grad_norm': 1.338027000427246, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.351405620574951, 'eval_runtime': 8.6361, 'eval_samples_per_second': 115.678, 'eval_steps_per_second': 7.295, 'epoch': 0.52}
{'loss': 2.4182, 'grad_norm': 0.8154435753822327, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.3348441123962402, 'eval_runtime': 8.6109, 'eval_samples_per_second': 116.016, 'eval_steps_per_second': 7.316, 'epoch': 0.56}
{'loss': 2.4012, 'grad_norm': 0.8782939910888672, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.3241171836853027, 'eval_runtime': 8.6183, 'eval_samples_per_second': 115.916, 'eval_steps_per_second': 7.31, 'epoch': 0.6}
{'loss': 2.3385, 'grad_norm': 0.7569019198417664, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.310572624206543, 'eval_runtime': 8.6285, 'eval_samples_per_second': 115.779, 'eval_steps_per_second': 7.301, 'epoch': 0.64}
{'loss': 2.3812, 'grad_norm': 1.2539794445037842, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2972447872161865, 'eval_runtime': 8.6091, 'eval_samples_per_second': 116.04, 'eval_steps_per_second': 7.318, 'epoch': 0.68}
{'loss': 2.3068, 'grad_norm': 0.6835440397262573, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.2852847576141357, 'eval_runtime': 8.6046, 'eval_samples_per_second': 116.101, 'eval_steps_per_second': 7.322, 'epoch': 0.72}
{'loss': 2.3027, 'grad_norm': 1.0897414684295654, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.276050567626953, 'eval_runtime': 8.619, 'eval_samples_per_second': 115.906, 'eval_steps_per_second': 7.309, 'epoch': 0.76}
{'loss': 2.3349, 'grad_norm': 0.6844255924224854, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.2682061195373535, 'eval_runtime': 8.6078, 'eval_samples_per_second': 116.058, 'eval_steps_per_second': 7.319, 'epoch': 0.8}
{'loss': 2.3346, 'grad_norm': 1.030428409576416, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.261932373046875, 'eval_runtime': 8.6178, 'eval_samples_per_second': 115.923, 'eval_steps_per_second': 7.31, 'epoch': 0.84}
{'loss': 2.2614, 'grad_norm': 0.999117374420166, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.256627321243286, 'eval_runtime': 8.5998, 'eval_samples_per_second': 116.165, 'eval_steps_per_second': 7.326, 'epoch': 0.88}
{'loss': 2.3131, 'grad_norm': 0.7458353638648987, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.2518844604492188, 'eval_runtime': 8.6129, 'eval_samples_per_second': 115.989, 'eval_steps_per_second': 7.315, 'epoch': 0.92}
{'loss': 2.3169, 'grad_norm': 0.8551487922668457, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.2499399185180664, 'eval_runtime': 8.6078, 'eval_samples_per_second': 116.058, 'eval_steps_per_second': 7.319, 'epoch': 0.96}
{'loss': 2.2049, 'grad_norm': 1.0654133558273315, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.248558759689331, 'eval_runtime': 8.5983, 'eval_samples_per_second': 116.185, 'eval_steps_per_second': 7.327, 'epoch': 1.0}
{'train_runtime': 502.6087, 'train_samples_per_second': 19.892, 'train_steps_per_second': 1.244, 'train_loss': 2.5370346862792967, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2975122928619385, 3.1346395015716553, 2.952500820159912, 2.7183775901794434, 2.605227470397949, 2.5556881427764893, 2.5190136432647705, 2.4870645999908447, 2.455065965652466, 2.422210693359375, 2.397590398788452, 2.370528221130371, 2.351405620574951, 2.3348441123962402, 2.3241171836853027, 2.310572624206543, 2.2972447872161865, 2.2852847576141357, 2.276050567626953, 2.2682061195373535, 2.261932373046875, 2.256627321243286, 2.2518844604492188, 2.2499399185180664, 2.248558759689331], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:07,  9.17s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:08,  1.22it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:39,  1.68it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:23,  2.21it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:32<00:13,  2.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:36<00:06,  2.99it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.74it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.60it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  1.2274394035339355
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8378 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.82712054767841
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.8527065792100481
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.8178323811105066
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.7382381668278245
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.8527130582866336
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.6774, dtype=torch.float64), 0, 0, tensor(0.1701, dtype=torch.float64), 0, tensor(0.1525, dtype=torch.float64), 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 8.887556553231707e-20, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.6774, dtype=torch.float64), tensor(1.3314e-05, dtype=torch.float64), tensor(5.0111e-17, dtype=torch.float64), tensor(0.1701, dtype=torch.float64), tensor(3.6310e-18, dtype=torch.float64), tensor(0.1525, dtype=torch.float64), tensor(5.1408e-17, dtype=torch.float64), tensor(1.5315e-17, dtype=torch.float64), tensor(8.4655e-18, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(8.8876e-19, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.677
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.17
  triviaqa: 0
  truthfulqa_gen: 0.152
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (8.887556553231707e-20,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  2
lora dropout:  8.887556553231707e-20
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 73,728 || all params: 8,030,334,976 || trainable%: 0.0009
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:50,  4.14s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:22,  1.10it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.78it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:34,  2.17it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.50it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:21,  2.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:17,  2.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:15,  2.83it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  3.16it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.70it/s]
Evaluation performance at step 25: 0.78
{'loss': 4.7038, 'grad_norm': 9.6610689163208, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 3.5797882080078125, 'eval_runtime': 4.3941, 'eval_samples_per_second': 227.348, 'eval_steps_per_second': 14.337, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:44,  5.30s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:30,  1.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:49,  1.67it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:17<00:44,  1.68it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:19<00:32,  2.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:24<00:30,  1.91it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:26<00:22,  2.24it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:29<00:18,  2.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:31<00:12,  2.77it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:34<00:09,  2.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:07,  2.41it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:43<00:04,  2.20it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:46<00:01,  2.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:46<00:00,  2.13it/s]
Evaluation performance at step 50: 0.65
{'loss': 2.6174, 'grad_norm': 4.732235908508301, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.65}
{'eval_loss': 1.8392095565795898, 'eval_runtime': 4.3229, 'eval_samples_per_second': 231.094, 'eval_steps_per_second': 14.573, 'epoch': 0.08}
{'loss': 1.5332, 'grad_norm': 2.898908853530884, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2935755252838135, 'eval_runtime': 4.3292, 'eval_samples_per_second': 230.759, 'eval_steps_per_second': 14.552, 'epoch': 0.12}
{'loss': 1.1769, 'grad_norm': 2.3730788230895996, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.111891269683838, 'eval_runtime': 4.3488, 'eval_samples_per_second': 229.717, 'eval_steps_per_second': 14.487, 'epoch': 0.16}
{'loss': 1.0815, 'grad_norm': 3.1183907985687256, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0722416639328003, 'eval_runtime': 4.3649, 'eval_samples_per_second': 228.873, 'eval_steps_per_second': 14.433, 'epoch': 0.2}
{'loss': 1.0641, 'grad_norm': 2.142697811126709, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0436840057373047, 'eval_runtime': 4.3583, 'eval_samples_per_second': 229.216, 'eval_steps_per_second': 14.455, 'epoch': 0.24}
{'loss': 1.0541, 'grad_norm': 2.4028310775756836, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.031321406364441, 'eval_runtime': 4.348, 'eval_samples_per_second': 229.761, 'eval_steps_per_second': 14.489, 'epoch': 0.28}
{'loss': 1.0263, 'grad_norm': 2.0502874851226807, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0218201875686646, 'eval_runtime': 4.3559, 'eval_samples_per_second': 229.345, 'eval_steps_per_second': 14.463, 'epoch': 0.32}
{'loss': 1.043, 'grad_norm': 2.952254295349121, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0122071504592896, 'eval_runtime': 4.3531, 'eval_samples_per_second': 229.492, 'eval_steps_per_second': 14.472, 'epoch': 0.36}
{'loss': 1.0217, 'grad_norm': 2.0557711124420166, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0055500268936157, 'eval_runtime': 4.3672, 'eval_samples_per_second': 228.751, 'eval_steps_per_second': 14.426, 'epoch': 0.4}
{'loss': 1.0115, 'grad_norm': 1.9797776937484741, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9980152249336243, 'eval_runtime': 4.36, 'eval_samples_per_second': 229.13, 'eval_steps_per_second': 14.45, 'epoch': 0.44}
{'loss': 0.9947, 'grad_norm': 2.156463384628296, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9946235418319702, 'eval_runtime': 4.3673, 'eval_samples_per_second': 228.747, 'eval_steps_per_second': 14.425, 'epoch': 0.48}
{'loss': 0.9969, 'grad_norm': 2.3363561630249023, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9898838400840759, 'eval_runtime': 4.3673, 'eval_samples_per_second': 228.745, 'eval_steps_per_second': 14.425, 'epoch': 0.52}
{'loss': 0.975, 'grad_norm': 2.2307183742523193, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.986703634262085, 'eval_runtime': 4.3788, 'eval_samples_per_second': 228.144, 'eval_steps_per_second': 14.387, 'epoch': 0.56}
{'loss': 0.9834, 'grad_norm': 2.1027798652648926, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9840282201766968, 'eval_runtime': 4.3728, 'eval_samples_per_second': 228.459, 'eval_steps_per_second': 14.407, 'epoch': 0.6}
{'loss': 0.9766, 'grad_norm': 2.742227554321289, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9811995625495911, 'eval_runtime': 4.3694, 'eval_samples_per_second': 228.637, 'eval_steps_per_second': 14.419, 'epoch': 0.64}
{'loss': 0.9823, 'grad_norm': 1.7975937128067017, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.977706253528595, 'eval_runtime': 4.3747, 'eval_samples_per_second': 228.356, 'eval_steps_per_second': 14.401, 'epoch': 0.68}
{'loss': 0.9913, 'grad_norm': 2.0302278995513916, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9775981307029724, 'eval_runtime': 4.3734, 'eval_samples_per_second': 228.426, 'eval_steps_per_second': 14.405, 'epoch': 0.72}
{'loss': 0.978, 'grad_norm': 2.1569125652313232, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.974445104598999, 'eval_runtime': 4.3834, 'eval_samples_per_second': 227.903, 'eval_steps_per_second': 14.372, 'epoch': 0.76}
{'loss': 0.9671, 'grad_norm': 2.1112658977508545, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9718919396400452, 'eval_runtime': 4.3774, 'eval_samples_per_second': 228.22, 'eval_steps_per_second': 14.392, 'epoch': 0.8}
{'loss': 0.9833, 'grad_norm': 2.076848030090332, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9701346158981323, 'eval_runtime': 4.3888, 'eval_samples_per_second': 227.627, 'eval_steps_per_second': 14.355, 'epoch': 0.84}
{'loss': 0.9812, 'grad_norm': 2.8167920112609863, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9694108366966248, 'eval_runtime': 4.3748, 'eval_samples_per_second': 228.352, 'eval_steps_per_second': 14.401, 'epoch': 0.88}
{'loss': 0.9862, 'grad_norm': 2.0226006507873535, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9684431552886963, 'eval_runtime': 4.3756, 'eval_samples_per_second': 228.313, 'eval_steps_per_second': 14.398, 'epoch': 0.92}
{'loss': 0.9906, 'grad_norm': 1.8812248706817627, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9675644636154175, 'eval_runtime': 4.3757, 'eval_samples_per_second': 228.305, 'eval_steps_per_second': 14.398, 'epoch': 0.96}
{'loss': 0.9945, 'grad_norm': 2.0964365005493164, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9674420356750488, 'eval_runtime': 4.3769, 'eval_samples_per_second': 228.242, 'eval_steps_per_second': 14.394, 'epoch': 1.0}
{'train_runtime': 274.7392, 'train_samples_per_second': 36.391, 'train_steps_per_second': 2.275, 'train_loss': 1.2445786987304688, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5797882080078125, 1.8392095565795898, 1.2935755252838135, 1.111891269683838, 1.0722416639328003, 1.0436840057373047, 1.031321406364441, 1.0218201875686646, 1.0122071504592896, 1.0055500268936157, 0.9980152249336243, 0.9946235418319702, 0.9898838400840759, 0.986703634262085, 0.9840282201766968, 0.9811995625495911, 0.977706253528595, 0.9775981307029724, 0.974445104598999, 0.9718919396400452, 0.9701346158981323, 0.9694108366966248, 0.9684431552886963, 0.9675644636154175, 0.9674420356750488], 'performance': [0.78, 0.65]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:49,  9.59s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:41<00:19,  1.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.87it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.65]
current iteration observed (possibly low-fid or predicted) performance:  1.2018580436706543
current iteration best possible performance (full train run):  0.735
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.9319 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.8418497427978844
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 0.7308965069734353
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 0.7096693479911057
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.8489058327033994
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.8488911386749087
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2447, dtype=torch.float64), 0, tensor(0.1764, dtype=torch.float64), 0, 0, tensor(0.5788, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(3.3383e-16, dtype=torch.float64), tensor(0.2447, dtype=torch.float64), tensor(1.0361e-16, dtype=torch.float64), tensor(0.1764, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5535e-16, dtype=torch.float64), tensor(0.5788, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.245
  rowan_hellaswag: 0
  sciq: 0.176
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.579
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:56,  3.00s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.24it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.95it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.76it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.81it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.26it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:06,  2.82it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.10it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.42it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.86it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.251, 'grad_norm': 1.925844669342041, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.183945655822754, 'eval_runtime': 8.3233, 'eval_samples_per_second': 120.025, 'eval_steps_per_second': 7.569, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:22,  3.86s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:19,  1.15it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.78it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:24,  2.75it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:20,  2.87it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  3.00it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:14,  2.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.20it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.76it/s]
Evaluation performance at step 50: 0.72
{'loss': 2.8353, 'grad_norm': 0.7997647523880005, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.72}
{'eval_loss': 2.737757444381714, 'eval_runtime': 8.3324, 'eval_samples_per_second': 119.893, 'eval_steps_per_second': 7.561, 'epoch': 0.08}
{'loss': 2.6159, 'grad_norm': 0.8877360820770264, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.5637502670288086, 'eval_runtime': 8.3461, 'eval_samples_per_second': 119.697, 'eval_steps_per_second': 7.548, 'epoch': 0.12}
{'loss': 2.4609, 'grad_norm': 0.5333132743835449, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.483375310897827, 'eval_runtime': 8.3791, 'eval_samples_per_second': 119.226, 'eval_steps_per_second': 7.519, 'epoch': 0.16}
{'loss': 2.5224, 'grad_norm': 0.598514974117279, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.4387423992156982, 'eval_runtime': 8.3892, 'eval_samples_per_second': 119.081, 'eval_steps_per_second': 7.51, 'epoch': 0.2}
{'loss': 2.4476, 'grad_norm': 0.6155624985694885, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.402956247329712, 'eval_runtime': 8.3869, 'eval_samples_per_second': 119.114, 'eval_steps_per_second': 7.512, 'epoch': 0.24}
{'loss': 2.455, 'grad_norm': 0.4227586090564728, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.379002094268799, 'eval_runtime': 8.3947, 'eval_samples_per_second': 119.003, 'eval_steps_per_second': 7.505, 'epoch': 0.28}
{'loss': 2.3903, 'grad_norm': 0.5998855829238892, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.3583061695098877, 'eval_runtime': 8.406, 'eval_samples_per_second': 118.843, 'eval_steps_per_second': 7.495, 'epoch': 0.32}
{'loss': 2.394, 'grad_norm': 0.5912752747535706, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.3393335342407227, 'eval_runtime': 8.4082, 'eval_samples_per_second': 118.812, 'eval_steps_per_second': 7.493, 'epoch': 0.36}
{'loss': 2.3483, 'grad_norm': 0.8026409149169922, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.3290443420410156, 'eval_runtime': 8.4191, 'eval_samples_per_second': 118.658, 'eval_steps_per_second': 7.483, 'epoch': 0.4}
{'loss': 2.3029, 'grad_norm': 0.7439007759094238, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.3201794624328613, 'eval_runtime': 8.4129, 'eval_samples_per_second': 118.747, 'eval_steps_per_second': 7.489, 'epoch': 0.44}
{'loss': 2.2851, 'grad_norm': 0.8032199144363403, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.3120298385620117, 'eval_runtime': 8.4305, 'eval_samples_per_second': 118.499, 'eval_steps_per_second': 7.473, 'epoch': 0.48}
{'loss': 2.2466, 'grad_norm': 0.6233994364738464, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.3064706325531006, 'eval_runtime': 8.4599, 'eval_samples_per_second': 118.086, 'eval_steps_per_second': 7.447, 'epoch': 0.52}
{'loss': 2.3119, 'grad_norm': 0.8652665019035339, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.302638530731201, 'eval_runtime': 8.4298, 'eval_samples_per_second': 118.509, 'eval_steps_per_second': 7.474, 'epoch': 0.56}
{'loss': 2.3078, 'grad_norm': 0.7468894720077515, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.2989940643310547, 'eval_runtime': 8.4225, 'eval_samples_per_second': 118.611, 'eval_steps_per_second': 7.48, 'epoch': 0.6}
{'loss': 2.2777, 'grad_norm': 0.6152198314666748, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.2959787845611572, 'eval_runtime': 8.4369, 'eval_samples_per_second': 118.409, 'eval_steps_per_second': 7.467, 'epoch': 0.64}
{'loss': 2.3381, 'grad_norm': 0.8026252388954163, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.293524742126465, 'eval_runtime': 8.4213, 'eval_samples_per_second': 118.628, 'eval_steps_per_second': 7.481, 'epoch': 0.68}
{'loss': 2.2261, 'grad_norm': 0.5297622084617615, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.291098117828369, 'eval_runtime': 8.4235, 'eval_samples_per_second': 118.597, 'eval_steps_per_second': 7.479, 'epoch': 0.72}
{'loss': 2.1991, 'grad_norm': 0.5246354341506958, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.288801670074463, 'eval_runtime': 8.4046, 'eval_samples_per_second': 118.863, 'eval_steps_per_second': 7.496, 'epoch': 0.76}
{'loss': 2.2148, 'grad_norm': 0.9455419182777405, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.2868640422821045, 'eval_runtime': 8.4163, 'eval_samples_per_second': 118.698, 'eval_steps_per_second': 7.485, 'epoch': 0.8}
{'loss': 2.2193, 'grad_norm': 0.6325042843818665, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.285386562347412, 'eval_runtime': 8.4199, 'eval_samples_per_second': 118.647, 'eval_steps_per_second': 7.482, 'epoch': 0.84}
{'loss': 2.2364, 'grad_norm': 0.705191969871521, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.2839367389678955, 'eval_runtime': 8.4293, 'eval_samples_per_second': 118.515, 'eval_steps_per_second': 7.474, 'epoch': 0.88}
{'loss': 2.3512, 'grad_norm': 0.6475433707237244, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.283214807510376, 'eval_runtime': 8.4364, 'eval_samples_per_second': 118.416, 'eval_steps_per_second': 7.468, 'epoch': 0.92}
{'loss': 2.261, 'grad_norm': 0.806796669960022, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.2826175689697266, 'eval_runtime': 8.4282, 'eval_samples_per_second': 118.53, 'eval_steps_per_second': 7.475, 'epoch': 0.96}
{'loss': 2.3541, 'grad_norm': 0.590313196182251, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.2825076580047607, 'eval_runtime': 8.4268, 'eval_samples_per_second': 118.551, 'eval_steps_per_second': 7.476, 'epoch': 1.0}
{'train_runtime': 402.9384, 'train_samples_per_second': 24.815, 'train_steps_per_second': 1.551, 'train_loss': 2.394113629150391, 'epoch': 1.0}
train_results:  {'eval_loss': [3.183945655822754, 2.737757444381714, 2.5637502670288086, 2.483375310897827, 2.4387423992156982, 2.402956247329712, 2.379002094268799, 2.3583061695098877, 2.3393335342407227, 2.3290443420410156, 2.3201794624328613, 2.3120298385620117, 2.3064706325531006, 2.302638530731201, 2.2989940643310547, 2.2959787845611572, 2.293524742126465, 2.291098117828369, 2.288801670074463, 2.2868640422821045, 2.285386562347412, 2.2839367389678955, 2.283214807510376, 2.2826175689697266, 2.2825076580047607], 'performance': [0.77, 0.72]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:47,  9.57s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:41<00:19,  1.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:51<00:01,  2.60it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:51<00:00,  1.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.72]
current iteration observed (possibly low-fid or predicted) performance:  1.2357836961746216
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.0497 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.8109357252868876
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.8440650227677566
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.7246542069991473
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.8346894542037621
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.8439162105303399
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0412, dtype=torch.float64), tensor(0.3997, dtype=torch.float64), tensor(0.2835, dtype=torch.float64), tensor(0.1836, dtype=torch.float64), 0, tensor(0.0387, dtype=torch.float64), tensor(0.0534, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 3.0814879110195774e-33, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0412, dtype=torch.float64), tensor(0.3997, dtype=torch.float64), tensor(0.2835, dtype=torch.float64), tensor(0.1836, dtype=torch.float64), tensor(1.0624e-17, dtype=torch.float64), tensor(0.0387, dtype=torch.float64), tensor(0.0534, dtype=torch.float64), tensor(7.5111e-18, dtype=torch.float64), tensor(5.2132e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.0815e-32, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.041
  gsm8k: 0.4
  rowan_hellaswag: 0.284
  sciq: 0.184
  triviaqa: 0
  truthfulqa_gen: 0.039
  wikitext: 0.053
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.0814879110195774e-33,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  3.0814879110195774e-33
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:54,  2.97s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.96it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.52it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:28,  2.37it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:20<00:28,  2.09it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:21,  2.39it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:17,  2.47it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:12,  2.80it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:08,  3.06it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:06,  3.07it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  3.30it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.58it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.74it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.6153, 'grad_norm': 3.107020378112793, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.419475555419922, 'eval_runtime': 8.7793, 'eval_samples_per_second': 113.79, 'eval_steps_per_second': 7.176, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:52,  2.96s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:50,  1.81it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.52it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:36,  2.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:28,  2.31it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:27,  2.15it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:20,  2.44it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.63it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.93it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.57it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.81it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.68it/s]
Evaluation performance at step 50: 0.73
{'loss': 3.1146, 'grad_norm': 1.028651237487793, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 2.9032986164093018, 'eval_runtime': 8.7997, 'eval_samples_per_second': 113.526, 'eval_steps_per_second': 7.159, 'epoch': 0.08}
{'loss': 2.788, 'grad_norm': 1.2042670249938965, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.700446605682373, 'eval_runtime': 8.833, 'eval_samples_per_second': 113.099, 'eval_steps_per_second': 7.132, 'epoch': 0.12}
{'loss': 2.6972, 'grad_norm': 0.6674069166183472, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.5902435779571533, 'eval_runtime': 8.9621, 'eval_samples_per_second': 111.47, 'eval_steps_per_second': 7.03, 'epoch': 0.16}
{'loss': 2.5577, 'grad_norm': 0.6473725438117981, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.528268337249756, 'eval_runtime': 8.9903, 'eval_samples_per_second': 111.12, 'eval_steps_per_second': 7.008, 'epoch': 0.2}
{'loss': 2.4699, 'grad_norm': 0.9682870507240295, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.484395980834961, 'eval_runtime': 9.0211, 'eval_samples_per_second': 110.74, 'eval_steps_per_second': 6.984, 'epoch': 0.24}
{'loss': 2.4334, 'grad_norm': 0.564220130443573, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.4540414810180664, 'eval_runtime': 9.0149, 'eval_samples_per_second': 110.817, 'eval_steps_per_second': 6.988, 'epoch': 0.28}
{'loss': 2.3979, 'grad_norm': 0.8622013926506042, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.4248061180114746, 'eval_runtime': 8.9613, 'eval_samples_per_second': 111.479, 'eval_steps_per_second': 7.03, 'epoch': 0.32}
{'loss': 2.4243, 'grad_norm': 0.8150326013565063, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.40299129486084, 'eval_runtime': 9.0075, 'eval_samples_per_second': 110.908, 'eval_steps_per_second': 6.994, 'epoch': 0.36}
{'loss': 2.3696, 'grad_norm': 0.9861132502555847, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.3881704807281494, 'eval_runtime': 8.9483, 'eval_samples_per_second': 111.641, 'eval_steps_per_second': 7.04, 'epoch': 0.4}
{'loss': 2.4063, 'grad_norm': 0.6697239875793457, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.377295732498169, 'eval_runtime': 8.9226, 'eval_samples_per_second': 111.962, 'eval_steps_per_second': 7.061, 'epoch': 0.44}
{'loss': 2.3443, 'grad_norm': 0.6098265647888184, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.367568254470825, 'eval_runtime': 8.9349, 'eval_samples_per_second': 111.809, 'eval_steps_per_second': 7.051, 'epoch': 0.48}
{'loss': 2.3016, 'grad_norm': 0.7729542851448059, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.3610382080078125, 'eval_runtime': 8.9329, 'eval_samples_per_second': 111.833, 'eval_steps_per_second': 7.053, 'epoch': 0.52}
{'loss': 2.3378, 'grad_norm': 1.0135799646377563, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.355475664138794, 'eval_runtime': 8.943, 'eval_samples_per_second': 111.708, 'eval_steps_per_second': 7.045, 'epoch': 0.56}
{'loss': 2.2891, 'grad_norm': 0.7220778465270996, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.349396228790283, 'eval_runtime': 8.9458, 'eval_samples_per_second': 111.672, 'eval_steps_per_second': 7.042, 'epoch': 0.6}
{'loss': 2.3858, 'grad_norm': 0.7572351098060608, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.344464063644409, 'eval_runtime': 8.9426, 'eval_samples_per_second': 111.713, 'eval_steps_per_second': 7.045, 'epoch': 0.64}
{'loss': 2.3339, 'grad_norm': 0.6869096159934998, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.339580774307251, 'eval_runtime': 8.9354, 'eval_samples_per_second': 111.802, 'eval_steps_per_second': 7.051, 'epoch': 0.68}
{'loss': 2.3211, 'grad_norm': 0.7285689115524292, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.334845781326294, 'eval_runtime': 8.9477, 'eval_samples_per_second': 111.649, 'eval_steps_per_second': 7.041, 'epoch': 0.72}
{'loss': 2.2384, 'grad_norm': 0.599052369594574, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.3309736251831055, 'eval_runtime': 8.9327, 'eval_samples_per_second': 111.836, 'eval_steps_per_second': 7.053, 'epoch': 0.76}
{'loss': 2.3473, 'grad_norm': 0.7649881839752197, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.328519821166992, 'eval_runtime': 8.9421, 'eval_samples_per_second': 111.719, 'eval_steps_per_second': 7.045, 'epoch': 0.8}
{'loss': 2.3373, 'grad_norm': 0.7213177680969238, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.326059341430664, 'eval_runtime': 8.9465, 'eval_samples_per_second': 111.664, 'eval_steps_per_second': 7.042, 'epoch': 0.84}
{'loss': 2.3168, 'grad_norm': 0.7169949412345886, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.3230772018432617, 'eval_runtime': 8.9381, 'eval_samples_per_second': 111.768, 'eval_steps_per_second': 7.048, 'epoch': 0.88}
{'loss': 2.2921, 'grad_norm': 0.8516845107078552, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.3216497898101807, 'eval_runtime': 8.9783, 'eval_samples_per_second': 111.268, 'eval_steps_per_second': 7.017, 'epoch': 0.92}
{'loss': 2.3452, 'grad_norm': 0.6068012118339539, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.320760726928711, 'eval_runtime': 8.999, 'eval_samples_per_second': 111.012, 'eval_steps_per_second': 7.001, 'epoch': 0.96}
{'loss': 2.3095, 'grad_norm': 0.6483579277992249, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.3204097747802734, 'eval_runtime': 8.9429, 'eval_samples_per_second': 111.709, 'eval_steps_per_second': 7.045, 'epoch': 1.0}
{'train_runtime': 430.6225, 'train_samples_per_second': 23.215, 'train_steps_per_second': 1.451, 'train_loss': 2.470977655029297, 'epoch': 1.0}
train_results:  {'eval_loss': [3.419475555419922, 2.9032986164093018, 2.700446605682373, 2.5902435779571533, 2.528268337249756, 2.484395980834961, 2.4540414810180664, 2.4248061180114746, 2.40299129486084, 2.3881704807281494, 2.377295732498169, 2.367568254470825, 2.3610382080078125, 2.355475664138794, 2.349396228790283, 2.344464063644409, 2.339580774307251, 2.334845781326294, 2.3309736251831055, 2.328519821166992, 2.326059341430664, 2.3230772018432617, 2.3216497898101807, 2.320760726928711, 2.3204097747802734], 'performance': [0.77, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<14:26,  8.75s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:03,  1.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:38,  1.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:23,  2.13it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:35<00:16,  2.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:42<00:08,  2.13it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:44<00:01,  2.87it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:44<00:00,  2.24it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  1.2349300384521484
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0462 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.8425745232816182
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.7503161989667713
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.8433476366328219
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.8099831852443192
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.8448602414398737
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.6387, dtype=torch.float64), tensor(0.1801, dtype=torch.float64), 0, 0, tensor(0.1813, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(4.7290e-17, dtype=torch.float64), tensor(6.9645e-17, dtype=torch.float64), tensor(0.6387, dtype=torch.float64), tensor(0.1801, dtype=torch.float64), tensor(2.3519e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1813, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.639
  sciq: 0.18
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.181
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:54,  2.97s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:45,  2.00it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.64it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.06it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:23,  2.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:18,  2.75it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:13,  2.61it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:09,  2.90it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.46it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.1305, 'grad_norm': 3.3270304203033447, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 4.08920431137085, 'eval_runtime': 8.8027, 'eval_samples_per_second': 113.489, 'eval_steps_per_second': 7.157, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:00,  3.64s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:17,  1.17it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:44,  1.87it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:23,  2.83it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:24,  2.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.67it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.68it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:12,  2.90it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.68it/s]
Evaluation performance at step 50: 0.74
{'loss': 3.7947, 'grad_norm': 1.2204973697662354, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 3.574312210083008, 'eval_runtime': 8.8174, 'eval_samples_per_second': 113.299, 'eval_steps_per_second': 7.145, 'epoch': 0.08}
{'loss': 3.4127, 'grad_norm': 1.0030256509780884, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.3235013484954834, 'eval_runtime': 8.932, 'eval_samples_per_second': 111.845, 'eval_steps_per_second': 7.053, 'epoch': 0.12}
{'loss': 3.2726, 'grad_norm': 0.774286687374115, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.1937367916107178, 'eval_runtime': 8.9166, 'eval_samples_per_second': 112.039, 'eval_steps_per_second': 7.066, 'epoch': 0.16}
{'loss': 3.1873, 'grad_norm': 0.7727444767951965, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.130606174468994, 'eval_runtime': 8.9104, 'eval_samples_per_second': 112.116, 'eval_steps_per_second': 7.07, 'epoch': 0.2}
{'loss': 3.0776, 'grad_norm': 0.7118217945098877, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.076998233795166, 'eval_runtime': 8.9478, 'eval_samples_per_second': 111.647, 'eval_steps_per_second': 7.041, 'epoch': 0.24}
{'loss': 3.0597, 'grad_norm': 0.664774477481842, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.0388946533203125, 'eval_runtime': 8.9532, 'eval_samples_per_second': 111.58, 'eval_steps_per_second': 7.037, 'epoch': 0.28}
{'loss': 3.0231, 'grad_norm': 0.7167917490005493, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 3.013873815536499, 'eval_runtime': 8.9909, 'eval_samples_per_second': 111.113, 'eval_steps_per_second': 7.007, 'epoch': 0.32}
{'loss': 3.0215, 'grad_norm': 0.8816431164741516, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.994642734527588, 'eval_runtime': 8.9686, 'eval_samples_per_second': 111.389, 'eval_steps_per_second': 7.025, 'epoch': 0.36}
{'loss': 3.0105, 'grad_norm': 0.7349467277526855, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.9798922538757324, 'eval_runtime': 8.9599, 'eval_samples_per_second': 111.497, 'eval_steps_per_second': 7.031, 'epoch': 0.4}
{'loss': 2.9744, 'grad_norm': 0.9400972723960876, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.9670767784118652, 'eval_runtime': 8.9734, 'eval_samples_per_second': 111.329, 'eval_steps_per_second': 7.021, 'epoch': 0.44}
{'loss': 2.9115, 'grad_norm': 0.7430660128593445, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.957473039627075, 'eval_runtime': 8.9645, 'eval_samples_per_second': 111.44, 'eval_steps_per_second': 7.028, 'epoch': 0.48}
{'loss': 2.944, 'grad_norm': 0.8572676181793213, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.9489920139312744, 'eval_runtime': 8.9706, 'eval_samples_per_second': 111.364, 'eval_steps_per_second': 7.023, 'epoch': 0.52}
{'loss': 2.9377, 'grad_norm': 0.7272757291793823, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.9381871223449707, 'eval_runtime': 8.9668, 'eval_samples_per_second': 111.411, 'eval_steps_per_second': 7.026, 'epoch': 0.56}
{'loss': 2.9192, 'grad_norm': 0.9349038600921631, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.932314395904541, 'eval_runtime': 8.9683, 'eval_samples_per_second': 111.392, 'eval_steps_per_second': 7.025, 'epoch': 0.6}
{'loss': 2.8814, 'grad_norm': 1.020681381225586, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.9269680976867676, 'eval_runtime': 8.9711, 'eval_samples_per_second': 111.358, 'eval_steps_per_second': 7.023, 'epoch': 0.64}
{'loss': 2.9295, 'grad_norm': 0.645976185798645, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.9231789112091064, 'eval_runtime': 8.9714, 'eval_samples_per_second': 111.354, 'eval_steps_per_second': 7.022, 'epoch': 0.68}
{'loss': 2.8923, 'grad_norm': 0.8114489316940308, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.9202322959899902, 'eval_runtime': 8.9657, 'eval_samples_per_second': 111.425, 'eval_steps_per_second': 7.027, 'epoch': 0.72}
{'loss': 2.8929, 'grad_norm': 0.7621587514877319, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.91729474067688, 'eval_runtime': 8.9598, 'eval_samples_per_second': 111.498, 'eval_steps_per_second': 7.031, 'epoch': 0.76}
{'loss': 2.8926, 'grad_norm': 0.8328849077224731, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.9146275520324707, 'eval_runtime': 8.9631, 'eval_samples_per_second': 111.457, 'eval_steps_per_second': 7.029, 'epoch': 0.8}
{'loss': 2.9182, 'grad_norm': 0.6873639822006226, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.912689447402954, 'eval_runtime': 8.9674, 'eval_samples_per_second': 111.403, 'eval_steps_per_second': 7.025, 'epoch': 0.84}
{'loss': 2.9306, 'grad_norm': 0.9026613831520081, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.9112188816070557, 'eval_runtime': 8.9723, 'eval_samples_per_second': 111.343, 'eval_steps_per_second': 7.022, 'epoch': 0.88}
{'loss': 2.9228, 'grad_norm': 0.8480944037437439, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.909888982772827, 'eval_runtime': 8.949, 'eval_samples_per_second': 111.632, 'eval_steps_per_second': 7.04, 'epoch': 0.92}
{'loss': 2.8881, 'grad_norm': 0.7562754154205322, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.909553289413452, 'eval_runtime': 8.9697, 'eval_samples_per_second': 111.375, 'eval_steps_per_second': 7.024, 'epoch': 0.96}
{'loss': 2.9089, 'grad_norm': 1.0876272916793823, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.909090757369995, 'eval_runtime': 8.9699, 'eval_samples_per_second': 111.372, 'eval_steps_per_second': 7.023, 'epoch': 1.0}
{'train_runtime': 426.1307, 'train_samples_per_second': 23.462, 'train_steps_per_second': 1.467, 'train_loss': 3.0693704467773437, 'epoch': 1.0}
train_results:  {'eval_loss': [4.08920431137085, 3.574312210083008, 3.3235013484954834, 3.1937367916107178, 3.130606174468994, 3.076998233795166, 3.0388946533203125, 3.013873815536499, 2.994642734527588, 2.9798922538757324, 2.9670767784118652, 2.957473039627075, 2.9489920139312744, 2.9381871223449707, 2.932314395904541, 2.9269680976867676, 2.9231789112091064, 2.9202322959899902, 2.91729474067688, 2.9146275520324707, 2.912689447402954, 2.9112188816070557, 2.909888982772827, 2.909553289413452, 2.909090757369995], 'performance': [0.77, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:47,  9.57s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:25,  2.03it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:38<00:17,  2.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:45<00:09,  2.09it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:49<00:01,  2.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:49<00:00,  2.02it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  1.2354788780212402
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1639 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.6926233416028781
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.7950373081618062
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.8373238033019254
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.7387599013318233
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.841401607036655
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1842, dtype=torch.float64), tensor(0.1805, dtype=torch.float64), 0, 0, tensor(0.6353, dtype=torch.float64), 0, 0, 1, 1, 0, 1, 0, 0, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(7.7628e-17, dtype=torch.float64), tensor(2.5134e-16, dtype=torch.float64), tensor(0.1842, dtype=torch.float64), tensor(0.1805, dtype=torch.float64), tensor(8.9034e-17, dtype=torch.float64), tensor(6.5492e-16, dtype=torch.float64), tensor(0.6353, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.184
  sciq: 0.181
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.635
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:00,  3.03s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.24it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:43,  1.93it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:32,  2.33it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.64it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.66it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.82it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.81it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  2.99it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:07,  3.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.47it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.59it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.82it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.98it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.9424, 'grad_norm': 4.534243583679199, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.604306697845459, 'eval_runtime': 8.4742, 'eval_samples_per_second': 117.887, 'eval_steps_per_second': 7.434, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:07,  4.32s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:22,  1.10it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:44,  1.86it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:30,  2.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.67it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.66it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:22,  2.26it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:18,  2.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:13,  2.66it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:08,  3.02it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.51it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:37<00:04,  2.73it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.56it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.093, 'grad_norm': 3.634321928024292, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 2.732699155807495, 'eval_runtime': 8.4536, 'eval_samples_per_second': 118.174, 'eval_steps_per_second': 7.452, 'epoch': 0.08}
{'loss': 2.5636, 'grad_norm': 2.878810167312622, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.4017670154571533, 'eval_runtime': 8.5018, 'eval_samples_per_second': 117.505, 'eval_steps_per_second': 7.41, 'epoch': 0.12}
{'loss': 2.3829, 'grad_norm': 2.6243298053741455, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.30600643157959, 'eval_runtime': 8.5059, 'eval_samples_per_second': 117.448, 'eval_steps_per_second': 7.407, 'epoch': 0.16}
{'loss': 2.2951, 'grad_norm': 4.147176742553711, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.251619577407837, 'eval_runtime': 8.5241, 'eval_samples_per_second': 117.197, 'eval_steps_per_second': 7.391, 'epoch': 0.2}
{'loss': 2.2384, 'grad_norm': 3.530026435852051, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.193915367126465, 'eval_runtime': 8.5233, 'eval_samples_per_second': 117.208, 'eval_steps_per_second': 7.392, 'epoch': 0.24}
{'loss': 2.2066, 'grad_norm': 2.4093246459960938, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.15901780128479, 'eval_runtime': 8.5526, 'eval_samples_per_second': 116.806, 'eval_steps_per_second': 7.366, 'epoch': 0.28}
{'loss': 2.1464, 'grad_norm': 3.8714849948883057, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.134500741958618, 'eval_runtime': 8.5294, 'eval_samples_per_second': 117.125, 'eval_steps_per_second': 7.386, 'epoch': 0.32}
{'loss': 2.132, 'grad_norm': 3.607285737991333, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.112121820449829, 'eval_runtime': 8.5265, 'eval_samples_per_second': 117.164, 'eval_steps_per_second': 7.389, 'epoch': 0.36}
{'loss': 2.1042, 'grad_norm': 2.662567138671875, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.101707935333252, 'eval_runtime': 8.5313, 'eval_samples_per_second': 117.099, 'eval_steps_per_second': 7.385, 'epoch': 0.4}
{'loss': 2.1477, 'grad_norm': 2.794780731201172, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0946335792541504, 'eval_runtime': 8.5422, 'eval_samples_per_second': 116.948, 'eval_steps_per_second': 7.375, 'epoch': 0.44}
{'loss': 2.1165, 'grad_norm': 2.5209012031555176, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.085585594177246, 'eval_runtime': 8.5455, 'eval_samples_per_second': 116.904, 'eval_steps_per_second': 7.372, 'epoch': 0.48}
{'loss': 2.1281, 'grad_norm': 3.3199591636657715, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.080533981323242, 'eval_runtime': 8.5613, 'eval_samples_per_second': 116.687, 'eval_steps_per_second': 7.359, 'epoch': 0.52}
{'loss': 2.088, 'grad_norm': 3.203754186630249, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.0762856006622314, 'eval_runtime': 8.5407, 'eval_samples_per_second': 116.969, 'eval_steps_per_second': 7.376, 'epoch': 0.56}
{'loss': 2.0679, 'grad_norm': 2.643507719039917, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.0727899074554443, 'eval_runtime': 8.5426, 'eval_samples_per_second': 116.944, 'eval_steps_per_second': 7.375, 'epoch': 0.6}
{'loss': 2.1369, 'grad_norm': 2.8163294792175293, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.0687732696533203, 'eval_runtime': 8.5433, 'eval_samples_per_second': 116.934, 'eval_steps_per_second': 7.374, 'epoch': 0.64}
{'loss': 2.1404, 'grad_norm': 2.7500202655792236, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.066028594970703, 'eval_runtime': 8.5406, 'eval_samples_per_second': 116.971, 'eval_steps_per_second': 7.377, 'epoch': 0.68}
{'loss': 2.0847, 'grad_norm': 2.9038197994232178, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.0631260871887207, 'eval_runtime': 8.5309, 'eval_samples_per_second': 117.104, 'eval_steps_per_second': 7.385, 'epoch': 0.72}
{'loss': 2.1337, 'grad_norm': 2.029773712158203, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.061070203781128, 'eval_runtime': 8.5414, 'eval_samples_per_second': 116.96, 'eval_steps_per_second': 7.376, 'epoch': 0.76}
{'loss': 2.1298, 'grad_norm': 2.4086151123046875, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.059718370437622, 'eval_runtime': 8.5357, 'eval_samples_per_second': 117.038, 'eval_steps_per_second': 7.381, 'epoch': 0.8}
{'loss': 2.1322, 'grad_norm': 2.89141845703125, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.057555675506592, 'eval_runtime': 8.5444, 'eval_samples_per_second': 116.918, 'eval_steps_per_second': 7.373, 'epoch': 0.84}
{'loss': 2.1112, 'grad_norm': 3.2664737701416016, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0557916164398193, 'eval_runtime': 8.542, 'eval_samples_per_second': 116.951, 'eval_steps_per_second': 7.375, 'epoch': 0.88}
{'loss': 2.0997, 'grad_norm': 2.3051722049713135, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0546789169311523, 'eval_runtime': 8.5568, 'eval_samples_per_second': 116.749, 'eval_steps_per_second': 7.363, 'epoch': 0.92}
{'loss': 2.1078, 'grad_norm': 2.5602328777313232, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0541725158691406, 'eval_runtime': 8.5505, 'eval_samples_per_second': 116.835, 'eval_steps_per_second': 7.368, 'epoch': 0.96}
{'loss': 2.0682, 'grad_norm': 3.11283802986145, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.05403733253479, 'eval_runtime': 8.548, 'eval_samples_per_second': 116.87, 'eval_steps_per_second': 7.37, 'epoch': 1.0}
{'train_runtime': 410.6326, 'train_samples_per_second': 24.348, 'train_steps_per_second': 1.522, 'train_loss': 2.271906390380859, 'epoch': 1.0}
train_results:  {'eval_loss': [3.604306697845459, 2.732699155807495, 2.4017670154571533, 2.30600643157959, 2.251619577407837, 2.193915367126465, 2.15901780128479, 2.134500741958618, 2.112121820449829, 2.101707935333252, 2.0946335792541504, 2.085585594177246, 2.080533981323242, 2.0762856006622314, 2.0727899074554443, 2.0687732696533203, 2.066028594970703, 2.0631260871887207, 2.061070203781128, 2.059718370437622, 2.057555675506592, 2.0557916164398193, 2.0546789169311523, 2.0541725158691406, 2.05403733253479], 'performance': [0.77, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:53,  9.63s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.87it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2364070415496826
current iteration best possible performance (full train run):  0.0
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6383 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 0.8420156484610224
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.8425463519348756
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 0.8255406394151283
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.8425444100637932
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.8080037314351342
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.6792, dtype=torch.float64), tensor(0.1774, dtype=torch.float64), 0, 0, tensor(0.1434, dtype=torch.float64), 0, 0, 1, 1, 0, 1, 0, 1, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(2.5805e-17, dtype=torch.float64), tensor(1.1578e-17, dtype=torch.float64), tensor(0.6792, dtype=torch.float64), tensor(0.1774, dtype=torch.float64), tensor(2.2240e-17, dtype=torch.float64), tensor(5.2081e-18, dtype=torch.float64), tensor(0.1434, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7978e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.679
  sciq: 0.177
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.143
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 90,112 || all params: 8,030,351,360 || trainable%: 0.0011
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:09,  4.34s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:23,  1.09it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.77it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.29it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:27,  2.41it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:21<00:28,  2.10it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:24<00:23,  2.20it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:27<00:18,  2.35it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:30<00:13,  2.62it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:31<00:08,  3.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:36<00:07,  2.52it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:38<00:03,  2.79it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:00,  3.09it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.48it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.0222, 'grad_norm': 5.143592357635498, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.47650146484375, 'eval_runtime': 8.9468, 'eval_samples_per_second': 111.66, 'eval_steps_per_second': 7.042, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:49,  4.14s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:21,  1.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.80it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.50it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.66it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:17,  2.85it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:18,  2.37it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:13,  2.67it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:06,  3.13it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  3.22it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.42it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.72it/s]
Evaluation performance at step 50: 0.72
{'loss': 2.9667, 'grad_norm': 2.866382598876953, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.72}
{'eval_loss': 2.471679210662842, 'eval_runtime': 8.882, 'eval_samples_per_second': 112.475, 'eval_steps_per_second': 7.093, 'epoch': 0.08}
{'loss': 2.3018, 'grad_norm': 1.9002047777175903, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.185375690460205, 'eval_runtime': 8.9516, 'eval_samples_per_second': 111.6, 'eval_steps_per_second': 7.038, 'epoch': 0.12}
{'loss': 2.1089, 'grad_norm': 2.1375558376312256, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0818021297454834, 'eval_runtime': 8.9738, 'eval_samples_per_second': 111.324, 'eval_steps_per_second': 7.02, 'epoch': 0.16}
{'loss': 2.0539, 'grad_norm': 1.6900441646575928, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.045053243637085, 'eval_runtime': 8.9964, 'eval_samples_per_second': 111.044, 'eval_steps_per_second': 7.003, 'epoch': 0.2}
{'loss': 2.0185, 'grad_norm': 1.888395071029663, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.026710271835327, 'eval_runtime': 9.0003, 'eval_samples_per_second': 110.996, 'eval_steps_per_second': 7.0, 'epoch': 0.24}
{'loss': 2.0297, 'grad_norm': 1.3799787759780884, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.0138494968414307, 'eval_runtime': 9.023, 'eval_samples_per_second': 110.717, 'eval_steps_per_second': 6.982, 'epoch': 0.28}
{'loss': 2.0032, 'grad_norm': 1.648139476776123, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.0036590099334717, 'eval_runtime': 9.0331, 'eval_samples_per_second': 110.593, 'eval_steps_per_second': 6.974, 'epoch': 0.32}
{'loss': 1.9976, 'grad_norm': 1.7873387336730957, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9954009056091309, 'eval_runtime': 9.014, 'eval_samples_per_second': 110.828, 'eval_steps_per_second': 6.989, 'epoch': 0.36}
{'loss': 1.9741, 'grad_norm': 1.627553105354309, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.988348364830017, 'eval_runtime': 9.004, 'eval_samples_per_second': 110.95, 'eval_steps_per_second': 6.997, 'epoch': 0.4}
{'loss': 1.9633, 'grad_norm': 1.676131010055542, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9821642637252808, 'eval_runtime': 9.0169, 'eval_samples_per_second': 110.792, 'eval_steps_per_second': 6.987, 'epoch': 0.44}
{'loss': 1.9541, 'grad_norm': 1.580310583114624, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9763391017913818, 'eval_runtime': 9.0057, 'eval_samples_per_second': 110.93, 'eval_steps_per_second': 6.996, 'epoch': 0.48}
{'loss': 1.9728, 'grad_norm': 1.8257912397384644, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9712307453155518, 'eval_runtime': 9.0107, 'eval_samples_per_second': 110.869, 'eval_steps_per_second': 6.992, 'epoch': 0.52}
{'loss': 1.9274, 'grad_norm': 1.500281810760498, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9675555229187012, 'eval_runtime': 8.9841, 'eval_samples_per_second': 111.196, 'eval_steps_per_second': 7.012, 'epoch': 0.56}
{'loss': 1.9658, 'grad_norm': 1.9034475088119507, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9660991430282593, 'eval_runtime': 9.0031, 'eval_samples_per_second': 110.962, 'eval_steps_per_second': 6.998, 'epoch': 0.6}
{'loss': 1.9788, 'grad_norm': 1.8522682189941406, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.961236834526062, 'eval_runtime': 9.0062, 'eval_samples_per_second': 110.924, 'eval_steps_per_second': 6.995, 'epoch': 0.64}
{'loss': 1.9509, 'grad_norm': 1.5675314664840698, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.958702564239502, 'eval_runtime': 9.0045, 'eval_samples_per_second': 110.944, 'eval_steps_per_second': 6.996, 'epoch': 0.68}
{'loss': 1.9478, 'grad_norm': 1.7527613639831543, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9571796655654907, 'eval_runtime': 9.007, 'eval_samples_per_second': 110.914, 'eval_steps_per_second': 6.995, 'epoch': 0.72}
{'loss': 1.9742, 'grad_norm': 1.8580483198165894, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9540892839431763, 'eval_runtime': 9.0101, 'eval_samples_per_second': 110.876, 'eval_steps_per_second': 6.992, 'epoch': 0.76}
{'loss': 1.9464, 'grad_norm': 1.8326468467712402, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9536412954330444, 'eval_runtime': 9.0084, 'eval_samples_per_second': 110.896, 'eval_steps_per_second': 6.993, 'epoch': 0.8}
{'loss': 1.9989, 'grad_norm': 1.483208179473877, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9517982006072998, 'eval_runtime': 8.9964, 'eval_samples_per_second': 111.044, 'eval_steps_per_second': 7.003, 'epoch': 0.84}
{'loss': 1.9713, 'grad_norm': 1.908583402633667, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9501545429229736, 'eval_runtime': 8.9914, 'eval_samples_per_second': 111.107, 'eval_steps_per_second': 7.007, 'epoch': 0.88}
{'loss': 1.9291, 'grad_norm': 1.8800740242004395, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.949562668800354, 'eval_runtime': 8.997, 'eval_samples_per_second': 111.037, 'eval_steps_per_second': 7.002, 'epoch': 0.92}
{'loss': 1.9425, 'grad_norm': 1.4355332851409912, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9488391876220703, 'eval_runtime': 9.0152, 'eval_samples_per_second': 110.812, 'eval_steps_per_second': 6.988, 'epoch': 0.96}
{'loss': 1.9777, 'grad_norm': 1.9996395111083984, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9484807252883911, 'eval_runtime': 9.0091, 'eval_samples_per_second': 110.887, 'eval_steps_per_second': 6.993, 'epoch': 1.0}
{'train_runtime': 435.9882, 'train_samples_per_second': 22.932, 'train_steps_per_second': 1.434, 'train_loss': 2.115102136230469, 'epoch': 1.0}
train_results:  {'eval_loss': [3.47650146484375, 2.471679210662842, 2.185375690460205, 2.0818021297454834, 2.045053243637085, 2.026710271835327, 2.0138494968414307, 2.0036590099334717, 1.9954009056091309, 1.988348364830017, 1.9821642637252808, 1.9763391017913818, 1.9712307453155518, 1.9675555229187012, 1.9660991430282593, 1.961236834526062, 1.958702564239502, 1.9571796655654907, 1.9540892839431763, 1.9536412954330444, 1.9517982006072998, 1.9501545429229736, 1.949562668800354, 1.9488391876220703, 1.9484807252883911], 'performance': [0.76, 0.72]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:53,  9.63s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.80it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.86it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.72]
current iteration observed (possibly low-fid or predicted) performance:  1.221704125404358
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826, 1.221704125404358]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2959 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 0.7828788371682163
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.823549920671683
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 0.8385471676190247
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.8261463618532989
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.8366332188580456
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1827, dtype=torch.float64), 0, tensor(0.7917, dtype=torch.float64), tensor(0.0256, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(6.9752e-19, dtype=torch.float64), tensor(1.7164e-16, dtype=torch.float64), tensor(0.1827, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7917, dtype=torch.float64), tensor(0.0256, dtype=torch.float64), tensor(6.3002e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.183
  triviaqa: 0
  truthfulqa_gen: 0.792
  wikitext: 0.026
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:34,  2.77s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:11,  1.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.01it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.56it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.73it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.86it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.18it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:09,  2.98it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.50it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.82it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.18it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.78it/s]
Evaluation performance at step 25: 0.75
{'loss': 5.3336, 'grad_norm': 6.961485862731934, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 5.057833194732666, 'eval_runtime': 5.5159, 'eval_samples_per_second': 181.111, 'eval_steps_per_second': 11.421, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:47,  3.51s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:58,  1.55it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:37,  2.24it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:27,  2.75it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:24,  2.78it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:20,  2.89it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.82it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.17it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:07,  3.47it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:06,  2.72it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.90it/s]
Evaluation performance at step 50: 0.76
{'loss': 4.3862, 'grad_norm': 6.441737174987793, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 3.847189426422119, 'eval_runtime': 3.106, 'eval_samples_per_second': 321.638, 'eval_steps_per_second': 20.283, 'epoch': 0.08}
{'loss': 3.5417, 'grad_norm': 2.1520626544952393, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.174903392791748, 'eval_runtime': 3.1082, 'eval_samples_per_second': 321.409, 'eval_steps_per_second': 20.269, 'epoch': 0.12}
{'loss': 3.0151, 'grad_norm': 1.7295284271240234, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.891648530960083, 'eval_runtime': 3.1344, 'eval_samples_per_second': 318.726, 'eval_steps_per_second': 20.1, 'epoch': 0.16}
{'loss': 2.7843, 'grad_norm': 1.434949278831482, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.749945640563965, 'eval_runtime': 3.107, 'eval_samples_per_second': 321.536, 'eval_steps_per_second': 20.277, 'epoch': 0.2}
{'loss': 2.693, 'grad_norm': 1.029861569404602, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.6771600246429443, 'eval_runtime': 3.1118, 'eval_samples_per_second': 321.038, 'eval_steps_per_second': 20.246, 'epoch': 0.24}
{'loss': 2.656, 'grad_norm': 0.984039306640625, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.6225502490997314, 'eval_runtime': 3.1214, 'eval_samples_per_second': 320.045, 'eval_steps_per_second': 20.183, 'epoch': 0.28}
{'loss': 2.6004, 'grad_norm': 1.0645921230316162, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.584172248840332, 'eval_runtime': 3.123, 'eval_samples_per_second': 319.881, 'eval_steps_per_second': 20.173, 'epoch': 0.32}
{'loss': 2.5599, 'grad_norm': 0.8256577253341675, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.5496087074279785, 'eval_runtime': 3.1264, 'eval_samples_per_second': 319.54, 'eval_steps_per_second': 20.151, 'epoch': 0.36}
{'loss': 2.5276, 'grad_norm': 0.7949996590614319, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.5363681316375732, 'eval_runtime': 3.124, 'eval_samples_per_second': 319.783, 'eval_steps_per_second': 20.167, 'epoch': 0.4}
{'loss': 2.5345, 'grad_norm': 0.7580326199531555, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.5262601375579834, 'eval_runtime': 3.1251, 'eval_samples_per_second': 319.673, 'eval_steps_per_second': 20.16, 'epoch': 0.44}
{'loss': 2.524, 'grad_norm': 1.0297911167144775, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.519256114959717, 'eval_runtime': 3.1281, 'eval_samples_per_second': 319.365, 'eval_steps_per_second': 20.14, 'epoch': 0.48}
{'loss': 2.5227, 'grad_norm': 0.9049617648124695, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.500218391418457, 'eval_runtime': 3.1352, 'eval_samples_per_second': 318.637, 'eval_steps_per_second': 20.094, 'epoch': 0.52}
{'loss': 2.5017, 'grad_norm': 1.032503604888916, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.489237070083618, 'eval_runtime': 3.1328, 'eval_samples_per_second': 318.885, 'eval_steps_per_second': 20.11, 'epoch': 0.56}
{'loss': 2.4912, 'grad_norm': 0.8685792088508606, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.4609460830688477, 'eval_runtime': 3.13, 'eval_samples_per_second': 319.174, 'eval_steps_per_second': 20.128, 'epoch': 0.6}
{'loss': 2.4645, 'grad_norm': 0.7413918972015381, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.449758768081665, 'eval_runtime': 3.1289, 'eval_samples_per_second': 319.286, 'eval_steps_per_second': 20.135, 'epoch': 0.64}
{'loss': 2.456, 'grad_norm': 0.9306073784828186, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.4441869258880615, 'eval_runtime': 3.1274, 'eval_samples_per_second': 319.439, 'eval_steps_per_second': 20.145, 'epoch': 0.68}
{'loss': 2.4608, 'grad_norm': 0.802575409412384, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.4402096271514893, 'eval_runtime': 3.1344, 'eval_samples_per_second': 318.722, 'eval_steps_per_second': 20.1, 'epoch': 0.72}
{'loss': 2.4578, 'grad_norm': 0.9150421619415283, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.4296376705169678, 'eval_runtime': 3.13, 'eval_samples_per_second': 319.174, 'eval_steps_per_second': 20.128, 'epoch': 0.76}
{'loss': 2.4435, 'grad_norm': 0.7885288000106812, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.4262444972991943, 'eval_runtime': 3.132, 'eval_samples_per_second': 318.963, 'eval_steps_per_second': 20.115, 'epoch': 0.8}
{'loss': 2.4499, 'grad_norm': 0.8908870816230774, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.424520969390869, 'eval_runtime': 3.1358, 'eval_samples_per_second': 318.582, 'eval_steps_per_second': 20.091, 'epoch': 0.84}
{'loss': 2.4475, 'grad_norm': 0.7268886566162109, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.4217848777770996, 'eval_runtime': 3.134, 'eval_samples_per_second': 318.766, 'eval_steps_per_second': 20.102, 'epoch': 0.88}
{'loss': 2.4595, 'grad_norm': 0.8503543734550476, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.419240713119507, 'eval_runtime': 3.1384, 'eval_samples_per_second': 318.313, 'eval_steps_per_second': 20.074, 'epoch': 0.92}
{'loss': 2.4288, 'grad_norm': 0.8217623829841614, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.4178450107574463, 'eval_runtime': 3.1381, 'eval_samples_per_second': 318.35, 'eval_steps_per_second': 20.076, 'epoch': 0.96}
{'loss': 2.42, 'grad_norm': 0.8298429250717163, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.4176676273345947, 'eval_runtime': 3.1368, 'eval_samples_per_second': 318.474, 'eval_steps_per_second': 20.084, 'epoch': 1.0}
{'train_runtime': 226.3868, 'train_samples_per_second': 44.168, 'train_steps_per_second': 2.761, 'train_loss': 2.766394573974609, 'epoch': 1.0}
train_results:  {'eval_loss': [5.057833194732666, 3.847189426422119, 3.174903392791748, 2.891648530960083, 2.749945640563965, 2.6771600246429443, 2.6225502490997314, 2.584172248840332, 2.5496087074279785, 2.5363681316375732, 2.5262601375579834, 2.519256114959717, 2.500218391418457, 2.489237070083618, 2.4609460830688477, 2.449758768081665, 2.4441869258880615, 2.4402096271514893, 2.4296376705169678, 2.4262444972991943, 2.424520969390869, 2.4217848777770996, 2.419240713119507, 2.4178450107574463, 2.4176676273345947], 'performance': [0.75, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:07<12:28,  7.57s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:13<00:53,  1.54it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:21<00:38,  1.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:26<00:22,  2.23it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:30<00:13,  2.63it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:06,  3.03it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.90it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.76it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2350049018859863
current iteration best possible performance (full train run):  0.735
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826, 1.221704125404358, 1.2350049018859863]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.2164 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.812049858422076
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.8319445906981925
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.7252857153110815
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.7445982242793097
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.8307479141203807
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1822, dtype=torch.float64), 0, 0, tensor(0.8178, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(8.2745e-17, dtype=torch.float64), tensor(6.1262e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1822, dtype=torch.float64), tensor(5.4527e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8178, dtype=torch.float64), tensor(1.0085e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.182
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.818
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:34,  2.77s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:44,  2.03it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.63it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.06it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:25,  2.66it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:26,  2.23it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:20,  2.49it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:16,  2.57it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:12,  2.86it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.11it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:06,  3.11it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.34it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.95it/s]
Evaluation performance at step 25: 0.76
{'loss': 3.563, 'grad_norm': 1.9844516515731812, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.5547609329223633, 'eval_runtime': 7.8521, 'eval_samples_per_second': 127.227, 'eval_steps_per_second': 8.023, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:26,  3.90s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:19,  1.14it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:45,  1.82it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.28it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.67it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:24,  2.37it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:20,  2.55it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.58it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:13,  2.65it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:08,  3.02it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.51it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:03,  2.80it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.59it/s]
Evaluation performance at step 50: 0.75
{'loss': 3.3137, 'grad_norm': 0.8278383016586304, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 3.2536399364471436, 'eval_runtime': 7.86, 'eval_samples_per_second': 127.099, 'eval_steps_per_second': 8.015, 'epoch': 0.08}
{'loss': 3.179, 'grad_norm': 0.7049570083618164, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.139836311340332, 'eval_runtime': 7.863, 'eval_samples_per_second': 127.051, 'eval_steps_per_second': 8.012, 'epoch': 0.12}
{'loss': 3.1106, 'grad_norm': 0.9778643250465393, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.0818867683410645, 'eval_runtime': 7.8899, 'eval_samples_per_second': 126.617, 'eval_steps_per_second': 7.985, 'epoch': 0.16}
{'loss': 3.0491, 'grad_norm': 0.6644728779792786, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.044696569442749, 'eval_runtime': 7.9147, 'eval_samples_per_second': 126.22, 'eval_steps_per_second': 7.96, 'epoch': 0.2}
{'loss': 3.0717, 'grad_norm': 0.9626627564430237, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.0223820209503174, 'eval_runtime': 7.9315, 'eval_samples_per_second': 125.953, 'eval_steps_per_second': 7.943, 'epoch': 0.24}
{'loss': 3.1434, 'grad_norm': 1.030580997467041, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.0063424110412598, 'eval_runtime': 7.9403, 'eval_samples_per_second': 125.814, 'eval_steps_per_second': 7.934, 'epoch': 0.28}
{'loss': 2.9984, 'grad_norm': 0.8246394991874695, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.9957103729248047, 'eval_runtime': 7.9357, 'eval_samples_per_second': 125.886, 'eval_steps_per_second': 7.939, 'epoch': 0.32}
{'loss': 3.1382, 'grad_norm': 1.0026826858520508, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.9837582111358643, 'eval_runtime': 7.9376, 'eval_samples_per_second': 125.857, 'eval_steps_per_second': 7.937, 'epoch': 0.36}
{'loss': 2.9847, 'grad_norm': 0.712004542350769, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.9744951725006104, 'eval_runtime': 7.9559, 'eval_samples_per_second': 125.567, 'eval_steps_per_second': 7.919, 'epoch': 0.4}
{'loss': 3.0113, 'grad_norm': 0.9064815640449524, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.9646453857421875, 'eval_runtime': 7.9451, 'eval_samples_per_second': 125.738, 'eval_steps_per_second': 7.929, 'epoch': 0.44}
{'loss': 2.9558, 'grad_norm': 0.6857349872589111, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.9553849697113037, 'eval_runtime': 7.9546, 'eval_samples_per_second': 125.588, 'eval_steps_per_second': 7.92, 'epoch': 0.48}
{'loss': 2.96, 'grad_norm': 0.9485573172569275, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.9494009017944336, 'eval_runtime': 7.9577, 'eval_samples_per_second': 125.539, 'eval_steps_per_second': 7.917, 'epoch': 0.52}
{'loss': 2.9416, 'grad_norm': 0.8671920895576477, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.9450879096984863, 'eval_runtime': 7.9469, 'eval_samples_per_second': 125.71, 'eval_steps_per_second': 7.928, 'epoch': 0.56}
{'loss': 2.9903, 'grad_norm': 1.128528118133545, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.9414634704589844, 'eval_runtime': 7.9657, 'eval_samples_per_second': 125.413, 'eval_steps_per_second': 7.909, 'epoch': 0.6}
{'loss': 3.028, 'grad_norm': 0.8142728805541992, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.9384446144104004, 'eval_runtime': 7.9387, 'eval_samples_per_second': 125.84, 'eval_steps_per_second': 7.936, 'epoch': 0.64}
{'loss': 2.9089, 'grad_norm': 1.2735337018966675, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.9346420764923096, 'eval_runtime': 7.9649, 'eval_samples_per_second': 125.426, 'eval_steps_per_second': 7.91, 'epoch': 0.68}
{'loss': 2.9261, 'grad_norm': 0.9397421479225159, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.931947708129883, 'eval_runtime': 7.9632, 'eval_samples_per_second': 125.452, 'eval_steps_per_second': 7.911, 'epoch': 0.72}
{'loss': 2.9438, 'grad_norm': 0.7694407105445862, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.9304416179656982, 'eval_runtime': 7.9452, 'eval_samples_per_second': 125.737, 'eval_steps_per_second': 7.929, 'epoch': 0.76}
{'loss': 2.8529, 'grad_norm': 0.8125625252723694, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.9282145500183105, 'eval_runtime': 7.9565, 'eval_samples_per_second': 125.558, 'eval_steps_per_second': 7.918, 'epoch': 0.8}
{'loss': 2.9467, 'grad_norm': 1.0001842975616455, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.9264285564422607, 'eval_runtime': 7.9322, 'eval_samples_per_second': 125.942, 'eval_steps_per_second': 7.942, 'epoch': 0.84}
{'loss': 2.9122, 'grad_norm': 0.8155283331871033, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.924865245819092, 'eval_runtime': 7.9465, 'eval_samples_per_second': 125.715, 'eval_steps_per_second': 7.928, 'epoch': 0.88}
{'loss': 2.9401, 'grad_norm': 1.0635963678359985, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.9240376949310303, 'eval_runtime': 7.9553, 'eval_samples_per_second': 125.577, 'eval_steps_per_second': 7.919, 'epoch': 0.92}
{'loss': 2.8762, 'grad_norm': 0.615601658821106, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.9236907958984375, 'eval_runtime': 7.9269, 'eval_samples_per_second': 126.027, 'eval_steps_per_second': 7.948, 'epoch': 0.96}
{'loss': 2.9543, 'grad_norm': 0.7126957774162292, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.9236483573913574, 'eval_runtime': 7.9536, 'eval_samples_per_second': 125.604, 'eval_steps_per_second': 7.921, 'epoch': 1.0}
{'train_runtime': 389.9877, 'train_samples_per_second': 25.639, 'train_steps_per_second': 1.603, 'train_loss': 3.028005859375, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5547609329223633, 3.2536399364471436, 3.139836311340332, 3.0818867683410645, 3.044696569442749, 3.0223820209503174, 3.0063424110412598, 2.9957103729248047, 2.9837582111358643, 2.9744951725006104, 2.9646453857421875, 2.9553849697113037, 2.9494009017944336, 2.9450879096984863, 2.9414634704589844, 2.9384446144104004, 2.9346420764923096, 2.931947708129883, 2.9304416179656982, 2.9282145500183105, 2.9264285564422607, 2.924865245819092, 2.9240376949310303, 2.9236907958984375, 2.9236483573913574], 'performance': [0.76, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<11:22,  6.90s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:52,  1.58it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:20<00:36,  1.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:24<00:21,  2.43it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:30<00:13,  2.56it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:07,  2.39it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.52it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  1.236490249633789
current iteration best possible performance (full train run):  0.7244999999999999
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826, 1.221704125404358, 1.2350049018859863, 1.236490249633789]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1742 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.7877538744467494
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.8303919849453779
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.8308492986375672
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.8155876283247219
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.708044016314967
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0518, dtype=torch.float64), tensor(0.1828, dtype=torch.float64), tensor(0.3006, dtype=torch.float64), 0, tensor(0.4649, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 4.2329675719960756e-18, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(1.3157e-16, dtype=torch.float64), tensor(5.3050e-17, dtype=torch.float64), tensor(0.0518, dtype=torch.float64), tensor(0.1828, dtype=torch.float64), tensor(0.3006, dtype=torch.float64), tensor(8.6719e-06, dtype=torch.float64), tensor(0.4649, dtype=torch.float64), tensor(5.5882e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(4.2330e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.052
  sciq: 0.183
  triviaqa: 0.301
  truthfulqa_gen: 0
  wikitext: 0.465
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (4.2329675719960756e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  4.2329675719960756e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:55,  2.98s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:48,  1.87it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.54it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:25,  2.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:23,  2.90it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.97it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  3.02it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:11,  3.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:07,  3.43it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.71it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.00it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.34it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.02it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.3655, 'grad_norm': 3.1883697509765625, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 4.265041351318359, 'eval_runtime': 7.6116, 'eval_samples_per_second': 131.247, 'eval_steps_per_second': 8.277, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:41,  5.27s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:29,  1.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:49,  1.67it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:15<00:35,  2.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:17<00:26,  2.53it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:21,  2.73it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:17,  2.84it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:15,  2.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:29<00:13,  2.55it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:31<00:09,  2.85it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:35<00:07,  2.43it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:38<00:04,  2.51it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:01,  2.90it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.48it/s]
Evaluation performance at step 50: 0.71
{'loss': 3.9095, 'grad_norm': 2.0717201232910156, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.71}
{'eval_loss': 3.5877857208251953, 'eval_runtime': 7.5984, 'eval_samples_per_second': 131.476, 'eval_steps_per_second': 8.291, 'epoch': 0.08}
{'loss': 3.4258, 'grad_norm': 0.7906321287155151, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.3788411617279053, 'eval_runtime': 7.6521, 'eval_samples_per_second': 130.552, 'eval_steps_per_second': 8.233, 'epoch': 0.12}
{'loss': 3.2955, 'grad_norm': 1.0362014770507812, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.2724757194519043, 'eval_runtime': 7.6457, 'eval_samples_per_second': 130.662, 'eval_steps_per_second': 8.24, 'epoch': 0.16}
{'loss': 3.2236, 'grad_norm': 0.7111498713493347, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.206984043121338, 'eval_runtime': 7.669, 'eval_samples_per_second': 130.264, 'eval_steps_per_second': 8.215, 'epoch': 0.2}
{'loss': 3.186, 'grad_norm': 0.680941641330719, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.162785530090332, 'eval_runtime': 7.6747, 'eval_samples_per_second': 130.168, 'eval_steps_per_second': 8.209, 'epoch': 0.24}
{'loss': 3.1743, 'grad_norm': 1.0557249784469604, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.130906820297241, 'eval_runtime': 7.7381, 'eval_samples_per_second': 129.102, 'eval_steps_per_second': 8.142, 'epoch': 0.28}
{'loss': 3.103, 'grad_norm': 0.8794417381286621, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 3.104414939880371, 'eval_runtime': 7.7319, 'eval_samples_per_second': 129.204, 'eval_steps_per_second': 8.148, 'epoch': 0.32}
{'loss': 3.1082, 'grad_norm': 0.7599307298660278, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 3.0894904136657715, 'eval_runtime': 7.7493, 'eval_samples_per_second': 128.915, 'eval_steps_per_second': 8.13, 'epoch': 0.36}
{'loss': 3.011, 'grad_norm': 0.8744343519210815, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 3.079895257949829, 'eval_runtime': 7.7288, 'eval_samples_per_second': 129.257, 'eval_steps_per_second': 8.151, 'epoch': 0.4}
{'loss': 3.089, 'grad_norm': 1.0271764993667603, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 3.069216012954712, 'eval_runtime': 7.7112, 'eval_samples_per_second': 129.551, 'eval_steps_per_second': 8.17, 'epoch': 0.44}
{'loss': 2.9966, 'grad_norm': 0.6691386699676514, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 3.05751633644104, 'eval_runtime': 7.7258, 'eval_samples_per_second': 129.307, 'eval_steps_per_second': 8.154, 'epoch': 0.48}
{'loss': 3.0924, 'grad_norm': 0.5414462685585022, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 3.048872232437134, 'eval_runtime': 7.7208, 'eval_samples_per_second': 129.39, 'eval_steps_per_second': 8.16, 'epoch': 0.52}
{'loss': 3.0197, 'grad_norm': 0.8181372880935669, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 3.043062210083008, 'eval_runtime': 7.7252, 'eval_samples_per_second': 129.318, 'eval_steps_per_second': 8.155, 'epoch': 0.56}
{'loss': 2.9835, 'grad_norm': 0.9566610455513, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 3.0393571853637695, 'eval_runtime': 7.7269, 'eval_samples_per_second': 129.289, 'eval_steps_per_second': 8.153, 'epoch': 0.6}
{'loss': 2.9848, 'grad_norm': 0.9466719627380371, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 3.0360324382781982, 'eval_runtime': 7.7115, 'eval_samples_per_second': 129.547, 'eval_steps_per_second': 8.17, 'epoch': 0.64}
{'loss': 3.002, 'grad_norm': 1.0158648490905762, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 3.0324161052703857, 'eval_runtime': 7.7332, 'eval_samples_per_second': 129.183, 'eval_steps_per_second': 8.147, 'epoch': 0.68}
{'loss': 2.9762, 'grad_norm': 0.6019386649131775, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 3.0297133922576904, 'eval_runtime': 7.7179, 'eval_samples_per_second': 129.439, 'eval_steps_per_second': 8.163, 'epoch': 0.72}
{'loss': 3.0277, 'grad_norm': 0.8978351354598999, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 3.0279548168182373, 'eval_runtime': 7.7102, 'eval_samples_per_second': 129.568, 'eval_steps_per_second': 8.171, 'epoch': 0.76}
{'loss': 3.0441, 'grad_norm': 0.6883558630943298, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 3.026686668395996, 'eval_runtime': 7.7462, 'eval_samples_per_second': 128.966, 'eval_steps_per_second': 8.133, 'epoch': 0.8}
{'loss': 3.0481, 'grad_norm': 0.8658890724182129, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 3.0254881381988525, 'eval_runtime': 7.7205, 'eval_samples_per_second': 129.395, 'eval_steps_per_second': 8.16, 'epoch': 0.84}
{'loss': 3.0734, 'grad_norm': 0.7719106674194336, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 3.0244319438934326, 'eval_runtime': 7.7076, 'eval_samples_per_second': 129.613, 'eval_steps_per_second': 8.174, 'epoch': 0.88}
{'loss': 3.0461, 'grad_norm': 0.9225959777832031, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 3.023420572280884, 'eval_runtime': 7.7057, 'eval_samples_per_second': 129.644, 'eval_steps_per_second': 8.176, 'epoch': 0.92}
{'loss': 3.0026, 'grad_norm': 0.7377159595489502, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 3.0230441093444824, 'eval_runtime': 7.6912, 'eval_samples_per_second': 129.888, 'eval_steps_per_second': 8.191, 'epoch': 0.96}
{'loss': 3.024, 'grad_norm': 0.7478697896003723, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 3.0227887630462646, 'eval_runtime': 7.7075, 'eval_samples_per_second': 129.615, 'eval_steps_per_second': 8.174, 'epoch': 1.0}
{'train_runtime': 381.0683, 'train_samples_per_second': 26.234, 'train_steps_per_second': 1.64, 'train_loss': 3.1685185302734373, 'epoch': 1.0}
train_results:  {'eval_loss': [4.265041351318359, 3.5877857208251953, 3.3788411617279053, 3.2724757194519043, 3.206984043121338, 3.162785530090332, 3.130906820297241, 3.104414939880371, 3.0894904136657715, 3.079895257949829, 3.069216012954712, 3.05751633644104, 3.048872232437134, 3.043062210083008, 3.0393571853637695, 3.0360324382781982, 3.0324161052703857, 3.0297133922576904, 3.0279548168182373, 3.026686668395996, 3.0254881381988525, 3.0244319438934326, 3.023420572280884, 3.0230441093444824, 3.0227887630462646], 'performance': [0.76, 0.71]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:50,  9.60s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:02,  1.32it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:41,  1.63it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:27,  1.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:35<00:15,  2.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:42<00:08,  2.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:44<00:01,  2.99it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:44<00:00,  2.24it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.71]
current iteration observed (possibly low-fid or predicted) performance:  1.2363049983978271
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826, 1.221704125404358, 1.2350049018859863, 1.236490249633789, 1.2363049983978271]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5735 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 0.8287519574240355
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.8237072874598793
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 0.8290049616516945
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.8283021024525319
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.777096307906677
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.6257, dtype=torch.float64), 0, tensor(0.1813, dtype=torch.float64), 0, 0, tensor(0.1930, dtype=torch.float64), 0, 0, 1, 1, 0, 1, 0, 0, 2, 4.163336342344338e-19, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(7.1812e-18, dtype=torch.float64), tensor(0.6257, dtype=torch.float64), tensor(6.3647e-17, dtype=torch.float64), tensor(0.1813, dtype=torch.float64), tensor(3.5680e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1930, dtype=torch.float64), tensor(4.2143e-17, dtype=torch.float64), tensor(1.0089e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(4.1633e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.626
  rowan_hellaswag: 0
  sciq: 0.181
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.193
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (4.163336342344338e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  4.163336342344338e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:31,  3.95s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:19,  1.14it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.77it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:34,  2.17it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:27,  2.44it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:21,  2.73it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:17,  2.90it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.55it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:12,  2.82it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:09,  3.00it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.50it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:04,  2.62it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:01,  2.99it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.58it/s]
Evaluation performance at step 25: 0.78
{'loss': 2.8602, 'grad_norm': 5.050268173217773, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 2.435898780822754, 'eval_runtime': 8.7654, 'eval_samples_per_second': 113.97, 'eval_steps_per_second': 7.187, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:41,  5.27s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:29,  1.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:50,  1.65it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:33,  2.21it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:19<00:33,  1.98it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:24<00:31,  1.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:26<00:23,  2.18it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:31<00:21,  2.03it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:35<00:17,  1.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:37<00:11,  2.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:42<00:08,  2.12it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:44<00:04,  2.34it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:46<00:01,  2.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:46<00:00,  2.13it/s]
Evaluation performance at step 50: 0.63
{'loss': 1.9321, 'grad_norm': 3.024052143096924, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 1.5933592319488525, 'eval_runtime': 8.7254, 'eval_samples_per_second': 114.493, 'eval_steps_per_second': 7.22, 'epoch': 0.08}
{'loss': 1.4635, 'grad_norm': 1.832545280456543, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3318867683410645, 'eval_runtime': 8.8047, 'eval_samples_per_second': 113.463, 'eval_steps_per_second': 7.155, 'epoch': 0.12}
{'loss': 1.2715, 'grad_norm': 1.776198387145996, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2302050590515137, 'eval_runtime': 8.7717, 'eval_samples_per_second': 113.889, 'eval_steps_per_second': 7.182, 'epoch': 0.16}
{'loss': 1.2909, 'grad_norm': 1.8397867679595947, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1799849271774292, 'eval_runtime': 8.8343, 'eval_samples_per_second': 113.082, 'eval_steps_per_second': 7.131, 'epoch': 0.2}
{'loss': 1.2131, 'grad_norm': 3.0186710357666016, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1450222730636597, 'eval_runtime': 8.8425, 'eval_samples_per_second': 112.977, 'eval_steps_per_second': 7.125, 'epoch': 0.24}
{'loss': 1.2437, 'grad_norm': 2.2751307487487793, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1310406923294067, 'eval_runtime': 8.8238, 'eval_samples_per_second': 113.216, 'eval_steps_per_second': 7.14, 'epoch': 0.28}
{'loss': 1.1413, 'grad_norm': 1.4427807331085205, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1224348545074463, 'eval_runtime': 8.8491, 'eval_samples_per_second': 112.893, 'eval_steps_per_second': 7.119, 'epoch': 0.32}
{'loss': 1.1144, 'grad_norm': 2.0482330322265625, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1164778470993042, 'eval_runtime': 8.8553, 'eval_samples_per_second': 112.813, 'eval_steps_per_second': 7.114, 'epoch': 0.36}
{'loss': 1.1206, 'grad_norm': 1.8235106468200684, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1089667081832886, 'eval_runtime': 8.8555, 'eval_samples_per_second': 112.811, 'eval_steps_per_second': 7.114, 'epoch': 0.4}
{'loss': 1.0844, 'grad_norm': 1.5264707803726196, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1035566329956055, 'eval_runtime': 8.8728, 'eval_samples_per_second': 112.591, 'eval_steps_per_second': 7.1, 'epoch': 0.44}
{'loss': 1.1412, 'grad_norm': 1.9563648700714111, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0999873876571655, 'eval_runtime': 8.8562, 'eval_samples_per_second': 112.802, 'eval_steps_per_second': 7.114, 'epoch': 0.48}
{'loss': 1.0635, 'grad_norm': 1.5295182466506958, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0971903800964355, 'eval_runtime': 8.872, 'eval_samples_per_second': 112.602, 'eval_steps_per_second': 7.101, 'epoch': 0.52}
{'loss': 1.0561, 'grad_norm': 1.6873562335968018, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0942332744598389, 'eval_runtime': 8.8848, 'eval_samples_per_second': 112.439, 'eval_steps_per_second': 7.091, 'epoch': 0.56}
{'loss': 1.1585, 'grad_norm': 2.169949769973755, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0910252332687378, 'eval_runtime': 8.86, 'eval_samples_per_second': 112.754, 'eval_steps_per_second': 7.111, 'epoch': 0.6}
{'loss': 1.076, 'grad_norm': 1.9629944562911987, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0886530876159668, 'eval_runtime': 8.8668, 'eval_samples_per_second': 112.668, 'eval_steps_per_second': 7.105, 'epoch': 0.64}
{'loss': 1.0913, 'grad_norm': 1.543607234954834, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0868308544158936, 'eval_runtime': 8.8496, 'eval_samples_per_second': 112.887, 'eval_steps_per_second': 7.119, 'epoch': 0.68}
{'loss': 1.1027, 'grad_norm': 1.685164213180542, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0856928825378418, 'eval_runtime': 8.8515, 'eval_samples_per_second': 112.862, 'eval_steps_per_second': 7.117, 'epoch': 0.72}
{'loss': 1.1014, 'grad_norm': 1.34793221950531, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0839738845825195, 'eval_runtime': 8.8581, 'eval_samples_per_second': 112.778, 'eval_steps_per_second': 7.112, 'epoch': 0.76}
{'loss': 1.0489, 'grad_norm': 2.141396999359131, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.082661509513855, 'eval_runtime': 8.8506, 'eval_samples_per_second': 112.873, 'eval_steps_per_second': 7.118, 'epoch': 0.8}
{'loss': 1.0885, 'grad_norm': 1.6170969009399414, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0816516876220703, 'eval_runtime': 8.8385, 'eval_samples_per_second': 113.028, 'eval_steps_per_second': 7.128, 'epoch': 0.84}
{'loss': 1.149, 'grad_norm': 1.6147780418395996, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0810312032699585, 'eval_runtime': 8.8253, 'eval_samples_per_second': 113.197, 'eval_steps_per_second': 7.139, 'epoch': 0.88}
{'loss': 1.0837, 'grad_norm': 1.8305587768554688, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0805922746658325, 'eval_runtime': 8.8172, 'eval_samples_per_second': 113.302, 'eval_steps_per_second': 7.145, 'epoch': 0.92}
{'loss': 1.067, 'grad_norm': 1.4250974655151367, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0799282789230347, 'eval_runtime': 8.8121, 'eval_samples_per_second': 113.367, 'eval_steps_per_second': 7.149, 'epoch': 0.96}
{'loss': 1.1291, 'grad_norm': 1.9670958518981934, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0796819925308228, 'eval_runtime': 8.8332, 'eval_samples_per_second': 113.096, 'eval_steps_per_second': 7.132, 'epoch': 1.0}
{'train_runtime': 440.3017, 'train_samples_per_second': 22.709, 'train_steps_per_second': 1.419, 'train_loss': 1.2436894409179688, 'epoch': 1.0}
train_results:  {'eval_loss': [2.435898780822754, 1.5933592319488525, 1.3318867683410645, 1.2302050590515137, 1.1799849271774292, 1.1450222730636597, 1.1310406923294067, 1.1224348545074463, 1.1164778470993042, 1.1089667081832886, 1.1035566329956055, 1.0999873876571655, 1.0971903800964355, 1.0942332744598389, 1.0910252332687378, 1.0886530876159668, 1.0868308544158936, 1.0856928825378418, 1.0839738845825195, 1.082661509513855, 1.0816516876220703, 1.0810312032699585, 1.0805922746658325, 1.0799282789230347, 1.0796819925308228], 'performance': [0.78, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:50,  9.60s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.87it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.234453558921814
current iteration best possible performance (full train run):  0.651
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826, 1.221704125404358, 1.2350049018859863, 1.236490249633789, 1.2363049983978271, 1.234453558921814]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8623 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.8803946971425503
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 0.8838931254188898
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.7541776498460855
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.8727330496169681
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.8802063666268268
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1186, dtype=torch.float64), tensor(0.1411, dtype=torch.float64), tensor(0.1080, dtype=torch.float64), tensor(0.2627, dtype=torch.float64), tensor(0.3696, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.6873e-16, dtype=torch.float64), tensor(0.1186, dtype=torch.float64), tensor(0.1411, dtype=torch.float64), tensor(0.1080, dtype=torch.float64), tensor(0.2627, dtype=torch.float64), tensor(0.3696, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.119
  sciq: 0.141
  triviaqa: 0.108
  truthfulqa_gen: 0.263
  wikitext: 0.37
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:04,  2.47s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.66it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.66it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.81it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.27it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.22it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.42it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.67it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.97it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.4527, 'grad_norm': 2.5146641731262207, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 4.296467304229736, 'eval_runtime': 8.0968, 'eval_samples_per_second': 123.381, 'eval_steps_per_second': 7.781, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:33,  3.37s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:15,  1.20it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:44,  1.88it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:23,  2.83it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:24,  2.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.68it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.98it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.65it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.17it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.72it/s]
Evaluation performance at step 50: 0.74
{'loss': 3.8668, 'grad_norm': 1.222655177116394, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 3.6489951610565186, 'eval_runtime': 8.1068, 'eval_samples_per_second': 123.229, 'eval_steps_per_second': 7.771, 'epoch': 0.08}
{'loss': 3.5335, 'grad_norm': 1.0939395427703857, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.3630151748657227, 'eval_runtime': 8.1525, 'eval_samples_per_second': 122.539, 'eval_steps_per_second': 7.728, 'epoch': 0.12}
{'loss': 3.2464, 'grad_norm': 1.0456112623214722, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.2103660106658936, 'eval_runtime': 8.1489, 'eval_samples_per_second': 122.594, 'eval_steps_per_second': 7.731, 'epoch': 0.16}
{'loss': 3.2079, 'grad_norm': 1.2135872840881348, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.1370906829833984, 'eval_runtime': 8.1562, 'eval_samples_per_second': 122.484, 'eval_steps_per_second': 7.724, 'epoch': 0.2}
{'loss': 3.0733, 'grad_norm': 0.7451589703559875, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.078810691833496, 'eval_runtime': 8.1639, 'eval_samples_per_second': 122.367, 'eval_steps_per_second': 7.717, 'epoch': 0.24}
{'loss': 3.1017, 'grad_norm': 0.991290271282196, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.0360286235809326, 'eval_runtime': 8.1701, 'eval_samples_per_second': 122.275, 'eval_steps_per_second': 7.711, 'epoch': 0.28}
{'loss': 3.0207, 'grad_norm': 1.387387752532959, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 3.0073373317718506, 'eval_runtime': 8.2074, 'eval_samples_per_second': 121.72, 'eval_steps_per_second': 7.676, 'epoch': 0.32}
{'loss': 3.0189, 'grad_norm': 0.8123291730880737, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.990391492843628, 'eval_runtime': 8.2003, 'eval_samples_per_second': 121.825, 'eval_steps_per_second': 7.683, 'epoch': 0.36}
{'loss': 2.9989, 'grad_norm': 0.9385244250297546, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.9685075283050537, 'eval_runtime': 8.1987, 'eval_samples_per_second': 121.848, 'eval_steps_per_second': 7.684, 'epoch': 0.4}
{'loss': 2.9498, 'grad_norm': 0.8487570285797119, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.9572677612304688, 'eval_runtime': 8.2183, 'eval_samples_per_second': 121.557, 'eval_steps_per_second': 7.666, 'epoch': 0.44}
{'loss': 2.9621, 'grad_norm': 0.8094989657402039, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.948822021484375, 'eval_runtime': 8.207, 'eval_samples_per_second': 121.726, 'eval_steps_per_second': 7.676, 'epoch': 0.48}
{'loss': 3.0124, 'grad_norm': 0.7999233603477478, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.942638635635376, 'eval_runtime': 8.2071, 'eval_samples_per_second': 121.723, 'eval_steps_per_second': 7.676, 'epoch': 0.52}
{'loss': 2.8716, 'grad_norm': 0.9675016403198242, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.9358909130096436, 'eval_runtime': 8.2088, 'eval_samples_per_second': 121.699, 'eval_steps_per_second': 7.675, 'epoch': 0.56}
{'loss': 2.9354, 'grad_norm': 1.0717226266860962, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.9313414096832275, 'eval_runtime': 8.2272, 'eval_samples_per_second': 121.427, 'eval_steps_per_second': 7.658, 'epoch': 0.6}
{'loss': 2.9147, 'grad_norm': 0.9047791957855225, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.927865505218506, 'eval_runtime': 8.2317, 'eval_samples_per_second': 121.36, 'eval_steps_per_second': 7.653, 'epoch': 0.64}
{'loss': 2.9291, 'grad_norm': 0.6206368803977966, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.924421548843384, 'eval_runtime': 8.2105, 'eval_samples_per_second': 121.674, 'eval_steps_per_second': 7.673, 'epoch': 0.68}
{'loss': 2.9454, 'grad_norm': 0.7067918181419373, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.921865463256836, 'eval_runtime': 8.2201, 'eval_samples_per_second': 121.531, 'eval_steps_per_second': 7.664, 'epoch': 0.72}
{'loss': 2.9893, 'grad_norm': 0.7194162011146545, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.9196324348449707, 'eval_runtime': 8.2223, 'eval_samples_per_second': 121.498, 'eval_steps_per_second': 7.662, 'epoch': 0.76}
{'loss': 2.9692, 'grad_norm': 0.7888144254684448, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.917480230331421, 'eval_runtime': 8.2201, 'eval_samples_per_second': 121.531, 'eval_steps_per_second': 7.664, 'epoch': 0.8}
{'loss': 2.902, 'grad_norm': 0.6297484636306763, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.9149844646453857, 'eval_runtime': 8.2108, 'eval_samples_per_second': 121.669, 'eval_steps_per_second': 7.673, 'epoch': 0.84}
{'loss': 2.8994, 'grad_norm': 0.873526394367218, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.9137344360351562, 'eval_runtime': 8.2101, 'eval_samples_per_second': 121.679, 'eval_steps_per_second': 7.673, 'epoch': 0.88}
{'loss': 2.9246, 'grad_norm': 0.7799810767173767, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.912442684173584, 'eval_runtime': 8.2006, 'eval_samples_per_second': 121.82, 'eval_steps_per_second': 7.682, 'epoch': 0.92}
{'loss': 2.9312, 'grad_norm': 0.9769154191017151, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.9116082191467285, 'eval_runtime': 8.2081, 'eval_samples_per_second': 121.709, 'eval_steps_per_second': 7.675, 'epoch': 0.96}
{'loss': 2.9247, 'grad_norm': 1.4683568477630615, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.910851001739502, 'eval_runtime': 8.234, 'eval_samples_per_second': 121.327, 'eval_steps_per_second': 7.651, 'epoch': 1.0}
{'train_runtime': 405.2579, 'train_samples_per_second': 24.666, 'train_steps_per_second': 1.542, 'train_loss': 3.10327177734375, 'epoch': 1.0}
train_results:  {'eval_loss': [4.296467304229736, 3.6489951610565186, 3.3630151748657227, 3.2103660106658936, 3.1370906829833984, 3.078810691833496, 3.0360286235809326, 3.0073373317718506, 2.990391492843628, 2.9685075283050537, 2.9572677612304688, 2.948822021484375, 2.942638635635376, 2.9358909130096436, 2.9313414096832275, 2.927865505218506, 2.924421548843384, 2.921865463256836, 2.9196324348449707, 2.917480230331421, 2.9149844646453857, 2.9137344360351562, 2.912442684173584, 2.9116082191467285, 2.910851001739502], 'performance': [0.77, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:50,  9.60s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:41<00:19,  1.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:51<00:01,  2.56it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:51<00:00,  1.94it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  1.2358765602111816
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826, 1.221704125404358, 1.2350049018859863, 1.236490249633789, 1.2363049983978271, 1.234453558921814, 1.2358765602111816]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9770 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.720791220327669
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.8490141108258656
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.8084756663688559
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.7942428806325251
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 0.8414144615558259
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0381, dtype=torch.float64), tensor(0.1713, dtype=torch.float64), tensor(0.3697, dtype=torch.float64), tensor(0.2829, dtype=torch.float64), tensor(0.1380, dtype=torch.float64), 0, 0, 1, 1, 0, 1, 0, 0, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0381, dtype=torch.float64), tensor(0.1713, dtype=torch.float64), tensor(0.3697, dtype=torch.float64), tensor(0.2829, dtype=torch.float64), tensor(0.1380, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0370e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.038
  sciq: 0.171
  triviaqa: 0.37
  truthfulqa_gen: 0.283
  wikitext: 0.138
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:39,  3.43s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:16,  1.20it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:45,  1.83it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:31,  2.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:24,  2.71it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:20,  2.94it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.99it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:13,  3.10it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.23it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:07,  3.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.69it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.84it/s]
Evaluation performance at step 25: 0.79
{'loss': 4.8951, 'grad_norm': 7.2167229652404785, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.79}
{'eval_loss': 4.027408599853516, 'eval_runtime': 6.2953, 'eval_samples_per_second': 158.69, 'eval_steps_per_second': 10.007, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:41,  5.26s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:29,  1.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:49,  1.67it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:34,  2.20it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:17<00:27,  2.40it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:20,  2.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:17,  3.00it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:13,  3.10it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:10,  3.39it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:07,  3.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:06,  3.05it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.22it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.74it/s]
Evaluation performance at step 50: 0.74
{'loss': 3.0698, 'grad_norm': 5.246810436248779, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 2.3695385456085205, 'eval_runtime': 6.3143, 'eval_samples_per_second': 158.212, 'eval_steps_per_second': 9.977, 'epoch': 0.08}
{'loss': 2.1119, 'grad_norm': 2.65075945854187, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8778691291809082, 'eval_runtime': 6.339, 'eval_samples_per_second': 157.595, 'eval_steps_per_second': 9.938, 'epoch': 0.12}
{'loss': 1.7462, 'grad_norm': 2.455559253692627, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.652275800704956, 'eval_runtime': 6.3427, 'eval_samples_per_second': 157.505, 'eval_steps_per_second': 9.933, 'epoch': 0.16}
{'loss': 1.4984, 'grad_norm': 3.652531385421753, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5357264280319214, 'eval_runtime': 6.3374, 'eval_samples_per_second': 157.636, 'eval_steps_per_second': 9.941, 'epoch': 0.2}
{'loss': 1.5264, 'grad_norm': 2.1221933364868164, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5017436742782593, 'eval_runtime': 6.3495, 'eval_samples_per_second': 157.336, 'eval_steps_per_second': 9.922, 'epoch': 0.24}
{'loss': 1.4891, 'grad_norm': 2.800856351852417, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4773120880126953, 'eval_runtime': 6.3556, 'eval_samples_per_second': 157.185, 'eval_steps_per_second': 9.913, 'epoch': 0.28}
{'loss': 1.5669, 'grad_norm': 2.273611307144165, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.459544062614441, 'eval_runtime': 6.3889, 'eval_samples_per_second': 156.365, 'eval_steps_per_second': 9.861, 'epoch': 0.32}
{'loss': 1.3879, 'grad_norm': 2.9962778091430664, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4491243362426758, 'eval_runtime': 6.3944, 'eval_samples_per_second': 156.231, 'eval_steps_per_second': 9.852, 'epoch': 0.36}
{'loss': 1.4904, 'grad_norm': 1.8597180843353271, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4359846115112305, 'eval_runtime': 6.3886, 'eval_samples_per_second': 156.372, 'eval_steps_per_second': 9.861, 'epoch': 0.4}
{'loss': 1.489, 'grad_norm': 2.233031988143921, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.424789309501648, 'eval_runtime': 6.4129, 'eval_samples_per_second': 155.78, 'eval_steps_per_second': 9.824, 'epoch': 0.44}
{'loss': 1.4281, 'grad_norm': 2.3793840408325195, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4167454242706299, 'eval_runtime': 6.4124, 'eval_samples_per_second': 155.792, 'eval_steps_per_second': 9.825, 'epoch': 0.48}
{'loss': 1.4139, 'grad_norm': 2.795788526535034, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4119540452957153, 'eval_runtime': 6.3988, 'eval_samples_per_second': 156.124, 'eval_steps_per_second': 9.846, 'epoch': 0.52}
{'loss': 1.4272, 'grad_norm': 2.435342311859131, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4080830812454224, 'eval_runtime': 6.4032, 'eval_samples_per_second': 156.016, 'eval_steps_per_second': 9.839, 'epoch': 0.56}
{'loss': 1.354, 'grad_norm': 4.120621681213379, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4018712043762207, 'eval_runtime': 6.3919, 'eval_samples_per_second': 156.292, 'eval_steps_per_second': 9.856, 'epoch': 0.6}
{'loss': 1.4398, 'grad_norm': 2.430370569229126, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3972487449645996, 'eval_runtime': 6.3856, 'eval_samples_per_second': 156.446, 'eval_steps_per_second': 9.866, 'epoch': 0.64}
{'loss': 1.4128, 'grad_norm': 2.5208592414855957, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3938267230987549, 'eval_runtime': 6.391, 'eval_samples_per_second': 156.314, 'eval_steps_per_second': 9.858, 'epoch': 0.68}
{'loss': 1.3647, 'grad_norm': 2.249889612197876, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3907930850982666, 'eval_runtime': 6.3906, 'eval_samples_per_second': 156.324, 'eval_steps_per_second': 9.858, 'epoch': 0.72}
{'loss': 1.3598, 'grad_norm': 2.556450843811035, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3880195617675781, 'eval_runtime': 6.381, 'eval_samples_per_second': 156.56, 'eval_steps_per_second': 9.873, 'epoch': 0.76}
{'loss': 1.3834, 'grad_norm': 2.9107491970062256, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3848316669464111, 'eval_runtime': 6.3923, 'eval_samples_per_second': 156.283, 'eval_steps_per_second': 9.856, 'epoch': 0.8}
{'loss': 1.3839, 'grad_norm': 2.487328052520752, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3828414678573608, 'eval_runtime': 6.3884, 'eval_samples_per_second': 156.377, 'eval_steps_per_second': 9.862, 'epoch': 0.84}
{'loss': 1.3162, 'grad_norm': 4.873669624328613, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3820277452468872, 'eval_runtime': 6.3901, 'eval_samples_per_second': 156.336, 'eval_steps_per_second': 9.859, 'epoch': 0.88}
{'loss': 1.4492, 'grad_norm': 3.0748276710510254, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.380550503730774, 'eval_runtime': 6.3929, 'eval_samples_per_second': 156.267, 'eval_steps_per_second': 9.855, 'epoch': 0.92}
{'loss': 1.3924, 'grad_norm': 2.709413528442383, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3794448375701904, 'eval_runtime': 6.3917, 'eval_samples_per_second': 156.296, 'eval_steps_per_second': 9.856, 'epoch': 0.96}
{'loss': 1.4001, 'grad_norm': 2.811271905899048, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.379370927810669, 'eval_runtime': 6.3881, 'eval_samples_per_second': 156.385, 'eval_steps_per_second': 9.862, 'epoch': 1.0}
{'train_runtime': 326.7998, 'train_samples_per_second': 30.591, 'train_steps_per_second': 1.912, 'train_loss': 1.6718667663574218, 'epoch': 1.0}
train_results:  {'eval_loss': [4.027408599853516, 2.3695385456085205, 1.8778691291809082, 1.652275800704956, 1.5357264280319214, 1.5017436742782593, 1.4773120880126953, 1.459544062614441, 1.4491243362426758, 1.4359846115112305, 1.424789309501648, 1.4167454242706299, 1.4119540452957153, 1.4080830812454224, 1.4018712043762207, 1.3972487449645996, 1.3938267230987549, 1.3907930850982666, 1.3880195617675781, 1.3848316669464111, 1.3828414678573608, 1.3820277452468872, 1.380550503730774, 1.3794448375701904, 1.379370927810669], 'performance': [0.79, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:53,  9.63s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.87it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.79, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  1.2365977764129639
current iteration best possible performance (full train run):  0.07350000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826, 1.221704125404358, 1.2350049018859863, 1.236490249633789, 1.2363049983978271, 1.234453558921814, 1.2358765602111816, 1.2365977764129639]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6857 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.8123536553980403
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.8045704374818987
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.8544991610060702
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.8026493786725598
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.8301281029374994
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1396, dtype=torch.float64), tensor(0.1667, dtype=torch.float64), tensor(0.4631, dtype=torch.float64), 0, tensor(0.2306, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 6.938893903907229e-19, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(8.0859e-17, dtype=torch.float64), tensor(5.5511e-16, dtype=torch.float64), tensor(0.1396, dtype=torch.float64), tensor(0.1667, dtype=torch.float64), tensor(0.4631, dtype=torch.float64), tensor(7.3925e-17, dtype=torch.float64), tensor(0.2306, dtype=torch.float64), tensor(1.9275e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(6.9389e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.14
  sciq: 0.167
  triviaqa: 0.463
  truthfulqa_gen: 0
  wikitext: 0.231
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (6.938893903907229e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  6.938893903907229e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:55,  2.99s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.24it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.95it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.75it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.09it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.63it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.93it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.78it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.7238, 'grad_norm': 4.4300971031188965, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 4.498783588409424, 'eval_runtime': 8.1828, 'eval_samples_per_second': 122.085, 'eval_steps_per_second': 7.699, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:41,  5.27s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:03,  1.42it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:38,  2.17it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:28,  2.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:22,  3.02it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:23,  2.54it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.70it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.04it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.82it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.17it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.76it/s]
Evaluation performance at step 50: 0.71
{'loss': 4.0339, 'grad_norm': 2.210421323776245, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.71}
{'eval_loss': 3.73404860496521, 'eval_runtime': 8.2105, 'eval_samples_per_second': 121.673, 'eval_steps_per_second': 7.673, 'epoch': 0.08}
{'loss': 3.4728, 'grad_norm': 1.2085899114608765, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.4399538040161133, 'eval_runtime': 8.2339, 'eval_samples_per_second': 121.328, 'eval_steps_per_second': 7.651, 'epoch': 0.12}
{'loss': 3.3506, 'grad_norm': 0.8291153907775879, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.275524377822876, 'eval_runtime': 8.2502, 'eval_samples_per_second': 121.088, 'eval_steps_per_second': 7.636, 'epoch': 0.16}
{'loss': 3.2194, 'grad_norm': 1.1194802522659302, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.1803009510040283, 'eval_runtime': 8.2632, 'eval_samples_per_second': 120.898, 'eval_steps_per_second': 7.624, 'epoch': 0.2}
{'loss': 3.141, 'grad_norm': 0.7437898516654968, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.133838176727295, 'eval_runtime': 8.2898, 'eval_samples_per_second': 120.509, 'eval_steps_per_second': 7.6, 'epoch': 0.24}
{'loss': 3.056, 'grad_norm': 0.9483975768089294, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.094845771789551, 'eval_runtime': 8.2745, 'eval_samples_per_second': 120.733, 'eval_steps_per_second': 7.614, 'epoch': 0.28}
{'loss': 3.0172, 'grad_norm': 0.7596504092216492, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 3.0667014122009277, 'eval_runtime': 8.3031, 'eval_samples_per_second': 120.316, 'eval_steps_per_second': 7.588, 'epoch': 0.32}
{'loss': 3.0426, 'grad_norm': 0.8837065100669861, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 3.043288230895996, 'eval_runtime': 8.3032, 'eval_samples_per_second': 120.315, 'eval_steps_per_second': 7.587, 'epoch': 0.36}
{'loss': 3.0611, 'grad_norm': 1.0032025575637817, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 3.0157887935638428, 'eval_runtime': 8.3023, 'eval_samples_per_second': 120.329, 'eval_steps_per_second': 7.588, 'epoch': 0.4}
{'loss': 3.0081, 'grad_norm': 0.8752511143684387, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 3.0010478496551514, 'eval_runtime': 8.3025, 'eval_samples_per_second': 120.325, 'eval_steps_per_second': 7.588, 'epoch': 0.44}
{'loss': 2.9571, 'grad_norm': 0.9242498874664307, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.9900834560394287, 'eval_runtime': 8.3167, 'eval_samples_per_second': 120.12, 'eval_steps_per_second': 7.575, 'epoch': 0.48}
{'loss': 2.9643, 'grad_norm': 0.9083336591720581, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.9822580814361572, 'eval_runtime': 8.3004, 'eval_samples_per_second': 120.356, 'eval_steps_per_second': 7.59, 'epoch': 0.52}
{'loss': 2.9329, 'grad_norm': 0.9179875254631042, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.9760584831237793, 'eval_runtime': 8.2993, 'eval_samples_per_second': 120.372, 'eval_steps_per_second': 7.591, 'epoch': 0.56}
{'loss': 2.9995, 'grad_norm': 0.7657446265220642, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.9721598625183105, 'eval_runtime': 8.2939, 'eval_samples_per_second': 120.45, 'eval_steps_per_second': 7.596, 'epoch': 0.6}
{'loss': 2.9649, 'grad_norm': 0.8278298377990723, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.9672532081604004, 'eval_runtime': 8.2952, 'eval_samples_per_second': 120.432, 'eval_steps_per_second': 7.595, 'epoch': 0.64}
{'loss': 2.9888, 'grad_norm': 0.8949536085128784, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.9627225399017334, 'eval_runtime': 8.3002, 'eval_samples_per_second': 120.358, 'eval_steps_per_second': 7.59, 'epoch': 0.68}
{'loss': 2.9455, 'grad_norm': 0.8972429633140564, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.9603078365325928, 'eval_runtime': 8.2936, 'eval_samples_per_second': 120.454, 'eval_steps_per_second': 7.596, 'epoch': 0.72}
{'loss': 2.9915, 'grad_norm': 0.8442258834838867, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.958102226257324, 'eval_runtime': 8.2972, 'eval_samples_per_second': 120.402, 'eval_steps_per_second': 7.593, 'epoch': 0.76}
{'loss': 2.9201, 'grad_norm': 1.8895331621170044, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.9558088779449463, 'eval_runtime': 8.2969, 'eval_samples_per_second': 120.406, 'eval_steps_per_second': 7.593, 'epoch': 0.8}
{'loss': 2.9645, 'grad_norm': 0.8788411617279053, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.9549291133880615, 'eval_runtime': 8.2923, 'eval_samples_per_second': 120.474, 'eval_steps_per_second': 7.597, 'epoch': 0.84}
{'loss': 2.9697, 'grad_norm': 0.7604557871818542, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.9539246559143066, 'eval_runtime': 8.3029, 'eval_samples_per_second': 120.32, 'eval_steps_per_second': 7.588, 'epoch': 0.88}
{'loss': 2.9279, 'grad_norm': 1.1914421319961548, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.9531519412994385, 'eval_runtime': 8.2951, 'eval_samples_per_second': 120.432, 'eval_steps_per_second': 7.595, 'epoch': 0.92}
{'loss': 2.9425, 'grad_norm': 0.9373786449432373, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.95200777053833, 'eval_runtime': 8.3002, 'eval_samples_per_second': 120.358, 'eval_steps_per_second': 7.59, 'epoch': 0.96}
{'loss': 2.9161, 'grad_norm': 1.0563335418701172, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.952177047729492, 'eval_runtime': 8.2958, 'eval_samples_per_second': 120.422, 'eval_steps_per_second': 7.594, 'epoch': 1.0}
{'train_runtime': 402.2941, 'train_samples_per_second': 24.855, 'train_steps_per_second': 1.554, 'train_loss': 3.140468701171875, 'epoch': 1.0}
train_results:  {'eval_loss': [4.498783588409424, 3.73404860496521, 3.4399538040161133, 3.275524377822876, 3.1803009510040283, 3.133838176727295, 3.094845771789551, 3.0667014122009277, 3.043288230895996, 3.0157887935638428, 3.0010478496551514, 2.9900834560394287, 2.9822580814361572, 2.9760584831237793, 2.9721598625183105, 2.9672532081604004, 2.9627225399017334, 2.9603078365325928, 2.958102226257324, 2.9558088779449463, 2.9549291133880615, 2.9539246559143066, 2.9531519412994385, 2.95200777053833, 2.952177047729492], 'performance': [0.77, 0.71]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:50,  9.60s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:41,  1.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:23,  2.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:36<00:16,  2.14it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:44<00:08,  2.14it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:45<00:01,  2.91it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:45<00:00,  2.19it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.71]
current iteration observed (possibly low-fid or predicted) performance:  1.2363841533660889
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826, 1.221704125404358, 1.2350049018859863, 1.236490249633789, 1.2363049983978271, 1.234453558921814, 1.2358765602111816, 1.2365977764129639, 1.2363841533660889]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0644 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.8168025324748456
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.8355571223173666
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.8352635024930787
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.8251409877446694
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.8179198435132494
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1197, dtype=torch.float64), tensor(0.1680, dtype=torch.float64), tensor(0.1264, dtype=torch.float64), tensor(0.2118, dtype=torch.float64), tensor(0.3236, dtype=torch.float64), 0, tensor(0.0500, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 1.942394533426294e-19, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(3.7928e-17, dtype=torch.float64), tensor(0.0005, dtype=torch.float64), tensor(0.1197, dtype=torch.float64), tensor(0.1680, dtype=torch.float64), tensor(0.1264, dtype=torch.float64), tensor(0.2118, dtype=torch.float64), tensor(0.3236, dtype=torch.float64), tensor(1.4587e-17, dtype=torch.float64), tensor(0.0500, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.9424e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.12
  sciq: 0.168
  triviaqa: 0.126
  truthfulqa_gen: 0.212
  wikitext: 0.324
  mmlu: 0
  arc_challenge: 0.05

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.942394533426294e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  1.942394533426294e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9991
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:04,  2.47s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.64it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.82it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  2.98it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.19it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.81it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.4076, 'grad_norm': 2.795797824859619, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 4.300527095794678, 'eval_runtime': 8.214, 'eval_samples_per_second': 121.621, 'eval_steps_per_second': 7.67, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:39,  3.43s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:16,  1.18it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:44,  1.87it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.43it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:23,  2.81it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:24,  2.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.55it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:13,  2.69it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.04it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.51it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:03,  2.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.61it/s]
Evaluation performance at step 50: 0.74
{'loss': 3.8616, 'grad_norm': 1.4955430030822754, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 3.6550204753875732, 'eval_runtime': 8.1924, 'eval_samples_per_second': 121.942, 'eval_steps_per_second': 7.69, 'epoch': 0.08}
{'loss': 3.483, 'grad_norm': 1.0388163328170776, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.3731954097747803, 'eval_runtime': 8.2296, 'eval_samples_per_second': 121.391, 'eval_steps_per_second': 7.655, 'epoch': 0.12}
{'loss': 3.2554, 'grad_norm': 1.1529450416564941, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.2201180458068848, 'eval_runtime': 8.2097, 'eval_samples_per_second': 121.685, 'eval_steps_per_second': 7.674, 'epoch': 0.16}
{'loss': 3.1524, 'grad_norm': 1.0988073348999023, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.1391232013702393, 'eval_runtime': 8.236, 'eval_samples_per_second': 121.296, 'eval_steps_per_second': 7.649, 'epoch': 0.2}
{'loss': 3.126, 'grad_norm': 0.8007983565330505, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.078153371810913, 'eval_runtime': 8.2548, 'eval_samples_per_second': 121.021, 'eval_steps_per_second': 7.632, 'epoch': 0.24}
{'loss': 3.0231, 'grad_norm': 0.8230569362640381, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.039438247680664, 'eval_runtime': 8.3011, 'eval_samples_per_second': 120.345, 'eval_steps_per_second': 7.589, 'epoch': 0.28}
{'loss': 3.0083, 'grad_norm': 0.8923653960227966, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 3.0143961906433105, 'eval_runtime': 8.2792, 'eval_samples_per_second': 120.664, 'eval_steps_per_second': 7.609, 'epoch': 0.32}
{'loss': 3.0117, 'grad_norm': 0.9590510725975037, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.994318962097168, 'eval_runtime': 8.2824, 'eval_samples_per_second': 120.617, 'eval_steps_per_second': 7.606, 'epoch': 0.36}
{'loss': 2.9481, 'grad_norm': 0.7249482274055481, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.9808709621429443, 'eval_runtime': 8.27, 'eval_samples_per_second': 120.798, 'eval_steps_per_second': 7.618, 'epoch': 0.4}
{'loss': 2.9427, 'grad_norm': 0.6707534790039062, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.9682846069335938, 'eval_runtime': 8.2469, 'eval_samples_per_second': 121.136, 'eval_steps_per_second': 7.639, 'epoch': 0.44}
{'loss': 2.9182, 'grad_norm': 0.923232913017273, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.958883762359619, 'eval_runtime': 8.2481, 'eval_samples_per_second': 121.118, 'eval_steps_per_second': 7.638, 'epoch': 0.48}
{'loss': 2.9637, 'grad_norm': 1.1982910633087158, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.949214458465576, 'eval_runtime': 8.2578, 'eval_samples_per_second': 120.976, 'eval_steps_per_second': 7.629, 'epoch': 0.52}
{'loss': 2.958, 'grad_norm': 0.823113739490509, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.9419355392456055, 'eval_runtime': 8.2557, 'eval_samples_per_second': 121.007, 'eval_steps_per_second': 7.631, 'epoch': 0.56}
{'loss': 2.9316, 'grad_norm': 1.027125597000122, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.9359560012817383, 'eval_runtime': 8.2676, 'eval_samples_per_second': 120.833, 'eval_steps_per_second': 7.62, 'epoch': 0.6}
{'loss': 2.9281, 'grad_norm': 0.8016055226325989, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.9315922260284424, 'eval_runtime': 8.255, 'eval_samples_per_second': 121.018, 'eval_steps_per_second': 7.632, 'epoch': 0.64}
{'loss': 2.9575, 'grad_norm': 0.832449197769165, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.9279847145080566, 'eval_runtime': 8.248, 'eval_samples_per_second': 121.12, 'eval_steps_per_second': 7.638, 'epoch': 0.68}
{'loss': 2.9069, 'grad_norm': 0.597037672996521, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.925067901611328, 'eval_runtime': 8.2568, 'eval_samples_per_second': 120.991, 'eval_steps_per_second': 7.63, 'epoch': 0.72}
{'loss': 2.8547, 'grad_norm': 0.8240216374397278, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.9221086502075195, 'eval_runtime': 8.2333, 'eval_samples_per_second': 121.337, 'eval_steps_per_second': 7.652, 'epoch': 0.76}
{'loss': 2.8837, 'grad_norm': 1.0414360761642456, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.920400381088257, 'eval_runtime': 8.2503, 'eval_samples_per_second': 121.086, 'eval_steps_per_second': 7.636, 'epoch': 0.8}
{'loss': 2.9715, 'grad_norm': 0.9057865738868713, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.916715621948242, 'eval_runtime': 8.2708, 'eval_samples_per_second': 120.786, 'eval_steps_per_second': 7.617, 'epoch': 0.84}
{'loss': 2.8754, 'grad_norm': 0.9898431301116943, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.914698362350464, 'eval_runtime': 8.2611, 'eval_samples_per_second': 120.929, 'eval_steps_per_second': 7.626, 'epoch': 0.88}
{'loss': 2.8965, 'grad_norm': 0.7339595556259155, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.9134886264801025, 'eval_runtime': 8.253, 'eval_samples_per_second': 121.047, 'eval_steps_per_second': 7.634, 'epoch': 0.92}
{'loss': 2.8648, 'grad_norm': 0.8748801946640015, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.9127304553985596, 'eval_runtime': 8.2486, 'eval_samples_per_second': 121.112, 'eval_steps_per_second': 7.638, 'epoch': 0.96}
{'loss': 2.8952, 'grad_norm': 0.9271159172058105, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.9124228954315186, 'eval_runtime': 8.2482, 'eval_samples_per_second': 121.117, 'eval_steps_per_second': 7.638, 'epoch': 1.0}
{'train_runtime': 410.5517, 'train_samples_per_second': 24.336, 'train_steps_per_second': 1.522, 'train_loss': 3.08102822265625, 'epoch': 1.0}
train_results:  {'eval_loss': [4.300527095794678, 3.6550204753875732, 3.3731954097747803, 3.2201180458068848, 3.1391232013702393, 3.078153371810913, 3.039438247680664, 3.0143961906433105, 2.994318962097168, 2.9808709621429443, 2.9682846069335938, 2.958883762359619, 2.949214458465576, 2.9419355392456055, 2.9359560012817383, 2.9315922260284424, 2.9279847145080566, 2.925067901611328, 2.9221086502075195, 2.920400381088257, 2.916715621948242, 2.914698362350464, 2.9134886264801025, 2.9127304553985596, 2.9124228954315186], 'performance': [0.77, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<10:13,  6.20s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<01:03,  1.30it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:41,  1.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:28,  1.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:38<00:18,  1.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:45<00:09,  1.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:48<00:01,  2.61it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.08it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  1.2361924648284912
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.6226346492767334, 0.794015645980835, 0.6125386953353882, 0.793671727180481, 1.0166096687316895, 1.226782202720642, 0.7708524465560913, 0.6542402505874634, 1.2025779485702515, 0.9818884134292603, 1.2089287042617798, 1.2165716886520386, 0.7168589234352112, 1.2275041341781616, 1.2271978855133057, 1.2274394035339355, 1.2018580436706543, 1.2357836961746216, 1.2349300384521484, 1.2354788780212402, 1.2364070415496826, 1.221704125404358, 1.2350049018859863, 1.236490249633789, 1.2363049983978271, 1.234453558921814, 1.2358765602111816, 1.2365977764129639, 1.2363841533660889, 1.2361924648284912]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0829 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.7229520671142375
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.7941936609011466
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.8293645053219006
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.727739455193773
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.74050009574633
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0819, dtype=torch.float64), tensor(0.1667, dtype=torch.float64), tensor(0.2005, dtype=torch.float64), tensor(0.1547, dtype=torch.float64), tensor(0.3963, dtype=torch.float64), 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.010936845921459748, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(7.2535e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0819, dtype=torch.float64), tensor(0.1667, dtype=torch.float64), tensor(0.2005, dtype=torch.float64), tensor(0.1547, dtype=torch.float64), tensor(0.3963, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.1094, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.7454999999999999, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/gsm8k/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 9, 1, 1, 0, 0, 1, 14, 0.03669660801939172, 9, 1]
Checking history sample input_X_between_0_1:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 0.28125, 1.0, 1.0, 0.0, 0.0, 1.0, 0.109375, 0.3669660801939172, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.7
Checking history sample input_X:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 4, 1, 1, 0, 1, 0, 45, 0.02996529708553877, 41, 0]
Checking history sample input_X_between_0_1:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 0.125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3515625, 0.29965297085538767, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 5, 0, 1, 1, 0, 0, 115, 0.08044623754842878, 26, 0]
Checking history sample input_X_between_0_1:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 0.15625, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8984375, 0.8044623754842878, 0.5416666666666666, 0.0]
Checking history sample performance at 625 steps:  0.73
Checking history sample input_X:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 30, 0, 1, 0, 1, 1, 121, 0.019265932894845208, 29, 1]
Checking history sample input_X_between_0_1:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 0.9375, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9453125, 0.19265932894845206, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 19, 0, 0, 1, 1, 1, 108, 0.09306302255978231, 35, 1]
Checking history sample input_X_between_0_1:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 0.59375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.84375, 0.930630225597823, 0.7291666666666666, 1.0]
Checking history sample performance at 625 steps:  0.62
Checking history sample input_X:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 14, 0, 0, 0, 1, 1, 44, 0.004820496247456452, 22, 1]
Checking history sample input_X_between_0_1:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 0.4375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.34375, 0.04820496247456452, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.63
Checking history sample input_X:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 24, 0, 1, 1, 0, 1, 79, 0.03164316476579598, 9, 1]
Checking history sample input_X_between_0_1:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6171875, 0.3164316476579597, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 29, 0, 1, 0, 1, 1, 23, 0.015858984905097274, 40, 0]
Checking history sample input_X_between_0_1:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 0.90625, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1796875, 0.15858984905097273, 0.8333333333333334, 0.0]
Checking history sample performance at 625 steps:  0.6
Checking history sample input_X:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 27, 0, 0, 1, 0, 0, 58, 0.02356583389888708, 4, 0]
Checking history sample input_X_between_0_1:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 0.84375, 0.0, 0.0, 1.0, 0.0, 0.0, 0.453125, 0.23565833898887079, 0.08333333333333333, 0.0]
Checking history sample performance at 625 steps:  0.65
Checking history sample input_X:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 18, 0, 1, 1, 0, 1, 102, 0.07944916714916157, 39, 0]
Checking history sample input_X_between_0_1:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 0.5625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.796875, 0.7944916714916157, 0.8125, 0.0]
Checking history sample performance at 625 steps:  0.62
Checking history sample input_X:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 16, 1, 0, 0, 0, 0, 57, 0.051177108415818844, 16, 1]
Checking history sample input_X_between_0_1:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4453125, 0.5117710841581884, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.71
Checking history sample input_X:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 21, 0, 1, 0, 0, 1, 80, 0.08438899074092726, 11, 1]
Checking history sample input_X_between_0_1:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 0.65625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.625, 0.8438899074092726, 0.22916666666666666, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 19, 0, 1, 1, 1, 0, 106, 0.021750066051679486, 16, 1]
Checking history sample input_X_between_0_1:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 0.59375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.828125, 0.21750066051679484, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 6, 1, 0, 0, 0, 1, 109, 0.0976089152670931, 17, 0]
Checking history sample input_X_between_0_1:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 0.1875, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8515625, 0.976089152670931, 0.3541666666666667, 0.0]
Checking history sample performance at 625 steps:  0.73
Checking history sample input_X:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 5, 0, 1, 1, 0, 1, 6, 0.045424701431822194, 31, 1]
Checking history sample input_X_between_0_1:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 0.15625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.046875, 0.4542470143182219, 0.6458333333333334, 1.0]
Checking history sample performance at 625 steps:  0.67
Checking history sample input_X:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 18, 1, 0, 0, 0, 0, 27, 0.050875722794158945, 27, 1]
Checking history sample input_X_between_0_1:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 0.5625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2109375, 0.5087572279415894, 0.5625, 1.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 25, 0, 0, 0, 1, 1, 5, 0.0932969995849881, 3, 0]
Checking history sample input_X_between_0_1:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 0.78125, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0390625, 0.932969995849881, 0.0625, 0.0]
Checking history sample performance at 625 steps:  0.65
Checking history sample input_X:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 14, 0, 0, 0, 1, 0, 14, 0.06234381073507801, 24, 1]
Checking history sample input_X_between_0_1:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 0.4375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.109375, 0.6234381073507801, 0.5, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 19, 1, 1, 0, 1, 0, 54, 0.0659080396027719, 16, 1]
Checking history sample input_X_between_0_1:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 0.59375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.421875, 0.659080396027719, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.67
Checking history sample input_X:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 30, 1, 1, 0, 1, 0, 113, 0.08743251828499332, 21, 0]
Checking history sample input_X_between_0_1:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 0.9375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8828125, 0.8743251828499332, 0.4375, 0.0]
Checking history sample performance at 625 steps:  0.63
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3797 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.8466325775872318
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.8466381325444159
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.8466308408632989
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.8466413620825513
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.8466325776282388
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.0355, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.1478, dtype=torch.float64), tensor(0.1111, dtype=torch.float64), tensor(0.2353, dtype=torch.float64), tensor(0.1017, dtype=torch.float64), tensor(0.2717, dtype=torch.float64), tensor(0.0678, dtype=torch.float64), 14, 1, 0, 1, 0, 0, 26, 0.061997739398963594, 19.612889282736287, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(4.0558e-19, dtype=torch.float64), tensor(0.0355, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.1478, dtype=torch.float64), tensor(0.1111, dtype=torch.float64), tensor(0.2353, dtype=torch.float64), tensor(0.1017, dtype=torch.float64), tensor(0.2717, dtype=torch.float64), tensor(0.0678, dtype=torch.float64), tensor(0.4378, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2061, dtype=torch.float64), tensor(0.6200, dtype=torch.float64), tensor(0.4086, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.036
  rowan_hellaswag: 0.029
  sciq: 0.148
  triviaqa: 0.111
  truthfulqa_gen: 0.235
  wikitext: 0.102
  mmlu: 0.272
  arc_challenge: 0.068

LoRA Parameters:
  lora_r: (26,)
  lora_dropout: (0.061997739398963594,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (19.612889282736287,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  26
lora dropout:  0.061997739398963594
lora alpha:  19.612889282736287
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 9,691,136 || all params: 8,039,952,384 || trainable%: 0.1205
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:09,  3.13s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:16,  1.18it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:44,  1.84it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:27,  2.46it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:20<00:26,  2.22it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:21,  2.41it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:17,  2.47it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:12,  2.83it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.54it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:03,  2.82it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.60it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.6454, 'grad_norm': 0.6929715871810913, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.502898931503296, 'eval_runtime': 8.6206, 'eval_samples_per_second': 115.886, 'eval_steps_per_second': 7.308, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:11,  3.15s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:16,  1.18it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.77it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:34,  2.19it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:28,  2.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:20<00:26,  2.20it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:20,  2.45it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:17,  2.47it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:12,  2.82it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:08,  3.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:06,  2.73it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:03,  2.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.20it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.61it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.0679, 'grad_norm': 0.41306594014167786, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.7299188375473022, 'eval_runtime': 8.63, 'eval_samples_per_second': 115.759, 'eval_steps_per_second': 7.3, 'epoch': 0.08}
{'loss': 1.7233, 'grad_norm': 0.4088399112224579, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5818723440170288, 'eval_runtime': 8.6979, 'eval_samples_per_second': 114.856, 'eval_steps_per_second': 7.243, 'epoch': 0.12}
{'loss': 1.5089, 'grad_norm': 0.3103426992893219, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.52261221408844, 'eval_runtime': 8.6973, 'eval_samples_per_second': 114.864, 'eval_steps_per_second': 7.244, 'epoch': 0.16}
{'loss': 1.5126, 'grad_norm': 0.34425419569015503, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4706089496612549, 'eval_runtime': 8.7085, 'eval_samples_per_second': 114.715, 'eval_steps_per_second': 7.234, 'epoch': 0.2}
{'loss': 1.4538, 'grad_norm': 0.3804221749305725, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4171910285949707, 'eval_runtime': 8.7213, 'eval_samples_per_second': 114.547, 'eval_steps_per_second': 7.224, 'epoch': 0.24}
{'loss': 1.3586, 'grad_norm': 0.3185378611087799, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.358973503112793, 'eval_runtime': 8.72, 'eval_samples_per_second': 114.564, 'eval_steps_per_second': 7.225, 'epoch': 0.28}
{'loss': 1.3464, 'grad_norm': 0.3393024206161499, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3370182514190674, 'eval_runtime': 8.736, 'eval_samples_per_second': 114.354, 'eval_steps_per_second': 7.212, 'epoch': 0.32}
{'loss': 1.4125, 'grad_norm': 0.3325076699256897, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3115615844726562, 'eval_runtime': 8.7477, 'eval_samples_per_second': 114.201, 'eval_steps_per_second': 7.202, 'epoch': 0.36}
{'loss': 1.2902, 'grad_norm': 0.33862537145614624, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2877599000930786, 'eval_runtime': 8.7356, 'eval_samples_per_second': 114.36, 'eval_steps_per_second': 7.212, 'epoch': 0.4}
{'loss': 1.2574, 'grad_norm': 0.34929585456848145, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2580498456954956, 'eval_runtime': 8.7275, 'eval_samples_per_second': 114.466, 'eval_steps_per_second': 7.219, 'epoch': 0.44}
{'loss': 1.2598, 'grad_norm': 0.3112754821777344, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2444865703582764, 'eval_runtime': 8.7334, 'eval_samples_per_second': 114.388, 'eval_steps_per_second': 7.214, 'epoch': 0.48}
{'loss': 1.2529, 'grad_norm': 0.35856375098228455, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2331417798995972, 'eval_runtime': 8.7327, 'eval_samples_per_second': 114.397, 'eval_steps_per_second': 7.214, 'epoch': 0.52}
{'loss': 1.2253, 'grad_norm': 0.38215693831443787, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2262606620788574, 'eval_runtime': 8.7373, 'eval_samples_per_second': 114.337, 'eval_steps_per_second': 7.21, 'epoch': 0.56}
{'loss': 1.2263, 'grad_norm': 0.3579546809196472, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2198091745376587, 'eval_runtime': 8.7593, 'eval_samples_per_second': 114.05, 'eval_steps_per_second': 7.192, 'epoch': 0.6}
{'loss': 1.1849, 'grad_norm': 0.3847469687461853, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2154107093811035, 'eval_runtime': 8.7413, 'eval_samples_per_second': 114.286, 'eval_steps_per_second': 7.207, 'epoch': 0.64}
{'loss': 1.2264, 'grad_norm': 0.3377477824687958, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2102524042129517, 'eval_runtime': 8.7398, 'eval_samples_per_second': 114.305, 'eval_steps_per_second': 7.208, 'epoch': 0.68}
{'loss': 1.2249, 'grad_norm': 0.3324485719203949, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2053096294403076, 'eval_runtime': 8.7466, 'eval_samples_per_second': 114.215, 'eval_steps_per_second': 7.203, 'epoch': 0.72}
{'loss': 1.2112, 'grad_norm': 0.37843841314315796, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2012403011322021, 'eval_runtime': 8.7461, 'eval_samples_per_second': 114.222, 'eval_steps_per_second': 7.203, 'epoch': 0.76}
{'loss': 1.2634, 'grad_norm': 0.3195929229259491, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.198928952217102, 'eval_runtime': 8.7384, 'eval_samples_per_second': 114.323, 'eval_steps_per_second': 7.21, 'epoch': 0.8}
{'loss': 1.2744, 'grad_norm': 0.3849055767059326, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1942708492279053, 'eval_runtime': 8.731, 'eval_samples_per_second': 114.42, 'eval_steps_per_second': 7.216, 'epoch': 0.84}
{'loss': 1.2251, 'grad_norm': 0.4006340205669403, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1907275915145874, 'eval_runtime': 8.7313, 'eval_samples_per_second': 114.416, 'eval_steps_per_second': 7.215, 'epoch': 0.88}
{'loss': 1.3455, 'grad_norm': 0.3150429129600525, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1887092590332031, 'eval_runtime': 8.7648, 'eval_samples_per_second': 113.978, 'eval_steps_per_second': 7.188, 'epoch': 0.92}
{'loss': 1.272, 'grad_norm': 0.30103933811187744, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1875168085098267, 'eval_runtime': 8.7518, 'eval_samples_per_second': 114.148, 'eval_steps_per_second': 7.199, 'epoch': 0.96}
{'loss': 1.181, 'grad_norm': 0.3577292263507843, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1868020296096802, 'eval_runtime': 8.7607, 'eval_samples_per_second': 114.032, 'eval_steps_per_second': 7.191, 'epoch': 1.0}
{'train_runtime': 459.9963, 'train_samples_per_second': 21.731, 'train_steps_per_second': 1.359, 'train_loss': 1.4379963012695312, 'epoch': 1.0}
train_results:  {'eval_loss': [2.502898931503296, 1.7299188375473022, 1.5818723440170288, 1.52261221408844, 1.4706089496612549, 1.4171910285949707, 1.358973503112793, 1.3370182514190674, 1.3115615844726562, 1.2877599000930786, 1.2580498456954956, 1.2444865703582764, 1.2331417798995972, 1.2262606620788574, 1.2198091745376587, 1.2154107093811035, 1.2102524042129517, 1.2053096294403076, 1.2012403011322021, 1.198928952217102, 1.1942708492279053, 1.1907275915145874, 1.1887092590332031, 1.1875168085098267, 1.1868020296096802], 'performance': [0.77, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<11:23,  6.91s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:07,  1.23it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:43,  1.54it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:24,  2.06it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:14,  2.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:41<00:08,  2.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:43<00:01,  2.95it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:43<00:00,  2.29it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.6240859627723694
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1487 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 0.8126177115111912
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 0.8126177115130447
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 0.8126177115128594
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 0.8126144894621207
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 0.8126179777936327
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4109, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), 0, tensor(0.5099, dtype=torch.float64), 0, 0, tensor(0.0510, dtype=torch.float64), 0, 1, 1, 0, 0, 0, 0, 127, 0.052758571520659714, 44.007414087214364, 1]
normalized proposed parameters for next round by BO: [tensor(6.8264e-19, dtype=torch.float64), tensor(0.4109, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(1.2942e-18, dtype=torch.float64), tensor(0.5099, dtype=torch.float64), tensor(2.5258e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0510, dtype=torch.float64), tensor(3.2123e-20, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9929, dtype=torch.float64), tensor(0.5276, dtype=torch.float64), tensor(0.9168, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.411
  rowan_hellaswag: 0.028
  sciq: 0
  triviaqa: 0.51
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.051
  arc_challenge: 0

LoRA Parameters:
  lora_r: (127,)
  lora_dropout: (0.052758571520659714,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (44.007414087214364,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  127
lora dropout:  0.052758571520659714
lora alpha:  44.007414087214364
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,040,384 || all params: 8,031,301,632 || trainable%: 0.0130
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:27,  4.52s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<00:59,  1.52it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:36,  2.26it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:27,  2.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:23,  2.80it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.91it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.81it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.85it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.5053, 'grad_norm': 0.23376090824604034, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.4006311893463135, 'eval_runtime': 8.5267, 'eval_samples_per_second': 117.161, 'eval_steps_per_second': 7.389, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:08,  3.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:48,  1.87it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:33,  2.51it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:25,  2.97it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  3.01it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.87it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.98it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  3.02it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.33it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:07,  3.46it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.71it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.01it/s]
Evaluation performance at step 50: 0.75
{'loss': 3.2634, 'grad_norm': 0.30800166726112366, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 3.0305087566375732, 'eval_runtime': 8.5191, 'eval_samples_per_second': 117.266, 'eval_steps_per_second': 7.395, 'epoch': 0.08}
{'loss': 2.8992, 'grad_norm': 0.5617011785507202, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.7612993717193604, 'eval_runtime': 8.5468, 'eval_samples_per_second': 116.886, 'eval_steps_per_second': 7.371, 'epoch': 0.12}
{'loss': 2.7875, 'grad_norm': 0.2942485213279724, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.642518997192383, 'eval_runtime': 8.588, 'eval_samples_per_second': 116.326, 'eval_steps_per_second': 7.336, 'epoch': 0.16}
{'loss': 2.6473, 'grad_norm': 0.1395755261182785, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.5542609691619873, 'eval_runtime': 8.6323, 'eval_samples_per_second': 115.728, 'eval_steps_per_second': 7.298, 'epoch': 0.2}
{'loss': 2.4923, 'grad_norm': 0.6989545822143555, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.4595236778259277, 'eval_runtime': 8.6544, 'eval_samples_per_second': 115.433, 'eval_steps_per_second': 7.28, 'epoch': 0.24}
{'loss': 2.4528, 'grad_norm': 0.7451454401016235, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.4128055572509766, 'eval_runtime': 8.6512, 'eval_samples_per_second': 115.475, 'eval_steps_per_second': 7.282, 'epoch': 0.28}
{'loss': 2.4336, 'grad_norm': 1.0177181959152222, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.373164653778076, 'eval_runtime': 8.6591, 'eval_samples_per_second': 115.37, 'eval_steps_per_second': 7.276, 'epoch': 0.32}
{'loss': 2.4256, 'grad_norm': 1.2148621082305908, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.3520541191101074, 'eval_runtime': 8.6631, 'eval_samples_per_second': 115.316, 'eval_steps_per_second': 7.272, 'epoch': 0.36}
{'loss': 2.3339, 'grad_norm': 0.6996058821678162, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.3323771953582764, 'eval_runtime': 8.6457, 'eval_samples_per_second': 115.549, 'eval_steps_per_second': 7.287, 'epoch': 0.4}
{'loss': 2.3285, 'grad_norm': 0.2743896543979645, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.313450574874878, 'eval_runtime': 8.665, 'eval_samples_per_second': 115.291, 'eval_steps_per_second': 7.271, 'epoch': 0.44}
{'loss': 2.2758, 'grad_norm': 0.26228246092796326, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.2969553470611572, 'eval_runtime': 8.6488, 'eval_samples_per_second': 115.507, 'eval_steps_per_second': 7.284, 'epoch': 0.48}
{'loss': 2.3473, 'grad_norm': 0.33154261112213135, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.280393600463867, 'eval_runtime': 8.6402, 'eval_samples_per_second': 115.623, 'eval_steps_per_second': 7.292, 'epoch': 0.52}
{'loss': 2.302, 'grad_norm': 0.15764540433883667, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.266317129135132, 'eval_runtime': 8.6501, 'eval_samples_per_second': 115.49, 'eval_steps_per_second': 7.283, 'epoch': 0.56}
{'loss': 2.2377, 'grad_norm': 0.31326615810394287, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.252121686935425, 'eval_runtime': 8.6523, 'eval_samples_per_second': 115.46, 'eval_steps_per_second': 7.281, 'epoch': 0.6}
{'loss': 2.2732, 'grad_norm': 0.2626967430114746, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.2398338317871094, 'eval_runtime': 8.644, 'eval_samples_per_second': 115.572, 'eval_steps_per_second': 7.288, 'epoch': 0.64}
{'loss': 2.2749, 'grad_norm': 0.3248816728591919, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2278597354888916, 'eval_runtime': 8.6229, 'eval_samples_per_second': 115.854, 'eval_steps_per_second': 7.306, 'epoch': 0.68}
{'loss': 2.2548, 'grad_norm': 0.27402791380882263, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.2178359031677246, 'eval_runtime': 8.6385, 'eval_samples_per_second': 115.645, 'eval_steps_per_second': 7.293, 'epoch': 0.72}
{'loss': 2.2059, 'grad_norm': 0.12366136163473129, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.20809268951416, 'eval_runtime': 8.6278, 'eval_samples_per_second': 115.788, 'eval_steps_per_second': 7.302, 'epoch': 0.76}
{'loss': 2.2185, 'grad_norm': 0.14195197820663452, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.199824333190918, 'eval_runtime': 8.6473, 'eval_samples_per_second': 115.528, 'eval_steps_per_second': 7.286, 'epoch': 0.8}
{'loss': 2.2622, 'grad_norm': 0.34970879554748535, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.1922905445098877, 'eval_runtime': 8.6416, 'eval_samples_per_second': 115.604, 'eval_steps_per_second': 7.29, 'epoch': 0.84}
{'loss': 2.2015, 'grad_norm': 0.17210662364959717, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.187633991241455, 'eval_runtime': 8.6157, 'eval_samples_per_second': 115.951, 'eval_steps_per_second': 7.312, 'epoch': 0.88}
{'loss': 2.1538, 'grad_norm': 0.19644612073898315, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.184689998626709, 'eval_runtime': 8.6452, 'eval_samples_per_second': 115.555, 'eval_steps_per_second': 7.287, 'epoch': 0.92}
{'loss': 2.1988, 'grad_norm': 0.15614768862724304, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1810641288757324, 'eval_runtime': 8.6483, 'eval_samples_per_second': 115.514, 'eval_steps_per_second': 7.285, 'epoch': 0.96}
{'loss': 2.2034, 'grad_norm': 0.143940269947052, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.1801846027374268, 'eval_runtime': 8.6255, 'eval_samples_per_second': 115.82, 'eval_steps_per_second': 7.304, 'epoch': 1.0}
{'train_runtime': 508.0385, 'train_samples_per_second': 19.68, 'train_steps_per_second': 1.23, 'train_loss': 2.439165985107422, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4006311893463135, 3.0305087566375732, 2.7612993717193604, 2.642518997192383, 2.5542609691619873, 2.4595236778259277, 2.4128055572509766, 2.373164653778076, 2.3520541191101074, 2.3323771953582764, 2.313450574874878, 2.2969553470611572, 2.280393600463867, 2.266317129135132, 2.252121686935425, 2.2398338317871094, 2.2278597354888916, 2.2178359031677246, 2.20809268951416, 2.199824333190918, 2.1922905445098877, 2.187633991241455, 2.184689998626709, 2.1810641288757324, 2.1801846027374268], 'performance': [0.77, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:51,  9.61s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:44,  1.49it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:24,  2.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:13,  2.51it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:06,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:00,  3.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.50it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.7939552068710327
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7449 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 0.7978049375888842
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 0.7972199220555363
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 0.7978050140013999
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 0.7978304915478247
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.7978251707833981
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4139, dtype=torch.float64), 0, 0, 0, tensor(0.0397, dtype=torch.float64), 0, tensor(0.2000, dtype=torch.float64), tensor(0.1639, dtype=torch.float64), tensor(0.1762, dtype=torch.float64), 32, 1, 0, 0, 0, 0, 5, 0.0337295915827445, 1.5076740768437955, 1]
normalized proposed parameters for next round by BO: [tensor(0.4139, dtype=torch.float64), tensor(6.8912e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0064, dtype=torch.float64), tensor(0.0397, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2000, dtype=torch.float64), tensor(0.1639, dtype=torch.float64), tensor(0.1762, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0425, dtype=torch.float64), tensor(0.3373, dtype=torch.float64), tensor(0.0314, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.414
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.04
  truthfulqa_gen: 0
  wikitext: 0.2
  mmlu: 0.164
  arc_challenge: 0.176

LoRA Parameters:
  lora_r: (5,)
  lora_dropout: (0.0337295915827445,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (1.5076740768437955,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  5
lora dropout:  0.0337295915827445
lora alpha:  1.5076740768437955
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,310,720 || all params: 8,031,571,968 || trainable%: 0.0163
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9934
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  993
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:04,  4.29s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:25,  1.06it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:47,  1.74it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:33,  2.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:17<00:27,  2.40it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:20<00:23,  2.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:19,  2.59it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:16,  2.55it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:12,  2.80it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:09,  2.93it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:35<00:07,  2.40it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:37<00:04,  2.69it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:01,  2.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.51it/s]
Evaluation performance at step 25: 0.78
{'loss': 4.2625, 'grad_norm': 0.5273153185844421, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 4.10841703414917, 'eval_runtime': 8.1507, 'eval_samples_per_second': 121.831, 'eval_steps_per_second': 7.729, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:46,  3.50s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:51,  1.78it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:35,  2.37it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.80it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:22,  2.93it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:26,  2.27it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:20,  2.51it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:16,  2.54it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:12,  2.89it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:09,  2.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.42it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:04,  2.71it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.70it/s]
Evaluation performance at step 50: 0.81
{'loss': 3.4852, 'grad_norm': 0.5343758463859558, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.81}
{'eval_loss': 2.9096357822418213, 'eval_runtime': 7.2993, 'eval_samples_per_second': 136.04, 'eval_steps_per_second': 8.631, 'epoch': 0.08}
{'loss': 2.5466, 'grad_norm': 0.22162848711013794, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 2.2314467430114746, 'eval_runtime': 7.3265, 'eval_samples_per_second': 135.536, 'eval_steps_per_second': 8.599, 'epoch': 0.12}
{'loss': 2.0926, 'grad_norm': 0.29348981380462646, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.806984782218933, 'eval_runtime': 7.3181, 'eval_samples_per_second': 135.69, 'eval_steps_per_second': 8.609, 'epoch': 0.16}
{'loss': 1.7603, 'grad_norm': 0.18173129856586456, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.6809533834457397, 'eval_runtime': 7.3484, 'eval_samples_per_second': 135.131, 'eval_steps_per_second': 8.573, 'epoch': 0.2}
{'loss': 1.6935, 'grad_norm': 0.18663613498210907, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.6581655740737915, 'eval_runtime': 7.3675, 'eval_samples_per_second': 134.78, 'eval_steps_per_second': 8.551, 'epoch': 0.24}
{'loss': 1.6474, 'grad_norm': 0.2036575824022293, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.639129877090454, 'eval_runtime': 7.3491, 'eval_samples_per_second': 135.119, 'eval_steps_per_second': 8.573, 'epoch': 0.28}
{'loss': 1.676, 'grad_norm': 0.17939676344394684, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.6288014650344849, 'eval_runtime': 7.3542, 'eval_samples_per_second': 135.025, 'eval_steps_per_second': 8.567, 'epoch': 0.32}
{'loss': 1.6816, 'grad_norm': 0.15104059875011444, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.6190996170043945, 'eval_runtime': 7.3496, 'eval_samples_per_second': 135.109, 'eval_steps_per_second': 8.572, 'epoch': 0.36}
{'loss': 1.5852, 'grad_norm': 0.18853703141212463, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.611372709274292, 'eval_runtime': 7.3499, 'eval_samples_per_second': 135.104, 'eval_steps_per_second': 8.572, 'epoch': 0.4}
{'loss': 1.603, 'grad_norm': 0.18327799439430237, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.60343599319458, 'eval_runtime': 7.3459, 'eval_samples_per_second': 135.178, 'eval_steps_per_second': 8.576, 'epoch': 0.44}
{'loss': 1.7, 'grad_norm': 0.1360068917274475, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.5975778102874756, 'eval_runtime': 7.3366, 'eval_samples_per_second': 135.35, 'eval_steps_per_second': 8.587, 'epoch': 0.48}
{'loss': 1.5711, 'grad_norm': 0.1774175763130188, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.5914218425750732, 'eval_runtime': 7.3485, 'eval_samples_per_second': 135.13, 'eval_steps_per_second': 8.573, 'epoch': 0.52}
{'loss': 1.5577, 'grad_norm': 0.14067880809307098, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.586065649986267, 'eval_runtime': 7.3485, 'eval_samples_per_second': 135.13, 'eval_steps_per_second': 8.573, 'epoch': 0.56}
{'loss': 1.5671, 'grad_norm': 0.15860694646835327, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.5813252925872803, 'eval_runtime': 7.3337, 'eval_samples_per_second': 135.402, 'eval_steps_per_second': 8.59, 'epoch': 0.6}
{'loss': 1.5908, 'grad_norm': 0.27538883686065674, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.5766220092773438, 'eval_runtime': 7.3466, 'eval_samples_per_second': 135.164, 'eval_steps_per_second': 8.575, 'epoch': 0.64}
{'loss': 1.6203, 'grad_norm': 0.19380851089954376, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.5738142728805542, 'eval_runtime': 7.3414, 'eval_samples_per_second': 135.261, 'eval_steps_per_second': 8.582, 'epoch': 0.68}
{'loss': 1.5935, 'grad_norm': 0.23151148855686188, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.5694234371185303, 'eval_runtime': 7.3618, 'eval_samples_per_second': 134.885, 'eval_steps_per_second': 8.558, 'epoch': 0.72}
{'loss': 1.5493, 'grad_norm': 0.17854629456996918, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.5670489072799683, 'eval_runtime': 7.3518, 'eval_samples_per_second': 135.07, 'eval_steps_per_second': 8.569, 'epoch': 0.76}
{'loss': 1.586, 'grad_norm': 0.16025608777999878, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.5648386478424072, 'eval_runtime': 7.362, 'eval_samples_per_second': 134.882, 'eval_steps_per_second': 8.557, 'epoch': 0.81}
{'loss': 1.5605, 'grad_norm': 0.16530965268611908, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.562901496887207, 'eval_runtime': 7.3714, 'eval_samples_per_second': 134.711, 'eval_steps_per_second': 8.547, 'epoch': 0.85}
{'loss': 1.6421, 'grad_norm': 0.21486449241638184, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.561667561531067, 'eval_runtime': 7.3606, 'eval_samples_per_second': 134.907, 'eval_steps_per_second': 8.559, 'epoch': 0.89}
{'loss': 1.5367, 'grad_norm': 0.15787462890148163, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.560351848602295, 'eval_runtime': 7.3612, 'eval_samples_per_second': 134.896, 'eval_steps_per_second': 8.558, 'epoch': 0.93}
{'loss': 1.6029, 'grad_norm': 0.19499504566192627, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.5598156452178955, 'eval_runtime': 7.3831, 'eval_samples_per_second': 134.496, 'eval_steps_per_second': 8.533, 'epoch': 0.97}
{'train_runtime': 453.8628, 'train_samples_per_second': 21.888, 'train_steps_per_second': 1.368, 'train_loss': 1.852434462395267, 'epoch': 1.0}
train_results:  {'eval_loss': [4.10841703414917, 2.9096357822418213, 2.2314467430114746, 1.806984782218933, 1.6809533834457397, 1.6581655740737915, 1.639129877090454, 1.6288014650344849, 1.6190996170043945, 1.611372709274292, 1.60343599319458, 1.5975778102874756, 1.5914218425750732, 1.586065649986267, 1.5813252925872803, 1.5766220092773438, 1.5738142728805542, 1.5694234371185303, 1.5670489072799683, 1.5648386478424072, 1.562901496887207, 1.561667561531067, 1.560351848602295, 1.5598156452178955], 'performance': [0.78, 0.81]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<16:21,  9.92s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:05,  1.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:42,  1.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:24,  2.06it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:36<00:16,  2.11it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:41<00:07,  2.48it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:43<00:00,  3.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:43<00:00,  2.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.81]
current iteration observed (possibly low-fid or predicted) performance:  0.6125386953353882
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8663 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.7863166225818282
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 0.7863166225818282
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 0.7863166225818282
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 0.7863166225818282
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 0.7863166225818282
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0541, dtype=torch.float64), 0, tensor(0.1720, dtype=torch.float64), tensor(0.7164, dtype=torch.float64), 0, 0, 0, tensor(0.0576, dtype=torch.float64), 9, 1, 0, 0, 0, 0, 128, 0.04512015437766561, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0541, dtype=torch.float64), tensor(1.0398e-17, dtype=torch.float64), tensor(0.1720, dtype=torch.float64), tensor(0.7164, dtype=torch.float64), tensor(8.7778e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9554e-18, dtype=torch.float64), tensor(0.0576, dtype=torch.float64), tensor(0.2793, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4512, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.054
  rowan_hellaswag: 0
  sciq: 0.172
  triviaqa: 0.716
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.058

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04512015437766561,)
  num_layers_to_apply: (9,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  9
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  128
lora dropout:  0.04512015437766561
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 9,437,184 || all params: 8,039,698,432 || trainable%: 0.1174
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:03,  3.06s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:48,  1.89it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:33,  2.49it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:25,  2.93it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.05it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:19,  3.04it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:16,  3.03it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.91it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:08,  3.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.00it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.8317, 'grad_norm': 0.9068865180015564, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.866339683532715, 'eval_runtime': 5.6704, 'eval_samples_per_second': 176.178, 'eval_steps_per_second': 11.11, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:05,  3.09s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<01:00,  1.51it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:38,  2.17it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:28,  2.65it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:31,  2.16it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:24,  2.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.65it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.94it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.88it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.74it/s]
Evaluation performance at step 50: 0.72
{'loss': 2.8538, 'grad_norm': 0.39353400468826294, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.72}
{'eval_loss': 2.437265396118164, 'eval_runtime': 5.6606, 'eval_samples_per_second': 176.484, 'eval_steps_per_second': 11.13, 'epoch': 0.08}
{'loss': 2.2549, 'grad_norm': 0.3753587603569031, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0861308574676514, 'eval_runtime': 5.6624, 'eval_samples_per_second': 176.427, 'eval_steps_per_second': 11.126, 'epoch': 0.12}
{'loss': 1.9057, 'grad_norm': 0.3988616466522217, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8103587627410889, 'eval_runtime': 5.6758, 'eval_samples_per_second': 176.011, 'eval_steps_per_second': 11.1, 'epoch': 0.16}
{'loss': 1.7826, 'grad_norm': 0.3786800801753998, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7343480587005615, 'eval_runtime': 5.6815, 'eval_samples_per_second': 175.835, 'eval_steps_per_second': 11.089, 'epoch': 0.2}
{'loss': 1.7059, 'grad_norm': 0.4388692080974579, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7114177942276, 'eval_runtime': 5.6903, 'eval_samples_per_second': 175.563, 'eval_steps_per_second': 11.072, 'epoch': 0.24}
{'loss': 1.6996, 'grad_norm': 0.45022040605545044, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.688913345336914, 'eval_runtime': 5.6899, 'eval_samples_per_second': 175.576, 'eval_steps_per_second': 11.072, 'epoch': 0.28}
{'loss': 1.6875, 'grad_norm': 0.45677638053894043, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6776063442230225, 'eval_runtime': 5.6932, 'eval_samples_per_second': 175.472, 'eval_steps_per_second': 11.066, 'epoch': 0.32}
{'loss': 1.6873, 'grad_norm': 0.4497535824775696, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6635642051696777, 'eval_runtime': 5.6989, 'eval_samples_per_second': 175.297, 'eval_steps_per_second': 11.055, 'epoch': 0.36}
{'loss': 1.6581, 'grad_norm': 0.4286321699619293, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6473087072372437, 'eval_runtime': 5.7014, 'eval_samples_per_second': 175.22, 'eval_steps_per_second': 11.05, 'epoch': 0.4}
{'loss': 1.6317, 'grad_norm': 0.33989232778549194, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6356884241104126, 'eval_runtime': 5.7116, 'eval_samples_per_second': 174.907, 'eval_steps_per_second': 11.03, 'epoch': 0.44}
{'loss': 1.6045, 'grad_norm': 0.3404797315597534, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.625494360923767, 'eval_runtime': 5.7162, 'eval_samples_per_second': 174.767, 'eval_steps_per_second': 11.021, 'epoch': 0.48}
{'loss': 1.6254, 'grad_norm': 0.2961495816707611, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6195063591003418, 'eval_runtime': 5.7161, 'eval_samples_per_second': 174.769, 'eval_steps_per_second': 11.021, 'epoch': 0.52}
{'loss': 1.6162, 'grad_norm': 0.4912014603614807, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6124825477600098, 'eval_runtime': 5.7114, 'eval_samples_per_second': 174.914, 'eval_steps_per_second': 11.031, 'epoch': 0.56}
{'loss': 1.632, 'grad_norm': 0.3426720201969147, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6079238653182983, 'eval_runtime': 5.722, 'eval_samples_per_second': 174.588, 'eval_steps_per_second': 11.01, 'epoch': 0.6}
{'loss': 1.5969, 'grad_norm': 0.4232316315174103, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6073542833328247, 'eval_runtime': 5.7373, 'eval_samples_per_second': 174.125, 'eval_steps_per_second': 10.981, 'epoch': 0.64}
{'loss': 1.6024, 'grad_norm': 0.32978585362434387, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6008821725845337, 'eval_runtime': 5.7258, 'eval_samples_per_second': 174.472, 'eval_steps_per_second': 11.003, 'epoch': 0.68}
{'loss': 1.5618, 'grad_norm': 0.27963823080062866, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5964235067367554, 'eval_runtime': 5.7307, 'eval_samples_per_second': 174.325, 'eval_steps_per_second': 10.993, 'epoch': 0.72}
{'loss': 1.5921, 'grad_norm': 0.27792906761169434, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5944228172302246, 'eval_runtime': 5.7146, 'eval_samples_per_second': 174.815, 'eval_steps_per_second': 11.024, 'epoch': 0.76}
{'loss': 1.58, 'grad_norm': 0.340541273355484, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5919004678726196, 'eval_runtime': 5.7311, 'eval_samples_per_second': 174.313, 'eval_steps_per_second': 10.993, 'epoch': 0.8}
{'loss': 1.5884, 'grad_norm': 0.4568660855293274, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5901172161102295, 'eval_runtime': 5.7128, 'eval_samples_per_second': 174.871, 'eval_steps_per_second': 11.028, 'epoch': 0.84}
{'loss': 1.6019, 'grad_norm': 0.2260223776102066, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5886973142623901, 'eval_runtime': 5.7135, 'eval_samples_per_second': 174.849, 'eval_steps_per_second': 11.027, 'epoch': 0.88}
{'loss': 1.5642, 'grad_norm': 0.2738579511642456, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5867953300476074, 'eval_runtime': 5.7204, 'eval_samples_per_second': 174.637, 'eval_steps_per_second': 11.013, 'epoch': 0.92}
{'loss': 1.5687, 'grad_norm': 0.29796552658081055, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5863490104675293, 'eval_runtime': 5.7347, 'eval_samples_per_second': 174.203, 'eval_steps_per_second': 10.986, 'epoch': 0.96}
{'loss': 1.6009, 'grad_norm': 0.3633973300457001, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5852562189102173, 'eval_runtime': 5.7058, 'eval_samples_per_second': 175.085, 'eval_steps_per_second': 11.041, 'epoch': 1.0}
{'train_runtime': 382.0122, 'train_samples_per_second': 26.169, 'train_steps_per_second': 1.636, 'train_loss': 1.8413620422363282, 'epoch': 1.0}
train_results:  {'eval_loss': [3.866339683532715, 2.437265396118164, 2.0861308574676514, 1.8103587627410889, 1.7343480587005615, 1.7114177942276, 1.688913345336914, 1.6776063442230225, 1.6635642051696777, 1.6473087072372437, 1.6356884241104126, 1.625494360923767, 1.6195063591003418, 1.6124825477600098, 1.6079238653182983, 1.6073542833328247, 1.6008821725845337, 1.5964235067367554, 1.5944228172302246, 1.5919004678726196, 1.5901172161102295, 1.5886973142623901, 1.5867953300476074, 1.5863490104675293, 1.5852562189102173], 'performance': [0.74, 0.72]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<11:20,  6.88s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:13<00:56,  1.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:21<00:38,  1.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:24,  2.10it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:31<00:14,  2.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:39<00:08,  2.32it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:41<00:00,  3.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:41<00:00,  2.42it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.72]
current iteration observed (possibly low-fid or predicted) performance:  0.7319226264953613
current iteration best possible performance (full train run):  0.735
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4577 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 0.7953964550323316
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 0.7953792911767095
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 0.7953792911767095
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.7953776857873192
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.7950293590157695
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3748, dtype=torch.float64), tensor(0.1778, dtype=torch.float64), tensor(0.0651, dtype=torch.float64), 0, tensor(0.2383, dtype=torch.float64), 0, tensor(0.0187, dtype=torch.float64), 0, tensor(0.1253, dtype=torch.float64), 1, 1, 1, 0, 1, 1, 128, 0.014526220082807142, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.3748, dtype=torch.float64), tensor(0.1778, dtype=torch.float64), tensor(0.0651, dtype=torch.float64), tensor(7.2336e-19, dtype=torch.float64), tensor(0.2383, dtype=torch.float64), tensor(6.4216e-19, dtype=torch.float64), tensor(0.0187, dtype=torch.float64), tensor(2.1486e-19, dtype=torch.float64), tensor(0.1253, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1453, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.375
  gsm8k: 0.178
  rowan_hellaswag: 0.065
  sciq: 0
  triviaqa: 0.238
  truthfulqa_gen: 0
  wikitext: 0.019
  mmlu: 0
  arc_challenge: 0.125

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.014526220082807142,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.014526220082807142
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 6,422,528 || all params: 8,036,683,776 || trainable%: 0.0799
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:37,  2.80s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.97it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.75it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:19,  2.60it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.09it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.79it/s]
Evaluation performance at step 25: 0.75
{'loss': 3.6667, 'grad_norm': 4.537247180938721, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.031524419784546, 'eval_runtime': 8.2164, 'eval_samples_per_second': 121.587, 'eval_steps_per_second': 7.668, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:03,  2.46s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:04<00:43,  2.10it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.67it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:11<00:21,  3.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:14<00:20,  2.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:17,  2.95it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.20it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:08,  3.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.93it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:32<00:00,  3.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:32<00:00,  3.04it/s]
Evaluation performance at step 50: 0.79
{'loss': 2.4654, 'grad_norm': 1.8134945631027222, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.79}
{'eval_loss': 2.1695401668548584, 'eval_runtime': 8.1897, 'eval_samples_per_second': 121.983, 'eval_steps_per_second': 7.693, 'epoch': 0.08}
{'loss': 1.8731, 'grad_norm': 1.1110222339630127, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6539902687072754, 'eval_runtime': 8.2352, 'eval_samples_per_second': 121.309, 'eval_steps_per_second': 7.65, 'epoch': 0.12}
{'loss': 1.5494, 'grad_norm': 0.691384494304657, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5238994359970093, 'eval_runtime': 8.2441, 'eval_samples_per_second': 121.177, 'eval_steps_per_second': 7.642, 'epoch': 0.16}
{'loss': 1.4304, 'grad_norm': 0.6649298071861267, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.383925437927246, 'eval_runtime': 8.2407, 'eval_samples_per_second': 121.227, 'eval_steps_per_second': 7.645, 'epoch': 0.2}
{'loss': 1.3543, 'grad_norm': 0.5987414121627808, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3401304483413696, 'eval_runtime': 8.2382, 'eval_samples_per_second': 121.264, 'eval_steps_per_second': 7.647, 'epoch': 0.24}
{'loss': 1.3251, 'grad_norm': 0.5151081681251526, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3102999925613403, 'eval_runtime': 8.2333, 'eval_samples_per_second': 121.337, 'eval_steps_per_second': 7.652, 'epoch': 0.28}
{'loss': 1.2684, 'grad_norm': 0.514723539352417, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2843773365020752, 'eval_runtime': 8.2356, 'eval_samples_per_second': 121.303, 'eval_steps_per_second': 7.65, 'epoch': 0.32}
{'loss': 1.3087, 'grad_norm': 0.7688680291175842, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2391397953033447, 'eval_runtime': 8.2231, 'eval_samples_per_second': 121.487, 'eval_steps_per_second': 7.661, 'epoch': 0.36}
{'loss': 1.1936, 'grad_norm': 0.9219130277633667, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2304059267044067, 'eval_runtime': 8.2307, 'eval_samples_per_second': 121.375, 'eval_steps_per_second': 7.654, 'epoch': 0.4}
{'loss': 1.2038, 'grad_norm': 0.5781376957893372, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2221112251281738, 'eval_runtime': 8.2296, 'eval_samples_per_second': 121.391, 'eval_steps_per_second': 7.655, 'epoch': 0.44}
{'loss': 1.2135, 'grad_norm': 0.6116816997528076, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2067607641220093, 'eval_runtime': 8.2381, 'eval_samples_per_second': 121.265, 'eval_steps_per_second': 7.647, 'epoch': 0.48}
{'loss': 1.1538, 'grad_norm': 0.5030052065849304, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2009658813476562, 'eval_runtime': 8.2615, 'eval_samples_per_second': 120.923, 'eval_steps_per_second': 7.626, 'epoch': 0.52}
{'loss': 1.1416, 'grad_norm': 0.5675360560417175, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1944819688796997, 'eval_runtime': 8.2371, 'eval_samples_per_second': 121.281, 'eval_steps_per_second': 7.648, 'epoch': 0.56}
{'loss': 1.1711, 'grad_norm': 0.7342604994773865, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1900956630706787, 'eval_runtime': 8.2338, 'eval_samples_per_second': 121.329, 'eval_steps_per_second': 7.651, 'epoch': 0.6}
{'loss': 1.1742, 'grad_norm': 0.8527205586433411, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1857765913009644, 'eval_runtime': 8.2449, 'eval_samples_per_second': 121.165, 'eval_steps_per_second': 7.641, 'epoch': 0.64}
{'loss': 1.1664, 'grad_norm': 0.43735364079475403, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1790449619293213, 'eval_runtime': 8.239, 'eval_samples_per_second': 121.253, 'eval_steps_per_second': 7.647, 'epoch': 0.68}
{'loss': 1.2188, 'grad_norm': 0.6066808104515076, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1759239435195923, 'eval_runtime': 8.2334, 'eval_samples_per_second': 121.335, 'eval_steps_per_second': 7.652, 'epoch': 0.72}
{'loss': 1.1797, 'grad_norm': 0.5243876576423645, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1735514402389526, 'eval_runtime': 8.2302, 'eval_samples_per_second': 121.382, 'eval_steps_per_second': 7.655, 'epoch': 0.76}
{'loss': 1.1625, 'grad_norm': 0.5570888519287109, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1709680557250977, 'eval_runtime': 8.2425, 'eval_samples_per_second': 121.202, 'eval_steps_per_second': 7.643, 'epoch': 0.8}
{'loss': 1.1888, 'grad_norm': 0.6527916193008423, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.168609857559204, 'eval_runtime': 8.2331, 'eval_samples_per_second': 121.339, 'eval_steps_per_second': 7.652, 'epoch': 0.84}
{'loss': 1.1494, 'grad_norm': 0.5229904651641846, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1653666496276855, 'eval_runtime': 8.2424, 'eval_samples_per_second': 121.202, 'eval_steps_per_second': 7.643, 'epoch': 0.88}
{'loss': 1.154, 'grad_norm': 0.6417436599731445, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1624436378479004, 'eval_runtime': 8.2234, 'eval_samples_per_second': 121.482, 'eval_steps_per_second': 7.661, 'epoch': 0.92}
{'loss': 1.1513, 'grad_norm': 0.4636130630970001, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1608006954193115, 'eval_runtime': 8.2366, 'eval_samples_per_second': 121.288, 'eval_steps_per_second': 7.649, 'epoch': 0.96}
{'loss': 1.146, 'grad_norm': 0.6475090980529785, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.16020667552948, 'eval_runtime': 8.2355, 'eval_samples_per_second': 121.304, 'eval_steps_per_second': 7.65, 'epoch': 1.0}
{'train_runtime': 489.3027, 'train_samples_per_second': 20.431, 'train_steps_per_second': 1.277, 'train_loss': 1.4003963409423827, 'epoch': 1.0}
train_results:  {'eval_loss': [3.031524419784546, 2.1695401668548584, 1.6539902687072754, 1.5238994359970093, 1.383925437927246, 1.3401304483413696, 1.3102999925613403, 1.2843773365020752, 1.2391397953033447, 1.2304059267044067, 1.2221112251281738, 1.2067607641220093, 1.2009658813476562, 1.1944819688796997, 1.1900956630706787, 1.1857765913009644, 1.1790449619293213, 1.1759239435195923, 1.1735514402389526, 1.1709680557250977, 1.168609857559204, 1.1653666496276855, 1.1624436378479004, 1.1608006954193115, 1.16020667552948], 'performance': [0.75, 0.79]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<10:52,  6.59s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:05,  1.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:20<00:35,  1.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:24<00:20,  2.46it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:11,  2.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:05,  3.36it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  4.34it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.97it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.79]
current iteration observed (possibly low-fid or predicted) performance:  0.7897405624389648
current iteration best possible performance (full train run):  0.756
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.7519 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 0.7863461968805359
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 0.7863399693045907
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 0.7872882268689658
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.7863461968805359
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 0.7863461968795742
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.7449, dtype=torch.float64), tensor(0.0814, dtype=torch.float64), 0, tensor(0.0203, dtype=torch.float64), 0, 0, tensor(0.1046, dtype=torch.float64), tensor(0.0489, dtype=torch.float64), 1, 1, 0, 0, 1, 0, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.7449, dtype=torch.float64), tensor(0.0814, dtype=torch.float64), tensor(4.4329e-18, dtype=torch.float64), tensor(0.0203, dtype=torch.float64), tensor(5.9138e-19, dtype=torch.float64), tensor(6.9919e-18, dtype=torch.float64), tensor(0.1046, dtype=torch.float64), tensor(0.0489, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.745
  rowan_hellaswag: 0.081
  sciq: 0
  triviaqa: 0.02
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.105
  arc_challenge: 0.049

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:42,  5.27s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:29,  1.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:48,  1.71it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:33,  2.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:25,  2.59it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:18,  2.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:14,  2.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:11,  3.13it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.65it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.68it/s]
Evaluation performance at step 25: 0.77
{'loss': 2.795, 'grad_norm': 0.835178554058075, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.38110613822937, 'eval_runtime': 8.8643, 'eval_samples_per_second': 112.7, 'eval_steps_per_second': 7.107, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:02,  2.45s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:42,  1.97it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.65it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.77it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:20,  2.51it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:16,  2.68it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  2.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.17it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.76it/s]
Evaluation performance at step 50: 0.78
{'loss': 1.9782, 'grad_norm': 0.6858806610107422, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 1.6893634796142578, 'eval_runtime': 8.8555, 'eval_samples_per_second': 112.811, 'eval_steps_per_second': 7.114, 'epoch': 0.08}
{'loss': 1.6364, 'grad_norm': 0.9837023615837097, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4574785232543945, 'eval_runtime': 8.8987, 'eval_samples_per_second': 112.264, 'eval_steps_per_second': 7.08, 'epoch': 0.12}
{'loss': 1.4919, 'grad_norm': 0.8437081575393677, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3741666078567505, 'eval_runtime': 8.9466, 'eval_samples_per_second': 111.663, 'eval_steps_per_second': 7.042, 'epoch': 0.16}
{'loss': 1.4011, 'grad_norm': 1.0183055400848389, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.327687382698059, 'eval_runtime': 8.9775, 'eval_samples_per_second': 111.279, 'eval_steps_per_second': 7.018, 'epoch': 0.2}
{'loss': 1.3208, 'grad_norm': 0.7933931946754456, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.298951268196106, 'eval_runtime': 8.972, 'eval_samples_per_second': 111.346, 'eval_steps_per_second': 7.022, 'epoch': 0.24}
{'loss': 1.2849, 'grad_norm': 0.7654902338981628, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.270066261291504, 'eval_runtime': 8.9861, 'eval_samples_per_second': 111.172, 'eval_steps_per_second': 7.011, 'epoch': 0.28}
{'loss': 1.2679, 'grad_norm': 0.6374661326408386, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2363475561141968, 'eval_runtime': 8.9891, 'eval_samples_per_second': 111.135, 'eval_steps_per_second': 7.009, 'epoch': 0.32}
{'loss': 1.2667, 'grad_norm': 0.6923930644989014, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2202380895614624, 'eval_runtime': 8.9944, 'eval_samples_per_second': 111.069, 'eval_steps_per_second': 7.004, 'epoch': 0.36}
{'loss': 1.2361, 'grad_norm': 0.7674503326416016, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2058606147766113, 'eval_runtime': 9.0221, 'eval_samples_per_second': 110.728, 'eval_steps_per_second': 6.983, 'epoch': 0.4}
{'loss': 1.2307, 'grad_norm': 0.8209730386734009, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1957316398620605, 'eval_runtime': 9.0079, 'eval_samples_per_second': 110.902, 'eval_steps_per_second': 6.994, 'epoch': 0.44}
{'loss': 1.202, 'grad_norm': 0.49689826369285583, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1808851957321167, 'eval_runtime': 8.9954, 'eval_samples_per_second': 111.056, 'eval_steps_per_second': 7.004, 'epoch': 0.48}
{'loss': 1.1688, 'grad_norm': 0.6666645407676697, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1696487665176392, 'eval_runtime': 8.9892, 'eval_samples_per_second': 111.134, 'eval_steps_per_second': 7.008, 'epoch': 0.52}
{'loss': 1.196, 'grad_norm': 0.6197577714920044, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1558784246444702, 'eval_runtime': 9.0385, 'eval_samples_per_second': 110.528, 'eval_steps_per_second': 6.97, 'epoch': 0.56}
{'loss': 1.1905, 'grad_norm': 0.6668899059295654, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1493998765945435, 'eval_runtime': 9.0583, 'eval_samples_per_second': 110.286, 'eval_steps_per_second': 6.955, 'epoch': 0.6}
{'loss': 1.1325, 'grad_norm': 0.5857388377189636, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1382815837860107, 'eval_runtime': 9.0384, 'eval_samples_per_second': 110.529, 'eval_steps_per_second': 6.97, 'epoch': 0.64}
{'loss': 1.1564, 'grad_norm': 0.5604771375656128, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1345032453536987, 'eval_runtime': 8.9974, 'eval_samples_per_second': 111.032, 'eval_steps_per_second': 7.002, 'epoch': 0.68}
{'loss': 1.1251, 'grad_norm': 0.5197754502296448, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1304343938827515, 'eval_runtime': 9.0, 'eval_samples_per_second': 111.0, 'eval_steps_per_second': 7.0, 'epoch': 0.72}
{'loss': 1.1202, 'grad_norm': 0.6810688972473145, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.126654028892517, 'eval_runtime': 9.0028, 'eval_samples_per_second': 110.966, 'eval_steps_per_second': 6.998, 'epoch': 0.76}
{'loss': 1.1596, 'grad_norm': 0.5655885934829712, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1243019104003906, 'eval_runtime': 8.9969, 'eval_samples_per_second': 111.038, 'eval_steps_per_second': 7.002, 'epoch': 0.8}
{'loss': 1.1354, 'grad_norm': 0.5187660455703735, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.121538758277893, 'eval_runtime': 8.992, 'eval_samples_per_second': 111.099, 'eval_steps_per_second': 7.006, 'epoch': 0.84}
{'loss': 1.1613, 'grad_norm': 0.5990810990333557, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.118340015411377, 'eval_runtime': 8.9963, 'eval_samples_per_second': 111.046, 'eval_steps_per_second': 7.003, 'epoch': 0.88}
{'loss': 1.1259, 'grad_norm': 0.7794157862663269, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1169148683547974, 'eval_runtime': 8.9859, 'eval_samples_per_second': 111.174, 'eval_steps_per_second': 7.011, 'epoch': 0.92}
{'loss': 1.1558, 'grad_norm': 0.605547308921814, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1168186664581299, 'eval_runtime': 9.0011, 'eval_samples_per_second': 110.986, 'eval_steps_per_second': 6.999, 'epoch': 0.96}
{'loss': 1.1393, 'grad_norm': 0.4298597276210785, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.115831971168518, 'eval_runtime': 8.9973, 'eval_samples_per_second': 111.033, 'eval_steps_per_second': 7.002, 'epoch': 1.0}
{'train_runtime': 525.3679, 'train_samples_per_second': 19.029, 'train_steps_per_second': 1.19, 'train_loss': 1.323142953491211, 'epoch': 1.0}
train_results:  {'eval_loss': [2.38110613822937, 1.6893634796142578, 1.4574785232543945, 1.3741666078567505, 1.327687382698059, 1.298951268196106, 1.270066261291504, 1.2363475561141968, 1.2202380895614624, 1.2058606147766113, 1.1957316398620605, 1.1808851957321167, 1.1696487665176392, 1.1558784246444702, 1.1493998765945435, 1.1382815837860107, 1.1345032453536987, 1.1304343938827515, 1.126654028892517, 1.1243019104003906, 1.121538758277893, 1.118340015411377, 1.1169148683547974, 1.1168186664581299, 1.115831971168518], 'performance': [0.77, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:53,  9.63s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<00:57,  1.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:20<00:33,  1.99it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:25<00:21,  2.39it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:30<00:13,  2.60it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:06,  3.10it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.91it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.77it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  0.800440788269043
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.2425 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 0.782423287730543
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 0.7824232877305276
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 0.7824232877101387
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.7824725422100649
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 0.7824232877305276
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0766, dtype=torch.float64), 0, tensor(0.7512, dtype=torch.float64), 0, 0, tensor(0.0939, dtype=torch.float64), tensor(0.0783, dtype=torch.float64), 1, 1, 0, 0, 1, 0, 128, 0.1, 10.731116960316747, 0]
normalized proposed parameters for next round by BO: [tensor(9.7038e-18, dtype=torch.float64), tensor(1.4498e-18, dtype=torch.float64), tensor(0.0766, dtype=torch.float64), tensor(3.4315e-18, dtype=torch.float64), tensor(0.7512, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.4208e-18, dtype=torch.float64), tensor(0.0939, dtype=torch.float64), tensor(0.0783, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2236, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.077
  sciq: 0
  triviaqa: 0.751
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.094
  arc_challenge: 0.078

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 1, 0],)
  lora_alpha: (10.731116960316747,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  10.731116960316747
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:10,  3.13s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:55,  1.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:36,  2.25it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.73it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.70it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:18,  2.81it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:18,  2.35it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:13,  2.67it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:09,  2.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.48it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:04,  2.52it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:01,  2.90it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.66it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.7564, 'grad_norm': 0.924171507358551, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 4.316638469696045, 'eval_runtime': 7.2228, 'eval_samples_per_second': 138.313, 'eval_steps_per_second': 8.722, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:45,  5.31s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:30,  1.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:49,  1.66it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:34,  2.16it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:19<00:34,  1.95it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:24<00:31,  1.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:26<00:23,  2.20it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:29<00:18,  2.28it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:32<00:13,  2.60it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:34<00:09,  2.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:07,  2.71it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:40<00:03,  2.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:41<00:00,  3.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:41<00:00,  2.39it/s]
Evaluation performance at step 50: 0.68
{'loss': 3.0764, 'grad_norm': 0.3757131099700928, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.68}
{'eval_loss': 2.3466503620147705, 'eval_runtime': 7.2414, 'eval_samples_per_second': 137.956, 'eval_steps_per_second': 8.7, 'epoch': 0.08}
{'loss': 2.0687, 'grad_norm': 0.34518179297447205, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9034514427185059, 'eval_runtime': 7.2916, 'eval_samples_per_second': 137.006, 'eval_steps_per_second': 8.64, 'epoch': 0.12}
{'loss': 1.8268, 'grad_norm': 0.1819561868906021, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7800132036209106, 'eval_runtime': 7.2954, 'eval_samples_per_second': 136.937, 'eval_steps_per_second': 8.636, 'epoch': 0.16}
{'loss': 1.7413, 'grad_norm': 0.18369989097118378, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7114123106002808, 'eval_runtime': 7.3243, 'eval_samples_per_second': 136.395, 'eval_steps_per_second': 8.601, 'epoch': 0.2}
{'loss': 1.702, 'grad_norm': 0.22896644473075867, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6687278747558594, 'eval_runtime': 7.3291, 'eval_samples_per_second': 136.306, 'eval_steps_per_second': 8.596, 'epoch': 0.24}
{'loss': 1.6644, 'grad_norm': 0.18329237401485443, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6318587064743042, 'eval_runtime': 7.3371, 'eval_samples_per_second': 136.158, 'eval_steps_per_second': 8.587, 'epoch': 0.28}
{'loss': 1.6656, 'grad_norm': 0.159662127494812, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6028133630752563, 'eval_runtime': 7.3396, 'eval_samples_per_second': 136.11, 'eval_steps_per_second': 8.584, 'epoch': 0.32}
{'loss': 1.6723, 'grad_norm': 0.19833774864673615, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.578739881515503, 'eval_runtime': 7.3309, 'eval_samples_per_second': 136.273, 'eval_steps_per_second': 8.594, 'epoch': 0.36}
{'loss': 1.5046, 'grad_norm': 0.2677929997444153, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5589467287063599, 'eval_runtime': 7.3268, 'eval_samples_per_second': 136.35, 'eval_steps_per_second': 8.599, 'epoch': 0.4}
{'loss': 1.5404, 'grad_norm': 0.19993378221988678, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5426825284957886, 'eval_runtime': 7.3331, 'eval_samples_per_second': 136.232, 'eval_steps_per_second': 8.591, 'epoch': 0.44}
{'loss': 1.5595, 'grad_norm': 0.24652697145938873, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5277738571166992, 'eval_runtime': 7.3537, 'eval_samples_per_second': 135.851, 'eval_steps_per_second': 8.567, 'epoch': 0.48}
{'loss': 1.5199, 'grad_norm': 0.2158648669719696, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5151474475860596, 'eval_runtime': 7.3281, 'eval_samples_per_second': 136.325, 'eval_steps_per_second': 8.597, 'epoch': 0.52}
{'loss': 1.4989, 'grad_norm': 0.20187610387802124, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.505240797996521, 'eval_runtime': 7.35, 'eval_samples_per_second': 135.918, 'eval_steps_per_second': 8.571, 'epoch': 0.56}
{'loss': 1.5759, 'grad_norm': 0.1762218028306961, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4959352016448975, 'eval_runtime': 7.3452, 'eval_samples_per_second': 136.008, 'eval_steps_per_second': 8.577, 'epoch': 0.6}
{'loss': 1.5064, 'grad_norm': 0.2245987355709076, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.488911747932434, 'eval_runtime': 7.3483, 'eval_samples_per_second': 135.949, 'eval_steps_per_second': 8.573, 'epoch': 0.64}
{'loss': 1.5687, 'grad_norm': 0.2181791067123413, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4824609756469727, 'eval_runtime': 7.3487, 'eval_samples_per_second': 135.942, 'eval_steps_per_second': 8.573, 'epoch': 0.68}
{'loss': 1.4836, 'grad_norm': 0.22351956367492676, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4767762422561646, 'eval_runtime': 7.3433, 'eval_samples_per_second': 136.043, 'eval_steps_per_second': 8.579, 'epoch': 0.72}
{'loss': 1.4739, 'grad_norm': 0.1918795257806778, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.473266839981079, 'eval_runtime': 7.3463, 'eval_samples_per_second': 135.987, 'eval_steps_per_second': 8.576, 'epoch': 0.76}
{'loss': 1.5089, 'grad_norm': 0.20131343603134155, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4695063829421997, 'eval_runtime': 7.347, 'eval_samples_per_second': 135.975, 'eval_steps_per_second': 8.575, 'epoch': 0.8}
{'loss': 1.4407, 'grad_norm': 0.21946312487125397, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.466357707977295, 'eval_runtime': 7.3296, 'eval_samples_per_second': 136.296, 'eval_steps_per_second': 8.595, 'epoch': 0.84}
{'loss': 1.4838, 'grad_norm': 0.25130459666252136, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4640051126480103, 'eval_runtime': 7.3347, 'eval_samples_per_second': 136.201, 'eval_steps_per_second': 8.589, 'epoch': 0.88}
{'loss': 1.5159, 'grad_norm': 0.18534401059150696, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4625049829483032, 'eval_runtime': 7.3406, 'eval_samples_per_second': 136.093, 'eval_steps_per_second': 8.582, 'epoch': 0.92}
{'loss': 1.4608, 'grad_norm': 0.2528122663497925, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4615206718444824, 'eval_runtime': 7.3392, 'eval_samples_per_second': 136.119, 'eval_steps_per_second': 8.584, 'epoch': 0.96}
{'loss': 1.4471, 'grad_norm': 0.1954430490732193, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4610604047775269, 'eval_runtime': 7.3457, 'eval_samples_per_second': 135.998, 'eval_steps_per_second': 8.576, 'epoch': 1.0}
{'train_runtime': 377.0171, 'train_samples_per_second': 26.519, 'train_steps_per_second': 1.658, 'train_loss': 1.770516973876953, 'epoch': 1.0}
train_results:  {'eval_loss': [4.316638469696045, 2.3466503620147705, 1.9034514427185059, 1.7800132036209106, 1.7114123106002808, 1.6687278747558594, 1.6318587064743042, 1.6028133630752563, 1.578739881515503, 1.5589467287063599, 1.5426825284957886, 1.5277738571166992, 1.5151474475860596, 1.505240797996521, 1.4959352016448975, 1.488911747932434, 1.4824609756469727, 1.4767762422561646, 1.473266839981079, 1.4695063829421997, 1.466357707977295, 1.4640051126480103, 1.4625049829483032, 1.4615206718444824, 1.4610604047775269], 'performance': [0.77, 0.68]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:51,  9.61s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:25<00:42,  1.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:33<00:29,  1.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:40<00:18,  1.86it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:48<00:09,  1.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:50<00:01,  2.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:50<00:00,  2.00it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.68]
current iteration observed (possibly low-fid or predicted) performance:  0.7532808184623718
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.7558 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 0.7741174558901744
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.7740714559389412
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 0.7741174558782375
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.773989008628105
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.7743741886878988
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.6522, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), 0, 0, 0, tensor(0.1277, dtype=torch.float64), tensor(0.1303, dtype=torch.float64), tensor(0.0332, dtype=torch.float64), 1, 1, 1, 0, 1, 0, 128, 5.416087376961712e-18, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(5.4874e-18, dtype=torch.float64), tensor(0.6522, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.2290e-18, dtype=torch.float64), tensor(0.1277, dtype=torch.float64), tensor(0.1303, dtype=torch.float64), tensor(0.0332, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(5.4161e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.652
  rowan_hellaswag: 0.057
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.128
  mmlu: 0.13
  arc_challenge: 0.033

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.416087376961712e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  5.416087376961712e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 4,063,232 || all params: 8,034,324,480 || trainable%: 0.0506
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /home/alfred/.cache/huggingface/datasets/gsm8k/main/0.0.0/cc7b047b6e5bb11b4f1af84efc572db110a51b3c (last modified on Wed Dec 31 01:12:59 2025).
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:55,  2.99s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.24it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:43,  1.89it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.60it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.63it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.64it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.78it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.03it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:05,  3.18it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.39it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.65it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.90it/s]
Evaluation performance at step 25: 0.79
{'loss': 2.6484, 'grad_norm': 1.3357537984848022, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.79}
{'eval_loss': 2.281388998031616, 'eval_runtime': 8.8059, 'eval_samples_per_second': 113.446, 'eval_steps_per_second': 7.154, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:26,  4.51s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:57,  1.58it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:36,  2.27it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:27,  2.69it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:23,  2.91it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:20,  2.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.00it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:07,  3.39it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.68it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.86it/s]
Evaluation performance at step 50: 0.79
{'loss': 2.1095, 'grad_norm': 2.0723729133605957, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.79}
{'eval_loss': 1.7210936546325684, 'eval_runtime': 8.8386, 'eval_samples_per_second': 113.026, 'eval_steps_per_second': 7.128, 'epoch': 0.08}
{'loss': 1.6267, 'grad_norm': 0.7313357591629028, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4981807470321655, 'eval_runtime': 8.8663, 'eval_samples_per_second': 112.674, 'eval_steps_per_second': 7.106, 'epoch': 0.12}
{'loss': 1.4894, 'grad_norm': 0.7740082144737244, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3854550123214722, 'eval_runtime': 8.8762, 'eval_samples_per_second': 112.548, 'eval_steps_per_second': 7.098, 'epoch': 0.16}
{'loss': 1.4199, 'grad_norm': 1.1642991304397583, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.327069878578186, 'eval_runtime': 8.8906, 'eval_samples_per_second': 112.366, 'eval_steps_per_second': 7.086, 'epoch': 0.2}
{'loss': 1.3465, 'grad_norm': 0.977523922920227, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2894951105117798, 'eval_runtime': 8.9295, 'eval_samples_per_second': 111.877, 'eval_steps_per_second': 7.055, 'epoch': 0.24}
{'loss': 1.3076, 'grad_norm': 0.8007394075393677, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2714649438858032, 'eval_runtime': 8.9451, 'eval_samples_per_second': 111.681, 'eval_steps_per_second': 7.043, 'epoch': 0.28}
{'loss': 1.2819, 'grad_norm': 0.7775124311447144, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2730845212936401, 'eval_runtime': 8.9411, 'eval_samples_per_second': 111.731, 'eval_steps_per_second': 7.046, 'epoch': 0.32}
{'loss': 1.2764, 'grad_norm': 0.5767627358436584, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.235177755355835, 'eval_runtime': 8.8932, 'eval_samples_per_second': 112.333, 'eval_steps_per_second': 7.084, 'epoch': 0.36}
{'loss': 1.2583, 'grad_norm': 0.49893248081207275, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2314702272415161, 'eval_runtime': 8.9062, 'eval_samples_per_second': 112.169, 'eval_steps_per_second': 7.074, 'epoch': 0.4}
{'loss': 1.2414, 'grad_norm': 0.6997730135917664, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2129024267196655, 'eval_runtime': 8.9039, 'eval_samples_per_second': 112.198, 'eval_steps_per_second': 7.076, 'epoch': 0.44}
{'loss': 1.2251, 'grad_norm': 0.4127603769302368, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2056636810302734, 'eval_runtime': 8.9067, 'eval_samples_per_second': 112.163, 'eval_steps_per_second': 7.073, 'epoch': 0.48}
{'loss': 1.263, 'grad_norm': 0.48718175292015076, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.193607211112976, 'eval_runtime': 8.8905, 'eval_samples_per_second': 112.368, 'eval_steps_per_second': 7.086, 'epoch': 0.52}
{'loss': 1.2452, 'grad_norm': 0.6747186183929443, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1799393892288208, 'eval_runtime': 8.8955, 'eval_samples_per_second': 112.304, 'eval_steps_per_second': 7.082, 'epoch': 0.56}
{'loss': 1.2133, 'grad_norm': 0.5497352480888367, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1726503372192383, 'eval_runtime': 8.9933, 'eval_samples_per_second': 111.083, 'eval_steps_per_second': 7.005, 'epoch': 0.6}
{'loss': 1.1967, 'grad_norm': 0.9639138579368591, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1655454635620117, 'eval_runtime': 8.9542, 'eval_samples_per_second': 111.568, 'eval_steps_per_second': 7.036, 'epoch': 0.64}
{'loss': 1.2257, 'grad_norm': 0.664611279964447, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1642882823944092, 'eval_runtime': 8.9559, 'eval_samples_per_second': 111.547, 'eval_steps_per_second': 7.034, 'epoch': 0.68}
{'loss': 1.232, 'grad_norm': 0.3801972270011902, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1601685285568237, 'eval_runtime': 8.9205, 'eval_samples_per_second': 111.989, 'eval_steps_per_second': 7.062, 'epoch': 0.72}
{'loss': 1.1577, 'grad_norm': 0.6102970242500305, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1550065279006958, 'eval_runtime': 8.9113, 'eval_samples_per_second': 112.105, 'eval_steps_per_second': 7.07, 'epoch': 0.76}
{'loss': 1.1533, 'grad_norm': 0.5160718560218811, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.154435157775879, 'eval_runtime': 8.9033, 'eval_samples_per_second': 112.205, 'eval_steps_per_second': 7.076, 'epoch': 0.8}
{'loss': 1.1811, 'grad_norm': 0.5249509215354919, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1507071256637573, 'eval_runtime': 8.897, 'eval_samples_per_second': 112.285, 'eval_steps_per_second': 7.081, 'epoch': 0.84}
{'loss': 1.175, 'grad_norm': 0.5268887281417847, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1485555171966553, 'eval_runtime': 8.902, 'eval_samples_per_second': 112.222, 'eval_steps_per_second': 7.077, 'epoch': 0.88}
{'loss': 1.1926, 'grad_norm': 0.49586838483810425, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1469868421554565, 'eval_runtime': 8.9022, 'eval_samples_per_second': 112.22, 'eval_steps_per_second': 7.077, 'epoch': 0.92}
{'loss': 1.1823, 'grad_norm': 0.49745088815689087, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1461488008499146, 'eval_runtime': 8.9166, 'eval_samples_per_second': 112.039, 'eval_steps_per_second': 7.066, 'epoch': 0.96}
{'loss': 1.156, 'grad_norm': 0.4857976734638214, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1449764966964722, 'eval_runtime': 8.8983, 'eval_samples_per_second': 112.268, 'eval_steps_per_second': 7.08, 'epoch': 1.0}
{'train_runtime': 524.6241, 'train_samples_per_second': 19.057, 'train_steps_per_second': 1.191, 'train_loss': 1.3521985626220703, 'epoch': 1.0}
train_results:  {'eval_loss': [2.281388998031616, 1.7210936546325684, 1.4981807470321655, 1.3854550123214722, 1.327069878578186, 1.2894951105117798, 1.2714649438858032, 1.2730845212936401, 1.235177755355835, 1.2314702272415161, 1.2129024267196655, 1.2056636810302734, 1.193607211112976, 1.1799393892288208, 1.1726503372192383, 1.1655454635620117, 1.1642882823944092, 1.1601685285568237, 1.1550065279006958, 1.154435157775879, 1.1507071256637573, 1.1485555171966553, 1.1469868421554565, 1.1461488008499146, 1.1449764966964722], 'performance': [0.79, 0.79]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:20,  5.66s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<01:02,  1.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:41,  1.63it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:26<00:23,  2.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:30<00:13,  2.64it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:06,  3.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.90it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.74it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.79, 0.79]
current iteration observed (possibly low-fid or predicted) performance:  0.7956867814064026
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.5410 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.7698878609907414
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.7698878609906931
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 0.7698878609906931
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 0.7698878609906932
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 0.7698878609906937
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2917, dtype=torch.float64), tensor(0.2380, dtype=torch.float64), tensor(0.0504, dtype=torch.float64), 0, 0, 0, tensor(0.0247, dtype=torch.float64), tensor(0.3943, dtype=torch.float64), 0, 1, 1, 1, 0, 0, 0, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.2917, dtype=torch.float64), tensor(0.2380, dtype=torch.float64), tensor(0.0504, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.6951e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0247, dtype=torch.float64), tensor(0.3943, dtype=torch.float64), tensor(0.0008, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.292
  gsm8k: 0.238
  rowan_hellaswag: 0.05
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.025
  mmlu: 0.394
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,703,936 || all params: 8,031,965,184 || trainable%: 0.0212
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9989
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  998
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:26,  3.30s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:15,  1.21it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:43,  1.92it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:24,  2.72it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:21,  2.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.86it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:14,  2.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:10,  3.25it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:07,  3.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.69it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.98it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Evaluation performance at step 25: 0.76
{'loss': 3.4738, 'grad_norm': 2.3212525844573975, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.1551878452301025, 'eval_runtime': 8.5936, 'eval_samples_per_second': 116.133, 'eval_steps_per_second': 7.331, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:38,  2.81s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.94it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.71it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:23,  2.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.69it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.07it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.67it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.29it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.79it/s]
Evaluation performance at step 50: 0.77
{'loss': 2.8657, 'grad_norm': 2.032470464706421, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 2.5229697227478027, 'eval_runtime': 8.5825, 'eval_samples_per_second': 116.283, 'eval_steps_per_second': 7.341, 'epoch': 0.08}
{'loss': 2.4536, 'grad_norm': 1.3447291851043701, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.230966567993164, 'eval_runtime': 8.5991, 'eval_samples_per_second': 116.059, 'eval_steps_per_second': 7.326, 'epoch': 0.12}
{'loss': 2.1801, 'grad_norm': 2.4731414318084717, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0119335651397705, 'eval_runtime': 8.6192, 'eval_samples_per_second': 115.788, 'eval_steps_per_second': 7.309, 'epoch': 0.16}
{'loss': 1.9045, 'grad_norm': 2.0142228603363037, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8493798971176147, 'eval_runtime': 8.7005, 'eval_samples_per_second': 114.706, 'eval_steps_per_second': 7.241, 'epoch': 0.2}
{'loss': 1.8537, 'grad_norm': 2.7992265224456787, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7447975873947144, 'eval_runtime': 8.7145, 'eval_samples_per_second': 114.522, 'eval_steps_per_second': 7.229, 'epoch': 0.24}
{'loss': 1.6672, 'grad_norm': 2.7865688800811768, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6592743396759033, 'eval_runtime': 8.7098, 'eval_samples_per_second': 114.583, 'eval_steps_per_second': 7.233, 'epoch': 0.28}
{'loss': 1.5997, 'grad_norm': 2.7720460891723633, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.604514718055725, 'eval_runtime': 8.6958, 'eval_samples_per_second': 114.769, 'eval_steps_per_second': 7.245, 'epoch': 0.32}
{'loss': 1.5941, 'grad_norm': 1.6418521404266357, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5603269338607788, 'eval_runtime': 8.7072, 'eval_samples_per_second': 114.618, 'eval_steps_per_second': 7.235, 'epoch': 0.36}
{'loss': 1.5072, 'grad_norm': 2.2394845485687256, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.533560872077942, 'eval_runtime': 8.7003, 'eval_samples_per_second': 114.709, 'eval_steps_per_second': 7.241, 'epoch': 0.4}
{'loss': 1.5226, 'grad_norm': 2.278045654296875, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5095690488815308, 'eval_runtime': 8.6872, 'eval_samples_per_second': 114.881, 'eval_steps_per_second': 7.252, 'epoch': 0.44}
{'loss': 1.5289, 'grad_norm': 1.8159193992614746, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4948478937149048, 'eval_runtime': 8.6695, 'eval_samples_per_second': 115.116, 'eval_steps_per_second': 7.267, 'epoch': 0.48}
{'loss': 1.4748, 'grad_norm': 2.056640148162842, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4801830053329468, 'eval_runtime': 8.6761, 'eval_samples_per_second': 115.029, 'eval_steps_per_second': 7.261, 'epoch': 0.52}
{'loss': 1.4121, 'grad_norm': 1.9872068166732788, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.470797061920166, 'eval_runtime': 8.6814, 'eval_samples_per_second': 114.958, 'eval_steps_per_second': 7.257, 'epoch': 0.56}
{'loss': 1.4025, 'grad_norm': 1.3598824739456177, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4620331525802612, 'eval_runtime': 8.6655, 'eval_samples_per_second': 115.169, 'eval_steps_per_second': 7.27, 'epoch': 0.6}
{'loss': 1.4678, 'grad_norm': 2.897838830947876, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4508811235427856, 'eval_runtime': 8.6772, 'eval_samples_per_second': 115.014, 'eval_steps_per_second': 7.26, 'epoch': 0.64}
{'loss': 1.4298, 'grad_norm': 1.4206639528274536, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4431512355804443, 'eval_runtime': 8.6838, 'eval_samples_per_second': 114.927, 'eval_steps_per_second': 7.255, 'epoch': 0.68}
{'loss': 1.4225, 'grad_norm': 1.953233003616333, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4365061521530151, 'eval_runtime': 8.6698, 'eval_samples_per_second': 115.112, 'eval_steps_per_second': 7.267, 'epoch': 0.72}
{'loss': 1.466, 'grad_norm': 2.3247625827789307, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4404027462005615, 'eval_runtime': 8.6357, 'eval_samples_per_second': 115.567, 'eval_steps_per_second': 7.295, 'epoch': 0.76}
{'loss': 1.4351, 'grad_norm': 1.5894408226013184, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4277890920639038, 'eval_runtime': 8.5774, 'eval_samples_per_second': 116.352, 'eval_steps_per_second': 7.345, 'epoch': 0.8}
{'loss': 1.4018, 'grad_norm': 1.6382635831832886, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.427146553993225, 'eval_runtime': 8.5976, 'eval_samples_per_second': 116.079, 'eval_steps_per_second': 7.328, 'epoch': 0.84}
{'loss': 1.4312, 'grad_norm': 1.372595191001892, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4202629327774048, 'eval_runtime': 8.6081, 'eval_samples_per_second': 115.938, 'eval_steps_per_second': 7.319, 'epoch': 0.88}
{'loss': 1.4144, 'grad_norm': 1.5687127113342285, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.419161081314087, 'eval_runtime': 8.6079, 'eval_samples_per_second': 115.941, 'eval_steps_per_second': 7.319, 'epoch': 0.92}
{'loss': 1.3739, 'grad_norm': 1.4949901103973389, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4175875186920166, 'eval_runtime': 8.6246, 'eval_samples_per_second': 115.715, 'eval_steps_per_second': 7.305, 'epoch': 0.96}
{'loss': 1.4309, 'grad_norm': 1.9481542110443115, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4163209199905396, 'eval_runtime': 8.6376, 'eval_samples_per_second': 115.541, 'eval_steps_per_second': 7.294, 'epoch': 1.0}
{'train_runtime': 508.2902, 'train_samples_per_second': 19.652, 'train_steps_per_second': 1.23, 'train_loss': 1.7085651184082031, 'epoch': 1.0}
train_results:  {'eval_loss': [3.1551878452301025, 2.5229697227478027, 2.230966567993164, 2.0119335651397705, 1.8493798971176147, 1.7447975873947144, 1.6592743396759033, 1.604514718055725, 1.5603269338607788, 1.533560872077942, 1.5095690488815308, 1.4948478937149048, 1.4801830053329468, 1.470797061920166, 1.4620331525802612, 1.4508811235427856, 1.4431512355804443, 1.4365061521530151, 1.4404027462005615, 1.4277890920639038, 1.427146553993225, 1.4202629327774048, 1.419161081314087, 1.4175875186920166, 1.4163209199905396], 'performance': [0.76, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:58,  9.68s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:16,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:39,  1.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:26,  1.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:14,  2.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:06,  2.80it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:00,  3.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.45it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  0.7947957515716553
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.1014 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 0.7656706719972292
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.7656706719973411
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 0.7656706719218578
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7591803606915268
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.7656724425356007
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0226, dtype=torch.float64), tensor(0.8049, dtype=torch.float64), tensor(0.0520, dtype=torch.float64), 0, 0, 0, tensor(0.0336, dtype=torch.float64), 0, tensor(0.0868, dtype=torch.float64), 1, 1, 1, 0, 1, 0, 58, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0226, dtype=torch.float64), tensor(0.8049, dtype=torch.float64), tensor(0.0520, dtype=torch.float64), tensor(1.7835e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.9370e-18, dtype=torch.float64), tensor(0.0336, dtype=torch.float64), tensor(1.2155e-18, dtype=torch.float64), tensor(0.0868, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4519, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.023
  gsm8k: 0.805
  rowan_hellaswag: 0.052
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.034
  mmlu: 0
  arc_challenge: 0.087

LoRA Parameters:
  lora_r: (58,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  58
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,841,152 || all params: 8,032,102,400 || trainable%: 0.0229
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:39,  4.03s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:20,  1.12it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.77it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.28it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.78it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:14,  2.88it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:10,  3.21it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.29it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.73it/s]
Evaluation performance at step 25: 0.76
{'loss': 2.5868, 'grad_norm': 2.8050613403320312, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 2.220888376235962, 'eval_runtime': 8.8124, 'eval_samples_per_second': 113.363, 'eval_steps_per_second': 7.149, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:32,  2.75s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:44,  2.04it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.54it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:25,  2.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.08it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.91it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:16,  3.01it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:11,  3.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:07,  3.53it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:06,  2.73it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:30<00:03,  3.00it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:32<00:00,  3.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:32<00:00,  3.05it/s]
Evaluation performance at step 50: 0.79
{'loss': 1.914, 'grad_norm': 1.3156784772872925, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.79}
{'eval_loss': 1.576760172843933, 'eval_runtime': 8.8981, 'eval_samples_per_second': 112.271, 'eval_steps_per_second': 7.08, 'epoch': 0.08}
{'loss': 1.4164, 'grad_norm': 0.9409385323524475, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3477659225463867, 'eval_runtime': 8.9329, 'eval_samples_per_second': 111.834, 'eval_steps_per_second': 7.053, 'epoch': 0.12}
{'loss': 1.3143, 'grad_norm': 1.0879186391830444, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2482247352600098, 'eval_runtime': 8.9529, 'eval_samples_per_second': 111.585, 'eval_steps_per_second': 7.037, 'epoch': 0.16}
{'loss': 1.2554, 'grad_norm': 0.821622908115387, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1931804418563843, 'eval_runtime': 8.9443, 'eval_samples_per_second': 111.692, 'eval_steps_per_second': 7.044, 'epoch': 0.2}
{'loss': 1.2064, 'grad_norm': 0.8064871430397034, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1607367992401123, 'eval_runtime': 8.9959, 'eval_samples_per_second': 111.051, 'eval_steps_per_second': 7.003, 'epoch': 0.24}
{'loss': 1.1663, 'grad_norm': 0.7557219862937927, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.137441635131836, 'eval_runtime': 9.0159, 'eval_samples_per_second': 110.805, 'eval_steps_per_second': 6.988, 'epoch': 0.28}
{'loss': 1.1543, 'grad_norm': 0.8683950304985046, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1156386137008667, 'eval_runtime': 8.9728, 'eval_samples_per_second': 111.337, 'eval_steps_per_second': 7.021, 'epoch': 0.32}
{'loss': 1.1303, 'grad_norm': 0.6564105749130249, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1053526401519775, 'eval_runtime': 8.9184, 'eval_samples_per_second': 112.016, 'eval_steps_per_second': 7.064, 'epoch': 0.36}
{'loss': 1.1113, 'grad_norm': 0.8390854597091675, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0927464962005615, 'eval_runtime': 8.9145, 'eval_samples_per_second': 112.064, 'eval_steps_per_second': 7.067, 'epoch': 0.4}
{'loss': 1.0863, 'grad_norm': 1.0342862606048584, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.077636957168579, 'eval_runtime': 8.9202, 'eval_samples_per_second': 111.993, 'eval_steps_per_second': 7.063, 'epoch': 0.44}
{'loss': 1.0958, 'grad_norm': 0.7346650958061218, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0624415874481201, 'eval_runtime': 8.9073, 'eval_samples_per_second': 112.155, 'eval_steps_per_second': 7.073, 'epoch': 0.48}
{'loss': 1.0805, 'grad_norm': 0.6605865955352783, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0538021326065063, 'eval_runtime': 8.8999, 'eval_samples_per_second': 112.249, 'eval_steps_per_second': 7.079, 'epoch': 0.52}
{'loss': 1.0998, 'grad_norm': 0.6602940559387207, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0500900745391846, 'eval_runtime': 8.89, 'eval_samples_per_second': 112.373, 'eval_steps_per_second': 7.087, 'epoch': 0.56}
{'loss': 1.054, 'grad_norm': 0.846294641494751, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0456432104110718, 'eval_runtime': 8.8491, 'eval_samples_per_second': 112.893, 'eval_steps_per_second': 7.119, 'epoch': 0.6}
{'loss': 1.0837, 'grad_norm': 0.6776096820831299, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0435938835144043, 'eval_runtime': 8.8731, 'eval_samples_per_second': 112.587, 'eval_steps_per_second': 7.1, 'epoch': 0.64}
{'loss': 1.0567, 'grad_norm': 0.852841854095459, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0386494398117065, 'eval_runtime': 8.8542, 'eval_samples_per_second': 112.828, 'eval_steps_per_second': 7.115, 'epoch': 0.68}
{'loss': 1.064, 'grad_norm': 0.7434735894203186, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.035814642906189, 'eval_runtime': 8.8585, 'eval_samples_per_second': 112.773, 'eval_steps_per_second': 7.112, 'epoch': 0.72}
{'loss': 1.0259, 'grad_norm': 0.6207467913627625, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0333845615386963, 'eval_runtime': 8.8576, 'eval_samples_per_second': 112.784, 'eval_steps_per_second': 7.113, 'epoch': 0.76}
{'loss': 1.0224, 'grad_norm': 0.8302003741264343, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0322659015655518, 'eval_runtime': 8.8735, 'eval_samples_per_second': 112.583, 'eval_steps_per_second': 7.1, 'epoch': 0.8}
{'loss': 1.0639, 'grad_norm': 0.8114145994186401, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0303679704666138, 'eval_runtime': 8.8728, 'eval_samples_per_second': 112.592, 'eval_steps_per_second': 7.1, 'epoch': 0.84}
{'loss': 1.0482, 'grad_norm': 0.8922667503356934, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0283695459365845, 'eval_runtime': 8.8874, 'eval_samples_per_second': 112.407, 'eval_steps_per_second': 7.089, 'epoch': 0.88}
{'loss': 1.0559, 'grad_norm': 0.7279489636421204, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.027476191520691, 'eval_runtime': 8.9169, 'eval_samples_per_second': 112.034, 'eval_steps_per_second': 7.065, 'epoch': 0.92}
{'loss': 1.0761, 'grad_norm': 0.7758225202560425, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.026185154914856, 'eval_runtime': 8.9246, 'eval_samples_per_second': 111.938, 'eval_steps_per_second': 7.059, 'epoch': 0.96}
{'loss': 1.0306, 'grad_norm': 1.0283148288726807, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0256673097610474, 'eval_runtime': 8.9539, 'eval_samples_per_second': 111.572, 'eval_steps_per_second': 7.036, 'epoch': 1.0}
{'train_runtime': 521.098, 'train_samples_per_second': 19.186, 'train_steps_per_second': 1.199, 'train_loss': 1.2079716552734374, 'epoch': 1.0}
train_results:  {'eval_loss': [2.220888376235962, 1.576760172843933, 1.3477659225463867, 1.2482247352600098, 1.1931804418563843, 1.1607367992401123, 1.137441635131836, 1.1156386137008667, 1.1053526401519775, 1.0927464962005615, 1.077636957168579, 1.0624415874481201, 1.0538021326065063, 1.0500900745391846, 1.0456432104110718, 1.0435938835144043, 1.0386494398117065, 1.035814642906189, 1.0333845615386963, 1.0322659015655518, 1.0303679704666138, 1.0283695459365845, 1.027476191520691, 1.026185154914856, 1.0256673097610474], 'performance': [0.76, 0.79]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<10:41,  6.48s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:05,  1.28it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:19<00:33,  1.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:25<00:22,  2.24it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:29<00:12,  2.69it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:07,  2.47it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.56it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.79]
current iteration observed (possibly low-fid or predicted) performance:  0.8657175302505493
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8251 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.7742437685286374
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.7732558772017695
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.7732558772013556
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.7732558771205298
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 0.7732558771833762
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.6830, dtype=torch.float64), tensor(0.0750, dtype=torch.float64), 0, 0, 0, tensor(0.0740, dtype=torch.float64), 0, tensor(0.1679, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 16, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.6830, dtype=torch.float64), tensor(0.0750, dtype=torch.float64), tensor(7.1890e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.4665e-17, dtype=torch.float64), tensor(0.0740, dtype=torch.float64), tensor(2.5437e-17, dtype=torch.float64), tensor(0.1679, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1264, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.683
  rowan_hellaswag: 0.075
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.074
  mmlu: 0
  arc_challenge: 0.168

LoRA Parameters:
  lora_r: (16,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  16
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 131,072 || all params: 8,030,392,320 || trainable%: 0.0016
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:26,  4.51s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:24,  1.08it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.77it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:25,  2.65it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.82it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:14,  2.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  3.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.32it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:05,  3.25it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.44it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.70it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.87it/s]
Evaluation performance at step 25: 0.77
{'loss': 2.9782, 'grad_norm': 0.4251970946788788, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.9476253986358643, 'eval_runtime': 8.7974, 'eval_samples_per_second': 113.556, 'eval_steps_per_second': 7.161, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:54,  2.98s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:47,  1.90it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.58it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.02it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.09it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:18,  2.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:11,  3.07it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.27it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:05,  3.34it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.02it/s]
Evaluation performance at step 50: 0.77
{'loss': 2.8526, 'grad_norm': 0.36525678634643555, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 2.698711633682251, 'eval_runtime': 8.8091, 'eval_samples_per_second': 113.405, 'eval_steps_per_second': 7.152, 'epoch': 0.08}
{'loss': 2.5596, 'grad_norm': 0.35909003019332886, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.5023558139801025, 'eval_runtime': 8.8243, 'eval_samples_per_second': 113.21, 'eval_steps_per_second': 7.139, 'epoch': 0.12}
{'loss': 2.4812, 'grad_norm': 0.31269580125808716, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.425123929977417, 'eval_runtime': 8.8609, 'eval_samples_per_second': 112.742, 'eval_steps_per_second': 7.11, 'epoch': 0.16}
{'loss': 2.4269, 'grad_norm': 0.5413200259208679, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.3701436519622803, 'eval_runtime': 8.8772, 'eval_samples_per_second': 112.535, 'eval_steps_per_second': 7.097, 'epoch': 0.2}
{'loss': 2.3774, 'grad_norm': 3.3918426036834717, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.3199942111968994, 'eval_runtime': 8.8921, 'eval_samples_per_second': 112.347, 'eval_steps_per_second': 7.085, 'epoch': 0.24}
{'loss': 2.2842, 'grad_norm': 1.463922381401062, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.2921457290649414, 'eval_runtime': 8.8999, 'eval_samples_per_second': 112.249, 'eval_steps_per_second': 7.079, 'epoch': 0.28}
{'loss': 2.2984, 'grad_norm': 1.5741082429885864, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.254014730453491, 'eval_runtime': 8.8998, 'eval_samples_per_second': 112.25, 'eval_steps_per_second': 7.079, 'epoch': 0.32}
{'loss': 2.2054, 'grad_norm': 0.7737528681755066, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.2245283126831055, 'eval_runtime': 8.9132, 'eval_samples_per_second': 112.082, 'eval_steps_per_second': 7.068, 'epoch': 0.36}
{'loss': 2.2473, 'grad_norm': 1.4975454807281494, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.2001466751098633, 'eval_runtime': 8.9171, 'eval_samples_per_second': 112.031, 'eval_steps_per_second': 7.065, 'epoch': 0.4}
{'loss': 2.269, 'grad_norm': 0.6781150102615356, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.1696412563323975, 'eval_runtime': 8.9022, 'eval_samples_per_second': 112.219, 'eval_steps_per_second': 7.077, 'epoch': 0.44}
{'loss': 2.1415, 'grad_norm': 0.7935245633125305, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.1426446437835693, 'eval_runtime': 8.9056, 'eval_samples_per_second': 112.176, 'eval_steps_per_second': 7.074, 'epoch': 0.48}
{'loss': 2.122, 'grad_norm': 0.31195583939552307, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.1134414672851562, 'eval_runtime': 8.8815, 'eval_samples_per_second': 112.48, 'eval_steps_per_second': 7.093, 'epoch': 0.52}
{'loss': 2.0937, 'grad_norm': 0.42813390493392944, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.0896871089935303, 'eval_runtime': 8.8921, 'eval_samples_per_second': 112.347, 'eval_steps_per_second': 7.085, 'epoch': 0.56}
{'loss': 2.0422, 'grad_norm': 0.33970150351524353, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.072618246078491, 'eval_runtime': 8.8886, 'eval_samples_per_second': 112.391, 'eval_steps_per_second': 7.088, 'epoch': 0.6}
{'loss': 2.0136, 'grad_norm': 0.780103325843811, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.055657386779785, 'eval_runtime': 8.8951, 'eval_samples_per_second': 112.309, 'eval_steps_per_second': 7.083, 'epoch': 0.64}
{'loss': 2.1185, 'grad_norm': 0.9467841982841492, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.039389133453369, 'eval_runtime': 8.8908, 'eval_samples_per_second': 112.364, 'eval_steps_per_second': 7.086, 'epoch': 0.68}
{'loss': 2.0845, 'grad_norm': 0.604925811290741, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.026275634765625, 'eval_runtime': 8.9004, 'eval_samples_per_second': 112.242, 'eval_steps_per_second': 7.078, 'epoch': 0.72}
{'loss': 1.9477, 'grad_norm': 0.9823409914970398, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.0158491134643555, 'eval_runtime': 8.8947, 'eval_samples_per_second': 112.314, 'eval_steps_per_second': 7.083, 'epoch': 0.76}
{'loss': 2.0115, 'grad_norm': 0.57159823179245, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.0075321197509766, 'eval_runtime': 8.8845, 'eval_samples_per_second': 112.443, 'eval_steps_per_second': 7.091, 'epoch': 0.8}
{'loss': 2.0119, 'grad_norm': 0.4665396809577942, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.000828504562378, 'eval_runtime': 8.8902, 'eval_samples_per_second': 112.371, 'eval_steps_per_second': 7.086, 'epoch': 0.84}
{'loss': 1.9968, 'grad_norm': 0.9708090424537659, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.996557593345642, 'eval_runtime': 8.9042, 'eval_samples_per_second': 112.194, 'eval_steps_per_second': 7.075, 'epoch': 0.88}
{'loss': 2.0422, 'grad_norm': 0.45599475502967834, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9934524297714233, 'eval_runtime': 8.9257, 'eval_samples_per_second': 111.924, 'eval_steps_per_second': 7.058, 'epoch': 0.92}
{'loss': 2.0488, 'grad_norm': 0.47511935234069824, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9901540279388428, 'eval_runtime': 8.9294, 'eval_samples_per_second': 111.878, 'eval_steps_per_second': 7.055, 'epoch': 0.96}
{'loss': 1.9274, 'grad_norm': 0.3088436424732208, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9902234077453613, 'eval_runtime': 8.9491, 'eval_samples_per_second': 111.631, 'eval_steps_per_second': 7.04, 'epoch': 1.0}
{'train_runtime': 524.1295, 'train_samples_per_second': 19.075, 'train_steps_per_second': 1.192, 'train_loss': 2.2232956970214843, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9476253986358643, 2.698711633682251, 2.5023558139801025, 2.425123929977417, 2.3701436519622803, 2.3199942111968994, 2.2921457290649414, 2.254014730453491, 2.2245283126831055, 2.2001466751098633, 2.1696412563323975, 2.1426446437835693, 2.1134414672851562, 2.0896871089935303, 2.072618246078491, 2.055657386779785, 2.039389133453369, 2.026275634765625, 2.0158491134643555, 2.0075321197509766, 2.000828504562378, 1.996557593345642, 1.9934524297714233, 1.9901540279388428, 1.9902234077453613], 'performance': [0.77, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:51,  9.61s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:25<00:42,  1.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:31<00:27,  1.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:36<00:15,  2.26it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:40<00:06,  2.73it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:42<00:00,  3.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:42<00:00,  2.35it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  1.1255799531936646
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8500 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.9077395132044308
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.9135674634768132
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.9117550448865475
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.9137394925509849
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.9128876718083777
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.6748, dtype=torch.float64), tensor(0.1247, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.2005, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.007809184108711162, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.6748, dtype=torch.float64), tensor(0.1247, dtype=torch.float64), tensor(1.5930e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2005, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.0781, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.675
  rowan_hellaswag: 0.125
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.2

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.007809184108711162,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.007809184108711162
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:41,  4.06s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:21,  1.12it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:45,  1.81it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.52it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.58it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.76it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:15,  2.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.99it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:05,  3.26it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.41it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.63it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.0562, 'grad_norm': 1.3897385597229004, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.983693838119507, 'eval_runtime': 8.8837, 'eval_samples_per_second': 112.454, 'eval_steps_per_second': 7.092, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:11,  3.14s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:47,  1.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.56it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:25,  2.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.11it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:22,  2.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:18,  2.79it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:14,  2.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.22it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:07,  3.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.68it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.97it/s]
Evaluation performance at step 50: 0.76
{'loss': 2.9315, 'grad_norm': 3.0494768619537354, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 2.755750894546509, 'eval_runtime': 8.8092, 'eval_samples_per_second': 113.404, 'eval_steps_per_second': 7.152, 'epoch': 0.08}
{'loss': 2.5924, 'grad_norm': 0.8221649527549744, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.5308897495269775, 'eval_runtime': 8.8312, 'eval_samples_per_second': 113.121, 'eval_steps_per_second': 7.134, 'epoch': 0.12}
{'loss': 2.5274, 'grad_norm': 0.8682307600975037, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.410944938659668, 'eval_runtime': 8.875, 'eval_samples_per_second': 112.564, 'eval_steps_per_second': 7.099, 'epoch': 0.16}
{'loss': 2.4287, 'grad_norm': 4.081249237060547, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.330070734024048, 'eval_runtime': 8.9173, 'eval_samples_per_second': 112.029, 'eval_steps_per_second': 7.065, 'epoch': 0.2}
{'loss': 2.3424, 'grad_norm': 3.281716823577881, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.274066925048828, 'eval_runtime': 8.9272, 'eval_samples_per_second': 111.905, 'eval_steps_per_second': 7.057, 'epoch': 0.24}
{'loss': 2.3113, 'grad_norm': 0.9001315236091614, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.239757776260376, 'eval_runtime': 8.9167, 'eval_samples_per_second': 112.037, 'eval_steps_per_second': 7.065, 'epoch': 0.28}
{'loss': 2.2586, 'grad_norm': 1.7166829109191895, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.2164194583892822, 'eval_runtime': 8.9103, 'eval_samples_per_second': 112.118, 'eval_steps_per_second': 7.07, 'epoch': 0.32}
{'loss': 2.2517, 'grad_norm': 1.8350238800048828, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.1941123008728027, 'eval_runtime': 8.9158, 'eval_samples_per_second': 112.048, 'eval_steps_per_second': 7.066, 'epoch': 0.36}
{'loss': 2.2115, 'grad_norm': 0.6891371607780457, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.173290967941284, 'eval_runtime': 8.9874, 'eval_samples_per_second': 111.156, 'eval_steps_per_second': 7.01, 'epoch': 0.4}
{'loss': 2.1991, 'grad_norm': 1.610310435295105, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.152634859085083, 'eval_runtime': 9.0043, 'eval_samples_per_second': 110.947, 'eval_steps_per_second': 6.997, 'epoch': 0.44}
{'loss': 2.1532, 'grad_norm': 2.4555211067199707, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.1366591453552246, 'eval_runtime': 8.965, 'eval_samples_per_second': 111.434, 'eval_steps_per_second': 7.027, 'epoch': 0.48}
{'loss': 2.1621, 'grad_norm': 1.476944923400879, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.1210007667541504, 'eval_runtime': 8.9424, 'eval_samples_per_second': 111.715, 'eval_steps_per_second': 7.045, 'epoch': 0.52}
{'loss': 2.1714, 'grad_norm': 1.1775093078613281, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.1030468940734863, 'eval_runtime': 8.9325, 'eval_samples_per_second': 111.839, 'eval_steps_per_second': 7.053, 'epoch': 0.56}
{'loss': 2.1086, 'grad_norm': 1.5766478776931763, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.0893876552581787, 'eval_runtime': 8.9079, 'eval_samples_per_second': 112.147, 'eval_steps_per_second': 7.072, 'epoch': 0.6}
{'loss': 2.1012, 'grad_norm': 2.812713623046875, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.080259084701538, 'eval_runtime': 8.8874, 'eval_samples_per_second': 112.406, 'eval_steps_per_second': 7.089, 'epoch': 0.64}
{'loss': 2.1479, 'grad_norm': 1.7834447622299194, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.0694234371185303, 'eval_runtime': 8.8952, 'eval_samples_per_second': 112.308, 'eval_steps_per_second': 7.082, 'epoch': 0.68}
{'loss': 2.1307, 'grad_norm': 2.2786030769348145, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.0592238903045654, 'eval_runtime': 8.9396, 'eval_samples_per_second': 111.75, 'eval_steps_per_second': 7.047, 'epoch': 0.72}
{'loss': 2.0386, 'grad_norm': 1.077894926071167, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.051886558532715, 'eval_runtime': 8.9383, 'eval_samples_per_second': 111.767, 'eval_steps_per_second': 7.048, 'epoch': 0.76}
{'loss': 2.0766, 'grad_norm': 1.6710432767868042, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.0442588329315186, 'eval_runtime': 8.9444, 'eval_samples_per_second': 111.69, 'eval_steps_per_second': 7.043, 'epoch': 0.8}
{'loss': 2.0638, 'grad_norm': 2.439008951187134, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0374104976654053, 'eval_runtime': 8.9992, 'eval_samples_per_second': 111.01, 'eval_steps_per_second': 7.001, 'epoch': 0.84}
{'loss': 2.0916, 'grad_norm': 2.013578176498413, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.030595541000366, 'eval_runtime': 9.0183, 'eval_samples_per_second': 110.775, 'eval_steps_per_second': 6.986, 'epoch': 0.88}
{'loss': 2.0688, 'grad_norm': 1.1350079774856567, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0244641304016113, 'eval_runtime': 9.0274, 'eval_samples_per_second': 110.663, 'eval_steps_per_second': 6.979, 'epoch': 0.92}
{'loss': 2.0734, 'grad_norm': 1.0190616846084595, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0198326110839844, 'eval_runtime': 9.0241, 'eval_samples_per_second': 110.703, 'eval_steps_per_second': 6.981, 'epoch': 0.96}
{'loss': 1.9985, 'grad_norm': 0.5920409560203552, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.018500328063965, 'eval_runtime': 9.0088, 'eval_samples_per_second': 110.891, 'eval_steps_per_second': 6.993, 'epoch': 1.0}
{'train_runtime': 516.7311, 'train_samples_per_second': 19.349, 'train_steps_per_second': 1.21, 'train_loss': 2.2598784362792967, 'epoch': 1.0}
train_results:  {'eval_loss': [2.983693838119507, 2.755750894546509, 2.5308897495269775, 2.410944938659668, 2.330070734024048, 2.274066925048828, 2.239757776260376, 2.2164194583892822, 2.1941123008728027, 2.173290967941284, 2.152634859085083, 2.1366591453552246, 2.1210007667541504, 2.1030468940734863, 2.0893876552581787, 2.080259084701538, 2.0694234371185303, 2.0592238903045654, 2.051886558532715, 2.0442588329315186, 2.0374104976654053, 2.030595541000366, 2.0244641304016113, 2.0198326110839844, 2.018500328063965], 'performance': [0.77, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<13:46,  8.34s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:11,  1.16it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:43,  1.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:24,  2.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:14,  2.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:41<00:08,  2.33it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:43<00:00,  3.09it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:43<00:00,  2.31it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2274508476257324
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8959 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.9576964038244199
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.9229071627955251
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 0.9684281515850727
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.945444957883462
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.9591718426607148
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.7671, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.2329, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.2034e-17, dtype=torch.float64), tensor(0.7671, dtype=torch.float64), tensor(1.4216e-17, dtype=torch.float64), tensor(7.5333e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.2180e-17, dtype=torch.float64), tensor(4.7311e-17, dtype=torch.float64), tensor(5.6200e-18, dtype=torch.float64), tensor(0.2329, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.767
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.233

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:35,  2.78s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:11,  1.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.00it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.55it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.65it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:20,  2.81it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.93it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.20it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.25it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.43it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.01it/s]
Evaluation performance at step 25: 0.76
{'loss': 2.7953, 'grad_norm': 4.151211738586426, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 2.4090683460235596, 'eval_runtime': 8.7824, 'eval_samples_per_second': 113.751, 'eval_steps_per_second': 7.173, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:43,  4.08s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:54,  1.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:34,  2.40it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.60it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:18,  2.78it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.10it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.63it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.93it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.86it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.0077, 'grad_norm': 8.696370124816895, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.7069557905197144, 'eval_runtime': 8.757, 'eval_samples_per_second': 114.081, 'eval_steps_per_second': 7.194, 'epoch': 0.08}
{'loss': 1.5271, 'grad_norm': 4.085549354553223, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3685332536697388, 'eval_runtime': 8.7355, 'eval_samples_per_second': 114.361, 'eval_steps_per_second': 7.212, 'epoch': 0.12}
{'loss': 1.3248, 'grad_norm': 4.582311153411865, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.269197940826416, 'eval_runtime': 8.7727, 'eval_samples_per_second': 113.876, 'eval_steps_per_second': 7.181, 'epoch': 0.16}
{'loss': 1.2513, 'grad_norm': 7.677075386047363, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1981182098388672, 'eval_runtime': 8.7904, 'eval_samples_per_second': 113.647, 'eval_steps_per_second': 7.167, 'epoch': 0.2}
{'loss': 1.1882, 'grad_norm': 5.00466251373291, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1679534912109375, 'eval_runtime': 8.7862, 'eval_samples_per_second': 113.7, 'eval_steps_per_second': 7.17, 'epoch': 0.24}
{'loss': 1.1526, 'grad_norm': 4.671009063720703, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1425997018814087, 'eval_runtime': 8.795, 'eval_samples_per_second': 113.587, 'eval_steps_per_second': 7.163, 'epoch': 0.28}
{'loss': 1.1339, 'grad_norm': 3.4662206172943115, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1349036693572998, 'eval_runtime': 8.8553, 'eval_samples_per_second': 112.814, 'eval_steps_per_second': 7.114, 'epoch': 0.32}
{'loss': 1.1263, 'grad_norm': 5.768499851226807, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1225175857543945, 'eval_runtime': 8.8982, 'eval_samples_per_second': 112.27, 'eval_steps_per_second': 7.08, 'epoch': 0.36}
{'loss': 1.1307, 'grad_norm': 4.832884788513184, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.110599398612976, 'eval_runtime': 8.8907, 'eval_samples_per_second': 112.364, 'eval_steps_per_second': 7.086, 'epoch': 0.4}
{'loss': 1.0782, 'grad_norm': 2.625002384185791, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0928961038589478, 'eval_runtime': 8.8713, 'eval_samples_per_second': 112.61, 'eval_steps_per_second': 7.102, 'epoch': 0.44}
{'loss': 1.0961, 'grad_norm': 3.383100748062134, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0516139268875122, 'eval_runtime': 8.8714, 'eval_samples_per_second': 112.61, 'eval_steps_per_second': 7.102, 'epoch': 0.48}
{'loss': 1.0365, 'grad_norm': 2.072049379348755, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0328385829925537, 'eval_runtime': 8.8627, 'eval_samples_per_second': 112.72, 'eval_steps_per_second': 7.108, 'epoch': 0.52}
{'loss': 1.0481, 'grad_norm': 2.7209086418151855, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0270265340805054, 'eval_runtime': 8.8436, 'eval_samples_per_second': 112.964, 'eval_steps_per_second': 7.124, 'epoch': 0.56}
{'loss': 1.0193, 'grad_norm': 2.2967231273651123, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0181658267974854, 'eval_runtime': 8.8321, 'eval_samples_per_second': 113.11, 'eval_steps_per_second': 7.133, 'epoch': 0.6}
{'loss': 1.0134, 'grad_norm': 3.590195417404175, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.003206729888916, 'eval_runtime': 8.822, 'eval_samples_per_second': 113.24, 'eval_steps_per_second': 7.141, 'epoch': 0.64}
{'loss': 1.002, 'grad_norm': 28.681760787963867, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.990516185760498, 'eval_runtime': 8.8271, 'eval_samples_per_second': 113.174, 'eval_steps_per_second': 7.137, 'epoch': 0.68}
{'loss': 0.9917, 'grad_norm': 9.871848106384277, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.980680525302887, 'eval_runtime': 8.812, 'eval_samples_per_second': 113.368, 'eval_steps_per_second': 7.149, 'epoch': 0.72}
{'loss': 0.9972, 'grad_norm': 745.8175659179688, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.073588490486145, 'eval_runtime': 8.8185, 'eval_samples_per_second': 113.284, 'eval_steps_per_second': 7.144, 'epoch': 0.76}
{'loss': 0.9825, 'grad_norm': 1.7890490293502808, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9718174934387207, 'eval_runtime': 8.8155, 'eval_samples_per_second': 113.323, 'eval_steps_per_second': 7.147, 'epoch': 0.8}
{'loss': 0.9833, 'grad_norm': 4.275814533233643, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9713584184646606, 'eval_runtime': 8.8044, 'eval_samples_per_second': 113.466, 'eval_steps_per_second': 7.156, 'epoch': 0.84}
{'loss': 0.9731, 'grad_norm': 6.894251346588135, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9707712531089783, 'eval_runtime': 8.8304, 'eval_samples_per_second': 113.133, 'eval_steps_per_second': 7.134, 'epoch': 0.88}
{'loss': 0.9724, 'grad_norm': 3.1202104091644287, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9679381251335144, 'eval_runtime': 8.8113, 'eval_samples_per_second': 113.377, 'eval_steps_per_second': 7.15, 'epoch': 0.92}
{'loss': 0.9661, 'grad_norm': 2.8315303325653076, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9670686721801758, 'eval_runtime': 8.8362, 'eval_samples_per_second': 113.058, 'eval_steps_per_second': 7.13, 'epoch': 0.96}
{'loss': 0.989, 'grad_norm': 2.7724108695983887, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9666818976402283, 'eval_runtime': 8.8596, 'eval_samples_per_second': 112.759, 'eval_steps_per_second': 7.111, 'epoch': 1.0}
{'train_runtime': 511.2892, 'train_samples_per_second': 19.556, 'train_steps_per_second': 1.222, 'train_loss': 1.191474819946289, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4090683460235596, 1.7069557905197144, 1.3685332536697388, 1.269197940826416, 1.1981182098388672, 1.1679534912109375, 1.1425997018814087, 1.1349036693572998, 1.1225175857543945, 1.110599398612976, 1.0928961038589478, 1.0516139268875122, 1.0328385829925537, 1.0270265340805054, 1.0181658267974854, 1.003206729888916, 0.990516185760498, 0.980680525302887, 1.073588490486145, 0.9718174934387207, 0.9713584184646606, 0.9707712531089783, 0.9679381251335144, 0.9670686721801758, 0.9666818976402283], 'performance': [0.76, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<10:20,  6.27s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:52,  1.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:17<00:31,  2.16it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:17,  2.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.08it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.66it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.85it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  1.2276301383972168
current iteration best possible performance (full train run):  0.8400000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.2880 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.8270220848058413
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.9461933681142581
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.9522924388688687
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.9395645514432371
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.9380770518559702
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4998, dtype=torch.float64), tensor(0.2730, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.2272, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 8.673617379884037e-19, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.4998, dtype=torch.float64), tensor(0.2730, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.4784e-17, dtype=torch.float64), tensor(5.8030e-17, dtype=torch.float64), tensor(0.2272, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(8.6736e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.5
  rowan_hellaswag: 0.273
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.227

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (8.673617379884037e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  8.673617379884037e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:12,  2.55s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:10,  1.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  1.99it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:27,  2.44it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.63it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.72it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.82it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.04it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.57it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.86it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.75it/s]
Evaluation performance at step 25: 0.76
{'loss': 3.2679, 'grad_norm': 3.891115665435791, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 2.9073944091796875, 'eval_runtime': 8.9692, 'eval_samples_per_second': 111.381, 'eval_steps_per_second': 7.024, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:36,  4.01s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:53,  1.69it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:35,  2.34it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:24,  2.77it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:21,  2.70it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.86it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.91it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.10it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.61it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.22it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.87it/s]
Evaluation performance at step 50: 0.78
{'loss': 2.5251, 'grad_norm': 4.235296726226807, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 2.2082791328430176, 'eval_runtime': 8.9251, 'eval_samples_per_second': 111.932, 'eval_steps_per_second': 7.059, 'epoch': 0.08}
{'loss': 2.018, 'grad_norm': 4.4696364402771, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8957189321517944, 'eval_runtime': 8.9364, 'eval_samples_per_second': 111.79, 'eval_steps_per_second': 7.05, 'epoch': 0.12}
{'loss': 1.8099, 'grad_norm': 7.52022123336792, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7305376529693604, 'eval_runtime': 8.9672, 'eval_samples_per_second': 111.405, 'eval_steps_per_second': 7.026, 'epoch': 0.16}
{'loss': 1.664, 'grad_norm': 5.189218997955322, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6365755796432495, 'eval_runtime': 8.9965, 'eval_samples_per_second': 111.043, 'eval_steps_per_second': 7.003, 'epoch': 0.2}
{'loss': 1.5793, 'grad_norm': 4.8740739822387695, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5762813091278076, 'eval_runtime': 9.01, 'eval_samples_per_second': 110.877, 'eval_steps_per_second': 6.992, 'epoch': 0.24}
{'loss': 1.5649, 'grad_norm': 8.491903305053711, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.542842149734497, 'eval_runtime': 9.0124, 'eval_samples_per_second': 110.848, 'eval_steps_per_second': 6.99, 'epoch': 0.28}
{'loss': 1.5269, 'grad_norm': 6.88460111618042, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5232924222946167, 'eval_runtime': 9.0037, 'eval_samples_per_second': 110.954, 'eval_steps_per_second': 6.997, 'epoch': 0.32}
{'loss': 1.4962, 'grad_norm': 11.264240264892578, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4983773231506348, 'eval_runtime': 9.0077, 'eval_samples_per_second': 110.905, 'eval_steps_per_second': 6.994, 'epoch': 0.36}
{'loss': 1.4875, 'grad_norm': 5.516670227050781, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4823464155197144, 'eval_runtime': 9.0007, 'eval_samples_per_second': 110.991, 'eval_steps_per_second': 6.999, 'epoch': 0.4}
{'loss': 1.4539, 'grad_norm': 6.589114189147949, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4636703729629517, 'eval_runtime': 8.9843, 'eval_samples_per_second': 111.195, 'eval_steps_per_second': 7.012, 'epoch': 0.44}
{'loss': 1.4021, 'grad_norm': 6.003326892852783, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4185832738876343, 'eval_runtime': 9.0048, 'eval_samples_per_second': 110.941, 'eval_steps_per_second': 6.996, 'epoch': 0.48}
{'loss': 1.3983, 'grad_norm': 4.790084362030029, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.39817476272583, 'eval_runtime': 8.9776, 'eval_samples_per_second': 111.277, 'eval_steps_per_second': 7.017, 'epoch': 0.52}
{'loss': 1.3845, 'grad_norm': 3.6817259788513184, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3879666328430176, 'eval_runtime': 8.9473, 'eval_samples_per_second': 111.654, 'eval_steps_per_second': 7.041, 'epoch': 0.56}
{'loss': 1.3735, 'grad_norm': 5.577662944793701, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3799277544021606, 'eval_runtime': 8.9436, 'eval_samples_per_second': 111.7, 'eval_steps_per_second': 7.044, 'epoch': 0.6}
{'loss': 1.3573, 'grad_norm': 4.272389888763428, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3725634813308716, 'eval_runtime': 8.9563, 'eval_samples_per_second': 111.542, 'eval_steps_per_second': 7.034, 'epoch': 0.64}
{'loss': 1.3989, 'grad_norm': 4.445127010345459, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3669482469558716, 'eval_runtime': 8.937, 'eval_samples_per_second': 111.783, 'eval_steps_per_second': 7.049, 'epoch': 0.68}
{'loss': 1.3574, 'grad_norm': 4.143712997436523, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3590666055679321, 'eval_runtime': 8.9447, 'eval_samples_per_second': 111.686, 'eval_steps_per_second': 7.043, 'epoch': 0.72}
{'loss': 1.3302, 'grad_norm': 3.3976099491119385, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3474754095077515, 'eval_runtime': 8.9461, 'eval_samples_per_second': 111.668, 'eval_steps_per_second': 7.042, 'epoch': 0.76}
{'loss': 1.3663, 'grad_norm': 7.118587970733643, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3361114263534546, 'eval_runtime': 8.9231, 'eval_samples_per_second': 111.957, 'eval_steps_per_second': 7.06, 'epoch': 0.8}
{'loss': 1.2982, 'grad_norm': 7.995762348175049, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3275130987167358, 'eval_runtime': 8.936, 'eval_samples_per_second': 111.795, 'eval_steps_per_second': 7.05, 'epoch': 0.84}
{'loss': 1.297, 'grad_norm': 4.38519287109375, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3226945400238037, 'eval_runtime': 8.946, 'eval_samples_per_second': 111.67, 'eval_steps_per_second': 7.042, 'epoch': 0.88}
{'loss': 1.3307, 'grad_norm': 5.608217716217041, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3205622434616089, 'eval_runtime': 8.9733, 'eval_samples_per_second': 111.331, 'eval_steps_per_second': 7.021, 'epoch': 0.92}
{'loss': 1.2984, 'grad_norm': 3.930417060852051, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.318569540977478, 'eval_runtime': 9.0213, 'eval_samples_per_second': 110.738, 'eval_steps_per_second': 6.983, 'epoch': 0.96}
{'loss': 1.2713, 'grad_norm': 3.1480929851531982, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.316941738128662, 'eval_runtime': 9.0077, 'eval_samples_per_second': 110.905, 'eval_steps_per_second': 6.994, 'epoch': 1.0}
{'train_runtime': 520.7888, 'train_samples_per_second': 19.2, 'train_steps_per_second': 1.2, 'train_loss': 1.5703072509765625, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9073944091796875, 2.2082791328430176, 1.8957189321517944, 1.7305376529693604, 1.6365755796432495, 1.5762813091278076, 1.542842149734497, 1.5232924222946167, 1.4983773231506348, 1.4823464155197144, 1.4636703729629517, 1.4185832738876343, 1.39817476272583, 1.3879666328430176, 1.3799277544021606, 1.3725634813308716, 1.3669482469558716, 1.3590666055679321, 1.3474754095077515, 1.3361114263534546, 1.3275130987167358, 1.3226945400238037, 1.3205622434616089, 1.318569540977478, 1.316941738128662], 'performance': [0.76, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:53,  9.63s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<00:59,  1.39it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:20<00:33,  2.02it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:18,  2.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:11,  2.99it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:35<00:07,  2.62it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.39it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.66it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  1.2279679775238037
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9991 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.9300245016951585
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 0.9476649739134136
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.9476920153239081
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.9428530040485346
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.8584849289850892
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.7805, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.2195, dtype=torch.float64), 1, 1, 1, 0, 0, 1, 2, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.7805, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1966e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.1629e-17, dtype=torch.float64), tensor(7.5470e-18, dtype=torch.float64), tensor(9.4169e-17, dtype=torch.float64), tensor(0.2195, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.78
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.22

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 63,488 || all params: 8,030,324,736 || trainable%: 0.0008
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:43,  4.07s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:21,  1.12it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.80it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:33,  2.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.56it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:15,  2.71it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  3.08it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.87it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.67it/s]
Evaluation performance at step 25: 0.77
{'loss': 2.6136, 'grad_norm': 18.05038833618164, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.2119596004486084, 'eval_runtime': 8.8576, 'eval_samples_per_second': 112.785, 'eval_steps_per_second': 7.113, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:39,  3.43s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:17,  1.18it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:45,  1.84it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.67it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:20,  2.82it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.63it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.77it/s]
Evaluation performance at step 50: 0.77
{'loss': 1.9242, 'grad_norm': 7.672039031982422, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 1.6604759693145752, 'eval_runtime': 8.8565, 'eval_samples_per_second': 112.799, 'eval_steps_per_second': 7.113, 'epoch': 0.08}
{'loss': 1.4599, 'grad_norm': 8.692427635192871, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2732176780700684, 'eval_runtime': 8.8871, 'eval_samples_per_second': 112.409, 'eval_steps_per_second': 7.089, 'epoch': 0.12}
{'loss': 1.2166, 'grad_norm': 6.049881935119629, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1633094549179077, 'eval_runtime': 8.9057, 'eval_samples_per_second': 112.176, 'eval_steps_per_second': 7.074, 'epoch': 0.16}
{'loss': 1.1496, 'grad_norm': 6.9510908126831055, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0824918746948242, 'eval_runtime': 8.9351, 'eval_samples_per_second': 111.806, 'eval_steps_per_second': 7.051, 'epoch': 0.2}
{'loss': 1.0633, 'grad_norm': 4.29417085647583, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0543612241744995, 'eval_runtime': 8.9067, 'eval_samples_per_second': 112.163, 'eval_steps_per_second': 7.073, 'epoch': 0.24}
{'loss': 1.0381, 'grad_norm': 3.2027640342712402, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0410212278366089, 'eval_runtime': 8.9157, 'eval_samples_per_second': 112.05, 'eval_steps_per_second': 7.066, 'epoch': 0.28}
{'loss': 1.0563, 'grad_norm': 3.1213696002960205, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0343859195709229, 'eval_runtime': 8.9027, 'eval_samples_per_second': 112.213, 'eval_steps_per_second': 7.076, 'epoch': 0.32}
{'loss': 1.0351, 'grad_norm': 3.9439637660980225, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0219531059265137, 'eval_runtime': 8.9073, 'eval_samples_per_second': 112.155, 'eval_steps_per_second': 7.073, 'epoch': 0.36}
{'loss': 1.0317, 'grad_norm': 3.4261579513549805, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0051798820495605, 'eval_runtime': 8.9044, 'eval_samples_per_second': 112.192, 'eval_steps_per_second': 7.075, 'epoch': 0.4}
{'loss': 0.9876, 'grad_norm': 3.80049991607666, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9865086078643799, 'eval_runtime': 8.9139, 'eval_samples_per_second': 112.072, 'eval_steps_per_second': 7.068, 'epoch': 0.44}
{'loss': 0.9907, 'grad_norm': 5.202792167663574, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9768181443214417, 'eval_runtime': 8.9197, 'eval_samples_per_second': 111.999, 'eval_steps_per_second': 7.063, 'epoch': 0.48}
{'loss': 0.9585, 'grad_norm': 8.454647064208984, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9708815217018127, 'eval_runtime': 8.9223, 'eval_samples_per_second': 111.967, 'eval_steps_per_second': 7.061, 'epoch': 0.52}
{'loss': 0.9796, 'grad_norm': 10.033230781555176, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9703591465950012, 'eval_runtime': 8.888, 'eval_samples_per_second': 112.399, 'eval_steps_per_second': 7.088, 'epoch': 0.56}
{'loss': 0.9712, 'grad_norm': 14.161321640014648, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9670136570930481, 'eval_runtime': 8.8567, 'eval_samples_per_second': 112.796, 'eval_steps_per_second': 7.113, 'epoch': 0.6}
{'loss': 0.9716, 'grad_norm': 4.167654037475586, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9613323211669922, 'eval_runtime': 8.8619, 'eval_samples_per_second': 112.73, 'eval_steps_per_second': 7.109, 'epoch': 0.64}
{'loss': 0.9678, 'grad_norm': 4.132359504699707, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9609410166740417, 'eval_runtime': 8.8474, 'eval_samples_per_second': 112.915, 'eval_steps_per_second': 7.121, 'epoch': 0.68}
{'loss': 0.9549, 'grad_norm': 3.0668742656707764, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9561064839363098, 'eval_runtime': 8.8638, 'eval_samples_per_second': 112.705, 'eval_steps_per_second': 7.108, 'epoch': 0.72}
{'loss': 0.9539, 'grad_norm': 6.1881890296936035, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9557848572731018, 'eval_runtime': 8.8787, 'eval_samples_per_second': 112.516, 'eval_steps_per_second': 7.096, 'epoch': 0.76}
{'loss': 0.9662, 'grad_norm': 2.3840744495391846, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9538991451263428, 'eval_runtime': 8.8827, 'eval_samples_per_second': 112.466, 'eval_steps_per_second': 7.092, 'epoch': 0.8}
{'loss': 0.9562, 'grad_norm': 6.909169673919678, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9538405537605286, 'eval_runtime': 8.9619, 'eval_samples_per_second': 111.471, 'eval_steps_per_second': 7.03, 'epoch': 0.84}
{'loss': 0.9686, 'grad_norm': 5.07733154296875, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.949498176574707, 'eval_runtime': 8.9747, 'eval_samples_per_second': 111.313, 'eval_steps_per_second': 7.02, 'epoch': 0.88}
{'loss': 0.9606, 'grad_norm': 3.766836643218994, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9489302039146423, 'eval_runtime': 8.9987, 'eval_samples_per_second': 111.017, 'eval_steps_per_second': 7.001, 'epoch': 0.92}
{'loss': 0.9626, 'grad_norm': 5.428950309753418, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9479832053184509, 'eval_runtime': 8.9797, 'eval_samples_per_second': 111.251, 'eval_steps_per_second': 7.016, 'epoch': 0.96}
{'loss': 0.9549, 'grad_norm': 3.8495867252349854, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.946927547454834, 'eval_runtime': 8.9666, 'eval_samples_per_second': 111.413, 'eval_steps_per_second': 7.026, 'epoch': 1.0}
{'train_runtime': 526.4614, 'train_samples_per_second': 18.993, 'train_steps_per_second': 1.187, 'train_loss': 1.1237258422851562, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2119596004486084, 1.6604759693145752, 1.2732176780700684, 1.1633094549179077, 1.0824918746948242, 1.0543612241744995, 1.0410212278366089, 1.0343859195709229, 1.0219531059265137, 1.0051798820495605, 0.9865086078643799, 0.9768181443214417, 0.9708815217018127, 0.9703591465950012, 0.9670136570930481, 0.9613323211669922, 0.9609410166740417, 0.9561064839363098, 0.9557848572731018, 0.9538991451263428, 0.9538405537605286, 0.949498176574707, 0.9489302039146423, 0.9479832053184509, 0.946927547454834], 'performance': [0.77, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:07<11:57,  7.25s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:06,  1.25it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:42,  1.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:26,  1.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:37<00:17,  1.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:45<00:09,  2.02it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:47<00:01,  2.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:47<00:00,  2.12it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  1.213857889175415
current iteration best possible performance (full train run):  0.756
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8127 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.9406996495131935
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.941424419306369
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.9414243367071002
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.8844124446983176
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.7949094375134053
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4341, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.3457, dtype=torch.float64), tensor(0.2203, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.0, 47.99999999999999, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.4341, dtype=torch.float64), tensor(4.7719e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.4286e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3457, dtype=torch.float64), tensor(0.2203, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.434
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.346
  arc_challenge: 0.22

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:02,  2.45s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:23,  2.80it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:20,  2.81it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.83it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.12it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.86it/s]
Evaluation performance at step 25: 0.76
{'loss': 3.0412, 'grad_norm': 4.325352191925049, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 2.7226991653442383, 'eval_runtime': 8.7592, 'eval_samples_per_second': 114.052, 'eval_steps_per_second': 7.192, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:41,  5.27s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:02,  1.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:37,  2.20it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:27,  2.72it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.77it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.73it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.87it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.28it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:07,  3.64it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:05,  3.46it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:30<00:03,  3.60it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:32<00:00,  3.82it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:32<00:00,  3.06it/s]
Evaluation performance at step 50: 0.78
{'loss': 2.2829, 'grad_norm': 3.192270517349243, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 1.9567487239837646, 'eval_runtime': 8.7224, 'eval_samples_per_second': 114.533, 'eval_steps_per_second': 7.223, 'epoch': 0.08}
{'loss': 1.7487, 'grad_norm': 7.698532581329346, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6440315246582031, 'eval_runtime': 8.8035, 'eval_samples_per_second': 113.478, 'eval_steps_per_second': 7.156, 'epoch': 0.12}
{'loss': 1.5659, 'grad_norm': 3.0784990787506104, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4852840900421143, 'eval_runtime': 8.8106, 'eval_samples_per_second': 113.386, 'eval_steps_per_second': 7.15, 'epoch': 0.16}
{'loss': 1.438, 'grad_norm': 4.732569217681885, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4184633493423462, 'eval_runtime': 8.8076, 'eval_samples_per_second': 113.425, 'eval_steps_per_second': 7.153, 'epoch': 0.2}
{'loss': 1.3831, 'grad_norm': 6.642943859100342, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3772298097610474, 'eval_runtime': 8.849, 'eval_samples_per_second': 112.895, 'eval_steps_per_second': 7.119, 'epoch': 0.24}
{'loss': 1.3759, 'grad_norm': 3.807274341583252, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3645894527435303, 'eval_runtime': 8.8818, 'eval_samples_per_second': 112.477, 'eval_steps_per_second': 7.093, 'epoch': 0.28}
{'loss': 1.2784, 'grad_norm': 5.091902732849121, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3385297060012817, 'eval_runtime': 8.8858, 'eval_samples_per_second': 112.427, 'eval_steps_per_second': 7.09, 'epoch': 0.32}
{'loss': 1.3149, 'grad_norm': 3.9828100204467773, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3269412517547607, 'eval_runtime': 8.9081, 'eval_samples_per_second': 112.145, 'eval_steps_per_second': 7.072, 'epoch': 0.36}
{'loss': 1.2496, 'grad_norm': 3.0171687602996826, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2966779470443726, 'eval_runtime': 8.8475, 'eval_samples_per_second': 112.913, 'eval_steps_per_second': 7.121, 'epoch': 0.4}
{'loss': 1.2505, 'grad_norm': 4.438414096832275, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2444859743118286, 'eval_runtime': 8.8666, 'eval_samples_per_second': 112.67, 'eval_steps_per_second': 7.105, 'epoch': 0.44}
{'loss': 1.2243, 'grad_norm': 4.818487644195557, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2382334470748901, 'eval_runtime': 8.8466, 'eval_samples_per_second': 112.925, 'eval_steps_per_second': 7.121, 'epoch': 0.48}
{'loss': 1.1917, 'grad_norm': 3.3709115982055664, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.226395606994629, 'eval_runtime': 8.867, 'eval_samples_per_second': 112.665, 'eval_steps_per_second': 7.105, 'epoch': 0.52}
{'loss': 1.2216, 'grad_norm': 4.209527492523193, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2179210186004639, 'eval_runtime': 8.8638, 'eval_samples_per_second': 112.706, 'eval_steps_per_second': 7.108, 'epoch': 0.56}
{'loss': 1.1817, 'grad_norm': 3.621264696121216, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1973763704299927, 'eval_runtime': 8.8538, 'eval_samples_per_second': 112.834, 'eval_steps_per_second': 7.116, 'epoch': 0.6}
{'loss': 1.1585, 'grad_norm': 7.284346103668213, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.181873083114624, 'eval_runtime': 8.8642, 'eval_samples_per_second': 112.701, 'eval_steps_per_second': 7.107, 'epoch': 0.64}
{'loss': 1.1673, 'grad_norm': 5.972184658050537, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.177111268043518, 'eval_runtime': 8.8632, 'eval_samples_per_second': 112.713, 'eval_steps_per_second': 7.108, 'epoch': 0.68}
{'loss': 1.1711, 'grad_norm': 9.302546501159668, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.181318759918213, 'eval_runtime': 8.8757, 'eval_samples_per_second': 112.554, 'eval_steps_per_second': 7.098, 'epoch': 0.72}
{'loss': 1.1543, 'grad_norm': 2.9255928993225098, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1673773527145386, 'eval_runtime': 8.8705, 'eval_samples_per_second': 112.62, 'eval_steps_per_second': 7.102, 'epoch': 0.76}
{'loss': 1.136, 'grad_norm': 9.078472137451172, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1663706302642822, 'eval_runtime': 8.8622, 'eval_samples_per_second': 112.726, 'eval_steps_per_second': 7.109, 'epoch': 0.8}
{'loss': 1.156, 'grad_norm': 7.430769443511963, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1643078327178955, 'eval_runtime': 8.8539, 'eval_samples_per_second': 112.831, 'eval_steps_per_second': 7.115, 'epoch': 0.84}
{'loss': 1.1661, 'grad_norm': 9.40092945098877, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.163794994354248, 'eval_runtime': 8.8464, 'eval_samples_per_second': 112.927, 'eval_steps_per_second': 7.122, 'epoch': 0.88}
{'loss': 1.1444, 'grad_norm': 15.677783012390137, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.163224458694458, 'eval_runtime': 8.8596, 'eval_samples_per_second': 112.76, 'eval_steps_per_second': 7.111, 'epoch': 0.92}
{'loss': 1.1372, 'grad_norm': 7.746218681335449, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1612015962600708, 'eval_runtime': 8.8663, 'eval_samples_per_second': 112.674, 'eval_steps_per_second': 7.106, 'epoch': 0.96}
{'loss': 1.1146, 'grad_norm': 5.530407905578613, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.16089928150177, 'eval_runtime': 8.8577, 'eval_samples_per_second': 112.784, 'eval_steps_per_second': 7.112, 'epoch': 1.0}
{'train_runtime': 508.9248, 'train_samples_per_second': 19.645, 'train_steps_per_second': 1.228, 'train_loss': 1.3701544494628906, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7226991653442383, 1.9567487239837646, 1.6440315246582031, 1.4852840900421143, 1.4184633493423462, 1.3772298097610474, 1.3645894527435303, 1.3385297060012817, 1.3269412517547607, 1.2966779470443726, 1.2444859743118286, 1.2382334470748901, 1.226395606994629, 1.2179210186004639, 1.1973763704299927, 1.181873083114624, 1.177111268043518, 1.181318759918213, 1.1673773527145386, 1.1663706302642822, 1.1643078327178955, 1.163794994354248, 1.163224458694458, 1.1612015962600708, 1.16089928150177], 'performance': [0.76, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:52,  9.62s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.87it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  1.2302820682525635
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9399 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.9361912185853036
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.9045407768806213
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.8939919351793764
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.794471592860963
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.915880539403852
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4199, dtype=torch.float64), 0, 0, 0, tensor(0.3570, dtype=torch.float64), 0, 0, tensor(0.2232, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.4199, dtype=torch.float64), tensor(8.7201e-17, dtype=torch.float64), tensor(2.6706e-18, dtype=torch.float64), tensor(7.0606e-18, dtype=torch.float64), tensor(0.3570, dtype=torch.float64), tensor(1.6132e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2232, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.42
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.357
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.223

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:13,  3.17s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:14,  1.21it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.94it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.61it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.75it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.75it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.5118, 'grad_norm': 1.7272542715072632, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.33573055267334, 'eval_runtime': 8.4437, 'eval_samples_per_second': 118.313, 'eval_steps_per_second': 7.461, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:08,  3.11s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:46,  1.96it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:33,  2.51it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:25,  2.97it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  3.01it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.87it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:19,  2.56it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:11,  3.10it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.29it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:05,  3.37it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:30<00:03,  3.54it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:31<00:00,  3.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:31<00:00,  3.14it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.2165, 'grad_norm': 4.83670711517334, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 2.967799425125122, 'eval_runtime': 8.4559, 'eval_samples_per_second': 118.142, 'eval_steps_per_second': 7.45, 'epoch': 0.08}
{'loss': 2.9247, 'grad_norm': 12.17949104309082, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.7897467613220215, 'eval_runtime': 8.434, 'eval_samples_per_second': 118.449, 'eval_steps_per_second': 7.47, 'epoch': 0.12}
{'loss': 2.8433, 'grad_norm': 6.548666000366211, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.6762630939483643, 'eval_runtime': 8.4749, 'eval_samples_per_second': 117.878, 'eval_steps_per_second': 7.434, 'epoch': 0.16}
{'loss': 2.7039, 'grad_norm': 6.908799648284912, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.633840322494507, 'eval_runtime': 8.4831, 'eval_samples_per_second': 117.764, 'eval_steps_per_second': 7.427, 'epoch': 0.2}
{'loss': 2.6375, 'grad_norm': 2.5055840015411377, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.580761671066284, 'eval_runtime': 8.4973, 'eval_samples_per_second': 117.567, 'eval_steps_per_second': 7.414, 'epoch': 0.24}
{'loss': 2.6436, 'grad_norm': 15.71629524230957, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.5468945503234863, 'eval_runtime': 8.575, 'eval_samples_per_second': 116.502, 'eval_steps_per_second': 7.347, 'epoch': 0.28}
{'loss': 2.5166, 'grad_norm': 2.5619280338287354, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.4835479259490967, 'eval_runtime': 8.5702, 'eval_samples_per_second': 116.567, 'eval_steps_per_second': 7.351, 'epoch': 0.32}
{'loss': 2.5153, 'grad_norm': 5.944936275482178, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.4509153366088867, 'eval_runtime': 8.5955, 'eval_samples_per_second': 116.224, 'eval_steps_per_second': 7.329, 'epoch': 0.36}
{'loss': 2.4408, 'grad_norm': 1.4272277355194092, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.4169158935546875, 'eval_runtime': 8.5756, 'eval_samples_per_second': 116.494, 'eval_steps_per_second': 7.346, 'epoch': 0.4}
{'loss': 2.4483, 'grad_norm': 3.122631549835205, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.39699387550354, 'eval_runtime': 8.5325, 'eval_samples_per_second': 117.082, 'eval_steps_per_second': 7.384, 'epoch': 0.44}
{'loss': 2.4006, 'grad_norm': 2.0167248249053955, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.378669023513794, 'eval_runtime': 8.5048, 'eval_samples_per_second': 117.464, 'eval_steps_per_second': 7.408, 'epoch': 0.48}
{'loss': 2.3791, 'grad_norm': 1.2239757776260376, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.3451924324035645, 'eval_runtime': 8.4975, 'eval_samples_per_second': 117.564, 'eval_steps_per_second': 7.414, 'epoch': 0.52}
{'loss': 2.3642, 'grad_norm': 4.435613632202148, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.3174824714660645, 'eval_runtime': 8.4988, 'eval_samples_per_second': 117.546, 'eval_steps_per_second': 7.413, 'epoch': 0.56}
{'loss': 2.3177, 'grad_norm': 4.025840759277344, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.295933723449707, 'eval_runtime': 8.4915, 'eval_samples_per_second': 117.647, 'eval_steps_per_second': 7.419, 'epoch': 0.6}
{'loss': 2.322, 'grad_norm': 1.648061752319336, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.285421133041382, 'eval_runtime': 8.55, 'eval_samples_per_second': 116.841, 'eval_steps_per_second': 7.368, 'epoch': 0.64}
{'loss': 2.3218, 'grad_norm': 2.700948476791382, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2716281414031982, 'eval_runtime': 8.561, 'eval_samples_per_second': 116.691, 'eval_steps_per_second': 7.359, 'epoch': 0.68}
{'loss': 2.288, 'grad_norm': 4.794797897338867, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.262974262237549, 'eval_runtime': 8.5709, 'eval_samples_per_second': 116.557, 'eval_steps_per_second': 7.35, 'epoch': 0.72}
{'loss': 2.2619, 'grad_norm': 2.2275235652923584, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.2528092861175537, 'eval_runtime': 8.6274, 'eval_samples_per_second': 115.794, 'eval_steps_per_second': 7.302, 'epoch': 0.76}
{'loss': 2.2457, 'grad_norm': 2.1393840312957764, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.2447359561920166, 'eval_runtime': 8.6331, 'eval_samples_per_second': 115.717, 'eval_steps_per_second': 7.297, 'epoch': 0.8}
{'loss': 2.3301, 'grad_norm': 2.8863306045532227, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.234760046005249, 'eval_runtime': 8.5891, 'eval_samples_per_second': 116.31, 'eval_steps_per_second': 7.335, 'epoch': 0.84}
{'loss': 2.2918, 'grad_norm': 3.5378823280334473, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.2274374961853027, 'eval_runtime': 8.5854, 'eval_samples_per_second': 116.36, 'eval_steps_per_second': 7.338, 'epoch': 0.88}
{'loss': 2.2674, 'grad_norm': 2.3701300621032715, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.221491813659668, 'eval_runtime': 8.5887, 'eval_samples_per_second': 116.316, 'eval_steps_per_second': 7.335, 'epoch': 0.92}
{'loss': 2.2933, 'grad_norm': 1.2623261213302612, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.2173423767089844, 'eval_runtime': 8.5856, 'eval_samples_per_second': 116.357, 'eval_steps_per_second': 7.338, 'epoch': 0.96}
{'loss': 2.2169, 'grad_norm': 1.1877425909042358, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.216158390045166, 'eval_runtime': 8.6006, 'eval_samples_per_second': 116.155, 'eval_steps_per_second': 7.325, 'epoch': 1.0}
{'train_runtime': 492.2253, 'train_samples_per_second': 20.312, 'train_steps_per_second': 1.27, 'train_loss': 2.508109704589844, 'epoch': 1.0}
train_results:  {'eval_loss': [3.33573055267334, 2.967799425125122, 2.7897467613220215, 2.6762630939483643, 2.633840322494507, 2.580761671066284, 2.5468945503234863, 2.4835479259490967, 2.4509153366088867, 2.4169158935546875, 2.39699387550354, 2.378669023513794, 2.3451924324035645, 2.3174824714660645, 2.295933723449707, 2.285421133041382, 2.2716281414031982, 2.262974262237549, 2.2528092861175537, 2.2447359561920166, 2.234760046005249, 2.2274374961853027, 2.221491813659668, 2.2173423767089844, 2.216158390045166], 'performance': [0.77, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:56,  9.67s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:09,  1.19it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:42,  1.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:25,  1.97it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:35<00:14,  2.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:42<00:08,  2.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:44<00:00,  3.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:44<00:00,  2.24it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.227649211883545
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9285 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.921042467194211
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 0.7451182510965553
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 0.7554873736582995
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.9176412743705963
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.8832446730711982
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1420, dtype=torch.float64), 0, 0, tensor(0.6377, dtype=torch.float64), 0, 0, 0, tensor(0.2203, dtype=torch.float64), 1, 0, 0, 1, 1, 0, 2, 0.0, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.1420, dtype=torch.float64), tensor(7.5588e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6377, dtype=torch.float64), tensor(2.8669e-17, dtype=torch.float64), tensor(5.4466e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2203, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.142
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.638
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.22

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 73,728 || all params: 8,030,334,976 || trainable%: 0.0009
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:26,  3.91s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:19,  1.14it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:48,  1.70it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:16<00:44,  1.69it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:18<00:31,  2.11it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:20<00:23,  2.49it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:19,  2.61it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:27<00:16,  2.54it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:29<00:12,  2.76it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:31<00:08,  3.09it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:06,  2.80it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:38<00:04,  2.59it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:42<00:01,  2.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:42<00:00,  2.38it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.9018, 'grad_norm': 15.601472854614258, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.7296149730682373, 'eval_runtime': 7.2392, 'eval_samples_per_second': 137.999, 'eval_steps_per_second': 8.703, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:41,  5.26s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:29,  1.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:50,  1.63it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:15<00:34,  2.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:18<00:30,  2.22it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:20<00:23,  2.50it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:19,  2.67it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:16,  2.58it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:11,  2.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:08,  3.19it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:35<00:07,  2.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:37<00:04,  2.71it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.05it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.52it/s]
Evaluation performance at step 50: 0.71
{'loss': 1.9555, 'grad_norm': 5.866491794586182, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.71}
{'eval_loss': 1.4933950901031494, 'eval_runtime': 7.2556, 'eval_samples_per_second': 137.686, 'eval_steps_per_second': 8.683, 'epoch': 0.08}
{'loss': 1.3096, 'grad_norm': 3.4758358001708984, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2440800666809082, 'eval_runtime': 7.2902, 'eval_samples_per_second': 137.033, 'eval_steps_per_second': 8.642, 'epoch': 0.12}
{'loss': 1.2001, 'grad_norm': 2.541236162185669, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1910374164581299, 'eval_runtime': 7.2991, 'eval_samples_per_second': 136.866, 'eval_steps_per_second': 8.631, 'epoch': 0.16}
{'loss': 1.1609, 'grad_norm': 2.815577983856201, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1643786430358887, 'eval_runtime': 7.3066, 'eval_samples_per_second': 136.725, 'eval_steps_per_second': 8.622, 'epoch': 0.2}
{'loss': 1.1532, 'grad_norm': 3.1855804920196533, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1531254053115845, 'eval_runtime': 7.3287, 'eval_samples_per_second': 136.314, 'eval_steps_per_second': 8.596, 'epoch': 0.24}
{'loss': 1.126, 'grad_norm': 2.8226354122161865, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1370857954025269, 'eval_runtime': 7.323, 'eval_samples_per_second': 136.419, 'eval_steps_per_second': 8.603, 'epoch': 0.28}
{'loss': 1.1206, 'grad_norm': 2.6837689876556396, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1281038522720337, 'eval_runtime': 7.3366, 'eval_samples_per_second': 136.167, 'eval_steps_per_second': 8.587, 'epoch': 0.32}
{'loss': 1.1219, 'grad_norm': 3.4450581073760986, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1191350221633911, 'eval_runtime': 7.3534, 'eval_samples_per_second': 135.855, 'eval_steps_per_second': 8.567, 'epoch': 0.36}
{'loss': 1.1073, 'grad_norm': 2.363358736038208, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1136345863342285, 'eval_runtime': 7.3305, 'eval_samples_per_second': 136.28, 'eval_steps_per_second': 8.594, 'epoch': 0.4}
{'loss': 1.0825, 'grad_norm': 2.6745553016662598, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1057860851287842, 'eval_runtime': 7.3277, 'eval_samples_per_second': 136.332, 'eval_steps_per_second': 8.597, 'epoch': 0.44}
{'loss': 1.0933, 'grad_norm': 2.6810286045074463, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1012890338897705, 'eval_runtime': 7.3308, 'eval_samples_per_second': 136.274, 'eval_steps_per_second': 8.594, 'epoch': 0.48}
{'loss': 1.0823, 'grad_norm': 2.986300230026245, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.097174048423767, 'eval_runtime': 7.3412, 'eval_samples_per_second': 136.081, 'eval_steps_per_second': 8.582, 'epoch': 0.52}
{'loss': 1.1205, 'grad_norm': 2.7430453300476074, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0945711135864258, 'eval_runtime': 7.3248, 'eval_samples_per_second': 136.386, 'eval_steps_per_second': 8.601, 'epoch': 0.56}
{'loss': 1.1023, 'grad_norm': 3.111640214920044, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0932639837265015, 'eval_runtime': 7.3197, 'eval_samples_per_second': 136.481, 'eval_steps_per_second': 8.607, 'epoch': 0.6}
{'loss': 1.079, 'grad_norm': 2.241813898086548, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0896154642105103, 'eval_runtime': 7.3678, 'eval_samples_per_second': 135.59, 'eval_steps_per_second': 8.551, 'epoch': 0.64}
{'loss': 1.1001, 'grad_norm': 2.6283023357391357, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0880552530288696, 'eval_runtime': 7.371, 'eval_samples_per_second': 135.531, 'eval_steps_per_second': 8.547, 'epoch': 0.68}
{'loss': 1.1074, 'grad_norm': 2.626837730407715, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0842278003692627, 'eval_runtime': 7.3747, 'eval_samples_per_second': 135.463, 'eval_steps_per_second': 8.543, 'epoch': 0.72}
{'loss': 1.0615, 'grad_norm': 2.348668336868286, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0832873582839966, 'eval_runtime': 7.3748, 'eval_samples_per_second': 135.461, 'eval_steps_per_second': 8.543, 'epoch': 0.76}
{'loss': 1.098, 'grad_norm': 2.479776620864868, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0802510976791382, 'eval_runtime': 7.3814, 'eval_samples_per_second': 135.34, 'eval_steps_per_second': 8.535, 'epoch': 0.8}
{'loss': 1.0741, 'grad_norm': 2.6491713523864746, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0793068408966064, 'eval_runtime': 7.4031, 'eval_samples_per_second': 134.944, 'eval_steps_per_second': 8.51, 'epoch': 0.84}
{'loss': 1.0688, 'grad_norm': 2.391913652420044, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.077636957168579, 'eval_runtime': 7.3567, 'eval_samples_per_second': 135.794, 'eval_steps_per_second': 8.564, 'epoch': 0.88}
{'loss': 1.0752, 'grad_norm': 2.4319465160369873, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0770063400268555, 'eval_runtime': 7.3566, 'eval_samples_per_second': 135.797, 'eval_steps_per_second': 8.564, 'epoch': 0.92}
{'loss': 1.0672, 'grad_norm': 2.6533665657043457, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0761104822158813, 'eval_runtime': 7.3997, 'eval_samples_per_second': 135.006, 'eval_steps_per_second': 8.514, 'epoch': 0.96}
{'loss': 1.1089, 'grad_norm': 2.6106362342834473, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.076000690460205, 'eval_runtime': 7.3593, 'eval_samples_per_second': 135.746, 'eval_steps_per_second': 8.561, 'epoch': 1.0}
{'train_runtime': 374.4296, 'train_samples_per_second': 26.705, 'train_steps_per_second': 1.669, 'train_loss': 1.259123028564453, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7296149730682373, 1.4933950901031494, 1.2440800666809082, 1.1910374164581299, 1.1643786430358887, 1.1531254053115845, 1.1370857954025269, 1.1281038522720337, 1.1191350221633911, 1.1136345863342285, 1.1057860851287842, 1.1012890338897705, 1.097174048423767, 1.0945711135864258, 1.0932639837265015, 1.0896154642105103, 1.0880552530288696, 1.0842278003692627, 1.0832873582839966, 1.0802510976791382, 1.0793068408966064, 1.077636957168579, 1.0770063400268555, 1.0761104822158813, 1.076000690460205], 'performance': [0.77, 0.71]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:55,  9.65s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:16,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.80it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.32it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.86it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.71]
current iteration observed (possibly low-fid or predicted) performance:  1.213118076324463
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7359 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.7709097466302279
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.9319375978971784
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.8969525236095847
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.8362437251681762
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.9285097907843289
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.9565, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.0435, dtype=torch.float64), 0, 1, 1, 0, 0, 0, 0, 128, 0.0, 1.4800000190734866, 1]
normalized proposed parameters for next round by BO: [tensor(0.9565, dtype=torch.float64), tensor(1.4209e-17, dtype=torch.float64), tensor(3.1075e-17, dtype=torch.float64), tensor(9.1841e-18, dtype=torch.float64), tensor(9.7082e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.6997e-18, dtype=torch.float64), tensor(0.0435, dtype=torch.float64), tensor(1.0697e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.956
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.044
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,048,576 || all params: 8,031,309,824 || trainable%: 0.0131
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:02,  2.45s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.05it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:28,  2.59it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:26,  2.52it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.21it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.68it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.98it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.84it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.7663, 'grad_norm': 0.009915392845869064, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 4.820112228393555, 'eval_runtime': 4.7864, 'eval_samples_per_second': 208.715, 'eval_steps_per_second': 13.162, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:53,  2.96s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:41,  1.99it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.54it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.82it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:14,  2.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.25it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.29it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Evaluation performance at step 50: 0.79
{'loss': 4.7516, 'grad_norm': 0.047120366245508194, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.79}
{'eval_loss': 4.728092670440674, 'eval_runtime': 4.7782, 'eval_samples_per_second': 209.075, 'eval_steps_per_second': 13.185, 'epoch': 0.08}
{'loss': 4.5895, 'grad_norm': 0.025668790563941002, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 4.557762622833252, 'eval_runtime': 4.8043, 'eval_samples_per_second': 207.94, 'eval_steps_per_second': 13.113, 'epoch': 0.12}
{'loss': 4.4689, 'grad_norm': 0.033600836992263794, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 4.4431843757629395, 'eval_runtime': 4.8235, 'eval_samples_per_second': 207.11, 'eval_steps_per_second': 13.061, 'epoch': 0.16}
{'loss': 4.3476, 'grad_norm': 0.08559198677539825, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 4.194772720336914, 'eval_runtime': 4.8137, 'eval_samples_per_second': 207.532, 'eval_steps_per_second': 13.088, 'epoch': 0.2}
{'loss': 4.1391, 'grad_norm': 0.08230330795049667, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 4.101471424102783, 'eval_runtime': 4.8097, 'eval_samples_per_second': 207.707, 'eval_steps_per_second': 13.099, 'epoch': 0.24}
{'loss': 3.9868, 'grad_norm': 0.042841456830501556, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.947974443435669, 'eval_runtime': 4.8217, 'eval_samples_per_second': 207.187, 'eval_steps_per_second': 13.066, 'epoch': 0.28}
{'loss': 3.9155, 'grad_norm': 0.03745520859956741, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 3.8752644062042236, 'eval_runtime': 4.821, 'eval_samples_per_second': 207.218, 'eval_steps_per_second': 13.068, 'epoch': 0.32}
{'loss': 3.7969, 'grad_norm': 0.029649650678038597, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 3.811918258666992, 'eval_runtime': 4.8292, 'eval_samples_per_second': 206.865, 'eval_steps_per_second': 13.046, 'epoch': 0.36}
{'loss': 3.7667, 'grad_norm': 0.02725781500339508, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 3.763996124267578, 'eval_runtime': 4.8421, 'eval_samples_per_second': 206.314, 'eval_steps_per_second': 13.011, 'epoch': 0.4}
{'loss': 3.743, 'grad_norm': 0.043400611728429794, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 3.729811429977417, 'eval_runtime': 4.8441, 'eval_samples_per_second': 206.229, 'eval_steps_per_second': 13.005, 'epoch': 0.44}
{'loss': 3.6933, 'grad_norm': 0.03665640577673912, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 3.707460403442383, 'eval_runtime': 4.8364, 'eval_samples_per_second': 206.558, 'eval_steps_per_second': 13.026, 'epoch': 0.48}
{'loss': 3.679, 'grad_norm': 0.020903103053569794, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 3.694411277770996, 'eval_runtime': 4.8542, 'eval_samples_per_second': 205.799, 'eval_steps_per_second': 12.978, 'epoch': 0.52}
{'loss': 3.6843, 'grad_norm': 0.043300431221723557, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 3.6827890872955322, 'eval_runtime': 4.8838, 'eval_samples_per_second': 204.552, 'eval_steps_per_second': 12.9, 'epoch': 0.56}
{'loss': 3.6474, 'grad_norm': 0.024941064417362213, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 3.670147657394409, 'eval_runtime': 4.8776, 'eval_samples_per_second': 204.815, 'eval_steps_per_second': 12.916, 'epoch': 0.6}
{'loss': 3.6233, 'grad_norm': 0.05966894328594208, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 3.649972438812256, 'eval_runtime': 4.9033, 'eval_samples_per_second': 203.742, 'eval_steps_per_second': 12.849, 'epoch': 0.64}
{'loss': 3.6333, 'grad_norm': 0.02933279052376747, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 3.636451005935669, 'eval_runtime': 4.9081, 'eval_samples_per_second': 203.543, 'eval_steps_per_second': 12.836, 'epoch': 0.68}
{'loss': 3.6157, 'grad_norm': 0.027234157547354698, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 3.625608205795288, 'eval_runtime': 4.8853, 'eval_samples_per_second': 204.491, 'eval_steps_per_second': 12.896, 'epoch': 0.72}
{'loss': 3.6312, 'grad_norm': 0.07692442089319229, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 3.6196610927581787, 'eval_runtime': 4.8758, 'eval_samples_per_second': 204.89, 'eval_steps_per_second': 12.921, 'epoch': 0.76}
{'loss': 3.6073, 'grad_norm': 0.01704123616218567, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 3.6114230155944824, 'eval_runtime': 4.8797, 'eval_samples_per_second': 204.726, 'eval_steps_per_second': 12.911, 'epoch': 0.8}
{'loss': 3.5948, 'grad_norm': 0.03618369251489639, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 3.6066272258758545, 'eval_runtime': 4.8758, 'eval_samples_per_second': 204.89, 'eval_steps_per_second': 12.921, 'epoch': 0.84}
{'loss': 3.5871, 'grad_norm': 0.06223003566265106, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 3.6026289463043213, 'eval_runtime': 4.8456, 'eval_samples_per_second': 206.165, 'eval_steps_per_second': 13.001, 'epoch': 0.88}
{'loss': 3.5877, 'grad_norm': 0.017271919175982475, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 3.5998685359954834, 'eval_runtime': 4.8628, 'eval_samples_per_second': 205.439, 'eval_steps_per_second': 12.956, 'epoch': 0.92}
{'loss': 3.5756, 'grad_norm': 0.03021290898323059, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 3.598191738128662, 'eval_runtime': 4.8654, 'eval_samples_per_second': 205.328, 'eval_steps_per_second': 12.949, 'epoch': 0.96}
{'loss': 3.5706, 'grad_norm': 0.038098450750112534, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 3.5976083278656006, 'eval_runtime': 4.8765, 'eval_samples_per_second': 204.861, 'eval_steps_per_second': 12.919, 'epoch': 1.0}
{'train_runtime': 330.7261, 'train_samples_per_second': 30.233, 'train_steps_per_second': 1.89, 'train_loss': 3.88009609375, 'epoch': 1.0}
train_results:  {'eval_loss': [4.820112228393555, 4.728092670440674, 4.557762622833252, 4.4431843757629395, 4.194772720336914, 4.101471424102783, 3.947974443435669, 3.8752644062042236, 3.811918258666992, 3.763996124267578, 3.729811429977417, 3.707460403442383, 3.694411277770996, 3.6827890872955322, 3.670147657394409, 3.649972438812256, 3.636451005935669, 3.625608205795288, 3.6196610927581787, 3.6114230155944824, 3.6066272258758545, 3.6026289463043213, 3.5998685359954834, 3.598191738128662, 3.5976083278656006], 'performance': [0.76, 0.79]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:57,  9.67s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:16,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.48it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:32<00:27,  1.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:37<00:15,  2.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:44<00:08,  2.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:46<00:01,  2.94it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:46<00:00,  2.15it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.79]
current iteration observed (possibly low-fid or predicted) performance:  0.7463162541389465
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3762 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.8635050209942479
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.9070253947619802
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.8782204845794181
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.921018998446209
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.9253734613561755
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.7810, dtype=torch.float64), 0, 0, 0, tensor(0.2190, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7066e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7810, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7917e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2190, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.781
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.219

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:03,  2.46s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.67it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.87it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.05it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Evaluation performance at step 25: 0.79
{'loss': 5.0475, 'grad_norm': 3.4606540203094482, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.79}
{'eval_loss': 4.875612735748291, 'eval_runtime': 5.2565, 'eval_samples_per_second': 190.052, 'eval_steps_per_second': 11.985, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:09,  3.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:48,  1.88it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.55it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:25,  3.00it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  2.94it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.95it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.18it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:05,  3.40it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:29<00:03,  3.56it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:31<00:00,  3.79it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:31<00:00,  3.19it/s]
Evaluation performance at step 50: 0.74
{'loss': 4.5759, 'grad_norm': 6.295082092285156, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 4.132719039916992, 'eval_runtime': 5.2518, 'eval_samples_per_second': 190.22, 'eval_steps_per_second': 11.996, 'epoch': 0.08}
{'loss': 3.9275, 'grad_norm': 1.217465877532959, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.8011693954467773, 'eval_runtime': 5.267, 'eval_samples_per_second': 189.672, 'eval_steps_per_second': 11.961, 'epoch': 0.12}
{'loss': 3.7224, 'grad_norm': 14.549470901489258, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.626760721206665, 'eval_runtime': 5.2643, 'eval_samples_per_second': 189.769, 'eval_steps_per_second': 11.967, 'epoch': 0.16}
{'loss': 3.5601, 'grad_norm': 1.4046307802200317, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.5061893463134766, 'eval_runtime': 5.286, 'eval_samples_per_second': 188.99, 'eval_steps_per_second': 11.918, 'epoch': 0.2}
{'loss': 3.4025, 'grad_norm': 10.953913688659668, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.376896858215332, 'eval_runtime': 5.2894, 'eval_samples_per_second': 188.87, 'eval_steps_per_second': 11.911, 'epoch': 0.24}
{'loss': 3.3119, 'grad_norm': 6.684808254241943, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.315490245819092, 'eval_runtime': 5.3001, 'eval_samples_per_second': 188.486, 'eval_steps_per_second': 11.886, 'epoch': 0.28}
{'loss': 3.3069, 'grad_norm': 5.964682579040527, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 3.272176504135132, 'eval_runtime': 5.3082, 'eval_samples_per_second': 188.2, 'eval_steps_per_second': 11.868, 'epoch': 0.32}
{'loss': 3.2173, 'grad_norm': 8.205181121826172, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 3.2198243141174316, 'eval_runtime': 5.3178, 'eval_samples_per_second': 187.861, 'eval_steps_per_second': 11.847, 'epoch': 0.36}
{'loss': 3.1972, 'grad_norm': 6.12467622756958, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 3.183903217315674, 'eval_runtime': 5.317, 'eval_samples_per_second': 187.889, 'eval_steps_per_second': 11.849, 'epoch': 0.4}
{'loss': 3.157, 'grad_norm': 6.728196620941162, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 3.1368961334228516, 'eval_runtime': 5.3228, 'eval_samples_per_second': 187.685, 'eval_steps_per_second': 11.836, 'epoch': 0.44}
{'loss': 3.1028, 'grad_norm': 11.004313468933105, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 3.097304344177246, 'eval_runtime': 5.3343, 'eval_samples_per_second': 187.28, 'eval_steps_per_second': 11.81, 'epoch': 0.48}
{'loss': 3.0912, 'grad_norm': 4.715384483337402, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 3.0563502311706543, 'eval_runtime': 5.3243, 'eval_samples_per_second': 187.629, 'eval_steps_per_second': 11.832, 'epoch': 0.52}
{'loss': 3.0725, 'grad_norm': 3.67156720161438, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 3.0311920642852783, 'eval_runtime': 5.3251, 'eval_samples_per_second': 187.603, 'eval_steps_per_second': 11.831, 'epoch': 0.56}
{'loss': 2.9986, 'grad_norm': 2.5637996196746826, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 3.004298210144043, 'eval_runtime': 5.3209, 'eval_samples_per_second': 187.75, 'eval_steps_per_second': 11.84, 'epoch': 0.6}
{'loss': 2.9984, 'grad_norm': 7.42757511138916, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.9698188304901123, 'eval_runtime': 5.3279, 'eval_samples_per_second': 187.504, 'eval_steps_per_second': 11.825, 'epoch': 0.64}
{'loss': 2.9185, 'grad_norm': 14.025374412536621, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.9309825897216797, 'eval_runtime': 5.3323, 'eval_samples_per_second': 187.349, 'eval_steps_per_second': 11.815, 'epoch': 0.68}
{'loss': 2.9534, 'grad_norm': 6.865281105041504, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.9049489498138428, 'eval_runtime': 5.3282, 'eval_samples_per_second': 187.491, 'eval_steps_per_second': 11.824, 'epoch': 0.72}
{'loss': 2.9057, 'grad_norm': 5.093320369720459, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.881777048110962, 'eval_runtime': 5.3203, 'eval_samples_per_second': 187.773, 'eval_steps_per_second': 11.842, 'epoch': 0.76}
{'loss': 2.9085, 'grad_norm': 15.91531753540039, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.860435724258423, 'eval_runtime': 5.3236, 'eval_samples_per_second': 187.656, 'eval_steps_per_second': 11.834, 'epoch': 0.8}
{'loss': 2.8551, 'grad_norm': 4.373497486114502, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.843522310256958, 'eval_runtime': 5.3369, 'eval_samples_per_second': 187.188, 'eval_steps_per_second': 11.805, 'epoch': 0.84}
{'loss': 2.8302, 'grad_norm': 2.3688364028930664, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.8338520526885986, 'eval_runtime': 5.3339, 'eval_samples_per_second': 187.291, 'eval_steps_per_second': 11.811, 'epoch': 0.88}
{'loss': 2.8473, 'grad_norm': 4.626784324645996, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.825504779815674, 'eval_runtime': 5.3374, 'eval_samples_per_second': 187.169, 'eval_steps_per_second': 11.803, 'epoch': 0.92}
{'loss': 2.8291, 'grad_norm': 5.758230209350586, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.820446014404297, 'eval_runtime': 5.3449, 'eval_samples_per_second': 186.908, 'eval_steps_per_second': 11.787, 'epoch': 0.96}
{'loss': 2.8211, 'grad_norm': 2.0488204956054688, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.816556692123413, 'eval_runtime': 5.3391, 'eval_samples_per_second': 187.111, 'eval_steps_per_second': 11.8, 'epoch': 1.0}
{'train_runtime': 349.7543, 'train_samples_per_second': 28.589, 'train_steps_per_second': 1.787, 'train_loss': 3.2623442016601563, 'epoch': 1.0}
train_results:  {'eval_loss': [4.875612735748291, 4.132719039916992, 3.8011693954467773, 3.626760721206665, 3.5061893463134766, 3.376896858215332, 3.315490245819092, 3.272176504135132, 3.2198243141174316, 3.183903217315674, 3.1368961334228516, 3.097304344177246, 3.0563502311706543, 3.0311920642852783, 3.004298210144043, 2.9698188304901123, 2.9309825897216797, 2.9049489498138428, 2.881777048110962, 2.860435724258423, 2.843522310256958, 2.8338520526885986, 2.825504779815674, 2.820446014404297, 2.816556692123413], 'performance': [0.79, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:57,  9.67s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:05,  1.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:38,  1.73it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:25,  2.04it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:14,  2.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:41<00:08,  2.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:43<00:00,  3.02it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:43<00:00,  2.30it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.79, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  1.2299857139587402
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0938 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.6845102127562249
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.9061339459526203
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.892848376949932
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.9112534626651199
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.9097236645095764
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2010, dtype=torch.float64), tensor(0.2474, dtype=torch.float64), 0, tensor(0.0422, dtype=torch.float64), 0, 0, tensor(0.2840, dtype=torch.float64), tensor(0.2254, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2010, dtype=torch.float64), tensor(0.2474, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0422, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2840, dtype=torch.float64), tensor(0.2254, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.201
  rowan_hellaswag: 0.247
  sciq: 0
  triviaqa: 0.042
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.284
  arc_challenge: 0.225

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:34,  2.78s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:11,  1.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.00it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.55it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.72it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.74it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.88it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.80it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.05it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.19it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:04,  2.65it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.02it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.74it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.786, 'grad_norm': 1.384100079536438, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.5897457599639893, 'eval_runtime': 8.774, 'eval_samples_per_second': 113.859, 'eval_steps_per_second': 7.18, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:08,  3.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:46,  1.95it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.60it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.04it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.17it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:19,  2.97it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:18,  2.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:11,  3.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.26it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.63it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.93it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.96it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.4677, 'grad_norm': 2.581359386444092, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 3.3135056495666504, 'eval_runtime': 8.7594, 'eval_samples_per_second': 114.049, 'eval_steps_per_second': 7.192, 'epoch': 0.08}
{'loss': 3.2288, 'grad_norm': 3.654254674911499, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.0830821990966797, 'eval_runtime': 8.7646, 'eval_samples_per_second': 113.981, 'eval_steps_per_second': 7.188, 'epoch': 0.12}
{'loss': 3.0464, 'grad_norm': 0.7508823275566101, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.990521192550659, 'eval_runtime': 8.7618, 'eval_samples_per_second': 114.017, 'eval_steps_per_second': 7.19, 'epoch': 0.16}
{'loss': 2.9297, 'grad_norm': 1.0985075235366821, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.924708604812622, 'eval_runtime': 8.7924, 'eval_samples_per_second': 113.621, 'eval_steps_per_second': 7.165, 'epoch': 0.2}
{'loss': 2.9389, 'grad_norm': 2.0306429862976074, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.8678531646728516, 'eval_runtime': 8.7962, 'eval_samples_per_second': 113.571, 'eval_steps_per_second': 7.162, 'epoch': 0.24}
{'loss': 2.8847, 'grad_norm': 2.3183887004852295, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.8194196224212646, 'eval_runtime': 8.7901, 'eval_samples_per_second': 113.65, 'eval_steps_per_second': 7.167, 'epoch': 0.28}
{'loss': 2.7901, 'grad_norm': 3.5027432441711426, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.772160053253174, 'eval_runtime': 8.8019, 'eval_samples_per_second': 113.498, 'eval_steps_per_second': 7.158, 'epoch': 0.32}
{'loss': 2.7302, 'grad_norm': 1.7778587341308594, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.7230169773101807, 'eval_runtime': 8.8182, 'eval_samples_per_second': 113.288, 'eval_steps_per_second': 7.144, 'epoch': 0.36}
{'loss': 2.7368, 'grad_norm': 2.9541001319885254, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.6825757026672363, 'eval_runtime': 8.9008, 'eval_samples_per_second': 112.238, 'eval_steps_per_second': 7.078, 'epoch': 0.4}
{'loss': 2.6886, 'grad_norm': 1.1330307722091675, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.6611413955688477, 'eval_runtime': 8.8999, 'eval_samples_per_second': 112.248, 'eval_steps_per_second': 7.079, 'epoch': 0.44}
{'loss': 2.5978, 'grad_norm': 2.881103038787842, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.6409153938293457, 'eval_runtime': 8.9119, 'eval_samples_per_second': 112.097, 'eval_steps_per_second': 7.069, 'epoch': 0.48}
{'loss': 2.6032, 'grad_norm': 2.1864147186279297, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.6187024116516113, 'eval_runtime': 8.9062, 'eval_samples_per_second': 112.168, 'eval_steps_per_second': 7.074, 'epoch': 0.52}
{'loss': 2.6553, 'grad_norm': 2.468512773513794, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.6013083457946777, 'eval_runtime': 8.8876, 'eval_samples_per_second': 112.404, 'eval_steps_per_second': 7.089, 'epoch': 0.56}
{'loss': 2.6124, 'grad_norm': 2.0350067615509033, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.58609938621521, 'eval_runtime': 8.9253, 'eval_samples_per_second': 111.929, 'eval_steps_per_second': 7.059, 'epoch': 0.6}
{'loss': 2.5472, 'grad_norm': 1.3115752935409546, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.5760717391967773, 'eval_runtime': 8.9745, 'eval_samples_per_second': 111.315, 'eval_steps_per_second': 7.02, 'epoch': 0.64}
{'loss': 2.635, 'grad_norm': 1.3582611083984375, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.560925245285034, 'eval_runtime': 8.954, 'eval_samples_per_second': 111.57, 'eval_steps_per_second': 7.036, 'epoch': 0.68}
{'loss': 2.6047, 'grad_norm': 2.555332660675049, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.550813913345337, 'eval_runtime': 8.9424, 'eval_samples_per_second': 111.715, 'eval_steps_per_second': 7.045, 'epoch': 0.72}
{'loss': 2.5956, 'grad_norm': 1.3897541761398315, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.5418245792388916, 'eval_runtime': 8.938, 'eval_samples_per_second': 111.77, 'eval_steps_per_second': 7.049, 'epoch': 0.76}
{'loss': 2.5555, 'grad_norm': 1.1171997785568237, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.5356674194335938, 'eval_runtime': 8.9274, 'eval_samples_per_second': 111.903, 'eval_steps_per_second': 7.057, 'epoch': 0.8}
{'loss': 2.5472, 'grad_norm': 1.5121766328811646, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.5284194946289062, 'eval_runtime': 8.9482, 'eval_samples_per_second': 111.642, 'eval_steps_per_second': 7.041, 'epoch': 0.84}
{'loss': 2.4829, 'grad_norm': 2.62587571144104, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.5243732929229736, 'eval_runtime': 8.9078, 'eval_samples_per_second': 112.149, 'eval_steps_per_second': 7.072, 'epoch': 0.88}
{'loss': 2.549, 'grad_norm': 1.7970045804977417, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.520745038986206, 'eval_runtime': 8.8539, 'eval_samples_per_second': 112.832, 'eval_steps_per_second': 7.116, 'epoch': 0.92}
{'loss': 2.5737, 'grad_norm': 1.1349256038665771, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.5184106826782227, 'eval_runtime': 8.8844, 'eval_samples_per_second': 112.445, 'eval_steps_per_second': 7.091, 'epoch': 0.96}
{'loss': 2.5894, 'grad_norm': 1.4469157457351685, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.5169854164123535, 'eval_runtime': 8.8838, 'eval_samples_per_second': 112.452, 'eval_steps_per_second': 7.092, 'epoch': 1.0}
{'train_runtime': 511.4107, 'train_samples_per_second': 19.548, 'train_steps_per_second': 1.222, 'train_loss': 2.775061346435547, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5897457599639893, 3.3135056495666504, 3.0830821990966797, 2.990521192550659, 2.924708604812622, 2.8678531646728516, 2.8194196224212646, 2.772160053253174, 2.7230169773101807, 2.6825757026672363, 2.6611413955688477, 2.6409153938293457, 2.6187024116516113, 2.6013083457946777, 2.58609938621521, 2.5760717391967773, 2.560925245285034, 2.550813913345337, 2.5418245792388916, 2.5356674194335938, 2.5284194946289062, 2.5243732929229736, 2.520745038986206, 2.5184106826782227, 2.5169854164123535], 'performance': [0.77, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:50,  9.60s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:09,  1.19it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:42,  1.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:26,  1.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:35<00:15,  2.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:43<00:08,  2.25it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:44<00:01,  3.00it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:44<00:00,  2.23it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2296924591064453
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9302 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 0.9118063431869956
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.9115657468048378
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 0.8773312664597608
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.911811292710839
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.8925464947070123
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.6882, dtype=torch.float64), 0, 0, 0, 0, tensor(0.0922, dtype=torch.float64), tensor(0.2195, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.0, 47.99999999999999, 1]
normalized proposed parameters for next round by BO: [tensor(4.6772e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6882, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.6567e-17, dtype=torch.float64), tensor(0.0922, dtype=torch.float64), tensor(0.2195, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.688
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.092
  arc_challenge: 0.22

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:36,  2.79s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:11,  1.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  1.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.65it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.87it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.05it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.93it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.27it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.81it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.0794, 'grad_norm': 1.2127007246017456, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 4.108334541320801, 'eval_runtime': 8.8911, 'eval_samples_per_second': 112.359, 'eval_steps_per_second': 7.086, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:10,  3.14s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:47,  1.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.57it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.02it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:20,  3.20it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:19,  2.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.91it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.30it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:07,  3.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:05,  3.33it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:29<00:03,  3.50it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:30<00:00,  3.74it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:30<00:00,  3.23it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.9549, 'grad_norm': 2.625209331512451, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 3.8937597274780273, 'eval_runtime': 8.9243, 'eval_samples_per_second': 111.942, 'eval_steps_per_second': 7.059, 'epoch': 0.08}
{'loss': 3.7263, 'grad_norm': 4.845090389251709, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.619441032409668, 'eval_runtime': 8.9456, 'eval_samples_per_second': 111.674, 'eval_steps_per_second': 7.043, 'epoch': 0.12}
{'loss': 3.4939, 'grad_norm': 0.8899580240249634, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.484231472015381, 'eval_runtime': 8.9675, 'eval_samples_per_second': 111.403, 'eval_steps_per_second': 7.025, 'epoch': 0.16}
{'loss': 3.4259, 'grad_norm': 2.4751739501953125, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.407633066177368, 'eval_runtime': 8.9711, 'eval_samples_per_second': 111.358, 'eval_steps_per_second': 7.023, 'epoch': 0.2}
{'loss': 3.3395, 'grad_norm': 3.7573184967041016, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.3397233486175537, 'eval_runtime': 8.9693, 'eval_samples_per_second': 111.38, 'eval_steps_per_second': 7.024, 'epoch': 0.24}
{'loss': 3.2681, 'grad_norm': 1.478238582611084, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.2860403060913086, 'eval_runtime': 8.9955, 'eval_samples_per_second': 111.055, 'eval_steps_per_second': 7.003, 'epoch': 0.28}
{'loss': 3.2433, 'grad_norm': 2.0685818195343018, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 3.238199472427368, 'eval_runtime': 9.0153, 'eval_samples_per_second': 110.811, 'eval_steps_per_second': 6.988, 'epoch': 0.32}
{'loss': 3.1593, 'grad_norm': 1.4047821760177612, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 3.180978775024414, 'eval_runtime': 9.0208, 'eval_samples_per_second': 110.744, 'eval_steps_per_second': 6.984, 'epoch': 0.36}
{'loss': 3.1313, 'grad_norm': 1.8039319515228271, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 3.148042678833008, 'eval_runtime': 9.0048, 'eval_samples_per_second': 110.94, 'eval_steps_per_second': 6.996, 'epoch': 0.4}
{'loss': 3.1148, 'grad_norm': 3.5892186164855957, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 3.120823383331299, 'eval_runtime': 8.9989, 'eval_samples_per_second': 111.013, 'eval_steps_per_second': 7.001, 'epoch': 0.44}
{'loss': 3.0657, 'grad_norm': 3.998835563659668, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 3.1064565181732178, 'eval_runtime': 8.9856, 'eval_samples_per_second': 111.178, 'eval_steps_per_second': 7.011, 'epoch': 0.48}
{'loss': 3.0627, 'grad_norm': 1.7516560554504395, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 3.083733558654785, 'eval_runtime': 8.9583, 'eval_samples_per_second': 111.516, 'eval_steps_per_second': 7.033, 'epoch': 0.52}
{'loss': 3.0551, 'grad_norm': 1.4156266450881958, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 3.0683772563934326, 'eval_runtime': 8.936, 'eval_samples_per_second': 111.795, 'eval_steps_per_second': 7.05, 'epoch': 0.56}
{'loss': 3.0315, 'grad_norm': 2.5156185626983643, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 3.0566697120666504, 'eval_runtime': 8.9313, 'eval_samples_per_second': 111.853, 'eval_steps_per_second': 7.054, 'epoch': 0.6}
{'loss': 3.0185, 'grad_norm': 2.243736982345581, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 3.0449814796447754, 'eval_runtime': 8.9468, 'eval_samples_per_second': 111.661, 'eval_steps_per_second': 7.042, 'epoch': 0.64}
{'loss': 3.0153, 'grad_norm': 1.9996178150177002, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 3.036619186401367, 'eval_runtime': 8.9559, 'eval_samples_per_second': 111.547, 'eval_steps_per_second': 7.034, 'epoch': 0.68}
{'loss': 3.0331, 'grad_norm': 1.9386893510818481, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 3.0225348472595215, 'eval_runtime': 8.9442, 'eval_samples_per_second': 111.693, 'eval_steps_per_second': 7.044, 'epoch': 0.72}
{'loss': 2.991, 'grad_norm': 1.5443153381347656, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 3.0116899013519287, 'eval_runtime': 8.9592, 'eval_samples_per_second': 111.506, 'eval_steps_per_second': 7.032, 'epoch': 0.76}
{'loss': 3.0192, 'grad_norm': 2.913896322250366, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 3.003521203994751, 'eval_runtime': 8.9402, 'eval_samples_per_second': 111.742, 'eval_steps_per_second': 7.047, 'epoch': 0.8}
{'loss': 2.9768, 'grad_norm': 2.366140604019165, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.9954230785369873, 'eval_runtime': 8.944, 'eval_samples_per_second': 111.696, 'eval_steps_per_second': 7.044, 'epoch': 0.84}
{'loss': 2.9569, 'grad_norm': 0.8799562454223633, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.9872913360595703, 'eval_runtime': 8.9523, 'eval_samples_per_second': 111.591, 'eval_steps_per_second': 7.037, 'epoch': 0.88}
{'loss': 2.9734, 'grad_norm': 1.7577285766601562, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.9820668697357178, 'eval_runtime': 8.9438, 'eval_samples_per_second': 111.698, 'eval_steps_per_second': 7.044, 'epoch': 0.92}
{'loss': 2.9502, 'grad_norm': 1.53726065158844, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.977951765060425, 'eval_runtime': 8.9698, 'eval_samples_per_second': 111.374, 'eval_steps_per_second': 7.024, 'epoch': 0.96}
{'loss': 2.9569, 'grad_norm': 0.8476958870887756, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.975494623184204, 'eval_runtime': 9.0192, 'eval_samples_per_second': 110.764, 'eval_steps_per_second': 6.985, 'epoch': 1.0}
{'train_runtime': 511.8528, 'train_samples_per_second': 19.535, 'train_steps_per_second': 1.221, 'train_loss': 3.201715869140625, 'epoch': 1.0}
train_results:  {'eval_loss': [4.108334541320801, 3.8937597274780273, 3.619441032409668, 3.484231472015381, 3.407633066177368, 3.3397233486175537, 3.2860403060913086, 3.238199472427368, 3.180978775024414, 3.148042678833008, 3.120823383331299, 3.1064565181732178, 3.083733558654785, 3.0683772563934326, 3.0566697120666504, 3.0449814796447754, 3.036619186401367, 3.0225348472595215, 3.0116899013519287, 3.003521203994751, 2.9954230785369873, 2.9872913360595703, 2.9820668697357178, 2.977951765060425, 2.975494623184204], 'performance': [0.76, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:57,  9.68s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:04,  1.28it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:37,  1.77it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:24,  2.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:14,  2.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:06,  2.77it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:00,  3.44it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.48it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2285890579223633
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453, 1.2285890579223633]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7248 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 0.9120402026897371
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.7539828061132148
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 0.9120587652713582
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.8393683308528903
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.9118331368895873
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0709, dtype=torch.float64), 0, 0, tensor(0.0109, dtype=torch.float64), 0, 0, tensor(0.6989, dtype=torch.float64), tensor(0.2193, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 2.790497846654826e-20, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(8.5885e-19, dtype=torch.float64), tensor(0.0709, dtype=torch.float64), tensor(8.5506e-19, dtype=torch.float64), tensor(1.2956e-16, dtype=torch.float64), tensor(0.0109, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.2623e-17, dtype=torch.float64), tensor(0.6989, dtype=torch.float64), tensor(0.2193, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(2.7905e-19, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.071
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.011
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.699
  arc_challenge: 0.219

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (2.790497846654826e-20,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  2.790497846654826e-20
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:35,  2.78s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:11,  1.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  1.99it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.82it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.80it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.15it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.25it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.97it/s]
Evaluation performance at step 25: 0.75
{'loss': 3.6721, 'grad_norm': 1.7388514280319214, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.5354702472686768, 'eval_runtime': 8.5312, 'eval_samples_per_second': 117.099, 'eval_steps_per_second': 7.385, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:08,  3.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:52,  1.72it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:34,  2.41it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:25,  2.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:26,  2.56it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:21,  2.79it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.92it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.82it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.67it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.93it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.4696, 'grad_norm': 4.089498043060303, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 3.2484235763549805, 'eval_runtime': 8.5156, 'eval_samples_per_second': 117.315, 'eval_steps_per_second': 7.398, 'epoch': 0.08}
{'loss': 3.0862, 'grad_norm': 2.955974578857422, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.9259722232818604, 'eval_runtime': 8.5372, 'eval_samples_per_second': 117.017, 'eval_steps_per_second': 7.379, 'epoch': 0.12}
{'loss': 2.889, 'grad_norm': 0.7824773788452148, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.785703659057617, 'eval_runtime': 8.5511, 'eval_samples_per_second': 116.827, 'eval_steps_per_second': 7.367, 'epoch': 0.16}
{'loss': 2.7983, 'grad_norm': 1.960566759109497, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.690363883972168, 'eval_runtime': 8.5877, 'eval_samples_per_second': 116.329, 'eval_steps_per_second': 7.336, 'epoch': 0.2}
{'loss': 2.6981, 'grad_norm': 2.5813491344451904, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.6026763916015625, 'eval_runtime': 8.5964, 'eval_samples_per_second': 116.211, 'eval_steps_per_second': 7.329, 'epoch': 0.24}
{'loss': 2.5856, 'grad_norm': 1.9795478582382202, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.5326180458068848, 'eval_runtime': 8.5994, 'eval_samples_per_second': 116.171, 'eval_steps_per_second': 7.326, 'epoch': 0.28}
{'loss': 2.5516, 'grad_norm': 2.7573180198669434, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.4884707927703857, 'eval_runtime': 8.6046, 'eval_samples_per_second': 116.101, 'eval_steps_per_second': 7.322, 'epoch': 0.32}
{'loss': 2.4795, 'grad_norm': 1.4236398935317993, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.4457738399505615, 'eval_runtime': 8.5839, 'eval_samples_per_second': 116.381, 'eval_steps_per_second': 7.339, 'epoch': 0.36}
{'loss': 2.4732, 'grad_norm': 1.7212085723876953, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.409144401550293, 'eval_runtime': 8.542, 'eval_samples_per_second': 116.952, 'eval_steps_per_second': 7.375, 'epoch': 0.4}
{'loss': 2.4688, 'grad_norm': 4.839909553527832, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.378664255142212, 'eval_runtime': 8.5551, 'eval_samples_per_second': 116.772, 'eval_steps_per_second': 7.364, 'epoch': 0.44}
{'loss': 2.3859, 'grad_norm': 3.723674774169922, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.3542935848236084, 'eval_runtime': 8.5626, 'eval_samples_per_second': 116.67, 'eval_steps_per_second': 7.358, 'epoch': 0.48}
{'loss': 2.3764, 'grad_norm': 4.196685314178467, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.331984281539917, 'eval_runtime': 8.5717, 'eval_samples_per_second': 116.546, 'eval_steps_per_second': 7.35, 'epoch': 0.52}
{'loss': 2.3578, 'grad_norm': 4.569955348968506, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.3138222694396973, 'eval_runtime': 8.5804, 'eval_samples_per_second': 116.428, 'eval_steps_per_second': 7.342, 'epoch': 0.56}
{'loss': 2.3123, 'grad_norm': 4.640691757202148, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.294772148132324, 'eval_runtime': 8.6562, 'eval_samples_per_second': 115.408, 'eval_steps_per_second': 7.278, 'epoch': 0.6}
{'loss': 2.3436, 'grad_norm': 2.5210282802581787, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.2801575660705566, 'eval_runtime': 8.6492, 'eval_samples_per_second': 115.502, 'eval_steps_per_second': 7.284, 'epoch': 0.64}
{'loss': 2.2761, 'grad_norm': 4.924488544464111, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.262131452560425, 'eval_runtime': 8.7037, 'eval_samples_per_second': 114.779, 'eval_steps_per_second': 7.238, 'epoch': 0.68}
{'loss': 2.2924, 'grad_norm': 3.3433053493499756, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.2477946281433105, 'eval_runtime': 8.717, 'eval_samples_per_second': 114.604, 'eval_steps_per_second': 7.227, 'epoch': 0.72}
{'loss': 2.2909, 'grad_norm': 3.546586513519287, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.2338128089904785, 'eval_runtime': 8.7286, 'eval_samples_per_second': 114.451, 'eval_steps_per_second': 7.218, 'epoch': 0.76}
{'loss': 2.2093, 'grad_norm': 2.6292736530303955, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.2201485633850098, 'eval_runtime': 8.7224, 'eval_samples_per_second': 114.533, 'eval_steps_per_second': 7.223, 'epoch': 0.8}
{'loss': 2.2514, 'grad_norm': 3.33882474899292, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.211636781692505, 'eval_runtime': 8.725, 'eval_samples_per_second': 114.498, 'eval_steps_per_second': 7.221, 'epoch': 0.84}
{'loss': 2.2459, 'grad_norm': 3.1346559524536133, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.2032575607299805, 'eval_runtime': 8.7183, 'eval_samples_per_second': 114.586, 'eval_steps_per_second': 7.226, 'epoch': 0.88}
{'loss': 2.2451, 'grad_norm': 1.9325425624847412, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.197171926498413, 'eval_runtime': 8.7261, 'eval_samples_per_second': 114.484, 'eval_steps_per_second': 7.22, 'epoch': 0.92}
{'loss': 2.2032, 'grad_norm': 1.185909390449524, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1920652389526367, 'eval_runtime': 8.7401, 'eval_samples_per_second': 114.301, 'eval_steps_per_second': 7.208, 'epoch': 0.96}
{'loss': 2.2557, 'grad_norm': 1.060075044631958, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.19020676612854, 'eval_runtime': 8.7199, 'eval_samples_per_second': 114.566, 'eval_steps_per_second': 7.225, 'epoch': 1.0}
{'train_runtime': 502.6421, 'train_samples_per_second': 19.891, 'train_steps_per_second': 1.243, 'train_loss': 2.528718225097656, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5354702472686768, 3.2484235763549805, 2.9259722232818604, 2.785703659057617, 2.690363883972168, 2.6026763916015625, 2.5326180458068848, 2.4884707927703857, 2.4457738399505615, 2.409144401550293, 2.378664255142212, 2.3542935848236084, 2.331984281539917, 2.3138222694396973, 2.294772148132324, 2.2801575660705566, 2.262131452560425, 2.2477946281433105, 2.2338128089904785, 2.2201485633850098, 2.211636781692505, 2.2032575607299805, 2.197171926498413, 2.1920652389526367, 2.19020676612854], 'performance': [0.75, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<11:32,  7.00s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:51,  1.61it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:19<00:35,  1.87it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:25<00:22,  2.28it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:30<00:13,  2.57it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:06,  3.00it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.80it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2324609756469727
current iteration best possible performance (full train run):  0.8400000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453, 1.2285890579223633, 1.2324609756469727]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4234 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.9011958020561672
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.8949827661498493
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.8095442397781858
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.8737577051734581
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.9014606671578334
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0102, dtype=torch.float64), 0, 0, tensor(0.4807, dtype=torch.float64), 0, 0, tensor(0.2907, dtype=torch.float64), tensor(0.2184, dtype=torch.float64), 1, 1, 1, 1, 0, 0, 2, 3.772434284354257e-19, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(3.3812e-17, dtype=torch.float64), tensor(0.0102, dtype=torch.float64), tensor(1.8278e-17, dtype=torch.float64), tensor(2.9410e-17, dtype=torch.float64), tensor(0.4807, dtype=torch.float64), tensor(2.1264e-17, dtype=torch.float64), tensor(2.4241e-17, dtype=torch.float64), tensor(0.2907, dtype=torch.float64), tensor(0.2184, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.7724e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.01
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.481
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.291
  arc_challenge: 0.218

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.772434284354257e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  2
lora dropout:  3.772434284354257e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 63,488 || all params: 8,030,324,736 || trainable%: 0.0008
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:03,  2.46s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:42,  1.96it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.75it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:20,  2.94it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:16,  3.01it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.97it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.31it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:07,  3.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.70it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.99it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.32it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.91it/s]
Evaluation performance at step 25: 0.76
{'loss': 3.8519, 'grad_norm': 63.65602111816406, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.0785398483276367, 'eval_runtime': 7.3883, 'eval_samples_per_second': 135.213, 'eval_steps_per_second': 8.527, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:40,  2.84s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:45,  1.98it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.60it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.03it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.10it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:14<00:19,  3.08it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:16,  3.06it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:13,  3.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:21<00:10,  3.38it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:23<00:07,  3.73it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:06,  2.81it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:29<00:03,  3.09it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:31<00:00,  3.40it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:31<00:00,  3.15it/s]
Evaluation performance at step 50: 0.78
{'loss': 2.5099, 'grad_norm': 7.553627014160156, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 2.073108196258545, 'eval_runtime': 7.3949, 'eval_samples_per_second': 135.093, 'eval_steps_per_second': 8.519, 'epoch': 0.08}
{'loss': 1.9129, 'grad_norm': 13.73271369934082, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7962615489959717, 'eval_runtime': 7.378, 'eval_samples_per_second': 135.403, 'eval_steps_per_second': 8.539, 'epoch': 0.12}
{'loss': 1.7003, 'grad_norm': 7.994892120361328, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6054327487945557, 'eval_runtime': 7.4089, 'eval_samples_per_second': 134.838, 'eval_steps_per_second': 8.503, 'epoch': 0.16}
{'loss': 1.5795, 'grad_norm': 6.90092658996582, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5357946157455444, 'eval_runtime': 7.4469, 'eval_samples_per_second': 134.15, 'eval_steps_per_second': 8.46, 'epoch': 0.2}
{'loss': 1.4622, 'grad_norm': 14.133313179016113, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4316904544830322, 'eval_runtime': 7.452, 'eval_samples_per_second': 134.058, 'eval_steps_per_second': 8.454, 'epoch': 0.24}
{'loss': 1.4248, 'grad_norm': 6.116255283355713, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.403868556022644, 'eval_runtime': 7.459, 'eval_samples_per_second': 133.932, 'eval_steps_per_second': 8.446, 'epoch': 0.28}
{'loss': 1.3764, 'grad_norm': 5.741690635681152, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.353894591331482, 'eval_runtime': 7.4541, 'eval_samples_per_second': 134.019, 'eval_steps_per_second': 8.452, 'epoch': 0.32}
{'loss': 1.3302, 'grad_norm': 13.629798889160156, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3164008855819702, 'eval_runtime': 7.5293, 'eval_samples_per_second': 132.681, 'eval_steps_per_second': 8.367, 'epoch': 0.36}
{'loss': 1.3269, 'grad_norm': 35.565006256103516, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3056273460388184, 'eval_runtime': 7.5788, 'eval_samples_per_second': 131.815, 'eval_steps_per_second': 8.313, 'epoch': 0.4}
{'loss': 1.3052, 'grad_norm': 4.60123348236084, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3022228479385376, 'eval_runtime': 7.6108, 'eval_samples_per_second': 131.261, 'eval_steps_per_second': 8.278, 'epoch': 0.44}
{'loss': 1.2838, 'grad_norm': 6.172369003295898, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2987865209579468, 'eval_runtime': 7.6081, 'eval_samples_per_second': 131.307, 'eval_steps_per_second': 8.281, 'epoch': 0.48}
{'loss': 1.2909, 'grad_norm': 37.443092346191406, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3003690242767334, 'eval_runtime': 7.6177, 'eval_samples_per_second': 131.142, 'eval_steps_per_second': 8.27, 'epoch': 0.52}
{'loss': 1.3086, 'grad_norm': 16.224609375, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.290926456451416, 'eval_runtime': 7.631, 'eval_samples_per_second': 130.913, 'eval_steps_per_second': 8.256, 'epoch': 0.56}
{'loss': 1.2802, 'grad_norm': 57.904029846191406, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2933694124221802, 'eval_runtime': 7.6087, 'eval_samples_per_second': 131.297, 'eval_steps_per_second': 8.28, 'epoch': 0.6}
{'loss': 1.2561, 'grad_norm': 12.010684967041016, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2821334600448608, 'eval_runtime': 7.6557, 'eval_samples_per_second': 130.491, 'eval_steps_per_second': 8.229, 'epoch': 0.64}
{'loss': 1.2856, 'grad_norm': 9.193357467651367, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2870069742202759, 'eval_runtime': 7.6202, 'eval_samples_per_second': 131.098, 'eval_steps_per_second': 8.267, 'epoch': 0.68}
{'loss': 1.2911, 'grad_norm': 7.910463809967041, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.276132583618164, 'eval_runtime': 7.619, 'eval_samples_per_second': 131.119, 'eval_steps_per_second': 8.269, 'epoch': 0.72}
{'loss': 1.2564, 'grad_norm': 12.932428359985352, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2731479406356812, 'eval_runtime': 7.6213, 'eval_samples_per_second': 131.079, 'eval_steps_per_second': 8.266, 'epoch': 0.76}
{'loss': 1.3045, 'grad_norm': 6.7206830978393555, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2745814323425293, 'eval_runtime': 7.6301, 'eval_samples_per_second': 130.928, 'eval_steps_per_second': 8.257, 'epoch': 0.8}
{'loss': 1.3011, 'grad_norm': 7.551242351531982, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2729694843292236, 'eval_runtime': 7.6089, 'eval_samples_per_second': 131.293, 'eval_steps_per_second': 8.28, 'epoch': 0.84}
{'loss': 1.2849, 'grad_norm': 8.222185134887695, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2677826881408691, 'eval_runtime': 7.6297, 'eval_samples_per_second': 130.936, 'eval_steps_per_second': 8.257, 'epoch': 0.88}
{'loss': 1.2876, 'grad_norm': 7.0509114265441895, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.264485239982605, 'eval_runtime': 7.5889, 'eval_samples_per_second': 131.64, 'eval_steps_per_second': 8.302, 'epoch': 0.92}
{'loss': 1.2838, 'grad_norm': 13.504204750061035, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2641150951385498, 'eval_runtime': 7.6084, 'eval_samples_per_second': 131.302, 'eval_steps_per_second': 8.28, 'epoch': 0.96}
{'loss': 1.244, 'grad_norm': 4.186975479125977, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2635749578475952, 'eval_runtime': 7.6295, 'eval_samples_per_second': 130.939, 'eval_steps_per_second': 8.257, 'epoch': 1.0}
{'train_runtime': 452.2325, 'train_samples_per_second': 22.108, 'train_steps_per_second': 1.382, 'train_loss': 1.5095485137939453, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0785398483276367, 2.073108196258545, 1.7962615489959717, 1.6054327487945557, 1.5357946157455444, 1.4316904544830322, 1.403868556022644, 1.353894591331482, 1.3164008855819702, 1.3056273460388184, 1.3022228479385376, 1.2987865209579468, 1.3003690242767334, 1.290926456451416, 1.2933694124221802, 1.2821334600448608, 1.2870069742202759, 1.276132583618164, 1.2731479406356812, 1.2745814323425293, 1.2729694843292236, 1.2677826881408691, 1.264485239982605, 1.2641150951385498, 1.2635749578475952], 'performance': [0.76, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:06,  9.16s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:03,  1.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:39,  1.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:25,  1.97it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:14,  2.47it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:06,  2.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.57it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  1.2289037704467773
current iteration best possible performance (full train run):  0.756
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453, 1.2285890579223633, 1.2324609756469727, 1.2289037704467773]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6590 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.8051384164162277
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.896535337841861
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.8965353574293677
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.8780594628685582
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.8330905358205821
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.7779, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.2221, dtype=torch.float64), 1, 1, 1, 0, 0, 0, 2, 1.0408340855860842e-18, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.7779, dtype=torch.float64), tensor(6.4073e-17, dtype=torch.float64), tensor(9.8541e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2221, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.0408e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.778
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.222

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.0408340855860842e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 0]
lora rank:  2
lora dropout:  1.0408340855860842e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 26,624 || all params: 8,030,287,872 || trainable%: 0.0003
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:03,  2.46s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  1.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.65it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.95it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.20it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.67it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.93it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.85it/s]
Evaluation performance at step 25: 0.76
{'loss': 2.7337, 'grad_norm': 38.00598907470703, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 2.4964497089385986, 'eval_runtime': 8.8217, 'eval_samples_per_second': 113.243, 'eval_steps_per_second': 7.141, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:06,  4.31s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:59,  1.53it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:38,  2.13it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:27,  2.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:22,  2.95it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:19,  3.05it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:16,  3.13it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:14,  3.00it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.28it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:07,  3.49it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:06,  2.72it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.95it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.2028, 'grad_norm': 13.5346040725708, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.9296693801879883, 'eval_runtime': 8.8229, 'eval_samples_per_second': 113.228, 'eval_steps_per_second': 7.141, 'epoch': 0.08}
{'loss': 1.798, 'grad_norm': 35.87129592895508, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6585341691970825, 'eval_runtime': 8.8343, 'eval_samples_per_second': 113.081, 'eval_steps_per_second': 7.131, 'epoch': 0.12}
{'loss': 1.5742, 'grad_norm': 12.759359359741211, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4810357093811035, 'eval_runtime': 8.8259, 'eval_samples_per_second': 113.189, 'eval_steps_per_second': 7.138, 'epoch': 0.16}
{'loss': 1.4793, 'grad_norm': 37.81698989868164, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3953777551651, 'eval_runtime': 8.87, 'eval_samples_per_second': 112.627, 'eval_steps_per_second': 7.103, 'epoch': 0.2}
{'loss': 1.3712, 'grad_norm': 36.95436096191406, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3104817867279053, 'eval_runtime': 8.8967, 'eval_samples_per_second': 112.289, 'eval_steps_per_second': 7.081, 'epoch': 0.24}
{'loss': 1.3286, 'grad_norm': 53.93266296386719, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2587089538574219, 'eval_runtime': 8.9542, 'eval_samples_per_second': 111.568, 'eval_steps_per_second': 7.036, 'epoch': 0.28}
{'loss': 1.2397, 'grad_norm': 46.95189666748047, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.225656509399414, 'eval_runtime': 8.9146, 'eval_samples_per_second': 112.063, 'eval_steps_per_second': 7.067, 'epoch': 0.32}
{'loss': 1.2165, 'grad_norm': 33.24501037597656, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2169491052627563, 'eval_runtime': 8.9065, 'eval_samples_per_second': 112.165, 'eval_steps_per_second': 7.073, 'epoch': 0.36}
{'loss': 1.2281, 'grad_norm': 42.709625244140625, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1953659057617188, 'eval_runtime': 8.8937, 'eval_samples_per_second': 112.327, 'eval_steps_per_second': 7.084, 'epoch': 0.4}
{'loss': 1.1845, 'grad_norm': 39.396697998046875, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1970359086990356, 'eval_runtime': 8.8528, 'eval_samples_per_second': 112.845, 'eval_steps_per_second': 7.116, 'epoch': 0.44}
{'loss': 1.2143, 'grad_norm': 29.36244773864746, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1719064712524414, 'eval_runtime': 8.8683, 'eval_samples_per_second': 112.648, 'eval_steps_per_second': 7.104, 'epoch': 0.48}
{'loss': 1.1816, 'grad_norm': 24.72171974182129, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1580262184143066, 'eval_runtime': 8.8625, 'eval_samples_per_second': 112.722, 'eval_steps_per_second': 7.109, 'epoch': 0.52}
{'loss': 1.1366, 'grad_norm': 28.805835723876953, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1517499685287476, 'eval_runtime': 8.8607, 'eval_samples_per_second': 112.745, 'eval_steps_per_second': 7.11, 'epoch': 0.56}
{'loss': 1.1522, 'grad_norm': 28.176122665405273, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1473575830459595, 'eval_runtime': 8.859, 'eval_samples_per_second': 112.767, 'eval_steps_per_second': 7.111, 'epoch': 0.6}
{'loss': 1.1471, 'grad_norm': 33.1763916015625, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1408703327178955, 'eval_runtime': 8.8592, 'eval_samples_per_second': 112.764, 'eval_steps_per_second': 7.111, 'epoch': 0.64}
{'loss': 1.1391, 'grad_norm': 22.016887664794922, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1325933933258057, 'eval_runtime': 8.8429, 'eval_samples_per_second': 112.972, 'eval_steps_per_second': 7.124, 'epoch': 0.68}
{'loss': 1.1307, 'grad_norm': 35.59402084350586, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1294207572937012, 'eval_runtime': 8.8496, 'eval_samples_per_second': 112.887, 'eval_steps_per_second': 7.119, 'epoch': 0.72}
{'loss': 1.1347, 'grad_norm': 27.547698974609375, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1269677877426147, 'eval_runtime': 8.8539, 'eval_samples_per_second': 112.832, 'eval_steps_per_second': 7.116, 'epoch': 0.76}
{'loss': 1.1307, 'grad_norm': 33.05811309814453, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1227668523788452, 'eval_runtime': 8.8365, 'eval_samples_per_second': 113.054, 'eval_steps_per_second': 7.13, 'epoch': 0.8}
{'loss': 1.1183, 'grad_norm': 25.854766845703125, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1194168329238892, 'eval_runtime': 8.864, 'eval_samples_per_second': 112.703, 'eval_steps_per_second': 7.107, 'epoch': 0.84}
{'loss': 1.134, 'grad_norm': 14.553300857543945, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1171258687973022, 'eval_runtime': 8.8975, 'eval_samples_per_second': 112.279, 'eval_steps_per_second': 7.081, 'epoch': 0.88}
{'loss': 1.125, 'grad_norm': 15.519380569458008, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1156134605407715, 'eval_runtime': 8.8931, 'eval_samples_per_second': 112.334, 'eval_steps_per_second': 7.084, 'epoch': 0.92}
{'loss': 1.1124, 'grad_norm': 14.613349914550781, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1118839979171753, 'eval_runtime': 8.9085, 'eval_samples_per_second': 112.139, 'eval_steps_per_second': 7.072, 'epoch': 0.96}
{'loss': 1.1226, 'grad_norm': 12.90214729309082, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1107542514801025, 'eval_runtime': 8.9214, 'eval_samples_per_second': 111.977, 'eval_steps_per_second': 7.062, 'epoch': 1.0}
{'train_runtime': 509.5446, 'train_samples_per_second': 19.623, 'train_steps_per_second': 1.227, 'train_loss': 1.3334263763427734, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4964497089385986, 1.9296693801879883, 1.6585341691970825, 1.4810357093811035, 1.3953777551651, 1.3104817867279053, 1.2587089538574219, 1.225656509399414, 1.2169491052627563, 1.1953659057617188, 1.1970359086990356, 1.1719064712524414, 1.1580262184143066, 1.1517499685287476, 1.1473575830459595, 1.1408703327178955, 1.1325933933258057, 1.1294207572937012, 1.1269677877426147, 1.1227668523788452, 1.1194168329238892, 1.1171258687973022, 1.1156134605407715, 1.1118839979171753, 1.1107542514801025], 'performance': [0.76, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:07<11:35,  7.03s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:13<00:57,  1.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:18<00:31,  2.14it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:24<00:20,  2.43it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:12,  2.77it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:06,  3.15it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.97it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.91it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  1.2248222827911377
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453, 1.2285890579223633, 1.2324609756469727, 1.2289037704467773, 1.2248222827911377]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8318 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 0.9015506824225258
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.6702673667148928
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 0.901405408367406
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.9015505878173284
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.8677160725815083
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.7803, dtype=torch.float64), 0, 0, 0, tensor(0.2196, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0001, dtype=torch.float64), tensor(2.1383e-17, dtype=torch.float64), tensor(1.1761e-16, dtype=torch.float64), tensor(0.7803, dtype=torch.float64), tensor(3.7365e-18, dtype=torch.float64), tensor(2.7863e-18, dtype=torch.float64), tensor(1.9785e-18, dtype=torch.float64), tensor(0.2196, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.78
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.22

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:03,  2.46s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.65it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.79it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.12it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.19it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.39it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.98it/s]
Evaluation performance at step 25: 0.79
{'loss': 4.7334, 'grad_norm': 8.43940258026123, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.79}
{'eval_loss': 4.084117412567139, 'eval_runtime': 5.1394, 'eval_samples_per_second': 194.38, 'eval_steps_per_second': 12.258, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:40,  2.83s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.52it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.69it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:20,  2.90it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:16,  3.02it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  3.06it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.25it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:07,  3.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.69it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.98it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.89it/s]
Evaluation performance at step 50: 0.77
{'loss': 3.1471, 'grad_norm': 9.230542182922363, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 2.319016695022583, 'eval_runtime': 5.123, 'eval_samples_per_second': 195.003, 'eval_steps_per_second': 12.297, 'epoch': 0.08}
{'loss': 2.0701, 'grad_norm': 5.207773208618164, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8502792119979858, 'eval_runtime': 5.1301, 'eval_samples_per_second': 194.731, 'eval_steps_per_second': 12.28, 'epoch': 0.12}
{'loss': 1.7457, 'grad_norm': 14.915103912353516, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.675565242767334, 'eval_runtime': 5.136, 'eval_samples_per_second': 194.511, 'eval_steps_per_second': 12.266, 'epoch': 0.16}
{'loss': 1.6326, 'grad_norm': 6.925330638885498, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.570032000541687, 'eval_runtime': 5.1473, 'eval_samples_per_second': 194.081, 'eval_steps_per_second': 12.239, 'epoch': 0.2}
{'loss': 1.5525, 'grad_norm': 14.41478443145752, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.531994342803955, 'eval_runtime': 5.1543, 'eval_samples_per_second': 193.818, 'eval_steps_per_second': 12.223, 'epoch': 0.24}
{'loss': 1.5159, 'grad_norm': 6.505041599273682, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.506529688835144, 'eval_runtime': 5.1581, 'eval_samples_per_second': 193.677, 'eval_steps_per_second': 12.214, 'epoch': 0.28}
{'loss': 1.4577, 'grad_norm': 4.302783966064453, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.471486210823059, 'eval_runtime': 5.1712, 'eval_samples_per_second': 193.186, 'eval_steps_per_second': 12.183, 'epoch': 0.32}
{'loss': 1.4542, 'grad_norm': 6.893202781677246, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.404408574104309, 'eval_runtime': 5.1836, 'eval_samples_per_second': 192.723, 'eval_steps_per_second': 12.154, 'epoch': 0.36}
{'loss': 1.3139, 'grad_norm': 3.212998151779175, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2982590198516846, 'eval_runtime': 5.1593, 'eval_samples_per_second': 193.63, 'eval_steps_per_second': 12.211, 'epoch': 0.4}
{'loss': 1.2856, 'grad_norm': 3.349074602127075, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2773391008377075, 'eval_runtime': 5.1594, 'eval_samples_per_second': 193.626, 'eval_steps_per_second': 12.211, 'epoch': 0.44}
{'loss': 1.3003, 'grad_norm': 3.5516278743743896, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2496966123580933, 'eval_runtime': 5.1697, 'eval_samples_per_second': 193.241, 'eval_steps_per_second': 12.186, 'epoch': 0.48}
{'loss': 1.2448, 'grad_norm': 4.466007709503174, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1739696264266968, 'eval_runtime': 5.1576, 'eval_samples_per_second': 193.695, 'eval_steps_per_second': 12.215, 'epoch': 0.52}
{'loss': 1.1839, 'grad_norm': 14.594494819641113, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.159704327583313, 'eval_runtime': 5.1685, 'eval_samples_per_second': 193.285, 'eval_steps_per_second': 12.189, 'epoch': 0.56}
{'loss': 1.1755, 'grad_norm': 11.038461685180664, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1673246622085571, 'eval_runtime': 5.1732, 'eval_samples_per_second': 193.109, 'eval_steps_per_second': 12.178, 'epoch': 0.6}
{'loss': 1.2068, 'grad_norm': 210.9664764404297, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1569736003875732, 'eval_runtime': 5.1673, 'eval_samples_per_second': 193.331, 'eval_steps_per_second': 12.192, 'epoch': 0.64}
{'loss': 1.1851, 'grad_norm': 12.831013679504395, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1521267890930176, 'eval_runtime': 5.1675, 'eval_samples_per_second': 193.324, 'eval_steps_per_second': 12.192, 'epoch': 0.68}
{'loss': 1.1547, 'grad_norm': 9.710192680358887, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1513396501541138, 'eval_runtime': 5.1565, 'eval_samples_per_second': 193.735, 'eval_steps_per_second': 12.218, 'epoch': 0.72}
{'loss': 1.1866, 'grad_norm': 12.070252418518066, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1467703580856323, 'eval_runtime': 5.1542, 'eval_samples_per_second': 193.822, 'eval_steps_per_second': 12.223, 'epoch': 0.76}
{'loss': 1.1717, 'grad_norm': 12.536229133605957, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.154192328453064, 'eval_runtime': 5.1619, 'eval_samples_per_second': 193.535, 'eval_steps_per_second': 12.205, 'epoch': 0.8}
{'loss': 1.1425, 'grad_norm': 2.2843096256256104, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1439937353134155, 'eval_runtime': 5.1575, 'eval_samples_per_second': 193.7, 'eval_steps_per_second': 12.215, 'epoch': 0.84}
{'loss': 1.1297, 'grad_norm': 19.260995864868164, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.140619158744812, 'eval_runtime': 5.1765, 'eval_samples_per_second': 192.989, 'eval_steps_per_second': 12.17, 'epoch': 0.88}
{'loss': 1.14, 'grad_norm': 46.936241149902344, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1362941265106201, 'eval_runtime': 5.1654, 'eval_samples_per_second': 193.402, 'eval_steps_per_second': 12.197, 'epoch': 0.92}
{'loss': 1.1446, 'grad_norm': 47.94572830200195, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1379326581954956, 'eval_runtime': 5.1657, 'eval_samples_per_second': 193.391, 'eval_steps_per_second': 12.196, 'epoch': 0.96}
{'loss': 1.1553, 'grad_norm': 3.875746488571167, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1387964487075806, 'eval_runtime': 5.1603, 'eval_samples_per_second': 193.595, 'eval_steps_per_second': 12.209, 'epoch': 1.0}
{'train_runtime': 344.2272, 'train_samples_per_second': 29.045, 'train_steps_per_second': 1.816, 'train_loss': 1.537219418334961, 'epoch': 1.0}
train_results:  {'eval_loss': [4.084117412567139, 2.319016695022583, 1.8502792119979858, 1.675565242767334, 1.570032000541687, 1.531994342803955, 1.506529688835144, 1.471486210823059, 1.404408574104309, 1.2982590198516846, 1.2773391008377075, 1.2496966123580933, 1.1739696264266968, 1.159704327583313, 1.1673246622085571, 1.1569736003875732, 1.1521267890930176, 1.1513396501541138, 1.1467703580856323, 1.154192328453064, 1.1439937353134155, 1.140619158744812, 1.1362941265106201, 1.1379326581954956, 1.1387964487075806], 'performance': [0.79, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<11:11,  6.79s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:52,  1.59it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:19<00:34,  1.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:20,  2.47it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:12,  2.79it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:06,  3.10it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.99it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.91it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.79, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  1.2302109003067017
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453, 1.2285890579223633, 1.2324609756469727, 1.2289037704467773, 1.2248222827911377, 1.2302109003067017]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1592 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.9019603297984671
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 0.8931210630583151
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.702384463322808
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.901960236203122
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.9019416406688892
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.7746, dtype=torch.float64), 0, 0, 0, tensor(0.2210, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 1.48101556931323e-17, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0044, dtype=torch.float64), tensor(2.5748e-18, dtype=torch.float64), tensor(1.3091e-16, dtype=torch.float64), tensor(0.7746, dtype=torch.float64), tensor(1.8866e-18, dtype=torch.float64), tensor(1.0542e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2210, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.4810e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.775
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.221

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.48101556931323e-17,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  1.48101556931323e-17
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9955
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  995
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:35,  4.60s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:25,  1.06it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:47,  1.75it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:34,  2.17it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:18<00:33,  2.01it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:27<00:41,  1.42it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:33<00:37,  1.36it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:39<00:31,  1.37it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:44<00:23,  1.47it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:49<00:18,  1.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:59<00:16,  1.18it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [01:04<00:08,  1.29it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [01:08<00:02,  1.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:08<00:00,  1.45it/s]
Evaluation performance at step 25: 0.78
{'loss': 5.0249, 'grad_norm': 4.283257961273193, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 4.8966383934021, 'eval_runtime': 6.1776, 'eval_samples_per_second': 161.067, 'eval_steps_per_second': 10.198, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:33,  5.19s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:38,  1.08s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:08,  1.21it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:19<00:47,  1.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:39,  1.72it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:30<00:40,  1.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:37<00:38,  1.32it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:41<00:30,  1.43it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:44<00:20,  1.71it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:47<00:14,  1.92it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:53<00:11,  1.72it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:55<00:05,  2.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:57<00:01,  2.38it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:57<00:00,  1.73it/s]
Evaluation performance at step 50: 0.76
{'loss': 4.558, 'grad_norm': 28.019155502319336, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 4.206611156463623, 'eval_runtime': 5.0645, 'eval_samples_per_second': 196.465, 'eval_steps_per_second': 12.44, 'epoch': 0.08}
{'loss': 3.976, 'grad_norm': 10.8817720413208, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 3.7792320251464844, 'eval_runtime': 5.4106, 'eval_samples_per_second': 183.897, 'eval_steps_per_second': 11.644, 'epoch': 0.12}
{'loss': 3.6987, 'grad_norm': 5.826149940490723, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 3.644536018371582, 'eval_runtime': 8.3218, 'eval_samples_per_second': 119.565, 'eval_steps_per_second': 7.57, 'epoch': 0.16}
{'loss': 3.5549, 'grad_norm': 6.696503639221191, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 3.49613094329834, 'eval_runtime': 8.3108, 'eval_samples_per_second': 119.723, 'eval_steps_per_second': 7.58, 'epoch': 0.2}
{'loss': 3.4452, 'grad_norm': 9.510396003723145, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 3.4116570949554443, 'eval_runtime': 7.2025, 'eval_samples_per_second': 138.146, 'eval_steps_per_second': 8.747, 'epoch': 0.24}
{'loss': 3.3496, 'grad_norm': 17.27337074279785, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 3.3632102012634277, 'eval_runtime': 8.0161, 'eval_samples_per_second': 124.126, 'eval_steps_per_second': 7.859, 'epoch': 0.28}
{'loss': 3.3279, 'grad_norm': 19.642154693603516, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 3.3420920372009277, 'eval_runtime': 7.9425, 'eval_samples_per_second': 125.275, 'eval_steps_per_second': 7.932, 'epoch': 0.32}
{'loss': 3.2301, 'grad_norm': 2.6476378440856934, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 3.2566111087799072, 'eval_runtime': 8.3455, 'eval_samples_per_second': 119.226, 'eval_steps_per_second': 7.549, 'epoch': 0.36}
{'loss': 3.2232, 'grad_norm': 3.7412819862365723, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 3.1966910362243652, 'eval_runtime': 8.3995, 'eval_samples_per_second': 118.46, 'eval_steps_per_second': 7.5, 'epoch': 0.4}
{'loss': 3.2052, 'grad_norm': 2.3219239711761475, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 3.1091177463531494, 'eval_runtime': 8.1137, 'eval_samples_per_second': 122.632, 'eval_steps_per_second': 7.765, 'epoch': 0.44}
{'loss': 3.0559, 'grad_norm': 15.011123657226562, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 3.062370777130127, 'eval_runtime': 8.204, 'eval_samples_per_second': 121.282, 'eval_steps_per_second': 7.679, 'epoch': 0.48}
{'loss': 3.0477, 'grad_norm': 6.08868408203125, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 3.026231288909912, 'eval_runtime': 7.5095, 'eval_samples_per_second': 132.499, 'eval_steps_per_second': 8.389, 'epoch': 0.52}
{'loss': 2.9973, 'grad_norm': 4.654029369354248, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 2.991628646850586, 'eval_runtime': 7.4134, 'eval_samples_per_second': 134.216, 'eval_steps_per_second': 8.498, 'epoch': 0.56}
{'loss': 2.9827, 'grad_norm': 4.8673834800720215, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 2.9681801795959473, 'eval_runtime': 8.2329, 'eval_samples_per_second': 120.856, 'eval_steps_per_second': 7.652, 'epoch': 0.6}
{'loss': 2.9407, 'grad_norm': 14.352046012878418, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 2.9463789463043213, 'eval_runtime': 8.0301, 'eval_samples_per_second': 123.909, 'eval_steps_per_second': 7.845, 'epoch': 0.64}
{'loss': 2.8881, 'grad_norm': 25.141197204589844, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 2.9301207065582275, 'eval_runtime': 7.798, 'eval_samples_per_second': 127.597, 'eval_steps_per_second': 8.079, 'epoch': 0.68}
{'loss': 2.9302, 'grad_norm': 3.65395188331604, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 2.9126343727111816, 'eval_runtime': 8.5096, 'eval_samples_per_second': 116.927, 'eval_steps_per_second': 7.403, 'epoch': 0.72}
{'loss': 2.9141, 'grad_norm': 28.68320655822754, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 2.8998374938964844, 'eval_runtime': 8.3282, 'eval_samples_per_second': 119.474, 'eval_steps_per_second': 7.565, 'epoch': 0.76}
{'loss': 2.9067, 'grad_norm': 22.091358184814453, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 2.8918888568878174, 'eval_runtime': 8.8513, 'eval_samples_per_second': 112.413, 'eval_steps_per_second': 7.118, 'epoch': 0.8}
{'loss': 2.859, 'grad_norm': 9.29356575012207, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 2.8786861896514893, 'eval_runtime': 7.0083, 'eval_samples_per_second': 141.975, 'eval_steps_per_second': 8.989, 'epoch': 0.84}
{'loss': 2.8476, 'grad_norm': 1.9525723457336426, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 2.8656814098358154, 'eval_runtime': 7.7458, 'eval_samples_per_second': 128.456, 'eval_steps_per_second': 8.133, 'epoch': 0.88}
{'loss': 2.8493, 'grad_norm': 2.721142292022705, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 2.858203649520874, 'eval_runtime': 8.824, 'eval_samples_per_second': 112.761, 'eval_steps_per_second': 7.14, 'epoch': 0.92}
{'loss': 2.8228, 'grad_norm': 3.0401341915130615, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 2.849897623062134, 'eval_runtime': 7.1336, 'eval_samples_per_second': 139.48, 'eval_steps_per_second': 8.831, 'epoch': 0.96}
{'train_runtime': 571.0345, 'train_samples_per_second': 17.433, 'train_steps_per_second': 1.091, 'train_loss': 3.2608043248159735, 'epoch': 1.0}
train_results:  {'eval_loss': [4.8966383934021, 4.206611156463623, 3.7792320251464844, 3.644536018371582, 3.49613094329834, 3.4116570949554443, 3.3632102012634277, 3.3420920372009277, 3.2566111087799072, 3.1966910362243652, 3.1091177463531494, 3.062370777130127, 3.026231288909912, 2.991628646850586, 2.9681801795959473, 2.9463789463043213, 2.9301207065582275, 2.9126343727111816, 2.8998374938964844, 2.8918888568878174, 2.8786861896514893, 2.8656814098358154, 2.858203649520874, 2.849897623062134], 'performance': [0.78, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:22<37:25, 22.69s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:41<02:50,  2.06s/it]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [01:09<02:07,  1.90s/it]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [01:27<01:19,  1.55s/it]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [01:50<00:52,  1.49s/it]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [02:17<00:29,  1.56s/it]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [02:29<00:03,  1.30s/it]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:29<00:00,  1.50s/it]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2300595045089722
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453, 1.2285890579223633, 1.2324609756469727, 1.2289037704467773, 1.2248222827911377, 1.2302109003067017, 1.2300595045089722]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 99.5892 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.6021058062374509
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.9332413336641953
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.739961428311111
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.6261889856071458
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 0.906889125813541
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3006, dtype=torch.float64), 0, 0, tensor(0.3884, dtype=torch.float64), 0, 0, tensor(0.0849, dtype=torch.float64), tensor(0.2228, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.042680334383993296, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(8.7254e-17, dtype=torch.float64), tensor(0.3006, dtype=torch.float64), tensor(0.0033, dtype=torch.float64), tensor(9.0730e-17, dtype=torch.float64), tensor(0.3884, dtype=torch.float64), tensor(2.5300e-17, dtype=torch.float64), tensor(2.4401e-17, dtype=torch.float64), tensor(0.0849, dtype=torch.float64), tensor(0.2228, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.4268, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.301
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.388
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.085
  arc_challenge: 0.223

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.042680334383993296,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.042680334383993296
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9965
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  996
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:16<27:33, 16.70s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:28<04:03,  2.67s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:34<02:12,  1.60s/it]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:37<01:16,  1.02s/it]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:39<00:50,  1.32it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:42<00:36,  1.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:50<00:37,  1.35it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:53<00:26,  1.65it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:55<00:17,  2.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:57<00:11,  2.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [01:00<00:07,  2.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [01:02<00:03,  2.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [01:03<00:00,  3.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:03<00:00,  1.56it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.6919, 'grad_norm': 3.8457043170928955, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.4459972381591797, 'eval_runtime': 8.1221, 'eval_samples_per_second': 122.629, 'eval_steps_per_second': 7.757, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:19,  3.83s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:56,  1.61it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:36,  2.29it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:26,  2.78it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:21,  3.05it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:19,  3.07it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:16,  3.04it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:14,  3.00it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.67it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.96it/s]
Evaluation performance at step 50: 0.71
{'loss': 3.0322, 'grad_norm': 1.1802523136138916, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.71}
{'eval_loss': 2.7835614681243896, 'eval_runtime': 8.0877, 'eval_samples_per_second': 123.149, 'eval_steps_per_second': 7.79, 'epoch': 0.08}
{'loss': 2.686, 'grad_norm': 1.4501463174819946, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 2.5275418758392334, 'eval_runtime': 8.1628, 'eval_samples_per_second': 122.017, 'eval_steps_per_second': 7.718, 'epoch': 0.12}
{'loss': 2.4393, 'grad_norm': 0.7014777064323425, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 2.4134860038757324, 'eval_runtime': 8.1378, 'eval_samples_per_second': 122.392, 'eval_steps_per_second': 7.742, 'epoch': 0.16}
{'loss': 2.3777, 'grad_norm': 0.7221799492835999, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 2.348860263824463, 'eval_runtime': 8.1507, 'eval_samples_per_second': 122.199, 'eval_steps_per_second': 7.729, 'epoch': 0.2}
{'loss': 2.3172, 'grad_norm': 0.7604289650917053, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 2.3032784461975098, 'eval_runtime': 8.1769, 'eval_samples_per_second': 121.807, 'eval_steps_per_second': 7.705, 'epoch': 0.24}
{'loss': 2.2413, 'grad_norm': 0.7706080675125122, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 2.2653961181640625, 'eval_runtime': 8.1875, 'eval_samples_per_second': 121.649, 'eval_steps_per_second': 7.695, 'epoch': 0.28}
{'loss': 2.2276, 'grad_norm': 0.8161025047302246, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 2.238478660583496, 'eval_runtime': 8.1955, 'eval_samples_per_second': 121.53, 'eval_steps_per_second': 7.687, 'epoch': 0.32}
{'loss': 2.228, 'grad_norm': 0.7710017561912537, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 2.2157511711120605, 'eval_runtime': 8.1885, 'eval_samples_per_second': 121.634, 'eval_steps_per_second': 7.694, 'epoch': 0.36}
{'loss': 2.1794, 'grad_norm': 0.8511344194412231, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 2.196321964263916, 'eval_runtime': 8.2039, 'eval_samples_per_second': 121.405, 'eval_steps_per_second': 7.679, 'epoch': 0.4}
{'loss': 2.2377, 'grad_norm': 0.7725396156311035, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 2.179922580718994, 'eval_runtime': 8.2029, 'eval_samples_per_second': 121.421, 'eval_steps_per_second': 7.68, 'epoch': 0.44}
{'loss': 2.2107, 'grad_norm': 1.0759594440460205, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 2.1677937507629395, 'eval_runtime': 8.2043, 'eval_samples_per_second': 121.4, 'eval_steps_per_second': 7.679, 'epoch': 0.48}
{'loss': 2.1408, 'grad_norm': 0.9581331610679626, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 2.155613899230957, 'eval_runtime': 8.2158, 'eval_samples_per_second': 121.23, 'eval_steps_per_second': 7.668, 'epoch': 0.52}
{'loss': 2.1423, 'grad_norm': 0.8774273991584778, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 2.145595073699951, 'eval_runtime': 8.2214, 'eval_samples_per_second': 121.147, 'eval_steps_per_second': 7.663, 'epoch': 0.56}
{'loss': 2.1635, 'grad_norm': 1.9766290187835693, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 2.1382384300231934, 'eval_runtime': 8.22, 'eval_samples_per_second': 121.168, 'eval_steps_per_second': 7.664, 'epoch': 0.6}
{'loss': 2.1293, 'grad_norm': 0.9597134590148926, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 2.1328847408294678, 'eval_runtime': 8.2289, 'eval_samples_per_second': 121.037, 'eval_steps_per_second': 7.656, 'epoch': 0.64}
{'loss': 2.2067, 'grad_norm': 1.0093082189559937, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 2.127709150314331, 'eval_runtime': 8.2131, 'eval_samples_per_second': 121.269, 'eval_steps_per_second': 7.671, 'epoch': 0.68}
{'loss': 2.125, 'grad_norm': 0.7129443287849426, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 2.1230616569519043, 'eval_runtime': 8.5691, 'eval_samples_per_second': 116.231, 'eval_steps_per_second': 7.352, 'epoch': 0.72}
{'loss': 2.0576, 'grad_norm': 0.7188094854354858, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 2.119046211242676, 'eval_runtime': 8.5317, 'eval_samples_per_second': 116.741, 'eval_steps_per_second': 7.384, 'epoch': 0.76}
{'loss': 2.1471, 'grad_norm': 0.8010233640670776, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 2.1161534786224365, 'eval_runtime': 8.5247, 'eval_samples_per_second': 116.836, 'eval_steps_per_second': 7.39, 'epoch': 0.8}
{'loss': 2.0831, 'grad_norm': 0.571252703666687, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 2.1145853996276855, 'eval_runtime': 8.5124, 'eval_samples_per_second': 117.006, 'eval_steps_per_second': 7.401, 'epoch': 0.84}
{'loss': 2.165, 'grad_norm': 1.030684471130371, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 2.1130459308624268, 'eval_runtime': 8.5539, 'eval_samples_per_second': 116.438, 'eval_steps_per_second': 7.365, 'epoch': 0.88}
{'loss': 2.1142, 'grad_norm': 0.6251826882362366, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 2.1118993759155273, 'eval_runtime': 8.5823, 'eval_samples_per_second': 116.053, 'eval_steps_per_second': 7.341, 'epoch': 0.92}
{'loss': 2.0942, 'grad_norm': 2.5740301609039307, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 2.1095998287200928, 'eval_runtime': 8.5056, 'eval_samples_per_second': 117.1, 'eval_steps_per_second': 7.407, 'epoch': 0.96}
{'train_runtime': 430.0909, 'train_samples_per_second': 23.17, 'train_steps_per_second': 1.449, 'train_loss': 2.300355344867247, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4459972381591797, 2.7835614681243896, 2.5275418758392334, 2.4134860038757324, 2.348860263824463, 2.3032784461975098, 2.2653961181640625, 2.238478660583496, 2.2157511711120605, 2.196321964263916, 2.179922580718994, 2.1677937507629395, 2.155613899230957, 2.145595073699951, 2.1382384300231934, 2.1328847408294678, 2.127709150314331, 2.1230616569519043, 2.119046211242676, 2.1161534786224365, 2.1145853996276855, 2.1130459308624268, 2.1118993759155273, 2.1095998287200928], 'performance': [0.77, 0.71]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:14<24:02, 14.57s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:23<01:32,  1.12s/it]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:35<01:01,  1.10it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:45<00:39,  1.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:52<00:22,  1.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:59<00:11,  1.71it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [01:01<00:01,  2.34it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:01<00:00,  1.62it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.71]
current iteration observed (possibly low-fid or predicted) performance:  1.236921787261963
current iteration best possible performance (full train run):  0.8295000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453, 1.2285890579223633, 1.2324609756469727, 1.2289037704467773, 1.2248222827911377, 1.2302109003067017, 1.2300595045089722, 1.236921787261963]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.1104 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.7877382898252328
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.7912071449437508
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.8588898111586845
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.859398207044267
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.854743374537112
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3861, dtype=torch.float64), 0, tensor(0.0528, dtype=torch.float64), tensor(0.2984, dtype=torch.float64), 0, 0, tensor(0.0415, dtype=torch.float64), tensor(0.2213, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.0013384058095789455, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(2.0825e-16, dtype=torch.float64), tensor(0.3861, dtype=torch.float64), tensor(7.5974e-17, dtype=torch.float64), tensor(0.0528, dtype=torch.float64), tensor(0.2984, dtype=torch.float64), tensor(8.8338e-19, dtype=torch.float64), tensor(4.5474e-17, dtype=torch.float64), tensor(0.0415, dtype=torch.float64), tensor(0.2213, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.0134, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.386
  rowan_hellaswag: 0
  sciq: 0.053
  triviaqa: 0.298
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.042
  arc_challenge: 0.221

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0013384058095789455,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0013384058095789455
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:54,  2.97s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.95it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.69it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:24,  2.39it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.62it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:15,  2.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  3.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.93it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.27it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.75it/s]
Evaluation performance at step 25: 0.78
{'loss': 3.4788, 'grad_norm': 2.944690227508545, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 3.290872097015381, 'eval_runtime': 8.4127, 'eval_samples_per_second': 118.748, 'eval_steps_per_second': 7.489, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:23,  3.87s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:55,  1.63it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:36,  2.30it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.80it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:21,  3.09it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:19,  3.09it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:16,  3.05it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:14,  3.01it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.29it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:07,  3.39it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.68it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.96it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.9022, 'grad_norm': 1.4514909982681274, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 2.6536154747009277, 'eval_runtime': 8.427, 'eval_samples_per_second': 118.547, 'eval_steps_per_second': 7.476, 'epoch': 0.08}
{'loss': 2.57, 'grad_norm': 0.7502831816673279, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.4131157398223877, 'eval_runtime': 8.4489, 'eval_samples_per_second': 118.24, 'eval_steps_per_second': 7.457, 'epoch': 0.12}
{'loss': 2.3844, 'grad_norm': 0.7847613096237183, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.2995457649230957, 'eval_runtime': 8.4656, 'eval_samples_per_second': 118.008, 'eval_steps_per_second': 7.442, 'epoch': 0.16}
{'loss': 2.3014, 'grad_norm': 0.7674944996833801, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.2382588386535645, 'eval_runtime': 8.4888, 'eval_samples_per_second': 117.685, 'eval_steps_per_second': 7.422, 'epoch': 0.2}
{'loss': 2.1838, 'grad_norm': 0.6661109328269958, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1941282749176025, 'eval_runtime': 8.513, 'eval_samples_per_second': 117.35, 'eval_steps_per_second': 7.4, 'epoch': 0.24}
{'loss': 2.1739, 'grad_norm': 0.6903030276298523, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.1584036350250244, 'eval_runtime': 8.5231, 'eval_samples_per_second': 117.211, 'eval_steps_per_second': 7.392, 'epoch': 0.28}
{'loss': 2.1063, 'grad_norm': 0.7387348413467407, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.1332318782806396, 'eval_runtime': 8.5302, 'eval_samples_per_second': 117.114, 'eval_steps_per_second': 7.386, 'epoch': 0.32}
{'loss': 2.1359, 'grad_norm': 0.7008763551712036, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.1156933307647705, 'eval_runtime': 8.5229, 'eval_samples_per_second': 117.213, 'eval_steps_per_second': 7.392, 'epoch': 0.36}
{'loss': 2.1074, 'grad_norm': 0.9719405174255371, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.101658821105957, 'eval_runtime': 8.5158, 'eval_samples_per_second': 117.312, 'eval_steps_per_second': 7.398, 'epoch': 0.4}
{'loss': 2.1115, 'grad_norm': 0.7255967259407043, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0894854068756104, 'eval_runtime': 8.5243, 'eval_samples_per_second': 117.194, 'eval_steps_per_second': 7.391, 'epoch': 0.44}
{'loss': 2.0746, 'grad_norm': 0.6450238227844238, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.0820600986480713, 'eval_runtime': 8.548, 'eval_samples_per_second': 116.87, 'eval_steps_per_second': 7.37, 'epoch': 0.48}
{'loss': 2.0575, 'grad_norm': 0.88008052110672, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.0766773223876953, 'eval_runtime': 8.5865, 'eval_samples_per_second': 116.346, 'eval_steps_per_second': 7.337, 'epoch': 0.52}
{'loss': 2.0747, 'grad_norm': 0.7091348767280579, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.0716054439544678, 'eval_runtime': 8.5454, 'eval_samples_per_second': 116.906, 'eval_steps_per_second': 7.372, 'epoch': 0.56}
{'loss': 2.0443, 'grad_norm': 0.7658272981643677, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.0662131309509277, 'eval_runtime': 8.5362, 'eval_samples_per_second': 117.031, 'eval_steps_per_second': 7.38, 'epoch': 0.6}
{'loss': 2.0956, 'grad_norm': 0.6696415543556213, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.0589499473571777, 'eval_runtime': 8.5223, 'eval_samples_per_second': 117.221, 'eval_steps_per_second': 7.392, 'epoch': 0.64}
{'loss': 2.0828, 'grad_norm': 0.8981508016586304, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.0545687675476074, 'eval_runtime': 8.5488, 'eval_samples_per_second': 116.858, 'eval_steps_per_second': 7.369, 'epoch': 0.68}
{'loss': 2.0507, 'grad_norm': 0.8104254007339478, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.051318883895874, 'eval_runtime': 8.5107, 'eval_samples_per_second': 117.382, 'eval_steps_per_second': 7.402, 'epoch': 0.72}
{'loss': 2.0255, 'grad_norm': 0.7289076447486877, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.0489981174468994, 'eval_runtime': 8.5799, 'eval_samples_per_second': 116.434, 'eval_steps_per_second': 7.343, 'epoch': 0.76}
{'loss': 2.0743, 'grad_norm': 1.0541859865188599, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.045823335647583, 'eval_runtime': 8.5839, 'eval_samples_per_second': 116.38, 'eval_steps_per_second': 7.339, 'epoch': 0.8}
{'loss': 2.0202, 'grad_norm': 0.7433708906173706, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0426042079925537, 'eval_runtime': 8.5762, 'eval_samples_per_second': 116.485, 'eval_steps_per_second': 7.346, 'epoch': 0.84}
{'loss': 2.0546, 'grad_norm': 1.2940784692764282, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0409204959869385, 'eval_runtime': 8.58, 'eval_samples_per_second': 116.434, 'eval_steps_per_second': 7.343, 'epoch': 0.88}
{'loss': 2.0071, 'grad_norm': 0.7983853220939636, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.0398919582366943, 'eval_runtime': 8.5736, 'eval_samples_per_second': 116.52, 'eval_steps_per_second': 7.348, 'epoch': 0.92}
{'loss': 2.0482, 'grad_norm': 0.8569035530090332, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.039372682571411, 'eval_runtime': 8.5708, 'eval_samples_per_second': 116.559, 'eval_steps_per_second': 7.351, 'epoch': 0.96}
{'loss': 2.0196, 'grad_norm': 0.7885079383850098, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.039029121398926, 'eval_runtime': 8.5663, 'eval_samples_per_second': 116.62, 'eval_steps_per_second': 7.354, 'epoch': 1.0}
{'train_runtime': 410.7816, 'train_samples_per_second': 24.337, 'train_steps_per_second': 1.521, 'train_loss': 2.2074129333496093, 'epoch': 1.0}
train_results:  {'eval_loss': [3.290872097015381, 2.6536154747009277, 2.4131157398223877, 2.2995457649230957, 2.2382588386535645, 2.1941282749176025, 2.1584036350250244, 2.1332318782806396, 2.1156933307647705, 2.101658821105957, 2.0894854068756104, 2.0820600986480713, 2.0766773223876953, 2.0716054439544678, 2.0662131309509277, 2.0589499473571777, 2.0545687675476074, 2.051318883895874, 2.0489981174468994, 2.045823335647583, 2.0426042079925537, 2.0409204959869385, 2.0398919582366943, 2.039372682571411, 2.039029121398926], 'performance': [0.78, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<14:49,  8.99s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:04,  1.29it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:41,  1.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:31<00:28,  1.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:35<00:15,  2.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:43<00:08,  2.21it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:45<00:01,  2.91it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:45<00:00,  2.20it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  1.2368407249450684
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453, 1.2285890579223633, 1.2324609756469727, 1.2289037704467773, 1.2248222827911377, 1.2302109003067017, 1.2300595045089722, 1.236921787261963, 1.2368407249450684]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.0402 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.8019813498513197
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.860317966922423
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.860317921499221
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.8485370679494719
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.8579295436033489
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3712, dtype=torch.float64), 0, tensor(0.0280, dtype=torch.float64), tensor(0.3691, dtype=torch.float64), 0, 0, 0, tensor(0.2220, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 87, 0.02285716141995678, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(7.8962e-19, dtype=torch.float64), tensor(0.3712, dtype=torch.float64), tensor(0.0003, dtype=torch.float64), tensor(0.0280, dtype=torch.float64), tensor(0.3691, dtype=torch.float64), tensor(1.2355e-16, dtype=torch.float64), tensor(1.2907e-18, dtype=torch.float64), tensor(0.0094, dtype=torch.float64), tensor(0.2220, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6828, dtype=torch.float64), tensor(0.2286, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.371
  rowan_hellaswag: 0
  sciq: 0.028
  triviaqa: 0.369
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.222

LoRA Parameters:
  lora_r: (87,)
  lora_dropout: (0.02285716141995678,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  87
lora dropout:  0.02285716141995678
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 712,704 || all params: 8,030,973,952 || trainable%: 0.0089
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9901
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  990
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:00,  3.03s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:14,  1.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.95it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:26,  2.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.58it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.71it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:15,  2.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.04it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.88it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.18it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.73it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.438, 'grad_norm': 0.7112406492233276, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.28275728225708, 'eval_runtime': 8.5387, 'eval_samples_per_second': 115.942, 'eval_steps_per_second': 7.261, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:19,  4.44s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:05,  1.38it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:40,  2.05it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.58it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:21,  2.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.77it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.05it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.19it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.55it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:04,  2.71it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.72it/s]
Evaluation performance at step 50: 0.71
{'loss': 2.8914, 'grad_norm': 0.1827215999364853, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.71}
{'eval_loss': 2.6594083309173584, 'eval_runtime': 8.5741, 'eval_samples_per_second': 115.464, 'eval_steps_per_second': 7.231, 'epoch': 0.08}
{'loss': 2.5347, 'grad_norm': 0.16972264647483826, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 2.45967435836792, 'eval_runtime': 8.5946, 'eval_samples_per_second': 115.188, 'eval_steps_per_second': 7.214, 'epoch': 0.12}
{'loss': 2.4021, 'grad_norm': 0.10643228888511658, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 2.3535852432250977, 'eval_runtime': 8.5652, 'eval_samples_per_second': 115.584, 'eval_steps_per_second': 7.239, 'epoch': 0.16}
{'loss': 2.3639, 'grad_norm': 0.11824877560138702, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 2.27553653717041, 'eval_runtime': 8.6016, 'eval_samples_per_second': 115.094, 'eval_steps_per_second': 7.208, 'epoch': 0.2}
{'loss': 2.3605, 'grad_norm': 0.11276506632566452, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 2.2119970321655273, 'eval_runtime': 8.617, 'eval_samples_per_second': 114.889, 'eval_steps_per_second': 7.195, 'epoch': 0.24}
{'loss': 2.2326, 'grad_norm': 0.11123871058225632, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 2.1700124740600586, 'eval_runtime': 8.6283, 'eval_samples_per_second': 114.739, 'eval_steps_per_second': 7.186, 'epoch': 0.28}
{'loss': 2.2513, 'grad_norm': 0.19327473640441895, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 2.135000467300415, 'eval_runtime': 8.6314, 'eval_samples_per_second': 114.698, 'eval_steps_per_second': 7.183, 'epoch': 0.32}
{'loss': 2.1157, 'grad_norm': 0.11421075463294983, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 2.1015782356262207, 'eval_runtime': 8.639, 'eval_samples_per_second': 114.597, 'eval_steps_per_second': 7.177, 'epoch': 0.36}
{'loss': 2.0785, 'grad_norm': 0.30269885063171387, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 2.0768277645111084, 'eval_runtime': 8.6445, 'eval_samples_per_second': 114.523, 'eval_steps_per_second': 7.172, 'epoch': 0.4}
{'loss': 2.1045, 'grad_norm': 0.11292026937007904, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 2.0569405555725098, 'eval_runtime': 8.6325, 'eval_samples_per_second': 114.683, 'eval_steps_per_second': 7.182, 'epoch': 0.44}
{'loss': 2.07, 'grad_norm': 0.1769006848335266, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 2.0412824153900146, 'eval_runtime': 8.6486, 'eval_samples_per_second': 114.47, 'eval_steps_per_second': 7.169, 'epoch': 0.48}
{'loss': 2.0287, 'grad_norm': 0.18834953010082245, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 2.027153491973877, 'eval_runtime': 8.6336, 'eval_samples_per_second': 114.668, 'eval_steps_per_second': 7.181, 'epoch': 0.53}
{'loss': 2.0215, 'grad_norm': 0.18438562750816345, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 2.0135128498077393, 'eval_runtime': 8.6055, 'eval_samples_per_second': 115.042, 'eval_steps_per_second': 7.205, 'epoch': 0.57}
{'loss': 1.9756, 'grad_norm': 0.10420844703912735, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 2.0056488513946533, 'eval_runtime': 8.5813, 'eval_samples_per_second': 115.367, 'eval_steps_per_second': 7.225, 'epoch': 0.61}
{'loss': 1.9983, 'grad_norm': 0.08986043930053711, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 1.9964466094970703, 'eval_runtime': 8.5749, 'eval_samples_per_second': 115.453, 'eval_steps_per_second': 7.23, 'epoch': 0.65}
{'loss': 1.9619, 'grad_norm': 0.14092634618282318, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 1.9908517599105835, 'eval_runtime': 8.5496, 'eval_samples_per_second': 115.796, 'eval_steps_per_second': 7.252, 'epoch': 0.69}
{'loss': 1.9864, 'grad_norm': 0.1429382562637329, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 1.9866020679473877, 'eval_runtime': 8.5354, 'eval_samples_per_second': 115.988, 'eval_steps_per_second': 7.264, 'epoch': 0.73}
{'loss': 1.9996, 'grad_norm': 0.11126267910003662, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 1.9835076332092285, 'eval_runtime': 8.533, 'eval_samples_per_second': 116.021, 'eval_steps_per_second': 7.266, 'epoch': 0.77}
{'loss': 1.9512, 'grad_norm': 0.09962660819292068, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 1.9806904792785645, 'eval_runtime': 8.5289, 'eval_samples_per_second': 116.077, 'eval_steps_per_second': 7.269, 'epoch': 0.81}
{'loss': 2.029, 'grad_norm': 0.11189938336610794, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 1.9783766269683838, 'eval_runtime': 8.5283, 'eval_samples_per_second': 116.084, 'eval_steps_per_second': 7.27, 'epoch': 0.85}
{'loss': 1.9654, 'grad_norm': 0.12295721471309662, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 1.9767297506332397, 'eval_runtime': 8.531, 'eval_samples_per_second': 116.047, 'eval_steps_per_second': 7.268, 'epoch': 0.89}
{'loss': 1.9398, 'grad_norm': 0.21492122113704681, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 1.9756122827529907, 'eval_runtime': 8.5229, 'eval_samples_per_second': 116.158, 'eval_steps_per_second': 7.275, 'epoch': 0.93}
{'loss': 2.0426, 'grad_norm': 0.13186946511268616, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 1.9746781587600708, 'eval_runtime': 8.5324, 'eval_samples_per_second': 116.029, 'eval_steps_per_second': 7.266, 'epoch': 0.97}
{'train_runtime': 401.9108, 'train_samples_per_second': 24.635, 'train_steps_per_second': 1.54, 'train_loss': 2.191039348842639, 'epoch': 1.0}
train_results:  {'eval_loss': [3.28275728225708, 2.6594083309173584, 2.45967435836792, 2.3535852432250977, 2.27553653717041, 2.2119970321655273, 2.1700124740600586, 2.135000467300415, 2.1015782356262207, 2.0768277645111084, 2.0569405555725098, 2.0412824153900146, 2.027153491973877, 2.0135128498077393, 2.0056488513946533, 1.9964466094970703, 1.9908517599105835, 1.9866020679473877, 1.9835076332092285, 1.9806904792785645, 1.9783766269683838, 1.9767297506332397, 1.9756122827529907, 1.9746781587600708], 'performance': [0.77, 0.71]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:48,  9.58s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:05,  1.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:42,  1.59it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:32<00:29,  1.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:39<00:18,  1.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:47<00:09,  1.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:49<00:01,  2.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:49<00:00,  2.04it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.71]
current iteration observed (possibly low-fid or predicted) performance:  0.8193420767784119
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8400000000000001
BO observations:  [0.6240859627723694, 0.7939552068710327, 0.6125386953353882, 0.7319226264953613, 0.7897405624389648, 0.800440788269043, 0.7532808184623718, 0.7956867814064026, 0.7947957515716553, 0.8657175302505493, 1.1255799531936646, 1.2274508476257324, 1.2276301383972168, 1.2279679775238037, 1.213857889175415, 1.2302820682525635, 1.227649211883545, 1.213118076324463, 0.7463162541389465, 1.2299857139587402, 1.2296924591064453, 1.2285890579223633, 1.2324609756469727, 1.2289037704467773, 1.2248222827911377, 1.2302109003067017, 1.2300595045089722, 1.236921787261963, 1.2368407249450684, 0.8193420767784119]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8422 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.8678392106217423
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.8517334072678726
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.8475446639437821
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.8289623792435211
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.7625940592186226
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1290, dtype=torch.float64), tensor(0.0693, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), 0, tensor(0.0331, dtype=torch.float64), tensor(0.7138, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.027701640421764824, 47.999999999999986, 0]
normalized proposed parameters for next round by BO: [tensor(1.2911e-16, dtype=torch.float64), tensor(1.4253e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1290, dtype=torch.float64), tensor(0.0693, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), tensor(4.2794e-17, dtype=torch.float64), tensor(0.0331, dtype=torch.float64), tensor(0.7138, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.2770, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/gsm8k/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 9, 1, 1, 0, 0, 1, 14, 0.03669660801939172, 9, 1]
Checking history sample input_X_between_0_1:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 0.28125, 1.0, 1.0, 0.0, 0.0, 1.0, 0.109375, 0.3669660801939172, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.7
Checking history sample input_X:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 4, 1, 1, 0, 1, 0, 45, 0.02996529708553877, 41, 0]
Checking history sample input_X_between_0_1:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 0.125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3515625, 0.29965297085538767, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 5, 0, 1, 1, 0, 0, 115, 0.08044623754842878, 26, 0]
Checking history sample input_X_between_0_1:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 0.15625, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8984375, 0.8044623754842878, 0.5416666666666666, 0.0]
Checking history sample performance at 625 steps:  0.73
Checking history sample input_X:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 30, 0, 1, 0, 1, 1, 121, 0.019265932894845208, 29, 1]
Checking history sample input_X_between_0_1:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 0.9375, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9453125, 0.19265932894845206, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 19, 0, 0, 1, 1, 1, 108, 0.09306302255978231, 35, 1]
Checking history sample input_X_between_0_1:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 0.59375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.84375, 0.930630225597823, 0.7291666666666666, 1.0]
Checking history sample performance at 625 steps:  0.62
Checking history sample input_X:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 14, 0, 0, 0, 1, 1, 44, 0.004820496247456452, 22, 1]
Checking history sample input_X_between_0_1:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 0.4375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.34375, 0.04820496247456452, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.63
Checking history sample input_X:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 24, 0, 1, 1, 0, 1, 79, 0.03164316476579598, 9, 1]
Checking history sample input_X_between_0_1:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6171875, 0.3164316476579597, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 29, 0, 1, 0, 1, 1, 23, 0.015858984905097274, 40, 0]
Checking history sample input_X_between_0_1:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 0.90625, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1796875, 0.15858984905097273, 0.8333333333333334, 0.0]
Checking history sample performance at 625 steps:  0.6
Checking history sample input_X:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 27, 0, 0, 1, 0, 0, 58, 0.02356583389888708, 4, 0]
Checking history sample input_X_between_0_1:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 0.84375, 0.0, 0.0, 1.0, 0.0, 0.0, 0.453125, 0.23565833898887079, 0.08333333333333333, 0.0]
Checking history sample performance at 625 steps:  0.65
Checking history sample input_X:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 18, 0, 1, 1, 0, 1, 102, 0.07944916714916157, 39, 0]
Checking history sample input_X_between_0_1:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 0.5625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.796875, 0.7944916714916157, 0.8125, 0.0]
Checking history sample performance at 625 steps:  0.62
Checking history sample input_X:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 16, 1, 0, 0, 0, 0, 57, 0.051177108415818844, 16, 1]
Checking history sample input_X_between_0_1:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4453125, 0.5117710841581884, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.71
Checking history sample input_X:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 21, 0, 1, 0, 0, 1, 80, 0.08438899074092726, 11, 1]
Checking history sample input_X_between_0_1:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 0.65625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.625, 0.8438899074092726, 0.22916666666666666, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 19, 0, 1, 1, 1, 0, 106, 0.021750066051679486, 16, 1]
Checking history sample input_X_between_0_1:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 0.59375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.828125, 0.21750066051679484, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 6, 1, 0, 0, 0, 1, 109, 0.0976089152670931, 17, 0]
Checking history sample input_X_between_0_1:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 0.1875, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8515625, 0.976089152670931, 0.3541666666666667, 0.0]
Checking history sample performance at 625 steps:  0.73
Checking history sample input_X:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 5, 0, 1, 1, 0, 1, 6, 0.045424701431822194, 31, 1]
Checking history sample input_X_between_0_1:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 0.15625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.046875, 0.4542470143182219, 0.6458333333333334, 1.0]
Checking history sample performance at 625 steps:  0.67
Checking history sample input_X:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 18, 1, 0, 0, 0, 0, 27, 0.050875722794158945, 27, 1]
Checking history sample input_X_between_0_1:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 0.5625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2109375, 0.5087572279415894, 0.5625, 1.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 25, 0, 0, 0, 1, 1, 5, 0.0932969995849881, 3, 0]
Checking history sample input_X_between_0_1:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 0.78125, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0390625, 0.932969995849881, 0.0625, 0.0]
Checking history sample performance at 625 steps:  0.65
Checking history sample input_X:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 14, 0, 0, 0, 1, 0, 14, 0.06234381073507801, 24, 1]
Checking history sample input_X_between_0_1:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 0.4375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.109375, 0.6234381073507801, 0.5, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 19, 1, 1, 0, 1, 0, 54, 0.0659080396027719, 16, 1]
Checking history sample input_X_between_0_1:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 0.59375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.421875, 0.659080396027719, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.67
Checking history sample input_X:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 30, 1, 1, 0, 1, 0, 113, 0.08743251828499332, 21, 0]
Checking history sample input_X_between_0_1:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 0.9375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8828125, 0.8743251828499332, 0.4375, 0.0]
Checking history sample performance at 625 steps:  0.63
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6089 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.8466325775872318
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.8466381325444159
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.8466308408632989
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.8466413620825513
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.8466325776282388
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.0355, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.1478, dtype=torch.float64), tensor(0.1111, dtype=torch.float64), tensor(0.2353, dtype=torch.float64), tensor(0.1017, dtype=torch.float64), tensor(0.2717, dtype=torch.float64), tensor(0.0678, dtype=torch.float64), 14, 1, 0, 1, 0, 0, 26, 0.061997739398963594, 19.612889282736287, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(4.0558e-19, dtype=torch.float64), tensor(0.0355, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.1478, dtype=torch.float64), tensor(0.1111, dtype=torch.float64), tensor(0.2353, dtype=torch.float64), tensor(0.1017, dtype=torch.float64), tensor(0.2717, dtype=torch.float64), tensor(0.0678, dtype=torch.float64), tensor(0.4378, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2061, dtype=torch.float64), tensor(0.6200, dtype=torch.float64), tensor(0.4086, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.036
  rowan_hellaswag: 0.029
  sciq: 0.148
  triviaqa: 0.111
  truthfulqa_gen: 0.235
  wikitext: 0.102
  mmlu: 0.272
  arc_challenge: 0.068

LoRA Parameters:
  lora_r: (26,)
  lora_dropout: (0.061997739398963594,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (19.612889282736287,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  26
lora dropout:  0.061997739398963594
lora alpha:  19.612889282736287
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 9,691,136 || all params: 8,039,952,384 || trainable%: 0.1205
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:08,  3.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:16,  1.19it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:44,  1.85it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.43it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:26,  2.51it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:20<00:26,  2.24it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:22,  2.26it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:17,  2.45it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:12,  2.82it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:05,  3.27it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  3.41it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.61it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.76it/s]
Evaluation performance at step 25: 0.76
{'loss': 3.604, 'grad_norm': 0.6634747385978699, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 2.4941768646240234, 'eval_runtime': 8.1651, 'eval_samples_per_second': 122.349, 'eval_steps_per_second': 7.716, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:50,  4.15s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:24,  1.08it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:49,  1.69it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:15<00:39,  1.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:18<00:31,  2.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:21<00:24,  2.37it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:19,  2.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:27<00:16,  2.55it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:11,  2.92it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:08,  3.29it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:35<00:07,  2.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:37<00:03,  2.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.53it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.0014, 'grad_norm': 0.4707750082015991, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.7056729793548584, 'eval_runtime': 8.2163, 'eval_samples_per_second': 121.587, 'eval_steps_per_second': 7.668, 'epoch': 0.08}
{'loss': 1.7198, 'grad_norm': 0.42927315831184387, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5579813718795776, 'eval_runtime': 8.2467, 'eval_samples_per_second': 121.14, 'eval_steps_per_second': 7.639, 'epoch': 0.12}
{'loss': 1.5198, 'grad_norm': 0.265818327665329, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4891825914382935, 'eval_runtime': 8.2912, 'eval_samples_per_second': 120.489, 'eval_steps_per_second': 7.598, 'epoch': 0.16}
{'loss': 1.464, 'grad_norm': 0.37933051586151123, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4389275312423706, 'eval_runtime': 8.2947, 'eval_samples_per_second': 120.438, 'eval_steps_per_second': 7.595, 'epoch': 0.2}
{'loss': 1.412, 'grad_norm': 0.32778415083885193, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3860708475112915, 'eval_runtime': 8.3075, 'eval_samples_per_second': 120.253, 'eval_steps_per_second': 7.584, 'epoch': 0.24}
{'loss': 1.3976, 'grad_norm': 0.35681581497192383, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3297513723373413, 'eval_runtime': 8.3164, 'eval_samples_per_second': 120.125, 'eval_steps_per_second': 7.575, 'epoch': 0.28}
{'loss': 1.3318, 'grad_norm': 0.3004923462867737, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3056399822235107, 'eval_runtime': 8.3146, 'eval_samples_per_second': 120.15, 'eval_steps_per_second': 7.577, 'epoch': 0.32}
{'loss': 1.3787, 'grad_norm': 0.31801894307136536, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.281775712966919, 'eval_runtime': 8.285, 'eval_samples_per_second': 120.579, 'eval_steps_per_second': 7.604, 'epoch': 0.36}
{'loss': 1.3451, 'grad_norm': 0.39551395177841187, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2567558288574219, 'eval_runtime': 8.2883, 'eval_samples_per_second': 120.531, 'eval_steps_per_second': 7.601, 'epoch': 0.4}
{'loss': 1.2929, 'grad_norm': 0.3458133637905121, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2297561168670654, 'eval_runtime': 8.3041, 'eval_samples_per_second': 120.302, 'eval_steps_per_second': 7.587, 'epoch': 0.44}
{'loss': 1.2651, 'grad_norm': 0.2951126992702484, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2109216451644897, 'eval_runtime': 8.2759, 'eval_samples_per_second': 120.711, 'eval_steps_per_second': 7.612, 'epoch': 0.48}
{'loss': 1.1908, 'grad_norm': 0.2613869607448578, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2024658918380737, 'eval_runtime': 8.3022, 'eval_samples_per_second': 120.329, 'eval_steps_per_second': 7.588, 'epoch': 0.52}
{'loss': 1.2314, 'grad_norm': 0.3618292808532715, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1974701881408691, 'eval_runtime': 8.2736, 'eval_samples_per_second': 120.745, 'eval_steps_per_second': 7.615, 'epoch': 0.56}
{'loss': 1.2088, 'grad_norm': 0.34091830253601074, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1898694038391113, 'eval_runtime': 8.2731, 'eval_samples_per_second': 120.752, 'eval_steps_per_second': 7.615, 'epoch': 0.6}
{'loss': 1.21, 'grad_norm': 0.349848210811615, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1827504634857178, 'eval_runtime': 8.2884, 'eval_samples_per_second': 120.53, 'eval_steps_per_second': 7.601, 'epoch': 0.64}
{'loss': 1.2271, 'grad_norm': 0.3526182174682617, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1783565282821655, 'eval_runtime': 8.2951, 'eval_samples_per_second': 120.432, 'eval_steps_per_second': 7.595, 'epoch': 0.68}
{'loss': 1.2333, 'grad_norm': 0.3208702802658081, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.174206256866455, 'eval_runtime': 8.2677, 'eval_samples_per_second': 120.832, 'eval_steps_per_second': 7.62, 'epoch': 0.72}
{'loss': 1.2386, 'grad_norm': 0.3228612244129181, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.169306993484497, 'eval_runtime': 8.2743, 'eval_samples_per_second': 120.735, 'eval_steps_per_second': 7.614, 'epoch': 0.76}
{'loss': 1.2475, 'grad_norm': 0.36008986830711365, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1655582189559937, 'eval_runtime': 8.2773, 'eval_samples_per_second': 120.692, 'eval_steps_per_second': 7.611, 'epoch': 0.8}
{'loss': 1.2495, 'grad_norm': 0.3915405869483948, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1617132425308228, 'eval_runtime': 8.2696, 'eval_samples_per_second': 120.805, 'eval_steps_per_second': 7.618, 'epoch': 0.84}
{'loss': 1.2348, 'grad_norm': 0.38055241107940674, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1580381393432617, 'eval_runtime': 8.2377, 'eval_samples_per_second': 121.271, 'eval_steps_per_second': 7.648, 'epoch': 0.88}
{'loss': 1.2618, 'grad_norm': 0.36261066794395447, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1558897495269775, 'eval_runtime': 8.2401, 'eval_samples_per_second': 121.237, 'eval_steps_per_second': 7.646, 'epoch': 0.92}
{'loss': 1.1781, 'grad_norm': 0.3183661103248596, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1547712087631226, 'eval_runtime': 8.2325, 'eval_samples_per_second': 121.349, 'eval_steps_per_second': 7.653, 'epoch': 0.96}
{'loss': 1.1849, 'grad_norm': 0.38888660073280334, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1539183855056763, 'eval_runtime': 8.2233, 'eval_samples_per_second': 121.484, 'eval_steps_per_second': 7.661, 'epoch': 1.0}
{'train_runtime': 449.8709, 'train_samples_per_second': 22.22, 'train_steps_per_second': 1.389, 'train_loss': 1.4251531860351563, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4941768646240234, 1.7056729793548584, 1.5579813718795776, 1.4891825914382935, 1.4389275312423706, 1.3860708475112915, 1.3297513723373413, 1.3056399822235107, 1.281775712966919, 1.2567558288574219, 1.2297561168670654, 1.2109216451644897, 1.2024658918380737, 1.1974701881408691, 1.1898694038391113, 1.1827504634857178, 1.1783565282821655, 1.174206256866455, 1.169306993484497, 1.1655582189559937, 1.1617132425308228, 1.1580381393432617, 1.1558897495269775, 1.1547712087631226, 1.1539183855056763], 'performance': [0.76, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<16:21,  9.92s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:17,  1.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:25,  2.01it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:38<00:17,  2.06it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:46<00:09,  2.06it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:48<00:01,  2.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.08it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.62408047914505
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.7665
BO observations:  [0.62408047914505]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0536 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 0.8123088023843414
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 0.8123088023861695
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 0.8123088023859886
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 0.8123055563391492
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 0.812309045847471
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4114, dtype=torch.float64), tensor(0.0303, dtype=torch.float64), 0, tensor(0.5083, dtype=torch.float64), 0, 0, tensor(0.0499, dtype=torch.float64), 0, 1, 1, 0, 0, 0, 0, 127, 0.05280215578374824, 44.07627937475198, 1]
normalized proposed parameters for next round by BO: [tensor(5.0892e-18, dtype=torch.float64), tensor(0.4114, dtype=torch.float64), tensor(0.0303, dtype=torch.float64), tensor(1.1121e-18, dtype=torch.float64), tensor(0.5083, dtype=torch.float64), tensor(3.6509e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0499, dtype=torch.float64), tensor(9.6922e-19, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9921, dtype=torch.float64), tensor(0.5280, dtype=torch.float64), tensor(0.9183, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.411
  rowan_hellaswag: 0.03
  sciq: 0
  triviaqa: 0.508
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.05
  arc_challenge: 0

LoRA Parameters:
  lora_r: (127,)
  lora_dropout: (0.05280215578374824,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (44.07627937475198,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  127
lora dropout:  0.05280215578374824
lora alpha:  44.07627937475198
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,040,384 || all params: 8,031,301,632 || trainable%: 0.0130
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:35,  2.78s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:11,  1.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  1.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.81it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.91it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.32it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.24it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.43it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.98it/s]
Evaluation performance at step 25: 0.78
{'loss': 3.5526, 'grad_norm': 0.19834332168102264, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 3.3904519081115723, 'eval_runtime': 8.6109, 'eval_samples_per_second': 116.016, 'eval_steps_per_second': 7.316, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:35,  2.78s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:46,  1.96it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:33,  2.51it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:25,  2.96it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.06it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:16,  3.01it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:15,  2.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.20it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:08,  3.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:05,  3.28it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:29<00:03,  3.32it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:31<00:00,  3.54it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:31<00:00,  3.15it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.258, 'grad_norm': 0.18633374571800232, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 3.018760919570923, 'eval_runtime': 8.6615, 'eval_samples_per_second': 115.338, 'eval_steps_per_second': 7.274, 'epoch': 0.08}
{'loss': 2.836, 'grad_norm': 0.8818798065185547, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.7697296142578125, 'eval_runtime': 8.6552, 'eval_samples_per_second': 115.422, 'eval_steps_per_second': 7.279, 'epoch': 0.12}
{'loss': 2.7848, 'grad_norm': 0.23309668898582458, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.6130619049072266, 'eval_runtime': 8.666, 'eval_samples_per_second': 115.278, 'eval_steps_per_second': 7.27, 'epoch': 0.16}
{'loss': 2.6183, 'grad_norm': 0.3615986704826355, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.5073647499084473, 'eval_runtime': 8.6974, 'eval_samples_per_second': 114.861, 'eval_steps_per_second': 7.244, 'epoch': 0.2}
{'loss': 2.454, 'grad_norm': 0.23708918690681458, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.4475021362304688, 'eval_runtime': 8.7321, 'eval_samples_per_second': 114.406, 'eval_steps_per_second': 7.215, 'epoch': 0.24}
{'loss': 2.5044, 'grad_norm': 0.5207960605621338, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.415351390838623, 'eval_runtime': 8.6971, 'eval_samples_per_second': 114.865, 'eval_steps_per_second': 7.244, 'epoch': 0.28}
{'loss': 2.3593, 'grad_norm': 0.0966753214597702, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.392686128616333, 'eval_runtime': 8.7113, 'eval_samples_per_second': 114.678, 'eval_steps_per_second': 7.232, 'epoch': 0.32}
{'loss': 2.4251, 'grad_norm': 0.5429110527038574, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.37005615234375, 'eval_runtime': 8.7222, 'eval_samples_per_second': 114.536, 'eval_steps_per_second': 7.223, 'epoch': 0.36}
{'loss': 2.3521, 'grad_norm': 0.3040015995502472, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.3518118858337402, 'eval_runtime': 8.7113, 'eval_samples_per_second': 114.678, 'eval_steps_per_second': 7.232, 'epoch': 0.4}
{'loss': 2.4131, 'grad_norm': 0.3161882162094116, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.3297219276428223, 'eval_runtime': 8.7182, 'eval_samples_per_second': 114.588, 'eval_steps_per_second': 7.226, 'epoch': 0.44}
{'loss': 2.3091, 'grad_norm': 0.12631438672542572, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.308835506439209, 'eval_runtime': 8.7146, 'eval_samples_per_second': 114.635, 'eval_steps_per_second': 7.229, 'epoch': 0.48}
{'loss': 2.351, 'grad_norm': 0.4285455346107483, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.2911105155944824, 'eval_runtime': 8.697, 'eval_samples_per_second': 114.868, 'eval_steps_per_second': 7.244, 'epoch': 0.52}
{'loss': 2.3301, 'grad_norm': 0.184007465839386, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.2731692790985107, 'eval_runtime': 8.6449, 'eval_samples_per_second': 115.559, 'eval_steps_per_second': 7.287, 'epoch': 0.56}
{'loss': 2.2376, 'grad_norm': 0.2761467695236206, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.2623844146728516, 'eval_runtime': 8.6466, 'eval_samples_per_second': 115.537, 'eval_steps_per_second': 7.286, 'epoch': 0.6}
{'loss': 2.2526, 'grad_norm': 0.615394651889801, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.2525293827056885, 'eval_runtime': 8.6622, 'eval_samples_per_second': 115.329, 'eval_steps_per_second': 7.273, 'epoch': 0.64}
{'loss': 2.2886, 'grad_norm': 0.2122327834367752, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2417941093444824, 'eval_runtime': 8.6514, 'eval_samples_per_second': 115.473, 'eval_steps_per_second': 7.282, 'epoch': 0.68}
{'loss': 2.2754, 'grad_norm': 0.2317429631948471, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.2306060791015625, 'eval_runtime': 8.6314, 'eval_samples_per_second': 115.74, 'eval_steps_per_second': 7.299, 'epoch': 0.72}
{'loss': 2.2267, 'grad_norm': 0.36603203415870667, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.2213921546936035, 'eval_runtime': 8.638, 'eval_samples_per_second': 115.651, 'eval_steps_per_second': 7.293, 'epoch': 0.76}
{'loss': 2.1978, 'grad_norm': 0.17387044429779053, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.213567018508911, 'eval_runtime': 8.6371, 'eval_samples_per_second': 115.663, 'eval_steps_per_second': 7.294, 'epoch': 0.8}
{'loss': 2.2775, 'grad_norm': 0.468097448348999, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.2090988159179688, 'eval_runtime': 8.6438, 'eval_samples_per_second': 115.574, 'eval_steps_per_second': 7.288, 'epoch': 0.84}
{'loss': 2.2409, 'grad_norm': 0.2518884241580963, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.201979637145996, 'eval_runtime': 8.6377, 'eval_samples_per_second': 115.656, 'eval_steps_per_second': 7.294, 'epoch': 0.88}
{'loss': 2.2064, 'grad_norm': 0.14311501383781433, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1989192962646484, 'eval_runtime': 8.6426, 'eval_samples_per_second': 115.59, 'eval_steps_per_second': 7.289, 'epoch': 0.92}
{'loss': 2.2254, 'grad_norm': 0.10921286791563034, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.1966893672943115, 'eval_runtime': 8.6674, 'eval_samples_per_second': 115.26, 'eval_steps_per_second': 7.269, 'epoch': 0.96}
{'loss': 2.1694, 'grad_norm': 0.21108633279800415, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.1960315704345703, 'eval_runtime': 8.6993, 'eval_samples_per_second': 114.836, 'eval_steps_per_second': 7.242, 'epoch': 1.0}
{'train_runtime': 502.9402, 'train_samples_per_second': 19.879, 'train_steps_per_second': 1.243, 'train_loss': 2.4458497924804687, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3904519081115723, 3.018760919570923, 2.7697296142578125, 2.6130619049072266, 2.5073647499084473, 2.4475021362304688, 2.415351390838623, 2.392686128616333, 2.37005615234375, 2.3518118858337402, 2.3297219276428223, 2.308835506439209, 2.2911105155944824, 2.2731692790985107, 2.2623844146728516, 2.2525293827056885, 2.2417941093444824, 2.2306060791015625, 2.2213921546936035, 2.213567018508911, 2.2090988159179688, 2.201979637145996, 2.1989192962646484, 2.1966893672943115, 2.1960315704345703], 'performance': [0.78, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:52,  9.63s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:30<00:25,  2.03it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:14,  2.49it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:06,  2.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:00,  3.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.46it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  0.7941675782203674
current iteration best possible performance (full train run):  0.777
max performance so far:  0.777
BO observations:  [0.62408047914505, 0.7941675782203674]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9483 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 0.8048978304861301
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 0.8043790349810407
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 0.8048979006561673
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 0.804887125355582
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.8049177674584241
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3992, dtype=torch.float64), 0, 0, tensor(0.0333, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), 0, tensor(0.1758, dtype=torch.float64), tensor(0.1682, dtype=torch.float64), tensor(0.1686, dtype=torch.float64), 32, 1, 0, 0, 0, 0, 7, 0.038196198141624095, 1.709650524729653, 1]
normalized proposed parameters for next round by BO: [tensor(0.3992, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0333, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1758, dtype=torch.float64), tensor(0.1682, dtype=torch.float64), tensor(0.1686, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0558, dtype=torch.float64), tensor(0.3820, dtype=torch.float64), tensor(0.0356, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.399
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.033
  triviaqa: 0.055
  truthfulqa_gen: 0
  wikitext: 0.176
  mmlu: 0.168
  arc_challenge: 0.169

LoRA Parameters:
  lora_r: (7,)
  lora_dropout: (0.038196198141624095,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (1.709650524729653,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  7
lora dropout:  0.038196198141624095
lora alpha:  1.709650524729653
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,835,008 || all params: 8,032,096,256 || trainable%: 0.0228
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:07,  5.53s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:34,  1.04s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:13<00:51,  1.61it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:15<00:34,  2.14it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:18<00:28,  2.31it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:21<00:24,  2.42it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:24<00:19,  2.55it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:16,  2.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:11,  2.99it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:31<00:08,  3.09it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:06,  3.16it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  3.32it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.53it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.65it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.3217, 'grad_norm': 0.4934324622154236, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 4.080836296081543, 'eval_runtime': 7.4999, 'eval_samples_per_second': 133.201, 'eval_steps_per_second': 8.4, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:04,  3.68s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:55,  1.65it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:36,  2.27it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:28,  2.64it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:23,  2.84it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:26,  2.24it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:20,  2.46it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:17,  2.51it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:12,  2.75it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:09,  2.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:06,  2.93it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:04,  2.65it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:01,  2.97it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.67it/s]
Evaluation performance at step 50: 0.8
{'loss': 3.4824, 'grad_norm': 0.5719628930091858, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.8}
{'eval_loss': 2.854942798614502, 'eval_runtime': 7.5088, 'eval_samples_per_second': 133.045, 'eval_steps_per_second': 8.39, 'epoch': 0.08}
{'loss': 2.4738, 'grad_norm': 0.19154417514801025, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.191761016845703, 'eval_runtime': 7.5059, 'eval_samples_per_second': 133.095, 'eval_steps_per_second': 8.393, 'epoch': 0.12}
{'loss': 1.9985, 'grad_norm': 0.2603698670864105, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8072327375411987, 'eval_runtime': 7.5338, 'eval_samples_per_second': 132.603, 'eval_steps_per_second': 8.362, 'epoch': 0.16}
{'loss': 1.7319, 'grad_norm': 0.25960293412208557, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6865202188491821, 'eval_runtime': 7.5394, 'eval_samples_per_second': 132.503, 'eval_steps_per_second': 8.356, 'epoch': 0.2}
{'loss': 1.7092, 'grad_norm': 0.13939513266086578, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6602518558502197, 'eval_runtime': 7.5486, 'eval_samples_per_second': 132.343, 'eval_steps_per_second': 8.346, 'epoch': 0.24}
{'loss': 1.6281, 'grad_norm': 0.17881450057029724, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6460617780685425, 'eval_runtime': 7.5554, 'eval_samples_per_second': 132.223, 'eval_steps_per_second': 8.338, 'epoch': 0.28}
{'loss': 1.7089, 'grad_norm': 0.18748347461223602, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.634756088256836, 'eval_runtime': 7.5641, 'eval_samples_per_second': 132.07, 'eval_steps_per_second': 8.329, 'epoch': 0.32}
{'loss': 1.6714, 'grad_norm': 0.10895083844661713, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6262348890304565, 'eval_runtime': 7.5589, 'eval_samples_per_second': 132.162, 'eval_steps_per_second': 8.335, 'epoch': 0.36}
{'loss': 1.5838, 'grad_norm': 0.14268416166305542, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6183675527572632, 'eval_runtime': 7.5807, 'eval_samples_per_second': 131.781, 'eval_steps_per_second': 8.311, 'epoch': 0.4}
{'loss': 1.6477, 'grad_norm': 0.1720820516347885, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6120065450668335, 'eval_runtime': 7.5757, 'eval_samples_per_second': 131.869, 'eval_steps_per_second': 8.316, 'epoch': 0.44}
{'loss': 1.5862, 'grad_norm': 0.24981838464736938, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.603871464729309, 'eval_runtime': 7.5992, 'eval_samples_per_second': 131.461, 'eval_steps_per_second': 8.29, 'epoch': 0.48}
{'loss': 1.5903, 'grad_norm': 0.17058907449245453, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5987799167633057, 'eval_runtime': 7.5744, 'eval_samples_per_second': 131.892, 'eval_steps_per_second': 8.318, 'epoch': 0.52}
{'loss': 1.5482, 'grad_norm': 0.14209407567977905, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5941987037658691, 'eval_runtime': 7.5578, 'eval_samples_per_second': 132.181, 'eval_steps_per_second': 8.336, 'epoch': 0.56}
{'loss': 1.592, 'grad_norm': 0.15820826590061188, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.589819073677063, 'eval_runtime': 7.553, 'eval_samples_per_second': 132.266, 'eval_steps_per_second': 8.341, 'epoch': 0.6}
{'loss': 1.6365, 'grad_norm': 0.2073628157377243, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5853044986724854, 'eval_runtime': 7.5573, 'eval_samples_per_second': 132.19, 'eval_steps_per_second': 8.336, 'epoch': 0.64}
{'loss': 1.5547, 'grad_norm': 0.119645856320858, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5827550888061523, 'eval_runtime': 7.5367, 'eval_samples_per_second': 132.551, 'eval_steps_per_second': 8.359, 'epoch': 0.68}
{'loss': 1.5375, 'grad_norm': 0.17165248095989227, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5791436433792114, 'eval_runtime': 7.5253, 'eval_samples_per_second': 132.753, 'eval_steps_per_second': 8.372, 'epoch': 0.72}
{'loss': 1.521, 'grad_norm': 0.20820704102516174, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5765037536621094, 'eval_runtime': 7.5201, 'eval_samples_per_second': 132.844, 'eval_steps_per_second': 8.378, 'epoch': 0.76}
{'loss': 1.5485, 'grad_norm': 0.168842613697052, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5746302604675293, 'eval_runtime': 7.5285, 'eval_samples_per_second': 132.695, 'eval_steps_per_second': 8.368, 'epoch': 0.8}
{'loss': 1.5813, 'grad_norm': 0.149712935090065, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5723735094070435, 'eval_runtime': 7.5474, 'eval_samples_per_second': 132.363, 'eval_steps_per_second': 8.347, 'epoch': 0.84}
{'loss': 1.5092, 'grad_norm': 0.25833964347839355, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5705193281173706, 'eval_runtime': 7.5422, 'eval_samples_per_second': 132.454, 'eval_steps_per_second': 8.353, 'epoch': 0.88}
{'loss': 1.5599, 'grad_norm': 0.2156601995229721, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5690999031066895, 'eval_runtime': 7.5325, 'eval_samples_per_second': 132.625, 'eval_steps_per_second': 8.364, 'epoch': 0.92}
{'loss': 1.5899, 'grad_norm': 0.20686788856983185, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5685081481933594, 'eval_runtime': 7.5381, 'eval_samples_per_second': 132.527, 'eval_steps_per_second': 8.358, 'epoch': 0.96}
{'loss': 1.5946, 'grad_norm': 0.15233300626277924, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5682564973831177, 'eval_runtime': 7.523, 'eval_samples_per_second': 132.793, 'eval_steps_per_second': 8.374, 'epoch': 1.0}
{'train_runtime': 464.5138, 'train_samples_per_second': 21.521, 'train_steps_per_second': 1.345, 'train_loss': 1.8362936584472656, 'epoch': 1.0}
train_results:  {'eval_loss': [4.080836296081543, 2.854942798614502, 2.191761016845703, 1.8072327375411987, 1.6865202188491821, 1.6602518558502197, 1.6460617780685425, 1.634756088256836, 1.6262348890304565, 1.6183675527572632, 1.6120065450668335, 1.603871464729309, 1.5987799167633057, 1.5941987037658691, 1.589819073677063, 1.5853044986724854, 1.5827550888061523, 1.5791436433792114, 1.5765037536621094, 1.5746302604675293, 1.5723735094070435, 1.5705193281173706, 1.5690999031066895, 1.5685081481933594, 1.5682564973831177], 'performance': [0.76, 0.8]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<16:22,  9.92s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:18,  1.06it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:27<00:47,  1.42it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:33<00:29,  1.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:41<00:18,  1.90it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:48<00:09,  1.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:50<00:01,  2.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:50<00:00,  1.97it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.8]
current iteration observed (possibly low-fid or predicted) performance:  0.6125386953353882
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.7875000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.0516 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.8039153964605906
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 0.803915396410941
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 0.803915396230665
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 0.803915396413172
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 0.8039153952653932
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0203, dtype=torch.float64), 0, tensor(0.2014, dtype=torch.float64), tensor(0.3771, dtype=torch.float64), 0, 0, 0, tensor(0.4013, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 128, 0.01979076227355135, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(4.2921e-18, dtype=torch.float64), tensor(0.0203, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2014, dtype=torch.float64), tensor(0.3771, dtype=torch.float64), tensor(9.0207e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4013, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1979, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.02
  rowan_hellaswag: 0
  sciq: 0.201
  triviaqa: 0.377
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.401

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.01979076227355135,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  128
lora dropout:  0.01979076227355135
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,048,576 || all params: 8,031,309,824 || trainable%: 0.0131
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:00,  3.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:15,  1.21it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:43,  1.91it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.77it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.88it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:14,  2.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.13it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.65it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.29it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.82it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.6912, 'grad_norm': 0.3215482532978058, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 4.485069274902344, 'eval_runtime': 6.4681, 'eval_samples_per_second': 154.45, 'eval_steps_per_second': 9.74, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:34,  2.78s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:46,  1.97it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.54it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:25,  3.00it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:19,  2.95it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:16,  3.03it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.25it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:07,  3.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:07,  2.69it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.88it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.02it/s]
Evaluation performance at step 50: 0.78
{'loss': 4.2673, 'grad_norm': 0.8248859643936157, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 3.9056448936462402, 'eval_runtime': 6.1998, 'eval_samples_per_second': 161.135, 'eval_steps_per_second': 10.162, 'epoch': 0.08}
{'loss': 3.6667, 'grad_norm': 0.18768054246902466, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.5236916542053223, 'eval_runtime': 6.2386, 'eval_samples_per_second': 160.132, 'eval_steps_per_second': 10.098, 'epoch': 0.12}
{'loss': 3.426, 'grad_norm': 0.36052587628364563, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.376492738723755, 'eval_runtime': 6.2052, 'eval_samples_per_second': 160.994, 'eval_steps_per_second': 10.153, 'epoch': 0.16}
{'loss': 3.3255, 'grad_norm': 0.5264178514480591, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.2362420558929443, 'eval_runtime': 6.2167, 'eval_samples_per_second': 160.696, 'eval_steps_per_second': 10.134, 'epoch': 0.2}
{'loss': 3.1963, 'grad_norm': 0.49624454975128174, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.134958505630493, 'eval_runtime': 6.2338, 'eval_samples_per_second': 160.254, 'eval_steps_per_second': 10.106, 'epoch': 0.24}
{'loss': 3.1355, 'grad_norm': 0.5700863599777222, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.037057638168335, 'eval_runtime': 6.2402, 'eval_samples_per_second': 160.092, 'eval_steps_per_second': 10.096, 'epoch': 0.28}
{'loss': 3.0031, 'grad_norm': 0.5919044613838196, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.9979634284973145, 'eval_runtime': 6.2458, 'eval_samples_per_second': 159.948, 'eval_steps_per_second': 10.087, 'epoch': 0.32}
{'loss': 2.9849, 'grad_norm': 0.26803869009017944, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.95279598236084, 'eval_runtime': 6.2503, 'eval_samples_per_second': 159.831, 'eval_steps_per_second': 10.079, 'epoch': 0.36}
{'loss': 2.9164, 'grad_norm': 0.7275590896606445, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.9184458255767822, 'eval_runtime': 6.2689, 'eval_samples_per_second': 159.359, 'eval_steps_per_second': 10.05, 'epoch': 0.4}
{'loss': 2.8938, 'grad_norm': 0.23334801197052002, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.8860905170440674, 'eval_runtime': 6.2659, 'eval_samples_per_second': 159.435, 'eval_steps_per_second': 10.054, 'epoch': 0.44}
{'loss': 2.8802, 'grad_norm': 0.5295703411102295, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.8591790199279785, 'eval_runtime': 6.2774, 'eval_samples_per_second': 159.143, 'eval_steps_per_second': 10.036, 'epoch': 0.48}
{'loss': 2.8432, 'grad_norm': 0.7236059904098511, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.8329124450683594, 'eval_runtime': 6.2825, 'eval_samples_per_second': 159.013, 'eval_steps_per_second': 10.028, 'epoch': 0.52}
{'loss': 2.7797, 'grad_norm': 0.5174214839935303, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.806734561920166, 'eval_runtime': 6.2982, 'eval_samples_per_second': 158.616, 'eval_steps_per_second': 10.003, 'epoch': 0.56}
{'loss': 2.7651, 'grad_norm': 0.6856720447540283, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.7785844802856445, 'eval_runtime': 6.301, 'eval_samples_per_second': 158.546, 'eval_steps_per_second': 9.998, 'epoch': 0.6}
{'loss': 2.7488, 'grad_norm': 0.6569664478302002, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.7482452392578125, 'eval_runtime': 6.2877, 'eval_samples_per_second': 158.882, 'eval_steps_per_second': 10.02, 'epoch': 0.64}
{'loss': 2.7329, 'grad_norm': 1.0440514087677002, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.7209577560424805, 'eval_runtime': 6.3093, 'eval_samples_per_second': 158.337, 'eval_steps_per_second': 9.985, 'epoch': 0.68}
{'loss': 2.7186, 'grad_norm': 1.189247727394104, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.6949634552001953, 'eval_runtime': 6.3361, 'eval_samples_per_second': 157.667, 'eval_steps_per_second': 9.943, 'epoch': 0.72}
{'loss': 2.7385, 'grad_norm': 0.39133378863334656, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.6750504970550537, 'eval_runtime': 6.3091, 'eval_samples_per_second': 158.344, 'eval_steps_per_second': 9.986, 'epoch': 0.76}
{'loss': 2.6733, 'grad_norm': 0.43115368485450745, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.6613097190856934, 'eval_runtime': 6.3256, 'eval_samples_per_second': 157.931, 'eval_steps_per_second': 9.96, 'epoch': 0.8}
{'loss': 2.6561, 'grad_norm': 0.2939327359199524, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.649909734725952, 'eval_runtime': 6.3065, 'eval_samples_per_second': 158.408, 'eval_steps_per_second': 9.99, 'epoch': 0.84}
{'loss': 2.6415, 'grad_norm': 0.5584763884544373, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.6421430110931396, 'eval_runtime': 6.298, 'eval_samples_per_second': 158.622, 'eval_steps_per_second': 10.003, 'epoch': 0.88}
{'loss': 2.5984, 'grad_norm': 0.342489093542099, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.6374032497406006, 'eval_runtime': 6.2972, 'eval_samples_per_second': 158.642, 'eval_steps_per_second': 10.004, 'epoch': 0.92}
{'loss': 2.623, 'grad_norm': 0.3429604172706604, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.635103940963745, 'eval_runtime': 6.2934, 'eval_samples_per_second': 158.738, 'eval_steps_per_second': 10.01, 'epoch': 0.96}
{'loss': 2.5853, 'grad_norm': 0.5023263692855835, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.6335349082946777, 'eval_runtime': 6.2863, 'eval_samples_per_second': 158.916, 'eval_steps_per_second': 10.022, 'epoch': 1.0}
{'train_runtime': 394.5706, 'train_samples_per_second': 25.336, 'train_steps_per_second': 1.584, 'train_loss': 3.0196459228515624, 'epoch': 1.0}
train_results:  {'eval_loss': [4.485069274902344, 3.9056448936462402, 3.5236916542053223, 3.376492738723755, 3.2362420558929443, 3.134958505630493, 3.037057638168335, 2.9979634284973145, 2.95279598236084, 2.9184458255767822, 2.8860905170440674, 2.8591790199279785, 2.8329124450683594, 2.806734561920166, 2.7785844802856445, 2.7482452392578125, 2.7209577560424805, 2.6949634552001953, 2.6750504970550537, 2.6613097190856934, 2.649909734725952, 2.6421430110931396, 2.6374032497406006, 2.635103940963745, 2.6335349082946777], 'performance': [0.75, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:50,  9.60s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:25<00:44,  1.50it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:32<00:28,  1.81it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:37<00:16,  2.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:41<00:07,  2.65it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:43<00:00,  3.49it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:43<00:00,  2.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  0.8026077151298523
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.7875000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6124 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 0.7922784934468371
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 0.7922784934468371
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 0.7922784934468371
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.7922784934468371
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.792278493446837
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.7488, dtype=torch.float64), 0, tensor(0.2113, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), 0, 0, 0, 0, 6, 1, 0, 0, 0, 0, 77, 0.038840512563759345, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(3.3020e-18, dtype=torch.float64), tensor(0.7488, dtype=torch.float64), tensor(4.1215e-18, dtype=torch.float64), tensor(0.2113, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), tensor(2.1063e-18, dtype=torch.float64), tensor(1.6241e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.7856e-18, dtype=torch.float64), tensor(0.1907, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6008, dtype=torch.float64), tensor(0.3884, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.749
  rowan_hellaswag: 0
  sciq: 0.211
  triviaqa: 0.04
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (77,)
  lora_dropout: (0.038840512563759345,)
  num_layers_to_apply: (6,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  6
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  77
lora dropout:  0.038840512563759345
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,784,704 || all params: 8,034,045,952 || trainable%: 0.0471
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:39,  4.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:54,  1.66it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:34,  2.38it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:26,  2.54it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.58it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.75it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.10it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.84it/s]
Evaluation performance at step 25: 0.75
{'loss': 2.861, 'grad_norm': 0.7357755303382874, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 2.471785068511963, 'eval_runtime': 8.8206, 'eval_samples_per_second': 113.257, 'eval_steps_per_second': 7.142, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:54,  3.58s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<01:01,  1.48it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:38,  2.16it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:30,  2.48it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.74it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:19,  2.97it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:16,  3.07it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:06,  2.90it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.13it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.42it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.95it/s]
Evaluation performance at step 50: 0.78
{'loss': 2.1346, 'grad_norm': 0.3993949592113495, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 1.8153942823410034, 'eval_runtime': 8.8162, 'eval_samples_per_second': 113.314, 'eval_steps_per_second': 7.146, 'epoch': 0.08}
{'loss': 1.7002, 'grad_norm': 0.2886928617954254, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5495895147323608, 'eval_runtime': 8.8479, 'eval_samples_per_second': 112.908, 'eval_steps_per_second': 7.12, 'epoch': 0.12}
{'loss': 1.4521, 'grad_norm': 0.372037798166275, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3906569480895996, 'eval_runtime': 8.8468, 'eval_samples_per_second': 112.922, 'eval_steps_per_second': 7.121, 'epoch': 0.16}
{'loss': 1.3685, 'grad_norm': 0.6207818984985352, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3025798797607422, 'eval_runtime': 8.8778, 'eval_samples_per_second': 112.528, 'eval_steps_per_second': 7.096, 'epoch': 0.2}
{'loss': 1.2838, 'grad_norm': 0.5592944622039795, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.263903260231018, 'eval_runtime': 8.9052, 'eval_samples_per_second': 112.181, 'eval_steps_per_second': 7.074, 'epoch': 0.24}
{'loss': 1.2666, 'grad_norm': 0.4915741980075836, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.235031247138977, 'eval_runtime': 8.9028, 'eval_samples_per_second': 112.211, 'eval_steps_per_second': 7.076, 'epoch': 0.28}
{'loss': 1.2235, 'grad_norm': 0.5315396189689636, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2222689390182495, 'eval_runtime': 8.9365, 'eval_samples_per_second': 111.789, 'eval_steps_per_second': 7.05, 'epoch': 0.32}
{'loss': 1.2304, 'grad_norm': 0.2986840605735779, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2131959199905396, 'eval_runtime': 8.9779, 'eval_samples_per_second': 111.273, 'eval_steps_per_second': 7.017, 'epoch': 0.36}
{'loss': 1.2031, 'grad_norm': 0.4847816228866577, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.201631784439087, 'eval_runtime': 8.995, 'eval_samples_per_second': 111.061, 'eval_steps_per_second': 7.004, 'epoch': 0.4}
{'loss': 1.1781, 'grad_norm': 0.4747420847415924, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1985111236572266, 'eval_runtime': 8.963, 'eval_samples_per_second': 111.458, 'eval_steps_per_second': 7.029, 'epoch': 0.44}
{'loss': 1.2045, 'grad_norm': 0.38054051995277405, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1921311616897583, 'eval_runtime': 8.9883, 'eval_samples_per_second': 111.145, 'eval_steps_per_second': 7.009, 'epoch': 0.48}
{'loss': 1.1749, 'grad_norm': 0.3831883668899536, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1873568296432495, 'eval_runtime': 9.0016, 'eval_samples_per_second': 110.98, 'eval_steps_per_second': 6.999, 'epoch': 0.52}
{'loss': 1.1815, 'grad_norm': 0.35507604479789734, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1828093528747559, 'eval_runtime': 8.9548, 'eval_samples_per_second': 111.561, 'eval_steps_per_second': 7.035, 'epoch': 0.56}
{'loss': 1.1864, 'grad_norm': 0.3894270658493042, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.179901361465454, 'eval_runtime': 8.9353, 'eval_samples_per_second': 111.804, 'eval_steps_per_second': 7.051, 'epoch': 0.6}
{'loss': 1.1966, 'grad_norm': 0.4053814709186554, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1798534393310547, 'eval_runtime': 8.906, 'eval_samples_per_second': 112.172, 'eval_steps_per_second': 7.074, 'epoch': 0.64}
{'loss': 1.1796, 'grad_norm': 0.4832150638103485, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1771818399429321, 'eval_runtime': 8.9192, 'eval_samples_per_second': 112.006, 'eval_steps_per_second': 7.063, 'epoch': 0.68}
{'loss': 1.1923, 'grad_norm': 0.35722020268440247, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1744251251220703, 'eval_runtime': 8.9326, 'eval_samples_per_second': 111.837, 'eval_steps_per_second': 7.053, 'epoch': 0.72}
{'loss': 1.1795, 'grad_norm': 0.2718261480331421, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1739559173583984, 'eval_runtime': 8.9248, 'eval_samples_per_second': 111.935, 'eval_steps_per_second': 7.059, 'epoch': 0.76}
{'loss': 1.1692, 'grad_norm': 0.44461384415626526, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.173350214958191, 'eval_runtime': 8.9237, 'eval_samples_per_second': 111.949, 'eval_steps_per_second': 7.06, 'epoch': 0.8}
{'loss': 1.1727, 'grad_norm': 0.25873008370399475, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.170791745185852, 'eval_runtime': 8.92, 'eval_samples_per_second': 111.995, 'eval_steps_per_second': 7.063, 'epoch': 0.84}
{'loss': 1.1508, 'grad_norm': 0.3528294563293457, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1704398393630981, 'eval_runtime': 8.9185, 'eval_samples_per_second': 112.015, 'eval_steps_per_second': 7.064, 'epoch': 0.88}
{'loss': 1.1703, 'grad_norm': 0.39166316390037537, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1688957214355469, 'eval_runtime': 8.9089, 'eval_samples_per_second': 112.136, 'eval_steps_per_second': 7.072, 'epoch': 0.92}
{'loss': 1.1412, 'grad_norm': 0.3557360768318176, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.167893409729004, 'eval_runtime': 8.9089, 'eval_samples_per_second': 112.135, 'eval_steps_per_second': 7.072, 'epoch': 0.96}
{'loss': 1.1676, 'grad_norm': 0.39778202772140503, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.167188286781311, 'eval_runtime': 8.9089, 'eval_samples_per_second': 112.135, 'eval_steps_per_second': 7.072, 'epoch': 1.0}
{'train_runtime': 517.8514, 'train_samples_per_second': 19.309, 'train_steps_per_second': 1.207, 'train_loss': 1.3347616973876952, 'epoch': 1.0}
train_results:  {'eval_loss': [2.471785068511963, 1.8153942823410034, 1.5495895147323608, 1.3906569480895996, 1.3025798797607422, 1.263903260231018, 1.235031247138977, 1.2222689390182495, 1.2131959199905396, 1.201631784439087, 1.1985111236572266, 1.1921311616897583, 1.1873568296432495, 1.1828093528747559, 1.179901361465454, 1.1798534393310547, 1.1771818399429321, 1.1744251251220703, 1.1739559173583984, 1.173350214958191, 1.170791745185852, 1.1704398393630981, 1.1688957214355469, 1.167893409729004, 1.167188286781311], 'performance': [0.75, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:07<12:32,  7.60s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:06,  1.25it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:21<00:36,  1.82it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:25,  1.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:15,  2.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:06,  2.74it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.61it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.53it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  0.7562128305435181
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.7875000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8820 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 0.7620946694798789
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 0.7620946694798789
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 0.7620946694798789
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.7620946694798789
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 0.7620946694798789
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1096, dtype=torch.float64), tensor(0.2657, dtype=torch.float64), tensor(0.5303, dtype=torch.float64), 0, 0, 0, tensor(0.0944, dtype=torch.float64), 4, 1, 0, 0, 0, 0, 62, 0.09233317536917209, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(8.5705e-18, dtype=torch.float64), tensor(0.1096, dtype=torch.float64), tensor(0.2657, dtype=torch.float64), tensor(0.5303, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.8940e-18, dtype=torch.float64), tensor(0.0944, dtype=torch.float64), tensor(0.1172, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4875, dtype=torch.float64), tensor(0.9233, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.11
  sciq: 0.266
  triviaqa: 0.53
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.094

LoRA Parameters:
  lora_r: (62,)
  lora_dropout: (0.09233317536917209,)
  num_layers_to_apply: (4,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  4
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  62
lora dropout:  0.09233317536917209
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,031,616 || all params: 8,032,292,864 || trainable%: 0.0253
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:37,  2.80s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.97it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.52it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.69it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:19,  2.63it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.88it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.77it/s]
Evaluation performance at step 25: 0.79
{'loss': 4.9753, 'grad_norm': 0.9838646054267883, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.79}
{'eval_loss': 4.423199653625488, 'eval_runtime': 7.6906, 'eval_samples_per_second': 129.9, 'eval_steps_per_second': 8.192, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<04:59,  3.02s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:48,  1.86it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:33,  2.46it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:25,  2.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:20,  3.24it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:14<00:17,  3.29it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:17,  2.90it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:16,  2.64it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:11,  3.00it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.92it/s]
Evaluation performance at step 50: 0.75
{'loss': 3.8909, 'grad_norm': 0.8345917463302612, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 3.3925230503082275, 'eval_runtime': 7.6616, 'eval_samples_per_second': 130.391, 'eval_steps_per_second': 8.223, 'epoch': 0.08}
{'loss': 3.0816, 'grad_norm': 1.4616448879241943, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.828974962234497, 'eval_runtime': 7.6837, 'eval_samples_per_second': 130.015, 'eval_steps_per_second': 8.199, 'epoch': 0.12}
{'loss': 2.6093, 'grad_norm': 0.9211130142211914, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.5270581245422363, 'eval_runtime': 7.7452, 'eval_samples_per_second': 128.984, 'eval_steps_per_second': 8.134, 'epoch': 0.16}
{'loss': 2.382, 'grad_norm': 0.9183474779129028, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.3233208656311035, 'eval_runtime': 7.7807, 'eval_samples_per_second': 128.395, 'eval_steps_per_second': 8.097, 'epoch': 0.2}
{'loss': 2.2282, 'grad_norm': 1.327786922454834, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.2069790363311768, 'eval_runtime': 7.8066, 'eval_samples_per_second': 127.969, 'eval_steps_per_second': 8.07, 'epoch': 0.24}
{'loss': 2.1835, 'grad_norm': 1.3679308891296387, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.125221014022827, 'eval_runtime': 7.7871, 'eval_samples_per_second': 128.289, 'eval_steps_per_second': 8.09, 'epoch': 0.28}
{'loss': 2.0622, 'grad_norm': 1.2708207368850708, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.05370831489563, 'eval_runtime': 7.7726, 'eval_samples_per_second': 128.529, 'eval_steps_per_second': 8.105, 'epoch': 0.32}
{'loss': 2.0764, 'grad_norm': 2.2841434478759766, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.0200960636138916, 'eval_runtime': 7.7826, 'eval_samples_per_second': 128.364, 'eval_steps_per_second': 8.095, 'epoch': 0.36}
{'loss': 1.9753, 'grad_norm': 1.4149061441421509, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9944857358932495, 'eval_runtime': 7.7732, 'eval_samples_per_second': 128.519, 'eval_steps_per_second': 8.105, 'epoch': 0.4}
{'loss': 1.9479, 'grad_norm': 1.0558874607086182, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9674357175827026, 'eval_runtime': 7.7659, 'eval_samples_per_second': 128.639, 'eval_steps_per_second': 8.112, 'epoch': 0.44}
{'loss': 1.98, 'grad_norm': 0.6648428440093994, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9504039287567139, 'eval_runtime': 7.776, 'eval_samples_per_second': 128.472, 'eval_steps_per_second': 8.102, 'epoch': 0.48}
{'loss': 1.9326, 'grad_norm': 0.9503942728042603, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9276636838912964, 'eval_runtime': 7.7635, 'eval_samples_per_second': 128.679, 'eval_steps_per_second': 8.115, 'epoch': 0.52}
{'loss': 1.8911, 'grad_norm': 1.0212947130203247, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9166237115859985, 'eval_runtime': 7.7716, 'eval_samples_per_second': 128.544, 'eval_steps_per_second': 8.106, 'epoch': 0.56}
{'loss': 1.958, 'grad_norm': 1.0874141454696655, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9046953916549683, 'eval_runtime': 7.7816, 'eval_samples_per_second': 128.38, 'eval_steps_per_second': 8.096, 'epoch': 0.6}
{'loss': 1.8637, 'grad_norm': 1.1712265014648438, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8885594606399536, 'eval_runtime': 7.7719, 'eval_samples_per_second': 128.54, 'eval_steps_per_second': 8.106, 'epoch': 0.64}
{'loss': 1.9099, 'grad_norm': 1.0644484758377075, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.877657413482666, 'eval_runtime': 7.7848, 'eval_samples_per_second': 128.326, 'eval_steps_per_second': 8.093, 'epoch': 0.68}
{'loss': 1.8392, 'grad_norm': 1.034921646118164, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8744022846221924, 'eval_runtime': 7.7658, 'eval_samples_per_second': 128.642, 'eval_steps_per_second': 8.113, 'epoch': 0.72}
{'loss': 1.8688, 'grad_norm': 1.133427381515503, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8644603490829468, 'eval_runtime': 7.752, 'eval_samples_per_second': 128.87, 'eval_steps_per_second': 8.127, 'epoch': 0.76}
{'loss': 1.8957, 'grad_norm': 1.3215053081512451, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8555227518081665, 'eval_runtime': 7.755, 'eval_samples_per_second': 128.821, 'eval_steps_per_second': 8.124, 'epoch': 0.8}
{'loss': 1.8777, 'grad_norm': 1.262017846107483, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.853151559829712, 'eval_runtime': 7.7645, 'eval_samples_per_second': 128.662, 'eval_steps_per_second': 8.114, 'epoch': 0.84}
{'loss': 1.8374, 'grad_norm': 1.0546672344207764, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8485678434371948, 'eval_runtime': 7.7557, 'eval_samples_per_second': 128.808, 'eval_steps_per_second': 8.123, 'epoch': 0.88}
{'loss': 1.8845, 'grad_norm': 1.777635097503662, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.842337727546692, 'eval_runtime': 7.7588, 'eval_samples_per_second': 128.757, 'eval_steps_per_second': 8.12, 'epoch': 0.92}
{'loss': 1.8186, 'grad_norm': 1.1102935075759888, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8384124040603638, 'eval_runtime': 7.7471, 'eval_samples_per_second': 128.952, 'eval_steps_per_second': 8.132, 'epoch': 0.96}
{'loss': 1.8022, 'grad_norm': 0.7353453040122986, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8365931510925293, 'eval_runtime': 7.7846, 'eval_samples_per_second': 128.331, 'eval_steps_per_second': 8.093, 'epoch': 1.0}
{'train_runtime': 465.2698, 'train_samples_per_second': 21.489, 'train_steps_per_second': 1.343, 'train_loss': 2.230880725097656, 'epoch': 1.0}
train_results:  {'eval_loss': [4.423199653625488, 3.3925230503082275, 2.828974962234497, 2.5270581245422363, 2.3233208656311035, 2.2069790363311768, 2.125221014022827, 2.05370831489563, 2.0200960636138916, 1.9944857358932495, 1.9674357175827026, 1.9504039287567139, 1.9276636838912964, 1.9166237115859985, 1.9046953916549683, 1.8885594606399536, 1.877657413482666, 1.8744022846221924, 1.8644603490829468, 1.8555227518081665, 1.853151559829712, 1.8485678434371948, 1.842337727546692, 1.8384124040603638, 1.8365931510925293], 'performance': [0.79, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<13:21,  8.10s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<01:00,  1.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:39,  1.68it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:24,  2.09it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:32<00:14,  2.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:36<00:06,  2.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.67it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.58it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.79, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8092976808547974
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.7875000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1817 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 0.7555148520867481
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 0.7555148520867481
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 0.7555148520867481
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.7555148520867481
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 0.7555148520867481
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1042, dtype=torch.float64), tensor(0.0236, dtype=torch.float64), tensor(0.3308, dtype=torch.float64), 0, 0, 0, tensor(0.5413, dtype=torch.float64), 5, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(3.4381e-18, dtype=torch.float64), tensor(4.0957e-18, dtype=torch.float64), tensor(0.1042, dtype=torch.float64), tensor(0.0236, dtype=torch.float64), tensor(0.3308, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.8560e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5413, dtype=torch.float64), tensor(0.1565, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.104
  sciq: 0.024
  triviaqa: 0.331
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.541

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (5,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  5
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 81,920 || all params: 8,030,343,168 || trainable%: 0.0010
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:37,  2.80s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.96it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.73it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:20,  2.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:16,  3.03it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  3.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:07,  3.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.68it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.88it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.2084, 'grad_norm': 3.374441623687744, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.8400583267211914, 'eval_runtime': 7.9899, 'eval_samples_per_second': 125.033, 'eval_steps_per_second': 7.885, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:24,  3.28s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:50,  1.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:33,  2.44it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.88it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.15it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:14<00:18,  3.22it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.84it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:11,  3.00it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.15it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.57it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:04,  2.70it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.88it/s]
Evaluation performance at step 50: 0.79
{'loss': 3.3745, 'grad_norm': 4.351597785949707, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.79}
{'eval_loss': 2.9283697605133057, 'eval_runtime': 7.9446, 'eval_samples_per_second': 125.746, 'eval_steps_per_second': 7.93, 'epoch': 0.08}
{'loss': 2.6782, 'grad_norm': 4.679696559906006, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.4876632690429688, 'eval_runtime': 8.0225, 'eval_samples_per_second': 124.525, 'eval_steps_per_second': 7.853, 'epoch': 0.12}
{'loss': 2.3403, 'grad_norm': 3.694206714630127, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.195054531097412, 'eval_runtime': 8.0286, 'eval_samples_per_second': 124.43, 'eval_steps_per_second': 7.847, 'epoch': 0.16}
{'loss': 2.063, 'grad_norm': 6.324560642242432, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.038158416748047, 'eval_runtime': 8.0179, 'eval_samples_per_second': 124.597, 'eval_steps_per_second': 7.857, 'epoch': 0.2}
{'loss': 1.9998, 'grad_norm': 6.605359077453613, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9212357997894287, 'eval_runtime': 8.0445, 'eval_samples_per_second': 124.184, 'eval_steps_per_second': 7.831, 'epoch': 0.24}
{'loss': 1.7886, 'grad_norm': 4.681396961212158, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.833085298538208, 'eval_runtime': 8.0378, 'eval_samples_per_second': 124.288, 'eval_steps_per_second': 7.838, 'epoch': 0.28}
{'loss': 1.7334, 'grad_norm': 6.555993556976318, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.780097484588623, 'eval_runtime': 8.0303, 'eval_samples_per_second': 124.403, 'eval_steps_per_second': 7.845, 'epoch': 0.32}
{'loss': 1.7045, 'grad_norm': 10.21315860748291, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7402331829071045, 'eval_runtime': 8.0029, 'eval_samples_per_second': 124.83, 'eval_steps_per_second': 7.872, 'epoch': 0.36}
{'loss': 1.6939, 'grad_norm': 3.0780394077301025, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7029191255569458, 'eval_runtime': 8.0168, 'eval_samples_per_second': 124.613, 'eval_steps_per_second': 7.858, 'epoch': 0.4}
{'loss': 1.7321, 'grad_norm': 5.32873010635376, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.667310118675232, 'eval_runtime': 8.0091, 'eval_samples_per_second': 124.733, 'eval_steps_per_second': 7.866, 'epoch': 0.44}
{'loss': 1.6826, 'grad_norm': 5.153262138366699, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.637671947479248, 'eval_runtime': 8.0636, 'eval_samples_per_second': 123.89, 'eval_steps_per_second': 7.813, 'epoch': 0.48}
{'loss': 1.6848, 'grad_norm': 5.7375168800354, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.626936435699463, 'eval_runtime': 8.0799, 'eval_samples_per_second': 123.64, 'eval_steps_per_second': 7.797, 'epoch': 0.52}
{'loss': 1.5961, 'grad_norm': 4.901981830596924, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.614177942276001, 'eval_runtime': 8.0516, 'eval_samples_per_second': 124.075, 'eval_steps_per_second': 7.825, 'epoch': 0.56}
{'loss': 1.564, 'grad_norm': 4.462284564971924, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6031454801559448, 'eval_runtime': 8.0422, 'eval_samples_per_second': 124.22, 'eval_steps_per_second': 7.834, 'epoch': 0.6}
{'loss': 1.6229, 'grad_norm': 4.610044956207275, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5933291912078857, 'eval_runtime': 8.0438, 'eval_samples_per_second': 124.195, 'eval_steps_per_second': 7.832, 'epoch': 0.64}
{'loss': 1.5655, 'grad_norm': 5.758460521697998, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5907775163650513, 'eval_runtime': 8.008, 'eval_samples_per_second': 124.75, 'eval_steps_per_second': 7.867, 'epoch': 0.68}
{'loss': 1.6034, 'grad_norm': 6.217437267303467, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5782930850982666, 'eval_runtime': 8.0043, 'eval_samples_per_second': 124.808, 'eval_steps_per_second': 7.871, 'epoch': 0.72}
{'loss': 1.6576, 'grad_norm': 4.010202407836914, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.575698733329773, 'eval_runtime': 7.9997, 'eval_samples_per_second': 124.88, 'eval_steps_per_second': 7.875, 'epoch': 0.76}
{'loss': 1.6122, 'grad_norm': 4.768439292907715, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5706231594085693, 'eval_runtime': 8.0062, 'eval_samples_per_second': 124.779, 'eval_steps_per_second': 7.869, 'epoch': 0.8}
{'loss': 1.5978, 'grad_norm': 6.680025100708008, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5683985948562622, 'eval_runtime': 7.9907, 'eval_samples_per_second': 125.02, 'eval_steps_per_second': 7.884, 'epoch': 0.84}
{'loss': 1.6102, 'grad_norm': 3.9880638122558594, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5623465776443481, 'eval_runtime': 8.0093, 'eval_samples_per_second': 124.73, 'eval_steps_per_second': 7.866, 'epoch': 0.88}
{'loss': 1.5925, 'grad_norm': 2.7516744136810303, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5596075057983398, 'eval_runtime': 8.0037, 'eval_samples_per_second': 124.818, 'eval_steps_per_second': 7.871, 'epoch': 0.92}
{'loss': 1.601, 'grad_norm': 4.130382537841797, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5569713115692139, 'eval_runtime': 8.0132, 'eval_samples_per_second': 124.669, 'eval_steps_per_second': 7.862, 'epoch': 0.96}
{'loss': 1.546, 'grad_norm': 3.323319911956787, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5554683208465576, 'eval_runtime': 7.9958, 'eval_samples_per_second': 124.94, 'eval_steps_per_second': 7.879, 'epoch': 1.0}
{'train_runtime': 470.075, 'train_samples_per_second': 21.271, 'train_steps_per_second': 1.33, 'train_loss': 1.9141398742675781, 'epoch': 1.0}
train_results:  {'eval_loss': [3.8400583267211914, 2.9283697605133057, 2.4876632690429688, 2.195054531097412, 2.038158416748047, 1.9212357997894287, 1.833085298538208, 1.780097484588623, 1.7402331829071045, 1.7029191255569458, 1.667310118675232, 1.637671947479248, 1.626936435699463, 1.614177942276001, 1.6031454801559448, 1.5933291912078857, 1.5907775163650513, 1.5782930850982666, 1.575698733329773, 1.5706231594085693, 1.5683985948562622, 1.5623465776443481, 1.5596075057983398, 1.5569713115692139, 1.5554683208465576], 'performance': [0.77, 0.79]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:07<12:57,  7.85s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<01:01,  1.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:40,  1.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:24,  2.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:32<00:14,  2.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:40<00:08,  2.32it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:42<00:00,  3.10it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:42<00:00,  2.37it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.79]
current iteration observed (possibly low-fid or predicted) performance:  1.1678935289382935
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.7875000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.0094 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 1.0401827618504942
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 1.036377039803721
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.0403324379295456
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.987162375377592
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.86175121989217
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2170, dtype=torch.float64), 0, tensor(0.1764, dtype=torch.float64), 0, 0, 0, tensor(0.6067, dtype=torch.float64), 1, 1, 0, 0, 1, 0, 2, 0.0, 47.99999999999999, 1]
normalized proposed parameters for next round by BO: [tensor(2.9641e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2170, dtype=torch.float64), tensor(7.8088e-18, dtype=torch.float64), tensor(0.1764, dtype=torch.float64), tensor(6.8462e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6067, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.217
  sciq: 0
  triviaqa: 0.176
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.607

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 1, 0],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:54,  5.40s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:10<01:32,  1.02s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:50,  1.64it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:34,  2.20it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:17<00:28,  2.37it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:21<00:24,  2.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:19,  2.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:15,  2.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:11,  2.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:08,  3.14it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:35<00:07,  2.54it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:37<00:03,  2.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.54it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.0299, 'grad_norm': 8.303240776062012, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.5562243461608887, 'eval_runtime': 8.5628, 'eval_samples_per_second': 116.668, 'eval_steps_per_second': 7.357, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:50,  4.15s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:22,  1.10it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:47,  1.76it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.30it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:28,  2.38it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:23,  2.48it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:19,  2.57it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.57it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:12,  2.84it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.57it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:03,  2.86it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.60it/s]
Evaluation performance at step 50: 0.77
{'loss': 2.8931, 'grad_norm': 17.986051559448242, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 2.3745007514953613, 'eval_runtime': 8.5532, 'eval_samples_per_second': 116.798, 'eval_steps_per_second': 7.366, 'epoch': 0.08}
{'loss': 2.1805, 'grad_norm': 27.911558151245117, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9929132461547852, 'eval_runtime': 8.5866, 'eval_samples_per_second': 116.343, 'eval_steps_per_second': 7.337, 'epoch': 0.12}
{'loss': 1.8006, 'grad_norm': 17.102752685546875, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8116965293884277, 'eval_runtime': 8.5963, 'eval_samples_per_second': 116.212, 'eval_steps_per_second': 7.329, 'epoch': 0.16}
{'loss': 1.6855, 'grad_norm': 12.93760871887207, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7484580278396606, 'eval_runtime': 8.6247, 'eval_samples_per_second': 115.831, 'eval_steps_per_second': 7.305, 'epoch': 0.2}
{'loss': 1.6805, 'grad_norm': 32.39322280883789, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.719340443611145, 'eval_runtime': 8.6588, 'eval_samples_per_second': 115.374, 'eval_steps_per_second': 7.276, 'epoch': 0.24}
{'loss': 1.6435, 'grad_norm': 7.928107738494873, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6683634519577026, 'eval_runtime': 8.6473, 'eval_samples_per_second': 115.528, 'eval_steps_per_second': 7.286, 'epoch': 0.28}
{'loss': 1.6573, 'grad_norm': 11.137114524841309, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6360704898834229, 'eval_runtime': 8.6105, 'eval_samples_per_second': 116.021, 'eval_steps_per_second': 7.317, 'epoch': 0.32}
{'loss': 1.5965, 'grad_norm': 7.611109256744385, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.618276834487915, 'eval_runtime': 8.6182, 'eval_samples_per_second': 115.918, 'eval_steps_per_second': 7.31, 'epoch': 0.36}
{'loss': 1.5943, 'grad_norm': 14.323189735412598, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5907090902328491, 'eval_runtime': 8.6071, 'eval_samples_per_second': 116.067, 'eval_steps_per_second': 7.32, 'epoch': 0.4}
{'loss': 1.5344, 'grad_norm': 17.44404411315918, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.567557454109192, 'eval_runtime': 8.5924, 'eval_samples_per_second': 116.265, 'eval_steps_per_second': 7.332, 'epoch': 0.44}
{'loss': 1.5364, 'grad_norm': 11.35299015045166, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5389803647994995, 'eval_runtime': 8.5867, 'eval_samples_per_second': 116.343, 'eval_steps_per_second': 7.337, 'epoch': 0.48}
{'loss': 1.4828, 'grad_norm': 6.550196647644043, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5165590047836304, 'eval_runtime': 8.5831, 'eval_samples_per_second': 116.392, 'eval_steps_per_second': 7.34, 'epoch': 0.52}
{'loss': 1.4504, 'grad_norm': 13.372894287109375, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5102465152740479, 'eval_runtime': 8.5731, 'eval_samples_per_second': 116.528, 'eval_steps_per_second': 7.349, 'epoch': 0.56}
{'loss': 1.5048, 'grad_norm': 13.922770500183105, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.490828037261963, 'eval_runtime': 8.5731, 'eval_samples_per_second': 116.527, 'eval_steps_per_second': 7.349, 'epoch': 0.6}
{'loss': 1.4429, 'grad_norm': 7.684943675994873, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4848676919937134, 'eval_runtime': 8.5795, 'eval_samples_per_second': 116.44, 'eval_steps_per_second': 7.343, 'epoch': 0.64}
{'loss': 1.4404, 'grad_norm': 7.161103248596191, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4741222858428955, 'eval_runtime': 8.5543, 'eval_samples_per_second': 116.783, 'eval_steps_per_second': 7.365, 'epoch': 0.68}
{'loss': 1.4151, 'grad_norm': 7.692903995513916, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4627342224121094, 'eval_runtime': 8.548, 'eval_samples_per_second': 116.869, 'eval_steps_per_second': 7.37, 'epoch': 0.72}
{'loss': 1.4703, 'grad_norm': 6.36315393447876, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4535502195358276, 'eval_runtime': 8.525, 'eval_samples_per_second': 117.185, 'eval_steps_per_second': 7.39, 'epoch': 0.76}
{'loss': 1.4849, 'grad_norm': 7.068440914154053, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.44881010055542, 'eval_runtime': 8.5014, 'eval_samples_per_second': 117.51, 'eval_steps_per_second': 7.411, 'epoch': 0.8}
{'loss': 1.4147, 'grad_norm': 7.708095550537109, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.44283127784729, 'eval_runtime': 8.5102, 'eval_samples_per_second': 117.388, 'eval_steps_per_second': 7.403, 'epoch': 0.84}
{'loss': 1.4307, 'grad_norm': 11.192680358886719, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4367190599441528, 'eval_runtime': 8.5287, 'eval_samples_per_second': 117.134, 'eval_steps_per_second': 7.387, 'epoch': 0.88}
{'loss': 1.4327, 'grad_norm': 8.13968276977539, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4309906959533691, 'eval_runtime': 8.5163, 'eval_samples_per_second': 117.304, 'eval_steps_per_second': 7.398, 'epoch': 0.92}
{'loss': 1.4068, 'grad_norm': 6.264631271362305, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4269793033599854, 'eval_runtime': 8.5334, 'eval_samples_per_second': 117.069, 'eval_steps_per_second': 7.383, 'epoch': 0.96}
{'loss': 1.4011, 'grad_norm': 5.507476806640625, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4262968301773071, 'eval_runtime': 8.5697, 'eval_samples_per_second': 116.574, 'eval_steps_per_second': 7.351, 'epoch': 1.0}
{'train_runtime': 513.4425, 'train_samples_per_second': 19.472, 'train_steps_per_second': 1.217, 'train_loss': 1.7044073791503906, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5562243461608887, 2.3745007514953613, 1.9929132461547852, 1.8116965293884277, 1.7484580278396606, 1.719340443611145, 1.6683634519577026, 1.6360704898834229, 1.618276834487915, 1.5907090902328491, 1.567557454109192, 1.5389803647994995, 1.5165590047836304, 1.5102465152740479, 1.490828037261963, 1.4848676919937134, 1.4741222858428955, 1.4627342224121094, 1.4535502195358276, 1.44881010055542, 1.44283127784729, 1.4367190599441528, 1.4309906959533691, 1.4269793033599854, 1.4262968301773071], 'performance': [0.76, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<14:40,  8.89s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:03,  1.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:21<00:35,  1.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:25<00:21,  2.39it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:30<00:12,  2.76it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:07,  2.49it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.54it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  1.2252730131149292
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.5136 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 1.050661277500098
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 1.0513902389223038
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.0516446390967997
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.0513638507852399
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.045027162066506
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1484, dtype=torch.float64), 0, tensor(0.0863, dtype=torch.float64), 0, 0, 0, tensor(0.7653, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 1.5456903734076127e-16, 15.58366669186919, 1]
normalized proposed parameters for next round by BO: [tensor(1.5530e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1484, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0863, dtype=torch.float64), tensor(7.3144e-17, dtype=torch.float64), tensor(6.7090e-18, dtype=torch.float64), tensor(4.5893e-17, dtype=torch.float64), tensor(0.7653, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.5457e-15, dtype=torch.float64), tensor(0.3247, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.148
  sciq: 0
  triviaqa: 0.086
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.765

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.5456903734076127e-16,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (15.58366669186919,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  1.5456903734076127e-16
lora alpha:  15.58366669186919
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:49,  4.14s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:22,  1.10it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.79it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.49it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:23,  2.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:21,  2.43it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:26<00:17,  2.50it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:28<00:12,  2.78it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:30<00:08,  3.01it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:06,  3.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  3.08it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.32it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.66it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.1841, 'grad_norm': 0.6595232486724854, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 4.1291937828063965, 'eval_runtime': 8.411, 'eval_samples_per_second': 118.772, 'eval_steps_per_second': 7.49, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:32,  4.57s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:24,  1.07it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.77it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.33it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.51it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:21,  2.42it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.61it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:11,  2.98it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.60it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:03,  2.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.63it/s]
Evaluation performance at step 50: 0.77
{'loss': 4.0365, 'grad_norm': 0.5875627398490906, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 3.882030963897705, 'eval_runtime': 8.3886, 'eval_samples_per_second': 119.09, 'eval_steps_per_second': 7.51, 'epoch': 0.08}
{'loss': 3.7788, 'grad_norm': 0.689893901348114, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.657365322113037, 'eval_runtime': 8.4051, 'eval_samples_per_second': 118.856, 'eval_steps_per_second': 7.495, 'epoch': 0.12}
{'loss': 3.5033, 'grad_norm': 1.081155776977539, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.334167718887329, 'eval_runtime': 8.4339, 'eval_samples_per_second': 118.45, 'eval_steps_per_second': 7.47, 'epoch': 0.16}
{'loss': 3.2102, 'grad_norm': 0.8341125845909119, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.1639275550842285, 'eval_runtime': 8.4344, 'eval_samples_per_second': 118.443, 'eval_steps_per_second': 7.469, 'epoch': 0.2}
{'loss': 3.109, 'grad_norm': 0.9585187435150146, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.039860248565674, 'eval_runtime': 8.4407, 'eval_samples_per_second': 118.355, 'eval_steps_per_second': 7.464, 'epoch': 0.24}
{'loss': 2.9296, 'grad_norm': 1.6776628494262695, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.98496413230896, 'eval_runtime': 8.4422, 'eval_samples_per_second': 118.334, 'eval_steps_per_second': 7.462, 'epoch': 0.28}
{'loss': 2.9319, 'grad_norm': 0.6921558380126953, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.9479079246520996, 'eval_runtime': 8.4127, 'eval_samples_per_second': 118.75, 'eval_steps_per_second': 7.489, 'epoch': 0.32}
{'loss': 2.9198, 'grad_norm': 1.293043851852417, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.9140968322753906, 'eval_runtime': 8.3827, 'eval_samples_per_second': 119.174, 'eval_steps_per_second': 7.516, 'epoch': 0.36}
{'loss': 2.8611, 'grad_norm': 4.037296295166016, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.8951597213745117, 'eval_runtime': 8.3836, 'eval_samples_per_second': 119.161, 'eval_steps_per_second': 7.515, 'epoch': 0.4}
{'loss': 2.8953, 'grad_norm': 1.3697649240493774, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.870857000350952, 'eval_runtime': 8.3922, 'eval_samples_per_second': 119.039, 'eval_steps_per_second': 7.507, 'epoch': 0.44}
{'loss': 2.8547, 'grad_norm': 1.4953837394714355, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.8518035411834717, 'eval_runtime': 8.398, 'eval_samples_per_second': 118.957, 'eval_steps_per_second': 7.502, 'epoch': 0.48}
{'loss': 2.8414, 'grad_norm': 1.0462043285369873, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.8379852771759033, 'eval_runtime': 8.3905, 'eval_samples_per_second': 119.063, 'eval_steps_per_second': 7.508, 'epoch': 0.52}
{'loss': 2.8334, 'grad_norm': 1.254432201385498, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.8253111839294434, 'eval_runtime': 8.3886, 'eval_samples_per_second': 119.09, 'eval_steps_per_second': 7.51, 'epoch': 0.56}
{'loss': 2.7794, 'grad_norm': 1.998903512954712, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.814992666244507, 'eval_runtime': 8.3968, 'eval_samples_per_second': 118.974, 'eval_steps_per_second': 7.503, 'epoch': 0.6}
{'loss': 2.8133, 'grad_norm': 2.645878314971924, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.810908794403076, 'eval_runtime': 8.4032, 'eval_samples_per_second': 118.883, 'eval_steps_per_second': 7.497, 'epoch': 0.64}
{'loss': 2.8127, 'grad_norm': 1.120483160018921, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.80122709274292, 'eval_runtime': 8.3952, 'eval_samples_per_second': 118.996, 'eval_steps_per_second': 7.504, 'epoch': 0.68}
{'loss': 2.8005, 'grad_norm': 0.6009553074836731, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.7911481857299805, 'eval_runtime': 8.3875, 'eval_samples_per_second': 119.105, 'eval_steps_per_second': 7.511, 'epoch': 0.72}
{'loss': 2.8584, 'grad_norm': 1.307007074356079, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.783250093460083, 'eval_runtime': 8.4267, 'eval_samples_per_second': 118.551, 'eval_steps_per_second': 7.476, 'epoch': 0.76}
{'loss': 2.7912, 'grad_norm': 1.0216714143753052, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.775479555130005, 'eval_runtime': 8.4444, 'eval_samples_per_second': 118.304, 'eval_steps_per_second': 7.461, 'epoch': 0.8}
{'loss': 2.794, 'grad_norm': 0.6448337435722351, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.7693405151367188, 'eval_runtime': 8.4539, 'eval_samples_per_second': 118.17, 'eval_steps_per_second': 7.452, 'epoch': 0.84}
{'loss': 2.7952, 'grad_norm': 0.7217640280723572, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.762749671936035, 'eval_runtime': 8.5014, 'eval_samples_per_second': 117.51, 'eval_steps_per_second': 7.411, 'epoch': 0.88}
{'loss': 2.7459, 'grad_norm': 1.2572239637374878, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.756906032562256, 'eval_runtime': 8.5294, 'eval_samples_per_second': 117.125, 'eval_steps_per_second': 7.386, 'epoch': 0.92}
{'loss': 2.7646, 'grad_norm': 0.5875183939933777, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.7530767917633057, 'eval_runtime': 8.5429, 'eval_samples_per_second': 116.939, 'eval_steps_per_second': 7.375, 'epoch': 0.96}
{'loss': 2.7326, 'grad_norm': 0.4640446901321411, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.751146078109741, 'eval_runtime': 8.5283, 'eval_samples_per_second': 117.139, 'eval_steps_per_second': 7.387, 'epoch': 1.0}
{'train_runtime': 500.1157, 'train_samples_per_second': 19.993, 'train_steps_per_second': 1.25, 'train_loss': 3.023070520019531, 'epoch': 1.0}
train_results:  {'eval_loss': [4.1291937828063965, 3.882030963897705, 3.657365322113037, 3.334167718887329, 3.1639275550842285, 3.039860248565674, 2.98496413230896, 2.9479079246520996, 2.9140968322753906, 2.8951597213745117, 2.870857000350952, 2.8518035411834717, 2.8379852771759033, 2.8253111839294434, 2.814992666244507, 2.810908794403076, 2.80122709274292, 2.7911481857299805, 2.783250093460083, 2.775479555130005, 2.7693405151367188, 2.762749671936035, 2.756906032562256, 2.7530767917633057, 2.751146078109741], 'performance': [0.77, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<16:09,  9.80s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:03,  1.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:40,  1.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:23,  2.13it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:14,  2.46it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:06,  2.74it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.56it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.51it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  0.8350085020065308
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9604 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 0.9647082214665259
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.9668191683330642
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 0.9690615847906514
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.898871192665828
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.9444706869521988
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2991, dtype=torch.float64), tensor(0.2017, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.4992, dtype=torch.float64), 1, 1, 0, 1, 0, 1, 2, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(9.3512e-17, dtype=torch.float64), tensor(0.2991, dtype=torch.float64), tensor(0.2017, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4992, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.299
  rowan_hellaswag: 0.202
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.499

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 90,112 || all params: 8,030,351,360 || trainable%: 0.0011
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:01,  3.05s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:54,  1.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:35,  2.32it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.58it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:27,  2.44it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:26,  2.24it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:20,  2.50it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.57it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:12,  2.81it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.07it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.13it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.70it/s]
Evaluation performance at step 25: 0.77
{'loss': 3.4387, 'grad_norm': 4.9434356689453125, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 2.758477210998535, 'eval_runtime': 8.8084, 'eval_samples_per_second': 113.414, 'eval_steps_per_second': 7.152, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:19,  3.83s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:57,  1.58it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:42,  1.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:31,  2.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.67it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:20,  2.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:20,  2.55it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.56it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:12,  2.78it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:09,  2.99it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.50it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:04,  2.62it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.68it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.3033, 'grad_norm': 2.4141669273376465, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.889155626296997, 'eval_runtime': 8.8081, 'eval_samples_per_second': 113.418, 'eval_steps_per_second': 7.152, 'epoch': 0.08}
{'loss': 1.6757, 'grad_norm': 2.0282540321350098, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.518216848373413, 'eval_runtime': 8.8577, 'eval_samples_per_second': 112.783, 'eval_steps_per_second': 7.112, 'epoch': 0.12}
{'loss': 1.4294, 'grad_norm': 1.5623219013214111, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3735172748565674, 'eval_runtime': 8.8646, 'eval_samples_per_second': 112.696, 'eval_steps_per_second': 7.107, 'epoch': 0.16}
{'loss': 1.3877, 'grad_norm': 1.743493914604187, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3002015352249146, 'eval_runtime': 8.905, 'eval_samples_per_second': 112.185, 'eval_steps_per_second': 7.075, 'epoch': 0.2}
{'loss': 1.3352, 'grad_norm': 1.840831995010376, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.283003330230713, 'eval_runtime': 8.8999, 'eval_samples_per_second': 112.248, 'eval_steps_per_second': 7.079, 'epoch': 0.24}
{'loss': 1.2846, 'grad_norm': 2.168654441833496, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2698291540145874, 'eval_runtime': 8.9248, 'eval_samples_per_second': 111.935, 'eval_steps_per_second': 7.059, 'epoch': 0.28}
{'loss': 1.2773, 'grad_norm': 1.657118320465088, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2600754499435425, 'eval_runtime': 8.9024, 'eval_samples_per_second': 112.217, 'eval_steps_per_second': 7.077, 'epoch': 0.32}
{'loss': 1.2662, 'grad_norm': 1.6309837102890015, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.252549648284912, 'eval_runtime': 8.9134, 'eval_samples_per_second': 112.078, 'eval_steps_per_second': 7.068, 'epoch': 0.36}
{'loss': 1.3018, 'grad_norm': 1.635940670967102, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2465749979019165, 'eval_runtime': 8.89, 'eval_samples_per_second': 112.374, 'eval_steps_per_second': 7.087, 'epoch': 0.4}
{'loss': 1.2021, 'grad_norm': 1.6671960353851318, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2421246767044067, 'eval_runtime': 8.9065, 'eval_samples_per_second': 112.165, 'eval_steps_per_second': 7.073, 'epoch': 0.44}
{'loss': 1.3121, 'grad_norm': 1.6022875308990479, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2376649379730225, 'eval_runtime': 8.8912, 'eval_samples_per_second': 112.358, 'eval_steps_per_second': 7.086, 'epoch': 0.48}
{'loss': 1.2546, 'grad_norm': 1.449568271636963, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2333167791366577, 'eval_runtime': 8.8986, 'eval_samples_per_second': 112.265, 'eval_steps_per_second': 7.08, 'epoch': 0.52}
{'loss': 1.1873, 'grad_norm': 1.6761691570281982, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2286852598190308, 'eval_runtime': 8.9168, 'eval_samples_per_second': 112.035, 'eval_steps_per_second': 7.065, 'epoch': 0.56}
{'loss': 1.2533, 'grad_norm': 1.6894618272781372, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.225394606590271, 'eval_runtime': 8.8904, 'eval_samples_per_second': 112.368, 'eval_steps_per_second': 7.086, 'epoch': 0.6}
{'loss': 1.2848, 'grad_norm': 1.675980806350708, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2216540575027466, 'eval_runtime': 8.8872, 'eval_samples_per_second': 112.409, 'eval_steps_per_second': 7.089, 'epoch': 0.64}
{'loss': 1.2537, 'grad_norm': 1.5961472988128662, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2193353176116943, 'eval_runtime': 8.9038, 'eval_samples_per_second': 112.2, 'eval_steps_per_second': 7.076, 'epoch': 0.68}
{'loss': 1.2714, 'grad_norm': 1.6095107793807983, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.217124104499817, 'eval_runtime': 8.9178, 'eval_samples_per_second': 112.024, 'eval_steps_per_second': 7.065, 'epoch': 0.72}
{'loss': 1.285, 'grad_norm': 1.4556254148483276, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2146278619766235, 'eval_runtime': 8.8942, 'eval_samples_per_second': 112.321, 'eval_steps_per_second': 7.083, 'epoch': 0.76}
{'loss': 1.2173, 'grad_norm': 1.789570689201355, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2128643989562988, 'eval_runtime': 8.9412, 'eval_samples_per_second': 111.73, 'eval_steps_per_second': 7.046, 'epoch': 0.8}
{'loss': 1.2479, 'grad_norm': 1.8969008922576904, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2113131284713745, 'eval_runtime': 8.9555, 'eval_samples_per_second': 111.551, 'eval_steps_per_second': 7.035, 'epoch': 0.84}
{'loss': 1.2316, 'grad_norm': 1.9461250305175781, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2101798057556152, 'eval_runtime': 8.9706, 'eval_samples_per_second': 111.364, 'eval_steps_per_second': 7.023, 'epoch': 0.88}
{'loss': 1.1676, 'grad_norm': 1.7000555992126465, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2092291116714478, 'eval_runtime': 9.0022, 'eval_samples_per_second': 110.973, 'eval_steps_per_second': 6.998, 'epoch': 0.92}
{'loss': 1.231, 'grad_norm': 1.7275493144989014, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.208633542060852, 'eval_runtime': 9.0295, 'eval_samples_per_second': 110.638, 'eval_steps_per_second': 6.977, 'epoch': 0.96}
{'loss': 1.2444, 'grad_norm': 1.5686101913452148, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2082780599594116, 'eval_runtime': 9.0244, 'eval_samples_per_second': 110.7, 'eval_steps_per_second': 6.981, 'epoch': 1.0}
{'train_runtime': 424.5464, 'train_samples_per_second': 23.55, 'train_steps_per_second': 1.472, 'train_loss': 1.4137655242919922, 'epoch': 1.0}
train_results:  {'eval_loss': [2.758477210998535, 1.889155626296997, 1.518216848373413, 1.3735172748565674, 1.3002015352249146, 1.283003330230713, 1.2698291540145874, 1.2600754499435425, 1.252549648284912, 1.2465749979019165, 1.2421246767044067, 1.2376649379730225, 1.2333167791366577, 1.2286852598190308, 1.225394606590271, 1.2216540575027466, 1.2193353176116943, 1.217124104499817, 1.2146278619766235, 1.2128643989562988, 1.2113131284713745, 1.2101798057556152, 1.2092291116714478, 1.208633542060852, 1.2082780599594116], 'performance': [0.77, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<16:10,  9.80s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:17,  1.08it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:46,  1.44it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:35<00:31,  1.64it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:38<00:16,  2.15it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:46<00:08,  2.13it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:48<00:01,  2.87it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.07it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  1.225351095199585
current iteration best possible performance (full train run):  0.7244999999999999
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.5812 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.9058765148139023
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.9500063776838648
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.9599714838026059
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.9589756815845711
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 0.9633106186883391
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2106, dtype=torch.float64), 0, tensor(0.2119, dtype=torch.float64), 0, 0, 0, tensor(0.5775, dtype=torch.float64), 18, 1, 0, 0, 0, 1, 2, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(3.7134e-17, dtype=torch.float64), tensor(7.2767e-18, dtype=torch.float64), tensor(0.2106, dtype=torch.float64), tensor(2.9283e-17, dtype=torch.float64), tensor(0.2119, dtype=torch.float64), tensor(2.4681e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5775, dtype=torch.float64), tensor(0.5777, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.211
  sciq: 0
  triviaqa: 0.212
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.577

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 958,464 || all params: 8,031,219,712 || trainable%: 0.0119
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:16,  3.19s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:50,  1.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:34,  2.38it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.82it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:22,  2.95it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:21,  2.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.88it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:15,  2.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  2.97it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.13it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:06,  2.87it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:04,  2.70it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.85it/s]
Evaluation performance at step 25: 0.76
{'loss': 3.4043, 'grad_norm': 3.4539735317230225, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 2.1432340145111084, 'eval_runtime': 9.1517, 'eval_samples_per_second': 109.16, 'eval_steps_per_second': 6.884, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:51,  3.55s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:20,  1.13it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:49,  1.66it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:14<00:34,  2.19it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:17<00:28,  2.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:18,  2.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.62it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:11,  2.94it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.55it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:03,  2.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.04it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.59it/s]
Evaluation performance at step 50: 0.74
{'loss': 1.6782, 'grad_norm': 1.601924180984497, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.4680440425872803, 'eval_runtime': 9.1567, 'eval_samples_per_second': 109.1, 'eval_steps_per_second': 6.88, 'epoch': 0.08}
{'loss': 1.3546, 'grad_norm': 1.4642009735107422, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3316397666931152, 'eval_runtime': 9.2261, 'eval_samples_per_second': 108.28, 'eval_steps_per_second': 6.828, 'epoch': 0.12}
{'loss': 1.2292, 'grad_norm': 1.257434606552124, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.205514907836914, 'eval_runtime': 9.2183, 'eval_samples_per_second': 108.372, 'eval_steps_per_second': 6.834, 'epoch': 0.16}
{'loss': 1.1993, 'grad_norm': 1.5593641996383667, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1745344400405884, 'eval_runtime': 9.2418, 'eval_samples_per_second': 108.096, 'eval_steps_per_second': 6.817, 'epoch': 0.2}
{'loss': 1.13, 'grad_norm': 1.433923363685608, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1497396230697632, 'eval_runtime': 9.2374, 'eval_samples_per_second': 108.147, 'eval_steps_per_second': 6.82, 'epoch': 0.24}
{'loss': 1.0986, 'grad_norm': 1.6120328903198242, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1184428930282593, 'eval_runtime': 9.2545, 'eval_samples_per_second': 107.947, 'eval_steps_per_second': 6.807, 'epoch': 0.28}
{'loss': 1.1402, 'grad_norm': 1.5363142490386963, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0943293571472168, 'eval_runtime': 9.2618, 'eval_samples_per_second': 107.863, 'eval_steps_per_second': 6.802, 'epoch': 0.32}
{'loss': 1.0512, 'grad_norm': 1.6058461666107178, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.071304440498352, 'eval_runtime': 9.2301, 'eval_samples_per_second': 108.233, 'eval_steps_per_second': 6.826, 'epoch': 0.36}
{'loss': 1.0811, 'grad_norm': 1.6970744132995605, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.045352578163147, 'eval_runtime': 9.2247, 'eval_samples_per_second': 108.296, 'eval_steps_per_second': 6.829, 'epoch': 0.4}
{'loss': 1.0821, 'grad_norm': 2.067160129547119, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0225623846054077, 'eval_runtime': 9.2186, 'eval_samples_per_second': 108.367, 'eval_steps_per_second': 6.834, 'epoch': 0.44}
{'loss': 1.0404, 'grad_norm': 2.0076253414154053, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0096402168273926, 'eval_runtime': 9.2289, 'eval_samples_per_second': 108.247, 'eval_steps_per_second': 6.826, 'epoch': 0.48}
{'loss': 1.0755, 'grad_norm': 2.3042306900024414, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0002177953720093, 'eval_runtime': 9.2167, 'eval_samples_per_second': 108.39, 'eval_steps_per_second': 6.835, 'epoch': 0.52}
{'loss': 1.0032, 'grad_norm': 1.886292815208435, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9800966382026672, 'eval_runtime': 9.2168, 'eval_samples_per_second': 108.388, 'eval_steps_per_second': 6.835, 'epoch': 0.56}
{'loss': 0.9928, 'grad_norm': 1.6538035869598389, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9589775800704956, 'eval_runtime': 9.2089, 'eval_samples_per_second': 108.482, 'eval_steps_per_second': 6.841, 'epoch': 0.6}
{'loss': 1.0313, 'grad_norm': 2.131302833557129, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9461739659309387, 'eval_runtime': 9.212, 'eval_samples_per_second': 108.446, 'eval_steps_per_second': 6.839, 'epoch': 0.64}
{'loss': 0.9502, 'grad_norm': 2.2224724292755127, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9350584745407104, 'eval_runtime': 9.2193, 'eval_samples_per_second': 108.359, 'eval_steps_per_second': 6.833, 'epoch': 0.68}
{'loss': 1.0397, 'grad_norm': 2.0258936882019043, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9195510149002075, 'eval_runtime': 9.27, 'eval_samples_per_second': 107.767, 'eval_steps_per_second': 6.796, 'epoch': 0.72}
{'loss': 1.0231, 'grad_norm': 2.531202554702759, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9043582677841187, 'eval_runtime': 9.291, 'eval_samples_per_second': 107.523, 'eval_steps_per_second': 6.781, 'epoch': 0.76}
{'loss': 0.9779, 'grad_norm': 2.42804217338562, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.893282413482666, 'eval_runtime': 9.3038, 'eval_samples_per_second': 107.376, 'eval_steps_per_second': 6.771, 'epoch': 0.8}
{'loss': 1.027, 'grad_norm': 2.226977825164795, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8819923400878906, 'eval_runtime': 9.2985, 'eval_samples_per_second': 107.437, 'eval_steps_per_second': 6.775, 'epoch': 0.84}
{'loss': 0.9979, 'grad_norm': 1.9194340705871582, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8752294778823853, 'eval_runtime': 9.2998, 'eval_samples_per_second': 107.421, 'eval_steps_per_second': 6.774, 'epoch': 0.88}
{'loss': 0.9069, 'grad_norm': 1.771182894706726, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8689107894897461, 'eval_runtime': 9.2721, 'eval_samples_per_second': 107.742, 'eval_steps_per_second': 6.795, 'epoch': 0.92}
{'loss': 0.9252, 'grad_norm': 2.285191297531128, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8647153973579407, 'eval_runtime': 9.2769, 'eval_samples_per_second': 107.687, 'eval_steps_per_second': 6.791, 'epoch': 0.96}
{'loss': 0.9231, 'grad_norm': 2.6586191654205322, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8637150526046753, 'eval_runtime': 9.261, 'eval_samples_per_second': 107.872, 'eval_steps_per_second': 6.803, 'epoch': 1.0}
{'train_runtime': 496.3757, 'train_samples_per_second': 20.144, 'train_steps_per_second': 1.259, 'train_loss': 1.1745208892822265, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1432340145111084, 1.4680440425872803, 1.3316397666931152, 1.205514907836914, 1.1745344400405884, 1.1497396230697632, 1.1184428930282593, 1.0943293571472168, 1.071304440498352, 1.045352578163147, 1.0225623846054077, 1.0096402168273926, 1.0002177953720093, 0.9800966382026672, 0.9589775800704956, 0.9461739659309387, 0.9350584745407104, 0.9195510149002075, 0.9043582677841187, 0.893282413482666, 0.8819923400878906, 0.8752294778823853, 0.8689107894897461, 0.8647153973579407, 0.8637150526046753], 'performance': [0.76, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<16:28,  9.99s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:18,  1.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:44,  1.52it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:31<00:26,  1.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:38<00:17,  2.03it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:43<00:08,  2.32it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:46<00:01,  2.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:46<00:00,  2.16it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.9880110025405884
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4265 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.9177622708925272
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.9156097822429471
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.9137206469820645
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.9161746291210762
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.9161653772356421
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3242, dtype=torch.float64), 0, tensor(0.1859, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.4899, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 3.122502256758253e-18, 47.99999999999999, 1]
normalized proposed parameters for next round by BO: [tensor(0.3242, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1859, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1969e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4899, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.1225e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.324
  gsm8k: 0
  rowan_hellaswag: 0.186
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.49

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.122502256758253e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  3.122502256758253e-18
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:39,  4.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:20,  1.12it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:45,  1.84it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.54it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.58it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.77it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:14,  2.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  3.11it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.29it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:05,  3.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.43it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.68it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.87it/s]
Evaluation performance at step 25: 0.78
{'loss': 4.0985, 'grad_norm': 5.6424689292907715, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 3.5681099891662598, 'eval_runtime': 8.3808, 'eval_samples_per_second': 119.201, 'eval_steps_per_second': 7.517, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:56,  3.60s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:18,  1.17it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:44,  1.88it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:26,  2.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:21,  2.74it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.84it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:14,  2.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.12it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:07,  3.50it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:05,  3.35it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.51it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.72it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.95it/s]
Evaluation performance at step 50: 0.78
{'loss': 3.0204, 'grad_norm': 5.960808753967285, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 2.553178548812866, 'eval_runtime': 8.3604, 'eval_samples_per_second': 119.492, 'eval_steps_per_second': 7.536, 'epoch': 0.08}
{'loss': 2.2279, 'grad_norm': 12.261221885681152, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9357378482818604, 'eval_runtime': 8.3727, 'eval_samples_per_second': 119.316, 'eval_steps_per_second': 7.524, 'epoch': 0.12}
{'loss': 1.8173, 'grad_norm': 11.079253196716309, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7380921840667725, 'eval_runtime': 8.3983, 'eval_samples_per_second': 118.953, 'eval_steps_per_second': 7.502, 'epoch': 0.16}
{'loss': 1.7483, 'grad_norm': 6.352323532104492, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.661908745765686, 'eval_runtime': 8.4143, 'eval_samples_per_second': 118.727, 'eval_steps_per_second': 7.487, 'epoch': 0.2}
{'loss': 1.6446, 'grad_norm': 5.933333396911621, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6277267932891846, 'eval_runtime': 8.3892, 'eval_samples_per_second': 119.081, 'eval_steps_per_second': 7.51, 'epoch': 0.24}
{'loss': 1.6248, 'grad_norm': 9.675559997558594, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5938407182693481, 'eval_runtime': 8.3927, 'eval_samples_per_second': 119.033, 'eval_steps_per_second': 7.507, 'epoch': 0.28}
{'loss': 1.5887, 'grad_norm': 7.412938594818115, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.588637351989746, 'eval_runtime': 8.3951, 'eval_samples_per_second': 118.998, 'eval_steps_per_second': 7.504, 'epoch': 0.32}
{'loss': 1.5728, 'grad_norm': 5.13921594619751, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5475720167160034, 'eval_runtime': 8.4069, 'eval_samples_per_second': 118.831, 'eval_steps_per_second': 7.494, 'epoch': 0.36}
{'loss': 1.6092, 'grad_norm': 4.592701435089111, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.530239462852478, 'eval_runtime': 8.416, 'eval_samples_per_second': 118.703, 'eval_steps_per_second': 7.486, 'epoch': 0.4}
{'loss': 1.5112, 'grad_norm': 5.220706939697266, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5172064304351807, 'eval_runtime': 8.4061, 'eval_samples_per_second': 118.842, 'eval_steps_per_second': 7.495, 'epoch': 0.44}
{'loss': 1.5019, 'grad_norm': 3.5325751304626465, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4995474815368652, 'eval_runtime': 8.4103, 'eval_samples_per_second': 118.783, 'eval_steps_per_second': 7.491, 'epoch': 0.48}
{'loss': 1.5161, 'grad_norm': 3.2827420234680176, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.478023886680603, 'eval_runtime': 8.4059, 'eval_samples_per_second': 118.845, 'eval_steps_per_second': 7.495, 'epoch': 0.52}
{'loss': 1.4244, 'grad_norm': 7.991831302642822, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4632850885391235, 'eval_runtime': 8.4057, 'eval_samples_per_second': 118.849, 'eval_steps_per_second': 7.495, 'epoch': 0.56}
{'loss': 1.4721, 'grad_norm': 4.641211986541748, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.436301827430725, 'eval_runtime': 8.4114, 'eval_samples_per_second': 118.768, 'eval_steps_per_second': 7.49, 'epoch': 0.6}
{'loss': 1.5082, 'grad_norm': 4.894790172576904, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3839585781097412, 'eval_runtime': 8.3894, 'eval_samples_per_second': 119.079, 'eval_steps_per_second': 7.509, 'epoch': 0.64}
{'loss': 1.3864, 'grad_norm': 6.952013969421387, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3622710704803467, 'eval_runtime': 8.3777, 'eval_samples_per_second': 119.245, 'eval_steps_per_second': 7.52, 'epoch': 0.68}
{'loss': 1.366, 'grad_norm': 5.04282283782959, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3558993339538574, 'eval_runtime': 8.3955, 'eval_samples_per_second': 118.992, 'eval_steps_per_second': 7.504, 'epoch': 0.72}
{'loss': 1.4425, 'grad_norm': 4.665362358093262, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.345434546470642, 'eval_runtime': 8.4468, 'eval_samples_per_second': 118.269, 'eval_steps_per_second': 7.458, 'epoch': 0.76}
{'loss': 1.3437, 'grad_norm': 3.6844594478607178, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3381946086883545, 'eval_runtime': 8.4601, 'eval_samples_per_second': 118.084, 'eval_steps_per_second': 7.447, 'epoch': 0.8}
{'loss': 1.3284, 'grad_norm': 4.714104175567627, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3306713104248047, 'eval_runtime': 8.4506, 'eval_samples_per_second': 118.216, 'eval_steps_per_second': 7.455, 'epoch': 0.84}
{'loss': 1.3209, 'grad_norm': 3.465818166732788, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3226940631866455, 'eval_runtime': 8.4437, 'eval_samples_per_second': 118.313, 'eval_steps_per_second': 7.461, 'epoch': 0.88}
{'loss': 1.3248, 'grad_norm': 4.563540458679199, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3174325227737427, 'eval_runtime': 8.4491, 'eval_samples_per_second': 118.237, 'eval_steps_per_second': 7.456, 'epoch': 0.92}
{'loss': 1.3468, 'grad_norm': 3.6230926513671875, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3124299049377441, 'eval_runtime': 8.4396, 'eval_samples_per_second': 118.37, 'eval_steps_per_second': 7.465, 'epoch': 0.96}
{'loss': 1.2887, 'grad_norm': 4.367450714111328, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3104454278945923, 'eval_runtime': 8.4905, 'eval_samples_per_second': 117.661, 'eval_steps_per_second': 7.42, 'epoch': 1.0}
{'train_runtime': 495.9382, 'train_samples_per_second': 20.16, 'train_steps_per_second': 1.26, 'train_loss': 1.681384521484375, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5681099891662598, 2.553178548812866, 1.9357378482818604, 1.7380921840667725, 1.661908745765686, 1.6277267932891846, 1.5938407182693481, 1.588637351989746, 1.5475720167160034, 1.530239462852478, 1.5172064304351807, 1.4995474815368652, 1.478023886680603, 1.4632850885391235, 1.436301827430725, 1.3839585781097412, 1.3622710704803467, 1.3558993339538574, 1.345434546470642, 1.3381946086883545, 1.3306713104248047, 1.3226940631866455, 1.3174325227737427, 1.3124299049377441, 1.3104454278945923], 'performance': [0.78, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<11:24,  6.91s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:07,  1.24it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:42,  1.59it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:26,  1.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:15,  2.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:42<00:08,  2.23it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:44<00:01,  2.97it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:44<00:00,  2.27it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  1.2303980588912964
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.2603 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.902312494807779
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.835658371876302
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 0.9025917479818583
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.9034682942510253
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.903749363552551
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1931, dtype=torch.float64), 0, tensor(0.2934, dtype=torch.float64), 0, 0, 0, tensor(0.5135, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 93, 4.33680868994202e-19, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(7.2666e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1931, dtype=torch.float64), tensor(6.9671e-17, dtype=torch.float64), tensor(0.2934, dtype=torch.float64), tensor(1.5423e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5135, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7246, dtype=torch.float64), tensor(4.3368e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.193
  sciq: 0
  triviaqa: 0.293
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.514

LoRA Parameters:
  lora_r: (93,)
  lora_dropout: (4.33680868994202e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  93
lora dropout:  4.33680868994202e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 761,856 || all params: 8,031,023,104 || trainable%: 0.0095
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:39,  4.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:20,  1.12it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:45,  1.83it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.39it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.60it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:20,  2.44it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.63it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:12,  2.91it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.07it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.54it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:03,  2.85it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.20it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.64it/s]
Evaluation performance at step 25: 0.78
{'loss': 4.2873, 'grad_norm': 0.27556025981903076, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 4.273135662078857, 'eval_runtime': 8.1442, 'eval_samples_per_second': 122.664, 'eval_steps_per_second': 7.736, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:35,  2.79s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:44,  2.06it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.57it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.02it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  3.03it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:16,  3.03it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:11,  3.12it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:08,  3.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:05,  3.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:29<00:03,  3.28it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:31<00:00,  3.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:31<00:00,  3.16it/s]
Evaluation performance at step 50: 0.77
{'loss': 4.0476, 'grad_norm': 0.23160094022750854, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 3.9003472328186035, 'eval_runtime': 8.1676, 'eval_samples_per_second': 122.313, 'eval_steps_per_second': 7.713, 'epoch': 0.08}
{'loss': 3.6935, 'grad_norm': 0.28961217403411865, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.553110122680664, 'eval_runtime': 8.1973, 'eval_samples_per_second': 121.87, 'eval_steps_per_second': 7.685, 'epoch': 0.12}
{'loss': 3.3809, 'grad_norm': 0.16886277496814728, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.3864378929138184, 'eval_runtime': 8.2197, 'eval_samples_per_second': 121.538, 'eval_steps_per_second': 7.665, 'epoch': 0.16}
{'loss': 3.2914, 'grad_norm': 0.16699455678462982, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.237443685531616, 'eval_runtime': 8.2362, 'eval_samples_per_second': 121.294, 'eval_steps_per_second': 7.649, 'epoch': 0.2}
{'loss': 3.1442, 'grad_norm': 0.13503338396549225, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 3.129871129989624, 'eval_runtime': 8.2545, 'eval_samples_per_second': 121.024, 'eval_steps_per_second': 7.632, 'epoch': 0.24}
{'loss': 3.0422, 'grad_norm': 0.07056017965078354, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 3.071153163909912, 'eval_runtime': 8.253, 'eval_samples_per_second': 121.047, 'eval_steps_per_second': 7.634, 'epoch': 0.28}
{'loss': 3.0142, 'grad_norm': 0.1066678836941719, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 3.0172321796417236, 'eval_runtime': 8.2817, 'eval_samples_per_second': 120.627, 'eval_steps_per_second': 7.607, 'epoch': 0.32}
{'loss': 2.9452, 'grad_norm': 0.13143394887447357, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.954550266265869, 'eval_runtime': 8.2583, 'eval_samples_per_second': 120.969, 'eval_steps_per_second': 7.629, 'epoch': 0.36}
{'loss': 2.9284, 'grad_norm': 0.3430825471878052, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.9000699520111084, 'eval_runtime': 8.2695, 'eval_samples_per_second': 120.805, 'eval_steps_per_second': 7.618, 'epoch': 0.4}
{'loss': 2.8144, 'grad_norm': 0.33155128359794617, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.8508288860321045, 'eval_runtime': 8.2853, 'eval_samples_per_second': 120.575, 'eval_steps_per_second': 7.604, 'epoch': 0.44}
{'loss': 2.7919, 'grad_norm': 0.35169562697410583, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.8135645389556885, 'eval_runtime': 8.2734, 'eval_samples_per_second': 120.749, 'eval_steps_per_second': 7.615, 'epoch': 0.48}
{'loss': 2.7455, 'grad_norm': 0.3811837136745453, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.7762670516967773, 'eval_runtime': 8.2851, 'eval_samples_per_second': 120.578, 'eval_steps_per_second': 7.604, 'epoch': 0.52}
{'loss': 2.6873, 'grad_norm': 0.23020215332508087, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.736325263977051, 'eval_runtime': 8.2571, 'eval_samples_per_second': 120.987, 'eval_steps_per_second': 7.63, 'epoch': 0.56}
{'loss': 2.7188, 'grad_norm': 0.20224983990192413, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.7087457180023193, 'eval_runtime': 8.2593, 'eval_samples_per_second': 120.955, 'eval_steps_per_second': 7.628, 'epoch': 0.6}
{'loss': 2.6957, 'grad_norm': 0.30612891912460327, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.68575119972229, 'eval_runtime': 8.2531, 'eval_samples_per_second': 121.046, 'eval_steps_per_second': 7.634, 'epoch': 0.64}
{'loss': 2.6648, 'grad_norm': 0.3305164873600006, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.6647465229034424, 'eval_runtime': 8.2613, 'eval_samples_per_second': 120.925, 'eval_steps_per_second': 7.626, 'epoch': 0.68}
{'loss': 2.6043, 'grad_norm': 0.5558146238327026, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.64897084236145, 'eval_runtime': 8.2707, 'eval_samples_per_second': 120.788, 'eval_steps_per_second': 7.617, 'epoch': 0.72}
{'loss': 2.7069, 'grad_norm': 0.23564492166042328, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.636566162109375, 'eval_runtime': 8.2637, 'eval_samples_per_second': 120.89, 'eval_steps_per_second': 7.624, 'epoch': 0.76}
{'loss': 2.6553, 'grad_norm': 0.269021600484848, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.6249701976776123, 'eval_runtime': 8.2626, 'eval_samples_per_second': 120.906, 'eval_steps_per_second': 7.625, 'epoch': 0.8}
{'loss': 2.5902, 'grad_norm': 0.19992634654045105, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.6149892807006836, 'eval_runtime': 8.2796, 'eval_samples_per_second': 120.657, 'eval_steps_per_second': 7.609, 'epoch': 0.84}
{'loss': 2.5643, 'grad_norm': 0.2609046697616577, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.6068100929260254, 'eval_runtime': 8.2937, 'eval_samples_per_second': 120.453, 'eval_steps_per_second': 7.596, 'epoch': 0.88}
{'loss': 2.576, 'grad_norm': 0.2827610671520233, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.6012320518493652, 'eval_runtime': 8.2843, 'eval_samples_per_second': 120.589, 'eval_steps_per_second': 7.605, 'epoch': 0.92}
{'loss': 2.5605, 'grad_norm': 0.1888941526412964, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.597808361053467, 'eval_runtime': 8.2665, 'eval_samples_per_second': 120.849, 'eval_steps_per_second': 7.621, 'epoch': 0.96}
{'loss': 2.5811, 'grad_norm': 0.1537819504737854, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.5966200828552246, 'eval_runtime': 8.2574, 'eval_samples_per_second': 120.982, 'eval_steps_per_second': 7.63, 'epoch': 1.0}
{'train_runtime': 492.0219, 'train_samples_per_second': 20.32, 'train_steps_per_second': 1.27, 'train_loss': 2.9492840576171875, 'epoch': 1.0}
train_results:  {'eval_loss': [4.273135662078857, 3.9003472328186035, 3.553110122680664, 3.3864378929138184, 3.237443685531616, 3.129871129989624, 3.071153163909912, 3.0172321796417236, 2.954550266265869, 2.9000699520111084, 2.8508288860321045, 2.8135645389556885, 2.7762670516967773, 2.736325263977051, 2.7087457180023193, 2.68575119972229, 2.6647465229034424, 2.64897084236145, 2.636566162109375, 2.6249701976776123, 2.6149892807006836, 2.6068100929260254, 2.6012320518493652, 2.597808361053467, 2.5966200828552246], 'performance': [0.78, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:51,  9.61s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:01,  1.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:40,  1.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:31<00:28,  1.78it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:39<00:18,  1.89it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:46<00:09,  1.98it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:50<00:01,  2.38it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:50<00:00,  1.97it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  0.7980332374572754
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3243 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.8824509941828421
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.8850578706814455
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.8894582643347015
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.8892045039013742
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.8892886083701712
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2265, dtype=torch.float64), 0, tensor(0.2139, dtype=torch.float64), 0, tensor(0.4354, dtype=torch.float64), 0, 0, 0, tensor(0.1242, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.061222192538013254, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.2265, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2139, dtype=torch.float64), tensor(4.6633e-18, dtype=torch.float64), tensor(0.4354, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0140e-17, dtype=torch.float64), tensor(1.4403e-17, dtype=torch.float64), tensor(0.1242, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.6122, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.227
  gsm8k: 0
  rowan_hellaswag: 0.214
  sciq: 0
  triviaqa: 0.435
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.124

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.061222192538013254,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.061222192538013254
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:10,  2.53s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:10,  1.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.01it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.55it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:23,  2.86it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.09it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:06,  3.14it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  3.32it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.44it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.94it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.4045, 'grad_norm': 7.240878105163574, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.8739895820617676, 'eval_runtime': 8.5822, 'eval_samples_per_second': 116.404, 'eval_steps_per_second': 7.341, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:45,  4.10s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:22,  1.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:47,  1.75it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.48it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:23,  2.53it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:18,  2.69it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:15,  2.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:11,  2.96it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.17it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:06,  3.14it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  3.34it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.61it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.78it/s]
Evaluation performance at step 50: 0.77
{'loss': 3.3177, 'grad_norm': 4.645930290222168, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 2.8088912963867188, 'eval_runtime': 8.4735, 'eval_samples_per_second': 117.897, 'eval_steps_per_second': 7.435, 'epoch': 0.08}
{'loss': 2.5137, 'grad_norm': 8.566322326660156, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.2911183834075928, 'eval_runtime': 8.5256, 'eval_samples_per_second': 117.176, 'eval_steps_per_second': 7.389, 'epoch': 0.12}
{'loss': 2.0972, 'grad_norm': 14.965288162231445, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.074631690979004, 'eval_runtime': 8.5659, 'eval_samples_per_second': 116.625, 'eval_steps_per_second': 7.355, 'epoch': 0.16}
{'loss': 2.0478, 'grad_norm': 8.898629188537598, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9813101291656494, 'eval_runtime': 8.5397, 'eval_samples_per_second': 116.984, 'eval_steps_per_second': 7.377, 'epoch': 0.2}
{'loss': 1.9709, 'grad_norm': 11.908042907714844, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9305963516235352, 'eval_runtime': 8.5451, 'eval_samples_per_second': 116.909, 'eval_steps_per_second': 7.373, 'epoch': 0.24}
{'loss': 1.8788, 'grad_norm': 11.749382019042969, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8914225101470947, 'eval_runtime': 8.553, 'eval_samples_per_second': 116.801, 'eval_steps_per_second': 7.366, 'epoch': 0.28}
{'loss': 1.8814, 'grad_norm': 6.9573655128479, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8465570211410522, 'eval_runtime': 8.5892, 'eval_samples_per_second': 116.309, 'eval_steps_per_second': 7.335, 'epoch': 0.32}
{'loss': 1.8029, 'grad_norm': 7.147810935974121, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.825723648071289, 'eval_runtime': 8.5678, 'eval_samples_per_second': 116.599, 'eval_steps_per_second': 7.353, 'epoch': 0.36}
{'loss': 1.8261, 'grad_norm': 6.629620552062988, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7822532653808594, 'eval_runtime': 8.5721, 'eval_samples_per_second': 116.541, 'eval_steps_per_second': 7.349, 'epoch': 0.4}
{'loss': 1.7564, 'grad_norm': 5.924038887023926, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.767940640449524, 'eval_runtime': 8.5742, 'eval_samples_per_second': 116.512, 'eval_steps_per_second': 7.348, 'epoch': 0.44}
{'loss': 1.752, 'grad_norm': 9.823348045349121, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.755558967590332, 'eval_runtime': 8.5755, 'eval_samples_per_second': 116.494, 'eval_steps_per_second': 7.346, 'epoch': 0.48}
{'loss': 1.7329, 'grad_norm': 5.928686618804932, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7309703826904297, 'eval_runtime': 8.5739, 'eval_samples_per_second': 116.516, 'eval_steps_per_second': 7.348, 'epoch': 0.52}
{'loss': 1.7206, 'grad_norm': 5.3088908195495605, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6993465423583984, 'eval_runtime': 8.5773, 'eval_samples_per_second': 116.47, 'eval_steps_per_second': 7.345, 'epoch': 0.56}
{'loss': 1.6608, 'grad_norm': 8.50153636932373, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6348873376846313, 'eval_runtime': 8.5677, 'eval_samples_per_second': 116.601, 'eval_steps_per_second': 7.353, 'epoch': 0.6}
{'loss': 1.6152, 'grad_norm': 4.880288124084473, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6109063625335693, 'eval_runtime': 8.6252, 'eval_samples_per_second': 115.823, 'eval_steps_per_second': 7.304, 'epoch': 0.64}
{'loss': 1.566, 'grad_norm': 5.3436970710754395, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6022064685821533, 'eval_runtime': 8.6253, 'eval_samples_per_second': 115.822, 'eval_steps_per_second': 7.304, 'epoch': 0.68}
{'loss': 1.5922, 'grad_norm': 4.9065046310424805, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5905869007110596, 'eval_runtime': 8.5971, 'eval_samples_per_second': 116.202, 'eval_steps_per_second': 7.328, 'epoch': 0.72}
{'loss': 1.5716, 'grad_norm': 5.5245208740234375, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5842008590698242, 'eval_runtime': 8.5953, 'eval_samples_per_second': 116.226, 'eval_steps_per_second': 7.33, 'epoch': 0.76}
{'loss': 1.5904, 'grad_norm': 4.980666637420654, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5761429071426392, 'eval_runtime': 8.5631, 'eval_samples_per_second': 116.663, 'eval_steps_per_second': 7.357, 'epoch': 0.8}
{'loss': 1.4996, 'grad_norm': 6.491889953613281, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5689055919647217, 'eval_runtime': 8.5703, 'eval_samples_per_second': 116.565, 'eval_steps_per_second': 7.351, 'epoch': 0.84}
{'loss': 1.5637, 'grad_norm': 6.122922420501709, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.559810996055603, 'eval_runtime': 8.5888, 'eval_samples_per_second': 116.314, 'eval_steps_per_second': 7.335, 'epoch': 0.88}
{'loss': 1.5053, 'grad_norm': 5.062385082244873, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5538557767868042, 'eval_runtime': 8.5889, 'eval_samples_per_second': 116.312, 'eval_steps_per_second': 7.335, 'epoch': 0.92}
{'loss': 1.5044, 'grad_norm': 5.02885103225708, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5461167097091675, 'eval_runtime': 8.6496, 'eval_samples_per_second': 115.497, 'eval_steps_per_second': 7.284, 'epoch': 0.96}
{'loss': 1.5969, 'grad_norm': 4.424712657928467, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5445725917816162, 'eval_runtime': 8.6764, 'eval_samples_per_second': 115.14, 'eval_steps_per_second': 7.261, 'epoch': 1.0}
{'train_runtime': 499.73, 'train_samples_per_second': 20.007, 'train_steps_per_second': 1.251, 'train_loss': 1.918765899658203, 'epoch': 1.0}
train_results:  {'eval_loss': [3.8739895820617676, 2.8088912963867188, 2.2911183834075928, 2.074631690979004, 1.9813101291656494, 1.9305963516235352, 1.8914225101470947, 1.8465570211410522, 1.825723648071289, 1.7822532653808594, 1.767940640449524, 1.755558967590332, 1.7309703826904297, 1.6993465423583984, 1.6348873376846313, 1.6109063625335693, 1.6022064685821533, 1.5905869007110596, 1.5842008590698242, 1.5761429071426392, 1.5689055919647217, 1.559810996055603, 1.5538557767868042, 1.5461167097091675, 1.5445725917816162], 'performance': [0.77, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<11:02,  6.69s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:06,  1.25it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:41,  1.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:26,  1.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:34<00:15,  2.29it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:06,  2.74it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:00,  3.56it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.49it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  1.2290406227111816
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.7101 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.8853599349154772
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 0.8864452917179564
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.8878190731351703
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.8028800960946046
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.8715204354754181
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2505, dtype=torch.float64), tensor(0.4737, dtype=torch.float64), tensor(0.2384, dtype=torch.float64), tensor(0.0374, dtype=torch.float64), 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.2505, dtype=torch.float64), tensor(0.4737, dtype=torch.float64), tensor(0.2384, dtype=torch.float64), tensor(0.0374, dtype=torch.float64), tensor(5.1271e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.25
  gsm8k: 0.474
  rowan_hellaswag: 0.238
  sciq: 0.037
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:06,  2.49s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.01it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.56it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:23,  2.86it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.72it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  2.96it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.80it/s]
Evaluation performance at step 25: 0.78
{'loss': 3.5203, 'grad_norm': 1.2723575830459595, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 3.4206833839416504, 'eval_runtime': 8.8579, 'eval_samples_per_second': 112.781, 'eval_steps_per_second': 7.112, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:01,  3.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.93it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.59it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.79it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.63it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.81it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.74it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.2396, 'grad_norm': 1.6947810649871826, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 3.155956745147705, 'eval_runtime': 8.8428, 'eval_samples_per_second': 112.974, 'eval_steps_per_second': 7.124, 'epoch': 0.08}
{'loss': 3.1311, 'grad_norm': 1.442717432975769, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.896899700164795, 'eval_runtime': 8.934, 'eval_samples_per_second': 111.82, 'eval_steps_per_second': 7.052, 'epoch': 0.12}
{'loss': 2.8201, 'grad_norm': 0.501705527305603, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.7686057090759277, 'eval_runtime': 8.9189, 'eval_samples_per_second': 112.01, 'eval_steps_per_second': 7.064, 'epoch': 0.16}
{'loss': 2.815, 'grad_norm': 1.0284041166305542, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.6478896141052246, 'eval_runtime': 8.9273, 'eval_samples_per_second': 111.904, 'eval_steps_per_second': 7.057, 'epoch': 0.2}
{'loss': 2.6142, 'grad_norm': 2.1340293884277344, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.557239532470703, 'eval_runtime': 8.9257, 'eval_samples_per_second': 111.924, 'eval_steps_per_second': 7.058, 'epoch': 0.24}
{'loss': 2.4768, 'grad_norm': 1.6274982690811157, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.5106124877929688, 'eval_runtime': 8.9344, 'eval_samples_per_second': 111.815, 'eval_steps_per_second': 7.051, 'epoch': 0.28}
{'loss': 2.5041, 'grad_norm': 2.6119589805603027, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.4790961742401123, 'eval_runtime': 8.9395, 'eval_samples_per_second': 111.751, 'eval_steps_per_second': 7.047, 'epoch': 0.32}
{'loss': 2.5049, 'grad_norm': 1.1301507949829102, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.450526237487793, 'eval_runtime': 8.9352, 'eval_samples_per_second': 111.805, 'eval_steps_per_second': 7.051, 'epoch': 0.36}
{'loss': 2.5008, 'grad_norm': 0.781129777431488, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.424971103668213, 'eval_runtime': 8.9374, 'eval_samples_per_second': 111.777, 'eval_steps_per_second': 7.049, 'epoch': 0.4}
{'loss': 2.5158, 'grad_norm': 1.0303549766540527, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.406581401824951, 'eval_runtime': 8.9391, 'eval_samples_per_second': 111.756, 'eval_steps_per_second': 7.048, 'epoch': 0.44}
{'loss': 2.4389, 'grad_norm': 3.7095327377319336, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.3894505500793457, 'eval_runtime': 8.9346, 'eval_samples_per_second': 111.813, 'eval_steps_per_second': 7.051, 'epoch': 0.48}
{'loss': 2.4367, 'grad_norm': 1.651764988899231, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.37398099899292, 'eval_runtime': 8.9298, 'eval_samples_per_second': 111.873, 'eval_steps_per_second': 7.055, 'epoch': 0.52}
{'loss': 2.3924, 'grad_norm': 0.8501615524291992, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.364457845687866, 'eval_runtime': 8.9315, 'eval_samples_per_second': 111.852, 'eval_steps_per_second': 7.054, 'epoch': 0.56}
{'loss': 2.4186, 'grad_norm': 1.2266714572906494, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.353670120239258, 'eval_runtime': 8.9447, 'eval_samples_per_second': 111.687, 'eval_steps_per_second': 7.043, 'epoch': 0.6}
{'loss': 2.3846, 'grad_norm': 1.189164400100708, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.3442347049713135, 'eval_runtime': 8.9374, 'eval_samples_per_second': 111.777, 'eval_steps_per_second': 7.049, 'epoch': 0.64}
{'loss': 2.375, 'grad_norm': 2.03769850730896, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.336827516555786, 'eval_runtime': 8.9345, 'eval_samples_per_second': 111.814, 'eval_steps_per_second': 7.051, 'epoch': 0.68}
{'loss': 2.3403, 'grad_norm': 0.6300445795059204, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.3294906616210938, 'eval_runtime': 8.9423, 'eval_samples_per_second': 111.716, 'eval_steps_per_second': 7.045, 'epoch': 0.72}
{'loss': 2.361, 'grad_norm': 1.4556260108947754, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.3221170902252197, 'eval_runtime': 8.9435, 'eval_samples_per_second': 111.701, 'eval_steps_per_second': 7.044, 'epoch': 0.76}
{'loss': 2.3003, 'grad_norm': 1.5324445962905884, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.3182730674743652, 'eval_runtime': 8.9658, 'eval_samples_per_second': 111.423, 'eval_steps_per_second': 7.027, 'epoch': 0.8}
{'loss': 2.3276, 'grad_norm': 1.796189308166504, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.3143601417541504, 'eval_runtime': 8.9668, 'eval_samples_per_second': 111.41, 'eval_steps_per_second': 7.026, 'epoch': 0.84}
{'loss': 2.3562, 'grad_norm': 1.2662173509597778, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.3105361461639404, 'eval_runtime': 8.9705, 'eval_samples_per_second': 111.365, 'eval_steps_per_second': 7.023, 'epoch': 0.88}
{'loss': 2.328, 'grad_norm': 1.3874280452728271, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.3073086738586426, 'eval_runtime': 9.0221, 'eval_samples_per_second': 110.728, 'eval_steps_per_second': 6.983, 'epoch': 0.92}
{'loss': 2.2885, 'grad_norm': 1.1208345890045166, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.3062565326690674, 'eval_runtime': 9.013, 'eval_samples_per_second': 110.839, 'eval_steps_per_second': 6.99, 'epoch': 0.96}
{'loss': 2.3289, 'grad_norm': 0.9531756043434143, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.3048336505889893, 'eval_runtime': 8.9792, 'eval_samples_per_second': 111.257, 'eval_steps_per_second': 7.016, 'epoch': 1.0}
{'train_runtime': 521.2698, 'train_samples_per_second': 19.178, 'train_steps_per_second': 1.199, 'train_loss': 2.548781829833984, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4206833839416504, 3.155956745147705, 2.896899700164795, 2.7686057090759277, 2.6478896141052246, 2.557239532470703, 2.5106124877929688, 2.4790961742401123, 2.450526237487793, 2.424971103668213, 2.406581401824951, 2.3894505500793457, 2.37398099899292, 2.364457845687866, 2.353670120239258, 2.3442347049713135, 2.336827516555786, 2.3294906616210938, 2.3221170902252197, 2.3182730674743652, 2.3143601417541504, 2.3105361461639404, 2.3073086738586426, 2.3062565326690674, 2.3048336505889893], 'performance': [0.78, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:07,  9.17s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:04,  1.28it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:39,  1.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:22,  2.26it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:31<00:13,  2.67it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:35<00:06,  3.09it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.71it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2270740270614624
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1027 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.8916158815491584
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.8875858693576588
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.8852054906644145
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.8857312816414372
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.8193993699291019
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3094, dtype=torch.float64), 0, tensor(0.2630, dtype=torch.float64), 0, 0, tensor(0.2483, dtype=torch.float64), 0, 0, tensor(0.1738, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.06958211626205926, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.3094, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2630, dtype=torch.float64), tensor(0.0054, dtype=torch.float64), tensor(1.9782e-17, dtype=torch.float64), tensor(0.2483, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7206e-17, dtype=torch.float64), tensor(0.1738, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.6958, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.309
  gsm8k: 0
  rowan_hellaswag: 0.263
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.248
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.174

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.06958211626205926,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.06958211626205926
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9944
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  994
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:02,  2.45s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.58it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.68it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:20,  2.89it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  3.00it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:11,  3.11it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.19it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.85it/s]
Evaluation performance at step 25: 0.78
{'loss': 4.5006, 'grad_norm': 1.4790430068969727, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 4.420177459716797, 'eval_runtime': 10.1067, 'eval_samples_per_second': 98.351, 'eval_steps_per_second': 6.233, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:32,  3.36s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:49,  1.84it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:32,  2.55it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:25,  2.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  2.92it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:15<00:20,  2.81it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:18<00:17,  2.93it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:14,  2.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:25<00:08,  3.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.85it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.19it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.95it/s]
Evaluation performance at step 50: 0.77
{'loss': 4.2595, 'grad_norm': 1.9674227237701416, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 4.053775310516357, 'eval_runtime': 8.6429, 'eval_samples_per_second': 115.008, 'eval_steps_per_second': 7.289, 'epoch': 0.08}
{'loss': 3.8617, 'grad_norm': 2.162220001220703, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 3.733544111251831, 'eval_runtime': 8.6134, 'eval_samples_per_second': 115.402, 'eval_steps_per_second': 7.314, 'epoch': 0.12}
{'loss': 3.6591, 'grad_norm': 3.7056796550750732, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 3.5788254737854004, 'eval_runtime': 8.6135, 'eval_samples_per_second': 115.4, 'eval_steps_per_second': 7.314, 'epoch': 0.16}
{'loss': 3.5266, 'grad_norm': 4.230363368988037, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 3.467987537384033, 'eval_runtime': 8.6526, 'eval_samples_per_second': 114.879, 'eval_steps_per_second': 7.281, 'epoch': 0.2}
{'loss': 3.4264, 'grad_norm': 2.8019256591796875, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 3.388828992843628, 'eval_runtime': 8.6938, 'eval_samples_per_second': 114.334, 'eval_steps_per_second': 7.247, 'epoch': 0.24}
{'loss': 3.3461, 'grad_norm': 1.3702369928359985, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 3.3174917697906494, 'eval_runtime': 8.6559, 'eval_samples_per_second': 114.835, 'eval_steps_per_second': 7.278, 'epoch': 0.28}
{'loss': 3.2918, 'grad_norm': 2.855513572692871, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 3.2447590827941895, 'eval_runtime': 8.6441, 'eval_samples_per_second': 114.992, 'eval_steps_per_second': 7.288, 'epoch': 0.32}
{'loss': 3.1959, 'grad_norm': 3.10831356048584, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 3.1810362339019775, 'eval_runtime': 8.6238, 'eval_samples_per_second': 115.262, 'eval_steps_per_second': 7.305, 'epoch': 0.36}
{'loss': 3.1373, 'grad_norm': 2.342217445373535, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 3.1411917209625244, 'eval_runtime': 8.6472, 'eval_samples_per_second': 114.95, 'eval_steps_per_second': 7.286, 'epoch': 0.4}
{'loss': 3.1143, 'grad_norm': 2.769970417022705, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 3.111816644668579, 'eval_runtime': 8.6351, 'eval_samples_per_second': 115.111, 'eval_steps_per_second': 7.296, 'epoch': 0.44}
{'loss': 3.0644, 'grad_norm': 3.612839937210083, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 3.0906455516815186, 'eval_runtime': 8.6354, 'eval_samples_per_second': 115.107, 'eval_steps_per_second': 7.296, 'epoch': 0.48}
{'loss': 3.0717, 'grad_norm': 1.3363161087036133, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 3.0737884044647217, 'eval_runtime': 8.6501, 'eval_samples_per_second': 114.911, 'eval_steps_per_second': 7.283, 'epoch': 0.52}
{'loss': 3.0952, 'grad_norm': 1.7424864768981934, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 3.0575473308563232, 'eval_runtime': 8.6464, 'eval_samples_per_second': 114.961, 'eval_steps_per_second': 7.286, 'epoch': 0.56}
{'loss': 3.0554, 'grad_norm': 2.3680739402770996, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 3.0379414558410645, 'eval_runtime': 8.6445, 'eval_samples_per_second': 114.986, 'eval_steps_per_second': 7.288, 'epoch': 0.6}
{'loss': 3.0118, 'grad_norm': 1.7074401378631592, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 3.013639450073242, 'eval_runtime': 8.639, 'eval_samples_per_second': 115.06, 'eval_steps_per_second': 7.293, 'epoch': 0.64}
{'loss': 3.0159, 'grad_norm': 5.315573215484619, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 2.987839698791504, 'eval_runtime': 8.6357, 'eval_samples_per_second': 115.104, 'eval_steps_per_second': 7.295, 'epoch': 0.68}
{'loss': 2.9715, 'grad_norm': 3.1065633296966553, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 2.971187114715576, 'eval_runtime': 8.6442, 'eval_samples_per_second': 114.99, 'eval_steps_per_second': 7.288, 'epoch': 0.72}
{'loss': 2.9121, 'grad_norm': 2.4308977127075195, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 2.9556450843811035, 'eval_runtime': 8.7066, 'eval_samples_per_second': 114.167, 'eval_steps_per_second': 7.236, 'epoch': 0.76}
{'loss': 2.9042, 'grad_norm': 2.7499396800994873, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 2.9443199634552, 'eval_runtime': 8.6821, 'eval_samples_per_second': 114.488, 'eval_steps_per_second': 7.256, 'epoch': 0.8}
{'loss': 2.905, 'grad_norm': 3.508971691131592, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 2.9350831508636475, 'eval_runtime': 8.692, 'eval_samples_per_second': 114.357, 'eval_steps_per_second': 7.248, 'epoch': 0.84}
{'loss': 2.9159, 'grad_norm': 1.9621962308883667, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 2.928743362426758, 'eval_runtime': 8.6802, 'eval_samples_per_second': 114.513, 'eval_steps_per_second': 7.258, 'epoch': 0.88}
{'loss': 2.9025, 'grad_norm': 1.9518038034439087, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 2.9235808849334717, 'eval_runtime': 8.6516, 'eval_samples_per_second': 114.893, 'eval_steps_per_second': 7.282, 'epoch': 0.92}
{'loss': 2.8901, 'grad_norm': 1.179787516593933, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 2.920769214630127, 'eval_runtime': 8.6585, 'eval_samples_per_second': 114.801, 'eval_steps_per_second': 7.276, 'epoch': 0.96}
{'train_runtime': 497.7174, 'train_samples_per_second': 19.979, 'train_steps_per_second': 1.25, 'train_loss': 3.2387758665912787, 'epoch': 1.0}
train_results:  {'eval_loss': [4.420177459716797, 4.053775310516357, 3.733544111251831, 3.5788254737854004, 3.467987537384033, 3.388828992843628, 3.3174917697906494, 3.2447590827941895, 3.1810362339019775, 3.1411917209625244, 3.111816644668579, 3.0906455516815186, 3.0737884044647217, 3.0575473308563232, 3.0379414558410645, 3.013639450073242, 2.987839698791504, 2.971187114715576, 2.9556450843811035, 2.9443199634552, 2.9350831508636475, 2.928743362426758, 2.9235808849334717, 2.920769214630127], 'performance': [0.78, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<13:25,  8.13s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:13<00:57,  1.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:21<00:38,  1.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:24,  2.07it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:15,  2.32it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:40<00:08,  2.25it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:42<00:00,  3.02it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:42<00:00,  2.34it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  1.22829008102417
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8963 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.8466822302345944
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.8839122985791117
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.8844306210639719
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.823616627713893
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.8839906639296149
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1789, dtype=torch.float64), 0, tensor(0.3135, dtype=torch.float64), tensor(0.0481, dtype=torch.float64), 0, 0, 0, 0, tensor(0.4595, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.1789, dtype=torch.float64), tensor(7.3449e-18, dtype=torch.float64), tensor(0.3135, dtype=torch.float64), tensor(0.0481, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.4132e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.4566e-19, dtype=torch.float64), tensor(0.4595, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.179
  gsm8k: 0
  rowan_hellaswag: 0.313
  sciq: 0.048
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.46

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:38,  4.03s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:20,  1.13it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:45,  1.84it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:24,  2.71it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:21,  2.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:17,  2.87it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:14,  2.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:10,  3.27it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:07,  3.64it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:06,  2.78it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.06it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.38it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.0677, 'grad_norm': 4.866339206695557, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.6126956939697266, 'eval_runtime': 8.8188, 'eval_samples_per_second': 113.281, 'eval_steps_per_second': 7.144, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:00,  3.64s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:20,  1.14it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:44,  1.85it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.39it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:26,  2.54it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:21,  2.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:17,  2.84it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:14,  2.91it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.13it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.65it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.76it/s]
Evaluation performance at step 50: 0.76
{'loss': 3.1041, 'grad_norm': 8.196453094482422, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 2.681804656982422, 'eval_runtime': 8.7848, 'eval_samples_per_second': 113.719, 'eval_steps_per_second': 7.171, 'epoch': 0.08}
{'loss': 2.3867, 'grad_norm': 9.467759132385254, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.2074973583221436, 'eval_runtime': 8.7604, 'eval_samples_per_second': 114.036, 'eval_steps_per_second': 7.191, 'epoch': 0.12}
{'loss': 2.0529, 'grad_norm': 16.508323669433594, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0053281784057617, 'eval_runtime': 8.7935, 'eval_samples_per_second': 113.606, 'eval_steps_per_second': 7.164, 'epoch': 0.16}
{'loss': 1.9091, 'grad_norm': 7.528054714202881, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.892287015914917, 'eval_runtime': 8.8398, 'eval_samples_per_second': 113.012, 'eval_steps_per_second': 7.127, 'epoch': 0.2}
{'loss': 1.8391, 'grad_norm': 8.074457168579102, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8325525522232056, 'eval_runtime': 8.8157, 'eval_samples_per_second': 113.32, 'eval_steps_per_second': 7.146, 'epoch': 0.24}
{'loss': 1.811, 'grad_norm': 10.313158988952637, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8019498586654663, 'eval_runtime': 8.8277, 'eval_samples_per_second': 113.166, 'eval_steps_per_second': 7.137, 'epoch': 0.28}
{'loss': 1.7705, 'grad_norm': 7.295889854431152, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7699604034423828, 'eval_runtime': 8.8237, 'eval_samples_per_second': 113.218, 'eval_steps_per_second': 7.14, 'epoch': 0.32}
{'loss': 1.7796, 'grad_norm': 11.84659194946289, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.752497911453247, 'eval_runtime': 8.8201, 'eval_samples_per_second': 113.264, 'eval_steps_per_second': 7.143, 'epoch': 0.36}
{'loss': 1.7129, 'grad_norm': 6.677021503448486, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7261741161346436, 'eval_runtime': 8.8356, 'eval_samples_per_second': 113.065, 'eval_steps_per_second': 7.13, 'epoch': 0.4}
{'loss': 1.6719, 'grad_norm': 6.533048629760742, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7067230939865112, 'eval_runtime': 8.8321, 'eval_samples_per_second': 113.11, 'eval_steps_per_second': 7.133, 'epoch': 0.44}
{'loss': 1.668, 'grad_norm': 6.843531131744385, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.689946174621582, 'eval_runtime': 8.8192, 'eval_samples_per_second': 113.275, 'eval_steps_per_second': 7.143, 'epoch': 0.48}
{'loss': 1.6626, 'grad_norm': 4.70815896987915, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.677596092224121, 'eval_runtime': 8.8339, 'eval_samples_per_second': 113.087, 'eval_steps_per_second': 7.132, 'epoch': 0.52}
{'loss': 1.6322, 'grad_norm': 7.228715419769287, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.668373703956604, 'eval_runtime': 8.8111, 'eval_samples_per_second': 113.38, 'eval_steps_per_second': 7.15, 'epoch': 0.56}
{'loss': 1.6256, 'grad_norm': 8.24916934967041, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6550644636154175, 'eval_runtime': 8.8162, 'eval_samples_per_second': 113.315, 'eval_steps_per_second': 7.146, 'epoch': 0.6}
{'loss': 1.6612, 'grad_norm': 6.2939324378967285, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6378618478775024, 'eval_runtime': 8.9367, 'eval_samples_per_second': 111.786, 'eval_steps_per_second': 7.05, 'epoch': 0.64}
{'loss': 1.6097, 'grad_norm': 4.267855644226074, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6095006465911865, 'eval_runtime': 8.9369, 'eval_samples_per_second': 111.784, 'eval_steps_per_second': 7.049, 'epoch': 0.68}
{'loss': 1.5557, 'grad_norm': 5.485295295715332, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5690438747406006, 'eval_runtime': 8.9577, 'eval_samples_per_second': 111.524, 'eval_steps_per_second': 7.033, 'epoch': 0.72}
{'loss': 1.511, 'grad_norm': 5.142802715301514, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5578827857971191, 'eval_runtime': 8.9306, 'eval_samples_per_second': 111.863, 'eval_steps_per_second': 7.054, 'epoch': 0.76}
{'loss': 1.5223, 'grad_norm': 6.7815446853637695, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5525001287460327, 'eval_runtime': 8.9435, 'eval_samples_per_second': 111.701, 'eval_steps_per_second': 7.044, 'epoch': 0.8}
{'loss': 1.5306, 'grad_norm': 4.670334815979004, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5421898365020752, 'eval_runtime': 8.8949, 'eval_samples_per_second': 112.311, 'eval_steps_per_second': 7.083, 'epoch': 0.84}
{'loss': 1.5043, 'grad_norm': 4.0563249588012695, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5377442836761475, 'eval_runtime': 8.8778, 'eval_samples_per_second': 112.527, 'eval_steps_per_second': 7.096, 'epoch': 0.88}
{'loss': 1.4587, 'grad_norm': 4.989802360534668, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.534374713897705, 'eval_runtime': 8.872, 'eval_samples_per_second': 112.601, 'eval_steps_per_second': 7.101, 'epoch': 0.92}
{'loss': 1.5335, 'grad_norm': 3.94411563873291, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.529893159866333, 'eval_runtime': 8.8742, 'eval_samples_per_second': 112.574, 'eval_steps_per_second': 7.099, 'epoch': 0.96}
{'loss': 1.5365, 'grad_norm': 3.4195384979248047, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.528070092201233, 'eval_runtime': 8.8836, 'eval_samples_per_second': 112.454, 'eval_steps_per_second': 7.092, 'epoch': 1.0}
{'train_runtime': 518.3101, 'train_samples_per_second': 19.292, 'train_steps_per_second': 1.206, 'train_loss': 1.8446962707519532, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6126956939697266, 2.681804656982422, 2.2074973583221436, 2.0053281784057617, 1.892287015914917, 1.8325525522232056, 1.8019498586654663, 1.7699604034423828, 1.752497911453247, 1.7261741161346436, 1.7067230939865112, 1.689946174621582, 1.677596092224121, 1.668373703956604, 1.6550644636154175, 1.6378618478775024, 1.6095006465911865, 1.5690438747406006, 1.5578827857971191, 1.5525001287460327, 1.5421898365020752, 1.5377442836761475, 1.534374713897705, 1.529893159866333, 1.528070092201233], 'performance': [0.76, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<09:59,  6.05s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<01:03,  1.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:40,  1.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:28<00:26,  1.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:15,  2.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:06,  2.80it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.65it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.56it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  1.2295026779174805
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2352 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.8889001408636141
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 0.8285949848115779
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 0.8697433940024528
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.888582644484426
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.8886623902586934
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3113, dtype=torch.float64), tensor(0.0171, dtype=torch.float64), tensor(0.3103, dtype=torch.float64), 0, 0, 0, tensor(0.0562, dtype=torch.float64), 0, tensor(0.3051, dtype=torch.float64), 1, 1, 0, 1, 0, 1, 2, 0.03156347930556296, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.3113, dtype=torch.float64), tensor(0.0171, dtype=torch.float64), tensor(0.3103, dtype=torch.float64), tensor(2.1640e-17, dtype=torch.float64), tensor(3.5969e-18, dtype=torch.float64), tensor(3.3134e-18, dtype=torch.float64), tensor(0.0562, dtype=torch.float64), tensor(5.0992e-17, dtype=torch.float64), tensor(0.3051, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.3156, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.311
  gsm8k: 0.017
  rowan_hellaswag: 0.31
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.056
  mmlu: 0
  arc_challenge: 0.305

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.03156347930556296,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.03156347930556296
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 90,112 || all params: 8,030,351,360 || trainable%: 0.0011
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:40,  4.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:21,  1.12it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:45,  1.83it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.66it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.78it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:15,  2.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.97it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.16it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:06,  3.06it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.81it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.0381, 'grad_norm': 6.418796062469482, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.483356475830078, 'eval_runtime': 8.674, 'eval_samples_per_second': 115.171, 'eval_steps_per_second': 7.263, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:47,  4.72s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:01,  1.47it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:38,  2.15it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:28,  2.65it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:28,  2.37it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:24,  2.36it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:20,  2.52it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.62it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.94it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.03it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:06,  2.97it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  3.15it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.17it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.72it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.979, 'grad_norm': 7.023415565490723, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.539623260498047, 'eval_runtime': 8.8771, 'eval_samples_per_second': 112.537, 'eval_steps_per_second': 7.097, 'epoch': 0.08}
{'loss': 2.2606, 'grad_norm': 12.985658645629883, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.083186149597168, 'eval_runtime': 8.8923, 'eval_samples_per_second': 112.344, 'eval_steps_per_second': 7.085, 'epoch': 0.12}
{'loss': 2.0277, 'grad_norm': 5.409685134887695, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9270412921905518, 'eval_runtime': 8.8405, 'eval_samples_per_second': 113.003, 'eval_steps_per_second': 7.126, 'epoch': 0.16}
{'loss': 1.9302, 'grad_norm': 9.379195213317871, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.849272608757019, 'eval_runtime': 8.8314, 'eval_samples_per_second': 113.12, 'eval_steps_per_second': 7.134, 'epoch': 0.2}
{'loss': 1.8362, 'grad_norm': 4.878628253936768, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.779673457145691, 'eval_runtime': 8.8385, 'eval_samples_per_second': 113.029, 'eval_steps_per_second': 7.128, 'epoch': 0.24}
{'loss': 1.7501, 'grad_norm': 4.784522533416748, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7312462329864502, 'eval_runtime': 8.7762, 'eval_samples_per_second': 113.831, 'eval_steps_per_second': 7.179, 'epoch': 0.28}
{'loss': 1.693, 'grad_norm': 3.296600103378296, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.675170660018921, 'eval_runtime': 8.7837, 'eval_samples_per_second': 113.734, 'eval_steps_per_second': 7.172, 'epoch': 0.32}
{'loss': 1.6591, 'grad_norm': 3.8596346378326416, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5989700555801392, 'eval_runtime': 8.7487, 'eval_samples_per_second': 114.188, 'eval_steps_per_second': 7.201, 'epoch': 0.36}
{'loss': 1.5925, 'grad_norm': 4.0761027336120605, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5667310953140259, 'eval_runtime': 8.6941, 'eval_samples_per_second': 114.906, 'eval_steps_per_second': 7.246, 'epoch': 0.4}
{'loss': 1.5205, 'grad_norm': 3.127734422683716, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5260189771652222, 'eval_runtime': 8.7469, 'eval_samples_per_second': 114.212, 'eval_steps_per_second': 7.203, 'epoch': 0.44}
{'loss': 1.4923, 'grad_norm': 2.704216241836548, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5124071836471558, 'eval_runtime': 8.7599, 'eval_samples_per_second': 114.042, 'eval_steps_per_second': 7.192, 'epoch': 0.48}
{'loss': 1.5275, 'grad_norm': 4.259409427642822, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4965505599975586, 'eval_runtime': 8.7565, 'eval_samples_per_second': 114.087, 'eval_steps_per_second': 7.195, 'epoch': 0.52}
{'loss': 1.4578, 'grad_norm': 4.115938186645508, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4894707202911377, 'eval_runtime': 8.833, 'eval_samples_per_second': 113.098, 'eval_steps_per_second': 7.132, 'epoch': 0.56}
{'loss': 1.4657, 'grad_norm': 2.2824227809906006, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4839067459106445, 'eval_runtime': 8.8312, 'eval_samples_per_second': 113.122, 'eval_steps_per_second': 7.134, 'epoch': 0.6}
{'loss': 1.4825, 'grad_norm': 3.596898078918457, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4833099842071533, 'eval_runtime': 8.9179, 'eval_samples_per_second': 112.022, 'eval_steps_per_second': 7.064, 'epoch': 0.64}
{'loss': 1.4481, 'grad_norm': 3.721176862716675, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4728082418441772, 'eval_runtime': 8.9653, 'eval_samples_per_second': 111.43, 'eval_steps_per_second': 7.027, 'epoch': 0.68}
{'loss': 1.486, 'grad_norm': 2.3725433349609375, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4676556587219238, 'eval_runtime': 8.9112, 'eval_samples_per_second': 112.105, 'eval_steps_per_second': 7.07, 'epoch': 0.72}
{'loss': 1.4924, 'grad_norm': 2.5809223651885986, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4626227617263794, 'eval_runtime': 8.8605, 'eval_samples_per_second': 112.747, 'eval_steps_per_second': 7.11, 'epoch': 0.76}
{'loss': 1.4193, 'grad_norm': 2.751758098602295, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4614404439926147, 'eval_runtime': 8.873, 'eval_samples_per_second': 112.588, 'eval_steps_per_second': 7.1, 'epoch': 0.8}
{'loss': 1.4804, 'grad_norm': 2.852360725402832, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4560596942901611, 'eval_runtime': 8.8569, 'eval_samples_per_second': 112.794, 'eval_steps_per_second': 7.113, 'epoch': 0.84}
{'loss': 1.4798, 'grad_norm': 3.7546982765197754, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.453456997871399, 'eval_runtime': 8.8573, 'eval_samples_per_second': 112.788, 'eval_steps_per_second': 7.113, 'epoch': 0.88}
{'loss': 1.4294, 'grad_norm': 3.4038801193237305, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4510574340820312, 'eval_runtime': 8.8627, 'eval_samples_per_second': 112.72, 'eval_steps_per_second': 7.108, 'epoch': 0.92}
{'loss': 1.4367, 'grad_norm': 2.653198719024658, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.447803020477295, 'eval_runtime': 8.7732, 'eval_samples_per_second': 113.869, 'eval_steps_per_second': 7.181, 'epoch': 0.96}
{'loss': 1.4341, 'grad_norm': 2.2975831031799316, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4472113847732544, 'eval_runtime': 8.7405, 'eval_samples_per_second': 114.295, 'eval_steps_per_second': 7.208, 'epoch': 1.0}
{'train_runtime': 517.909, 'train_samples_per_second': 19.305, 'train_steps_per_second': 1.207, 'train_loss': 1.75275849609375, 'epoch': 1.0}
train_results:  {'eval_loss': [3.483356475830078, 2.539623260498047, 2.083186149597168, 1.9270412921905518, 1.849272608757019, 1.779673457145691, 1.7312462329864502, 1.675170660018921, 1.5989700555801392, 1.5667310953140259, 1.5260189771652222, 1.5124071836471558, 1.4965505599975586, 1.4894707202911377, 1.4839067459106445, 1.4833099842071533, 1.4728082418441772, 1.4676556587219238, 1.4626227617263794, 1.4614404439926147, 1.4560596942901611, 1.453456997871399, 1.4510574340820312, 1.447803020477295, 1.4472113847732544], 'performance': [0.77, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:55,  9.65s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:02,  1.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:22<00:37,  1.79it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:22,  2.22it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:14,  2.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:06,  2.75it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.59it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.55it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  1.2157247066497803
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9354 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.8632796794833972
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.8827887654589563
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.8884362866119214
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.883873076006973
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.8786002799919939
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5448, dtype=torch.float64), tensor(0.3364, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.1189, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 2.2117724318704293e-18, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(6.9493e-17, dtype=torch.float64), tensor(0.5448, dtype=torch.float64), tensor(0.3364, dtype=torch.float64), tensor(6.1361e-18, dtype=torch.float64), tensor(1.3942e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.7038e-17, dtype=torch.float64), tensor(2.7627e-17, dtype=torch.float64), tensor(0.1189, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(2.2118e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.545
  rowan_hellaswag: 0.336
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.119

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (2.2117724318704293e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  2.2117724318704293e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:07,  2.50s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:27,  2.44it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:23,  2.53it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.72it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:10,  3.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.29it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.81it/s]
Evaluation performance at step 25: 0.78
{'loss': 3.2221, 'grad_norm': 3.84739089012146, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 2.8424441814422607, 'eval_runtime': 8.9042, 'eval_samples_per_second': 112.194, 'eval_steps_per_second': 7.075, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:57,  3.61s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:17,  1.17it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:43,  1.89it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.64it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:21,  2.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.66it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:15,  2.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  2.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.29it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.75it/s]
Evaluation performance at step 50: 0.8
{'loss': 2.5478, 'grad_norm': 3.3978381156921387, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.8}
{'eval_loss': 2.1877498626708984, 'eval_runtime': 8.9159, 'eval_samples_per_second': 112.047, 'eval_steps_per_second': 7.066, 'epoch': 0.08}
{'loss': 1.9651, 'grad_norm': 7.335197925567627, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8799635171890259, 'eval_runtime': 8.943, 'eval_samples_per_second': 111.707, 'eval_steps_per_second': 7.045, 'epoch': 0.12}
{'loss': 1.8534, 'grad_norm': 7.315997123718262, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7143535614013672, 'eval_runtime': 8.9806, 'eval_samples_per_second': 111.24, 'eval_steps_per_second': 7.015, 'epoch': 0.16}
{'loss': 1.678, 'grad_norm': 4.630434513092041, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6572388410568237, 'eval_runtime': 8.9908, 'eval_samples_per_second': 111.113, 'eval_steps_per_second': 7.007, 'epoch': 0.2}
{'loss': 1.6708, 'grad_norm': 6.684574127197266, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6275408267974854, 'eval_runtime': 8.9688, 'eval_samples_per_second': 111.386, 'eval_steps_per_second': 7.024, 'epoch': 0.24}
{'loss': 1.6181, 'grad_norm': 5.144291877746582, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5815538167953491, 'eval_runtime': 8.9789, 'eval_samples_per_second': 111.261, 'eval_steps_per_second': 7.016, 'epoch': 0.28}
{'loss': 1.541, 'grad_norm': 5.300172328948975, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5576176643371582, 'eval_runtime': 8.9578, 'eval_samples_per_second': 111.523, 'eval_steps_per_second': 7.033, 'epoch': 0.32}
{'loss': 1.5565, 'grad_norm': 7.065187454223633, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5426445007324219, 'eval_runtime': 8.981, 'eval_samples_per_second': 111.235, 'eval_steps_per_second': 7.015, 'epoch': 0.36}
{'loss': 1.5297, 'grad_norm': 3.9931366443634033, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.524703025817871, 'eval_runtime': 8.9545, 'eval_samples_per_second': 111.564, 'eval_steps_per_second': 7.036, 'epoch': 0.4}
{'loss': 1.4974, 'grad_norm': 7.197834014892578, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5147509574890137, 'eval_runtime': 8.9704, 'eval_samples_per_second': 111.366, 'eval_steps_per_second': 7.023, 'epoch': 0.44}
{'loss': 1.5024, 'grad_norm': 5.360472679138184, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4980052709579468, 'eval_runtime': 8.966, 'eval_samples_per_second': 111.421, 'eval_steps_per_second': 7.027, 'epoch': 0.48}
{'loss': 1.5055, 'grad_norm': 7.620611190795898, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.492876648902893, 'eval_runtime': 8.9618, 'eval_samples_per_second': 111.473, 'eval_steps_per_second': 7.03, 'epoch': 0.52}
{'loss': 1.5222, 'grad_norm': 5.2194294929504395, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4832139015197754, 'eval_runtime': 8.9866, 'eval_samples_per_second': 111.166, 'eval_steps_per_second': 7.01, 'epoch': 0.56}
{'loss': 1.4684, 'grad_norm': 7.3711934089660645, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4679296016693115, 'eval_runtime': 8.9779, 'eval_samples_per_second': 111.274, 'eval_steps_per_second': 7.017, 'epoch': 0.6}
{'loss': 1.4141, 'grad_norm': 3.843080997467041, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4520117044448853, 'eval_runtime': 8.9596, 'eval_samples_per_second': 111.501, 'eval_steps_per_second': 7.032, 'epoch': 0.64}
{'loss': 1.4876, 'grad_norm': 4.320308208465576, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4290636777877808, 'eval_runtime': 8.9486, 'eval_samples_per_second': 111.638, 'eval_steps_per_second': 7.04, 'epoch': 0.68}
{'loss': 1.4552, 'grad_norm': 3.5482594966888428, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4059809446334839, 'eval_runtime': 8.9612, 'eval_samples_per_second': 111.481, 'eval_steps_per_second': 7.03, 'epoch': 0.72}
{'loss': 1.3868, 'grad_norm': 3.6844725608825684, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4017720222473145, 'eval_runtime': 8.9763, 'eval_samples_per_second': 111.293, 'eval_steps_per_second': 7.018, 'epoch': 0.76}
{'loss': 1.3638, 'grad_norm': 7.745449066162109, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3985894918441772, 'eval_runtime': 8.9634, 'eval_samples_per_second': 111.454, 'eval_steps_per_second': 7.029, 'epoch': 0.8}
{'loss': 1.3945, 'grad_norm': 3.6155333518981934, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3942863941192627, 'eval_runtime': 8.9611, 'eval_samples_per_second': 111.482, 'eval_steps_per_second': 7.03, 'epoch': 0.84}
{'loss': 1.425, 'grad_norm': 5.935733795166016, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3888622522354126, 'eval_runtime': 8.9935, 'eval_samples_per_second': 111.08, 'eval_steps_per_second': 7.005, 'epoch': 0.88}
{'loss': 1.3918, 'grad_norm': 2.843477725982666, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.386041283607483, 'eval_runtime': 8.9876, 'eval_samples_per_second': 111.153, 'eval_steps_per_second': 7.01, 'epoch': 0.92}
{'loss': 1.3731, 'grad_norm': 3.6505675315856934, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3841077089309692, 'eval_runtime': 9.0209, 'eval_samples_per_second': 110.743, 'eval_steps_per_second': 6.984, 'epoch': 0.96}
{'loss': 1.3652, 'grad_norm': 3.7450568675994873, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3830077648162842, 'eval_runtime': 8.9741, 'eval_samples_per_second': 111.321, 'eval_steps_per_second': 7.02, 'epoch': 1.0}
{'train_runtime': 526.3153, 'train_samples_per_second': 18.996, 'train_steps_per_second': 1.188, 'train_loss': 1.6294283325195313, 'epoch': 1.0}
train_results:  {'eval_loss': [2.8424441814422607, 2.1877498626708984, 1.8799635171890259, 1.7143535614013672, 1.6572388410568237, 1.6275408267974854, 1.5815538167953491, 1.5576176643371582, 1.5426445007324219, 1.524703025817871, 1.5147509574890137, 1.4980052709579468, 1.492876648902893, 1.4832139015197754, 1.4679296016693115, 1.4520117044448853, 1.4290636777877808, 1.4059809446334839, 1.4017720222473145, 1.3985894918441772, 1.3942863941192627, 1.3888622522354126, 1.386041283607483, 1.3841077089309692, 1.3830077648162842], 'performance': [0.78, 0.8]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<10:58,  6.65s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:53,  1.54it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:20<00:37,  1.79it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:24<00:21,  2.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:29<00:13,  2.65it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:06,  3.15it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.92it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.8]
current iteration observed (possibly low-fid or predicted) performance:  1.2277302742004395
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9968 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.8762929236953761
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.8762994751307303
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.8623608400343608
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.8724475601567572
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.8762948531098134
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.6845, dtype=torch.float64), 0, tensor(0.3068, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 2, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.6845, dtype=torch.float64), tensor(0.0087, dtype=torch.float64), tensor(0.3068, dtype=torch.float64), tensor(1.0588e-16, dtype=torch.float64), tensor(1.9697e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5698e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.5630e-05, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.684
  gsm8k: 0
  rowan_hellaswag: 0.307
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 90,112 || all params: 8,030,351,360 || trainable%: 0.0011
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9911
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  991
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:36,  2.80s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:11,  1.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.00it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.54it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:27,  2.42it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:20<00:27,  2.14it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:21,  2.32it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:17,  2.52it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:12,  2.89it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.11it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:06,  3.09it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  3.22it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.75it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.3187, 'grad_norm': 9.558693885803223, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.6414709091186523, 'eval_runtime': 8.7386, 'eval_samples_per_second': 113.405, 'eval_steps_per_second': 7.095, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:02,  3.05s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.94it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:28,  2.36it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:24,  2.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.67it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  3.02it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:05,  3.18it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.48it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.82it/s]
Evaluation performance at step 50: 0.78
{'loss': 3.084, 'grad_norm': 40.504032135009766, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 2.4930078983306885, 'eval_runtime': 8.6973, 'eval_samples_per_second': 113.944, 'eval_steps_per_second': 7.129, 'epoch': 0.08}
{'loss': 2.1388, 'grad_norm': 10.907712936401367, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.99301016330719, 'eval_runtime': 8.7061, 'eval_samples_per_second': 113.828, 'eval_steps_per_second': 7.121, 'epoch': 0.12}
{'loss': 1.9452, 'grad_norm': 10.521978378295898, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.8730411529541016, 'eval_runtime': 8.7384, 'eval_samples_per_second': 113.408, 'eval_steps_per_second': 7.095, 'epoch': 0.16}
{'loss': 1.7532, 'grad_norm': 5.1269450187683105, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 1.8088984489440918, 'eval_runtime': 8.7758, 'eval_samples_per_second': 112.924, 'eval_steps_per_second': 7.065, 'epoch': 0.2}
{'loss': 1.7761, 'grad_norm': 9.014575958251953, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 1.7354416847229004, 'eval_runtime': 8.7907, 'eval_samples_per_second': 112.733, 'eval_steps_per_second': 7.053, 'epoch': 0.24}
{'loss': 1.7103, 'grad_norm': 4.686811923980713, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 1.6833813190460205, 'eval_runtime': 8.8035, 'eval_samples_per_second': 112.57, 'eval_steps_per_second': 7.043, 'epoch': 0.28}
{'loss': 1.6381, 'grad_norm': 4.379027366638184, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 1.6083028316497803, 'eval_runtime': 8.8051, 'eval_samples_per_second': 112.549, 'eval_steps_per_second': 7.041, 'epoch': 0.32}
{'loss': 1.6051, 'grad_norm': 3.520720958709717, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 1.5896755456924438, 'eval_runtime': 8.8151, 'eval_samples_per_second': 112.421, 'eval_steps_per_second': 7.033, 'epoch': 0.36}
{'loss': 1.5862, 'grad_norm': 4.947708606719971, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 1.561338186264038, 'eval_runtime': 8.8231, 'eval_samples_per_second': 112.319, 'eval_steps_per_second': 7.027, 'epoch': 0.4}
{'loss': 1.5734, 'grad_norm': 6.175703525543213, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 1.5496265888214111, 'eval_runtime': 8.8106, 'eval_samples_per_second': 112.478, 'eval_steps_per_second': 7.037, 'epoch': 0.44}
{'loss': 1.5424, 'grad_norm': 2.7265465259552, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 1.5292468070983887, 'eval_runtime': 8.825, 'eval_samples_per_second': 112.295, 'eval_steps_per_second': 7.025, 'epoch': 0.48}
{'loss': 1.5223, 'grad_norm': 4.345482349395752, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 1.508875846862793, 'eval_runtime': 8.8035, 'eval_samples_per_second': 112.569, 'eval_steps_per_second': 7.043, 'epoch': 0.52}
{'loss': 1.4879, 'grad_norm': 8.036744117736816, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 1.4848546981811523, 'eval_runtime': 8.8106, 'eval_samples_per_second': 112.478, 'eval_steps_per_second': 7.037, 'epoch': 0.56}
{'loss': 1.4031, 'grad_norm': 5.640336036682129, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 1.4740909337997437, 'eval_runtime': 8.8062, 'eval_samples_per_second': 112.534, 'eval_steps_per_second': 7.04, 'epoch': 0.6}
{'loss': 1.4231, 'grad_norm': 4.949799060821533, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 1.4733806848526, 'eval_runtime': 8.8001, 'eval_samples_per_second': 112.613, 'eval_steps_per_second': 7.045, 'epoch': 0.65}
{'loss': 1.4072, 'grad_norm': 5.118355751037598, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 1.4616667032241821, 'eval_runtime': 8.7963, 'eval_samples_per_second': 112.661, 'eval_steps_per_second': 7.048, 'epoch': 0.69}
{'loss': 1.4913, 'grad_norm': 6.296799659729004, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 1.4509526491165161, 'eval_runtime': 8.7942, 'eval_samples_per_second': 112.688, 'eval_steps_per_second': 7.05, 'epoch': 0.73}
{'loss': 1.4506, 'grad_norm': 3.6865415573120117, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 1.450199007987976, 'eval_runtime': 8.7866, 'eval_samples_per_second': 112.785, 'eval_steps_per_second': 7.056, 'epoch': 0.77}
{'loss': 1.4759, 'grad_norm': 4.489234447479248, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 1.4456146955490112, 'eval_runtime': 8.8231, 'eval_samples_per_second': 112.319, 'eval_steps_per_second': 7.027, 'epoch': 0.81}
{'loss': 1.3977, 'grad_norm': 5.803640842437744, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 1.4435970783233643, 'eval_runtime': 8.853, 'eval_samples_per_second': 111.94, 'eval_steps_per_second': 7.003, 'epoch': 0.85}
{'loss': 1.4545, 'grad_norm': 4.367314338684082, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 1.440229892730713, 'eval_runtime': 8.8084, 'eval_samples_per_second': 112.506, 'eval_steps_per_second': 7.039, 'epoch': 0.89}
{'loss': 1.4446, 'grad_norm': 3.750009059906006, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 1.4374197721481323, 'eval_runtime': 8.8204, 'eval_samples_per_second': 112.353, 'eval_steps_per_second': 7.029, 'epoch': 0.93}
{'loss': 1.4023, 'grad_norm': 3.978588819503784, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 1.4356954097747803, 'eval_runtime': 8.8584, 'eval_samples_per_second': 111.871, 'eval_steps_per_second': 6.999, 'epoch': 0.97}
{'train_runtime': 505.2024, 'train_samples_per_second': 19.618, 'train_steps_per_second': 1.227, 'train_loss': 1.7415232689149918, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6414709091186523, 2.4930078983306885, 1.99301016330719, 1.8730411529541016, 1.8088984489440918, 1.7354416847229004, 1.6833813190460205, 1.6083028316497803, 1.5896755456924438, 1.561338186264038, 1.5496265888214111, 1.5292468070983887, 1.508875846862793, 1.4848546981811523, 1.4740909337997437, 1.4733806848526, 1.4616667032241821, 1.4509526491165161, 1.450199007987976, 1.4456146955490112, 1.4435970783233643, 1.440229892730713, 1.4374197721481323, 1.4356954097747803], 'performance': [0.77, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<09:58,  6.05s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:13<00:56,  1.46it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:19<00:34,  1.94it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:20,  2.52it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:30<00:14,  2.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:38<00:08,  2.31it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:01,  2.97it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.47it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  1.2087997198104858
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3323 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.8456442909008044
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.8403843004289946
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.8641475428953089
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.878412775109071
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.8707707087050998
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1647, dtype=torch.float64), 0, tensor(0.2944, dtype=torch.float64), 0, 0, 0, 0, tensor(0.0544, dtype=torch.float64), tensor(0.4851, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.01742200595099158, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.1647, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2944, dtype=torch.float64), tensor(0.0014, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.8201e-19, dtype=torch.float64), tensor(2.8726e-17, dtype=torch.float64), tensor(0.0544, dtype=torch.float64), tensor(0.4851, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.1742, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.165
  gsm8k: 0
  rowan_hellaswag: 0.294
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.054
  arc_challenge: 0.485

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.01742200595099158,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.01742200595099158
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9983
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  998
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:02,  2.45s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.68it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:20,  2.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.97it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:10,  3.32it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:07,  3.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.70it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.99it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.32it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.91it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.0831, 'grad_norm': 5.207871437072754, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 3.5533041954040527, 'eval_runtime': 8.7427, 'eval_samples_per_second': 114.152, 'eval_steps_per_second': 7.206, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:28,  4.53s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:09<01:24,  1.07it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.78it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:32,  2.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:25,  2.58it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:22,  2.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:18,  2.69it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:11,  2.94it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.16it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.65it/s]
Evaluation performance at step 50: 0.78
{'loss': 3.0717, 'grad_norm': 6.2926554679870605, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 2.6195380687713623, 'eval_runtime': 8.7532, 'eval_samples_per_second': 114.015, 'eval_steps_per_second': 7.197, 'epoch': 0.08}
{'loss': 2.3097, 'grad_norm': 9.176942825317383, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 2.11962890625, 'eval_runtime': 8.8448, 'eval_samples_per_second': 112.835, 'eval_steps_per_second': 7.123, 'epoch': 0.12}
{'loss': 1.95, 'grad_norm': 10.31811809539795, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.9189872741699219, 'eval_runtime': 8.856, 'eval_samples_per_second': 112.692, 'eval_steps_per_second': 7.114, 'epoch': 0.16}
{'loss': 1.8856, 'grad_norm': 7.073517322540283, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.8436338901519775, 'eval_runtime': 8.8694, 'eval_samples_per_second': 112.522, 'eval_steps_per_second': 7.103, 'epoch': 0.2}
{'loss': 1.7781, 'grad_norm': 12.774304389953613, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 1.7968792915344238, 'eval_runtime': 8.8325, 'eval_samples_per_second': 112.992, 'eval_steps_per_second': 7.133, 'epoch': 0.24}
{'loss': 1.7575, 'grad_norm': 10.820920944213867, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 1.7588797807693481, 'eval_runtime': 8.8297, 'eval_samples_per_second': 113.028, 'eval_steps_per_second': 7.135, 'epoch': 0.28}
{'loss': 1.7489, 'grad_norm': 6.023221969604492, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 1.7312546968460083, 'eval_runtime': 8.8428, 'eval_samples_per_second': 112.86, 'eval_steps_per_second': 7.124, 'epoch': 0.32}
{'loss': 1.728, 'grad_norm': 4.234077453613281, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 1.6936572790145874, 'eval_runtime': 8.8599, 'eval_samples_per_second': 112.643, 'eval_steps_per_second': 7.111, 'epoch': 0.36}
{'loss': 1.7242, 'grad_norm': 6.12371301651001, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 1.6647284030914307, 'eval_runtime': 8.8574, 'eval_samples_per_second': 112.674, 'eval_steps_per_second': 7.113, 'epoch': 0.4}
{'loss': 1.6814, 'grad_norm': 5.895061016082764, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 1.6427820920944214, 'eval_runtime': 8.84, 'eval_samples_per_second': 112.896, 'eval_steps_per_second': 7.127, 'epoch': 0.44}
{'loss': 1.631, 'grad_norm': 9.984090805053711, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 1.6278173923492432, 'eval_runtime': 8.8436, 'eval_samples_per_second': 112.849, 'eval_steps_per_second': 7.124, 'epoch': 0.48}
{'loss': 1.6203, 'grad_norm': 3.122810125350952, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 1.6108124256134033, 'eval_runtime': 8.8509, 'eval_samples_per_second': 112.757, 'eval_steps_per_second': 7.118, 'epoch': 0.52}
{'loss': 1.6052, 'grad_norm': 6.536059856414795, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 1.5981227159500122, 'eval_runtime': 8.8367, 'eval_samples_per_second': 112.938, 'eval_steps_per_second': 7.129, 'epoch': 0.56}
{'loss': 1.6278, 'grad_norm': 5.827798843383789, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 1.5886425971984863, 'eval_runtime': 8.8368, 'eval_samples_per_second': 112.936, 'eval_steps_per_second': 7.129, 'epoch': 0.6}
{'loss': 1.5958, 'grad_norm': 8.116494178771973, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 1.5758849382400513, 'eval_runtime': 8.8571, 'eval_samples_per_second': 112.678, 'eval_steps_per_second': 7.113, 'epoch': 0.64}
{'loss': 1.5406, 'grad_norm': 5.62128210067749, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 1.5482274293899536, 'eval_runtime': 8.8441, 'eval_samples_per_second': 112.843, 'eval_steps_per_second': 7.123, 'epoch': 0.68}
{'loss': 1.523, 'grad_norm': 3.902876853942871, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 1.5069255828857422, 'eval_runtime': 8.8668, 'eval_samples_per_second': 112.554, 'eval_steps_per_second': 7.105, 'epoch': 0.72}
{'loss': 1.5382, 'grad_norm': 4.188431739807129, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 1.4974489212036133, 'eval_runtime': 8.9091, 'eval_samples_per_second': 112.02, 'eval_steps_per_second': 7.071, 'epoch': 0.76}
{'loss': 1.4398, 'grad_norm': 4.291591167449951, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 1.494315266609192, 'eval_runtime': 8.9784, 'eval_samples_per_second': 111.156, 'eval_steps_per_second': 7.017, 'epoch': 0.8}
{'loss': 1.5014, 'grad_norm': 3.7755074501037598, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 1.4844743013381958, 'eval_runtime': 8.9867, 'eval_samples_per_second': 111.053, 'eval_steps_per_second': 7.01, 'epoch': 0.84}
{'loss': 1.4521, 'grad_norm': 3.1657354831695557, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 1.480533480644226, 'eval_runtime': 9.0044, 'eval_samples_per_second': 110.835, 'eval_steps_per_second': 6.997, 'epoch': 0.88}
{'loss': 1.5128, 'grad_norm': 2.731776714324951, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 1.4762693643569946, 'eval_runtime': 9.0037, 'eval_samples_per_second': 110.843, 'eval_steps_per_second': 6.997, 'epoch': 0.92}
{'loss': 1.4378, 'grad_norm': 3.947049379348755, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 1.473797082901001, 'eval_runtime': 9.0068, 'eval_samples_per_second': 110.805, 'eval_steps_per_second': 6.995, 'epoch': 0.96}
{'train_runtime': 515.067, 'train_samples_per_second': 19.382, 'train_steps_per_second': 1.211, 'train_loss': 1.8076052421178572, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5533041954040527, 2.6195380687713623, 2.11962890625, 1.9189872741699219, 1.8436338901519775, 1.7968792915344238, 1.7588797807693481, 1.7312546968460083, 1.6936572790145874, 1.6647284030914307, 1.6427820920944214, 1.6278173923492432, 1.6108124256134033, 1.5981227159500122, 1.5886425971984863, 1.5758849382400513, 1.5482274293899536, 1.5069255828857422, 1.4974489212036133, 1.494315266609192, 1.4844743013381958, 1.480533480644226, 1.4762693643569946, 1.473797082901001], 'performance': [0.77, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<13:19,  8.08s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:16<01:11,  1.16it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:24<00:43,  1.53it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:31<00:27,  1.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:36<00:15,  2.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:39<00:07,  2.68it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:41<00:00,  3.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:41<00:00,  2.39it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  1.230318546295166
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2349 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 0.8632558917348224
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.8632398903824775
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 0.8639223622846876
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.8627851370594595
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.8636024252258652
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0602, dtype=torch.float64), 0, tensor(0.2915, dtype=torch.float64), tensor(0.0205, dtype=torch.float64), tensor(0.3231, dtype=torch.float64), 0, 0, 0, tensor(0.3047, dtype=torch.float64), 1, 1, 0, 1, 0, 1, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0602, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2915, dtype=torch.float64), tensor(0.0205, dtype=torch.float64), tensor(0.3231, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3047, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.06
  gsm8k: 0
  rowan_hellaswag: 0.291
  sciq: 0.02
  triviaqa: 0.323
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.305

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 90,112 || all params: 8,030,351,360 || trainable%: 0.0011
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:03,  2.46s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:09,  1.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  2.02it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:31,  2.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:28,  2.36it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.63it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.81it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.80it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  2.98it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:07,  3.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:05,  3.29it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.47it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.94it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.2117, 'grad_norm': 5.934469223022461, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.3606107234954834, 'eval_runtime': 8.5209, 'eval_samples_per_second': 117.242, 'eval_steps_per_second': 7.394, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:02,  3.05s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:14,  1.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:45,  1.82it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:31,  2.39it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:26,  2.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:23,  2.53it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.74it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.57it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:13,  2.68it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:09,  2.89it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.44it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:37<00:04,  2.41it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:01,  2.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.53it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.8819, 'grad_norm': 3.842536449432373, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.357118844985962, 'eval_runtime': 8.5894, 'eval_samples_per_second': 116.306, 'eval_steps_per_second': 7.335, 'epoch': 0.08}
{'loss': 2.0578, 'grad_norm': 3.4383511543273926, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.891084909439087, 'eval_runtime': 8.5865, 'eval_samples_per_second': 116.345, 'eval_steps_per_second': 7.337, 'epoch': 0.12}
{'loss': 1.7806, 'grad_norm': 2.1878957748413086, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7143093347549438, 'eval_runtime': 8.5832, 'eval_samples_per_second': 116.391, 'eval_steps_per_second': 7.34, 'epoch': 0.16}
{'loss': 1.649, 'grad_norm': 2.2987470626831055, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6424336433410645, 'eval_runtime': 8.6155, 'eval_samples_per_second': 115.954, 'eval_steps_per_second': 7.312, 'epoch': 0.2}
{'loss': 1.6343, 'grad_norm': 2.3043339252471924, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6205464601516724, 'eval_runtime': 8.6732, 'eval_samples_per_second': 115.183, 'eval_steps_per_second': 7.264, 'epoch': 0.24}
{'loss': 1.6308, 'grad_norm': 1.6937623023986816, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6044028997421265, 'eval_runtime': 8.7143, 'eval_samples_per_second': 114.64, 'eval_steps_per_second': 7.23, 'epoch': 0.28}
{'loss': 1.6198, 'grad_norm': 1.939139485359192, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5926798582077026, 'eval_runtime': 8.6944, 'eval_samples_per_second': 114.901, 'eval_steps_per_second': 7.246, 'epoch': 0.32}
{'loss': 1.5512, 'grad_norm': 1.9003664255142212, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.584265947341919, 'eval_runtime': 8.6543, 'eval_samples_per_second': 115.434, 'eval_steps_per_second': 7.28, 'epoch': 0.36}
{'loss': 1.5256, 'grad_norm': 1.9061551094055176, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.577649474143982, 'eval_runtime': 8.65, 'eval_samples_per_second': 115.492, 'eval_steps_per_second': 7.283, 'epoch': 0.4}
{'loss': 1.5255, 'grad_norm': 2.07067608833313, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5699681043624878, 'eval_runtime': 8.6657, 'eval_samples_per_second': 115.282, 'eval_steps_per_second': 7.27, 'epoch': 0.44}
{'loss': 1.6076, 'grad_norm': 2.0881810188293457, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5664935111999512, 'eval_runtime': 8.6501, 'eval_samples_per_second': 115.49, 'eval_steps_per_second': 7.283, 'epoch': 0.48}
{'loss': 1.5835, 'grad_norm': 1.9113596677780151, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.56098210811615, 'eval_runtime': 8.6529, 'eval_samples_per_second': 115.452, 'eval_steps_per_second': 7.281, 'epoch': 0.52}
{'loss': 1.5144, 'grad_norm': 2.2984070777893066, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.55613112449646, 'eval_runtime': 8.6562, 'eval_samples_per_second': 115.409, 'eval_steps_per_second': 7.278, 'epoch': 0.56}
{'loss': 1.5464, 'grad_norm': 1.9109901189804077, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5525133609771729, 'eval_runtime': 8.6451, 'eval_samples_per_second': 115.556, 'eval_steps_per_second': 7.287, 'epoch': 0.6}
{'loss': 1.5266, 'grad_norm': 2.0168895721435547, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5491743087768555, 'eval_runtime': 8.652, 'eval_samples_per_second': 115.464, 'eval_steps_per_second': 7.282, 'epoch': 0.64}
{'loss': 1.563, 'grad_norm': 1.8424934148788452, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5477346181869507, 'eval_runtime': 8.6447, 'eval_samples_per_second': 115.562, 'eval_steps_per_second': 7.288, 'epoch': 0.68}
{'loss': 1.5306, 'grad_norm': 1.7565364837646484, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5446816682815552, 'eval_runtime': 8.6841, 'eval_samples_per_second': 115.037, 'eval_steps_per_second': 7.255, 'epoch': 0.72}
{'loss': 1.5536, 'grad_norm': 1.8617980480194092, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5423946380615234, 'eval_runtime': 8.7125, 'eval_samples_per_second': 114.663, 'eval_steps_per_second': 7.231, 'epoch': 0.76}
{'loss': 1.4947, 'grad_norm': 1.9590924978256226, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.540439486503601, 'eval_runtime': 8.6753, 'eval_samples_per_second': 115.155, 'eval_steps_per_second': 7.262, 'epoch': 0.8}
{'loss': 1.5714, 'grad_norm': 1.653317928314209, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5381118059158325, 'eval_runtime': 8.6951, 'eval_samples_per_second': 114.892, 'eval_steps_per_second': 7.245, 'epoch': 0.84}
{'loss': 1.527, 'grad_norm': 1.9603914022445679, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.537173867225647, 'eval_runtime': 8.6988, 'eval_samples_per_second': 114.843, 'eval_steps_per_second': 7.242, 'epoch': 0.88}
{'loss': 1.5738, 'grad_norm': 1.9276069402694702, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5360262393951416, 'eval_runtime': 8.6926, 'eval_samples_per_second': 114.925, 'eval_steps_per_second': 7.248, 'epoch': 0.92}
{'loss': 1.4713, 'grad_norm': 1.9192523956298828, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5348283052444458, 'eval_runtime': 8.6703, 'eval_samples_per_second': 115.221, 'eval_steps_per_second': 7.266, 'epoch': 0.96}
{'loss': 1.5382, 'grad_norm': 2.0499064922332764, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5347075462341309, 'eval_runtime': 8.6775, 'eval_samples_per_second': 115.125, 'eval_steps_per_second': 7.26, 'epoch': 1.0}
{'train_runtime': 415.352, 'train_samples_per_second': 24.069, 'train_steps_per_second': 1.505, 'train_loss': 1.7468088134765625, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3606107234954834, 2.357118844985962, 1.891084909439087, 1.7143093347549438, 1.6424336433410645, 1.6205464601516724, 1.6044028997421265, 1.5926798582077026, 1.584265947341919, 1.577649474143982, 1.5699681043624878, 1.5664935111999512, 1.56098210811615, 1.55613112449646, 1.5525133609771729, 1.5491743087768555, 1.5477346181869507, 1.5446816682815552, 1.5423946380615234, 1.540439486503601, 1.5381118059158325, 1.537173867225647, 1.5360262393951416, 1.5348283052444458, 1.5347075462341309], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:53,  9.63s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.80it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:53<00:01,  2.32it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.86it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  1.2250876426696777
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166, 1.2250876426696777]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1004 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 0.8380239245078607
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.8577206001011849
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 0.8563661544311141
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.8578967342958903
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.8561257889789481
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0136, dtype=torch.float64), 0, tensor(0.2824, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.7040, dtype=torch.float64), 1, 1, 0, 0, 0, 0, 2, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0136, dtype=torch.float64), tensor(1.6554e-16, dtype=torch.float64), tensor(0.2824, dtype=torch.float64), tensor(1.2215e-17, dtype=torch.float64), tensor(2.5291e-17, dtype=torch.float64), tensor(6.7981e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.2179e-17, dtype=torch.float64), tensor(0.7040, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.014
  gsm8k: 0
  rowan_hellaswag: 0.282
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.704

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 16,384 || all params: 8,030,277,632 || trainable%: 0.0002
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:02,  2.45s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:04<00:41,  2.19it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:29,  2.77it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:23,  3.17it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:11<00:22,  3.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:14<00:20,  2.89it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:16,  3.03it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:11,  3.13it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:08,  3.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:05,  3.24it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:29<00:03,  3.43it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:30<00:00,  3.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:30<00:00,  3.23it/s]
Evaluation performance at step 25: 0.77
{'loss': 4.1225, 'grad_norm': 1.4748128652572632, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.77}
{'eval_loss': 4.052690505981445, 'eval_runtime': 8.6997, 'eval_samples_per_second': 114.831, 'eval_steps_per_second': 7.242, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:30,  2.73s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:11,  1.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  1.99it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:23,  2.85it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.77it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.28it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:07,  3.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:07,  2.69it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  2.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.88it/s]
Evaluation performance at step 50: 0.77
{'loss': 3.9157, 'grad_norm': 1.2868385314941406, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 3.7372171878814697, 'eval_runtime': 8.7772, 'eval_samples_per_second': 113.817, 'eval_steps_per_second': 7.178, 'epoch': 0.08}
{'loss': 3.5422, 'grad_norm': 1.6606171131134033, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.317765235900879, 'eval_runtime': 8.7547, 'eval_samples_per_second': 114.111, 'eval_steps_per_second': 7.196, 'epoch': 0.12}
{'loss': 3.1901, 'grad_norm': 3.9883334636688232, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 3.1377124786376953, 'eval_runtime': 8.7442, 'eval_samples_per_second': 114.248, 'eval_steps_per_second': 7.205, 'epoch': 0.16}
{'loss': 3.1137, 'grad_norm': 4.582434177398682, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 3.058063507080078, 'eval_runtime': 8.7754, 'eval_samples_per_second': 113.841, 'eval_steps_per_second': 7.179, 'epoch': 0.2}
{'loss': 3.0155, 'grad_norm': 3.9216725826263428, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.9703752994537354, 'eval_runtime': 8.8585, 'eval_samples_per_second': 112.773, 'eval_steps_per_second': 7.112, 'epoch': 0.24}
{'loss': 2.8941, 'grad_norm': 2.3436570167541504, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.9102730751037598, 'eval_runtime': 8.9375, 'eval_samples_per_second': 111.776, 'eval_steps_per_second': 7.049, 'epoch': 0.28}
{'loss': 2.8733, 'grad_norm': 1.5284498929977417, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.8603715896606445, 'eval_runtime': 8.907, 'eval_samples_per_second': 112.16, 'eval_steps_per_second': 7.073, 'epoch': 0.32}
{'loss': 2.827, 'grad_norm': 1.233705759048462, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.815227508544922, 'eval_runtime': 8.9172, 'eval_samples_per_second': 112.031, 'eval_steps_per_second': 7.065, 'epoch': 0.36}
{'loss': 2.776, 'grad_norm': 3.586035966873169, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.775869607925415, 'eval_runtime': 8.9212, 'eval_samples_per_second': 111.98, 'eval_steps_per_second': 7.062, 'epoch': 0.4}
{'loss': 2.7654, 'grad_norm': 7.224945545196533, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.7360637187957764, 'eval_runtime': 8.889, 'eval_samples_per_second': 112.387, 'eval_steps_per_second': 7.087, 'epoch': 0.44}
{'loss': 2.7171, 'grad_norm': 7.048989772796631, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.706015110015869, 'eval_runtime': 8.8368, 'eval_samples_per_second': 113.05, 'eval_steps_per_second': 7.129, 'epoch': 0.48}
{'loss': 2.727, 'grad_norm': 5.309513568878174, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.6733460426330566, 'eval_runtime': 8.8005, 'eval_samples_per_second': 113.516, 'eval_steps_per_second': 7.159, 'epoch': 0.52}
{'loss': 2.6456, 'grad_norm': 1.5039753913879395, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.649785280227661, 'eval_runtime': 8.7784, 'eval_samples_per_second': 113.803, 'eval_steps_per_second': 7.177, 'epoch': 0.56}
{'loss': 2.627, 'grad_norm': 1.2956594228744507, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.6286163330078125, 'eval_runtime': 8.8218, 'eval_samples_per_second': 113.242, 'eval_steps_per_second': 7.141, 'epoch': 0.6}
{'loss': 2.6655, 'grad_norm': 4.671381950378418, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.6158504486083984, 'eval_runtime': 8.8303, 'eval_samples_per_second': 113.134, 'eval_steps_per_second': 7.135, 'epoch': 0.64}
{'loss': 2.5988, 'grad_norm': 4.5570387840271, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.6010286808013916, 'eval_runtime': 8.8639, 'eval_samples_per_second': 112.705, 'eval_steps_per_second': 7.108, 'epoch': 0.68}
{'loss': 2.6278, 'grad_norm': 2.02575945854187, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.5862438678741455, 'eval_runtime': 8.8001, 'eval_samples_per_second': 113.521, 'eval_steps_per_second': 7.159, 'epoch': 0.72}
{'loss': 2.642, 'grad_norm': 3.9620842933654785, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.5692949295043945, 'eval_runtime': 8.8234, 'eval_samples_per_second': 113.221, 'eval_steps_per_second': 7.14, 'epoch': 0.76}
{'loss': 2.6041, 'grad_norm': 3.4873464107513428, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.5534842014312744, 'eval_runtime': 8.8036, 'eval_samples_per_second': 113.476, 'eval_steps_per_second': 7.156, 'epoch': 0.8}
{'loss': 2.6349, 'grad_norm': 3.013410806655884, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.5420191287994385, 'eval_runtime': 8.8145, 'eval_samples_per_second': 113.336, 'eval_steps_per_second': 7.147, 'epoch': 0.84}
{'loss': 2.592, 'grad_norm': 2.1143949031829834, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.530839204788208, 'eval_runtime': 8.8127, 'eval_samples_per_second': 113.359, 'eval_steps_per_second': 7.149, 'epoch': 0.88}
{'loss': 2.5322, 'grad_norm': 1.8355265855789185, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.5215039253234863, 'eval_runtime': 8.8055, 'eval_samples_per_second': 113.452, 'eval_steps_per_second': 7.155, 'epoch': 0.92}
{'loss': 2.5774, 'grad_norm': 2.852351665496826, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.5175371170043945, 'eval_runtime': 8.8031, 'eval_samples_per_second': 113.482, 'eval_steps_per_second': 7.157, 'epoch': 0.96}
{'loss': 2.5098, 'grad_norm': 1.7370903491973877, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.5141372680664062, 'eval_runtime': 8.7903, 'eval_samples_per_second': 113.649, 'eval_steps_per_second': 7.167, 'epoch': 1.0}
{'train_runtime': 512.1202, 'train_samples_per_second': 19.525, 'train_steps_per_second': 1.22, 'train_loss': 2.869472509765625, 'epoch': 1.0}
train_results:  {'eval_loss': [4.052690505981445, 3.7372171878814697, 3.317765235900879, 3.1377124786376953, 3.058063507080078, 2.9703752994537354, 2.9102730751037598, 2.8603715896606445, 2.815227508544922, 2.775869607925415, 2.7360637187957764, 2.706015110015869, 2.6733460426330566, 2.649785280227661, 2.6286163330078125, 2.6158504486083984, 2.6010286808013916, 2.5862438678741455, 2.5692949295043945, 2.5534842014312744, 2.5420191287994385, 2.530839204788208, 2.5215039253234863, 2.5175371170043945, 2.5141372680664062], 'performance': [0.77, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:24,  5.71s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:12<00:51,  1.62it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:19<00:34,  1.95it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:26<00:25,  1.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:31<00:14,  2.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:35<00:06,  2.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.63it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.67it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.77, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  1.230519413948059
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166, 1.2250876426696777, 1.230519413948059]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2043 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.8574316993051755
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.8601932421697989
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.8530419361468613
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.859919727447761
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.8597077531324654
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2249, dtype=torch.float64), 0, tensor(0.2864, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.4887, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 3.252606517456514e-18, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.2249, dtype=torch.float64), tensor(8.6840e-17, dtype=torch.float64), tensor(0.2864, dtype=torch.float64), tensor(4.8522e-17, dtype=torch.float64), tensor(8.7673e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1327e-17, dtype=torch.float64), tensor(6.5244e-18, dtype=torch.float64), tensor(0.4887, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.2526e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.225
  gsm8k: 0
  rowan_hellaswag: 0.286
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.489

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.252606517456514e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  3.252606517456514e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:44,  2.87s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.94it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:26,  2.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.66it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:18,  2.76it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.92it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:07,  2.49it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:03,  2.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.69it/s]
Evaluation performance at step 25: 0.78
{'loss': 4.0856, 'grad_norm': 5.441146373748779, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 3.5577776432037354, 'eval_runtime': 8.835, 'eval_samples_per_second': 113.073, 'eval_steps_per_second': 7.131, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:06,  3.70s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:20,  1.14it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:45,  1.84it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:26,  2.55it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:22,  2.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:18,  2.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.94it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.32it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:05,  3.22it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.39it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.56it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.83it/s]
Evaluation performance at step 50: 0.8
{'loss': 3.0791, 'grad_norm': 4.535063743591309, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.8}
{'eval_loss': 2.6382124423980713, 'eval_runtime': 8.7507, 'eval_samples_per_second': 114.162, 'eval_steps_per_second': 7.199, 'epoch': 0.08}
{'loss': 2.3629, 'grad_norm': 17.65993881225586, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0854952335357666, 'eval_runtime': 8.7873, 'eval_samples_per_second': 113.687, 'eval_steps_per_second': 7.169, 'epoch': 0.12}
{'loss': 1.943, 'grad_norm': 11.940441131591797, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8517858982086182, 'eval_runtime': 8.8657, 'eval_samples_per_second': 112.681, 'eval_steps_per_second': 7.106, 'epoch': 0.16}
{'loss': 1.8921, 'grad_norm': 9.945318222045898, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7854115962982178, 'eval_runtime': 8.8145, 'eval_samples_per_second': 113.336, 'eval_steps_per_second': 7.147, 'epoch': 0.2}
{'loss': 1.7969, 'grad_norm': 11.443102836608887, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7404135465621948, 'eval_runtime': 8.8168, 'eval_samples_per_second': 113.307, 'eval_steps_per_second': 7.145, 'epoch': 0.24}
{'loss': 1.6881, 'grad_norm': 5.14171028137207, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7159796953201294, 'eval_runtime': 8.8218, 'eval_samples_per_second': 113.242, 'eval_steps_per_second': 7.141, 'epoch': 0.28}
{'loss': 1.7115, 'grad_norm': 8.271588325500488, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6841986179351807, 'eval_runtime': 8.803, 'eval_samples_per_second': 113.484, 'eval_steps_per_second': 7.157, 'epoch': 0.32}
{'loss': 1.6742, 'grad_norm': 13.964893341064453, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.667386770248413, 'eval_runtime': 8.8031, 'eval_samples_per_second': 113.483, 'eval_steps_per_second': 7.157, 'epoch': 0.36}
{'loss': 1.6689, 'grad_norm': 5.4283952713012695, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6459542512893677, 'eval_runtime': 8.799, 'eval_samples_per_second': 113.536, 'eval_steps_per_second': 7.16, 'epoch': 0.4}
{'loss': 1.6245, 'grad_norm': 5.0176005363464355, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6296882629394531, 'eval_runtime': 8.8041, 'eval_samples_per_second': 113.469, 'eval_steps_per_second': 7.156, 'epoch': 0.44}
{'loss': 1.64, 'grad_norm': 5.848258972167969, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.620811104774475, 'eval_runtime': 8.8263, 'eval_samples_per_second': 113.185, 'eval_steps_per_second': 7.138, 'epoch': 0.48}
{'loss': 1.6001, 'grad_norm': 4.416325092315674, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.602448582649231, 'eval_runtime': 8.8191, 'eval_samples_per_second': 113.276, 'eval_steps_per_second': 7.144, 'epoch': 0.52}
{'loss': 1.5862, 'grad_norm': 6.811209678649902, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5907586812973022, 'eval_runtime': 8.805, 'eval_samples_per_second': 113.458, 'eval_steps_per_second': 7.155, 'epoch': 0.56}
{'loss': 1.5764, 'grad_norm': 8.509300231933594, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5770481824874878, 'eval_runtime': 8.8105, 'eval_samples_per_second': 113.388, 'eval_steps_per_second': 7.151, 'epoch': 0.6}
{'loss': 1.6172, 'grad_norm': 4.1373090744018555, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5592433214187622, 'eval_runtime': 8.8064, 'eval_samples_per_second': 113.441, 'eval_steps_per_second': 7.154, 'epoch': 0.64}
{'loss': 1.5555, 'grad_norm': 5.26472806930542, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5428931713104248, 'eval_runtime': 8.808, 'eval_samples_per_second': 113.42, 'eval_steps_per_second': 7.153, 'epoch': 0.68}
{'loss': 1.5174, 'grad_norm': 5.0972771644592285, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5111541748046875, 'eval_runtime': 8.8114, 'eval_samples_per_second': 113.376, 'eval_steps_per_second': 7.15, 'epoch': 0.72}
{'loss': 1.5543, 'grad_norm': 4.605811595916748, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4779847860336304, 'eval_runtime': 8.8072, 'eval_samples_per_second': 113.429, 'eval_steps_per_second': 7.153, 'epoch': 0.76}
{'loss': 1.4922, 'grad_norm': 4.729788303375244, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4633407592773438, 'eval_runtime': 8.8162, 'eval_samples_per_second': 113.314, 'eval_steps_per_second': 7.146, 'epoch': 0.8}
{'loss': 1.443, 'grad_norm': 5.046387672424316, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.457640290260315, 'eval_runtime': 8.797, 'eval_samples_per_second': 113.562, 'eval_steps_per_second': 7.162, 'epoch': 0.84}
{'loss': 1.4326, 'grad_norm': 4.442418575286865, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4546476602554321, 'eval_runtime': 8.8158, 'eval_samples_per_second': 113.319, 'eval_steps_per_second': 7.146, 'epoch': 0.88}
{'loss': 1.4225, 'grad_norm': 3.4837071895599365, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4501276016235352, 'eval_runtime': 8.8, 'eval_samples_per_second': 113.523, 'eval_steps_per_second': 7.159, 'epoch': 0.92}
{'loss': 1.4452, 'grad_norm': 4.453866481781006, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4461685419082642, 'eval_runtime': 8.8061, 'eval_samples_per_second': 113.443, 'eval_steps_per_second': 7.154, 'epoch': 0.96}
{'loss': 1.4894, 'grad_norm': 3.407668113708496, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4453814029693604, 'eval_runtime': 8.8736, 'eval_samples_per_second': 112.581, 'eval_steps_per_second': 7.1, 'epoch': 1.0}
{'train_runtime': 525.376, 'train_samples_per_second': 19.03, 'train_steps_per_second': 1.19, 'train_loss': 1.7959573547363281, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5577776432037354, 2.6382124423980713, 2.0854952335357666, 1.8517858982086182, 1.7854115962982178, 1.7404135465621948, 1.7159796953201294, 1.6841986179351807, 1.667386770248413, 1.6459542512893677, 1.6296882629394531, 1.620811104774475, 1.602448582649231, 1.5907586812973022, 1.5770481824874878, 1.5592433214187622, 1.5428931713104248, 1.5111541748046875, 1.4779847860336304, 1.4633407592773438, 1.457640290260315, 1.4546476602554321, 1.4501276016235352, 1.4461685419082642, 1.4453814029693604], 'performance': [0.78, 0.8]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<10:51,  6.58s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:15<01:05,  1.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:23<00:41,  1.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:26,  1.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:33<00:14,  2.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:06,  2.80it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.53it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.8]
current iteration observed (possibly low-fid or predicted) performance:  1.230392336845398
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166, 1.2250876426696777, 1.230519413948059, 1.230392336845398]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.4781 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.855702284247482
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.847943021285707
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.8552482074372015
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.8547223581574771
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.8549770294793595
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0196, dtype=torch.float64), tensor(0.2571, dtype=torch.float64), tensor(0.0192, dtype=torch.float64), 0, tensor(0.3743, dtype=torch.float64), 0, 0, tensor(0.3297, dtype=torch.float64), 1, 1, 0, 1, 0, 1, 2, 0.016218862237604357, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.3527e-17, dtype=torch.float64), tensor(0.0196, dtype=torch.float64), tensor(0.2571, dtype=torch.float64), tensor(0.0192, dtype=torch.float64), tensor(2.0715e-17, dtype=torch.float64), tensor(0.3743, dtype=torch.float64), tensor(4.2999e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3297, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.1622, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.02
  rowan_hellaswag: 0.257
  sciq: 0.019
  triviaqa: 0
  truthfulqa_gen: 0.374
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.33

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.016218862237604357,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.016218862237604357
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 90,112 || all params: 8,030,351,360 || trainable%: 0.0011
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:37,  2.81s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:46,  1.97it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:31,  2.65it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:09<00:24,  3.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  3.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:14<00:19,  3.07it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:17,  2.88it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:11,  3.18it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:08,  3.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:29<00:07,  2.65it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:31<00:03,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:32<00:00,  3.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:32<00:00,  3.03it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.171, 'grad_norm': 7.166294097900391, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.479262351989746, 'eval_runtime': 8.607, 'eval_samples_per_second': 116.068, 'eval_steps_per_second': 7.32, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<05:00,  3.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.95it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.73it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.74it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.84it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:05,  3.27it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.44it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  3.67it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  2.94it/s]
Evaluation performance at step 50: 0.78
{'loss': 2.9964, 'grad_norm': 8.963844299316406, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 2.4656174182891846, 'eval_runtime': 8.6068, 'eval_samples_per_second': 116.071, 'eval_steps_per_second': 7.32, 'epoch': 0.08}
{'loss': 2.2626, 'grad_norm': 10.516341209411621, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0665857791900635, 'eval_runtime': 8.614, 'eval_samples_per_second': 115.973, 'eval_steps_per_second': 7.314, 'epoch': 0.12}
{'loss': 1.9156, 'grad_norm': 8.419053077697754, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.90506112575531, 'eval_runtime': 8.5952, 'eval_samples_per_second': 116.228, 'eval_steps_per_second': 7.33, 'epoch': 0.16}
{'loss': 1.8146, 'grad_norm': 3.9915151596069336, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8035054206848145, 'eval_runtime': 8.6465, 'eval_samples_per_second': 115.538, 'eval_steps_per_second': 7.286, 'epoch': 0.2}
{'loss': 1.7459, 'grad_norm': 12.283777236938477, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7612512111663818, 'eval_runtime': 8.649, 'eval_samples_per_second': 115.504, 'eval_steps_per_second': 7.284, 'epoch': 0.24}
{'loss': 1.6993, 'grad_norm': 6.289682388305664, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6700865030288696, 'eval_runtime': 8.6619, 'eval_samples_per_second': 115.332, 'eval_steps_per_second': 7.273, 'epoch': 0.28}
{'loss': 1.6126, 'grad_norm': 4.817105770111084, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5717061758041382, 'eval_runtime': 8.6711, 'eval_samples_per_second': 115.21, 'eval_steps_per_second': 7.266, 'epoch': 0.32}
{'loss': 1.5295, 'grad_norm': 3.947890520095825, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5386652946472168, 'eval_runtime': 8.6519, 'eval_samples_per_second': 115.465, 'eval_steps_per_second': 7.282, 'epoch': 0.36}
{'loss': 1.5313, 'grad_norm': 5.167840003967285, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.510463833808899, 'eval_runtime': 8.6556, 'eval_samples_per_second': 115.417, 'eval_steps_per_second': 7.279, 'epoch': 0.4}
{'loss': 1.4824, 'grad_norm': 3.659914016723633, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4674173593521118, 'eval_runtime': 8.6586, 'eval_samples_per_second': 115.377, 'eval_steps_per_second': 7.276, 'epoch': 0.44}
{'loss': 1.4579, 'grad_norm': 3.131455659866333, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.443238377571106, 'eval_runtime': 8.6622, 'eval_samples_per_second': 115.329, 'eval_steps_per_second': 7.273, 'epoch': 0.48}
{'loss': 1.448, 'grad_norm': 3.1150283813476562, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4329283237457275, 'eval_runtime': 8.6504, 'eval_samples_per_second': 115.486, 'eval_steps_per_second': 7.283, 'epoch': 0.52}
{'loss': 1.3745, 'grad_norm': 3.586749792098999, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.423719882965088, 'eval_runtime': 8.6473, 'eval_samples_per_second': 115.527, 'eval_steps_per_second': 7.286, 'epoch': 0.56}
{'loss': 1.4272, 'grad_norm': 3.1695563793182373, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.417500615119934, 'eval_runtime': 8.6446, 'eval_samples_per_second': 115.564, 'eval_steps_per_second': 7.288, 'epoch': 0.6}
{'loss': 1.4106, 'grad_norm': 3.45219349861145, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4099570512771606, 'eval_runtime': 8.6466, 'eval_samples_per_second': 115.536, 'eval_steps_per_second': 7.286, 'epoch': 0.64}
{'loss': 1.3816, 'grad_norm': 3.2640161514282227, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.40495765209198, 'eval_runtime': 8.6701, 'eval_samples_per_second': 115.223, 'eval_steps_per_second': 7.266, 'epoch': 0.68}
{'loss': 1.3818, 'grad_norm': 2.870619773864746, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4012547731399536, 'eval_runtime': 8.6652, 'eval_samples_per_second': 115.289, 'eval_steps_per_second': 7.27, 'epoch': 0.72}
{'loss': 1.3924, 'grad_norm': 2.939547300338745, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3986400365829468, 'eval_runtime': 8.673, 'eval_samples_per_second': 115.185, 'eval_steps_per_second': 7.264, 'epoch': 0.76}
{'loss': 1.3911, 'grad_norm': 2.2059483528137207, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3919371366500854, 'eval_runtime': 8.6266, 'eval_samples_per_second': 115.805, 'eval_steps_per_second': 7.303, 'epoch': 0.8}
{'loss': 1.3999, 'grad_norm': 3.7644705772399902, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3904215097427368, 'eval_runtime': 8.6642, 'eval_samples_per_second': 115.302, 'eval_steps_per_second': 7.271, 'epoch': 0.84}
{'loss': 1.3661, 'grad_norm': 3.0807125568389893, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.388287901878357, 'eval_runtime': 8.7258, 'eval_samples_per_second': 114.488, 'eval_steps_per_second': 7.22, 'epoch': 0.88}
{'loss': 1.3645, 'grad_norm': 2.6165456771850586, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.38468599319458, 'eval_runtime': 8.7761, 'eval_samples_per_second': 113.832, 'eval_steps_per_second': 7.179, 'epoch': 0.92}
{'loss': 1.3494, 'grad_norm': 2.979973793029785, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3826472759246826, 'eval_runtime': 8.797, 'eval_samples_per_second': 113.562, 'eval_steps_per_second': 7.162, 'epoch': 0.96}
{'loss': 1.406, 'grad_norm': 2.657590627670288, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3815114498138428, 'eval_runtime': 8.7875, 'eval_samples_per_second': 113.684, 'eval_steps_per_second': 7.169, 'epoch': 1.0}
{'train_runtime': 507.6816, 'train_samples_per_second': 19.693, 'train_steps_per_second': 1.231, 'train_loss': 1.6924821166992188, 'epoch': 1.0}
train_results:  {'eval_loss': [3.479262351989746, 2.4656174182891846, 2.0665857791900635, 1.90506112575531, 1.8035054206848145, 1.7612512111663818, 1.6700865030288696, 1.5717061758041382, 1.5386652946472168, 1.510463833808899, 1.4674173593521118, 1.443238377571106, 1.4329283237457275, 1.423719882965088, 1.417500615119934, 1.4099570512771606, 1.40495765209198, 1.4012547731399536, 1.3986400365829468, 1.3919371366500854, 1.3904215097427368, 1.388287901878357, 1.38468599319458, 1.3826472759246826, 1.3815114498138428], 'performance': [0.76, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<16:08,  9.79s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:17,  1.08it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:31<00:26,  1.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:39<00:17,  1.98it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:43<00:07,  2.44it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:44<00:00,  3.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:44<00:00,  2.22it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  1.2156025171279907
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166, 1.2250876426696777, 1.230519413948059, 1.230392336845398, 1.2156025171279907]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8279 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 0.8590840082967816
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.858409043652341
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 0.8589379211868295
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.856962852340168
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.8544101962401794
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1698, dtype=torch.float64), tensor(0.4247, dtype=torch.float64), tensor(0.2663, dtype=torch.float64), 0, 0, 0, 0, tensor(0.0202, dtype=torch.float64), tensor(0.1137, dtype=torch.float64), 1, 1, 1, 1, 0, 0, 2, 0.1, 47.99999999999999, 1]
normalized proposed parameters for next round by BO: [tensor(0.1698, dtype=torch.float64), tensor(0.4247, dtype=torch.float64), tensor(0.2663, dtype=torch.float64), tensor(0.0054, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.6488e-18, dtype=torch.float64), tensor(0.0202, dtype=torch.float64), tensor(0.1137, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.17
  gsm8k: 0.425
  rowan_hellaswag: 0.266
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.02
  arc_challenge: 0.114

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 63,488 || all params: 8,030,324,736 || trainable%: 0.0008
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9943
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  994
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:17,  2.60s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:10,  1.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:41,  1.99it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.46it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.71it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.91it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:14,  2.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:24<00:10,  3.28it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:26<00:08,  3.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.65it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  2.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.86it/s]
Evaluation performance at step 25: 0.78
{'loss': 3.313, 'grad_norm': 8.309469223022461, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.78}
{'eval_loss': 2.819870948791504, 'eval_runtime': 10.6712, 'eval_samples_per_second': 93.148, 'eval_steps_per_second': 5.904, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:40,  2.83s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:48,  1.87it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:34,  2.43it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:25,  2.96it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:21,  3.10it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:14<00:19,  3.10it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:16,  3.10it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:20<00:14,  2.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:10,  3.29it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:07,  3.56it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:06,  2.89it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:30<00:03,  3.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:32<00:00,  3.40it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:32<00:00,  3.11it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.391, 'grad_norm': 10.139389991760254, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.157832145690918, 'eval_runtime': 8.8333, 'eval_samples_per_second': 112.529, 'eval_steps_per_second': 7.132, 'epoch': 0.08}
{'loss': 1.9626, 'grad_norm': 9.862135887145996, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.8059029579162598, 'eval_runtime': 8.8731, 'eval_samples_per_second': 112.024, 'eval_steps_per_second': 7.1, 'epoch': 0.12}
{'loss': 1.6941, 'grad_norm': 7.914233207702637, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.6256901025772095, 'eval_runtime': 8.9157, 'eval_samples_per_second': 111.489, 'eval_steps_per_second': 7.066, 'epoch': 0.16}
{'loss': 1.5563, 'grad_norm': 5.571995735168457, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.4888617992401123, 'eval_runtime': 8.9247, 'eval_samples_per_second': 111.377, 'eval_steps_per_second': 7.059, 'epoch': 0.2}
{'loss': 1.4788, 'grad_norm': 6.191802978515625, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.4469220638275146, 'eval_runtime': 8.9209, 'eval_samples_per_second': 111.424, 'eval_steps_per_second': 7.062, 'epoch': 0.24}
{'loss': 1.4299, 'grad_norm': 6.487665176391602, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 1.4232714176177979, 'eval_runtime': 8.9101, 'eval_samples_per_second': 111.558, 'eval_steps_per_second': 7.071, 'epoch': 0.28}
{'loss': 1.4139, 'grad_norm': 5.244521141052246, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 1.4030935764312744, 'eval_runtime': 8.8628, 'eval_samples_per_second': 112.154, 'eval_steps_per_second': 7.108, 'epoch': 0.32}
{'loss': 1.363, 'grad_norm': 4.0872883796691895, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 1.387292742729187, 'eval_runtime': 8.8626, 'eval_samples_per_second': 112.156, 'eval_steps_per_second': 7.109, 'epoch': 0.36}
{'loss': 1.3836, 'grad_norm': 3.889486312866211, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 1.3600162267684937, 'eval_runtime': 8.8543, 'eval_samples_per_second': 112.262, 'eval_steps_per_second': 7.115, 'epoch': 0.4}
{'loss': 1.3597, 'grad_norm': 6.231764316558838, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 1.342326045036316, 'eval_runtime': 8.8597, 'eval_samples_per_second': 112.194, 'eval_steps_per_second': 7.111, 'epoch': 0.44}
{'loss': 1.3208, 'grad_norm': 4.804968357086182, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 1.334411859512329, 'eval_runtime': 8.8564, 'eval_samples_per_second': 112.236, 'eval_steps_per_second': 7.114, 'epoch': 0.48}
{'loss': 1.3141, 'grad_norm': 4.8672709465026855, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 1.3312138319015503, 'eval_runtime': 8.8538, 'eval_samples_per_second': 112.268, 'eval_steps_per_second': 7.116, 'epoch': 0.52}
{'loss': 1.2866, 'grad_norm': 5.796092510223389, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 1.3204532861709595, 'eval_runtime': 8.8664, 'eval_samples_per_second': 112.109, 'eval_steps_per_second': 7.105, 'epoch': 0.56}
{'loss': 1.3247, 'grad_norm': 28.15900993347168, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 1.3156572580337524, 'eval_runtime': 8.9079, 'eval_samples_per_second': 111.586, 'eval_steps_per_second': 7.072, 'epoch': 0.6}
{'loss': 1.3447, 'grad_norm': 6.674718379974365, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 1.3094764947891235, 'eval_runtime': 8.8959, 'eval_samples_per_second': 111.737, 'eval_steps_per_second': 7.082, 'epoch': 0.64}
{'loss': 1.3319, 'grad_norm': 8.823298454284668, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 1.310457706451416, 'eval_runtime': 8.8938, 'eval_samples_per_second': 111.764, 'eval_steps_per_second': 7.084, 'epoch': 0.68}
{'loss': 1.2772, 'grad_norm': 4.152535915374756, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 1.3057993650436401, 'eval_runtime': 8.8847, 'eval_samples_per_second': 111.877, 'eval_steps_per_second': 7.091, 'epoch': 0.72}
{'loss': 1.2959, 'grad_norm': 6.355427265167236, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 1.3016732931137085, 'eval_runtime': 8.8949, 'eval_samples_per_second': 111.749, 'eval_steps_per_second': 7.083, 'epoch': 0.76}
{'loss': 1.2985, 'grad_norm': 6.233160495758057, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 1.2968432903289795, 'eval_runtime': 8.8884, 'eval_samples_per_second': 111.831, 'eval_steps_per_second': 7.088, 'epoch': 0.8}
{'loss': 1.3197, 'grad_norm': 3.9459784030914307, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 1.2975953817367554, 'eval_runtime': 8.8892, 'eval_samples_per_second': 111.821, 'eval_steps_per_second': 7.087, 'epoch': 0.84}
{'loss': 1.3169, 'grad_norm': 8.675713539123535, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 1.2941840887069702, 'eval_runtime': 8.8941, 'eval_samples_per_second': 111.76, 'eval_steps_per_second': 7.083, 'epoch': 0.88}
{'loss': 1.3186, 'grad_norm': 6.4889349937438965, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 1.292187213897705, 'eval_runtime': 8.9165, 'eval_samples_per_second': 111.479, 'eval_steps_per_second': 7.066, 'epoch': 0.92}
{'loss': 1.2939, 'grad_norm': 4.322459697723389, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 1.2903271913528442, 'eval_runtime': 8.8997, 'eval_samples_per_second': 111.689, 'eval_steps_per_second': 7.079, 'epoch': 0.96}
{'train_runtime': 508.3878, 'train_samples_per_second': 19.558, 'train_steps_per_second': 1.223, 'train_loss': 1.5070789404620695, 'epoch': 1.0}
train_results:  {'eval_loss': [2.819870948791504, 2.157832145690918, 1.8059029579162598, 1.6256901025772095, 1.4888617992401123, 1.4469220638275146, 1.4232714176177979, 1.4030935764312744, 1.387292742729187, 1.3600162267684937, 1.342326045036316, 1.334411859512329, 1.3312138319015503, 1.3204532861709595, 1.3156572580337524, 1.3094764947891235, 1.310457706451416, 1.3057993650436401, 1.3016732931137085, 1.2968432903289795, 1.2975953817367554, 1.2941840887069702, 1.292187213897705, 1.2903271913528442], 'performance': [0.78, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:08<13:22,  8.11s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<01:00,  1.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:20<00:34,  1.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:24<00:20,  2.45it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:30<00:13,  2.64it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:37<00:07,  2.43it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:39<00:00,  3.18it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.52it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.78, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  1.2250956296920776
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166, 1.2250876426696777, 1.230519413948059, 1.230392336845398, 1.2156025171279907, 1.2250956296920776]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0274 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.8528167657890594
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 0.8533871769645572
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.8097300649319035
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.8530503917373933
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.8533841960946495
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2117, dtype=torch.float64), 0, tensor(0.2655, dtype=torch.float64), tensor(0.0129, dtype=torch.float64), 0, 0, 0, 0, tensor(0.5040, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 1.2932153539695151e-17, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2117, dtype=torch.float64), tensor(2.6661e-18, dtype=torch.float64), tensor(0.2655, dtype=torch.float64), tensor(0.0129, dtype=torch.float64), tensor(4.9473e-18, dtype=torch.float64), tensor(4.5340e-18, dtype=torch.float64), tensor(1.1633e-18, dtype=torch.float64), tensor(0.0058, dtype=torch.float64), tensor(0.5040, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.2932e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.212
  gsm8k: 0
  rowan_hellaswag: 0.265
  sciq: 0.013
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.504

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.2932153539695151e-17,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  1.2932153539695151e-17
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9940
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  994
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:41,  2.84s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:13,  1.24it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:43,  1.93it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.43it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.70it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:21,  2.77it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:17,  2.84it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:15,  2.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:11,  3.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:30<00:05,  3.26it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:32<00:03,  3.41it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.93it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.0849, 'grad_norm': 6.512261867523193, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.552216053009033, 'eval_runtime': 9.3622, 'eval_samples_per_second': 106.171, 'eval_steps_per_second': 6.729, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:46,  2.90s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:47,  1.93it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:35,  2.33it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:26,  2.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  2.95it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:25,  2.32it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:21,  2.35it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:17,  2.48it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:14,  2.49it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:09,  2.73it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:33<00:08,  2.33it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:38<00:05,  2.14it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:40<00:01,  2.48it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:40<00:00,  2.48it/s]
Evaluation performance at step 50: 0.78
{'loss': 2.9697, 'grad_norm': 3.879941701889038, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 2.438606023788452, 'eval_runtime': 8.6635, 'eval_samples_per_second': 114.734, 'eval_steps_per_second': 7.272, 'epoch': 0.08}
{'loss': 2.1815, 'grad_norm': 3.508174180984497, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.8606892824172974, 'eval_runtime': 8.7706, 'eval_samples_per_second': 113.333, 'eval_steps_per_second': 7.183, 'epoch': 0.12}
{'loss': 1.7197, 'grad_norm': 1.8265146017074585, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.6639258861541748, 'eval_runtime': 8.7705, 'eval_samples_per_second': 113.335, 'eval_steps_per_second': 7.183, 'epoch': 0.16}
{'loss': 1.624, 'grad_norm': 2.490509271621704, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.5576093196868896, 'eval_runtime': 8.8093, 'eval_samples_per_second': 112.836, 'eval_steps_per_second': 7.152, 'epoch': 0.2}
{'loss': 1.5972, 'grad_norm': 2.327730178833008, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.4979292154312134, 'eval_runtime': 8.8059, 'eval_samples_per_second': 112.879, 'eval_steps_per_second': 7.154, 'epoch': 0.24}
{'loss': 1.5114, 'grad_norm': 2.3607401847839355, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 1.4509109258651733, 'eval_runtime': 8.787, 'eval_samples_per_second': 113.122, 'eval_steps_per_second': 7.17, 'epoch': 0.28}
{'loss': 1.4346, 'grad_norm': 2.134276866912842, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 1.4348888397216797, 'eval_runtime': 8.817, 'eval_samples_per_second': 112.737, 'eval_steps_per_second': 7.145, 'epoch': 0.32}
{'loss': 1.4374, 'grad_norm': 2.434508800506592, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 1.4261207580566406, 'eval_runtime': 8.8308, 'eval_samples_per_second': 112.561, 'eval_steps_per_second': 7.134, 'epoch': 0.36}
{'loss': 1.4412, 'grad_norm': 2.2191410064697266, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 1.4160012006759644, 'eval_runtime': 8.8186, 'eval_samples_per_second': 112.716, 'eval_steps_per_second': 7.144, 'epoch': 0.4}
{'loss': 1.4042, 'grad_norm': 2.2322897911071777, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 1.4114584922790527, 'eval_runtime': 8.7965, 'eval_samples_per_second': 113.0, 'eval_steps_per_second': 7.162, 'epoch': 0.44}
{'loss': 1.4405, 'grad_norm': 2.427809238433838, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 1.4050465822219849, 'eval_runtime': 8.8086, 'eval_samples_per_second': 112.845, 'eval_steps_per_second': 7.152, 'epoch': 0.48}
{'loss': 1.4241, 'grad_norm': 2.068993091583252, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 1.4009902477264404, 'eval_runtime': 8.8111, 'eval_samples_per_second': 112.813, 'eval_steps_per_second': 7.15, 'epoch': 0.52}
{'loss': 1.4069, 'grad_norm': 2.0616729259490967, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 1.3982110023498535, 'eval_runtime': 8.8218, 'eval_samples_per_second': 112.675, 'eval_steps_per_second': 7.141, 'epoch': 0.56}
{'loss': 1.3959, 'grad_norm': 2.3013155460357666, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 1.3936954736709595, 'eval_runtime': 8.8163, 'eval_samples_per_second': 112.745, 'eval_steps_per_second': 7.146, 'epoch': 0.6}
{'loss': 1.457, 'grad_norm': 2.085129737854004, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 1.3907872438430786, 'eval_runtime': 8.8428, 'eval_samples_per_second': 112.408, 'eval_steps_per_second': 7.124, 'epoch': 0.64}
{'loss': 1.4774, 'grad_norm': 2.2379209995269775, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 1.3885551691055298, 'eval_runtime': 8.8054, 'eval_samples_per_second': 112.885, 'eval_steps_per_second': 7.155, 'epoch': 0.68}
{'loss': 1.3587, 'grad_norm': 2.129030227661133, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 1.386359453201294, 'eval_runtime': 8.764, 'eval_samples_per_second': 113.419, 'eval_steps_per_second': 7.188, 'epoch': 0.72}
{'loss': 1.4065, 'grad_norm': 2.5861737728118896, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 1.384932279586792, 'eval_runtime': 8.7678, 'eval_samples_per_second': 113.369, 'eval_steps_per_second': 7.185, 'epoch': 0.76}
{'loss': 1.4205, 'grad_norm': 2.174166679382324, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 1.383185863494873, 'eval_runtime': 8.7545, 'eval_samples_per_second': 113.542, 'eval_steps_per_second': 7.196, 'epoch': 0.8}
{'loss': 1.4469, 'grad_norm': 1.9931801557540894, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 1.3816139698028564, 'eval_runtime': 8.6982, 'eval_samples_per_second': 114.276, 'eval_steps_per_second': 7.243, 'epoch': 0.84}
{'loss': 1.3611, 'grad_norm': 2.142977237701416, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 1.3806216716766357, 'eval_runtime': 8.7151, 'eval_samples_per_second': 114.055, 'eval_steps_per_second': 7.229, 'epoch': 0.88}
{'loss': 1.418, 'grad_norm': 2.0114564895629883, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 1.3798682689666748, 'eval_runtime': 8.7057, 'eval_samples_per_second': 114.178, 'eval_steps_per_second': 7.237, 'epoch': 0.92}
{'loss': 1.4669, 'grad_norm': 2.1159629821777344, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 1.3792563676834106, 'eval_runtime': 8.6922, 'eval_samples_per_second': 114.355, 'eval_steps_per_second': 7.248, 'epoch': 0.96}
{'train_runtime': 423.966, 'train_samples_per_second': 23.445, 'train_steps_per_second': 1.467, 'train_loss': 1.6532570786798115, 'epoch': 1.0}
train_results:  {'eval_loss': [3.552216053009033, 2.438606023788452, 1.8606892824172974, 1.6639258861541748, 1.5576093196868896, 1.4979292154312134, 1.4509109258651733, 1.4348888397216797, 1.4261207580566406, 1.4160012006759644, 1.4114584922790527, 1.4050465822219849, 1.4009902477264404, 1.3982110023498535, 1.3936954736709595, 1.3907872438430786, 1.3885551691055298, 1.386359453201294, 1.384932279586792, 1.383185863494873, 1.3816139698028564, 1.3806216716766357, 1.3798682689666748, 1.3792563676834106], 'performance': [0.74, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:50,  9.60s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:25<00:43,  1.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:31<00:26,  1.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:36<00:15,  2.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:44<00:08,  2.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:45<00:01,  2.96it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:45<00:00,  2.17it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  1.2380144596099854
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166, 1.2250876426696777, 1.230519413948059, 1.230392336845398, 1.2156025171279907, 1.2250956296920776, 1.2380144596099854]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7228 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.7542386497407385
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.8452654680916603
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.8464157542231936
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.8473791912184179
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 0.8450450870041415
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3428, dtype=torch.float64), tensor(0.2621, dtype=torch.float64), tensor(0.0145, dtype=torch.float64), 0, 0, tensor(0.0165, dtype=torch.float64), tensor(0.0174, dtype=torch.float64), tensor(0.3466, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.09928252857339957, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(5.4984e-17, dtype=torch.float64), tensor(0.3428, dtype=torch.float64), tensor(0.2621, dtype=torch.float64), tensor(0.0145, dtype=torch.float64), tensor(1.7618e-17, dtype=torch.float64), tensor(3.1056e-18, dtype=torch.float64), tensor(0.0165, dtype=torch.float64), tensor(0.0174, dtype=torch.float64), tensor(0.3466, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.9928, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.343
  rowan_hellaswag: 0.262
  sciq: 0.014
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.017
  mmlu: 0.017
  arc_challenge: 0.347

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.09928252857339957,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.09928252857339957
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:54,  2.98s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.95it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:25,  2.35it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.60it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.61it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:11,  2.98it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:05,  3.17it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  3.03it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:35<00:00,  3.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:35<00:00,  2.79it/s]
Evaluation performance at step 25: 0.75
{'loss': 3.4987, 'grad_norm': 4.661393165588379, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.0399999618530273, 'eval_runtime': 8.7585, 'eval_samples_per_second': 114.061, 'eval_steps_per_second': 7.193, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:34,  3.99s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:03,  1.43it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:30,  2.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.69it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:17<00:22,  2.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:22,  2.29it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:18,  2.39it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:12,  2.71it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:09,  2.93it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:34<00:07,  2.47it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:36<00:04,  2.72it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:38<00:00,  3.05it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.61it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.6141, 'grad_norm': 2.816648006439209, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 2.221050500869751, 'eval_runtime': 8.7574, 'eval_samples_per_second': 114.075, 'eval_steps_per_second': 7.194, 'epoch': 0.08}
{'loss': 1.9633, 'grad_norm': 2.0857770442962646, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8071191310882568, 'eval_runtime': 8.8079, 'eval_samples_per_second': 113.421, 'eval_steps_per_second': 7.153, 'epoch': 0.12}
{'loss': 1.708, 'grad_norm': 2.141282320022583, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6431413888931274, 'eval_runtime': 8.9344, 'eval_samples_per_second': 111.815, 'eval_steps_per_second': 7.051, 'epoch': 0.16}
{'loss': 1.5811, 'grad_norm': 2.0607492923736572, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5539913177490234, 'eval_runtime': 8.9787, 'eval_samples_per_second': 111.263, 'eval_steps_per_second': 7.017, 'epoch': 0.2}
{'loss': 1.496, 'grad_norm': 1.6437947750091553, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5066462755203247, 'eval_runtime': 9.0192, 'eval_samples_per_second': 110.764, 'eval_steps_per_second': 6.985, 'epoch': 0.24}
{'loss': 1.4894, 'grad_norm': 1.7400180101394653, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4660791158676147, 'eval_runtime': 8.988, 'eval_samples_per_second': 111.148, 'eval_steps_per_second': 7.009, 'epoch': 0.28}
{'loss': 1.4447, 'grad_norm': 2.1477880477905273, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4534341096878052, 'eval_runtime': 9.0351, 'eval_samples_per_second': 110.569, 'eval_steps_per_second': 6.973, 'epoch': 0.32}
{'loss': 1.4125, 'grad_norm': 1.9471631050109863, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4443413019180298, 'eval_runtime': 9.0495, 'eval_samples_per_second': 110.393, 'eval_steps_per_second': 6.962, 'epoch': 0.36}
{'loss': 1.3914, 'grad_norm': 2.209948778152466, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4356071949005127, 'eval_runtime': 9.0797, 'eval_samples_per_second': 110.025, 'eval_steps_per_second': 6.939, 'epoch': 0.4}
{'loss': 1.4, 'grad_norm': 2.5492215156555176, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4302031993865967, 'eval_runtime': 9.0392, 'eval_samples_per_second': 110.519, 'eval_steps_per_second': 6.97, 'epoch': 0.44}
{'loss': 1.379, 'grad_norm': 2.374044179916382, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.425916314125061, 'eval_runtime': 9.0364, 'eval_samples_per_second': 110.552, 'eval_steps_per_second': 6.972, 'epoch': 0.48}
{'loss': 1.388, 'grad_norm': 1.6766108274459839, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.422196388244629, 'eval_runtime': 9.0375, 'eval_samples_per_second': 110.539, 'eval_steps_per_second': 6.971, 'epoch': 0.52}
{'loss': 1.4366, 'grad_norm': 2.1593105792999268, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4178125858306885, 'eval_runtime': 9.0619, 'eval_samples_per_second': 110.241, 'eval_steps_per_second': 6.952, 'epoch': 0.56}
{'loss': 1.3964, 'grad_norm': 1.9279024600982666, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4145317077636719, 'eval_runtime': 9.033, 'eval_samples_per_second': 110.594, 'eval_steps_per_second': 6.974, 'epoch': 0.6}
{'loss': 1.3608, 'grad_norm': 1.8402228355407715, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4113292694091797, 'eval_runtime': 9.0384, 'eval_samples_per_second': 110.529, 'eval_steps_per_second': 6.97, 'epoch': 0.64}
{'loss': 1.3715, 'grad_norm': 2.104663372039795, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4084224700927734, 'eval_runtime': 9.0647, 'eval_samples_per_second': 110.207, 'eval_steps_per_second': 6.95, 'epoch': 0.68}
{'loss': 1.3778, 'grad_norm': 2.1325771808624268, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4063419103622437, 'eval_runtime': 9.0715, 'eval_samples_per_second': 110.126, 'eval_steps_per_second': 6.945, 'epoch': 0.72}
{'loss': 1.364, 'grad_norm': 2.057199716567993, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4048330783843994, 'eval_runtime': 9.0594, 'eval_samples_per_second': 110.272, 'eval_steps_per_second': 6.954, 'epoch': 0.76}
{'loss': 1.4032, 'grad_norm': 1.796236276626587, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4035634994506836, 'eval_runtime': 9.0595, 'eval_samples_per_second': 110.271, 'eval_steps_per_second': 6.954, 'epoch': 0.8}
{'loss': 1.3942, 'grad_norm': 1.7692135572433472, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4018875360488892, 'eval_runtime': 9.0431, 'eval_samples_per_second': 110.47, 'eval_steps_per_second': 6.967, 'epoch': 0.84}
{'loss': 1.3638, 'grad_norm': 1.7190461158752441, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4009759426116943, 'eval_runtime': 9.0718, 'eval_samples_per_second': 110.122, 'eval_steps_per_second': 6.945, 'epoch': 0.88}
{'loss': 1.3591, 'grad_norm': 1.6525846719741821, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.400152564048767, 'eval_runtime': 9.0363, 'eval_samples_per_second': 110.554, 'eval_steps_per_second': 6.972, 'epoch': 0.92}
{'loss': 1.4255, 'grad_norm': 1.7472565174102783, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.399512529373169, 'eval_runtime': 9.0332, 'eval_samples_per_second': 110.592, 'eval_steps_per_second': 6.974, 'epoch': 0.96}
{'loss': 1.3942, 'grad_norm': 1.6853680610656738, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3993319272994995, 'eval_runtime': 9.0354, 'eval_samples_per_second': 110.566, 'eval_steps_per_second': 6.973, 'epoch': 1.0}
{'train_runtime': 432.3346, 'train_samples_per_second': 23.123, 'train_steps_per_second': 1.446, 'train_loss': 1.5765312255859374, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0399999618530273, 2.221050500869751, 1.8071191310882568, 1.6431413888931274, 1.5539913177490234, 1.5066462755203247, 1.4660791158676147, 1.4534341096878052, 1.4443413019180298, 1.4356071949005127, 1.4302031993865967, 1.425916314125061, 1.422196388244629, 1.4178125858306885, 1.4145317077636719, 1.4113292694091797, 1.4084224700927734, 1.4063419103622437, 1.4048330783843994, 1.4035634994506836, 1.4018875360488892, 1.4009759426116943, 1.400152564048767, 1.399512529373169, 1.3993319272994995], 'performance': [0.75, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:07<12:58,  7.86s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<01:01,  1.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:21<00:37,  1.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:23,  2.16it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:32<00:14,  2.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:36<00:06,  2.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:00,  3.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.64it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  1.236227035522461
current iteration best possible performance (full train run):  0.735
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166, 1.2250876426696777, 1.230519413948059, 1.230392336845398, 1.2156025171279907, 1.2250956296920776, 1.2380144596099854, 1.236227035522461]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7866 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.8440956470660348
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.7940319596168316
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.8472396958695687
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.8402458303381553
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.8369915474385714
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1984, dtype=torch.float64), tensor(0.2641, dtype=torch.float64), tensor(0.2626, dtype=torch.float64), 0, 0, 0, 0, tensor(0.0216, dtype=torch.float64), tensor(0.2438, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.07190117219970787, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.1984, dtype=torch.float64), tensor(0.2641, dtype=torch.float64), tensor(0.2626, dtype=torch.float64), tensor(0.0096, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.5926e-18, dtype=torch.float64), tensor(9.2350e-18, dtype=torch.float64), tensor(0.0216, dtype=torch.float64), tensor(0.2438, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.7190, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.198
  gsm8k: 0.264
  rowan_hellaswag: 0.263
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.022
  arc_challenge: 0.244

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.07190117219970787,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.07190117219970787
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9901
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  990
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:37,  4.02s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:20,  1.13it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:45,  1.82it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:13<00:31,  2.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:26,  2.55it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:20<00:25,  2.31it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:19,  2.57it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:16,  2.59it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:12,  2.89it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:29<00:08,  3.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:06,  3.12it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:34<00:03,  3.34it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:36<00:00,  3.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.76it/s]
Evaluation performance at step 25: 0.76
{'loss': 3.6862, 'grad_norm': 4.544183731079102, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.2140185832977295, 'eval_runtime': 8.742, 'eval_samples_per_second': 113.247, 'eval_steps_per_second': 7.092, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:55,  2.99s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:56,  1.62it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:37,  2.21it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:28,  2.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:23,  2.81it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:20,  2.82it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:18,  2.72it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:22<00:15,  2.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:25<00:12,  2.71it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:08,  3.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:07,  2.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:04,  2.31it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:01,  2.68it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.64it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.7023, 'grad_norm': 4.755705833435059, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 2.3409037590026855, 'eval_runtime': 8.6785, 'eval_samples_per_second': 114.076, 'eval_steps_per_second': 7.144, 'epoch': 0.08}
{'loss': 2.0701, 'grad_norm': 2.507093906402588, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 1.8527039289474487, 'eval_runtime': 8.7307, 'eval_samples_per_second': 113.393, 'eval_steps_per_second': 7.101, 'epoch': 0.12}
{'loss': 1.7715, 'grad_norm': 2.0648605823516846, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 1.6687246561050415, 'eval_runtime': 8.7457, 'eval_samples_per_second': 113.199, 'eval_steps_per_second': 7.089, 'epoch': 0.16}
{'loss': 1.6252, 'grad_norm': 1.9893739223480225, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 1.5678718090057373, 'eval_runtime': 8.7654, 'eval_samples_per_second': 112.944, 'eval_steps_per_second': 7.073, 'epoch': 0.2}
{'loss': 1.5577, 'grad_norm': 2.088148832321167, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 1.5130935907363892, 'eval_runtime': 8.7867, 'eval_samples_per_second': 112.671, 'eval_steps_per_second': 7.056, 'epoch': 0.24}
{'loss': 1.4813, 'grad_norm': 1.8057482242584229, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 1.468975305557251, 'eval_runtime': 8.7976, 'eval_samples_per_second': 112.531, 'eval_steps_per_second': 7.047, 'epoch': 0.28}
{'loss': 1.5186, 'grad_norm': 2.1061463356018066, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 1.4564437866210938, 'eval_runtime': 8.817, 'eval_samples_per_second': 112.283, 'eval_steps_per_second': 7.032, 'epoch': 0.32}
{'loss': 1.4251, 'grad_norm': 1.4393583536148071, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 1.4446910619735718, 'eval_runtime': 8.8211, 'eval_samples_per_second': 112.231, 'eval_steps_per_second': 7.029, 'epoch': 0.36}
{'loss': 1.4221, 'grad_norm': 1.98786199092865, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 1.4363020658493042, 'eval_runtime': 8.8483, 'eval_samples_per_second': 111.886, 'eval_steps_per_second': 7.007, 'epoch': 0.4}
{'loss': 1.4845, 'grad_norm': 2.464962959289551, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 1.4317876100540161, 'eval_runtime': 8.8386, 'eval_samples_per_second': 112.008, 'eval_steps_per_second': 7.015, 'epoch': 0.44}
{'loss': 1.3765, 'grad_norm': 2.2873623371124268, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 1.427605152130127, 'eval_runtime': 8.8544, 'eval_samples_per_second': 111.809, 'eval_steps_per_second': 7.002, 'epoch': 0.48}
{'loss': 1.4594, 'grad_norm': 1.8009048700332642, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 1.421983003616333, 'eval_runtime': 8.8459, 'eval_samples_per_second': 111.917, 'eval_steps_per_second': 7.009, 'epoch': 0.53}
{'loss': 1.4233, 'grad_norm': 1.7783939838409424, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 1.417716145515442, 'eval_runtime': 8.8721, 'eval_samples_per_second': 111.585, 'eval_steps_per_second': 6.988, 'epoch': 0.57}
{'loss': 1.4083, 'grad_norm': 2.2398805618286133, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 1.4146809577941895, 'eval_runtime': 8.8685, 'eval_samples_per_second': 111.631, 'eval_steps_per_second': 6.991, 'epoch': 0.61}
{'loss': 1.4545, 'grad_norm': 2.0485332012176514, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 1.4118205308914185, 'eval_runtime': 8.8596, 'eval_samples_per_second': 111.743, 'eval_steps_per_second': 6.998, 'epoch': 0.65}
{'loss': 1.3857, 'grad_norm': 1.5974822044372559, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 1.4094181060791016, 'eval_runtime': 8.8493, 'eval_samples_per_second': 111.874, 'eval_steps_per_second': 7.006, 'epoch': 0.69}
{'loss': 1.4158, 'grad_norm': 1.5641746520996094, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 1.4072909355163574, 'eval_runtime': 8.8525, 'eval_samples_per_second': 111.833, 'eval_steps_per_second': 7.004, 'epoch': 0.73}
{'loss': 1.4376, 'grad_norm': 1.835610270500183, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 1.4052270650863647, 'eval_runtime': 8.8873, 'eval_samples_per_second': 111.395, 'eval_steps_per_second': 6.976, 'epoch': 0.77}
{'loss': 1.3821, 'grad_norm': 1.7368295192718506, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 1.4035050868988037, 'eval_runtime': 8.9436, 'eval_samples_per_second': 110.694, 'eval_steps_per_second': 6.932, 'epoch': 0.81}
{'loss': 1.4502, 'grad_norm': 1.8901456594467163, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 1.4020909070968628, 'eval_runtime': 8.9553, 'eval_samples_per_second': 110.549, 'eval_steps_per_second': 6.923, 'epoch': 0.85}
{'loss': 1.4229, 'grad_norm': 2.201029062271118, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 1.4011566638946533, 'eval_runtime': 8.9608, 'eval_samples_per_second': 110.482, 'eval_steps_per_second': 6.919, 'epoch': 0.89}
{'loss': 1.3306, 'grad_norm': 1.9891753196716309, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 1.4002107381820679, 'eval_runtime': 8.9528, 'eval_samples_per_second': 110.579, 'eval_steps_per_second': 6.925, 'epoch': 0.93}
{'loss': 1.3788, 'grad_norm': 1.953035831451416, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 1.3997474908828735, 'eval_runtime': 8.9352, 'eval_samples_per_second': 110.798, 'eval_steps_per_second': 6.939, 'epoch': 0.97}
{'train_runtime': 419.3371, 'train_samples_per_second': 23.611, 'train_steps_per_second': 1.476, 'train_loss': 1.6205985850394253, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2140185832977295, 2.3409037590026855, 1.8527039289474487, 1.6687246561050415, 1.5678718090057373, 1.5130935907363892, 1.468975305557251, 1.4564437866210938, 1.4446910619735718, 1.4363020658493042, 1.4317876100540161, 1.427605152130127, 1.421983003616333, 1.417716145515442, 1.4146809577941895, 1.4118205308914185, 1.4094181060791016, 1.4072909355163574, 1.4052270650863647, 1.4035050868988037, 1.4020909070968628, 1.4011566638946533, 1.4002107381820679, 1.3997474908828735], 'performance': [0.76, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<11:19,  6.86s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:13<00:56,  1.48it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:19<00:34,  1.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:27<00:25,  1.97it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:35<00:17,  2.01it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:39<00:07,  2.41it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:41<00:00,  3.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:41<00:00,  2.43it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  1.2362620830535889
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166, 1.2250876426696777, 1.230519413948059, 1.230392336845398, 1.2156025171279907, 1.2250956296920776, 1.2380144596099854, 1.236227035522461, 1.2362620830535889]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6793 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.8466597251311534
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.8464593095187611
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.8447277572789061
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.84740410316638
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.8472230335044282
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2650, dtype=torch.float64), tensor(0.0184, dtype=torch.float64), 0, 0, tensor(0.0155, dtype=torch.float64), tensor(0.0134, dtype=torch.float64), tensor(0.6877, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.0003e-16, dtype=torch.float64), tensor(0.2650, dtype=torch.float64), tensor(0.0184, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0155, dtype=torch.float64), tensor(0.0134, dtype=torch.float64), tensor(0.6877, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.265
  sciq: 0.018
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.016
  mmlu: 0.013
  arc_challenge: 0.688

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'gsm8k': (1.0, 'exact_match,strict-match')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:38,  2.81s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:12,  1.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:42,  1.96it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:29,  2.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:25,  2.61it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:25,  2.33it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:19,  2.57it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:24<00:16,  2.59it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:12,  2.91it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:08,  3.14it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:05,  3.26it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:33<00:03,  3.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:34<00:00,  3.70it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.87it/s]
Evaluation performance at step 25: 0.75
{'loss': 3.9216, 'grad_norm': 5.292600154876709, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.413771152496338, 'eval_runtime': 8.6245, 'eval_samples_per_second': 115.832, 'eval_steps_per_second': 7.305, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:26,  2.69s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:50,  1.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:36,  2.29it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:29,  2.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:25,  2.68it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:16<00:20,  2.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:19,  2.65it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:23<00:16,  2.60it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:13,  2.60it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:28<00:09,  2.83it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:07,  2.42it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:35<00:04,  2.53it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:37<00:01,  2.89it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.67it/s]
Evaluation performance at step 50: 0.72
{'loss': 2.8889, 'grad_norm': 3.947563886642456, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.72}
{'eval_loss': 2.367457866668701, 'eval_runtime': 8.639, 'eval_samples_per_second': 115.639, 'eval_steps_per_second': 7.293, 'epoch': 0.08}
{'loss': 2.1103, 'grad_norm': 2.6317782402038574, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8318461179733276, 'eval_runtime': 8.732, 'eval_samples_per_second': 114.406, 'eval_steps_per_second': 7.215, 'epoch': 0.12}
{'loss': 1.6841, 'grad_norm': 2.3477163314819336, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6495028734207153, 'eval_runtime': 8.6861, 'eval_samples_per_second': 115.011, 'eval_steps_per_second': 7.253, 'epoch': 0.16}
{'loss': 1.5402, 'grad_norm': 2.186734199523926, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.553640604019165, 'eval_runtime': 8.7306, 'eval_samples_per_second': 114.425, 'eval_steps_per_second': 7.216, 'epoch': 0.2}
{'loss': 1.4862, 'grad_norm': 2.515042781829834, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4999359846115112, 'eval_runtime': 8.6733, 'eval_samples_per_second': 115.182, 'eval_steps_per_second': 7.264, 'epoch': 0.24}
{'loss': 1.4759, 'grad_norm': 2.1098968982696533, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4570202827453613, 'eval_runtime': 8.6866, 'eval_samples_per_second': 115.005, 'eval_steps_per_second': 7.253, 'epoch': 0.28}
{'loss': 1.476, 'grad_norm': 2.0533769130706787, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.441487431526184, 'eval_runtime': 8.6948, 'eval_samples_per_second': 114.896, 'eval_steps_per_second': 7.246, 'epoch': 0.32}
{'loss': 1.4483, 'grad_norm': 1.8061414957046509, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4328073263168335, 'eval_runtime': 8.705, 'eval_samples_per_second': 114.761, 'eval_steps_per_second': 7.237, 'epoch': 0.36}
{'loss': 1.4496, 'grad_norm': 2.5490832328796387, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4268896579742432, 'eval_runtime': 8.7112, 'eval_samples_per_second': 114.68, 'eval_steps_per_second': 7.232, 'epoch': 0.4}
{'loss': 1.4496, 'grad_norm': 1.9682035446166992, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4212653636932373, 'eval_runtime': 8.7122, 'eval_samples_per_second': 114.666, 'eval_steps_per_second': 7.231, 'epoch': 0.44}
{'loss': 1.4296, 'grad_norm': 2.1415388584136963, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.415758728981018, 'eval_runtime': 8.7016, 'eval_samples_per_second': 114.807, 'eval_steps_per_second': 7.24, 'epoch': 0.48}
{'loss': 1.443, 'grad_norm': 1.7586973905563354, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4104150533676147, 'eval_runtime': 8.7167, 'eval_samples_per_second': 114.608, 'eval_steps_per_second': 7.228, 'epoch': 0.52}
{'loss': 1.3578, 'grad_norm': 2.022822856903076, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4080723524093628, 'eval_runtime': 8.7235, 'eval_samples_per_second': 114.519, 'eval_steps_per_second': 7.222, 'epoch': 0.56}
{'loss': 1.4342, 'grad_norm': 2.2969746589660645, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4047106504440308, 'eval_runtime': 8.7232, 'eval_samples_per_second': 114.522, 'eval_steps_per_second': 7.222, 'epoch': 0.6}
{'loss': 1.415, 'grad_norm': 2.478144884109497, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4023994207382202, 'eval_runtime': 8.7353, 'eval_samples_per_second': 114.363, 'eval_steps_per_second': 7.212, 'epoch': 0.64}
{'loss': 1.4546, 'grad_norm': 2.0222549438476562, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.399519920349121, 'eval_runtime': 8.6993, 'eval_samples_per_second': 114.836, 'eval_steps_per_second': 7.242, 'epoch': 0.68}
{'loss': 1.3538, 'grad_norm': 2.6229255199432373, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3971097469329834, 'eval_runtime': 8.7094, 'eval_samples_per_second': 114.704, 'eval_steps_per_second': 7.234, 'epoch': 0.72}
{'loss': 1.3724, 'grad_norm': 2.0366759300231934, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3954354524612427, 'eval_runtime': 8.707, 'eval_samples_per_second': 114.735, 'eval_steps_per_second': 7.236, 'epoch': 0.76}
{'loss': 1.4387, 'grad_norm': 1.9857765436172485, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3938264846801758, 'eval_runtime': 8.7165, 'eval_samples_per_second': 114.611, 'eval_steps_per_second': 7.228, 'epoch': 0.8}
{'loss': 1.3883, 'grad_norm': 2.2555172443389893, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3920960426330566, 'eval_runtime': 8.7091, 'eval_samples_per_second': 114.708, 'eval_steps_per_second': 7.234, 'epoch': 0.84}
{'loss': 1.3365, 'grad_norm': 1.9024219512939453, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3912733793258667, 'eval_runtime': 8.6965, 'eval_samples_per_second': 114.873, 'eval_steps_per_second': 7.244, 'epoch': 0.88}
{'loss': 1.404, 'grad_norm': 1.788421392440796, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3904573917388916, 'eval_runtime': 8.7014, 'eval_samples_per_second': 114.808, 'eval_steps_per_second': 7.24, 'epoch': 0.92}
{'loss': 1.3922, 'grad_norm': 1.8367722034454346, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3897463083267212, 'eval_runtime': 8.6963, 'eval_samples_per_second': 114.876, 'eval_steps_per_second': 7.244, 'epoch': 0.96}
{'loss': 1.4531, 'grad_norm': 2.420731782913208, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3895821571350098, 'eval_runtime': 8.7004, 'eval_samples_per_second': 114.822, 'eval_steps_per_second': 7.241, 'epoch': 1.0}
{'train_runtime': 414.9296, 'train_samples_per_second': 24.096, 'train_steps_per_second': 1.506, 'train_loss': 1.62414697265625, 'epoch': 1.0}
train_results:  {'eval_loss': [3.413771152496338, 2.367457866668701, 1.8318461179733276, 1.6495028734207153, 1.553640604019165, 1.4999359846115112, 1.4570202827453613, 1.441487431526184, 1.4328073263168335, 1.4268896579742432, 1.4212653636932373, 1.415758728981018, 1.4104150533676147, 1.4080723524093628, 1.4047106504440308, 1.4023994207382202, 1.399519920349121, 1.3971097469329834, 1.3954354524612427, 1.3938264846801758, 1.3920960426330566, 1.3912733793258667, 1.3904573917388916, 1.3897463083267212, 1.3895821571350098], 'performance': [0.75, 0.72]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['gsm8k']
Overwriting default num_fewshot of gsm8k from 5 to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:52,  9.62s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:18<01:15,  1.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:26<00:45,  1.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:34<00:30,  1.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:42<00:19,  1.80it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:49<00:09,  1.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:51<00:01,  2.55it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:51<00:00,  1.93it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.72]
current iteration observed (possibly low-fid or predicted) performance:  1.2383525371551514
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.62408047914505, 0.7941675782203674, 0.6125386953353882, 0.8026077151298523, 0.7562128305435181, 0.8092976808547974, 1.1678935289382935, 1.2252730131149292, 0.8350085020065308, 1.225351095199585, 0.9880110025405884, 1.2303980588912964, 0.7980332374572754, 1.2290406227111816, 1.2270740270614624, 1.22829008102417, 1.2295026779174805, 1.2157247066497803, 1.2277302742004395, 1.2087997198104858, 1.230318546295166, 1.2250876426696777, 1.230519413948059, 1.230392336845398, 1.2156025171279907, 1.2250956296920776, 1.2380144596099854, 1.236227035522461, 1.2362620830535889, 1.2383525371551514]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7055 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.8488569822509486
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.847763866535806
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.8477429693840361
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.8627379873747945
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.7693451591292118
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0966, dtype=torch.float64), 0, tensor(0.2634, dtype=torch.float64), tensor(0.0163, dtype=torch.float64), 0, 0, tensor(0.0164, dtype=torch.float64), tensor(0.0146, dtype=torch.float64), tensor(0.5928, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.006446140235875918, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0966, dtype=torch.float64), tensor(9.8481e-18, dtype=torch.float64), tensor(0.2634, dtype=torch.float64), tensor(0.0163, dtype=torch.float64), tensor(5.5811e-18, dtype=torch.float64), tensor(1.9435e-18, dtype=torch.float64), tensor(0.0164, dtype=torch.float64), tensor(0.0146, dtype=torch.float64), tensor(0.5928, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.0645, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.7665, 0.777, 0.7875000000000001, 0.7875000000000001, 0.7875000000000001, 0.7875000000000001, 0.7875000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'gsm8k', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_gsm8k_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['gsm8k'], 'weight': [1.0], 'random': [[0.7454999999999999, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001], [0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001], [0.7665, 0.777, 0.7875000000000001, 0.7875000000000001, 0.7875000000000001, 0.7875000000000001, 0.7875000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001]], 'random_full_inputs': [[[0, 0.03539291596086143, 0.02841582208593765, 0.14693162717501546, 0.11296029909927936, 0.23533289628061813, 0.09952191407714561, 0.2727720391670112, 0.06867248615413113, 14, 1, 0, 1, 1, 1, 26, 0.06319152730770125, 19.453576244216457, 0], [0, 0.4116222067575205, 0.04983489566805843, 0, 0.5010588903638228, 0, 0, 0.03748400721059832, 0, 1, 1, 0, 0, 0, 0, 128, 0.05259140646327104, 43.68586075601523, 1], [0.36611252598235083, 0, 0, 0.04104572699698201, 0.06992321148866594, 0, 0.1786664324560806, 0.1741888952276686, 0.17006320784825196, 32, 1, 0, 0, 0, 0, 7, 0.032816262115845915, 1.4800000190734866, 1], [0, 0.13750616182899167, 0, 0.1920757425222759, 0.3121267393573872, 0, 0, 0, 0.3582913562913453, 2, 1, 0, 0, 0, 0, 128, 0.016678297032101424, 48.0, 1], [0, 0.7224586178342507, 0.026001344359198385, 0.18163983794761293, 0.0699001998589381, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 16, 0.08100208055194043, 48.0, 1], [0, 0.8398929909708995, 0, 0.1601070090291005, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1], [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 104, 1.1796119636642283e-17, 39.06070870990225, 1], [0, 0.8511647053474917, 0, 0.14883529465250833, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 2, 3.399847640995838e-17, 1.4800000190735003, 0], [0, 0.7982825031843419, 0, 0.2017174968156577, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 0.0, 48.0, 0], [0, 0.8265892530985142, 0, 0.17341074690148592, 0, 0, 0, 0, 0, 18, 1, 0, 0, 0, 1, 2, 0.0, 48.0, 0], [0, 0.6375002027833496, 0.19369165075954206, 0.16880814645710843, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0.0, 48.0, 1], [0, 0.5536022203226706, 0, 0.17020101120228803, 0.27619676847504127, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 2.3330148220055246e-17, 48.0, 1], [0, 0, 0, 0, 0, 0.970512919053658, 0, 0.029487080946342035, 0, 6, 1, 0, 0, 0, 0, 128, 0.1, 1.4800000190734863, 1], [0.35950461213525686, 0.4691206012604283, 0, 0.17137478660431488, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 6.938893903907231e-19, 48.0, 1], [0.05920083995047966, 0.34496914925587435, 0, 0.17230444830688613, 0, 0.4235255624867598, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1.3877787807814456e-18, 48.0, 1], [0.15699146908951825, 0.33137853705628756, 0, 0.1743119384624135, 0, 0, 0.33731805539178056, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1.3877787807814463e-18, 48.0, 1], [0.6773818507611836, 0, 0, 0.17012466546398963, 0, 0.1524801697535017, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 8.887556553231707e-20, 48.0, 0], [0, 0.24472922779565875, 0, 0.1764389287619388, 0, 0, 0.5788318434424021, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0], [0.04119467317469495, 0.3996874887097146, 0.28350670152829033, 0.18360866290491962, 0, 0.0386520589994673, 0.0533504146829132, 0, 0, 1, 1, 0, 0, 0, 0, 2, 3.0814879110195774e-33, 48.0, 0], [0, 0, 0.6386533369179193, 0.1800614491310313, 0, 0, 0.1812852139510494, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0], [0, 0, 0.18416889434224007, 0.1805315281278326, 0, 0, 0.6352995775299264, 0, 0, 1, 1, 0, 1, 0, 0, 2, 0.0, 48.0, 0], [0, 0, 0.6791920452022095, 0.1773746349956517, 0, 0, 0.14343331980213891, 0, 0, 1, 1, 0, 1, 0, 1, 2, 0.0, 48.0, 0], [0, 0, 0, 0.18273234866733667, 0, 0.7917038420866873, 0.025563809245976475, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0], [0, 0, 0, 0.18218086556389812, 0, 0, 0.8178191344361018, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0], [0, 0, 0.05178607709376243, 0.18277687327056372, 0.30056994745665544, 0, 0.46485843023852597, 0, 0, 1, 1, 0, 0, 0, 0, 2, 4.2329675719960756e-18, 48.0, 0], [0, 0.6256588942772815, 0, 0.18133820198428782, 0, 0, 0.19300290373843074, 0, 0, 1, 1, 0, 1, 0, 0, 2, 4.163336342344338e-19, 48.0, 0], [0, 0, 0.11864577047604896, 0.14109675225713733, 0.10799347777365972, 0.26269757588565645, 0.3695664236074974, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 0], [0, 0, 0.038124977553810994, 0.1713140866974751, 0.36967907096123553, 0.28288714757471534, 0.1379947172127634, 0, 0, 1, 1, 0, 1, 0, 0, 2, 0.0, 48.0, 0], [0, 0, 0.13958700773477345, 0.16671082794062655, 0.4631013794142508, 0, 0.23060078491034852, 0, 0, 1, 1, 0, 0, 0, 0, 2, 6.938893903907229e-19, 48.0, 0], [0, 0, 0.11973106801226954, 0.16795385908111543, 0.12635921304903727, 0.21179885562039177, 0.3235922202912913, 0, 0.05003191334931165, 1, 1, 0, 0, 0, 0, 2, 1.942394533426294e-19, 48.0, 0]], [[0, 0.03551374819888498, 0.029128762715322224, 0.14778895652770546, 0.11113055439457427, 0.23532011567910663, 0.1016602957232614, 0.271666099272662, 0.06779146748848305, 14, 1, 0, 1, 0, 0, 26, 0.061997739398963594, 19.612889282736287, 0], [0, 0.4108566153674388, 0.02827773805609173, 0, 0.509906142970214, 0, 0, 0.05095950360625559, 0, 1, 1, 0, 0, 0, 0, 127, 0.052758571520659714, 44.007414087214364, 1], [0.4139256802322136, 0, 0, 0, 0.039673708543601235, 0, 0.19997990703137217, 0.16385178363751982, 0.17620925315609975, 32, 1, 0, 0, 0, 0, 5, 0.0337295915827445, 1.5076740768437955, 1], [0, 0.0540697951324127, 0, 0.17197373789864456, 0.7163689016503308, 0, 0, 0, 0.057587565318611825, 9, 1, 0, 0, 0, 0, 128, 0.04512015437766561, 48.0, 1], [0.3748363684949118, 0.17783942670984612, 0.06510533137960554, 0, 0.238261353059201, 0, 0.01868865703432954, 0, 0.1252688633221061, 1, 1, 1, 0, 1, 1, 128, 0.014526220082807142, 48.0, 1], [0, 0.744935014350688, 0.0813562229128065, 0, 0.020299429313833328, 0, 0, 0.10455602851650492, 0.04885330490616716, 1, 1, 0, 0, 1, 0, 128, 0.1, 48.0, 1], [0, 0, 0.0766078134544556, 0, 0.7511512983673209, 0, 0, 0.09394728771125875, 0.07829360046696474, 1, 1, 0, 0, 1, 0, 128, 0.1, 10.731116960316747, 0], [0, 0.6522203037946565, 0.05654865355524876, 0, 0, 0, 0.12773415616560244, 0.13028765163321995, 0.033209234851272515, 1, 1, 1, 0, 1, 0, 128, 5.416087376961712e-18, 48.0, 1], [0.29174905506325166, 0.23803799470012327, 0.05044102915679086, 0, 0, 0, 0.02469959982023585, 0.39425916985388615, 0, 1, 1, 1, 0, 0, 0, 128, 0.1, 48.0, 1], [0.02264315393275855, 0.804935456433346, 0.05204898995695327, 0, 0, 0, 0.03356635000417011, 0, 0.08680604967277225, 1, 1, 1, 0, 1, 0, 58, 0.1, 48.0, 1], [0, 0.6830481644626478, 0.07499087413847612, 0, 0, 0, 0.07402131665009967, 0, 0.16793964474877626, 1, 1, 0, 0, 0, 0, 16, 0.1, 48.0, 1], [0, 0.674799718989121, 0.12474922578792991, 0, 0, 0, 0, 0, 0.20045105522294945, 1, 1, 0, 0, 0, 0, 2, 0.007809184108711162, 48.0, 1], [0, 0.7670821888363679, 0, 0, 0, 0, 0, 0, 0.23291781116363186, 1, 1, 0, 1, 0, 0, 2, 0.0, 48.0, 1], [0, 0.4997545567002484, 0.27303808183458783, 0, 0, 0, 0, 0, 0.22720736146516388, 1, 1, 0, 1, 0, 0, 2, 8.673617379884037e-19, 48.0, 1], [0, 0.7804942019867701, 0, 0, 0, 0, 0, 0, 0.21950579801323344, 1, 1, 1, 0, 0, 1, 2, 0.0, 48.0, 1], [0, 0.4340759030369455, 0, 0, 0, 0, 0, 0.34567396033747494, 0.22025013662557952, 1, 1, 0, 1, 0, 0, 2, 0.0, 47.99999999999999, 1], [0, 0.4198563481115401, 0, 0, 0, 0.35698777400290405, 0, 0, 0.2231558778855557, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1], [0, 0.1420131430256163, 0, 0, 0.6376734217232392, 0, 0, 0, 0.22031343525114458, 1, 0, 0, 1, 1, 0, 2, 0.0, 47.99999999999999, 0], [0.956498558945775, 0, 0, 0, 0, 0, 0, 0.0435014410542251, 0, 1, 1, 0, 0, 0, 0, 128, 0.0, 1.4800000190734866, 1], [0, 0, 0, 0, 0.7810439514796736, 0, 0, 0, 0.2189560485203271, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1], [0, 0.20103670799242135, 0.24738749786298814, 0, 0.04218949771115425, 0, 0, 0.28398621981783617, 0.2254000766156004, 1, 1, 0, 0, 0, 0, 2, 0.1, 48.0, 1], [0, 0, 0.6882217199621042, 0, 0, 0, 0, 0.09223510449410288, 0.21954317554379316, 1, 1, 0, 0, 0, 0, 2, 0.0, 47.99999999999999, 1], [0, 0.07085390679315669, 0, 0, 0.010947676880154852, 0, 0, 0.6988819313706032, 0.21931648495608552, 1, 1, 0, 0, 0, 0, 2, 2.790497846654826e-20, 48.0, 1], [0, 0.010244207267603524, 0, 0, 0.4806880380532935, 0, 0, 0.2906521322327576, 0.21841562244634538, 1, 1, 1, 1, 0, 0, 2, 3.772434284354257e-19, 48.0, 1], [0, 0.7779499966391739, 0, 0, 0, 0, 0, 0, 0.2220500033608263, 1, 1, 1, 0, 0, 0, 2, 1.0408340855860842e-18, 48.0, 1], [0, 0, 0, 0, 0.7803162185942265, 0, 0, 0, 0.21956762260970203, 1, 1, 0, 1, 0, 0, 2, 0.1, 48.0, 1], [0, 0, 0, 0, 0.7746412299711742, 0, 0, 0, 0.2209541840101103, 1, 1, 0, 0, 0, 0, 2, 1.48101556931323e-17, 48.0, 1], [0, 0.30062060485340253, 0, 0, 0.3883821729614995, 0, 0, 0.08488701574031093, 0.22283521955526023, 1, 1, 0, 0, 0, 0, 2, 0.042680334383993296, 48.0, 0], [0, 0.3860951213457038, 0, 0.052772731419949155, 0.2983606396098055, 0, 0, 0.04150539014761101, 0.22126611747693034, 1, 1, 0, 0, 0, 0, 2, 0.0013384058095789455, 48.0, 0], [0, 0.3712030524139479, 0, 0.027969460827499307, 0.3691371395639562, 0, 0, 0, 0.22198397706363213, 1, 1, 0, 0, 0, 0, 87, 0.02285716141995678, 48.0, 0]], [[0, 0.03551374819888498, 0.029128762715322224, 0.14778895652770546, 0.11113055439457427, 0.23532011567910663, 0.1016602957232614, 0.271666099272662, 0.06779146748848305, 14, 1, 0, 1, 0, 0, 26, 0.061997739398963594, 19.612889282736287, 0], [0, 0.41144580747021003, 0.0303390578713048, 0, 0.5083400254874761, 0, 0, 0.04987510917100913, 0, 1, 1, 0, 0, 0, 0, 127, 0.05280215578374824, 44.07627937475198, 1], [0.39921990633900506, 0, 0, 0.03329143723580076, 0.05485148454278511, 0, 0.17581764941117486, 0.16817968962374308, 0.1686398328474912, 32, 1, 0, 0, 0, 0, 7, 0.038196198141624095, 1.709650524729653, 1], [0, 0.020264874171086432, 0, 0.2013964111038033, 0.3770718650882849, 0, 0, 0, 0.4012668496368254, 1, 1, 0, 0, 0, 0, 128, 0.01979076227355135, 48.0, 1], [0, 0.7488261008129539, 0, 0.2113063891587489, 0.039867510028297234, 0, 0, 0, 0, 6, 1, 0, 0, 0, 0, 77, 0.038840512563759345, 48.0, 1], [0, 0, 0.1096187014412198, 0.2656752682515232, 0.5302859996754152, 0, 0, 0, 0.09442003063184182, 4, 1, 0, 0, 0, 0, 62, 0.09233317536917209, 48.0, 1], [0, 0, 0.10421891212334405, 0.02360081026982174, 0.3308326439399411, 0, 0, 0, 0.5413476336668933, 5, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1], [0, 0, 0.21697276073810204, 0, 0.17637332622658322, 0, 0, 0, 0.6066539130353148, 1, 1, 0, 0, 1, 0, 2, 0.0, 47.99999999999999, 1], [0, 0, 0.14835651440469108, 0, 0.08633740600954314, 0, 0, 0, 0.7653060795857657, 1, 1, 0, 0, 0, 0, 2, 1.5456903734076127e-16, 15.58366669186919, 1], [0, 0.2990919316931622, 0.20174489801529547, 0, 0, 0, 0, 0, 0.49916317029154234, 1, 1, 0, 1, 0, 1, 2, 0.1, 48.0, 0], [0, 0, 0.21064450003571375, 0, 0.21190206106055906, 0, 0, 0, 0.5774534389037271, 18, 1, 0, 0, 0, 1, 2, 0.1, 48.0, 0], [0.324176085328188, 0, 0.18594387177420413, 0, 0, 0, 0, 0, 0.48988004289760795, 1, 1, 0, 1, 0, 0, 2, 3.122502256758253e-18, 47.99999999999999, 1], [0, 0, 0.19306389176225877, 0, 0.29338766115223197, 0, 0, 0, 0.5135484470855093, 1, 1, 0, 0, 0, 0, 93, 4.33680868994202e-19, 48.0, 1], [0.22653761838580136, 0, 0.21386230851005272, 0, 0.4354028803807351, 0, 0, 0, 0.12419719272341094, 1, 1, 0, 1, 0, 0, 2, 0.061222192538013254, 48.0, 1], [0.25046583691661195, 0.4737445585680441, 0.23839106002329102, 0.037398544492053064, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0.0, 48.0, 1], [0.3094159502571904, 0, 0.26303968918187937, 0, 0, 0.24833630425376035, 0, 0, 0.17379306419616664, 1, 1, 0, 0, 0, 0, 2, 0.06958211626205926, 48.0, 1], [0.1789094337694854, 0, 0.31346836376108483, 0.048119510686351695, 0, 0, 0, 0, 0.45950269178307807, 1, 1, 0, 1, 0, 0, 2, 0.1, 48.0, 1], [0.3113006281737451, 0.01711808999825852, 0.31029330329981053, 0, 0, 0, 0.05618124264351618, 0, 0.3051067358846698, 1, 1, 0, 1, 0, 1, 2, 0.03156347930556296, 48.0, 1], [0, 0.5447902525467795, 0.3363540223735165, 0, 0, 0, 0, 0, 0.11885572507970354, 1, 1, 0, 1, 0, 0, 2, 2.2117724318704293e-18, 48.0, 1], [0.6844602692094286, 0, 0.30678163519200713, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 2, 0.1, 48.0, 1], [0.16467375333047887, 0, 0.2943965756660684, 0, 0, 0, 0, 0.054414113317647866, 0.4850796652503937, 1, 1, 0, 1, 0, 0, 2, 0.01742200595099158, 48.0, 1], [0.060184220342001625, 0, 0.2914567129949238, 0.02048744991842734, 0.3231328729972099, 0, 0, 0, 0.3047387437474375, 1, 1, 0, 1, 0, 1, 2, 0.0, 48.0, 0], [0.013630491866045483, 0, 0.2824178636189681, 0, 0, 0, 0, 0, 0.7039516445149863, 1, 1, 0, 0, 0, 0, 2, 0.1, 48.0, 1], [0.22486468143200763, 0, 0.2864388259177225, 0, 0, 0, 0, 0, 0.4886964926502697, 1, 1, 0, 1, 0, 0, 2, 3.252606517456514e-18, 48.0, 1], [0, 0.01963303923207253, 0.2571450659698742, 0.019237665021713594, 0, 0.374309351873227, 0, 0, 0.32967487790311273, 1, 1, 0, 1, 0, 1, 2, 0.016218862237604357, 48.0, 1], [0.16975147784802502, 0.42465587009689215, 0.2662675253277639, 0, 0, 0, 0, 0.020182613919936403, 0.11370128986920681, 1, 1, 1, 1, 0, 0, 2, 0.1, 47.99999999999999, 1], [0.2117349295495759, 0, 0.2654593219769967, 0.01294808172054645, 0, 0, 0, 0, 0.5040392157164467, 1, 1, 0, 1, 0, 0, 2, 1.2932153539695151e-17, 48.0, 0], [0, 0.34284938861766445, 0.26209768811404155, 0.01447618454980568, 0, 0, 0.016536685402633696, 0.017419233794960236, 0.3466208195208943, 1, 1, 0, 1, 0, 0, 2, 0.09928252857339957, 48.0, 0], [0.19837431620714113, 0.2640987815616933, 0.26256994130376976, 0, 0, 0, 0, 0.021619093056813393, 0.24378729098480134, 1, 1, 0, 1, 0, 0, 2, 0.07190117219970787, 48.0, 0], [0, 0, 0.26498059899089993, 0.01835722124163007, 0, 0, 0.015528983836116301, 0.013412671285230722, 0.6877205246461231, 1, 1, 0, 1, 0, 0, 2, 0.0, 48.0, 0]]], 'random_full_train_performance': [0.7665, 0.777, 0.7875000000000001, 0.7875000000000001, 0.7665, 0.7875000000000001, 0.7665, 0.8190000000000001, 0.8085000000000001, 0.7244999999999999, 0.777, 0.7875000000000001, 0.798, 0.7665, 0.7454999999999999, 0.8085000000000001, 0.777, 0.8085000000000001, 0.798, 0.7665, 0.7875000000000001, 0.6930000000000001, 0.777, 0.7875000000000001, 0.798, 0.798, 0.6930000000000001, 0.735, 0.8085000000000001, 0.7140000000000001]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.054 MB of 0.054 MB uploadedwandb: \ 0.054 MB of 0.054 MB uploadedwandb: | 0.054 MB of 0.054 MB uploadedwandb: / 0.054 MB of 0.054 MB uploadedwandb: - 0.054 MB of 0.054 MB uploadedwandb: \ 0.054 MB of 0.072 MB uploadedwandb: | 0.799 MB of 1.720 MB uploadedwandb: / 1.720 MB of 1.720 MB uploadedwandb: - 1.720 MB of 1.720 MB uploadedwandb: \ 1.720 MB of 1.720 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñá‚ñÅ‚ñá‚ñÑ‚ñà‚ñÅ‚ñá‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÇ
wandb:            eval/runtime ‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÅ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá
wandb: eval/samples_per_second ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÜ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   eval/steps_per_second ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÜ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             train/epoch ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñà‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñà‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÖ
wandb:         train/grad_norm ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÖ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÅ
wandb:              train/loss ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñà‚ñÅ‚ñá‚ñÑ‚ñà‚ñÅ‚ñá‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                eval/loss 1.38958
wandb:             eval/runtime 8.7004
wandb:  eval/samples_per_second 114.822
wandb:    eval/steps_per_second 7.241
wandb:               total_flos 1.1352425864626176e+17
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 2.42073
wandb:      train/learning_rate 0.0
wandb:               train/loss 1.4531
wandb:               train_loss 1.62415
wandb:            train_runtime 414.9296
wandb: train_samples_per_second 24.096
wandb:   train_steps_per_second 1.506
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/9ejh3okx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260101_233441-9ejh3okx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

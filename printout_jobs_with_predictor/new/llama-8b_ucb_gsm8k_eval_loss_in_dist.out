2026-01-01 15:22:05.703298: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-01 15:22:05.733094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-01 15:22:05.733153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-01 15:22:05.734360: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-01 15:22:05.739051: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-01 15:22:06.550053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'gsm8k', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_gsm8k_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['gsm8k']
evaluation tasks and weights:  {'gsm8k': (1.0, 'exact_match,strict-match')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
gsm8k
evaluation dataset:
data domain:  gsm8k  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/gsm8k/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 9, 1, 1, 0, 0, 1, 14, 0.03669660801939172, 9, 1]
Checking history sample input_X_between_0_1:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 0.28125, 1.0, 1.0, 0.0, 0.0, 1.0, 0.109375, 0.3669660801939172, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1832736730575562
Checking history sample input_X:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 4, 1, 1, 0, 1, 0, 45, 0.02996529708553877, 41, 0]
Checking history sample input_X_between_0_1:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 0.125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3515625, 0.29965297085538767, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2217429876327515
Checking history sample input_X:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 5, 0, 1, 1, 0, 0, 115, 0.08044623754842878, 26, 0]
Checking history sample input_X_between_0_1:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 0.15625, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8984375, 0.8044623754842878, 0.5416666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1418228149414062
Checking history sample input_X:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 30, 0, 1, 0, 1, 1, 121, 0.019265932894845208, 29, 1]
Checking history sample input_X_between_0_1:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 0.9375, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9453125, 0.19265932894845206, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8670127987861633
Checking history sample input_X:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 19, 0, 0, 1, 1, 1, 108, 0.09306302255978231, 35, 1]
Checking history sample input_X_between_0_1:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 0.59375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.84375, 0.930630225597823, 0.7291666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.3226383924484253
Checking history sample input_X:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 14, 0, 0, 0, 1, 1, 44, 0.004820496247456452, 22, 1]
Checking history sample input_X_between_0_1:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 0.4375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.34375, 0.04820496247456452, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9786784052848816
Checking history sample input_X:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 24, 0, 1, 1, 0, 1, 79, 0.03164316476579598, 9, 1]
Checking history sample input_X_between_0_1:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6171875, 0.3164316476579597, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.201198697090149
Checking history sample input_X:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 29, 0, 1, 0, 1, 1, 23, 0.015858984905097274, 40, 0]
Checking history sample input_X_between_0_1:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 0.90625, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1796875, 0.15858984905097273, 0.8333333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.079849362373352
Checking history sample input_X:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 27, 0, 0, 1, 0, 0, 58, 0.02356583389888708, 4, 0]
Checking history sample input_X_between_0_1:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 0.84375, 0.0, 0.0, 1.0, 0.0, 0.0, 0.453125, 0.23565833898887079, 0.08333333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3622353076934814
Checking history sample input_X:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 18, 0, 1, 1, 0, 1, 102, 0.07944916714916157, 39, 0]
Checking history sample input_X_between_0_1:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 0.5625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.796875, 0.7944916714916157, 0.8125, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1885993480682373
Checking history sample input_X:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 16, 1, 0, 0, 0, 0, 57, 0.051177108415818844, 16, 1]
Checking history sample input_X_between_0_1:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4453125, 0.5117710841581884, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.7646844387054443
Checking history sample input_X:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 21, 0, 1, 0, 0, 1, 80, 0.08438899074092726, 11, 1]
Checking history sample input_X_between_0_1:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 0.65625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.625, 0.8438899074092726, 0.22916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.194746494293213
Checking history sample input_X:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 19, 0, 1, 1, 1, 0, 106, 0.021750066051679486, 16, 1]
Checking history sample input_X_between_0_1:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 0.59375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.828125, 0.21750066051679484, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.357306957244873
Checking history sample input_X:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 6, 1, 0, 0, 0, 1, 109, 0.0976089152670931, 17, 0]
Checking history sample input_X_between_0_1:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 0.1875, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8515625, 0.976089152670931, 0.3541666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0162200927734375
Checking history sample input_X:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 5, 0, 1, 1, 0, 1, 6, 0.045424701431822194, 31, 1]
Checking history sample input_X_between_0_1:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 0.15625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.046875, 0.4542470143182219, 0.6458333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2814624309539795
Checking history sample input_X:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 18, 1, 0, 0, 0, 0, 27, 0.050875722794158945, 27, 1]
Checking history sample input_X_between_0_1:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 0.5625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2109375, 0.5087572279415894, 0.5625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.6545288562774658
Checking history sample input_X:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 25, 0, 0, 0, 1, 1, 5, 0.0932969995849881, 3, 0]
Checking history sample input_X_between_0_1:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 0.78125, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0390625, 0.932969995849881, 0.0625, 0.0]
Checking history sample eval_loss at 625 steps:  -1.4124306440353394
Checking history sample input_X:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 14, 0, 0, 0, 1, 0, 14, 0.06234381073507801, 24, 1]
Checking history sample input_X_between_0_1:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 0.4375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.109375, 0.6234381073507801, 0.5, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9628919959068298
Checking history sample input_X:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 19, 1, 1, 0, 1, 0, 54, 0.0659080396027719, 16, 1]
Checking history sample input_X_between_0_1:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 0.59375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.421875, 0.659080396027719, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4756159782409668
Checking history sample input_X:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 30, 1, 1, 0, 1, 0, 113, 0.08743251828499332, 21, 0]
Checking history sample input_X_between_0_1:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 0.9375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8828125, 0.8743251828499332, 0.4375, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9556006193161011
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.1824 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6549451947212219, 0.581122636795044, 0.4394689202308655, 0.0038709044456481934, 0.12411868572235107, 0.7553650140762329, 0.7025225162506104, 0.8282178044319153, 0.37849199771881104, 0.9303632974624634, 0.9965434670448303, 0.7267047166824341, 0.6706951260566711, 0.4342486262321472, 0.3958852291107178, 0.11984018981456757, 0.2099323272705078, 0.9677094221115112, 0.11420929431915283]  ‚Üí  acq = -0.6092938304018309
X = [0.40243154764175415, 0.07826733589172363, 0.08017539978027344, 0.31200170516967773, 0.14650726318359375, 0.4445154070854187, 0.22343933582305908, 0.33215200901031494, 0.6393386125564575, 0.815319836139679, 0.7810757756233215, 0.9635238647460938, 0.8830317854881287, 0.3339361548423767, 0.042904794216156006, 0.22763045132160187, 0.641526460647583, 0.20095549523830414, 0.6443977355957031]  ‚Üí  acq = -0.6075638751871818
X = [0.8815333247184753, 0.022556722164154053, 0.5699729323387146, 0.4533590078353882, 0.2668758034706116, 0.45830070972442627, 0.8807499408721924, 0.892153263092041, 0.9773105978965759, 0.1383569836616516, 0.32560062408447266, 0.12259423732757568, 0.5776329636573792, 0.43875932693481445, 0.30058127641677856, 0.5809701085090637, 0.887756884098053, 0.740179181098938, 0.7725746631622314]  ‚Üí  acq = -0.6092937143094719
X = [0.47665417194366455, 0.7221311926841736, 0.6025275588035583, 0.7923617959022522, 0.8916401267051697, 0.0024828314781188965, 0.24775218963623047, 0.21860486268997192, 0.6357476711273193, 0.205609992146492, 0.5861812829971313, 0.2811993956565857, 0.6518978476524353, 0.2616347670555115, 0.9672335386276245, 0.4107861816883087, 0.9421751499176025, 0.9009174108505249, 0.4525725245475769]  ‚Üí  acq = -0.6093014207082784
X = [0.9697707891464233, 0.645775556564331, 0.7356160879135132, 0.73679119348526, 0.056879281997680664, 0.6512827277183533, 0.8247882723808289, 0.7734095454216003, 0.6373727321624756, 0.6653141975402832, 0.18850946426391602, 0.13400322198867798, 0.38446635007858276, 0.6799762845039368, 0.5053118467330933, 0.13130946457386017, 0.3579455018043518, 0.12991487979888916, 0.7011235356330872]  ‚Üí  acq = -0.6092938320536875
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.1792, dtype=torch.float64), 0, tensor(0.0211, dtype=torch.float64), tensor(0.2048, dtype=torch.float64), 0, tensor(0.3280, dtype=torch.float64), 0, 0, tensor(0.2668, dtype=torch.float64), 32, 0, 0, 1, 1, 0, 2, 0.0, 48.0, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.1792, dtype=torch.float64), tensor(8.6204e-18, dtype=torch.float64), tensor(0.0211, dtype=torch.float64), tensor(0.2048, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3280, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1874e-17, dtype=torch.float64), tensor(0.2668, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.179
  gsm8k: 0
  rowan_hellaswag: 0.021
  sciq: 0.205
  triviaqa: 0
  truthfulqa_gen: 0.328
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.267

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,359,296 || all params: 8,032,620,544 || trainable%: 0.0294
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260101_152436-sd03w17x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/sd03w17x
{'loss': 2.8713, 'grad_norm': 4.979069709777832, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4225407838821411, 'eval_runtime': 10.1897, 'eval_samples_per_second': 98.138, 'eval_steps_per_second': 6.183, 'epoch': 0.04}
{'loss': 1.0309, 'grad_norm': 2.3306050300598145, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3596774339675903, 'eval_runtime': 10.2489, 'eval_samples_per_second': 97.571, 'eval_steps_per_second': 6.147, 'epoch': 0.08}
{'loss': 0.915, 'grad_norm': 2.0593535900115967, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3744010925292969, 'eval_runtime': 10.2494, 'eval_samples_per_second': 97.567, 'eval_steps_per_second': 6.147, 'epoch': 0.12}
{'loss': 0.8799, 'grad_norm': 1.7623896598815918, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3752939701080322, 'eval_runtime': 10.2721, 'eval_samples_per_second': 97.351, 'eval_steps_per_second': 6.133, 'epoch': 0.16}
{'loss': 0.8521, 'grad_norm': 1.6778980493545532, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.402582049369812, 'eval_runtime': 10.3094, 'eval_samples_per_second': 96.999, 'eval_steps_per_second': 6.111, 'epoch': 0.2}
{'loss': 0.8107, 'grad_norm': 1.935681700706482, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.428802728652954, 'eval_runtime': 10.3303, 'eval_samples_per_second': 96.802, 'eval_steps_per_second': 6.099, 'epoch': 0.24}
{'loss': 0.8144, 'grad_norm': 2.409092903137207, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4327744245529175, 'eval_runtime': 10.3287, 'eval_samples_per_second': 96.818, 'eval_steps_per_second': 6.1, 'epoch': 0.28}
{'loss': 0.8032, 'grad_norm': 1.8722405433654785, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4346723556518555, 'eval_runtime': 10.2823, 'eval_samples_per_second': 97.255, 'eval_steps_per_second': 6.127, 'epoch': 0.32}
{'loss': 0.7811, 'grad_norm': 1.8674960136413574, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4201663732528687, 'eval_runtime': 10.2831, 'eval_samples_per_second': 97.247, 'eval_steps_per_second': 6.127, 'epoch': 0.36}
{'loss': 0.7678, 'grad_norm': 2.301119089126587, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.408930778503418, 'eval_runtime': 10.2818, 'eval_samples_per_second': 97.26, 'eval_steps_per_second': 6.127, 'epoch': 0.4}
{'loss': 0.7243, 'grad_norm': 1.9897278547286987, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4175113439559937, 'eval_runtime': 10.2634, 'eval_samples_per_second': 97.433, 'eval_steps_per_second': 6.138, 'epoch': 0.44}
{'loss': 0.7416, 'grad_norm': 1.9209823608398438, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4330670833587646, 'eval_runtime': 10.278, 'eval_samples_per_second': 97.295, 'eval_steps_per_second': 6.13, 'epoch': 0.48}
{'loss': 0.7198, 'grad_norm': 1.8723561763763428, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4825689792633057, 'eval_runtime': 10.2559, 'eval_samples_per_second': 97.505, 'eval_steps_per_second': 6.143, 'epoch': 0.52}
{'loss': 0.7116, 'grad_norm': 1.9657846689224243, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.44203519821167, 'eval_runtime': 10.2313, 'eval_samples_per_second': 97.74, 'eval_steps_per_second': 6.158, 'epoch': 0.56}
{'loss': 0.7223, 'grad_norm': 1.9818962812423706, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.457697868347168, 'eval_runtime': 10.2781, 'eval_samples_per_second': 97.294, 'eval_steps_per_second': 6.13, 'epoch': 0.6}
{'loss': 0.6904, 'grad_norm': 2.3402276039123535, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4520817995071411, 'eval_runtime': 10.282, 'eval_samples_per_second': 97.257, 'eval_steps_per_second': 6.127, 'epoch': 0.64}
{'loss': 0.6711, 'grad_norm': 2.3702094554901123, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4615713357925415, 'eval_runtime': 10.2862, 'eval_samples_per_second': 97.217, 'eval_steps_per_second': 6.125, 'epoch': 0.68}
{'loss': 0.6845, 'grad_norm': 2.25192928314209, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4475295543670654, 'eval_runtime': 10.3008, 'eval_samples_per_second': 97.08, 'eval_steps_per_second': 6.116, 'epoch': 0.72}
{'loss': 0.6805, 'grad_norm': 2.15431547164917, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4542101621627808, 'eval_runtime': 10.3008, 'eval_samples_per_second': 97.08, 'eval_steps_per_second': 6.116, 'epoch': 0.76}
{'loss': 0.6863, 'grad_norm': 1.892798662185669, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.456628680229187, 'eval_runtime': 10.2844, 'eval_samples_per_second': 97.235, 'eval_steps_per_second': 6.126, 'epoch': 0.8}
{'loss': 0.6277, 'grad_norm': 1.8291431665420532, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4724494218826294, 'eval_runtime': 10.2891, 'eval_samples_per_second': 97.19, 'eval_steps_per_second': 6.123, 'epoch': 0.84}
{'loss': 0.6494, 'grad_norm': 2.1888089179992676, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4778300523757935, 'eval_runtime': 10.2886, 'eval_samples_per_second': 97.195, 'eval_steps_per_second': 6.123, 'epoch': 0.88}
{'loss': 0.5913, 'grad_norm': 2.036374568939209, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4875203371047974, 'eval_runtime': 10.2883, 'eval_samples_per_second': 97.198, 'eval_steps_per_second': 6.123, 'epoch': 0.92}
{'loss': 0.6117, 'grad_norm': 2.0159833431243896, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4833121299743652, 'eval_runtime': 10.2894, 'eval_samples_per_second': 97.187, 'eval_steps_per_second': 6.123, 'epoch': 0.96}
{'loss': 0.6577, 'grad_norm': 2.197705030441284, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4836045503616333, 'eval_runtime': 10.2853, 'eval_samples_per_second': 97.226, 'eval_steps_per_second': 6.125, 'epoch': 1.0}
{'train_runtime': 427.1576, 'train_samples_per_second': 23.406, 'train_steps_per_second': 1.463, 'train_loss': 0.8278588912963867, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4225407838821411, 1.3596774339675903, 1.3744010925292969, 1.3752939701080322, 1.402582049369812, 1.428802728652954, 1.4327744245529175, 1.4346723556518555, 1.4201663732528687, 1.408930778503418, 1.4175113439559937, 1.4330670833587646, 1.4825689792633057, 1.44203519821167, 1.457697868347168, 1.4520817995071411, 1.4615713357925415, 1.4475295543670654, 1.4542101621627808, 1.456628680229187, 1.4724494218826294, 1.4778300523757935, 1.4875203371047974, 1.4833121299743652, 1.4836045503616333], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4225407838821411, 1.3596774339675903, 1.3744010925292969, 1.3752939701080322, 1.402582049369812, 1.428802728652954, 1.4327744245529175, 1.4346723556518555, 1.4201663732528687, 1.408930778503418, 1.4175113439559937, 1.4330670833587646, 1.4825689792633057, 1.44203519821167, 1.457697868347168, 1.4520817995071411, 1.4615713357925415, 1.4475295543670654, 1.4542101621627808, 1.456628680229187, 1.4724494218826294, 1.4778300523757935, 1.4875203371047974, 1.4833121299743652, 1.4836045503616333]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.3896827697753906
current iteration best possible eval_loss (full train run):  -1.4836045503616333
max eval_loss so far:  -1.4836045503616333
BO observations:  [-2.3896827697753906]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9478 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.34876803775227305
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.36238755263421996
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.3482770349256141
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.372377382739959
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.3628567567625569
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2068, dtype=torch.float64), tensor(0.3356, dtype=torch.float64), 0, tensor(0.4017, dtype=torch.float64), tensor(0.0200, dtype=torch.float64), 0, 0, 0, tensor(0.0358, dtype=torch.float64), 31, 0, 1, 0, 1, 1, 128, 0.09976883115501559, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2068, dtype=torch.float64), tensor(0.3356, dtype=torch.float64), tensor(2.6822e-17, dtype=torch.float64), tensor(0.4017, dtype=torch.float64), tensor(0.0200, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.1816e-18, dtype=torch.float64), tensor(6.7832e-18, dtype=torch.float64), tensor(0.0358, dtype=torch.float64), tensor(0.9703, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9977, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.207
  gsm8k: 0.336
  rowan_hellaswag: 0
  sciq: 0.402
  triviaqa: 0.02
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.036

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09976883115501559,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.09976883115501559
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 166,592,512 || all params: 8,196,853,760 || trainable%: 2.0324
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.3796, 'grad_norm': 0.507567822933197, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1225935220718384, 'eval_runtime': 10.296, 'eval_samples_per_second': 97.125, 'eval_steps_per_second': 6.119, 'epoch': 0.04}
{'loss': 0.9933, 'grad_norm': 0.4893414378166199, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9072770476341248, 'eval_runtime': 10.2629, 'eval_samples_per_second': 97.439, 'eval_steps_per_second': 6.139, 'epoch': 0.08}
{'loss': 0.8708, 'grad_norm': 0.22654004395008087, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8711513876914978, 'eval_runtime': 10.2893, 'eval_samples_per_second': 97.188, 'eval_steps_per_second': 6.123, 'epoch': 0.12}
{'loss': 0.8668, 'grad_norm': 0.20105774700641632, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8646605014801025, 'eval_runtime': 10.4087, 'eval_samples_per_second': 96.073, 'eval_steps_per_second': 6.053, 'epoch': 0.16}
{'loss': 0.8576, 'grad_norm': 0.2199186384677887, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8526634573936462, 'eval_runtime': 10.4608, 'eval_samples_per_second': 95.595, 'eval_steps_per_second': 6.022, 'epoch': 0.2}
{'loss': 0.8531, 'grad_norm': 0.2088836431503296, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8477907180786133, 'eval_runtime': 10.4953, 'eval_samples_per_second': 95.281, 'eval_steps_per_second': 6.003, 'epoch': 0.24}
{'loss': 0.8284, 'grad_norm': 0.24470514059066772, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8477572202682495, 'eval_runtime': 10.4723, 'eval_samples_per_second': 95.49, 'eval_steps_per_second': 6.016, 'epoch': 0.28}
{'loss': 0.8132, 'grad_norm': 0.21230044960975647, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8384144306182861, 'eval_runtime': 10.4898, 'eval_samples_per_second': 95.331, 'eval_steps_per_second': 6.006, 'epoch': 0.32}
{'loss': 0.8361, 'grad_norm': 0.21270853281021118, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8379244804382324, 'eval_runtime': 10.4926, 'eval_samples_per_second': 95.305, 'eval_steps_per_second': 6.004, 'epoch': 0.36}
{'loss': 0.8219, 'grad_norm': 0.19209091365337372, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8319759964942932, 'eval_runtime': 10.4907, 'eval_samples_per_second': 95.323, 'eval_steps_per_second': 6.005, 'epoch': 0.4}
{'loss': 0.8131, 'grad_norm': 0.2232227921485901, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8295477628707886, 'eval_runtime': 10.4992, 'eval_samples_per_second': 95.246, 'eval_steps_per_second': 6.0, 'epoch': 0.44}
{'loss': 0.8051, 'grad_norm': 0.23048511147499084, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.832461953163147, 'eval_runtime': 10.4849, 'eval_samples_per_second': 95.376, 'eval_steps_per_second': 6.009, 'epoch': 0.48}
{'loss': 0.8014, 'grad_norm': 0.23327894508838654, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8288292288780212, 'eval_runtime': 10.5145, 'eval_samples_per_second': 95.107, 'eval_steps_per_second': 5.992, 'epoch': 0.52}
{'loss': 0.7785, 'grad_norm': 0.2180795818567276, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8313575983047485, 'eval_runtime': 10.495, 'eval_samples_per_second': 95.284, 'eval_steps_per_second': 6.003, 'epoch': 0.56}
{'loss': 0.8218, 'grad_norm': 0.21304406225681305, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8228481411933899, 'eval_runtime': 10.4854, 'eval_samples_per_second': 95.37, 'eval_steps_per_second': 6.008, 'epoch': 0.6}
{'loss': 0.7842, 'grad_norm': 0.23339183628559113, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8210607171058655, 'eval_runtime': 10.455, 'eval_samples_per_second': 95.648, 'eval_steps_per_second': 6.026, 'epoch': 0.64}
{'loss': 0.8011, 'grad_norm': 0.21403302252292633, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8173884749412537, 'eval_runtime': 10.4026, 'eval_samples_per_second': 96.13, 'eval_steps_per_second': 6.056, 'epoch': 0.68}
{'loss': 0.7821, 'grad_norm': 0.2215433120727539, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.816648542881012, 'eval_runtime': 10.4035, 'eval_samples_per_second': 96.122, 'eval_steps_per_second': 6.056, 'epoch': 0.72}
{'loss': 0.7994, 'grad_norm': 0.21298424899578094, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8146823644638062, 'eval_runtime': 10.4129, 'eval_samples_per_second': 96.035, 'eval_steps_per_second': 6.05, 'epoch': 0.76}
{'loss': 0.7728, 'grad_norm': 0.2338416576385498, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8138139843940735, 'eval_runtime': 10.4107, 'eval_samples_per_second': 96.055, 'eval_steps_per_second': 6.051, 'epoch': 0.8}
{'loss': 0.7992, 'grad_norm': 0.19477689266204834, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8126450181007385, 'eval_runtime': 10.4248, 'eval_samples_per_second': 95.925, 'eval_steps_per_second': 6.043, 'epoch': 0.84}
{'loss': 0.792, 'grad_norm': 0.23480068147182465, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8116927742958069, 'eval_runtime': 10.4345, 'eval_samples_per_second': 95.836, 'eval_steps_per_second': 6.038, 'epoch': 0.88}
{'loss': 0.7651, 'grad_norm': 0.2305702120065689, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8097200989723206, 'eval_runtime': 10.4179, 'eval_samples_per_second': 95.989, 'eval_steps_per_second': 6.047, 'epoch': 0.92}
{'loss': 0.7847, 'grad_norm': 0.1832769215106964, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8086330890655518, 'eval_runtime': 10.4239, 'eval_samples_per_second': 95.933, 'eval_steps_per_second': 6.044, 'epoch': 0.96}
{'loss': 0.7813, 'grad_norm': 0.21479374170303345, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.808354914188385, 'eval_runtime': 10.4234, 'eval_samples_per_second': 95.938, 'eval_steps_per_second': 6.044, 'epoch': 1.0}
{'train_runtime': 484.1793, 'train_samples_per_second': 20.649, 'train_steps_per_second': 1.291, 'train_loss': 0.8801179107666015, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1225935220718384, 0.9072770476341248, 0.8711513876914978, 0.8646605014801025, 0.8526634573936462, 0.8477907180786133, 0.8477572202682495, 0.8384144306182861, 0.8379244804382324, 0.8319759964942932, 0.8295477628707886, 0.832461953163147, 0.8288292288780212, 0.8313575983047485, 0.8228481411933899, 0.8210607171058655, 0.8173884749412537, 0.816648542881012, 0.8146823644638062, 0.8138139843940735, 0.8126450181007385, 0.8116927742958069, 0.8097200989723206, 0.8086330890655518, 0.808354914188385], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1225935220718384, 0.9072770476341248, 0.8711513876914978, 0.8646605014801025, 0.8526634573936462, 0.8477907180786133, 0.8477572202682495, 0.8384144306182861, 0.8379244804382324, 0.8319759964942932, 0.8295477628707886, 0.832461953163147, 0.8288292288780212, 0.8313575983047485, 0.8228481411933899, 0.8210607171058655, 0.8173884749412537, 0.816648542881012, 0.8146823644638062, 0.8138139843940735, 0.8126450181007385, 0.8116927742958069, 0.8097200989723206, 0.8086330890655518, 0.808354914188385]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1563680171966553
current iteration best possible eval_loss (full train run):  -0.808354914188385
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1711 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.5787847267414903
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.5787767707774492
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.5787822818383739
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.5786292859402211
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.5786273096637076
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0433, dtype=torch.float64), 0, tensor(0.0800, dtype=torch.float64), tensor(0.1246, dtype=torch.float64), tensor(0.2831, dtype=torch.float64), tensor(0.2949, dtype=torch.float64), tensor(0.1742, dtype=torch.float64), 5, 1, 1, 0, 1, 1, 128, 0.03394422492111412, 12.52993411337282, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0433, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0800, dtype=torch.float64), tensor(0.1246, dtype=torch.float64), tensor(0.2831, dtype=torch.float64), tensor(0.2949, dtype=torch.float64), tensor(0.1742, dtype=torch.float64), tensor(0.1605, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3394, dtype=torch.float64), tensor(0.2610, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.043
  sciq: 0
  triviaqa: 0.08
  truthfulqa_gen: 0.125
  wikitext: 0.283
  mmlu: 0.295
  arc_challenge: 0.174

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03394422492111412,)
  num_layers_to_apply: (5,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (12.52993411337282,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  5
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.03394422492111412
lora alpha:  12.52993411337282
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 32,112,640 || all params: 8,062,373,888 || trainable%: 0.3983
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5418, 'grad_norm': 1.7007858753204346, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8472076654434204, 'eval_runtime': 9.1722, 'eval_samples_per_second': 109.025, 'eval_steps_per_second': 6.869, 'epoch': 0.04}
{'loss': 2.3509, 'grad_norm': 0.43292319774627686, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5043383836746216, 'eval_runtime': 9.1578, 'eval_samples_per_second': 109.197, 'eval_steps_per_second': 6.879, 'epoch': 0.08}
{'loss': 1.8487, 'grad_norm': 0.36919984221458435, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4432549476623535, 'eval_runtime': 9.1962, 'eval_samples_per_second': 108.74, 'eval_steps_per_second': 6.851, 'epoch': 0.12}
{'loss': 1.7297, 'grad_norm': 0.19750453531742096, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4157313108444214, 'eval_runtime': 9.1788, 'eval_samples_per_second': 108.947, 'eval_steps_per_second': 6.864, 'epoch': 0.16}
{'loss': 1.5714, 'grad_norm': 0.3740684390068054, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4279624223709106, 'eval_runtime': 9.1985, 'eval_samples_per_second': 108.714, 'eval_steps_per_second': 6.849, 'epoch': 0.2}
{'loss': 1.5506, 'grad_norm': 0.281909316778183, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4050168991088867, 'eval_runtime': 9.1999, 'eval_samples_per_second': 108.696, 'eval_steps_per_second': 6.848, 'epoch': 0.24}
{'loss': 1.5534, 'grad_norm': 0.2273970991373062, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.40614652633667, 'eval_runtime': 9.2088, 'eval_samples_per_second': 108.591, 'eval_steps_per_second': 6.841, 'epoch': 0.28}
{'loss': 1.5134, 'grad_norm': 0.25736358761787415, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4085302352905273, 'eval_runtime': 9.2304, 'eval_samples_per_second': 108.337, 'eval_steps_per_second': 6.825, 'epoch': 0.32}
{'loss': 1.4936, 'grad_norm': 0.2449633777141571, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3664426803588867, 'eval_runtime': 9.2303, 'eval_samples_per_second': 108.339, 'eval_steps_per_second': 6.825, 'epoch': 0.36}
{'loss': 1.5296, 'grad_norm': 0.26537010073661804, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.391510009765625, 'eval_runtime': 9.2295, 'eval_samples_per_second': 108.348, 'eval_steps_per_second': 6.826, 'epoch': 0.4}
{'loss': 1.4638, 'grad_norm': 0.5410733819007874, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3921215534210205, 'eval_runtime': 9.1797, 'eval_samples_per_second': 108.936, 'eval_steps_per_second': 6.863, 'epoch': 0.44}
{'loss': 1.4382, 'grad_norm': 0.2333255261182785, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.33082914352417, 'eval_runtime': 9.147, 'eval_samples_per_second': 109.325, 'eval_steps_per_second': 6.887, 'epoch': 0.48}
{'loss': 1.4358, 'grad_norm': 0.27930930256843567, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3562391996383667, 'eval_runtime': 9.1579, 'eval_samples_per_second': 109.196, 'eval_steps_per_second': 6.879, 'epoch': 0.52}
{'loss': 1.4825, 'grad_norm': 0.3180685341358185, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3732044696807861, 'eval_runtime': 9.169, 'eval_samples_per_second': 109.063, 'eval_steps_per_second': 6.871, 'epoch': 0.56}
{'loss': 1.423, 'grad_norm': 0.2688438296318054, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.36629056930542, 'eval_runtime': 9.1432, 'eval_samples_per_second': 109.37, 'eval_steps_per_second': 6.89, 'epoch': 0.6}
{'loss': 1.4264, 'grad_norm': 0.24969251453876495, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4090182781219482, 'eval_runtime': 9.154, 'eval_samples_per_second': 109.242, 'eval_steps_per_second': 6.882, 'epoch': 0.64}
{'loss': 1.4062, 'grad_norm': 0.2501969337463379, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4186463356018066, 'eval_runtime': 9.1293, 'eval_samples_per_second': 109.538, 'eval_steps_per_second': 6.901, 'epoch': 0.68}
{'loss': 1.3478, 'grad_norm': 0.6352458000183105, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4117014408111572, 'eval_runtime': 9.1317, 'eval_samples_per_second': 109.508, 'eval_steps_per_second': 6.899, 'epoch': 0.72}
{'loss': 1.4385, 'grad_norm': 0.1933729499578476, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4037858247756958, 'eval_runtime': 9.129, 'eval_samples_per_second': 109.541, 'eval_steps_per_second': 6.901, 'epoch': 0.76}
{'loss': 1.3945, 'grad_norm': 0.2546669840812683, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.382770299911499, 'eval_runtime': 9.1249, 'eval_samples_per_second': 109.591, 'eval_steps_per_second': 6.904, 'epoch': 0.8}
{'loss': 1.4065, 'grad_norm': 0.3469367027282715, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3807482719421387, 'eval_runtime': 9.1215, 'eval_samples_per_second': 109.631, 'eval_steps_per_second': 6.907, 'epoch': 0.84}
{'loss': 1.3957, 'grad_norm': 0.28298407793045044, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3609553575515747, 'eval_runtime': 9.1439, 'eval_samples_per_second': 109.362, 'eval_steps_per_second': 6.89, 'epoch': 0.88}
{'loss': 1.3984, 'grad_norm': 0.2821897864341736, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.38573157787323, 'eval_runtime': 9.2218, 'eval_samples_per_second': 108.438, 'eval_steps_per_second': 6.832, 'epoch': 0.92}
{'loss': 1.4179, 'grad_norm': 0.3997718393802643, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.379558801651001, 'eval_runtime': 9.2083, 'eval_samples_per_second': 108.598, 'eval_steps_per_second': 6.842, 'epoch': 0.96}
{'loss': 1.3915, 'grad_norm': 0.23720452189445496, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.383530855178833, 'eval_runtime': 9.238, 'eval_samples_per_second': 108.248, 'eval_steps_per_second': 6.82, 'epoch': 1.0}
{'train_runtime': 420.5937, 'train_samples_per_second': 23.766, 'train_steps_per_second': 1.486, 'train_loss': 1.5979979919433593, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8472076654434204, 1.5043383836746216, 1.4432549476623535, 1.4157313108444214, 1.4279624223709106, 1.4050168991088867, 1.40614652633667, 1.4085302352905273, 1.3664426803588867, 1.391510009765625, 1.3921215534210205, 1.33082914352417, 1.3562391996383667, 1.3732044696807861, 1.36629056930542, 1.4090182781219482, 1.4186463356018066, 1.4117014408111572, 1.4037858247756958, 1.382770299911499, 1.3807482719421387, 1.3609553575515747, 1.38573157787323, 1.379558801651001, 1.383530855178833], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8472076654434204, 1.5043383836746216, 1.4432549476623535, 1.4157313108444214, 1.4279624223709106, 1.4050168991088867, 1.40614652633667, 1.4085302352905273, 1.3664426803588867, 1.391510009765625, 1.3921215534210205, 1.33082914352417, 1.3562391996383667, 1.3732044696807861, 1.36629056930542, 1.4090182781219482, 1.4186463356018066, 1.4117014408111572, 1.4037858247756958, 1.382770299911499, 1.3807482719421387, 1.3609553575515747, 1.38573157787323, 1.379558801651001, 1.383530855178833]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3532873392105103
current iteration best possible eval_loss (full train run):  -1.383530855178833
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6412 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.5644048407071209
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.5644048382683642
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.5644048385033584
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.5644048381640123
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.5644048384830684
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0503, dtype=torch.float64), tensor(0.3701, dtype=torch.float64), 0, 0, tensor(0.0117, dtype=torch.float64), tensor(0.3023, dtype=torch.float64), tensor(0.0522, dtype=torch.float64), 0, tensor(0.2134, dtype=torch.float64), 24, 1, 1, 0, 1, 0, 128, 0.1, 7.947738874650424, 1]
normalized proposed parameters for next round by BO: [tensor(0.0503, dtype=torch.float64), tensor(0.3701, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0117, dtype=torch.float64), tensor(0.3023, dtype=torch.float64), tensor(0.0522, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2134, dtype=torch.float64), tensor(0.7374, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1656, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.05
  gsm8k: 0.37
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.012
  truthfulqa_gen: 0.302
  wikitext: 0.052
  mmlu: 0
  arc_challenge: 0.213

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (7.947738874650424,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  7.947738874650424
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 97,517,568 || all params: 8,127,778,816 || trainable%: 1.1998
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0132, 'grad_norm': 0.6276857852935791, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6149340867996216, 'eval_runtime': 9.8372, 'eval_samples_per_second': 101.655, 'eval_steps_per_second': 6.404, 'epoch': 0.04}
{'loss': 1.5612, 'grad_norm': 0.18410074710845947, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0994807481765747, 'eval_runtime': 9.8336, 'eval_samples_per_second': 101.692, 'eval_steps_per_second': 6.407, 'epoch': 0.08}
{'loss': 1.1168, 'grad_norm': 0.27871832251548767, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9836068153381348, 'eval_runtime': 9.8498, 'eval_samples_per_second': 101.525, 'eval_steps_per_second': 6.396, 'epoch': 0.12}
{'loss': 1.0428, 'grad_norm': 0.13303330540657043, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.937522828578949, 'eval_runtime': 9.871, 'eval_samples_per_second': 101.307, 'eval_steps_per_second': 6.382, 'epoch': 0.16}
{'loss': 1.0267, 'grad_norm': 0.13015428185462952, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9240384101867676, 'eval_runtime': 9.8904, 'eval_samples_per_second': 101.108, 'eval_steps_per_second': 6.37, 'epoch': 0.2}
{'loss': 0.9826, 'grad_norm': 0.13444003462791443, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9124928116798401, 'eval_runtime': 9.9038, 'eval_samples_per_second': 100.971, 'eval_steps_per_second': 6.361, 'epoch': 0.24}
{'loss': 0.9796, 'grad_norm': 0.1534876823425293, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8966811299324036, 'eval_runtime': 9.8954, 'eval_samples_per_second': 101.057, 'eval_steps_per_second': 6.367, 'epoch': 0.28}
{'loss': 0.9325, 'grad_norm': 0.15945297479629517, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8832413554191589, 'eval_runtime': 9.8836, 'eval_samples_per_second': 101.178, 'eval_steps_per_second': 6.374, 'epoch': 0.32}
{'loss': 0.9112, 'grad_norm': 0.11234571039676666, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8736193180084229, 'eval_runtime': 9.8968, 'eval_samples_per_second': 101.043, 'eval_steps_per_second': 6.366, 'epoch': 0.36}
{'loss': 0.9247, 'grad_norm': 0.12504589557647705, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8634085655212402, 'eval_runtime': 9.8454, 'eval_samples_per_second': 101.57, 'eval_steps_per_second': 6.399, 'epoch': 0.4}
{'loss': 0.886, 'grad_norm': 0.13502541184425354, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.862809419631958, 'eval_runtime': 9.8188, 'eval_samples_per_second': 101.845, 'eval_steps_per_second': 6.416, 'epoch': 0.44}
{'loss': 0.9157, 'grad_norm': 0.14819619059562683, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8624586462974548, 'eval_runtime': 9.8084, 'eval_samples_per_second': 101.953, 'eval_steps_per_second': 6.423, 'epoch': 0.48}
{'loss': 0.8965, 'grad_norm': 0.1310908943414688, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8579543232917786, 'eval_runtime': 9.8267, 'eval_samples_per_second': 101.763, 'eval_steps_per_second': 6.411, 'epoch': 0.52}
{'loss': 0.8819, 'grad_norm': 0.12473011761903763, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8545313477516174, 'eval_runtime': 9.8164, 'eval_samples_per_second': 101.871, 'eval_steps_per_second': 6.418, 'epoch': 0.56}
{'loss': 0.889, 'grad_norm': 0.1392655223608017, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8526607751846313, 'eval_runtime': 9.8184, 'eval_samples_per_second': 101.849, 'eval_steps_per_second': 6.416, 'epoch': 0.6}
{'loss': 0.8841, 'grad_norm': 0.1323452591896057, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8465836048126221, 'eval_runtime': 9.8258, 'eval_samples_per_second': 101.773, 'eval_steps_per_second': 6.412, 'epoch': 0.64}
{'loss': 0.8936, 'grad_norm': 0.13958774507045746, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.848113477230072, 'eval_runtime': 9.9035, 'eval_samples_per_second': 100.975, 'eval_steps_per_second': 6.361, 'epoch': 0.68}
{'loss': 0.8641, 'grad_norm': 0.13332246243953705, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8441914319992065, 'eval_runtime': 9.9079, 'eval_samples_per_second': 100.929, 'eval_steps_per_second': 6.359, 'epoch': 0.72}
{'loss': 0.861, 'grad_norm': 0.13994450867176056, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8461090326309204, 'eval_runtime': 9.8855, 'eval_samples_per_second': 101.159, 'eval_steps_per_second': 6.373, 'epoch': 0.76}
{'loss': 0.901, 'grad_norm': 0.11155454814434052, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8427000045776367, 'eval_runtime': 9.8862, 'eval_samples_per_second': 101.151, 'eval_steps_per_second': 6.373, 'epoch': 0.8}
{'loss': 0.8706, 'grad_norm': 0.11395534873008728, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8440784811973572, 'eval_runtime': 9.8874, 'eval_samples_per_second': 101.139, 'eval_steps_per_second': 6.372, 'epoch': 0.84}
{'loss': 0.859, 'grad_norm': 0.14075635373592377, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8419183492660522, 'eval_runtime': 9.8871, 'eval_samples_per_second': 101.142, 'eval_steps_per_second': 6.372, 'epoch': 0.88}
{'loss': 0.8721, 'grad_norm': 0.11227941513061523, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8390707969665527, 'eval_runtime': 9.8842, 'eval_samples_per_second': 101.172, 'eval_steps_per_second': 6.374, 'epoch': 0.92}
{'loss': 0.879, 'grad_norm': 0.130384624004364, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8396614193916321, 'eval_runtime': 9.8762, 'eval_samples_per_second': 101.254, 'eval_steps_per_second': 6.379, 'epoch': 0.96}
{'loss': 0.8807, 'grad_norm': 0.12742526829242706, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8388087749481201, 'eval_runtime': 9.8993, 'eval_samples_per_second': 101.017, 'eval_steps_per_second': 6.364, 'epoch': 1.0}
{'train_runtime': 462.3684, 'train_samples_per_second': 21.623, 'train_steps_per_second': 1.352, 'train_loss': 1.0290187194824219, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6149340867996216, 1.0994807481765747, 0.9836068153381348, 0.937522828578949, 0.9240384101867676, 0.9124928116798401, 0.8966811299324036, 0.8832413554191589, 0.8736193180084229, 0.8634085655212402, 0.862809419631958, 0.8624586462974548, 0.8579543232917786, 0.8545313477516174, 0.8526607751846313, 0.8465836048126221, 0.848113477230072, 0.8441914319992065, 0.8461090326309204, 0.8427000045776367, 0.8440784811973572, 0.8419183492660522, 0.8390707969665527, 0.8396614193916321, 0.8388087749481201], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6149340867996216, 1.0994807481765747, 0.9836068153381348, 0.937522828578949, 0.9240384101867676, 0.9124928116798401, 0.8966811299324036, 0.8832413554191589, 0.8736193180084229, 0.8634085655212402, 0.862809419631958, 0.8624586462974548, 0.8579543232917786, 0.8545313477516174, 0.8526607751846313, 0.8465836048126221, 0.848113477230072, 0.8441914319992065, 0.8461090326309204, 0.8427000045776367, 0.8440784811973572, 0.8419183492660522, 0.8390707969665527, 0.8396614193916321, 0.8388087749481201]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0237957239151
current iteration best possible eval_loss (full train run):  -0.8388087749481201
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6828 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.6945430767972736
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.6945430767744167
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.6945430767758536
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.6945430767757139
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.6945430767781673
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.6795, dtype=torch.float64), 0, 0, 0, tensor(0.0957, dtype=torch.float64), 0, 0, tensor(0.2248, dtype=torch.float64), 1, 1, 0, 0, 1, 1, 128, 0.1, 28.10351891421814, 0]
normalized proposed parameters for next round by BO: [tensor(1.9547e-17, dtype=torch.float64), tensor(0.6795, dtype=torch.float64), tensor(7.8785e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.4845e-17, dtype=torch.float64), tensor(0.0957, dtype=torch.float64), tensor(3.0945e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2248, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5855, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.679
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.096
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.225

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (28.10351891421814,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  28.10351891421814
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 5,767,168 || all params: 8,036,028,416 || trainable%: 0.0718
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9093, 'grad_norm': 0.8236772418022156, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.923261284828186, 'eval_runtime': 8.8678, 'eval_samples_per_second': 112.767, 'eval_steps_per_second': 7.104, 'epoch': 0.04}
{'loss': 1.6534, 'grad_norm': 0.4019530117511749, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1910359859466553, 'eval_runtime': 8.9518, 'eval_samples_per_second': 111.709, 'eval_steps_per_second': 7.038, 'epoch': 0.08}
{'loss': 1.1911, 'grad_norm': 0.21224407851696014, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0499358177185059, 'eval_runtime': 8.9582, 'eval_samples_per_second': 111.63, 'eval_steps_per_second': 7.033, 'epoch': 0.12}
{'loss': 1.057, 'grad_norm': 0.15685461461544037, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0108907222747803, 'eval_runtime': 9.0058, 'eval_samples_per_second': 111.04, 'eval_steps_per_second': 6.996, 'epoch': 0.16}
{'loss': 1.0176, 'grad_norm': 0.18221715092658997, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.993960440158844, 'eval_runtime': 9.0196, 'eval_samples_per_second': 110.87, 'eval_steps_per_second': 6.985, 'epoch': 0.2}
{'loss': 1.0099, 'grad_norm': 0.1981888711452484, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9773874282836914, 'eval_runtime': 9.0129, 'eval_samples_per_second': 110.953, 'eval_steps_per_second': 6.99, 'epoch': 0.24}
{'loss': 1.0071, 'grad_norm': 0.15891321003437042, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9676945805549622, 'eval_runtime': 9.022, 'eval_samples_per_second': 110.84, 'eval_steps_per_second': 6.983, 'epoch': 0.28}
{'loss': 0.9939, 'grad_norm': 0.21964393556118011, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9646278023719788, 'eval_runtime': 9.0258, 'eval_samples_per_second': 110.794, 'eval_steps_per_second': 6.98, 'epoch': 0.32}
{'loss': 0.9744, 'grad_norm': 0.16161087155342102, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9539896845817566, 'eval_runtime': 9.0277, 'eval_samples_per_second': 110.77, 'eval_steps_per_second': 6.978, 'epoch': 0.36}
{'loss': 0.9776, 'grad_norm': 0.19670364260673523, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9529545903205872, 'eval_runtime': 9.0226, 'eval_samples_per_second': 110.833, 'eval_steps_per_second': 6.983, 'epoch': 0.4}
{'loss': 0.9673, 'grad_norm': 0.22066053748130798, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9488018751144409, 'eval_runtime': 9.0162, 'eval_samples_per_second': 110.911, 'eval_steps_per_second': 6.987, 'epoch': 0.44}
{'loss': 0.9641, 'grad_norm': 0.18532030284404755, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9508163332939148, 'eval_runtime': 9.0093, 'eval_samples_per_second': 110.997, 'eval_steps_per_second': 6.993, 'epoch': 0.48}
{'loss': 0.9593, 'grad_norm': 0.20646753907203674, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9437591433525085, 'eval_runtime': 9.008, 'eval_samples_per_second': 111.012, 'eval_steps_per_second': 6.994, 'epoch': 0.52}
{'loss': 0.9489, 'grad_norm': 0.2011808156967163, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9424033164978027, 'eval_runtime': 9.0309, 'eval_samples_per_second': 110.731, 'eval_steps_per_second': 6.976, 'epoch': 0.56}
{'loss': 0.9563, 'grad_norm': 0.17183344066143036, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9395759105682373, 'eval_runtime': 9.0612, 'eval_samples_per_second': 110.36, 'eval_steps_per_second': 6.953, 'epoch': 0.6}
{'loss': 0.9437, 'grad_norm': 0.16617079079151154, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9374587535858154, 'eval_runtime': 9.0671, 'eval_samples_per_second': 110.288, 'eval_steps_per_second': 6.948, 'epoch': 0.64}
{'loss': 0.9499, 'grad_norm': 0.15593606233596802, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9353401064872742, 'eval_runtime': 9.0513, 'eval_samples_per_second': 110.481, 'eval_steps_per_second': 6.96, 'epoch': 0.68}
{'loss': 0.9353, 'grad_norm': 0.21074047684669495, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.935107946395874, 'eval_runtime': 9.0567, 'eval_samples_per_second': 110.415, 'eval_steps_per_second': 6.956, 'epoch': 0.72}
{'loss': 0.9306, 'grad_norm': 0.16885195672512054, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9342973232269287, 'eval_runtime': 9.0641, 'eval_samples_per_second': 110.325, 'eval_steps_per_second': 6.95, 'epoch': 0.76}
{'loss': 0.9288, 'grad_norm': 0.17321066558361053, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9313406348228455, 'eval_runtime': 9.0454, 'eval_samples_per_second': 110.554, 'eval_steps_per_second': 6.965, 'epoch': 0.8}
{'loss': 0.9274, 'grad_norm': 0.1547883152961731, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9304437637329102, 'eval_runtime': 9.0392, 'eval_samples_per_second': 110.63, 'eval_steps_per_second': 6.97, 'epoch': 0.84}
{'loss': 0.9345, 'grad_norm': 0.13372468948364258, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9302850365638733, 'eval_runtime': 9.0474, 'eval_samples_per_second': 110.529, 'eval_steps_per_second': 6.963, 'epoch': 0.88}
{'loss': 0.915, 'grad_norm': 0.13583414256572723, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.930020272731781, 'eval_runtime': 9.0404, 'eval_samples_per_second': 110.615, 'eval_steps_per_second': 6.969, 'epoch': 0.92}
{'loss': 0.9263, 'grad_norm': 0.14992853999137878, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9289247393608093, 'eval_runtime': 9.059, 'eval_samples_per_second': 110.387, 'eval_steps_per_second': 6.954, 'epoch': 0.96}
{'loss': 0.9357, 'grad_norm': 0.18601635098457336, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9287604689598083, 'eval_runtime': 9.0304, 'eval_samples_per_second': 110.736, 'eval_steps_per_second': 6.976, 'epoch': 1.0}
{'train_runtime': 324.1624, 'train_samples_per_second': 30.846, 'train_steps_per_second': 1.928, 'train_loss': 1.0765791687011719, 'epoch': 1.0}
train_results:  {'eval_loss': [1.923261284828186, 1.1910359859466553, 1.0499358177185059, 1.0108907222747803, 0.993960440158844, 0.9773874282836914, 0.9676945805549622, 0.9646278023719788, 0.9539896845817566, 0.9529545903205872, 0.9488018751144409, 0.9508163332939148, 0.9437591433525085, 0.9424033164978027, 0.9395759105682373, 0.9374587535858154, 0.9353401064872742, 0.935107946395874, 0.9342973232269287, 0.9313406348228455, 0.9304437637329102, 0.9302850365638733, 0.930020272731781, 0.9289247393608093, 0.9287604689598083], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.923261284828186, 1.1910359859466553, 1.0499358177185059, 1.0108907222747803, 0.993960440158844, 0.9773874282836914, 0.9676945805549622, 0.9646278023719788, 0.9539896845817566, 0.9529545903205872, 0.9488018751144409, 0.9508163332939148, 0.9437591433525085, 0.9424033164978027, 0.9395759105682373, 0.9374587535858154, 0.9353401064872742, 0.935107946395874, 0.9342973232269287, 0.9313406348228455, 0.9304437637329102, 0.9302850365638733, 0.930020272731781, 0.9289247393608093, 0.9287604689598083]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.6149659156799316
current iteration best possible eval_loss (full train run):  -0.9287604689598083
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9580 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.7034152389335121
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.7053945113093208
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.7053389802680684
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.7048733591619463
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.7053897218248836
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3243, dtype=torch.float64), tensor(0.2323, dtype=torch.float64), 0, tensor(0.1898, dtype=torch.float64), 0, 0, 0, tensor(0.0278, dtype=torch.float64), tensor(0.2258, dtype=torch.float64), 26, 0, 1, 0, 1, 1, 2, 3.0357660829594127e-19, 19.675877712940242, 0]
normalized proposed parameters for next round by BO: [tensor(0.3243, dtype=torch.float64), tensor(0.2323, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1898, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.6451e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0278, dtype=torch.float64), tensor(0.2258, dtype=torch.float64), tensor(0.8273, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.0358e-18, dtype=torch.float64), tensor(0.4099, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.324
  gsm8k: 0.232
  rowan_hellaswag: 0
  sciq: 0.19
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.028
  arc_challenge: 0.226

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.0357660829594127e-19,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (19.675877712940242,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  2
lora dropout:  3.0357660829594127e-19
lora alpha:  19.675877712940242
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,183,168 || all params: 8,032,444,416 || trainable%: 0.0272
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.857, 'grad_norm': 4.774225234985352, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3727184534072876, 'eval_runtime': 10.1516, 'eval_samples_per_second': 98.507, 'eval_steps_per_second': 6.206, 'epoch': 0.04}
{'loss': 1.1411, 'grad_norm': 1.5514953136444092, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.934320330619812, 'eval_runtime': 10.2735, 'eval_samples_per_second': 97.338, 'eval_steps_per_second': 6.132, 'epoch': 0.08}
{'loss': 0.9123, 'grad_norm': 0.9829100966453552, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8916933536529541, 'eval_runtime': 10.2714, 'eval_samples_per_second': 97.358, 'eval_steps_per_second': 6.134, 'epoch': 0.12}
{'loss': 0.9189, 'grad_norm': 0.9781246185302734, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.881781816482544, 'eval_runtime': 10.2931, 'eval_samples_per_second': 97.152, 'eval_steps_per_second': 6.121, 'epoch': 0.16}
{'loss': 0.9053, 'grad_norm': 0.7833995223045349, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8744704723358154, 'eval_runtime': 10.3435, 'eval_samples_per_second': 96.679, 'eval_steps_per_second': 6.091, 'epoch': 0.2}
{'loss': 0.8818, 'grad_norm': 1.0342615842819214, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8678027391433716, 'eval_runtime': 10.3283, 'eval_samples_per_second': 96.821, 'eval_steps_per_second': 6.1, 'epoch': 0.24}
{'loss': 0.8591, 'grad_norm': 0.8653951287269592, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.866948664188385, 'eval_runtime': 10.3459, 'eval_samples_per_second': 96.657, 'eval_steps_per_second': 6.089, 'epoch': 0.28}
{'loss': 0.8564, 'grad_norm': 0.7709598541259766, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8612958788871765, 'eval_runtime': 10.3345, 'eval_samples_per_second': 96.764, 'eval_steps_per_second': 6.096, 'epoch': 0.32}
{'loss': 0.8443, 'grad_norm': 0.7889951467514038, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8572266697883606, 'eval_runtime': 10.3231, 'eval_samples_per_second': 96.87, 'eval_steps_per_second': 6.103, 'epoch': 0.36}
{'loss': 0.8703, 'grad_norm': 0.8263788819313049, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8515270948410034, 'eval_runtime': 10.3408, 'eval_samples_per_second': 96.704, 'eval_steps_per_second': 6.092, 'epoch': 0.4}
{'loss': 0.8242, 'grad_norm': 0.753140926361084, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8513680696487427, 'eval_runtime': 10.3357, 'eval_samples_per_second': 96.752, 'eval_steps_per_second': 6.095, 'epoch': 0.44}
{'loss': 0.852, 'grad_norm': 0.8816570043563843, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8475630879402161, 'eval_runtime': 10.3206, 'eval_samples_per_second': 96.894, 'eval_steps_per_second': 6.104, 'epoch': 0.48}
{'loss': 0.8358, 'grad_norm': 0.9187991619110107, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8488368391990662, 'eval_runtime': 10.3296, 'eval_samples_per_second': 96.809, 'eval_steps_per_second': 6.099, 'epoch': 0.52}
{'loss': 0.8329, 'grad_norm': 1.0052318572998047, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8463765978813171, 'eval_runtime': 10.316, 'eval_samples_per_second': 96.937, 'eval_steps_per_second': 6.107, 'epoch': 0.56}
{'loss': 0.8153, 'grad_norm': 0.8667845129966736, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8429380655288696, 'eval_runtime': 10.3419, 'eval_samples_per_second': 96.694, 'eval_steps_per_second': 6.092, 'epoch': 0.6}
{'loss': 0.8117, 'grad_norm': 0.8924363255500793, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8421019315719604, 'eval_runtime': 10.3359, 'eval_samples_per_second': 96.75, 'eval_steps_per_second': 6.095, 'epoch': 0.64}
{'loss': 0.7988, 'grad_norm': 1.0208392143249512, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8419516682624817, 'eval_runtime': 10.3229, 'eval_samples_per_second': 96.872, 'eval_steps_per_second': 6.103, 'epoch': 0.68}
{'loss': 0.7975, 'grad_norm': 1.1163400411605835, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.83984375, 'eval_runtime': 10.3004, 'eval_samples_per_second': 97.084, 'eval_steps_per_second': 6.116, 'epoch': 0.72}
{'loss': 0.7941, 'grad_norm': 0.8266165256500244, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8371438384056091, 'eval_runtime': 10.2877, 'eval_samples_per_second': 97.204, 'eval_steps_per_second': 6.124, 'epoch': 0.76}
{'loss': 0.8129, 'grad_norm': 0.9226499199867249, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8366938233375549, 'eval_runtime': 10.3053, 'eval_samples_per_second': 97.037, 'eval_steps_per_second': 6.113, 'epoch': 0.8}
{'loss': 0.7725, 'grad_norm': 1.046496033668518, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.836556613445282, 'eval_runtime': 10.2839, 'eval_samples_per_second': 97.239, 'eval_steps_per_second': 6.126, 'epoch': 0.84}
{'loss': 0.7996, 'grad_norm': 1.039974570274353, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8355383276939392, 'eval_runtime': 10.2727, 'eval_samples_per_second': 97.345, 'eval_steps_per_second': 6.133, 'epoch': 0.88}
{'loss': 0.7987, 'grad_norm': 1.2398755550384521, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8346131443977356, 'eval_runtime': 10.2761, 'eval_samples_per_second': 97.314, 'eval_steps_per_second': 6.131, 'epoch': 0.92}
{'loss': 0.7855, 'grad_norm': 0.868582546710968, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8338327407836914, 'eval_runtime': 10.2634, 'eval_samples_per_second': 97.433, 'eval_steps_per_second': 6.138, 'epoch': 0.96}
{'loss': 0.8038, 'grad_norm': 1.2919687032699585, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8330869078636169, 'eval_runtime': 10.2589, 'eval_samples_per_second': 97.476, 'eval_steps_per_second': 6.141, 'epoch': 1.0}
{'train_runtime': 458.3749, 'train_samples_per_second': 21.81, 'train_steps_per_second': 1.364, 'train_loss': 0.9272791107177735, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3727184534072876, 0.934320330619812, 0.8916933536529541, 0.881781816482544, 0.8744704723358154, 0.8678027391433716, 0.866948664188385, 0.8612958788871765, 0.8572266697883606, 0.8515270948410034, 0.8513680696487427, 0.8475630879402161, 0.8488368391990662, 0.8463765978813171, 0.8429380655288696, 0.8421019315719604, 0.8419516682624817, 0.83984375, 0.8371438384056091, 0.8366938233375549, 0.836556613445282, 0.8355383276939392, 0.8346131443977356, 0.8338327407836914, 0.8330869078636169], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3727184534072876, 0.934320330619812, 0.8916933536529541, 0.881781816482544, 0.8744704723358154, 0.8678027391433716, 0.866948664188385, 0.8612958788871765, 0.8572266697883606, 0.8515270948410034, 0.8513680696487427, 0.8475630879402161, 0.8488368391990662, 0.8463765978813171, 0.8429380655288696, 0.8421019315719604, 0.8419516682624817, 0.83984375, 0.8371438384056091, 0.8366938233375549, 0.836556613445282, 0.8355383276939392, 0.8346131443977356, 0.8338327407836914, 0.8330869078636169]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.472610354423523
current iteration best possible eval_loss (full train run):  -0.8330869078636169
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.1795 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.8137373482506159
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.8137373482598568
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.8137373482598566
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.8137373221002382
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.8137373482254021
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0655, dtype=torch.float64), tensor(0.2511, dtype=torch.float64), 0, tensor(0.1076, dtype=torch.float64), 0, tensor(0.0541, dtype=torch.float64), tensor(0.1893, dtype=torch.float64), 0, tensor(0.3325, dtype=torch.float64), 1, 0, 1, 0, 0, 0, 128, 0.026018275590245665, 33.02209143916859, 0]
normalized proposed parameters for next round by BO: [tensor(0.0655, dtype=torch.float64), tensor(0.2511, dtype=torch.float64), tensor(3.0461e-16, dtype=torch.float64), tensor(0.1076, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0541, dtype=torch.float64), tensor(0.1893, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3325, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2602, dtype=torch.float64), tensor(0.6880, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.065
  gsm8k: 0.251
  rowan_hellaswag: 0
  sciq: 0.108
  triviaqa: 0
  truthfulqa_gen: 0.054
  wikitext: 0.189
  mmlu: 0
  arc_challenge: 0.333

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.026018275590245665,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (33.02209143916859,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  128
lora dropout:  0.026018275590245665
lora alpha:  33.02209143916859
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 655,360 || all params: 8,030,916,608 || trainable%: 0.0082
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6787, 'grad_norm': 0.46788525581359863, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.5157885551452637, 'eval_runtime': 8.8491, 'eval_samples_per_second': 113.006, 'eval_steps_per_second': 7.119, 'epoch': 0.04}
{'loss': 3.0067, 'grad_norm': 0.48533719778060913, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.9092484712600708, 'eval_runtime': 8.9366, 'eval_samples_per_second': 111.899, 'eval_steps_per_second': 7.05, 'epoch': 0.08}
{'loss': 2.4252, 'grad_norm': 0.2828465700149536, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7255183458328247, 'eval_runtime': 8.8942, 'eval_samples_per_second': 112.433, 'eval_steps_per_second': 7.083, 'epoch': 0.12}
{'loss': 2.1496, 'grad_norm': 0.1696658730506897, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5984687805175781, 'eval_runtime': 8.9062, 'eval_samples_per_second': 112.281, 'eval_steps_per_second': 7.074, 'epoch': 0.16}
{'loss': 1.9895, 'grad_norm': 0.1754126101732254, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.498115062713623, 'eval_runtime': 8.9473, 'eval_samples_per_second': 111.765, 'eval_steps_per_second': 7.041, 'epoch': 0.2}
{'loss': 1.8364, 'grad_norm': 0.12237383425235748, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4128737449645996, 'eval_runtime': 8.9581, 'eval_samples_per_second': 111.63, 'eval_steps_per_second': 7.033, 'epoch': 0.24}
{'loss': 1.7538, 'grad_norm': 0.14096425473690033, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3604074716567993, 'eval_runtime': 8.9498, 'eval_samples_per_second': 111.734, 'eval_steps_per_second': 7.039, 'epoch': 0.28}
{'loss': 1.6526, 'grad_norm': 0.14042845368385315, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3171619176864624, 'eval_runtime': 8.9556, 'eval_samples_per_second': 111.662, 'eval_steps_per_second': 7.035, 'epoch': 0.32}
{'loss': 1.6315, 'grad_norm': 0.1463361233472824, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2887446880340576, 'eval_runtime': 8.9632, 'eval_samples_per_second': 111.568, 'eval_steps_per_second': 7.029, 'epoch': 0.36}
{'loss': 1.5707, 'grad_norm': 0.1289592981338501, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2642728090286255, 'eval_runtime': 8.9666, 'eval_samples_per_second': 111.525, 'eval_steps_per_second': 7.026, 'epoch': 0.4}
{'loss': 1.5872, 'grad_norm': 0.12477140873670578, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2446515560150146, 'eval_runtime': 8.9734, 'eval_samples_per_second': 111.441, 'eval_steps_per_second': 7.021, 'epoch': 0.44}
{'loss': 1.5242, 'grad_norm': 0.10406079888343811, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.223596215248108, 'eval_runtime': 8.9851, 'eval_samples_per_second': 111.296, 'eval_steps_per_second': 7.012, 'epoch': 0.48}
{'loss': 1.4372, 'grad_norm': 0.103154256939888, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.205610990524292, 'eval_runtime': 9.0139, 'eval_samples_per_second': 110.94, 'eval_steps_per_second': 6.989, 'epoch': 0.52}
{'loss': 1.4028, 'grad_norm': 0.10851942747831345, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1932233572006226, 'eval_runtime': 8.9833, 'eval_samples_per_second': 111.318, 'eval_steps_per_second': 7.013, 'epoch': 0.56}
{'loss': 1.4001, 'grad_norm': 0.11327856779098511, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1835167407989502, 'eval_runtime': 8.9978, 'eval_samples_per_second': 111.138, 'eval_steps_per_second': 7.002, 'epoch': 0.6}
{'loss': 1.3928, 'grad_norm': 0.13368350267410278, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1743888854980469, 'eval_runtime': 8.9842, 'eval_samples_per_second': 111.306, 'eval_steps_per_second': 7.012, 'epoch': 0.64}
{'loss': 1.3795, 'grad_norm': 0.10110989212989807, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1665263175964355, 'eval_runtime': 8.9862, 'eval_samples_per_second': 111.282, 'eval_steps_per_second': 7.011, 'epoch': 0.68}
{'loss': 1.3976, 'grad_norm': 0.10325206071138382, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1625282764434814, 'eval_runtime': 8.9948, 'eval_samples_per_second': 111.175, 'eval_steps_per_second': 7.004, 'epoch': 0.72}
{'loss': 1.4121, 'grad_norm': 0.13488972187042236, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1553630828857422, 'eval_runtime': 8.9977, 'eval_samples_per_second': 111.139, 'eval_steps_per_second': 7.002, 'epoch': 0.76}
{'loss': 1.4283, 'grad_norm': 0.11335737258195877, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1536860466003418, 'eval_runtime': 8.9918, 'eval_samples_per_second': 111.212, 'eval_steps_per_second': 7.006, 'epoch': 0.8}
{'loss': 1.3879, 'grad_norm': 0.11559262871742249, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1487796306610107, 'eval_runtime': 8.9601, 'eval_samples_per_second': 111.606, 'eval_steps_per_second': 7.031, 'epoch': 0.84}
{'loss': 1.4005, 'grad_norm': 0.1105775386095047, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1467816829681396, 'eval_runtime': 8.981, 'eval_samples_per_second': 111.347, 'eval_steps_per_second': 7.015, 'epoch': 0.88}
{'loss': 1.339, 'grad_norm': 0.11022768169641495, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1451380252838135, 'eval_runtime': 8.9987, 'eval_samples_per_second': 111.127, 'eval_steps_per_second': 7.001, 'epoch': 0.92}
{'loss': 1.3716, 'grad_norm': 0.12024486809968948, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.144693374633789, 'eval_runtime': 8.9834, 'eval_samples_per_second': 111.317, 'eval_steps_per_second': 7.013, 'epoch': 0.96}
{'loss': 1.3939, 'grad_norm': 0.11698390543460846, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1439045667648315, 'eval_runtime': 8.9896, 'eval_samples_per_second': 111.24, 'eval_steps_per_second': 7.008, 'epoch': 1.0}
{'train_runtime': 316.358, 'train_samples_per_second': 31.6, 'train_steps_per_second': 1.976, 'train_loss': 1.7179700500488282, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5157885551452637, 1.9092484712600708, 1.7255183458328247, 1.5984687805175781, 1.498115062713623, 1.4128737449645996, 1.3604074716567993, 1.3171619176864624, 1.2887446880340576, 1.2642728090286255, 1.2446515560150146, 1.223596215248108, 1.205610990524292, 1.1932233572006226, 1.1835167407989502, 1.1743888854980469, 1.1665263175964355, 1.1625282764434814, 1.1553630828857422, 1.1536860466003418, 1.1487796306610107, 1.1467816829681396, 1.1451380252838135, 1.144693374633789, 1.1439045667648315], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.5157885551452637, 1.9092484712600708, 1.7255183458328247, 1.5984687805175781, 1.498115062713623, 1.4128737449645996, 1.3604074716567993, 1.3171619176864624, 1.2887446880340576, 1.2642728090286255, 1.2446515560150146, 1.223596215248108, 1.205610990524292, 1.1932233572006226, 1.1835167407989502, 1.1743888854980469, 1.1665263175964355, 1.1625282764434814, 1.1553630828857422, 1.1536860466003418, 1.1487796306610107, 1.1467816829681396, 1.1451380252838135, 1.144693374633789, 1.1439045667648315]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.853865623474121
current iteration best possible eval_loss (full train run):  -1.1439045667648315
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6319 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.8077025163875126
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.8077025163875126
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.8077025163875126
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.8077025163875126
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.8077025163875126
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.6061, dtype=torch.float64), tensor(0.2530, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.1409, dtype=torch.float64), 1, 0, 0, 1, 0, 1, 2, 0.09999999999999037, 1.48000001907349, 0]
normalized proposed parameters for next round by BO: [tensor(0.6061, dtype=torch.float64), tensor(0.2530, dtype=torch.float64), tensor(1.2365e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0413e-18, dtype=torch.float64), tensor(8.7466e-17, dtype=torch.float64), tensor(1.2460e-17, dtype=torch.float64), tensor(4.9957e-17, dtype=torch.float64), tensor(0.1409, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.606
  gsm8k: 0.253
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.141

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.09999999999999037,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (1.48000001907349,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.09999999999999037
lora alpha:  1.48000001907349
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 73,728 || all params: 8,030,334,976 || trainable%: 0.0009
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8358, 'grad_norm': 0.2813275158405304, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.6235313415527344, 'eval_runtime': 8.935, 'eval_samples_per_second': 111.919, 'eval_steps_per_second': 7.051, 'epoch': 0.04}
{'loss': 3.648, 'grad_norm': 1.0175811052322388, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.101578712463379, 'eval_runtime': 8.9757, 'eval_samples_per_second': 111.412, 'eval_steps_per_second': 7.019, 'epoch': 0.08}
{'loss': 2.7005, 'grad_norm': 0.6428741216659546, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7387839555740356, 'eval_runtime': 9.0305, 'eval_samples_per_second': 110.736, 'eval_steps_per_second': 6.976, 'epoch': 0.12}
{'loss': 2.0678, 'grad_norm': 0.48744791746139526, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5880823135375977, 'eval_runtime': 9.0248, 'eval_samples_per_second': 110.806, 'eval_steps_per_second': 6.981, 'epoch': 0.16}
{'loss': 1.7917, 'grad_norm': 0.4095310568809509, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5154571533203125, 'eval_runtime': 9.0464, 'eval_samples_per_second': 110.541, 'eval_steps_per_second': 6.964, 'epoch': 0.2}
{'loss': 1.6547, 'grad_norm': 0.4900834560394287, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4606025218963623, 'eval_runtime': 9.0594, 'eval_samples_per_second': 110.382, 'eval_steps_per_second': 6.954, 'epoch': 0.24}
{'loss': 1.5334, 'grad_norm': 0.4563126564025879, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4083324670791626, 'eval_runtime': 9.0886, 'eval_samples_per_second': 110.027, 'eval_steps_per_second': 6.932, 'epoch': 0.28}
{'loss': 1.5152, 'grad_norm': 0.5055792331695557, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.363582968711853, 'eval_runtime': 9.0816, 'eval_samples_per_second': 110.113, 'eval_steps_per_second': 6.937, 'epoch': 0.32}
{'loss': 1.4267, 'grad_norm': 0.536819338798523, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.332830548286438, 'eval_runtime': 9.0504, 'eval_samples_per_second': 110.492, 'eval_steps_per_second': 6.961, 'epoch': 0.36}
{'loss': 1.3974, 'grad_norm': 0.5566033124923706, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3087700605392456, 'eval_runtime': 9.0842, 'eval_samples_per_second': 110.082, 'eval_steps_per_second': 6.935, 'epoch': 0.4}
{'loss': 1.3764, 'grad_norm': 0.4748350977897644, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2875968217849731, 'eval_runtime': 9.0607, 'eval_samples_per_second': 110.367, 'eval_steps_per_second': 6.953, 'epoch': 0.44}
{'loss': 1.3341, 'grad_norm': 0.45085427165031433, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2673768997192383, 'eval_runtime': 9.0711, 'eval_samples_per_second': 110.241, 'eval_steps_per_second': 6.945, 'epoch': 0.48}
{'loss': 1.3246, 'grad_norm': 0.4293093979358673, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2511008977890015, 'eval_runtime': 9.0643, 'eval_samples_per_second': 110.323, 'eval_steps_per_second': 6.95, 'epoch': 0.52}
{'loss': 1.2873, 'grad_norm': 0.4782911241054535, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.236170768737793, 'eval_runtime': 9.0533, 'eval_samples_per_second': 110.457, 'eval_steps_per_second': 6.959, 'epoch': 0.56}
{'loss': 1.2902, 'grad_norm': 0.45743638277053833, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2180339097976685, 'eval_runtime': 9.0683, 'eval_samples_per_second': 110.275, 'eval_steps_per_second': 6.947, 'epoch': 0.6}
{'loss': 1.2735, 'grad_norm': 0.47148874402046204, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2039867639541626, 'eval_runtime': 9.123, 'eval_samples_per_second': 109.612, 'eval_steps_per_second': 6.906, 'epoch': 0.64}
{'loss': 1.2491, 'grad_norm': 0.4976450502872467, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1889643669128418, 'eval_runtime': 9.143, 'eval_samples_per_second': 109.373, 'eval_steps_per_second': 6.891, 'epoch': 0.68}
{'loss': 1.2033, 'grad_norm': 0.4581684172153473, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1768829822540283, 'eval_runtime': 9.1111, 'eval_samples_per_second': 109.756, 'eval_steps_per_second': 6.915, 'epoch': 0.72}
{'loss': 1.2051, 'grad_norm': 0.5157362222671509, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.163824439048767, 'eval_runtime': 9.1017, 'eval_samples_per_second': 109.869, 'eval_steps_per_second': 6.922, 'epoch': 0.76}
{'loss': 1.1936, 'grad_norm': 0.45026540756225586, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.15952730178833, 'eval_runtime': 9.126, 'eval_samples_per_second': 109.577, 'eval_steps_per_second': 6.903, 'epoch': 0.8}
{'loss': 1.198, 'grad_norm': 0.4303412139415741, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1531213521957397, 'eval_runtime': 9.1377, 'eval_samples_per_second': 109.437, 'eval_steps_per_second': 6.895, 'epoch': 0.84}
{'loss': 1.1857, 'grad_norm': 0.48621684312820435, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1484225988388062, 'eval_runtime': 9.1142, 'eval_samples_per_second': 109.719, 'eval_steps_per_second': 6.912, 'epoch': 0.88}
{'loss': 1.168, 'grad_norm': 0.4392230212688446, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1462575197219849, 'eval_runtime': 9.067, 'eval_samples_per_second': 110.29, 'eval_steps_per_second': 6.948, 'epoch': 0.92}
{'loss': 1.1833, 'grad_norm': 0.4681503474712372, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1448320150375366, 'eval_runtime': 9.0227, 'eval_samples_per_second': 110.831, 'eval_steps_per_second': 6.982, 'epoch': 0.96}
{'loss': 1.1685, 'grad_norm': 0.4575546979904175, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1445688009262085, 'eval_runtime': 9.0196, 'eval_samples_per_second': 110.869, 'eval_steps_per_second': 6.985, 'epoch': 1.0}
{'train_runtime': 316.3804, 'train_samples_per_second': 31.604, 'train_steps_per_second': 1.975, 'train_loss': 1.6084770599365235, 'epoch': 1.0}
train_results:  {'eval_loss': [2.6235313415527344, 2.101578712463379, 1.7387839555740356, 1.5880823135375977, 1.5154571533203125, 1.4606025218963623, 1.4083324670791626, 1.363582968711853, 1.332830548286438, 1.3087700605392456, 1.2875968217849731, 1.2673768997192383, 1.2511008977890015, 1.236170768737793, 1.2180339097976685, 1.2039867639541626, 1.1889643669128418, 1.1768829822540283, 1.163824439048767, 1.15952730178833, 1.1531213521957397, 1.1484225988388062, 1.1462575197219849, 1.1448320150375366, 1.1445688009262085], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.6235313415527344, 2.101578712463379, 1.7387839555740356, 1.5880823135375977, 1.5154571533203125, 1.4606025218963623, 1.4083324670791626, 1.363582968711853, 1.332830548286438, 1.3087700605392456, 1.2875968217849731, 1.2673768997192383, 1.2511008977890015, 1.236170768737793, 1.2180339097976685, 1.2039867639541626, 1.1889643669128418, 1.1768829822540283, 1.163824439048767, 1.15952730178833, 1.1531213521957397, 1.1484225988388062, 1.1462575197219849, 1.1448320150375366, 1.1445688009262085]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.4781453609466553
current iteration best possible eval_loss (full train run):  -1.1445688009262085
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.9847 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.8453085298222155
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.8453085274158499
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.8453085298222155
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.8453091205111513
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.8453085298222153
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0425, dtype=torch.float64), tensor(0.3172, dtype=torch.float64), tensor(0.0817, dtype=torch.float64), tensor(0.0424, dtype=torch.float64), tensor(0.0171, dtype=torch.float64), tensor(0.3040, dtype=torch.float64), tensor(0.0186, dtype=torch.float64), tensor(0.1170, dtype=torch.float64), tensor(0.0594, dtype=torch.float64), 32, 1, 1, 1, 1, 0, 113, 0.04754490589832197, 23.42063691116207, 0]
normalized proposed parameters for next round by BO: [tensor(0.0425, dtype=torch.float64), tensor(0.3172, dtype=torch.float64), tensor(0.0817, dtype=torch.float64), tensor(0.0424, dtype=torch.float64), tensor(0.0171, dtype=torch.float64), tensor(0.3040, dtype=torch.float64), tensor(0.0186, dtype=torch.float64), tensor(0.1170, dtype=torch.float64), tensor(0.0594, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8847, dtype=torch.float64), tensor(0.4754, dtype=torch.float64), tensor(0.4879, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.043
  gsm8k: 0.317
  rowan_hellaswag: 0.082
  sciq: 0.042
  triviaqa: 0.017
  truthfulqa_gen: 0.304
  wikitext: 0.019
  mmlu: 0.117
  arc_challenge: 0.059

LoRA Parameters:
  lora_r: (113,)
  lora_dropout: (0.04754490589832197,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (23.42063691116207,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  113
lora dropout:  0.04754490589832197
lora alpha:  23.42063691116207
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 181,436,416 || all params: 8,211,697,664 || trainable%: 2.2095
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6786, 'grad_norm': 0.451911598443985, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2259125709533691, 'eval_runtime': 11.6414, 'eval_samples_per_second': 85.9, 'eval_steps_per_second': 5.412, 'epoch': 0.04}
{'loss': 1.3264, 'grad_norm': 0.44725891947746277, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9150419235229492, 'eval_runtime': 11.646, 'eval_samples_per_second': 85.867, 'eval_steps_per_second': 5.41, 'epoch': 0.08}
{'loss': 1.1091, 'grad_norm': 0.1984167993068695, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8727139830589294, 'eval_runtime': 11.6527, 'eval_samples_per_second': 85.817, 'eval_steps_per_second': 5.406, 'epoch': 0.12}
{'loss': 1.0898, 'grad_norm': 0.20324036478996277, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8716554045677185, 'eval_runtime': 11.8048, 'eval_samples_per_second': 84.711, 'eval_steps_per_second': 5.337, 'epoch': 0.16}
{'loss': 1.0848, 'grad_norm': 0.16557683050632477, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8572267293930054, 'eval_runtime': 11.7262, 'eval_samples_per_second': 85.279, 'eval_steps_per_second': 5.373, 'epoch': 0.2}
{'loss': 1.0734, 'grad_norm': 0.16901683807373047, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8568878173828125, 'eval_runtime': 11.7263, 'eval_samples_per_second': 85.278, 'eval_steps_per_second': 5.373, 'epoch': 0.24}
{'loss': 1.0063, 'grad_norm': 0.15434302389621735, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8574270606040955, 'eval_runtime': 11.7162, 'eval_samples_per_second': 85.352, 'eval_steps_per_second': 5.377, 'epoch': 0.28}
{'loss': 1.0709, 'grad_norm': 0.15176334977149963, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.846443235874176, 'eval_runtime': 11.7132, 'eval_samples_per_second': 85.374, 'eval_steps_per_second': 5.379, 'epoch': 0.32}
{'loss': 1.0231, 'grad_norm': 0.1786756068468094, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.844448447227478, 'eval_runtime': 11.7115, 'eval_samples_per_second': 85.386, 'eval_steps_per_second': 5.379, 'epoch': 0.36}
{'loss': 1.0605, 'grad_norm': 0.19382329285144806, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8417375087738037, 'eval_runtime': 11.7126, 'eval_samples_per_second': 85.378, 'eval_steps_per_second': 5.379, 'epoch': 0.4}
{'loss': 0.9727, 'grad_norm': 0.16950230300426483, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8395023345947266, 'eval_runtime': 11.7156, 'eval_samples_per_second': 85.356, 'eval_steps_per_second': 5.377, 'epoch': 0.44}
{'loss': 0.9919, 'grad_norm': 0.18743212521076202, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8381600379943848, 'eval_runtime': 11.7144, 'eval_samples_per_second': 85.365, 'eval_steps_per_second': 5.378, 'epoch': 0.48}
{'loss': 1.0005, 'grad_norm': 0.1768179088830948, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8305221199989319, 'eval_runtime': 11.7032, 'eval_samples_per_second': 85.447, 'eval_steps_per_second': 5.383, 'epoch': 0.52}
{'loss': 0.9982, 'grad_norm': 0.18921096622943878, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8289582133293152, 'eval_runtime': 11.6456, 'eval_samples_per_second': 85.869, 'eval_steps_per_second': 5.41, 'epoch': 0.56}
{'loss': 0.94, 'grad_norm': 0.21125024557113647, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8335109949111938, 'eval_runtime': 11.6575, 'eval_samples_per_second': 85.782, 'eval_steps_per_second': 5.404, 'epoch': 0.6}
{'loss': 0.9265, 'grad_norm': 0.1726994812488556, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8285788893699646, 'eval_runtime': 11.7104, 'eval_samples_per_second': 85.394, 'eval_steps_per_second': 5.38, 'epoch': 0.64}
{'loss': 0.9531, 'grad_norm': 0.17807850241661072, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8265256285667419, 'eval_runtime': 11.7787, 'eval_samples_per_second': 84.899, 'eval_steps_per_second': 5.349, 'epoch': 0.68}
{'loss': 0.9161, 'grad_norm': 0.17668111622333527, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.822425901889801, 'eval_runtime': 11.7929, 'eval_samples_per_second': 84.797, 'eval_steps_per_second': 5.342, 'epoch': 0.72}
{'loss': 0.9889, 'grad_norm': 0.20597052574157715, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8237508535385132, 'eval_runtime': 11.7974, 'eval_samples_per_second': 84.765, 'eval_steps_per_second': 5.34, 'epoch': 0.76}
{'loss': 0.9302, 'grad_norm': 0.17098000645637512, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8197535276412964, 'eval_runtime': 11.7524, 'eval_samples_per_second': 85.089, 'eval_steps_per_second': 5.361, 'epoch': 0.8}
{'loss': 0.9839, 'grad_norm': 0.18708756566047668, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8181132674217224, 'eval_runtime': 11.7244, 'eval_samples_per_second': 85.292, 'eval_steps_per_second': 5.373, 'epoch': 0.84}
{'loss': 0.982, 'grad_norm': 0.21026726067066193, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8180381059646606, 'eval_runtime': 11.7087, 'eval_samples_per_second': 85.406, 'eval_steps_per_second': 5.381, 'epoch': 0.88}
{'loss': 0.9331, 'grad_norm': 0.16340896487236023, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8164882659912109, 'eval_runtime': 11.7566, 'eval_samples_per_second': 85.058, 'eval_steps_per_second': 5.359, 'epoch': 0.92}
{'loss': 0.8761, 'grad_norm': 0.22362910211086273, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8155477643013, 'eval_runtime': 11.7634, 'eval_samples_per_second': 85.009, 'eval_steps_per_second': 5.356, 'epoch': 0.96}
{'loss': 0.9198, 'grad_norm': 0.2268918752670288, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8153017163276672, 'eval_runtime': 11.7461, 'eval_samples_per_second': 85.134, 'eval_steps_per_second': 5.363, 'epoch': 1.0}
{'train_runtime': 569.4067, 'train_samples_per_second': 17.555, 'train_steps_per_second': 1.098, 'train_loss': 1.0734336547851562, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2259125709533691, 0.9150419235229492, 0.8727139830589294, 0.8716554045677185, 0.8572267293930054, 0.8568878173828125, 0.8574270606040955, 0.846443235874176, 0.844448447227478, 0.8417375087738037, 0.8395023345947266, 0.8381600379943848, 0.8305221199989319, 0.8289582133293152, 0.8335109949111938, 0.8285788893699646, 0.8265256285667419, 0.822425901889801, 0.8237508535385132, 0.8197535276412964, 0.8181132674217224, 0.8180381059646606, 0.8164882659912109, 0.8155477643013, 0.8153017163276672], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2259125709533691, 0.9150419235229492, 0.8727139830589294, 0.8716554045677185, 0.8572267293930054, 0.8568878173828125, 0.8574270606040955, 0.846443235874176, 0.844448447227478, 0.8417375087738037, 0.8395023345947266, 0.8381600379943848, 0.8305221199989319, 0.8289582133293152, 0.8335109949111938, 0.8285788893699646, 0.8265256285667419, 0.822425901889801, 0.8237508535385132, 0.8197535276412964, 0.8181132674217224, 0.8180381059646606, 0.8164882659912109, 0.8155477643013, 0.8153017163276672]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0328302383422852
current iteration best possible eval_loss (full train run):  -0.8153017163276672
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.4087 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.7259216780437882
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.7259216780437695
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.7260279248206516
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.7259216780437882
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.7259216780437882
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2121, dtype=torch.float64), 0, tensor(0.4450, dtype=torch.float64), 0, 0, 0, tensor(0.0697, dtype=torch.float64), tensor(0.2732, dtype=torch.float64), 13, 0, 1, 0, 1, 1, 75, 0.1, 3.786098016392903, 1]
normalized proposed parameters for next round by BO: [tensor(1.5741e-17, dtype=torch.float64), tensor(0.2121, dtype=torch.float64), tensor(7.6079e-17, dtype=torch.float64), tensor(0.4450, dtype=torch.float64), tensor(2.6532e-18, dtype=torch.float64), tensor(2.4272e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0697, dtype=torch.float64), tensor(0.2732, dtype=torch.float64), tensor(0.3953, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5898, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0789, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.212
  rowan_hellaswag: 0
  sciq: 0.445
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.07
  arc_challenge: 0.273

LoRA Parameters:
  lora_r: (75,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (3.786098016392903,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  75
lora dropout:  0.1
lora alpha:  3.786098016392903
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 40,934,400 || all params: 8,071,195,648 || trainable%: 0.5072
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6734, 'grad_norm': 0.6081637740135193, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8616522550582886, 'eval_runtime': 9.7603, 'eval_samples_per_second': 102.456, 'eval_steps_per_second': 6.455, 'epoch': 0.04}
{'loss': 1.943, 'grad_norm': 0.4107208251953125, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3440470695495605, 'eval_runtime': 9.7779, 'eval_samples_per_second': 102.272, 'eval_steps_per_second': 6.443, 'epoch': 0.08}
{'loss': 1.2649, 'grad_norm': 0.3012252748012543, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1136213541030884, 'eval_runtime': 9.7969, 'eval_samples_per_second': 102.073, 'eval_steps_per_second': 6.431, 'epoch': 0.12}
{'loss': 1.1444, 'grad_norm': 0.2868955433368683, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0294362306594849, 'eval_runtime': 9.8002, 'eval_samples_per_second': 102.039, 'eval_steps_per_second': 6.428, 'epoch': 0.16}
{'loss': 1.0381, 'grad_norm': 0.14729346334934235, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9781255722045898, 'eval_runtime': 9.8051, 'eval_samples_per_second': 101.987, 'eval_steps_per_second': 6.425, 'epoch': 0.2}
{'loss': 1.0599, 'grad_norm': 0.15785621106624603, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.960240364074707, 'eval_runtime': 9.8051, 'eval_samples_per_second': 101.988, 'eval_steps_per_second': 6.425, 'epoch': 0.24}
{'loss': 0.9592, 'grad_norm': 0.12541957199573517, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9362147450447083, 'eval_runtime': 9.8065, 'eval_samples_per_second': 101.973, 'eval_steps_per_second': 6.424, 'epoch': 0.28}
{'loss': 0.9621, 'grad_norm': 0.11963111907243729, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9064319133758545, 'eval_runtime': 9.7918, 'eval_samples_per_second': 102.126, 'eval_steps_per_second': 6.434, 'epoch': 0.32}
{'loss': 0.9281, 'grad_norm': 0.12089947611093521, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9045963883399963, 'eval_runtime': 9.7986, 'eval_samples_per_second': 102.056, 'eval_steps_per_second': 6.43, 'epoch': 0.36}
{'loss': 0.9416, 'grad_norm': 0.13457515835762024, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8929706811904907, 'eval_runtime': 9.7733, 'eval_samples_per_second': 102.32, 'eval_steps_per_second': 6.446, 'epoch': 0.4}
{'loss': 0.9106, 'grad_norm': 0.12171637266874313, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8953824043273926, 'eval_runtime': 9.7953, 'eval_samples_per_second': 102.089, 'eval_steps_per_second': 6.432, 'epoch': 0.44}
{'loss': 0.8891, 'grad_norm': 0.10546926409006119, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8888973593711853, 'eval_runtime': 9.7878, 'eval_samples_per_second': 102.168, 'eval_steps_per_second': 6.437, 'epoch': 0.48}
{'loss': 0.8904, 'grad_norm': 0.1334749311208725, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8838617205619812, 'eval_runtime': 9.7684, 'eval_samples_per_second': 102.37, 'eval_steps_per_second': 6.449, 'epoch': 0.52}
{'loss': 0.9129, 'grad_norm': 0.12254873663187027, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8826891183853149, 'eval_runtime': 9.8233, 'eval_samples_per_second': 101.799, 'eval_steps_per_second': 6.413, 'epoch': 0.56}
{'loss': 0.8569, 'grad_norm': 0.11307386308908463, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8834580779075623, 'eval_runtime': 9.8373, 'eval_samples_per_second': 101.653, 'eval_steps_per_second': 6.404, 'epoch': 0.6}
{'loss': 0.8785, 'grad_norm': 0.16790276765823364, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8786673545837402, 'eval_runtime': 9.8355, 'eval_samples_per_second': 101.673, 'eval_steps_per_second': 6.405, 'epoch': 0.64}
{'loss': 0.8828, 'grad_norm': 0.12166598439216614, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8779469132423401, 'eval_runtime': 9.7994, 'eval_samples_per_second': 102.047, 'eval_steps_per_second': 6.429, 'epoch': 0.68}
{'loss': 0.8983, 'grad_norm': 0.12773191928863525, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8790038824081421, 'eval_runtime': 9.8026, 'eval_samples_per_second': 102.014, 'eval_steps_per_second': 6.427, 'epoch': 0.72}
{'loss': 0.8431, 'grad_norm': 0.12560537457466125, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.878478467464447, 'eval_runtime': 9.7725, 'eval_samples_per_second': 102.328, 'eval_steps_per_second': 6.447, 'epoch': 0.76}
{'loss': 0.8699, 'grad_norm': 0.10655411332845688, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8748934864997864, 'eval_runtime': 9.7662, 'eval_samples_per_second': 102.394, 'eval_steps_per_second': 6.451, 'epoch': 0.8}
{'loss': 0.8667, 'grad_norm': 0.12333370745182037, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8738999962806702, 'eval_runtime': 9.7583, 'eval_samples_per_second': 102.477, 'eval_steps_per_second': 6.456, 'epoch': 0.84}
{'loss': 0.8573, 'grad_norm': 0.13232538104057312, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8725832104682922, 'eval_runtime': 9.7568, 'eval_samples_per_second': 102.493, 'eval_steps_per_second': 6.457, 'epoch': 0.88}
{'loss': 0.8865, 'grad_norm': 0.10893122106790543, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8718997240066528, 'eval_runtime': 9.7525, 'eval_samples_per_second': 102.537, 'eval_steps_per_second': 6.46, 'epoch': 0.92}
{'loss': 0.8698, 'grad_norm': 0.11482210457324982, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.87203049659729, 'eval_runtime': 9.8546, 'eval_samples_per_second': 101.476, 'eval_steps_per_second': 6.393, 'epoch': 0.96}
{'loss': 0.8457, 'grad_norm': 0.1393541544675827, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8715403079986572, 'eval_runtime': 9.8118, 'eval_samples_per_second': 101.918, 'eval_steps_per_second': 6.421, 'epoch': 1.0}
{'train_runtime': 446.4669, 'train_samples_per_second': 22.394, 'train_steps_per_second': 1.4, 'train_loss': 1.0829316955566406, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8616522550582886, 1.3440470695495605, 1.1136213541030884, 1.0294362306594849, 0.9781255722045898, 0.960240364074707, 0.9362147450447083, 0.9064319133758545, 0.9045963883399963, 0.8929706811904907, 0.8953824043273926, 0.8888973593711853, 0.8838617205619812, 0.8826891183853149, 0.8834580779075623, 0.8786673545837402, 0.8779469132423401, 0.8790038824081421, 0.878478467464447, 0.8748934864997864, 0.8738999962806702, 0.8725832104682922, 0.8718997240066528, 0.87203049659729, 0.8715403079986572], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8616522550582886, 1.3440470695495605, 1.1136213541030884, 1.0294362306594849, 0.9781255722045898, 0.960240364074707, 0.9362147450447083, 0.9064319133758545, 0.9045963883399963, 0.8929706811904907, 0.8953824043273926, 0.8888973593711853, 0.8838617205619812, 0.8826891183853149, 0.8834580779075623, 0.8786673545837402, 0.8779469132423401, 0.8790038824081421, 0.878478467464447, 0.8748934864997864, 0.8738999962806702, 0.8725832104682922, 0.8718997240066528, 0.87203049659729, 0.8715403079986572]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0567100048065186
current iteration best possible eval_loss (full train run):  -0.8715403079986572
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.5114 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.8761849860038532
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.8761587706617951
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.8773060724797127
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.8761587706617951
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.8761587706617951
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4200, dtype=torch.float64), 0, tensor(0.1647, dtype=torch.float64), tensor(0.0203, dtype=torch.float64), tensor(0.2483, dtype=torch.float64), 0, tensor(0.0620, dtype=torch.float64), tensor(0.0847, dtype=torch.float64), 26, 0, 1, 0, 1, 1, 128, 0.0868514220044383, 7.40042499817633, 1]
normalized proposed parameters for next round by BO: [tensor(8.3635e-18, dtype=torch.float64), tensor(0.4200, dtype=torch.float64), tensor(1.1025e-17, dtype=torch.float64), tensor(0.1647, dtype=torch.float64), tensor(0.0203, dtype=torch.float64), tensor(0.2483, dtype=torch.float64), tensor(4.3368e-18, dtype=torch.float64), tensor(0.0620, dtype=torch.float64), tensor(0.0847, dtype=torch.float64), tensor(0.8259, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8685, dtype=torch.float64), tensor(0.1542, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.42
  rowan_hellaswag: 0
  sciq: 0.165
  triviaqa: 0.02
  truthfulqa_gen: 0.248
  wikitext: 0
  mmlu: 0.062
  arc_challenge: 0.085

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0868514220044383,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (7.40042499817633,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.0868514220044383
lora alpha:  7.40042499817633
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 139,722,752 || all params: 8,169,984,000 || trainable%: 1.7102
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9275, 'grad_norm': 0.5307250022888184, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5276689529418945, 'eval_runtime': 10.0377, 'eval_samples_per_second': 99.624, 'eval_steps_per_second': 6.276, 'epoch': 0.04}
{'loss': 1.4314, 'grad_norm': 0.18240031599998474, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0484142303466797, 'eval_runtime': 10.0895, 'eval_samples_per_second': 99.112, 'eval_steps_per_second': 6.244, 'epoch': 0.08}
{'loss': 1.103, 'grad_norm': 0.3138984143733978, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9631598591804504, 'eval_runtime': 10.1075, 'eval_samples_per_second': 98.937, 'eval_steps_per_second': 6.233, 'epoch': 0.12}
{'loss': 1.018, 'grad_norm': 0.10414449870586395, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9247469305992126, 'eval_runtime': 10.093, 'eval_samples_per_second': 99.079, 'eval_steps_per_second': 6.242, 'epoch': 0.16}
{'loss': 0.9843, 'grad_norm': 0.1777128428220749, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9091958999633789, 'eval_runtime': 10.1371, 'eval_samples_per_second': 98.648, 'eval_steps_per_second': 6.215, 'epoch': 0.2}
{'loss': 0.9804, 'grad_norm': 0.10330533981323242, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8962558507919312, 'eval_runtime': 10.1012, 'eval_samples_per_second': 98.998, 'eval_steps_per_second': 6.237, 'epoch': 0.24}
{'loss': 0.93, 'grad_norm': 0.11922882497310638, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.882586658000946, 'eval_runtime': 10.1048, 'eval_samples_per_second': 98.963, 'eval_steps_per_second': 6.235, 'epoch': 0.28}
{'loss': 0.8932, 'grad_norm': 0.16729792952537537, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8691142797470093, 'eval_runtime': 10.1501, 'eval_samples_per_second': 98.521, 'eval_steps_per_second': 6.207, 'epoch': 0.32}
{'loss': 0.928, 'grad_norm': 0.14163243770599365, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8578879833221436, 'eval_runtime': 10.1445, 'eval_samples_per_second': 98.575, 'eval_steps_per_second': 6.21, 'epoch': 0.36}
{'loss': 0.878, 'grad_norm': 0.12774860858917236, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8479170203208923, 'eval_runtime': 10.1327, 'eval_samples_per_second': 98.69, 'eval_steps_per_second': 6.217, 'epoch': 0.4}
{'loss': 0.9178, 'grad_norm': 0.10652666538953781, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8504499197006226, 'eval_runtime': 10.2074, 'eval_samples_per_second': 97.968, 'eval_steps_per_second': 6.172, 'epoch': 0.44}
{'loss': 0.8484, 'grad_norm': 0.10974005609750748, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8425535559654236, 'eval_runtime': 10.1868, 'eval_samples_per_second': 98.166, 'eval_steps_per_second': 6.184, 'epoch': 0.48}
{'loss': 0.8854, 'grad_norm': 0.10052749514579773, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8412315845489502, 'eval_runtime': 10.1558, 'eval_samples_per_second': 98.465, 'eval_steps_per_second': 6.203, 'epoch': 0.52}
{'loss': 0.8561, 'grad_norm': 0.1352134346961975, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8391573429107666, 'eval_runtime': 10.1426, 'eval_samples_per_second': 98.594, 'eval_steps_per_second': 6.211, 'epoch': 0.56}
{'loss': 0.8506, 'grad_norm': 0.13485103845596313, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8344564437866211, 'eval_runtime': 10.151, 'eval_samples_per_second': 98.513, 'eval_steps_per_second': 6.206, 'epoch': 0.6}
{'loss': 0.8221, 'grad_norm': 0.12633343040943146, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8334031105041504, 'eval_runtime': 10.1428, 'eval_samples_per_second': 98.592, 'eval_steps_per_second': 6.211, 'epoch': 0.64}
{'loss': 0.84, 'grad_norm': 0.11699508130550385, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8332547545433044, 'eval_runtime': 10.0829, 'eval_samples_per_second': 99.178, 'eval_steps_per_second': 6.248, 'epoch': 0.68}
{'loss': 0.8435, 'grad_norm': 0.11450755596160889, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8302858471870422, 'eval_runtime': 10.0846, 'eval_samples_per_second': 99.161, 'eval_steps_per_second': 6.247, 'epoch': 0.72}
{'loss': 0.8211, 'grad_norm': 0.14029386639595032, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8288848400115967, 'eval_runtime': 10.0968, 'eval_samples_per_second': 99.042, 'eval_steps_per_second': 6.24, 'epoch': 0.76}
{'loss': 0.8352, 'grad_norm': 0.13803082704544067, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8259778618812561, 'eval_runtime': 10.0942, 'eval_samples_per_second': 99.066, 'eval_steps_per_second': 6.241, 'epoch': 0.8}
{'loss': 0.8301, 'grad_norm': 0.11476226150989532, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8272672295570374, 'eval_runtime': 10.0878, 'eval_samples_per_second': 99.129, 'eval_steps_per_second': 6.245, 'epoch': 0.84}
{'loss': 0.8296, 'grad_norm': 0.11868828535079956, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8256428241729736, 'eval_runtime': 10.09, 'eval_samples_per_second': 99.108, 'eval_steps_per_second': 6.244, 'epoch': 0.88}
{'loss': 0.8223, 'grad_norm': 0.11213264614343643, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8231016397476196, 'eval_runtime': 10.1076, 'eval_samples_per_second': 98.936, 'eval_steps_per_second': 6.233, 'epoch': 0.92}
{'loss': 0.8188, 'grad_norm': 0.11679860949516296, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8246702551841736, 'eval_runtime': 10.2077, 'eval_samples_per_second': 97.966, 'eval_steps_per_second': 6.172, 'epoch': 0.96}
{'loss': 0.7844, 'grad_norm': 0.13384000957012177, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8233049511909485, 'eval_runtime': 10.1799, 'eval_samples_per_second': 98.233, 'eval_steps_per_second': 6.189, 'epoch': 1.0}
{'train_runtime': 478.1095, 'train_samples_per_second': 20.909, 'train_steps_per_second': 1.307, 'train_loss': 0.9871685791015625, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5276689529418945, 1.0484142303466797, 0.9631598591804504, 0.9247469305992126, 0.9091958999633789, 0.8962558507919312, 0.882586658000946, 0.8691142797470093, 0.8578879833221436, 0.8479170203208923, 0.8504499197006226, 0.8425535559654236, 0.8412315845489502, 0.8391573429107666, 0.8344564437866211, 0.8334031105041504, 0.8332547545433044, 0.8302858471870422, 0.8288848400115967, 0.8259778618812561, 0.8272672295570374, 0.8256428241729736, 0.8231016397476196, 0.8246702551841736, 0.8233049511909485], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5276689529418945, 1.0484142303466797, 0.9631598591804504, 0.9247469305992126, 0.9091958999633789, 0.8962558507919312, 0.882586658000946, 0.8691142797470093, 0.8578879833221436, 0.8479170203208923, 0.8504499197006226, 0.8425535559654236, 0.8412315845489502, 0.8391573429107666, 0.8344564437866211, 0.8334031105041504, 0.8332547545433044, 0.8302858471870422, 0.8288848400115967, 0.8259778618812561, 0.8272672295570374, 0.8256428241729736, 0.8231016397476196, 0.8246702551841736, 0.8233049511909485]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.8233049511909485
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.2574 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.8669702560576176
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.8668806702857154
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.8668793849939705
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.8668793849918491
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.8668793849935484
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1782, dtype=torch.float64), tensor(0.1794, dtype=torch.float64), 0, tensor(0.1765, dtype=torch.float64), tensor(0.0376, dtype=torch.float64), tensor(0.0981, dtype=torch.float64), 0, tensor(0.1120, dtype=torch.float64), tensor(0.2182, dtype=torch.float64), 27, 0, 1, 0, 0, 1, 54, 0.0261306443329736, 25.873480143783276, 1]
normalized proposed parameters for next round by BO: [tensor(0.1782, dtype=torch.float64), tensor(0.1794, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1765, dtype=torch.float64), tensor(0.0376, dtype=torch.float64), tensor(0.0981, dtype=torch.float64), tensor(1.7890e-18, dtype=torch.float64), tensor(0.1120, dtype=torch.float64), tensor(0.2182, dtype=torch.float64), tensor(0.8530, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4204, dtype=torch.float64), tensor(0.2613, dtype=torch.float64), tensor(0.5390, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.178
  gsm8k: 0.179
  rowan_hellaswag: 0
  sciq: 0.176
  triviaqa: 0.038
  truthfulqa_gen: 0.098
  wikitext: 0
  mmlu: 0.112
  arc_challenge: 0.218

LoRA Parameters:
  lora_r: (54,)
  lora_dropout: (0.0261306443329736,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (25.873480143783276,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  54
lora dropout:  0.0261306443329736
lora alpha:  25.873480143783276
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 34,338,816 || all params: 8,064,600,064 || trainable%: 0.4258
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0582, 'grad_norm': 0.7763189673423767, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4580844640731812, 'eval_runtime': 9.5895, 'eval_samples_per_second': 104.28, 'eval_steps_per_second': 6.57, 'epoch': 0.04}
{'loss': 1.4137, 'grad_norm': 0.3281453847885132, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0784175395965576, 'eval_runtime': 9.664, 'eval_samples_per_second': 103.476, 'eval_steps_per_second': 6.519, 'epoch': 0.08}
{'loss': 1.1963, 'grad_norm': 0.26483795046806335, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.001861333847046, 'eval_runtime': 9.6464, 'eval_samples_per_second': 103.666, 'eval_steps_per_second': 6.531, 'epoch': 0.12}
{'loss': 1.1281, 'grad_norm': 0.3099604547023773, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9591037034988403, 'eval_runtime': 9.6573, 'eval_samples_per_second': 103.549, 'eval_steps_per_second': 6.524, 'epoch': 0.16}
{'loss': 1.0267, 'grad_norm': 0.2695137858390808, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9391380548477173, 'eval_runtime': 9.6564, 'eval_samples_per_second': 103.558, 'eval_steps_per_second': 6.524, 'epoch': 0.2}
{'loss': 1.0615, 'grad_norm': 0.26353952288627625, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9243587255477905, 'eval_runtime': 9.6516, 'eval_samples_per_second': 103.609, 'eval_steps_per_second': 6.527, 'epoch': 0.24}
{'loss': 1.009, 'grad_norm': 0.28108206391334534, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9241591691970825, 'eval_runtime': 9.6321, 'eval_samples_per_second': 103.82, 'eval_steps_per_second': 6.541, 'epoch': 0.28}
{'loss': 0.9792, 'grad_norm': 0.23019565641880035, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.911513090133667, 'eval_runtime': 9.6339, 'eval_samples_per_second': 103.801, 'eval_steps_per_second': 6.539, 'epoch': 0.32}
{'loss': 1.0095, 'grad_norm': 0.3118172287940979, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.904769778251648, 'eval_runtime': 9.6306, 'eval_samples_per_second': 103.835, 'eval_steps_per_second': 6.542, 'epoch': 0.36}
{'loss': 1.0174, 'grad_norm': 0.23060037195682526, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8991407752037048, 'eval_runtime': 9.6401, 'eval_samples_per_second': 103.734, 'eval_steps_per_second': 6.535, 'epoch': 0.4}
{'loss': 0.9708, 'grad_norm': 0.26537877321243286, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8863825798034668, 'eval_runtime': 9.6432, 'eval_samples_per_second': 103.7, 'eval_steps_per_second': 6.533, 'epoch': 0.44}
{'loss': 0.9131, 'grad_norm': 0.3126687705516815, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8737189769744873, 'eval_runtime': 9.645, 'eval_samples_per_second': 103.68, 'eval_steps_per_second': 6.532, 'epoch': 0.48}
{'loss': 0.8979, 'grad_norm': 0.26730290055274963, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8596100807189941, 'eval_runtime': 9.6423, 'eval_samples_per_second': 103.71, 'eval_steps_per_second': 6.534, 'epoch': 0.52}
{'loss': 0.8753, 'grad_norm': 0.30957865715026855, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8542019128799438, 'eval_runtime': 9.6358, 'eval_samples_per_second': 103.779, 'eval_steps_per_second': 6.538, 'epoch': 0.56}
{'loss': 0.8817, 'grad_norm': 0.27694302797317505, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8564171195030212, 'eval_runtime': 9.645, 'eval_samples_per_second': 103.681, 'eval_steps_per_second': 6.532, 'epoch': 0.6}
{'loss': 0.9205, 'grad_norm': 0.28296181559562683, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8550760746002197, 'eval_runtime': 9.6285, 'eval_samples_per_second': 103.858, 'eval_steps_per_second': 6.543, 'epoch': 0.64}
{'loss': 0.9045, 'grad_norm': 0.3122834265232086, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8535053730010986, 'eval_runtime': 9.6252, 'eval_samples_per_second': 103.894, 'eval_steps_per_second': 6.545, 'epoch': 0.68}
{'loss': 0.8548, 'grad_norm': 0.279620498418808, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8503995537757874, 'eval_runtime': 9.6288, 'eval_samples_per_second': 103.856, 'eval_steps_per_second': 6.543, 'epoch': 0.72}
{'loss': 0.8826, 'grad_norm': 0.2781863510608673, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8487996459007263, 'eval_runtime': 9.6203, 'eval_samples_per_second': 103.947, 'eval_steps_per_second': 6.549, 'epoch': 0.76}
{'loss': 0.8536, 'grad_norm': 0.32657867670059204, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8435441851615906, 'eval_runtime': 9.6178, 'eval_samples_per_second': 103.974, 'eval_steps_per_second': 6.55, 'epoch': 0.8}
{'loss': 0.8454, 'grad_norm': 0.290691614151001, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.846514880657196, 'eval_runtime': 9.6259, 'eval_samples_per_second': 103.886, 'eval_steps_per_second': 6.545, 'epoch': 0.84}
{'loss': 0.8891, 'grad_norm': 0.2614921033382416, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8436258435249329, 'eval_runtime': 9.6322, 'eval_samples_per_second': 103.819, 'eval_steps_per_second': 6.541, 'epoch': 0.88}
{'loss': 0.8452, 'grad_norm': 0.3218533992767334, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8432908654212952, 'eval_runtime': 9.6278, 'eval_samples_per_second': 103.866, 'eval_steps_per_second': 6.544, 'epoch': 0.92}
{'loss': 0.8403, 'grad_norm': 0.30997157096862793, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8418018817901611, 'eval_runtime': 9.6314, 'eval_samples_per_second': 103.827, 'eval_steps_per_second': 6.541, 'epoch': 0.96}
{'loss': 0.8582, 'grad_norm': 0.29832711815834045, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8422247171401978, 'eval_runtime': 9.6312, 'eval_samples_per_second': 103.829, 'eval_steps_per_second': 6.541, 'epoch': 1.0}
{'train_runtime': 436.1934, 'train_samples_per_second': 22.916, 'train_steps_per_second': 1.433, 'train_loss': 1.0453035491943359, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4580844640731812, 1.0784175395965576, 1.001861333847046, 0.9591037034988403, 0.9391380548477173, 0.9243587255477905, 0.9241591691970825, 0.911513090133667, 0.904769778251648, 0.8991407752037048, 0.8863825798034668, 0.8737189769744873, 0.8596100807189941, 0.8542019128799438, 0.8564171195030212, 0.8550760746002197, 0.8535053730010986, 0.8503995537757874, 0.8487996459007263, 0.8435441851615906, 0.846514880657196, 0.8436258435249329, 0.8432908654212952, 0.8418018817901611, 0.8422247171401978], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4580844640731812, 1.0784175395965576, 1.001861333847046, 0.9591037034988403, 0.9391380548477173, 0.9243587255477905, 0.9241591691970825, 0.911513090133667, 0.904769778251648, 0.8991407752037048, 0.8863825798034668, 0.8737189769744873, 0.8596100807189941, 0.8542019128799438, 0.8564171195030212, 0.8550760746002197, 0.8535053730010986, 0.8503995537757874, 0.8487996459007263, 0.8435441851615906, 0.846514880657196, 0.8436258435249329, 0.8432908654212952, 0.8418018817901611, 0.8422247171401978]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.11109459400177
current iteration best possible eval_loss (full train run):  -0.8422247171401978
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 30.5441 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -0.8693503682825255
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -0.8720537597150594
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -0.8693503699762178
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -0.8693115229264193
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -0.8693123231339164
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2102, dtype=torch.float64), tensor(0.1715, dtype=torch.float64), tensor(0.0570, dtype=torch.float64), tensor(0.1360, dtype=torch.float64), tensor(0.0414, dtype=torch.float64), tensor(0.3399, dtype=torch.float64), 0, tensor(0.0441, dtype=torch.float64), 0, 32, 0, 0, 0, 1, 0, 115, 0.03481212708410577, 35.86978397832534, 1]
normalized proposed parameters for next round by BO: [tensor(0.2102, dtype=torch.float64), tensor(0.1715, dtype=torch.float64), tensor(0.0570, dtype=torch.float64), tensor(0.1360, dtype=torch.float64), tensor(0.0414, dtype=torch.float64), tensor(0.3399, dtype=torch.float64), tensor(1.5969e-18, dtype=torch.float64), tensor(0.0441, dtype=torch.float64), tensor(3.0565e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8964, dtype=torch.float64), tensor(0.3481, dtype=torch.float64), tensor(0.7473, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.21
  gsm8k: 0.171
  rowan_hellaswag: 0.057
  sciq: 0.136
  triviaqa: 0.041
  truthfulqa_gen: 0.34
  wikitext: 0
  mmlu: 0.044
  arc_challenge: 0

LoRA Parameters:
  lora_r: (115,)
  lora_dropout: (0.03481212708410577,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (35.86978397832534,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  115
lora dropout:  0.03481212708410577
lora alpha:  35.86978397832534
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 67,829,760 || all params: 8,098,091,008 || trainable%: 0.8376
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.179, 'grad_norm': 0.8262355327606201, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3613309860229492, 'eval_runtime': 9.9783, 'eval_samples_per_second': 100.217, 'eval_steps_per_second': 6.314, 'epoch': 0.04}
{'loss': 1.362, 'grad_norm': 0.41292399168014526, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9876829981803894, 'eval_runtime': 9.9833, 'eval_samples_per_second': 100.167, 'eval_steps_per_second': 6.311, 'epoch': 0.08}
{'loss': 1.1118, 'grad_norm': 0.22069700062274933, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9160441160202026, 'eval_runtime': 9.9917, 'eval_samples_per_second': 100.083, 'eval_steps_per_second': 6.305, 'epoch': 0.12}
{'loss': 1.0813, 'grad_norm': 0.24066689610481262, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9015690088272095, 'eval_runtime': 9.9822, 'eval_samples_per_second': 100.178, 'eval_steps_per_second': 6.311, 'epoch': 0.16}
{'loss': 1.013, 'grad_norm': 0.18587666749954224, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8923932909965515, 'eval_runtime': 10.0568, 'eval_samples_per_second': 99.435, 'eval_steps_per_second': 6.264, 'epoch': 0.2}
{'loss': 1.0183, 'grad_norm': 0.21977408230304718, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8820172548294067, 'eval_runtime': 10.0145, 'eval_samples_per_second': 99.855, 'eval_steps_per_second': 6.291, 'epoch': 0.24}
{'loss': 0.9297, 'grad_norm': 0.2174166440963745, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8785750269889832, 'eval_runtime': 10.0123, 'eval_samples_per_second': 99.877, 'eval_steps_per_second': 6.292, 'epoch': 0.28}
{'loss': 1.0189, 'grad_norm': 0.20351554453372955, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8700928688049316, 'eval_runtime': 10.0172, 'eval_samples_per_second': 99.828, 'eval_steps_per_second': 6.289, 'epoch': 0.32}
{'loss': 0.9693, 'grad_norm': 0.22516635060310364, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8656758069992065, 'eval_runtime': 10.0121, 'eval_samples_per_second': 99.879, 'eval_steps_per_second': 6.292, 'epoch': 0.36}
{'loss': 0.982, 'grad_norm': 0.2125840187072754, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8653886914253235, 'eval_runtime': 9.9944, 'eval_samples_per_second': 100.056, 'eval_steps_per_second': 6.304, 'epoch': 0.4}
{'loss': 0.9333, 'grad_norm': 0.24989494681358337, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8615679740905762, 'eval_runtime': 9.9884, 'eval_samples_per_second': 100.116, 'eval_steps_per_second': 6.307, 'epoch': 0.44}
{'loss': 0.9701, 'grad_norm': 0.23071962594985962, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8675223588943481, 'eval_runtime': 9.9833, 'eval_samples_per_second': 100.168, 'eval_steps_per_second': 6.311, 'epoch': 0.48}
{'loss': 0.9341, 'grad_norm': 0.24296216666698456, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8532494902610779, 'eval_runtime': 9.9761, 'eval_samples_per_second': 100.239, 'eval_steps_per_second': 6.315, 'epoch': 0.52}
{'loss': 0.9903, 'grad_norm': 0.22858567535877228, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8511333465576172, 'eval_runtime': 9.9859, 'eval_samples_per_second': 100.141, 'eval_steps_per_second': 6.309, 'epoch': 0.56}
{'loss': 0.9171, 'grad_norm': 0.258463978767395, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8535781502723694, 'eval_runtime': 10.0007, 'eval_samples_per_second': 99.993, 'eval_steps_per_second': 6.3, 'epoch': 0.6}
{'loss': 0.8971, 'grad_norm': 0.2229015976190567, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8509255647659302, 'eval_runtime': 9.9871, 'eval_samples_per_second': 100.129, 'eval_steps_per_second': 6.308, 'epoch': 0.64}
{'loss': 0.9364, 'grad_norm': 0.24701917171478271, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8528954982757568, 'eval_runtime': 9.9876, 'eval_samples_per_second': 100.124, 'eval_steps_per_second': 6.308, 'epoch': 0.68}
{'loss': 0.9045, 'grad_norm': 0.22244980931282043, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8454732894897461, 'eval_runtime': 9.9906, 'eval_samples_per_second': 100.094, 'eval_steps_per_second': 6.306, 'epoch': 0.72}
{'loss': 0.9268, 'grad_norm': 0.25652873516082764, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8488749861717224, 'eval_runtime': 9.9824, 'eval_samples_per_second': 100.176, 'eval_steps_per_second': 6.311, 'epoch': 0.76}
{'loss': 0.9174, 'grad_norm': 0.3379642963409424, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8414191603660583, 'eval_runtime': 9.9933, 'eval_samples_per_second': 100.067, 'eval_steps_per_second': 6.304, 'epoch': 0.8}
{'loss': 0.9098, 'grad_norm': 0.2053765505552292, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8437912464141846, 'eval_runtime': 9.9732, 'eval_samples_per_second': 100.268, 'eval_steps_per_second': 6.317, 'epoch': 0.84}
{'loss': 0.8957, 'grad_norm': 0.17826798558235168, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8426916599273682, 'eval_runtime': 9.9782, 'eval_samples_per_second': 100.218, 'eval_steps_per_second': 6.314, 'epoch': 0.88}
{'loss': 0.9325, 'grad_norm': 0.26687684655189514, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.841217041015625, 'eval_runtime': 9.9856, 'eval_samples_per_second': 100.144, 'eval_steps_per_second': 6.309, 'epoch': 0.92}
{'loss': 0.8703, 'grad_norm': 0.23939135670661926, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.841050386428833, 'eval_runtime': 9.9757, 'eval_samples_per_second': 100.244, 'eval_steps_per_second': 6.315, 'epoch': 0.96}
{'loss': 0.9407, 'grad_norm': 0.3323659896850586, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8401809334754944, 'eval_runtime': 9.9849, 'eval_samples_per_second': 100.151, 'eval_steps_per_second': 6.31, 'epoch': 1.0}
{'train_runtime': 460.0528, 'train_samples_per_second': 21.728, 'train_steps_per_second': 1.359, 'train_loss': 1.061652572631836, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3613309860229492, 0.9876829981803894, 0.9160441160202026, 0.9015690088272095, 0.8923932909965515, 0.8820172548294067, 0.8785750269889832, 0.8700928688049316, 0.8656758069992065, 0.8653886914253235, 0.8615679740905762, 0.8675223588943481, 0.8532494902610779, 0.8511333465576172, 0.8535781502723694, 0.8509255647659302, 0.8528954982757568, 0.8454732894897461, 0.8488749861717224, 0.8414191603660583, 0.8437912464141846, 0.8426916599273682, 0.841217041015625, 0.841050386428833, 0.8401809334754944], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3613309860229492, 0.9876829981803894, 0.9160441160202026, 0.9015690088272095, 0.8923932909965515, 0.8820172548294067, 0.8785750269889832, 0.8700928688049316, 0.8656758069992065, 0.8653886914253235, 0.8615679740905762, 0.8675223588943481, 0.8532494902610779, 0.8511333465576172, 0.8535781502723694, 0.8509255647659302, 0.8528954982757568, 0.8454732894897461, 0.8488749861717224, 0.8414191603660583, 0.8437912464141846, 0.8426916599273682, 0.841217041015625, 0.841050386428833, 0.8401809334754944]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.065903663635254
current iteration best possible eval_loss (full train run):  -0.8401809334754944
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.9464 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -0.909956168796067
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -0.9099559663853821
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -0.909956078077471
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.9099546818822032
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -0.9097885719879373
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0379, dtype=torch.float64), tensor(0.2057, dtype=torch.float64), tensor(0.0339, dtype=torch.float64), 0, 0, tensor(0.5084, dtype=torch.float64), tensor(0.0502, dtype=torch.float64), 0, tensor(0.1609, dtype=torch.float64), 1, 0, 0, 0, 1, 1, 30, 0.006220020922815982, 15.109825549878174, 1]
normalized proposed parameters for next round by BO: [tensor(0.0379, dtype=torch.float64), tensor(0.2057, dtype=torch.float64), tensor(0.0339, dtype=torch.float64), tensor(1.0789e-18, dtype=torch.float64), tensor(2.1838e-18, dtype=torch.float64), tensor(0.5084, dtype=torch.float64), tensor(0.0502, dtype=torch.float64), tensor(0.0030, dtype=torch.float64), tensor(0.1609, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2381, dtype=torch.float64), tensor(0.0622, dtype=torch.float64), tensor(0.3148, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.038
  gsm8k: 0.206
  rowan_hellaswag: 0.034
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.508
  wikitext: 0.05
  mmlu: 0
  arc_challenge: 0.161

LoRA Parameters:
  lora_r: (30,)
  lora_dropout: (0.006220020922815982,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (15.109825549878174,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  30
lora dropout:  0.006220020922815982
lora alpha:  15.109825549878174
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,105,920 || all params: 8,031,367,168 || trainable%: 0.0138
length of training data:  9966
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8037, 'grad_norm': 1.2033662796020508, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.2618467807769775, 'eval_runtime': 8.9197, 'eval_samples_per_second': 112.112, 'eval_steps_per_second': 7.063, 'epoch': 0.04}
{'loss': 2.8177, 'grad_norm': 1.1832380294799805, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.6635541915893555, 'eval_runtime': 8.9338, 'eval_samples_per_second': 111.935, 'eval_steps_per_second': 7.052, 'epoch': 0.08}
{'loss': 2.0619, 'grad_norm': 0.8033961057662964, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.4577964544296265, 'eval_runtime': 8.977, 'eval_samples_per_second': 111.396, 'eval_steps_per_second': 7.018, 'epoch': 0.12}
{'loss': 1.7095, 'grad_norm': 2.0980873107910156, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.3357319831848145, 'eval_runtime': 9.0233, 'eval_samples_per_second': 110.824, 'eval_steps_per_second': 6.982, 'epoch': 0.16}
{'loss': 1.6342, 'grad_norm': 2.207028388977051, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.2822167873382568, 'eval_runtime': 9.047, 'eval_samples_per_second': 110.534, 'eval_steps_per_second': 6.964, 'epoch': 0.2}
{'loss': 1.4841, 'grad_norm': 1.657903790473938, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.2318259477615356, 'eval_runtime': 9.0271, 'eval_samples_per_second': 110.778, 'eval_steps_per_second': 6.979, 'epoch': 0.24}
{'loss': 1.5177, 'grad_norm': 2.5236282348632812, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.2269198894500732, 'eval_runtime': 9.025, 'eval_samples_per_second': 110.804, 'eval_steps_per_second': 6.981, 'epoch': 0.28}
{'loss': 1.4689, 'grad_norm': 2.290064573287964, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.1849080324172974, 'eval_runtime': 9.0314, 'eval_samples_per_second': 110.724, 'eval_steps_per_second': 6.976, 'epoch': 0.32}
{'loss': 1.4401, 'grad_norm': 1.6100690364837646, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.1502010822296143, 'eval_runtime': 9.0069, 'eval_samples_per_second': 111.026, 'eval_steps_per_second': 6.995, 'epoch': 0.36}
{'loss': 1.438, 'grad_norm': 1.1603044271469116, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.138906478881836, 'eval_runtime': 9.0117, 'eval_samples_per_second': 110.967, 'eval_steps_per_second': 6.991, 'epoch': 0.4}
{'loss': 1.4126, 'grad_norm': 1.3769657611846924, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.143337607383728, 'eval_runtime': 8.9766, 'eval_samples_per_second': 111.401, 'eval_steps_per_second': 7.018, 'epoch': 0.44}
{'loss': 1.4062, 'grad_norm': 2.1917500495910645, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.1474461555480957, 'eval_runtime': 8.9804, 'eval_samples_per_second': 111.354, 'eval_steps_per_second': 7.015, 'epoch': 0.48}
{'loss': 1.3784, 'grad_norm': 2.5417263507843018, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.1304502487182617, 'eval_runtime': 8.9878, 'eval_samples_per_second': 111.262, 'eval_steps_per_second': 7.009, 'epoch': 0.52}
{'loss': 1.362, 'grad_norm': 2.379243850708008, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.0916731357574463, 'eval_runtime': 8.999, 'eval_samples_per_second': 111.124, 'eval_steps_per_second': 7.001, 'epoch': 0.56}
{'loss': 1.3017, 'grad_norm': 2.1920487880706787, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.0731650590896606, 'eval_runtime': 8.9719, 'eval_samples_per_second': 111.46, 'eval_steps_per_second': 7.022, 'epoch': 0.6}
{'loss': 1.3181, 'grad_norm': 1.0073153972625732, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 1.0665271282196045, 'eval_runtime': 8.9824, 'eval_samples_per_second': 111.328, 'eval_steps_per_second': 7.014, 'epoch': 0.64}
{'loss': 1.3014, 'grad_norm': 1.6801894903182983, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 1.0792492628097534, 'eval_runtime': 8.9796, 'eval_samples_per_second': 111.364, 'eval_steps_per_second': 7.016, 'epoch': 0.68}
{'loss': 1.2513, 'grad_norm': 1.2326468229293823, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 1.064068078994751, 'eval_runtime': 8.9655, 'eval_samples_per_second': 111.538, 'eval_steps_per_second': 7.027, 'epoch': 0.72}
{'loss': 1.2743, 'grad_norm': 2.0524563789367676, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 1.060665488243103, 'eval_runtime': 8.9733, 'eval_samples_per_second': 111.442, 'eval_steps_per_second': 7.021, 'epoch': 0.76}
{'loss': 1.2407, 'grad_norm': 1.6517561674118042, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 1.0636321306228638, 'eval_runtime': 8.9741, 'eval_samples_per_second': 111.431, 'eval_steps_per_second': 7.02, 'epoch': 0.8}
{'loss': 1.2774, 'grad_norm': 1.8785086870193481, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 1.0603824853897095, 'eval_runtime': 8.9717, 'eval_samples_per_second': 111.462, 'eval_steps_per_second': 7.022, 'epoch': 0.84}
{'loss': 1.23, 'grad_norm': 1.2277816534042358, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 1.0516178607940674, 'eval_runtime': 8.9675, 'eval_samples_per_second': 111.514, 'eval_steps_per_second': 7.025, 'epoch': 0.88}
{'loss': 1.1775, 'grad_norm': 0.8642047643661499, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 1.04425048828125, 'eval_runtime': 8.9849, 'eval_samples_per_second': 111.298, 'eval_steps_per_second': 7.012, 'epoch': 0.92}
{'loss': 1.2282, 'grad_norm': 1.6681938171386719, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 1.0500917434692383, 'eval_runtime': 9.0148, 'eval_samples_per_second': 110.928, 'eval_steps_per_second': 6.988, 'epoch': 0.96}
{'train_runtime': 393.6293, 'train_samples_per_second': 25.318, 'train_steps_per_second': 1.583, 'train_loss': 1.552043654562765, 'epoch': 1.0}
train_results:  {'eval_loss': [2.2618467807769775, 1.6635541915893555, 1.4577964544296265, 1.3357319831848145, 1.2822167873382568, 1.2318259477615356, 1.2269198894500732, 1.1849080324172974, 1.1502010822296143, 1.138906478881836, 1.143337607383728, 1.1474461555480957, 1.1304502487182617, 1.0916731357574463, 1.0731650590896606, 1.0665271282196045, 1.0792492628097534, 1.064068078994751, 1.060665488243103, 1.0636321306228638, 1.0603824853897095, 1.0516178607940674, 1.04425048828125, 1.0500917434692383], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.2618467807769775, 1.6635541915893555, 1.4577964544296265, 1.3357319831848145, 1.2822167873382568, 1.2318259477615356, 1.2269198894500732, 1.1849080324172974, 1.1502010822296143, 1.138906478881836, 1.143337607383728, 1.1474461555480957, 1.1304502487182617, 1.0916731357574463, 1.0731650590896606, 1.0665271282196045, 1.0792492628097534, 1.064068078994751, 1.060665488243103, 1.0636321306228638, 1.0603824853897095, 1.0516178607940674, 1.04425048828125, 1.0500917434692383]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.716129183769226
current iteration best possible eval_loss (full train run):  -1.0500917434692383
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.4937 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5840210318565369, 0.5403404831886292, 0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.49047714471817017, 0.8636659979820251, 0.10396885871887207, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.09389074146747589, 0.5328817963600159, 0.1704922765493393, 0.8951139450073242]  ‚Üí  acq = -0.9239883482573368
X = [0.20226186513900757, 0.6831576228141785, 0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.24446256458759308, 0.7839422821998596, 0.6810406446456909, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.06929440051317215, 0.9363643527030945, 0.4914133846759796, 0.07482516765594482]  ‚Üí  acq = -0.9058403191721903
X = [0.7401218414306641, 0.8363668322563171, 0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.5837264060974121, 0.37286609411239624, 0.8461750149726868, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.32410305738449097, 0.8762340545654297, 0.1423635184764862, 0.6519256234169006]  ‚Üí  acq = -0.9239883486172992
X = [0.5809492468833923, 0.9983730316162109, 0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.5152989625930786, 0.0025587081909179688, 0.5287423133850098, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.04051867127418518, 0.9936825633049011, 0.13591282069683075, 0.053788065910339355]  ‚Üí  acq = -0.923988347149549
X = [0.3528999090194702, 0.5949426293373108, 0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.40121349692344666, 0.7189667820930481, 0.7950335144996643, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4900456964969635, 0.8887658715248108, 0.9805734157562256, 0.013015925884246826]  ‚Üí  acq = -0.9239883482671185
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1450, dtype=torch.float64), tensor(0.2582, dtype=torch.float64), 0, tensor(0.2274, dtype=torch.float64), 0, 0, tensor(0.0702, dtype=torch.float64), tensor(0.0796, dtype=torch.float64), tensor(0.2197, dtype=torch.float64), 1, 0, 1, 1, 1, 1, 2, 0.07687089220180171, 31.667482324515483, 1]
normalized proposed parameters for next round by BO: [tensor(0.1450, dtype=torch.float64), tensor(0.2582, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2274, dtype=torch.float64), tensor(1.9032e-17, dtype=torch.float64), tensor(7.3449e-17, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), tensor(0.0796, dtype=torch.float64), tensor(0.2197, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.7687, dtype=torch.float64), tensor(0.6597, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.145
  gsm8k: 0.258
  rowan_hellaswag: 0
  sciq: 0.227
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.07
  mmlu: 0.08
  arc_challenge: 0.22

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.07687089220180171,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (31.667482324515483,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  2
lora dropout:  0.07687089220180171
lora alpha:  31.667482324515483
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 120,832 || all params: 8,030,382,080 || trainable%: 0.0015
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3445, 'grad_norm': 12.429859161376953, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9388782978057861, 'eval_runtime': 8.9527, 'eval_samples_per_second': 111.698, 'eval_steps_per_second': 7.037, 'epoch': 0.04}
{'loss': 2.2473, 'grad_norm': 6.898841857910156, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4823676347732544, 'eval_runtime': 8.9964, 'eval_samples_per_second': 111.155, 'eval_steps_per_second': 7.003, 'epoch': 0.08}
{'loss': 1.7425, 'grad_norm': 5.7813310623168945, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3073073625564575, 'eval_runtime': 9.0299, 'eval_samples_per_second': 110.743, 'eval_steps_per_second': 6.977, 'epoch': 0.12}
{'loss': 1.4902, 'grad_norm': 6.210762977600098, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1916773319244385, 'eval_runtime': 9.0318, 'eval_samples_per_second': 110.72, 'eval_steps_per_second': 6.975, 'epoch': 0.16}
{'loss': 1.2881, 'grad_norm': 5.618035316467285, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1044236421585083, 'eval_runtime': 9.0576, 'eval_samples_per_second': 110.405, 'eval_steps_per_second': 6.955, 'epoch': 0.2}
{'loss': 1.2419, 'grad_norm': 4.312932968139648, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.059100866317749, 'eval_runtime': 9.0624, 'eval_samples_per_second': 110.346, 'eval_steps_per_second': 6.952, 'epoch': 0.24}
{'loss': 1.2104, 'grad_norm': 4.015994548797607, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0261296033859253, 'eval_runtime': 9.077, 'eval_samples_per_second': 110.169, 'eval_steps_per_second': 6.941, 'epoch': 0.28}
{'loss': 1.1297, 'grad_norm': 2.914487361907959, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.006709337234497, 'eval_runtime': 9.0922, 'eval_samples_per_second': 109.985, 'eval_steps_per_second': 6.929, 'epoch': 0.32}
{'loss': 1.1791, 'grad_norm': 3.0822832584381104, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9945908188819885, 'eval_runtime': 9.0747, 'eval_samples_per_second': 110.196, 'eval_steps_per_second': 6.942, 'epoch': 0.36}
{'loss': 1.1457, 'grad_norm': 2.5859668254852295, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9932740330696106, 'eval_runtime': 9.0982, 'eval_samples_per_second': 109.912, 'eval_steps_per_second': 6.924, 'epoch': 0.4}
{'loss': 1.125, 'grad_norm': 3.244847536087036, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9822013974189758, 'eval_runtime': 9.1114, 'eval_samples_per_second': 109.753, 'eval_steps_per_second': 6.914, 'epoch': 0.44}
{'loss': 1.1018, 'grad_norm': 3.6417605876922607, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9780147075653076, 'eval_runtime': 9.107, 'eval_samples_per_second': 109.805, 'eval_steps_per_second': 6.918, 'epoch': 0.48}
{'loss': 1.1273, 'grad_norm': 3.1333072185516357, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9730280637741089, 'eval_runtime': 9.0796, 'eval_samples_per_second': 110.137, 'eval_steps_per_second': 6.939, 'epoch': 0.52}
{'loss': 1.0863, 'grad_norm': 3.112901449203491, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9675761461257935, 'eval_runtime': 9.0359, 'eval_samples_per_second': 110.67, 'eval_steps_per_second': 6.972, 'epoch': 0.56}
{'loss': 1.122, 'grad_norm': 3.1872782707214355, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9684742093086243, 'eval_runtime': 9.0383, 'eval_samples_per_second': 110.64, 'eval_steps_per_second': 6.97, 'epoch': 0.6}
{'loss': 1.1183, 'grad_norm': 5.093835353851318, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9637080430984497, 'eval_runtime': 9.0796, 'eval_samples_per_second': 110.138, 'eval_steps_per_second': 6.939, 'epoch': 0.64}
{'loss': 1.0962, 'grad_norm': 3.7619194984436035, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9732407331466675, 'eval_runtime': 9.0586, 'eval_samples_per_second': 110.392, 'eval_steps_per_second': 6.955, 'epoch': 0.68}
{'loss': 1.1037, 'grad_norm': 12.304287910461426, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9621278047561646, 'eval_runtime': 9.021, 'eval_samples_per_second': 110.853, 'eval_steps_per_second': 6.984, 'epoch': 0.72}
{'loss': 1.0983, 'grad_norm': 1.5924654006958008, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9582518339157104, 'eval_runtime': 9.0157, 'eval_samples_per_second': 110.918, 'eval_steps_per_second': 6.988, 'epoch': 0.76}
{'loss': 1.0913, 'grad_norm': 2.87674617767334, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9563395977020264, 'eval_runtime': 9.0316, 'eval_samples_per_second': 110.723, 'eval_steps_per_second': 6.976, 'epoch': 0.8}
{'loss': 1.0616, 'grad_norm': 2.270261526107788, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9524135589599609, 'eval_runtime': 9.0443, 'eval_samples_per_second': 110.567, 'eval_steps_per_second': 6.966, 'epoch': 0.84}
{'loss': 1.1166, 'grad_norm': 2.372701644897461, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.953453004360199, 'eval_runtime': 9.0319, 'eval_samples_per_second': 110.718, 'eval_steps_per_second': 6.975, 'epoch': 0.88}
{'loss': 1.0839, 'grad_norm': 3.256714344024658, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9514946937561035, 'eval_runtime': 9.0266, 'eval_samples_per_second': 110.784, 'eval_steps_per_second': 6.979, 'epoch': 0.92}
{'loss': 1.0812, 'grad_norm': 1.6943309307098389, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9522101879119873, 'eval_runtime': 8.9837, 'eval_samples_per_second': 111.313, 'eval_steps_per_second': 7.013, 'epoch': 0.96}
{'loss': 1.0951, 'grad_norm': 2.4480397701263428, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9498053193092346, 'eval_runtime': 8.9714, 'eval_samples_per_second': 111.465, 'eval_steps_per_second': 7.022, 'epoch': 1.0}
{'train_runtime': 409.8677, 'train_samples_per_second': 24.388, 'train_steps_per_second': 1.525, 'train_loss': 1.3011258270263673, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9388782978057861, 1.4823676347732544, 1.3073073625564575, 1.1916773319244385, 1.1044236421585083, 1.059100866317749, 1.0261296033859253, 1.006709337234497, 0.9945908188819885, 0.9932740330696106, 0.9822013974189758, 0.9780147075653076, 0.9730280637741089, 0.9675761461257935, 0.9684742093086243, 0.9637080430984497, 0.9732407331466675, 0.9621278047561646, 0.9582518339157104, 0.9563395977020264, 0.9524135589599609, 0.953453004360199, 0.9514946937561035, 0.9522101879119873, 0.9498053193092346], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9388782978057861, 1.4823676347732544, 1.3073073625564575, 1.1916773319244385, 1.1044236421585083, 1.059100866317749, 1.0261296033859253, 1.006709337234497, 0.9945908188819885, 0.9932740330696106, 0.9822013974189758, 0.9780147075653076, 0.9730280637741089, 0.9675761461257935, 0.9684742093086243, 0.9637080430984497, 0.9732407331466675, 0.9621278047561646, 0.9582518339157104, 0.9563395977020264, 0.9524135589599609, 0.953453004360199, 0.9514946937561035, 0.9522101879119873, 0.9498053193092346]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.9551810026168823
current iteration best possible eval_loss (full train run):  -0.9498053193092346
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.2264 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -0.8778826719141047
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.8671406864327511
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -0.8723836541906778
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -0.8617944385552799
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -0.8685939382826646
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2420, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.4603, dtype=torch.float64), tensor(0.0392, dtype=torch.float64), tensor(0.2586, dtype=torch.float64), 11, 0, 0, 1, 0, 1, 2, 0.03292437310728359, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2420, dtype=torch.float64), tensor(3.0785e-17, dtype=torch.float64), tensor(1.0405e-16, dtype=torch.float64), tensor(2.7267e-17, dtype=torch.float64), tensor(7.2479e-17, dtype=torch.float64), tensor(3.2748e-17, dtype=torch.float64), tensor(0.4603, dtype=torch.float64), tensor(0.0392, dtype=torch.float64), tensor(0.2586, dtype=torch.float64), tensor(0.3330, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.3292, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.242
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.46
  mmlu: 0.039
  arc_challenge: 0.259

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.03292437310728359,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  2
lora dropout:  0.03292437310728359
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 811,008 || all params: 8,031,072,256 || trainable%: 0.0101
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2247, 'grad_norm': 4.7654876708984375, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6525071859359741, 'eval_runtime': 9.2637, 'eval_samples_per_second': 107.948, 'eval_steps_per_second': 6.801, 'epoch': 0.04}
{'loss': 1.8658, 'grad_norm': 2.0043227672576904, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5496426820755005, 'eval_runtime': 9.3145, 'eval_samples_per_second': 107.359, 'eval_steps_per_second': 6.764, 'epoch': 0.08}
{'loss': 1.5833, 'grad_norm': 1.5837829113006592, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5628198385238647, 'eval_runtime': 9.3354, 'eval_samples_per_second': 107.119, 'eval_steps_per_second': 6.749, 'epoch': 0.12}
{'loss': 1.5719, 'grad_norm': 1.7345592975616455, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5654138326644897, 'eval_runtime': 9.3721, 'eval_samples_per_second': 106.7, 'eval_steps_per_second': 6.722, 'epoch': 0.16}
{'loss': 1.4014, 'grad_norm': 1.692710041999817, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5141178369522095, 'eval_runtime': 9.3766, 'eval_samples_per_second': 106.649, 'eval_steps_per_second': 6.719, 'epoch': 0.2}
{'loss': 1.4273, 'grad_norm': 1.4000544548034668, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5275002717971802, 'eval_runtime': 9.4065, 'eval_samples_per_second': 106.309, 'eval_steps_per_second': 6.697, 'epoch': 0.24}
{'loss': 1.4794, 'grad_norm': 1.5134024620056152, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5134104490280151, 'eval_runtime': 9.4275, 'eval_samples_per_second': 106.073, 'eval_steps_per_second': 6.683, 'epoch': 0.28}
{'loss': 1.4539, 'grad_norm': 1.3927472829818726, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5268505811691284, 'eval_runtime': 9.4241, 'eval_samples_per_second': 106.111, 'eval_steps_per_second': 6.685, 'epoch': 0.32}
{'loss': 1.3742, 'grad_norm': 1.3228193521499634, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5432733297348022, 'eval_runtime': 9.4072, 'eval_samples_per_second': 106.302, 'eval_steps_per_second': 6.697, 'epoch': 0.36}
{'loss': 1.3515, 'grad_norm': 1.6605515480041504, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5770937204360962, 'eval_runtime': 9.353, 'eval_samples_per_second': 106.918, 'eval_steps_per_second': 6.736, 'epoch': 0.4}
{'loss': 1.3266, 'grad_norm': 1.8855613470077515, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5806176662445068, 'eval_runtime': 9.3668, 'eval_samples_per_second': 106.76, 'eval_steps_per_second': 6.726, 'epoch': 0.44}
{'loss': 1.3226, 'grad_norm': 1.4587088823318481, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5761938095092773, 'eval_runtime': 9.3452, 'eval_samples_per_second': 107.007, 'eval_steps_per_second': 6.741, 'epoch': 0.48}
{'loss': 1.3835, 'grad_norm': 1.6947999000549316, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5777199268341064, 'eval_runtime': 9.3653, 'eval_samples_per_second': 106.777, 'eval_steps_per_second': 6.727, 'epoch': 0.52}
{'loss': 1.311, 'grad_norm': 1.668674111366272, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6435307264328003, 'eval_runtime': 9.3598, 'eval_samples_per_second': 106.839, 'eval_steps_per_second': 6.731, 'epoch': 0.56}
{'loss': 1.2768, 'grad_norm': 1.81353759765625, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.594010829925537, 'eval_runtime': 9.3595, 'eval_samples_per_second': 106.843, 'eval_steps_per_second': 6.731, 'epoch': 0.6}
{'loss': 1.315, 'grad_norm': 1.7473597526550293, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5934534072875977, 'eval_runtime': 9.3693, 'eval_samples_per_second': 106.731, 'eval_steps_per_second': 6.724, 'epoch': 0.64}
{'loss': 1.4016, 'grad_norm': 1.8404476642608643, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5821821689605713, 'eval_runtime': 9.3559, 'eval_samples_per_second': 106.885, 'eval_steps_per_second': 6.734, 'epoch': 0.68}
{'loss': 1.3157, 'grad_norm': 1.7386735677719116, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.589006781578064, 'eval_runtime': 9.3603, 'eval_samples_per_second': 106.835, 'eval_steps_per_second': 6.731, 'epoch': 0.72}
{'loss': 1.3688, 'grad_norm': 1.8624249696731567, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5581997632980347, 'eval_runtime': 9.4076, 'eval_samples_per_second': 106.297, 'eval_steps_per_second': 6.697, 'epoch': 0.76}
{'loss': 1.3649, 'grad_norm': 1.9242396354675293, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5868767499923706, 'eval_runtime': 9.4299, 'eval_samples_per_second': 106.046, 'eval_steps_per_second': 6.681, 'epoch': 0.8}
{'loss': 1.3321, 'grad_norm': 2.0401368141174316, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5798894166946411, 'eval_runtime': 9.4145, 'eval_samples_per_second': 106.219, 'eval_steps_per_second': 6.692, 'epoch': 0.84}
{'loss': 1.3317, 'grad_norm': 1.9491488933563232, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5752943754196167, 'eval_runtime': 9.4142, 'eval_samples_per_second': 106.223, 'eval_steps_per_second': 6.692, 'epoch': 0.88}
{'loss': 1.2897, 'grad_norm': 1.7495925426483154, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5916800498962402, 'eval_runtime': 9.3975, 'eval_samples_per_second': 106.412, 'eval_steps_per_second': 6.704, 'epoch': 0.92}
{'loss': 1.3813, 'grad_norm': 1.765486717224121, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5925817489624023, 'eval_runtime': 9.4472, 'eval_samples_per_second': 105.852, 'eval_steps_per_second': 6.669, 'epoch': 0.96}
{'loss': 1.3657, 'grad_norm': 2.4268009662628174, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.592583417892456, 'eval_runtime': 9.435, 'eval_samples_per_second': 105.988, 'eval_steps_per_second': 6.677, 'epoch': 1.0}
{'train_runtime': 354.177, 'train_samples_per_second': 28.226, 'train_steps_per_second': 1.765, 'train_loss': 1.4728137573242188, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6525071859359741, 1.5496426820755005, 1.5628198385238647, 1.5654138326644897, 1.5141178369522095, 1.5275002717971802, 1.5134104490280151, 1.5268505811691284, 1.5432733297348022, 1.5770937204360962, 1.5806176662445068, 1.5761938095092773, 1.5777199268341064, 1.6435307264328003, 1.594010829925537, 1.5934534072875977, 1.5821821689605713, 1.589006781578064, 1.5581997632980347, 1.5868767499923706, 1.5798894166946411, 1.5752943754196167, 1.5916800498962402, 1.5925817489624023, 1.592583417892456], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6525071859359741, 1.5496426820755005, 1.5628198385238647, 1.5654138326644897, 1.5141178369522095, 1.5275002717971802, 1.5134104490280151, 1.5268505811691284, 1.5432733297348022, 1.5770937204360962, 1.5806176662445068, 1.5761938095092773, 1.5777199268341064, 1.6435307264328003, 1.594010829925537, 1.5934534072875977, 1.5821821689605713, 1.589006781578064, 1.5581997632980347, 1.5868767499923706, 1.5798894166946411, 1.5752943754196167, 1.5916800498962402, 1.5925817489624023, 1.592583417892456]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.3828556537628174
current iteration best possible eval_loss (full train run):  -1.592583417892456
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.9959 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -0.8742664268769614
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -0.8742664293135078
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -0.874266427793947
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.8741041310310446
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -0.8742664269219685
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3558, dtype=torch.float64), tensor(0.1874, dtype=torch.float64), tensor(0.0259, dtype=torch.float64), tensor(0.1476, dtype=torch.float64), tensor(0.0530, dtype=torch.float64), 0, 0, 0, tensor(0.2266, dtype=torch.float64), 14, 1, 1, 0, 1, 1, 88, 0.1, 10.39501895284633, 0]
normalized proposed parameters for next round by BO: [tensor(0.3558, dtype=torch.float64), tensor(0.1874, dtype=torch.float64), tensor(0.0259, dtype=torch.float64), tensor(0.1476, dtype=torch.float64), tensor(0.0530, dtype=torch.float64), tensor(3.9432e-18, dtype=torch.float64), tensor(7.6839e-19, dtype=torch.float64), tensor(0.0036, dtype=torch.float64), tensor(0.2266, dtype=torch.float64), tensor(0.4330, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6848, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2166, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.356
  gsm8k: 0.187
  rowan_hellaswag: 0.026
  sciq: 0.148
  triviaqa: 0.053
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.227

LoRA Parameters:
  lora_r: (88,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (10.39501895284633,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  88
lora dropout:  0.1
lora alpha:  10.39501895284633
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 61,816,832 || all params: 8,092,078,080 || trainable%: 0.7639
length of training data:  9961
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6191, 'grad_norm': 0.5594616532325745, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6496425867080688, 'eval_runtime': 9.6535, 'eval_samples_per_second': 103.589, 'eval_steps_per_second': 6.526, 'epoch': 0.04}
{'loss': 1.5872, 'grad_norm': 0.2310977578163147, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1670315265655518, 'eval_runtime': 9.6959, 'eval_samples_per_second': 103.136, 'eval_steps_per_second': 6.498, 'epoch': 0.08}
{'loss': 1.1546, 'grad_norm': 0.15880388021469116, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.0052074193954468, 'eval_runtime': 9.676, 'eval_samples_per_second': 103.349, 'eval_steps_per_second': 6.511, 'epoch': 0.12}
{'loss': 1.0761, 'grad_norm': 0.11954506486654282, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 0.9561863541603088, 'eval_runtime': 9.6811, 'eval_samples_per_second': 103.294, 'eval_steps_per_second': 6.508, 'epoch': 0.16}
{'loss': 1.0177, 'grad_norm': 0.11522278934717178, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 0.9412967562675476, 'eval_runtime': 9.6985, 'eval_samples_per_second': 103.109, 'eval_steps_per_second': 6.496, 'epoch': 0.2}
{'loss': 1.0173, 'grad_norm': 0.10379501432180405, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 0.9299118518829346, 'eval_runtime': 9.7176, 'eval_samples_per_second': 102.906, 'eval_steps_per_second': 6.483, 'epoch': 0.24}
{'loss': 1.0088, 'grad_norm': 0.13647185266017914, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 0.9220806956291199, 'eval_runtime': 9.718, 'eval_samples_per_second': 102.902, 'eval_steps_per_second': 6.483, 'epoch': 0.28}
{'loss': 1.0002, 'grad_norm': 0.1188095360994339, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 0.9179672002792358, 'eval_runtime': 9.7077, 'eval_samples_per_second': 103.011, 'eval_steps_per_second': 6.49, 'epoch': 0.32}
{'loss': 0.9617, 'grad_norm': 0.12139632552862167, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 0.9180643558502197, 'eval_runtime': 9.7724, 'eval_samples_per_second': 102.329, 'eval_steps_per_second': 6.447, 'epoch': 0.36}
{'loss': 0.9559, 'grad_norm': 0.11035002022981644, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 0.9137927293777466, 'eval_runtime': 9.749, 'eval_samples_per_second': 102.575, 'eval_steps_per_second': 6.462, 'epoch': 0.4}
{'loss': 0.9762, 'grad_norm': 0.12318678200244904, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 0.9085749387741089, 'eval_runtime': 9.784, 'eval_samples_per_second': 102.208, 'eval_steps_per_second': 6.439, 'epoch': 0.44}
{'loss': 0.9561, 'grad_norm': 0.13391561806201935, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 0.9025471210479736, 'eval_runtime': 9.7961, 'eval_samples_per_second': 102.082, 'eval_steps_per_second': 6.431, 'epoch': 0.48}
{'loss': 0.9431, 'grad_norm': 0.11768969893455505, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 0.9005125761032104, 'eval_runtime': 9.8097, 'eval_samples_per_second': 101.94, 'eval_steps_per_second': 6.422, 'epoch': 0.52}
{'loss': 0.9326, 'grad_norm': 0.12094561010599136, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 0.8991432785987854, 'eval_runtime': 9.7846, 'eval_samples_per_second': 102.201, 'eval_steps_per_second': 6.439, 'epoch': 0.56}
{'loss': 0.96, 'grad_norm': 0.13533584773540497, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 0.898969829082489, 'eval_runtime': 9.8171, 'eval_samples_per_second': 101.863, 'eval_steps_per_second': 6.417, 'epoch': 0.6}
{'loss': 0.9351, 'grad_norm': 0.11938159167766571, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 0.8970453143119812, 'eval_runtime': 9.7943, 'eval_samples_per_second': 102.1, 'eval_steps_per_second': 6.432, 'epoch': 0.64}
{'loss': 0.9315, 'grad_norm': 0.12994319200515747, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 0.8937647342681885, 'eval_runtime': 9.7906, 'eval_samples_per_second': 102.138, 'eval_steps_per_second': 6.435, 'epoch': 0.68}
{'loss': 0.9565, 'grad_norm': 0.12487271428108215, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 0.8944814205169678, 'eval_runtime': 9.7955, 'eval_samples_per_second': 102.088, 'eval_steps_per_second': 6.432, 'epoch': 0.72}
{'loss': 0.9308, 'grad_norm': 0.1402575522661209, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 0.8927124738693237, 'eval_runtime': 9.7785, 'eval_samples_per_second': 102.265, 'eval_steps_per_second': 6.443, 'epoch': 0.76}
{'loss': 0.9266, 'grad_norm': 0.1305810958147049, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.8920126557350159, 'eval_runtime': 9.7764, 'eval_samples_per_second': 102.287, 'eval_steps_per_second': 6.444, 'epoch': 0.8}
{'loss': 0.9406, 'grad_norm': 0.11891695111989975, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.8921248912811279, 'eval_runtime': 9.7507, 'eval_samples_per_second': 102.557, 'eval_steps_per_second': 6.461, 'epoch': 0.84}
{'loss': 0.9187, 'grad_norm': 0.12268974632024765, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.8906089067459106, 'eval_runtime': 9.7127, 'eval_samples_per_second': 102.958, 'eval_steps_per_second': 6.486, 'epoch': 0.88}
{'loss': 0.9338, 'grad_norm': 0.13663674890995026, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.8889703154563904, 'eval_runtime': 9.7183, 'eval_samples_per_second': 102.899, 'eval_steps_per_second': 6.483, 'epoch': 0.92}
{'loss': 0.8895, 'grad_norm': 0.14167869091033936, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.8896401524543762, 'eval_runtime': 9.7273, 'eval_samples_per_second': 102.803, 'eval_steps_per_second': 6.477, 'epoch': 0.96}
{'train_runtime': 382.5999, 'train_samples_per_second': 26.035, 'train_steps_per_second': 1.628, 'train_loss': 1.0996755229527457, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6496425867080688, 1.1670315265655518, 1.0052074193954468, 0.9561863541603088, 0.9412967562675476, 0.9299118518829346, 0.9220806956291199, 0.9179672002792358, 0.9180643558502197, 0.9137927293777466, 0.9085749387741089, 0.9025471210479736, 0.9005125761032104, 0.8991432785987854, 0.898969829082489, 0.8970453143119812, 0.8937647342681885, 0.8944814205169678, 0.8927124738693237, 0.8920126557350159, 0.8921248912811279, 0.8906089067459106, 0.8889703154563904, 0.8896401524543762], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.6496425867080688, 1.1670315265655518, 1.0052074193954468, 0.9561863541603088, 0.9412967562675476, 0.9299118518829346, 0.9220806956291199, 0.9179672002792358, 0.9180643558502197, 0.9137927293777466, 0.9085749387741089, 0.9025471210479736, 0.9005125761032104, 0.8991432785987854, 0.898969829082489, 0.8970453143119812, 0.8937647342681885, 0.8944814205169678, 0.8927124738693237, 0.8920126557350159, 0.8921248912811279, 0.8906089067459106, 0.8889703154563904, 0.8896401524543762]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0787910223007202
current iteration best possible eval_loss (full train run):  -0.8896401524543762
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.5067 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.35491812229156494, 0.0927686095237732, 0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7262229919433594, 0.8165992498397827, 0.8520699143409729, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9559857845306396, 0.3273009657859802, 0.9828505516052246, 0.5352497100830078]  ‚Üí  acq = -0.9016972960838077
X = [0.8189860582351685, 0.7869956493377686, 0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8973821401596069, 0.7314273715019226, 0.9184417128562927, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9420246481895447, 0.9331071972846985, 0.31236356496810913, 0.36077266931533813]  ‚Üí  acq = -0.9016972960838077
X = [0.7738390564918518, 0.5021045207977295, 0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5741828680038452, 0.5517953038215637, 0.7892404794692993, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9668935537338257, 0.5031551122665405, 0.7251023054122925, 0.2269490361213684]  ‚Üí  acq = -0.9016972960838077
X = [0.20103693008422852, 0.27278316020965576, 0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.14547844231128693, 0.0291365385055542, 0.3460528254508972, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.32679685950279236, 0.4884251356124878, 0.9665257930755615, 0.2274898886680603]  ‚Üí  acq = -0.9016972960838077
X = [0.6374526023864746, 0.36883002519607544, 0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3681538701057434, 0.6981962323188782, 0.7622081637382507, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.5123441219329834, 0.3907546401023865, 0.21269100904464722, 0.09688013792037964]  ‚Üí  acq = -0.9016972960838077
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.8142, dtype=torch.float64), tensor(0.0513, dtype=torch.float64), 0, tensor(0.0284, dtype=torch.float64), 0, 0, 0, tensor(0.1061, dtype=torch.float64), 16, 0, 1, 0, 1, 1, 128, 0.1, 26.539926571520834, 0]
normalized proposed parameters for next round by BO: [tensor(1.7069e-17, dtype=torch.float64), tensor(0.8142, dtype=torch.float64), tensor(0.0513, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0284, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5257e-18, dtype=torch.float64), tensor(0.1061, dtype=torch.float64), tensor(0.4884, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5529, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.814
  rowan_hellaswag: 0.051
  sciq: 0
  triviaqa: 0.028
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.106

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (16,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (26.539926571520834,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  16
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  26.539926571520834
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 85,983,232 || all params: 8,116,244,480 || trainable%: 1.0594
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.3538, 'grad_norm': 0.44638514518737793, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3307262659072876, 'eval_runtime': 9.6001, 'eval_samples_per_second': 104.166, 'eval_steps_per_second': 6.562, 'epoch': 0.04}
{'loss': 1.2338, 'grad_norm': 0.20814268290996552, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9370454549789429, 'eval_runtime': 9.6388, 'eval_samples_per_second': 103.748, 'eval_steps_per_second': 6.536, 'epoch': 0.08}
{'loss': 0.9941, 'grad_norm': 0.1287296861410141, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8894610404968262, 'eval_runtime': 9.6729, 'eval_samples_per_second': 103.382, 'eval_steps_per_second': 6.513, 'epoch': 0.12}
{'loss': 0.9951, 'grad_norm': 0.14487846195697784, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8776711225509644, 'eval_runtime': 9.7045, 'eval_samples_per_second': 103.045, 'eval_steps_per_second': 6.492, 'epoch': 0.16}
{'loss': 0.9388, 'grad_norm': 0.11503467708826065, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8682168126106262, 'eval_runtime': 9.716, 'eval_samples_per_second': 102.923, 'eval_steps_per_second': 6.484, 'epoch': 0.2}
{'loss': 0.9546, 'grad_norm': 0.11437033861875534, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8626559972763062, 'eval_runtime': 9.7237, 'eval_samples_per_second': 102.842, 'eval_steps_per_second': 6.479, 'epoch': 0.24}
{'loss': 0.938, 'grad_norm': 0.10814618319272995, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8586001992225647, 'eval_runtime': 9.6841, 'eval_samples_per_second': 103.262, 'eval_steps_per_second': 6.506, 'epoch': 0.28}
{'loss': 0.9152, 'grad_norm': 0.136980801820755, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8543092012405396, 'eval_runtime': 9.672, 'eval_samples_per_second': 103.391, 'eval_steps_per_second': 6.514, 'epoch': 0.32}
{'loss': 0.9044, 'grad_norm': 0.10965925455093384, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8512483239173889, 'eval_runtime': 9.6839, 'eval_samples_per_second': 103.265, 'eval_steps_per_second': 6.506, 'epoch': 0.36}
{'loss': 0.8778, 'grad_norm': 0.116511270403862, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8488656282424927, 'eval_runtime': 9.6814, 'eval_samples_per_second': 103.291, 'eval_steps_per_second': 6.507, 'epoch': 0.4}
{'loss': 0.8792, 'grad_norm': 0.12873412668704987, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8461777567863464, 'eval_runtime': 9.6839, 'eval_samples_per_second': 103.264, 'eval_steps_per_second': 6.506, 'epoch': 0.44}
{'loss': 0.9042, 'grad_norm': 0.112216055393219, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8450385332107544, 'eval_runtime': 9.6995, 'eval_samples_per_second': 103.098, 'eval_steps_per_second': 6.495, 'epoch': 0.48}
{'loss': 0.9321, 'grad_norm': 0.1305907666683197, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8424746990203857, 'eval_runtime': 9.7783, 'eval_samples_per_second': 102.267, 'eval_steps_per_second': 6.443, 'epoch': 0.52}
{'loss': 0.8961, 'grad_norm': 0.12255344539880753, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8399397134780884, 'eval_runtime': 9.7506, 'eval_samples_per_second': 102.558, 'eval_steps_per_second': 6.461, 'epoch': 0.56}
{'loss': 0.9075, 'grad_norm': 0.13233165442943573, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8387857675552368, 'eval_runtime': 9.7569, 'eval_samples_per_second': 102.492, 'eval_steps_per_second': 6.457, 'epoch': 0.6}
{'loss': 0.8651, 'grad_norm': 0.1209593117237091, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8371232151985168, 'eval_runtime': 9.753, 'eval_samples_per_second': 102.532, 'eval_steps_per_second': 6.46, 'epoch': 0.64}
{'loss': 0.876, 'grad_norm': 0.13866481184959412, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8358892798423767, 'eval_runtime': 9.7706, 'eval_samples_per_second': 102.348, 'eval_steps_per_second': 6.448, 'epoch': 0.68}
{'loss': 0.9183, 'grad_norm': 0.13500702381134033, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8353012800216675, 'eval_runtime': 9.7661, 'eval_samples_per_second': 102.395, 'eval_steps_per_second': 6.451, 'epoch': 0.72}
{'loss': 0.8868, 'grad_norm': 0.16080473363399506, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8336750268936157, 'eval_runtime': 9.7564, 'eval_samples_per_second': 102.497, 'eval_steps_per_second': 6.457, 'epoch': 0.76}
{'loss': 0.8651, 'grad_norm': 0.14856404066085815, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8334384560585022, 'eval_runtime': 9.7476, 'eval_samples_per_second': 102.589, 'eval_steps_per_second': 6.463, 'epoch': 0.8}
{'loss': 0.8699, 'grad_norm': 0.13082821667194366, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8321617245674133, 'eval_runtime': 9.77, 'eval_samples_per_second': 102.354, 'eval_steps_per_second': 6.448, 'epoch': 0.84}
{'loss': 0.8502, 'grad_norm': 0.1377755105495453, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8312717080116272, 'eval_runtime': 9.7753, 'eval_samples_per_second': 102.299, 'eval_steps_per_second': 6.445, 'epoch': 0.88}
{'loss': 0.8696, 'grad_norm': 0.13461977243423462, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8307900428771973, 'eval_runtime': 9.8035, 'eval_samples_per_second': 102.004, 'eval_steps_per_second': 6.426, 'epoch': 0.92}
{'loss': 0.877, 'grad_norm': 0.1435571312904358, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8300725221633911, 'eval_runtime': 9.7816, 'eval_samples_per_second': 102.232, 'eval_steps_per_second': 6.441, 'epoch': 0.96}
{'loss': 0.8788, 'grad_norm': 0.1435365229845047, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8296436667442322, 'eval_runtime': 9.7801, 'eval_samples_per_second': 102.248, 'eval_steps_per_second': 6.442, 'epoch': 1.0}
{'train_runtime': 412.4252, 'train_samples_per_second': 24.242, 'train_steps_per_second': 1.515, 'train_loss': 0.9752564636230469, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3307262659072876, 0.9370454549789429, 0.8894610404968262, 0.8776711225509644, 0.8682168126106262, 0.8626559972763062, 0.8586001992225647, 0.8543092012405396, 0.8512483239173889, 0.8488656282424927, 0.8461777567863464, 0.8450385332107544, 0.8424746990203857, 0.8399397134780884, 0.8387857675552368, 0.8371232151985168, 0.8358892798423767, 0.8353012800216675, 0.8336750268936157, 0.8334384560585022, 0.8321617245674133, 0.8312717080116272, 0.8307900428771973, 0.8300725221633911, 0.8296436667442322], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3307262659072876, 0.9370454549789429, 0.8894610404968262, 0.8776711225509644, 0.8682168126106262, 0.8626559972763062, 0.8586001992225647, 0.8543092012405396, 0.8512483239173889, 0.8488656282424927, 0.8461777567863464, 0.8450385332107544, 0.8424746990203857, 0.8399397134780884, 0.8387857675552368, 0.8371232151985168, 0.8358892798423767, 0.8353012800216675, 0.8336750268936157, 0.8334384560585022, 0.8321617245674133, 0.8312717080116272, 0.8307900428771973, 0.8300725221633911, 0.8296436667442322]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.162550687789917
current iteration best possible eval_loss (full train run):  -0.8296436667442322
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.7563 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -0.934197728542735
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.9122157441077001
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -0.934197728542735
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -0.934197728542735
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -0.934197728542735
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2373, dtype=torch.float64), tensor(0.2088, dtype=torch.float64), tensor(0.0960, dtype=torch.float64), tensor(0.1974, dtype=torch.float64), tensor(0.0260, dtype=torch.float64), 0, 0, tensor(0.0625, dtype=torch.float64), tensor(0.1720, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 58, 0.01676321355996016, 17.972908416021703, 0]
normalized proposed parameters for next round by BO: [tensor(0.2373, dtype=torch.float64), tensor(0.2088, dtype=torch.float64), tensor(0.0960, dtype=torch.float64), tensor(0.1974, dtype=torch.float64), tensor(0.0260, dtype=torch.float64), tensor(2.9464e-18, dtype=torch.float64), tensor(4.0121e-19, dtype=torch.float64), tensor(0.0625, dtype=torch.float64), tensor(0.1720, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4563, dtype=torch.float64), tensor(0.1676, dtype=torch.float64), tensor(0.3744, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.237
  gsm8k: 0.209
  rowan_hellaswag: 0.096
  sciq: 0.197
  triviaqa: 0.026
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.063
  arc_challenge: 0.172

LoRA Parameters:
  lora_r: (58,)
  lora_dropout: (0.01676321355996016,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (17.972908416021703,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  58
lora dropout:  0.01676321355996016
lora alpha:  17.972908416021703
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 77,922,304 || all params: 8,108,183,552 || trainable%: 0.9610
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0253, 'grad_norm': 1.0217887163162231, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.356486439704895, 'eval_runtime': 10.4691, 'eval_samples_per_second': 95.519, 'eval_steps_per_second': 6.018, 'epoch': 0.04}
{'loss': 1.3796, 'grad_norm': 0.42232075333595276, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9660939574241638, 'eval_runtime': 10.4511, 'eval_samples_per_second': 95.684, 'eval_steps_per_second': 6.028, 'epoch': 0.08}
{'loss': 1.1266, 'grad_norm': 0.2506692111492157, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8952497839927673, 'eval_runtime': 10.4684, 'eval_samples_per_second': 95.526, 'eval_steps_per_second': 6.018, 'epoch': 0.12}
{'loss': 1.1146, 'grad_norm': 0.2585750222206116, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8847985863685608, 'eval_runtime': 10.5035, 'eval_samples_per_second': 95.206, 'eval_steps_per_second': 5.998, 'epoch': 0.16}
{'loss': 1.028, 'grad_norm': 0.18050231039524078, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8817687034606934, 'eval_runtime': 10.5116, 'eval_samples_per_second': 95.133, 'eval_steps_per_second': 5.993, 'epoch': 0.2}
{'loss': 1.0601, 'grad_norm': 0.22235260903835297, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8696643114089966, 'eval_runtime': 10.5217, 'eval_samples_per_second': 95.041, 'eval_steps_per_second': 5.988, 'epoch': 0.24}
{'loss': 1.0674, 'grad_norm': 0.1899063140153885, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.864703357219696, 'eval_runtime': 10.5731, 'eval_samples_per_second': 94.58, 'eval_steps_per_second': 5.959, 'epoch': 0.28}
{'loss': 1.0155, 'grad_norm': 0.18092933297157288, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8581804037094116, 'eval_runtime': 10.6377, 'eval_samples_per_second': 94.005, 'eval_steps_per_second': 5.922, 'epoch': 0.32}
{'loss': 1.0101, 'grad_norm': 0.21125942468643188, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8574501276016235, 'eval_runtime': 10.6204, 'eval_samples_per_second': 94.158, 'eval_steps_per_second': 5.932, 'epoch': 0.36}
{'loss': 1.0691, 'grad_norm': 0.22303704917430878, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8527127504348755, 'eval_runtime': 10.6316, 'eval_samples_per_second': 94.059, 'eval_steps_per_second': 5.926, 'epoch': 0.4}
{'loss': 1.0711, 'grad_norm': 0.1908637285232544, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8491278290748596, 'eval_runtime': 10.6231, 'eval_samples_per_second': 94.134, 'eval_steps_per_second': 5.93, 'epoch': 0.44}
{'loss': 0.9941, 'grad_norm': 0.1892276257276535, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8522006869316101, 'eval_runtime': 10.6076, 'eval_samples_per_second': 94.272, 'eval_steps_per_second': 5.939, 'epoch': 0.48}
{'loss': 0.944, 'grad_norm': 0.19965144991874695, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8454446792602539, 'eval_runtime': 10.6548, 'eval_samples_per_second': 93.855, 'eval_steps_per_second': 5.913, 'epoch': 0.52}
{'loss': 1.0134, 'grad_norm': 0.21127691864967346, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8414903879165649, 'eval_runtime': 10.6393, 'eval_samples_per_second': 93.991, 'eval_steps_per_second': 5.921, 'epoch': 0.56}
{'loss': 1.0162, 'grad_norm': 0.19907964766025543, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.846237301826477, 'eval_runtime': 10.644, 'eval_samples_per_second': 93.949, 'eval_steps_per_second': 5.919, 'epoch': 0.6}
{'loss': 1.0075, 'grad_norm': 0.18385113775730133, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8400037884712219, 'eval_runtime': 10.6087, 'eval_samples_per_second': 94.262, 'eval_steps_per_second': 5.939, 'epoch': 0.64}
{'loss': 1.0328, 'grad_norm': 0.20412075519561768, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8368216753005981, 'eval_runtime': 10.6204, 'eval_samples_per_second': 94.159, 'eval_steps_per_second': 5.932, 'epoch': 0.68}
{'loss': 0.9675, 'grad_norm': 0.1813150942325592, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8368818163871765, 'eval_runtime': 10.6163, 'eval_samples_per_second': 94.195, 'eval_steps_per_second': 5.934, 'epoch': 0.72}
{'loss': 0.9624, 'grad_norm': 0.19952616095542908, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8355210423469543, 'eval_runtime': 10.6089, 'eval_samples_per_second': 94.26, 'eval_steps_per_second': 5.938, 'epoch': 0.76}
{'loss': 1.0322, 'grad_norm': 0.21294544637203217, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8340629935264587, 'eval_runtime': 10.6132, 'eval_samples_per_second': 94.222, 'eval_steps_per_second': 5.936, 'epoch': 0.8}
{'loss': 1.004, 'grad_norm': 0.20776166021823883, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8342083692550659, 'eval_runtime': 10.6303, 'eval_samples_per_second': 94.071, 'eval_steps_per_second': 5.926, 'epoch': 0.84}
{'loss': 1.0083, 'grad_norm': 0.1743459552526474, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8320049047470093, 'eval_runtime': 10.6255, 'eval_samples_per_second': 94.113, 'eval_steps_per_second': 5.929, 'epoch': 0.88}
{'loss': 0.9984, 'grad_norm': 0.1934044063091278, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8319951891899109, 'eval_runtime': 10.6108, 'eval_samples_per_second': 94.244, 'eval_steps_per_second': 5.937, 'epoch': 0.92}
{'loss': 1.0132, 'grad_norm': 0.1787051260471344, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8320759534835815, 'eval_runtime': 10.6091, 'eval_samples_per_second': 94.258, 'eval_steps_per_second': 5.938, 'epoch': 0.96}
{'loss': 0.9804, 'grad_norm': 0.22568278014659882, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8315131068229675, 'eval_runtime': 10.59, 'eval_samples_per_second': 94.429, 'eval_steps_per_second': 5.949, 'epoch': 1.0}
{'train_runtime': 504.0002, 'train_samples_per_second': 19.833, 'train_steps_per_second': 1.24, 'train_loss': 1.1176705505371094, 'epoch': 1.0}
train_results:  {'eval_loss': [1.356486439704895, 0.9660939574241638, 0.8952497839927673, 0.8847985863685608, 0.8817687034606934, 0.8696643114089966, 0.864703357219696, 0.8581804037094116, 0.8574501276016235, 0.8527127504348755, 0.8491278290748596, 0.8522006869316101, 0.8454446792602539, 0.8414903879165649, 0.846237301826477, 0.8400037884712219, 0.8368216753005981, 0.8368818163871765, 0.8355210423469543, 0.8340629935264587, 0.8342083692550659, 0.8320049047470093, 0.8319951891899109, 0.8320759534835815, 0.8315131068229675], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.356486439704895, 0.9660939574241638, 0.8952497839927673, 0.8847985863685608, 0.8817687034606934, 0.8696643114089966, 0.864703357219696, 0.8581804037094116, 0.8574501276016235, 0.8527127504348755, 0.8491278290748596, 0.8522006869316101, 0.8454446792602539, 0.8414903879165649, 0.846237301826477, 0.8400037884712219, 0.8368216753005981, 0.8368818163871765, 0.8355210423469543, 0.8340629935264587, 0.8342083692550659, 0.8320049047470093, 0.8319951891899109, 0.8320759534835815, 0.8315131068229675]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.8315131068229675
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.3012 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.9063016004119822
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.9063018077235576
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.8986368928622988
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.9063018077235576
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -0.9063018077235576
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4749, dtype=torch.float64), 0, 0, tensor(0.0114, dtype=torch.float64), 0, 0, tensor(0.2029, dtype=torch.float64), tensor(0.3108, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 128, 0.04261842420913471, 12.601480064615009, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.4749, dtype=torch.float64), tensor(8.0920e-18, dtype=torch.float64), tensor(1.0876e-17, dtype=torch.float64), tensor(0.0114, dtype=torch.float64), tensor(8.7610e-18, dtype=torch.float64), tensor(1.1907e-17, dtype=torch.float64), tensor(0.2029, dtype=torch.float64), tensor(0.3108, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4262, dtype=torch.float64), tensor(0.2625, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.475
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.011
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.203
  arc_challenge: 0.311

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04261842420913471,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (12.601480064615009,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.04261842420913471
lora alpha:  12.601480064615009
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4929, 'grad_norm': 0.4324219226837158, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3427973985671997, 'eval_runtime': 10.3647, 'eval_samples_per_second': 96.481, 'eval_steps_per_second': 6.078, 'epoch': 0.04}
{'loss': 1.2104, 'grad_norm': 0.1972840428352356, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9639714360237122, 'eval_runtime': 10.4152, 'eval_samples_per_second': 96.014, 'eval_steps_per_second': 6.049, 'epoch': 0.08}
{'loss': 1.0182, 'grad_norm': 0.1324206292629242, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8774651885032654, 'eval_runtime': 10.4416, 'eval_samples_per_second': 95.771, 'eval_steps_per_second': 6.034, 'epoch': 0.12}
{'loss': 0.9641, 'grad_norm': 0.18942125141620636, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.863734781742096, 'eval_runtime': 10.4875, 'eval_samples_per_second': 95.352, 'eval_steps_per_second': 6.007, 'epoch': 0.16}
{'loss': 0.9488, 'grad_norm': 0.10072644054889679, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8581214547157288, 'eval_runtime': 10.4473, 'eval_samples_per_second': 95.719, 'eval_steps_per_second': 6.03, 'epoch': 0.2}
{'loss': 0.9068, 'grad_norm': 0.11214527487754822, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8525307178497314, 'eval_runtime': 10.4461, 'eval_samples_per_second': 95.73, 'eval_steps_per_second': 6.031, 'epoch': 0.24}
{'loss': 0.9266, 'grad_norm': 0.09920381009578705, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8458017110824585, 'eval_runtime': 10.4467, 'eval_samples_per_second': 95.724, 'eval_steps_per_second': 6.031, 'epoch': 0.28}
{'loss': 0.8987, 'grad_norm': 0.10683055967092514, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8444029092788696, 'eval_runtime': 10.4464, 'eval_samples_per_second': 95.727, 'eval_steps_per_second': 6.031, 'epoch': 0.32}
{'loss': 0.9334, 'grad_norm': 0.11335759609937668, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.839535653591156, 'eval_runtime': 10.4493, 'eval_samples_per_second': 95.7, 'eval_steps_per_second': 6.029, 'epoch': 0.36}
{'loss': 0.8769, 'grad_norm': 0.11043666303157806, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.834738552570343, 'eval_runtime': 10.4538, 'eval_samples_per_second': 95.659, 'eval_steps_per_second': 6.027, 'epoch': 0.4}
{'loss': 0.8913, 'grad_norm': 0.10932615399360657, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.833151638507843, 'eval_runtime': 10.4642, 'eval_samples_per_second': 95.564, 'eval_steps_per_second': 6.021, 'epoch': 0.44}
{'loss': 0.9155, 'grad_norm': 0.10443095862865448, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8313180804252625, 'eval_runtime': 10.4543, 'eval_samples_per_second': 95.655, 'eval_steps_per_second': 6.026, 'epoch': 0.48}
{'loss': 0.886, 'grad_norm': 0.10463649034500122, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8284292221069336, 'eval_runtime': 10.4636, 'eval_samples_per_second': 95.57, 'eval_steps_per_second': 6.021, 'epoch': 0.52}
{'loss': 0.8712, 'grad_norm': 0.11617500334978104, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8287610411643982, 'eval_runtime': 10.4658, 'eval_samples_per_second': 95.549, 'eval_steps_per_second': 6.02, 'epoch': 0.56}
{'loss': 0.8637, 'grad_norm': 0.120539590716362, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8283971548080444, 'eval_runtime': 10.5074, 'eval_samples_per_second': 95.171, 'eval_steps_per_second': 5.996, 'epoch': 0.6}
{'loss': 0.8721, 'grad_norm': 0.12892255187034607, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8209736347198486, 'eval_runtime': 10.4807, 'eval_samples_per_second': 95.414, 'eval_steps_per_second': 6.011, 'epoch': 0.64}
{'loss': 0.8632, 'grad_norm': 0.10830659419298172, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8205820322036743, 'eval_runtime': 10.4434, 'eval_samples_per_second': 95.754, 'eval_steps_per_second': 6.033, 'epoch': 0.68}
{'loss': 0.8649, 'grad_norm': 0.14255519211292267, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8207979798316956, 'eval_runtime': 10.4372, 'eval_samples_per_second': 95.811, 'eval_steps_per_second': 6.036, 'epoch': 0.72}
{'loss': 0.8829, 'grad_norm': 0.13760077953338623, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8190588355064392, 'eval_runtime': 10.4412, 'eval_samples_per_second': 95.775, 'eval_steps_per_second': 6.034, 'epoch': 0.76}
{'loss': 0.8385, 'grad_norm': 0.12563268840312958, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8171806931495667, 'eval_runtime': 10.428, 'eval_samples_per_second': 95.896, 'eval_steps_per_second': 6.041, 'epoch': 0.8}
{'loss': 0.8544, 'grad_norm': 0.15543730556964874, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8163073658943176, 'eval_runtime': 10.4388, 'eval_samples_per_second': 95.796, 'eval_steps_per_second': 6.035, 'epoch': 0.84}
{'loss': 0.8292, 'grad_norm': 0.16140013933181763, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8154489398002625, 'eval_runtime': 10.4371, 'eval_samples_per_second': 95.812, 'eval_steps_per_second': 6.036, 'epoch': 0.88}
{'loss': 0.8366, 'grad_norm': 0.13471370935440063, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8143855929374695, 'eval_runtime': 10.4717, 'eval_samples_per_second': 95.495, 'eval_steps_per_second': 6.016, 'epoch': 0.92}
{'loss': 0.8351, 'grad_norm': 0.1572461724281311, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8141805529594421, 'eval_runtime': 10.4799, 'eval_samples_per_second': 95.421, 'eval_steps_per_second': 6.012, 'epoch': 0.96}
{'loss': 0.8142, 'grad_norm': 0.12967991828918457, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8135549426078796, 'eval_runtime': 10.4833, 'eval_samples_per_second': 95.39, 'eval_steps_per_second': 6.01, 'epoch': 1.0}
{'train_runtime': 499.9982, 'train_samples_per_second': 19.996, 'train_steps_per_second': 1.25, 'train_loss': 0.9638246124267578, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3427973985671997, 0.9639714360237122, 0.8774651885032654, 0.863734781742096, 0.8581214547157288, 0.8525307178497314, 0.8458017110824585, 0.8444029092788696, 0.839535653591156, 0.834738552570343, 0.833151638507843, 0.8313180804252625, 0.8284292221069336, 0.8287610411643982, 0.8283971548080444, 0.8209736347198486, 0.8205820322036743, 0.8207979798316956, 0.8190588355064392, 0.8171806931495667, 0.8163073658943176, 0.8154489398002625, 0.8143855929374695, 0.8141805529594421, 0.8135549426078796], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3427973985671997, 0.9639714360237122, 0.8774651885032654, 0.863734781742096, 0.8581214547157288, 0.8525307178497314, 0.8458017110824585, 0.8444029092788696, 0.839535653591156, 0.834738552570343, 0.833151638507843, 0.8313180804252625, 0.8284292221069336, 0.8287610411643982, 0.8283971548080444, 0.8209736347198486, 0.8205820322036743, 0.8207979798316956, 0.8190588355064392, 0.8171806931495667, 0.8163073658943176, 0.8154489398002625, 0.8143855929374695, 0.8141805529594421, 0.8135549426078796]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.8135549426078796
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.2876 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.9358804534220364
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -0.9247880877567911
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.9247880877567911
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -0.9247880877567911
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -0.9247880877567911
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0414, dtype=torch.float64), 0, 0, tensor(0.0167, dtype=torch.float64), tensor(0.7843, dtype=torch.float64), 0, tensor(0.1575, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 1, 127, 5.2358595498595044e-20, 30.351870885748518, 0]
normalized proposed parameters for next round by BO: [tensor(3.3168e-19, dtype=torch.float64), tensor(0.0414, dtype=torch.float64), tensor(2.1800e-19, dtype=torch.float64), tensor(1.2492e-17, dtype=torch.float64), tensor(0.0167, dtype=torch.float64), tensor(0.7843, dtype=torch.float64), tensor(9.6046e-18, dtype=torch.float64), tensor(0.1575, dtype=torch.float64), tensor(2.6110e-19, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9884, dtype=torch.float64), tensor(5.2359e-19, dtype=torch.float64), tensor(0.6323, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.041
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.017
  truthfulqa_gen: 0.784
  wikitext: 0
  mmlu: 0.158
  arc_challenge: 0

LoRA Parameters:
  lora_r: (127,)
  lora_dropout: (5.2358595498595044e-20,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (30.351870885748518,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  127
lora dropout:  5.2358595498595044e-20
lora alpha:  30.351870885748518
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 170,622,976 || all params: 8,200,884,224 || trainable%: 2.0805
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.047, 'grad_norm': 1.3353866338729858, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3166838884353638, 'eval_runtime': 11.2305, 'eval_samples_per_second': 89.043, 'eval_steps_per_second': 5.61, 'epoch': 0.04}
{'loss': 1.129, 'grad_norm': 0.44751837849617004, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.067426323890686, 'eval_runtime': 11.2404, 'eval_samples_per_second': 88.965, 'eval_steps_per_second': 5.605, 'epoch': 0.08}
{'loss': 0.8895, 'grad_norm': 0.3156154453754425, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9718250632286072, 'eval_runtime': 11.2266, 'eval_samples_per_second': 89.074, 'eval_steps_per_second': 5.612, 'epoch': 0.12}
{'loss': 0.8598, 'grad_norm': 0.27251261472702026, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9594070911407471, 'eval_runtime': 11.2295, 'eval_samples_per_second': 89.051, 'eval_steps_per_second': 5.61, 'epoch': 0.16}
{'loss': 0.7069, 'grad_norm': 0.25331681966781616, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9353886246681213, 'eval_runtime': 11.2306, 'eval_samples_per_second': 89.043, 'eval_steps_per_second': 5.61, 'epoch': 0.2}
{'loss': 0.7333, 'grad_norm': 0.33669987320899963, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9369573593139648, 'eval_runtime': 11.2328, 'eval_samples_per_second': 89.025, 'eval_steps_per_second': 5.609, 'epoch': 0.24}
{'loss': 0.6474, 'grad_norm': 0.24833638966083527, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.917698085308075, 'eval_runtime': 11.2336, 'eval_samples_per_second': 89.019, 'eval_steps_per_second': 5.608, 'epoch': 0.28}
{'loss': 0.6428, 'grad_norm': 0.2312246412038803, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9185155034065247, 'eval_runtime': 11.2409, 'eval_samples_per_second': 88.961, 'eval_steps_per_second': 5.605, 'epoch': 0.32}
{'loss': 0.6363, 'grad_norm': 0.2557908594608307, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9046430587768555, 'eval_runtime': 11.2376, 'eval_samples_per_second': 88.987, 'eval_steps_per_second': 5.606, 'epoch': 0.36}
{'loss': 0.5748, 'grad_norm': 0.2500690519809723, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.908076286315918, 'eval_runtime': 11.2299, 'eval_samples_per_second': 89.048, 'eval_steps_per_second': 5.61, 'epoch': 0.4}
{'loss': 0.5645, 'grad_norm': 0.2125551700592041, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9061236381530762, 'eval_runtime': 11.2374, 'eval_samples_per_second': 88.989, 'eval_steps_per_second': 5.606, 'epoch': 0.44}
{'loss': 0.5611, 'grad_norm': 0.1545015573501587, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9033862352371216, 'eval_runtime': 11.2395, 'eval_samples_per_second': 88.972, 'eval_steps_per_second': 5.605, 'epoch': 0.48}
{'loss': 0.5404, 'grad_norm': 0.2320871204137802, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8892700672149658, 'eval_runtime': 11.2651, 'eval_samples_per_second': 88.77, 'eval_steps_per_second': 5.592, 'epoch': 0.52}
{'loss': 0.5679, 'grad_norm': 0.194113090634346, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8896378874778748, 'eval_runtime': 11.2386, 'eval_samples_per_second': 88.979, 'eval_steps_per_second': 5.606, 'epoch': 0.56}
{'loss': 0.5705, 'grad_norm': 0.17438961565494537, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8891704082489014, 'eval_runtime': 11.2436, 'eval_samples_per_second': 88.94, 'eval_steps_per_second': 5.603, 'epoch': 0.6}
{'loss': 0.5732, 'grad_norm': 0.20781001448631287, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8939464092254639, 'eval_runtime': 11.2393, 'eval_samples_per_second': 88.973, 'eval_steps_per_second': 5.605, 'epoch': 0.64}
{'loss': 0.5757, 'grad_norm': 0.16183346509933472, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8809884786605835, 'eval_runtime': 11.2364, 'eval_samples_per_second': 88.996, 'eval_steps_per_second': 5.607, 'epoch': 0.68}
{'loss': 0.4753, 'grad_norm': 0.15023064613342285, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8850462436676025, 'eval_runtime': 11.2602, 'eval_samples_per_second': 88.808, 'eval_steps_per_second': 5.595, 'epoch': 0.72}
{'loss': 0.4517, 'grad_norm': 0.15252912044525146, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8781105875968933, 'eval_runtime': 11.2287, 'eval_samples_per_second': 89.058, 'eval_steps_per_second': 5.611, 'epoch': 0.76}
{'loss': 0.5006, 'grad_norm': 0.19143074750900269, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8772872090339661, 'eval_runtime': 11.2405, 'eval_samples_per_second': 88.964, 'eval_steps_per_second': 5.605, 'epoch': 0.8}
{'loss': 0.551, 'grad_norm': 0.1426016092300415, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8781393766403198, 'eval_runtime': 11.2648, 'eval_samples_per_second': 88.772, 'eval_steps_per_second': 5.593, 'epoch': 0.84}
{'loss': 0.5074, 'grad_norm': 0.18728423118591309, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8744237422943115, 'eval_runtime': 11.268, 'eval_samples_per_second': 88.747, 'eval_steps_per_second': 5.591, 'epoch': 0.88}
{'loss': 0.5321, 'grad_norm': 0.20204554498195648, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8724539875984192, 'eval_runtime': 11.2661, 'eval_samples_per_second': 88.762, 'eval_steps_per_second': 5.592, 'epoch': 0.92}
{'loss': 0.4691, 'grad_norm': 0.1281362622976303, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8737220168113708, 'eval_runtime': 11.2366, 'eval_samples_per_second': 88.995, 'eval_steps_per_second': 5.607, 'epoch': 0.96}
{'loss': 0.4825, 'grad_norm': 0.18728508055210114, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.873649537563324, 'eval_runtime': 11.2766, 'eval_samples_per_second': 88.679, 'eval_steps_per_second': 5.587, 'epoch': 1.0}
{'train_runtime': 485.207, 'train_samples_per_second': 20.606, 'train_steps_per_second': 1.288, 'train_loss': 0.7115950439453125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3166838884353638, 1.067426323890686, 0.9718250632286072, 0.9594070911407471, 0.9353886246681213, 0.9369573593139648, 0.917698085308075, 0.9185155034065247, 0.9046430587768555, 0.908076286315918, 0.9061236381530762, 0.9033862352371216, 0.8892700672149658, 0.8896378874778748, 0.8891704082489014, 0.8939464092254639, 0.8809884786605835, 0.8850462436676025, 0.8781105875968933, 0.8772872090339661, 0.8781393766403198, 0.8744237422943115, 0.8724539875984192, 0.8737220168113708, 0.873649537563324], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3166838884353638, 1.067426323890686, 0.9718250632286072, 0.9594070911407471, 0.9353886246681213, 0.9369573593139648, 0.917698085308075, 0.9185155034065247, 0.9046430587768555, 0.908076286315918, 0.9061236381530762, 0.9033862352371216, 0.8892700672149658, 0.8896378874778748, 0.8891704082489014, 0.8939464092254639, 0.8809884786605835, 0.8850462436676025, 0.8781105875968933, 0.8772872090339661, 0.8781393766403198, 0.8744237422943115, 0.8724539875984192, 0.8737220168113708, 0.873649537563324]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0377874374389648
current iteration best possible eval_loss (full train run):  -0.873649537563324
max eval_loss so far:  -0.808354914188385
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.0435 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -0.9459266740348484
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -0.9459265046693748
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -0.9459266740348484
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -0.945789581789747
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.9458446379493342
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.8027, dtype=torch.float64), tensor(0.0149, dtype=torch.float64), 0, tensor(0.0151, dtype=torch.float64), tensor(0.0651, dtype=torch.float64), 0, tensor(0.1023, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 0, 2, 1.0318782230199975e-16, 25.31839397594849, 0]
normalized proposed parameters for next round by BO: [tensor(3.3158e-19, dtype=torch.float64), tensor(0.8027, dtype=torch.float64), tensor(0.0149, dtype=torch.float64), tensor(1.7058e-19, dtype=torch.float64), tensor(0.0151, dtype=torch.float64), tensor(0.0651, dtype=torch.float64), tensor(1.0975e-17, dtype=torch.float64), tensor(0.1023, dtype=torch.float64), tensor(1.2505e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.0319e-15, dtype=torch.float64), tensor(0.5275, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.803
  rowan_hellaswag: 0.015
  sciq: 0
  triviaqa: 0.015
  truthfulqa_gen: 0.065
  wikitext: 0
  mmlu: 0.102
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.0318782230199975e-16,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (25.31839397594849,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  1.0318782230199975e-16
lora alpha:  25.31839397594849
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,507,328 || all params: 8,031,768,576 || trainable%: 0.0188
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.1537, 'grad_norm': 3.521315336227417, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.183377981185913, 'eval_runtime': 9.8374, 'eval_samples_per_second': 101.653, 'eval_steps_per_second': 6.404, 'epoch': 0.04}
{'loss': 1.0664, 'grad_norm': 2.135352373123169, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.8868447542190552, 'eval_runtime': 9.8753, 'eval_samples_per_second': 101.263, 'eval_steps_per_second': 6.38, 'epoch': 0.08}
{'loss': 0.9538, 'grad_norm': 0.892536997795105, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8602231740951538, 'eval_runtime': 9.9129, 'eval_samples_per_second': 100.879, 'eval_steps_per_second': 6.355, 'epoch': 0.12}
{'loss': 0.9055, 'grad_norm': 1.4312268495559692, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8474622964859009, 'eval_runtime': 9.8993, 'eval_samples_per_second': 101.018, 'eval_steps_per_second': 6.364, 'epoch': 0.16}
{'loss': 0.917, 'grad_norm': 1.0387537479400635, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8415150046348572, 'eval_runtime': 9.8685, 'eval_samples_per_second': 101.332, 'eval_steps_per_second': 6.384, 'epoch': 0.2}
{'loss': 0.8655, 'grad_norm': 1.3100894689559937, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8322732448577881, 'eval_runtime': 9.8818, 'eval_samples_per_second': 101.196, 'eval_steps_per_second': 6.375, 'epoch': 0.24}
{'loss': 0.8982, 'grad_norm': 0.7570924162864685, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.827305793762207, 'eval_runtime': 9.8837, 'eval_samples_per_second': 101.177, 'eval_steps_per_second': 6.374, 'epoch': 0.28}
{'loss': 0.9055, 'grad_norm': 0.9058602452278137, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8254531025886536, 'eval_runtime': 9.8945, 'eval_samples_per_second': 101.066, 'eval_steps_per_second': 6.367, 'epoch': 0.32}
{'loss': 0.8746, 'grad_norm': 0.8194965124130249, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8215054869651794, 'eval_runtime': 9.9008, 'eval_samples_per_second': 101.002, 'eval_steps_per_second': 6.363, 'epoch': 0.36}
{'loss': 0.8525, 'grad_norm': 1.10977303981781, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8180651664733887, 'eval_runtime': 9.8957, 'eval_samples_per_second': 101.054, 'eval_steps_per_second': 6.366, 'epoch': 0.4}
{'loss': 0.8556, 'grad_norm': 0.8216707110404968, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8151739239692688, 'eval_runtime': 9.8955, 'eval_samples_per_second': 101.056, 'eval_steps_per_second': 6.367, 'epoch': 0.44}
{'loss': 0.8957, 'grad_norm': 0.8329374194145203, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8159750699996948, 'eval_runtime': 9.8994, 'eval_samples_per_second': 101.016, 'eval_steps_per_second': 6.364, 'epoch': 0.48}
{'loss': 0.888, 'grad_norm': 0.9087982773780823, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8133819699287415, 'eval_runtime': 9.8964, 'eval_samples_per_second': 101.047, 'eval_steps_per_second': 6.366, 'epoch': 0.52}
{'loss': 0.8498, 'grad_norm': 0.8020092844963074, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8115435838699341, 'eval_runtime': 9.9616, 'eval_samples_per_second': 100.385, 'eval_steps_per_second': 6.324, 'epoch': 0.56}
{'loss': 0.8538, 'grad_norm': 0.8419099450111389, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.809907078742981, 'eval_runtime': 9.9896, 'eval_samples_per_second': 100.104, 'eval_steps_per_second': 6.307, 'epoch': 0.6}
{'loss': 0.8634, 'grad_norm': 0.849118173122406, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8087315559387207, 'eval_runtime': 9.9739, 'eval_samples_per_second': 100.261, 'eval_steps_per_second': 6.316, 'epoch': 0.64}
{'loss': 0.8549, 'grad_norm': 1.0203334093093872, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8073378801345825, 'eval_runtime': 9.9886, 'eval_samples_per_second': 100.114, 'eval_steps_per_second': 6.307, 'epoch': 0.68}
{'loss': 0.8396, 'grad_norm': 0.8713899850845337, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8062856197357178, 'eval_runtime': 9.9751, 'eval_samples_per_second': 100.25, 'eval_steps_per_second': 6.316, 'epoch': 0.72}
{'loss': 0.8382, 'grad_norm': 1.036668062210083, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8057454228401184, 'eval_runtime': 9.9565, 'eval_samples_per_second': 100.437, 'eval_steps_per_second': 6.328, 'epoch': 0.76}
{'loss': 0.8353, 'grad_norm': 1.0002521276474, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8045495748519897, 'eval_runtime': 9.978, 'eval_samples_per_second': 100.22, 'eval_steps_per_second': 6.314, 'epoch': 0.8}
{'loss': 0.8442, 'grad_norm': 0.7852038741111755, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8029533624649048, 'eval_runtime': 9.9489, 'eval_samples_per_second': 100.514, 'eval_steps_per_second': 6.332, 'epoch': 0.84}
{'loss': 0.8389, 'grad_norm': 0.8457656502723694, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8027227520942688, 'eval_runtime': 9.9062, 'eval_samples_per_second': 100.946, 'eval_steps_per_second': 6.36, 'epoch': 0.88}
{'loss': 0.8264, 'grad_norm': 0.9161605834960938, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8013024926185608, 'eval_runtime': 9.973, 'eval_samples_per_second': 100.27, 'eval_steps_per_second': 6.317, 'epoch': 0.92}
{'loss': 0.8353, 'grad_norm': 0.8050327301025391, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8011947274208069, 'eval_runtime': 9.9772, 'eval_samples_per_second': 100.229, 'eval_steps_per_second': 6.314, 'epoch': 0.96}
{'loss': 0.8217, 'grad_norm': 1.332194447517395, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.800920307636261, 'eval_runtime': 9.9807, 'eval_samples_per_second': 100.194, 'eval_steps_per_second': 6.312, 'epoch': 1.0}
{'train_runtime': 474.7105, 'train_samples_per_second': 21.057, 'train_steps_per_second': 1.317, 'train_loss': 0.9253390197753906, 'epoch': 1.0}
train_results:  {'eval_loss': [1.183377981185913, 0.8868447542190552, 0.8602231740951538, 0.8474622964859009, 0.8415150046348572, 0.8322732448577881, 0.827305793762207, 0.8254531025886536, 0.8215054869651794, 0.8180651664733887, 0.8151739239692688, 0.8159750699996948, 0.8133819699287415, 0.8115435838699341, 0.809907078742981, 0.8087315559387207, 0.8073378801345825, 0.8062856197357178, 0.8057454228401184, 0.8045495748519897, 0.8029533624649048, 0.8027227520942688, 0.8013024926185608, 0.8011947274208069, 0.800920307636261], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.183377981185913, 0.8868447542190552, 0.8602231740951538, 0.8474622964859009, 0.8415150046348572, 0.8322732448577881, 0.827305793762207, 0.8254531025886536, 0.8215054869651794, 0.8180651664733887, 0.8151739239692688, 0.8159750699996948, 0.8133819699287415, 0.8115435838699341, 0.809907078742981, 0.8087315559387207, 0.8073378801345825, 0.8062856197357178, 0.8057454228401184, 0.8045495748519897, 0.8029533624649048, 0.8027227520942688, 0.8013024926185608, 0.8011947274208069, 0.800920307636261]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.5357768535614014
current iteration best possible eval_loss (full train run):  -0.800920307636261
max eval_loss so far:  -0.800920307636261
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648, -1.5357768535614014]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.4086 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -0.9198815966403391
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -0.9198815966403391
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -0.9198815966403391
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -0.9198815966403391
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -0.9200314638154468
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2194, dtype=torch.float64), tensor(0.1116, dtype=torch.float64), 0, tensor(0.0412, dtype=torch.float64), tensor(0.0971, dtype=torch.float64), tensor(0.4398, dtype=torch.float64), 0, 0, tensor(0.0908, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 106, 0.09999999999999998, 34.15097035806102, 0]
normalized proposed parameters for next round by BO: [tensor(0.2194, dtype=torch.float64), tensor(0.1116, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0412, dtype=torch.float64), tensor(0.0971, dtype=torch.float64), tensor(0.4398, dtype=torch.float64), tensor(1.7645e-17, dtype=torch.float64), tensor(9.1879e-18, dtype=torch.float64), tensor(0.0908, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8296, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.7115, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.219
  gsm8k: 0.112
  rowan_hellaswag: 0
  sciq: 0.041
  triviaqa: 0.097
  truthfulqa_gen: 0.44
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.091

LoRA Parameters:
  lora_r: (106,)
  lora_dropout: (0.09999999999999998,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (34.15097035806102,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  106
lora dropout:  0.09999999999999998
lora alpha:  34.15097035806102
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 142,409,728 || all params: 8,172,670,976 || trainable%: 1.7425
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9151, 'grad_norm': 0.6090778708457947, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2523568868637085, 'eval_runtime': 10.6571, 'eval_samples_per_second': 93.834, 'eval_steps_per_second': 5.912, 'epoch': 0.04}
{'loss': 1.0749, 'grad_norm': 0.4958948493003845, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9454084038734436, 'eval_runtime': 10.6764, 'eval_samples_per_second': 93.665, 'eval_steps_per_second': 5.901, 'epoch': 0.08}
{'loss': 0.8814, 'grad_norm': 0.262214332818985, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9083335399627686, 'eval_runtime': 10.7008, 'eval_samples_per_second': 93.451, 'eval_steps_per_second': 5.887, 'epoch': 0.12}
{'loss': 0.8399, 'grad_norm': 0.22908036410808563, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8919873833656311, 'eval_runtime': 10.7373, 'eval_samples_per_second': 93.133, 'eval_steps_per_second': 5.867, 'epoch': 0.16}
{'loss': 0.835, 'grad_norm': 0.22925499081611633, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8901891112327576, 'eval_runtime': 10.7405, 'eval_samples_per_second': 93.105, 'eval_steps_per_second': 5.866, 'epoch': 0.2}
{'loss': 0.8248, 'grad_norm': 0.21939228475093842, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8792206645011902, 'eval_runtime': 10.7648, 'eval_samples_per_second': 92.895, 'eval_steps_per_second': 5.852, 'epoch': 0.24}
{'loss': 0.7837, 'grad_norm': 0.20270924270153046, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8786001205444336, 'eval_runtime': 10.7599, 'eval_samples_per_second': 92.937, 'eval_steps_per_second': 5.855, 'epoch': 0.28}
{'loss': 0.7884, 'grad_norm': 0.23984424769878387, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8698058128356934, 'eval_runtime': 10.7737, 'eval_samples_per_second': 92.819, 'eval_steps_per_second': 5.848, 'epoch': 0.32}
{'loss': 0.7251, 'grad_norm': 0.23886853456497192, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8727172613143921, 'eval_runtime': 10.7937, 'eval_samples_per_second': 92.647, 'eval_steps_per_second': 5.837, 'epoch': 0.36}
{'loss': 0.7343, 'grad_norm': 0.25838547945022583, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8708087801933289, 'eval_runtime': 10.7943, 'eval_samples_per_second': 92.642, 'eval_steps_per_second': 5.836, 'epoch': 0.4}
{'loss': 0.7218, 'grad_norm': 0.21903161704540253, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8655127286911011, 'eval_runtime': 10.8174, 'eval_samples_per_second': 92.443, 'eval_steps_per_second': 5.824, 'epoch': 0.44}
{'loss': 0.6858, 'grad_norm': 0.22887295484542847, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.863516628742218, 'eval_runtime': 10.7891, 'eval_samples_per_second': 92.686, 'eval_steps_per_second': 5.839, 'epoch': 0.48}
{'loss': 0.7, 'grad_norm': 0.2748975455760956, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.859312891960144, 'eval_runtime': 10.7802, 'eval_samples_per_second': 92.763, 'eval_steps_per_second': 5.844, 'epoch': 0.52}
{'loss': 0.7013, 'grad_norm': 0.23268446326255798, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8550686240196228, 'eval_runtime': 10.8107, 'eval_samples_per_second': 92.501, 'eval_steps_per_second': 5.828, 'epoch': 0.56}
{'loss': 0.6792, 'grad_norm': 0.25557971000671387, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8517467975616455, 'eval_runtime': 10.773, 'eval_samples_per_second': 92.825, 'eval_steps_per_second': 5.848, 'epoch': 0.6}
{'loss': 0.6898, 'grad_norm': 0.24731843173503876, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8524660468101501, 'eval_runtime': 10.7557, 'eval_samples_per_second': 92.974, 'eval_steps_per_second': 5.857, 'epoch': 0.64}
{'loss': 0.6812, 'grad_norm': 0.20877863466739655, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.850501537322998, 'eval_runtime': 10.6973, 'eval_samples_per_second': 93.481, 'eval_steps_per_second': 5.889, 'epoch': 0.68}
{'loss': 0.6726, 'grad_norm': 0.21717606484889984, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8476601839065552, 'eval_runtime': 10.7008, 'eval_samples_per_second': 93.451, 'eval_steps_per_second': 5.887, 'epoch': 0.72}
{'loss': 0.6801, 'grad_norm': 0.23845353722572327, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8429440855979919, 'eval_runtime': 10.6941, 'eval_samples_per_second': 93.509, 'eval_steps_per_second': 5.891, 'epoch': 0.76}
{'loss': 0.6168, 'grad_norm': 0.21657274663448334, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8462250828742981, 'eval_runtime': 10.687, 'eval_samples_per_second': 93.572, 'eval_steps_per_second': 5.895, 'epoch': 0.8}
{'loss': 0.6468, 'grad_norm': 0.21113817393779755, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.843388557434082, 'eval_runtime': 10.7235, 'eval_samples_per_second': 93.253, 'eval_steps_per_second': 5.875, 'epoch': 0.84}
{'loss': 0.6292, 'grad_norm': 0.19393396377563477, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8462321758270264, 'eval_runtime': 10.7667, 'eval_samples_per_second': 92.879, 'eval_steps_per_second': 5.851, 'epoch': 0.88}
{'loss': 0.6786, 'grad_norm': 0.21659131348133087, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8410660624504089, 'eval_runtime': 10.7811, 'eval_samples_per_second': 92.755, 'eval_steps_per_second': 5.844, 'epoch': 0.92}
{'loss': 0.6498, 'grad_norm': 0.19686679542064667, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8415937423706055, 'eval_runtime': 10.7608, 'eval_samples_per_second': 92.93, 'eval_steps_per_second': 5.855, 'epoch': 0.96}
{'loss': 0.6439, 'grad_norm': 0.22590439021587372, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8415119647979736, 'eval_runtime': 10.7595, 'eval_samples_per_second': 92.941, 'eval_steps_per_second': 5.855, 'epoch': 1.0}
{'train_runtime': 471.1604, 'train_samples_per_second': 21.218, 'train_steps_per_second': 1.327, 'train_loss': 0.8191764282226562, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2523568868637085, 0.9454084038734436, 0.9083335399627686, 0.8919873833656311, 0.8901891112327576, 0.8792206645011902, 0.8786001205444336, 0.8698058128356934, 0.8727172613143921, 0.8708087801933289, 0.8655127286911011, 0.863516628742218, 0.859312891960144, 0.8550686240196228, 0.8517467975616455, 0.8524660468101501, 0.850501537322998, 0.8476601839065552, 0.8429440855979919, 0.8462250828742981, 0.843388557434082, 0.8462321758270264, 0.8410660624504089, 0.8415937423706055, 0.8415119647979736], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2523568868637085, 0.9454084038734436, 0.9083335399627686, 0.8919873833656311, 0.8901891112327576, 0.8792206645011902, 0.8786001205444336, 0.8698058128356934, 0.8727172613143921, 0.8708087801933289, 0.8655127286911011, 0.863516628742218, 0.859312891960144, 0.8550686240196228, 0.8517467975616455, 0.8524660468101501, 0.850501537322998, 0.8476601839065552, 0.8429440855979919, 0.8462250828742981, 0.843388557434082, 0.8462321758270264, 0.8410660624504089, 0.8415937423706055, 0.8415119647979736]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0416327714920044
current iteration best possible eval_loss (full train run):  -0.8415119647979736
max eval_loss so far:  -0.800920307636261
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648, -1.5357768535614014, -1.0416327714920044]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 18.0123 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -0.8788297047422176
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -0.9817602116323241
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -0.9823100839597043
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -0.9817034464307284
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -0.9818167825984971
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1092, dtype=torch.float64), tensor(0.3090, dtype=torch.float64), tensor(0.0502, dtype=torch.float64), tensor(0.1394, dtype=torch.float64), tensor(0.0794, dtype=torch.float64), tensor(0.1026, dtype=torch.float64), 0, tensor(0.0822, dtype=torch.float64), tensor(0.1279, dtype=torch.float64), 28, 0, 1, 0, 1, 1, 63, 0.04529822474538283, 26.06585062232876, 0]
normalized proposed parameters for next round by BO: [tensor(0.1092, dtype=torch.float64), tensor(0.3090, dtype=torch.float64), tensor(0.0502, dtype=torch.float64), tensor(0.1394, dtype=torch.float64), tensor(0.0794, dtype=torch.float64), tensor(0.1026, dtype=torch.float64), tensor(2.1690e-18, dtype=torch.float64), tensor(0.0822, dtype=torch.float64), tensor(0.1279, dtype=torch.float64), tensor(0.8904, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4950, dtype=torch.float64), tensor(0.4530, dtype=torch.float64), tensor(0.5430, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.109
  gsm8k: 0.309
  rowan_hellaswag: 0.05
  sciq: 0.139
  triviaqa: 0.079
  truthfulqa_gen: 0.103
  wikitext: 0
  mmlu: 0.082
  arc_challenge: 0.128

LoRA Parameters:
  lora_r: (63,)
  lora_dropout: (0.04529822474538283,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (26.06585062232876,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  63
lora dropout:  0.04529822474538283
lora alpha:  26.06585062232876
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 74,059,776 || all params: 8,104,321,024 || trainable%: 0.9138
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8054, 'grad_norm': 1.0522112846374512, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3359309434890747, 'eval_runtime': 10.6572, 'eval_samples_per_second': 93.834, 'eval_steps_per_second': 5.912, 'epoch': 0.04}
{'loss': 1.239, 'grad_norm': 0.3581852614879608, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9158031940460205, 'eval_runtime': 10.6814, 'eval_samples_per_second': 93.621, 'eval_steps_per_second': 5.898, 'epoch': 0.08}
{'loss': 1.0927, 'grad_norm': 0.24602051079273224, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8929314017295837, 'eval_runtime': 10.7414, 'eval_samples_per_second': 93.098, 'eval_steps_per_second': 5.865, 'epoch': 0.12}
{'loss': 1.0532, 'grad_norm': 0.2554001808166504, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8755525350570679, 'eval_runtime': 10.7472, 'eval_samples_per_second': 93.047, 'eval_steps_per_second': 5.862, 'epoch': 0.16}
{'loss': 1.0092, 'grad_norm': 0.188237726688385, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8666611313819885, 'eval_runtime': 10.7667, 'eval_samples_per_second': 92.879, 'eval_steps_per_second': 5.851, 'epoch': 0.2}
{'loss': 0.9833, 'grad_norm': 0.24003390967845917, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8628950119018555, 'eval_runtime': 10.7636, 'eval_samples_per_second': 92.906, 'eval_steps_per_second': 5.853, 'epoch': 0.24}
{'loss': 0.9909, 'grad_norm': 0.22188489139080048, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8590279817581177, 'eval_runtime': 10.7574, 'eval_samples_per_second': 92.959, 'eval_steps_per_second': 5.856, 'epoch': 0.28}
{'loss': 0.9964, 'grad_norm': 0.2060748040676117, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8493205308914185, 'eval_runtime': 10.7671, 'eval_samples_per_second': 92.876, 'eval_steps_per_second': 5.851, 'epoch': 0.32}
{'loss': 0.9781, 'grad_norm': 0.2063857465982437, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8461783528327942, 'eval_runtime': 10.8048, 'eval_samples_per_second': 92.551, 'eval_steps_per_second': 5.831, 'epoch': 0.36}
{'loss': 0.9722, 'grad_norm': 0.21312803030014038, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8475289344787598, 'eval_runtime': 10.7769, 'eval_samples_per_second': 92.791, 'eval_steps_per_second': 5.846, 'epoch': 0.4}
{'loss': 0.9688, 'grad_norm': 0.24448029696941376, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8412199020385742, 'eval_runtime': 10.765, 'eval_samples_per_second': 92.894, 'eval_steps_per_second': 5.852, 'epoch': 0.44}
{'loss': 0.9782, 'grad_norm': 0.20084123313426971, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.843816876411438, 'eval_runtime': 10.7656, 'eval_samples_per_second': 92.888, 'eval_steps_per_second': 5.852, 'epoch': 0.48}
{'loss': 0.9319, 'grad_norm': 0.22169438004493713, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8369582295417786, 'eval_runtime': 10.7586, 'eval_samples_per_second': 92.948, 'eval_steps_per_second': 5.856, 'epoch': 0.52}
{'loss': 0.9556, 'grad_norm': 0.24366699159145355, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8357528448104858, 'eval_runtime': 10.7659, 'eval_samples_per_second': 92.886, 'eval_steps_per_second': 5.852, 'epoch': 0.56}
{'loss': 0.9672, 'grad_norm': 0.23017853498458862, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8414872884750366, 'eval_runtime': 10.7636, 'eval_samples_per_second': 92.905, 'eval_steps_per_second': 5.853, 'epoch': 0.6}
{'loss': 0.9263, 'grad_norm': 0.2202654629945755, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8334412574768066, 'eval_runtime': 10.7991, 'eval_samples_per_second': 92.6, 'eval_steps_per_second': 5.834, 'epoch': 0.64}
{'loss': 0.9627, 'grad_norm': 0.20989908277988434, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8304360508918762, 'eval_runtime': 10.772, 'eval_samples_per_second': 92.834, 'eval_steps_per_second': 5.849, 'epoch': 0.68}
{'loss': 0.9185, 'grad_norm': 0.21642059087753296, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8263447284698486, 'eval_runtime': 10.784, 'eval_samples_per_second': 92.73, 'eval_steps_per_second': 5.842, 'epoch': 0.72}
{'loss': 0.96, 'grad_norm': 0.20434992015361786, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8280434012413025, 'eval_runtime': 10.7733, 'eval_samples_per_second': 92.822, 'eval_steps_per_second': 5.848, 'epoch': 0.76}
{'loss': 0.9273, 'grad_norm': 0.25609782338142395, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8251922726631165, 'eval_runtime': 10.7545, 'eval_samples_per_second': 92.984, 'eval_steps_per_second': 5.858, 'epoch': 0.8}
{'loss': 0.951, 'grad_norm': 0.2014743983745575, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8246465921401978, 'eval_runtime': 10.7023, 'eval_samples_per_second': 93.438, 'eval_steps_per_second': 5.887, 'epoch': 0.84}
{'loss': 0.9965, 'grad_norm': 0.17381328344345093, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8227378129959106, 'eval_runtime': 10.7052, 'eval_samples_per_second': 93.412, 'eval_steps_per_second': 5.885, 'epoch': 0.88}
{'loss': 0.9009, 'grad_norm': 0.35511985421180725, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8218426704406738, 'eval_runtime': 10.703, 'eval_samples_per_second': 93.431, 'eval_steps_per_second': 5.886, 'epoch': 0.92}
{'loss': 0.9419, 'grad_norm': 0.21886810660362244, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.820982813835144, 'eval_runtime': 10.6828, 'eval_samples_per_second': 93.609, 'eval_steps_per_second': 5.897, 'epoch': 0.96}
{'loss': 0.9726, 'grad_norm': 0.24758537113666534, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8205469846725464, 'eval_runtime': 10.6892, 'eval_samples_per_second': 93.552, 'eval_steps_per_second': 5.894, 'epoch': 1.0}
{'train_runtime': 502.7551, 'train_samples_per_second': 19.882, 'train_steps_per_second': 1.243, 'train_loss': 1.0551886413574219, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3359309434890747, 0.9158031940460205, 0.8929314017295837, 0.8755525350570679, 0.8666611313819885, 0.8628950119018555, 0.8590279817581177, 0.8493205308914185, 0.8461783528327942, 0.8475289344787598, 0.8412199020385742, 0.843816876411438, 0.8369582295417786, 0.8357528448104858, 0.8414872884750366, 0.8334412574768066, 0.8304360508918762, 0.8263447284698486, 0.8280434012413025, 0.8251922726631165, 0.8246465921401978, 0.8227378129959106, 0.8218426704406738, 0.820982813835144, 0.8205469846725464], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3359309434890747, 0.9158031940460205, 0.8929314017295837, 0.8755525350570679, 0.8666611313819885, 0.8628950119018555, 0.8590279817581177, 0.8493205308914185, 0.8461783528327942, 0.8475289344787598, 0.8412199020385742, 0.843816876411438, 0.8369582295417786, 0.8357528448104858, 0.8414872884750366, 0.8334412574768066, 0.8304360508918762, 0.8263447284698486, 0.8280434012413025, 0.8251922726631165, 0.8246465921401978, 0.8227378129959106, 0.8218426704406738, 0.820982813835144, 0.8205469846725464]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0556604862213135
current iteration best possible eval_loss (full train run):  -0.8205469846725464
max eval_loss so far:  -0.800920307636261
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648, -1.5357768535614014, -1.0416327714920044, -1.0556604862213135]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.5481 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -0.9965640079562997
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -0.9965632244800315
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -0.9965632224536385
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -0.9967499251452054
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -0.9965632224533554
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0821, dtype=torch.float64), 0, tensor(0.3594, dtype=torch.float64), tensor(0.0310, dtype=torch.float64), tensor(0.0158, dtype=torch.float64), tensor(0.0368, dtype=torch.float64), tensor(0.1796, dtype=torch.float64), tensor(0.2952, dtype=torch.float64), 24, 0, 0, 0, 1, 0, 59, 0.08249818523828001, 7.616486875926668, 1]
normalized proposed parameters for next round by BO: [tensor(7.5495e-19, dtype=torch.float64), tensor(0.0821, dtype=torch.float64), tensor(3.3772e-17, dtype=torch.float64), tensor(0.3594, dtype=torch.float64), tensor(0.0310, dtype=torch.float64), tensor(0.0158, dtype=torch.float64), tensor(0.0368, dtype=torch.float64), tensor(0.1796, dtype=torch.float64), tensor(0.2952, dtype=torch.float64), tensor(0.7349, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4596, dtype=torch.float64), tensor(0.8250, dtype=torch.float64), tensor(0.1587, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.082
  rowan_hellaswag: 0
  sciq: 0.359
  triviaqa: 0.031
  truthfulqa_gen: 0.016
  wikitext: 0.037
  mmlu: 0.18
  arc_challenge: 0.295

LoRA Parameters:
  lora_r: (59,)
  lora_dropout: (0.08249818523828001,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (7.616486875926668,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  59
lora dropout:  0.08249818523828001
lora alpha:  7.616486875926668
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 26,099,712 || all params: 8,056,360,960 || trainable%: 0.3240
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6408, 'grad_norm': 1.3853408098220825, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7410788536071777, 'eval_runtime': 9.5809, 'eval_samples_per_second': 104.375, 'eval_steps_per_second': 6.576, 'epoch': 0.04}
{'loss': 1.7672, 'grad_norm': 0.4942632019519806, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2731655836105347, 'eval_runtime': 9.6699, 'eval_samples_per_second': 103.413, 'eval_steps_per_second': 6.515, 'epoch': 0.08}
{'loss': 1.3177, 'grad_norm': 0.5218014717102051, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1406601667404175, 'eval_runtime': 9.6383, 'eval_samples_per_second': 103.753, 'eval_steps_per_second': 6.536, 'epoch': 0.12}
{'loss': 1.2385, 'grad_norm': 0.20184150338172913, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0301456451416016, 'eval_runtime': 9.6713, 'eval_samples_per_second': 103.399, 'eval_steps_per_second': 6.514, 'epoch': 0.16}
{'loss': 1.1663, 'grad_norm': 0.2126300185918808, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9954323768615723, 'eval_runtime': 9.6837, 'eval_samples_per_second': 103.266, 'eval_steps_per_second': 6.506, 'epoch': 0.2}
{'loss': 1.0781, 'grad_norm': 0.21525128185749054, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9664284586906433, 'eval_runtime': 9.6526, 'eval_samples_per_second': 103.599, 'eval_steps_per_second': 6.527, 'epoch': 0.24}
{'loss': 1.0948, 'grad_norm': 0.2116609513759613, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.952339231967926, 'eval_runtime': 9.6882, 'eval_samples_per_second': 103.218, 'eval_steps_per_second': 6.503, 'epoch': 0.28}
{'loss': 1.0885, 'grad_norm': 0.2655733823776245, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9504323601722717, 'eval_runtime': 9.6812, 'eval_samples_per_second': 103.293, 'eval_steps_per_second': 6.507, 'epoch': 0.32}
{'loss': 1.0205, 'grad_norm': 0.23691998422145844, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9380771517753601, 'eval_runtime': 9.6705, 'eval_samples_per_second': 103.408, 'eval_steps_per_second': 6.515, 'epoch': 0.36}
{'loss': 1.0329, 'grad_norm': 0.17747953534126282, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9197789430618286, 'eval_runtime': 9.6845, 'eval_samples_per_second': 103.258, 'eval_steps_per_second': 6.505, 'epoch': 0.4}
{'loss': 1.0402, 'grad_norm': 0.26096928119659424, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9176146984100342, 'eval_runtime': 9.6853, 'eval_samples_per_second': 103.249, 'eval_steps_per_second': 6.505, 'epoch': 0.44}
{'loss': 1.0103, 'grad_norm': 0.18563002347946167, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9141206741333008, 'eval_runtime': 9.6811, 'eval_samples_per_second': 103.294, 'eval_steps_per_second': 6.508, 'epoch': 0.48}
{'loss': 1.0066, 'grad_norm': 0.21182630956172943, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9018906354904175, 'eval_runtime': 9.6424, 'eval_samples_per_second': 103.708, 'eval_steps_per_second': 6.534, 'epoch': 0.52}
{'loss': 0.9291, 'grad_norm': 0.1917705237865448, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9020848274230957, 'eval_runtime': 9.7075, 'eval_samples_per_second': 103.013, 'eval_steps_per_second': 6.49, 'epoch': 0.56}
{'loss': 0.9675, 'grad_norm': 0.1930534392595291, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8984705209732056, 'eval_runtime': 9.7491, 'eval_samples_per_second': 102.574, 'eval_steps_per_second': 6.462, 'epoch': 0.6}
{'loss': 0.9398, 'grad_norm': 0.17779779434204102, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8934897184371948, 'eval_runtime': 9.7746, 'eval_samples_per_second': 102.306, 'eval_steps_per_second': 6.445, 'epoch': 0.64}
{'loss': 0.9793, 'grad_norm': 0.1831052303314209, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.899264395236969, 'eval_runtime': 9.7962, 'eval_samples_per_second': 102.08, 'eval_steps_per_second': 6.431, 'epoch': 0.68}
{'loss': 1.0084, 'grad_norm': 0.17590947449207306, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8965975046157837, 'eval_runtime': 9.6893, 'eval_samples_per_second': 103.207, 'eval_steps_per_second': 6.502, 'epoch': 0.72}
{'loss': 1.0203, 'grad_norm': 0.2041664570569992, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8938742280006409, 'eval_runtime': 9.6789, 'eval_samples_per_second': 103.317, 'eval_steps_per_second': 6.509, 'epoch': 0.76}
{'loss': 1.0315, 'grad_norm': 0.18900066614151, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8909343481063843, 'eval_runtime': 9.6612, 'eval_samples_per_second': 103.507, 'eval_steps_per_second': 6.521, 'epoch': 0.8}
{'loss': 0.9811, 'grad_norm': 0.17928798496723175, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.892063558101654, 'eval_runtime': 9.6089, 'eval_samples_per_second': 104.07, 'eval_steps_per_second': 6.556, 'epoch': 0.84}
{'loss': 0.9326, 'grad_norm': 0.17234750092029572, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8881564140319824, 'eval_runtime': 9.6118, 'eval_samples_per_second': 104.039, 'eval_steps_per_second': 6.554, 'epoch': 0.88}
{'loss': 0.9409, 'grad_norm': 0.1854153573513031, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8879284858703613, 'eval_runtime': 9.616, 'eval_samples_per_second': 103.993, 'eval_steps_per_second': 6.552, 'epoch': 0.92}
{'loss': 0.9928, 'grad_norm': 0.22686974704265594, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8890498876571655, 'eval_runtime': 9.6201, 'eval_samples_per_second': 103.949, 'eval_steps_per_second': 6.549, 'epoch': 0.96}
{'loss': 0.9612, 'grad_norm': 0.1969647854566574, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8885555267333984, 'eval_runtime': 9.6195, 'eval_samples_per_second': 103.955, 'eval_steps_per_second': 6.549, 'epoch': 1.0}
{'train_runtime': 431.6198, 'train_samples_per_second': 23.162, 'train_steps_per_second': 1.448, 'train_loss': 1.167468115234375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7410788536071777, 1.2731655836105347, 1.1406601667404175, 1.0301456451416016, 0.9954323768615723, 0.9664284586906433, 0.952339231967926, 0.9504323601722717, 0.9380771517753601, 0.9197789430618286, 0.9176146984100342, 0.9141206741333008, 0.9018906354904175, 0.9020848274230957, 0.8984705209732056, 0.8934897184371948, 0.899264395236969, 0.8965975046157837, 0.8938742280006409, 0.8909343481063843, 0.892063558101654, 0.8881564140319824, 0.8879284858703613, 0.8890498876571655, 0.8885555267333984], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7410788536071777, 1.2731655836105347, 1.1406601667404175, 1.0301456451416016, 0.9954323768615723, 0.9664284586906433, 0.952339231967926, 0.9504323601722717, 0.9380771517753601, 0.9197789430618286, 0.9176146984100342, 0.9141206741333008, 0.9018906354904175, 0.9020848274230957, 0.8984705209732056, 0.8934897184371948, 0.899264395236969, 0.8965975046157837, 0.8938742280006409, 0.8909343481063843, 0.892063558101654, 0.8881564140319824, 0.8879284858703613, 0.8890498876571655, 0.8885555267333984]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.027631163597107
current iteration best possible eval_loss (full train run):  -0.8885555267333984
max eval_loss so far:  -0.800920307636261
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648, -1.5357768535614014, -1.0416327714920044, -1.0556604862213135, -1.027631163597107]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.7851 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -0.9553276721251198
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -0.9553276721251198
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -0.955327672122644
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -0.9553276721251198
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -0.9553276721251198
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3046, dtype=torch.float64), tensor(0.0488, dtype=torch.float64), tensor(0.5952, dtype=torch.float64), 0, tensor(0.0514, dtype=torch.float64), 0, 14, 0, 1, 0, 1, 1, 128, 0.1, 31.914363267421024, 0]
normalized proposed parameters for next round by BO: [tensor(4.4161e-20, dtype=torch.float64), tensor(1.0686e-21, dtype=torch.float64), tensor(1.3509e-17, dtype=torch.float64), tensor(0.3046, dtype=torch.float64), tensor(0.0488, dtype=torch.float64), tensor(0.5952, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0514, dtype=torch.float64), tensor(5.2125e-18, dtype=torch.float64), tensor(0.4460, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6649, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.305
  triviaqa: 0.049
  truthfulqa_gen: 0.595
  wikitext: 0
  mmlu: 0.051
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (31.914363267421024,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  31.914363267421024
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,235,328 || all params: 8,105,496,576 || trainable%: 0.9282
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.9513, 'grad_norm': 1.3221485614776611, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6433573961257935, 'eval_runtime': 9.5459, 'eval_samples_per_second': 104.757, 'eval_steps_per_second': 6.6, 'epoch': 0.04}
{'loss': 1.2783, 'grad_norm': 0.37360355257987976, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4733262062072754, 'eval_runtime': 9.6062, 'eval_samples_per_second': 104.1, 'eval_steps_per_second': 6.558, 'epoch': 0.08}
{'loss': 0.9172, 'grad_norm': 0.2127387672662735, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4622992277145386, 'eval_runtime': 9.6348, 'eval_samples_per_second': 103.79, 'eval_steps_per_second': 6.539, 'epoch': 0.12}
{'loss': 0.9015, 'grad_norm': 0.21042241156101227, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4608930349349976, 'eval_runtime': 9.6107, 'eval_samples_per_second': 104.05, 'eval_steps_per_second': 6.555, 'epoch': 0.16}
{'loss': 0.8568, 'grad_norm': 0.2499280869960785, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4803413152694702, 'eval_runtime': 9.6317, 'eval_samples_per_second': 103.824, 'eval_steps_per_second': 6.541, 'epoch': 0.2}
{'loss': 0.8338, 'grad_norm': 0.2586725056171417, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.503559947013855, 'eval_runtime': 9.6364, 'eval_samples_per_second': 103.774, 'eval_steps_per_second': 6.538, 'epoch': 0.24}
{'loss': 0.8141, 'grad_norm': 0.27038344740867615, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5002925395965576, 'eval_runtime': 9.6175, 'eval_samples_per_second': 103.977, 'eval_steps_per_second': 6.551, 'epoch': 0.28}
{'loss': 0.7761, 'grad_norm': 0.2628752291202545, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4851990938186646, 'eval_runtime': 9.6468, 'eval_samples_per_second': 103.661, 'eval_steps_per_second': 6.531, 'epoch': 0.32}
{'loss': 0.7422, 'grad_norm': 0.28006213903427124, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.470348834991455, 'eval_runtime': 9.6457, 'eval_samples_per_second': 103.673, 'eval_steps_per_second': 6.531, 'epoch': 0.36}
{'loss': 0.7259, 'grad_norm': 0.3154461979866028, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.482478380203247, 'eval_runtime': 9.6395, 'eval_samples_per_second': 103.74, 'eval_steps_per_second': 6.536, 'epoch': 0.4}
{'loss': 0.689, 'grad_norm': 0.3092670738697052, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.49413001537323, 'eval_runtime': 9.6595, 'eval_samples_per_second': 103.525, 'eval_steps_per_second': 6.522, 'epoch': 0.44}
{'loss': 0.6271, 'grad_norm': 0.284088671207428, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4692462682724, 'eval_runtime': 9.634, 'eval_samples_per_second': 103.799, 'eval_steps_per_second': 6.539, 'epoch': 0.48}
{'loss': 0.6407, 'grad_norm': 0.2610131502151489, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4884226322174072, 'eval_runtime': 9.6395, 'eval_samples_per_second': 103.74, 'eval_steps_per_second': 6.536, 'epoch': 0.52}
{'loss': 0.6379, 'grad_norm': 0.29403746128082275, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4755133390426636, 'eval_runtime': 9.7229, 'eval_samples_per_second': 102.85, 'eval_steps_per_second': 6.48, 'epoch': 0.56}
{'loss': 0.5999, 'grad_norm': 0.2549590468406677, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4589817523956299, 'eval_runtime': 9.7203, 'eval_samples_per_second': 102.877, 'eval_steps_per_second': 6.481, 'epoch': 0.6}
{'loss': 0.66, 'grad_norm': 0.3576446771621704, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4874992370605469, 'eval_runtime': 9.6785, 'eval_samples_per_second': 103.322, 'eval_steps_per_second': 6.509, 'epoch': 0.64}
{'loss': 0.5526, 'grad_norm': 0.2141934335231781, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4769001007080078, 'eval_runtime': 9.6693, 'eval_samples_per_second': 103.42, 'eval_steps_per_second': 6.515, 'epoch': 0.68}
{'loss': 0.5982, 'grad_norm': 0.24582314491271973, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5021244287490845, 'eval_runtime': 9.6404, 'eval_samples_per_second': 103.73, 'eval_steps_per_second': 6.535, 'epoch': 0.72}
{'loss': 0.5934, 'grad_norm': 0.20283187925815582, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5014201402664185, 'eval_runtime': 9.6338, 'eval_samples_per_second': 103.801, 'eval_steps_per_second': 6.539, 'epoch': 0.76}
{'loss': 0.5266, 'grad_norm': 0.3011232316493988, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4958504438400269, 'eval_runtime': 9.5979, 'eval_samples_per_second': 104.189, 'eval_steps_per_second': 6.564, 'epoch': 0.8}
{'loss': 0.536, 'grad_norm': 0.252347469329834, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5056092739105225, 'eval_runtime': 9.6029, 'eval_samples_per_second': 104.135, 'eval_steps_per_second': 6.561, 'epoch': 0.84}
{'loss': 0.5731, 'grad_norm': 0.25063368678092957, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4981147050857544, 'eval_runtime': 9.605, 'eval_samples_per_second': 104.112, 'eval_steps_per_second': 6.559, 'epoch': 0.88}
{'loss': 0.54, 'grad_norm': 0.25490543246269226, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5165653228759766, 'eval_runtime': 9.6293, 'eval_samples_per_second': 103.85, 'eval_steps_per_second': 6.543, 'epoch': 0.92}
{'loss': 0.5195, 'grad_norm': 0.2556779682636261, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5089609622955322, 'eval_runtime': 9.6588, 'eval_samples_per_second': 103.533, 'eval_steps_per_second': 6.523, 'epoch': 0.96}
{'loss': 0.5484, 'grad_norm': 0.2702777683734894, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5088714361190796, 'eval_runtime': 9.6893, 'eval_samples_per_second': 103.207, 'eval_steps_per_second': 6.502, 'epoch': 1.0}
{'train_runtime': 323.7366, 'train_samples_per_second': 30.886, 'train_steps_per_second': 1.931, 'train_loss': 0.8255785125732422, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6433573961257935, 1.4733262062072754, 1.4622992277145386, 1.4608930349349976, 1.4803413152694702, 1.503559947013855, 1.5002925395965576, 1.4851990938186646, 1.470348834991455, 1.482478380203247, 1.49413001537323, 1.4692462682724, 1.4884226322174072, 1.4755133390426636, 1.4589817523956299, 1.4874992370605469, 1.4769001007080078, 1.5021244287490845, 1.5014201402664185, 1.4958504438400269, 1.5056092739105225, 1.4981147050857544, 1.5165653228759766, 1.5089609622955322, 1.5088714361190796], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6433573961257935, 1.4733262062072754, 1.4622992277145386, 1.4608930349349976, 1.4803413152694702, 1.503559947013855, 1.5002925395965576, 1.4851990938186646, 1.470348834991455, 1.482478380203247, 1.49413001537323, 1.4692462682724, 1.4884226322174072, 1.4755133390426636, 1.4589817523956299, 1.4874992370605469, 1.4769001007080078, 1.5021244287490845, 1.5014201402664185, 1.4958504438400269, 1.5056092739105225, 1.4981147050857544, 1.5165653228759766, 1.5089609622955322, 1.5088714361190796]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3610514402389526
current iteration best possible eval_loss (full train run):  -1.5088714361190796
max eval_loss so far:  -0.800920307636261
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648, -1.5357768535614014, -1.0416327714920044, -1.0556604862213135, -1.027631163597107, -1.3610514402389526]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 23.9095 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -1.014052436951647
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -0.9982039993927609
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -0.9925755704226129
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -0.9982039993927609
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -0.998195022807475
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0691, dtype=torch.float64), tensor(0.4707, dtype=torch.float64), tensor(0.0193, dtype=torch.float64), 0, 0, tensor(0.2894, dtype=torch.float64), tensor(0.1515, dtype=torch.float64), 32, 0, 1, 1, 0, 1, 128, 0.051916152849778345, 17.720178550524302, 1]
normalized proposed parameters for next round by BO: [tensor(8.0815e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0691, dtype=torch.float64), tensor(0.4707, dtype=torch.float64), tensor(0.0193, dtype=torch.float64), tensor(4.8973e-18, dtype=torch.float64), tensor(2.3600e-17, dtype=torch.float64), tensor(0.2894, dtype=torch.float64), tensor(0.1515, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5192, dtype=torch.float64), tensor(0.3692, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.069
  sciq: 0.471
  triviaqa: 0.019
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.289
  arc_challenge: 0.151

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.051916152849778345,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (17.720178550524302,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.051916152849778345
lora alpha:  17.720178550524302
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2612, 'grad_norm': 0.3222557604312897, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5848736763000488, 'eval_runtime': 10.1929, 'eval_samples_per_second': 98.108, 'eval_steps_per_second': 6.181, 'epoch': 0.04}
{'loss': 1.6413, 'grad_norm': 0.17441228032112122, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4692853689193726, 'eval_runtime': 10.2041, 'eval_samples_per_second': 98.0, 'eval_steps_per_second': 6.174, 'epoch': 0.08}
{'loss': 1.4098, 'grad_norm': 0.20234933495521545, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4454495906829834, 'eval_runtime': 10.2644, 'eval_samples_per_second': 97.424, 'eval_steps_per_second': 6.138, 'epoch': 0.12}
{'loss': 1.2682, 'grad_norm': 0.16792437434196472, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4196683168411255, 'eval_runtime': 10.2867, 'eval_samples_per_second': 97.213, 'eval_steps_per_second': 6.124, 'epoch': 0.16}
{'loss': 1.1704, 'grad_norm': 0.15733473002910614, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.384942650794983, 'eval_runtime': 10.2976, 'eval_samples_per_second': 97.11, 'eval_steps_per_second': 6.118, 'epoch': 0.2}
{'loss': 1.1667, 'grad_norm': 0.1624545305967331, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3714362382888794, 'eval_runtime': 10.3037, 'eval_samples_per_second': 97.052, 'eval_steps_per_second': 6.114, 'epoch': 0.24}
{'loss': 1.1143, 'grad_norm': 0.17105823755264282, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3698420524597168, 'eval_runtime': 10.3095, 'eval_samples_per_second': 96.998, 'eval_steps_per_second': 6.111, 'epoch': 0.28}
{'loss': 1.1033, 'grad_norm': 0.13424406945705414, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.359283447265625, 'eval_runtime': 10.298, 'eval_samples_per_second': 97.106, 'eval_steps_per_second': 6.118, 'epoch': 0.32}
{'loss': 1.0878, 'grad_norm': 0.12838055193424225, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3615208864212036, 'eval_runtime': 10.2772, 'eval_samples_per_second': 97.303, 'eval_steps_per_second': 6.13, 'epoch': 0.36}
{'loss': 1.0998, 'grad_norm': 0.1347440779209137, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3494614362716675, 'eval_runtime': 10.2875, 'eval_samples_per_second': 97.205, 'eval_steps_per_second': 6.124, 'epoch': 0.4}
{'loss': 1.0978, 'grad_norm': 0.1498057246208191, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.367668628692627, 'eval_runtime': 10.2619, 'eval_samples_per_second': 97.448, 'eval_steps_per_second': 6.139, 'epoch': 0.44}
{'loss': 1.0994, 'grad_norm': 0.1828199326992035, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.346511721611023, 'eval_runtime': 10.2297, 'eval_samples_per_second': 97.755, 'eval_steps_per_second': 6.159, 'epoch': 0.48}
{'loss': 1.0815, 'grad_norm': 0.145463764667511, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3878685235977173, 'eval_runtime': 10.2308, 'eval_samples_per_second': 97.744, 'eval_steps_per_second': 6.158, 'epoch': 0.52}
{'loss': 1.0814, 'grad_norm': 0.13426066935062408, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3685657978057861, 'eval_runtime': 10.2321, 'eval_samples_per_second': 97.732, 'eval_steps_per_second': 6.157, 'epoch': 0.56}
{'loss': 1.0879, 'grad_norm': 0.19322271645069122, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3888779878616333, 'eval_runtime': 10.2526, 'eval_samples_per_second': 97.537, 'eval_steps_per_second': 6.145, 'epoch': 0.6}
{'loss': 1.0965, 'grad_norm': 0.14275191724300385, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3735878467559814, 'eval_runtime': 10.2749, 'eval_samples_per_second': 97.324, 'eval_steps_per_second': 6.131, 'epoch': 0.64}
{'loss': 1.116, 'grad_norm': 0.1476477086544037, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.375899076461792, 'eval_runtime': 10.3073, 'eval_samples_per_second': 97.018, 'eval_steps_per_second': 6.112, 'epoch': 0.68}
{'loss': 1.0622, 'grad_norm': 0.1539565622806549, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3805315494537354, 'eval_runtime': 10.3058, 'eval_samples_per_second': 97.033, 'eval_steps_per_second': 6.113, 'epoch': 0.72}
{'loss': 1.0381, 'grad_norm': 0.1719624400138855, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3908426761627197, 'eval_runtime': 10.3108, 'eval_samples_per_second': 96.986, 'eval_steps_per_second': 6.11, 'epoch': 0.76}
{'loss': 1.1019, 'grad_norm': 0.1444777250289917, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3792424201965332, 'eval_runtime': 10.3408, 'eval_samples_per_second': 96.704, 'eval_steps_per_second': 6.092, 'epoch': 0.8}
{'loss': 1.0709, 'grad_norm': 0.15129268169403076, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.393243432044983, 'eval_runtime': 10.3593, 'eval_samples_per_second': 96.531, 'eval_steps_per_second': 6.081, 'epoch': 0.84}
{'loss': 1.0268, 'grad_norm': 0.15962176024913788, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3974751234054565, 'eval_runtime': 10.3409, 'eval_samples_per_second': 96.703, 'eval_steps_per_second': 6.092, 'epoch': 0.88}
{'loss': 1.0974, 'grad_norm': 0.1576448231935501, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3977875709533691, 'eval_runtime': 10.302, 'eval_samples_per_second': 97.068, 'eval_steps_per_second': 6.115, 'epoch': 0.92}
{'loss': 1.0226, 'grad_norm': 0.1892959326505661, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.396477460861206, 'eval_runtime': 10.3111, 'eval_samples_per_second': 96.982, 'eval_steps_per_second': 6.11, 'epoch': 0.96}
{'loss': 0.9995, 'grad_norm': 0.17838449776172638, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3953857421875, 'eval_runtime': 10.2962, 'eval_samples_per_second': 97.123, 'eval_steps_per_second': 6.119, 'epoch': 1.0}
{'train_runtime': 479.592, 'train_samples_per_second': 20.845, 'train_steps_per_second': 1.303, 'train_loss': 1.216119534301758, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5848736763000488, 1.4692853689193726, 1.4454495906829834, 1.4196683168411255, 1.384942650794983, 1.3714362382888794, 1.3698420524597168, 1.359283447265625, 1.3615208864212036, 1.3494614362716675, 1.367668628692627, 1.346511721611023, 1.3878685235977173, 1.3685657978057861, 1.3888779878616333, 1.3735878467559814, 1.375899076461792, 1.3805315494537354, 1.3908426761627197, 1.3792424201965332, 1.393243432044983, 1.3974751234054565, 1.3977875709533691, 1.396477460861206, 1.3953857421875], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5848736763000488, 1.4692853689193726, 1.4454495906829834, 1.4196683168411255, 1.384942650794983, 1.3714362382888794, 1.3698420524597168, 1.359283447265625, 1.3615208864212036, 1.3494614362716675, 1.367668628692627, 1.346511721611023, 1.3878685235977173, 1.3685657978057861, 1.3888779878616333, 1.3735878467559814, 1.375899076461792, 1.3805315494537354, 1.3908426761627197, 1.3792424201965332, 1.393243432044983, 1.3974751234054565, 1.3977875709533691, 1.396477460861206, 1.3953857421875]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0364465713500977
current iteration best possible eval_loss (full train run):  -1.3953857421875
max eval_loss so far:  -0.800920307636261
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648, -1.5357768535614014, -1.0416327714920044, -1.0556604862213135, -1.027631163597107, -1.3610514402389526, -1.0364465713500977]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.6383 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -0.9237136557749857
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -0.967804532248692
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -0.967804532248692
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -0.967804532248692
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -0.967804532248692
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1938, dtype=torch.float64), 0, tensor(0.0321, dtype=torch.float64), 0, tensor(0.0278, dtype=torch.float64), tensor(0.0415, dtype=torch.float64), 0, tensor(0.2592, dtype=torch.float64), tensor(0.4457, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 128, 0.1, 18.945024636939902, 0]
normalized proposed parameters for next round by BO: [tensor(0.1938, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0321, dtype=torch.float64), tensor(3.9423e-18, dtype=torch.float64), tensor(0.0278, dtype=torch.float64), tensor(0.0415, dtype=torch.float64), tensor(4.8991e-20, dtype=torch.float64), tensor(0.2592, dtype=torch.float64), tensor(0.4457, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3947, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.194
  gsm8k: 0
  rowan_hellaswag: 0.032
  sciq: 0
  triviaqa: 0.028
  truthfulqa_gen: 0.041
  wikitext: 0
  mmlu: 0.259
  arc_challenge: 0.446

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (18.945024636939902,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  18.945024636939902
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9634, 'grad_norm': 0.5779732465744019, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.503596544265747, 'eval_runtime': 10.449, 'eval_samples_per_second': 95.703, 'eval_steps_per_second': 6.029, 'epoch': 0.04}
{'loss': 1.3385, 'grad_norm': 0.2103537768125534, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3841580152511597, 'eval_runtime': 10.4435, 'eval_samples_per_second': 95.754, 'eval_steps_per_second': 6.032, 'epoch': 0.08}
{'loss': 1.0872, 'grad_norm': 0.17325131595134735, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3791533708572388, 'eval_runtime': 10.466, 'eval_samples_per_second': 95.547, 'eval_steps_per_second': 6.019, 'epoch': 0.12}
{'loss': 1.0821, 'grad_norm': 0.15389327704906464, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3876440525054932, 'eval_runtime': 10.4384, 'eval_samples_per_second': 95.8, 'eval_steps_per_second': 6.035, 'epoch': 0.16}
{'loss': 1.0129, 'grad_norm': 0.15934796631336212, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.41265070438385, 'eval_runtime': 10.4404, 'eval_samples_per_second': 95.782, 'eval_steps_per_second': 6.034, 'epoch': 0.2}
{'loss': 1.0683, 'grad_norm': 0.14963391423225403, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.389224648475647, 'eval_runtime': 10.4509, 'eval_samples_per_second': 95.685, 'eval_steps_per_second': 6.028, 'epoch': 0.24}
{'loss': 1.0126, 'grad_norm': 0.17608971893787384, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4018324613571167, 'eval_runtime': 10.447, 'eval_samples_per_second': 95.721, 'eval_steps_per_second': 6.03, 'epoch': 0.28}
{'loss': 0.9757, 'grad_norm': 0.16098801791667938, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.414966106414795, 'eval_runtime': 10.4506, 'eval_samples_per_second': 95.688, 'eval_steps_per_second': 6.028, 'epoch': 0.32}
{'loss': 0.963, 'grad_norm': 0.16240644454956055, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4106181859970093, 'eval_runtime': 10.4519, 'eval_samples_per_second': 95.676, 'eval_steps_per_second': 6.028, 'epoch': 0.36}
{'loss': 0.9742, 'grad_norm': 0.1572834849357605, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4284417629241943, 'eval_runtime': 10.4666, 'eval_samples_per_second': 95.542, 'eval_steps_per_second': 6.019, 'epoch': 0.4}
{'loss': 0.9255, 'grad_norm': 0.1709928959608078, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4184160232543945, 'eval_runtime': 10.4425, 'eval_samples_per_second': 95.763, 'eval_steps_per_second': 6.033, 'epoch': 0.44}
{'loss': 0.9631, 'grad_norm': 0.1602764129638672, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4236056804656982, 'eval_runtime': 10.4318, 'eval_samples_per_second': 95.86, 'eval_steps_per_second': 6.039, 'epoch': 0.48}
{'loss': 0.9618, 'grad_norm': 0.20064464211463928, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.414609432220459, 'eval_runtime': 10.3942, 'eval_samples_per_second': 96.207, 'eval_steps_per_second': 6.061, 'epoch': 0.52}
{'loss': 0.9431, 'grad_norm': 0.17596153914928436, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.43442702293396, 'eval_runtime': 10.4077, 'eval_samples_per_second': 96.082, 'eval_steps_per_second': 6.053, 'epoch': 0.56}
{'loss': 0.9221, 'grad_norm': 0.2566089630126953, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4302009344100952, 'eval_runtime': 10.3977, 'eval_samples_per_second': 96.175, 'eval_steps_per_second': 6.059, 'epoch': 0.6}
{'loss': 0.8871, 'grad_norm': 0.22549130022525787, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4286051988601685, 'eval_runtime': 10.4028, 'eval_samples_per_second': 96.128, 'eval_steps_per_second': 6.056, 'epoch': 0.64}
{'loss': 0.8649, 'grad_norm': 0.2543885409832001, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4459484815597534, 'eval_runtime': 10.3905, 'eval_samples_per_second': 96.242, 'eval_steps_per_second': 6.063, 'epoch': 0.68}
{'loss': 0.8288, 'grad_norm': 0.24993395805358887, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.455081582069397, 'eval_runtime': 10.3815, 'eval_samples_per_second': 96.326, 'eval_steps_per_second': 6.069, 'epoch': 0.72}
{'loss': 0.8392, 'grad_norm': 0.3046902120113373, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4526872634887695, 'eval_runtime': 10.3871, 'eval_samples_per_second': 96.273, 'eval_steps_per_second': 6.065, 'epoch': 0.76}
{'loss': 0.8609, 'grad_norm': 0.25854361057281494, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4453810453414917, 'eval_runtime': 10.451, 'eval_samples_per_second': 95.684, 'eval_steps_per_second': 6.028, 'epoch': 0.8}
{'loss': 0.8277, 'grad_norm': 0.1885916292667389, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4383649826049805, 'eval_runtime': 10.4663, 'eval_samples_per_second': 95.545, 'eval_steps_per_second': 6.019, 'epoch': 0.84}
{'loss': 0.8232, 'grad_norm': 0.22695814073085785, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.447080373764038, 'eval_runtime': 10.4625, 'eval_samples_per_second': 95.58, 'eval_steps_per_second': 6.022, 'epoch': 0.88}
{'loss': 0.8118, 'grad_norm': 0.26363444328308105, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4503965377807617, 'eval_runtime': 10.4871, 'eval_samples_per_second': 95.355, 'eval_steps_per_second': 6.007, 'epoch': 0.92}
{'loss': 0.7949, 'grad_norm': 0.2312941998243332, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4572978019714355, 'eval_runtime': 10.5345, 'eval_samples_per_second': 94.927, 'eval_steps_per_second': 5.98, 'epoch': 0.96}
{'loss': 0.8323, 'grad_norm': 0.3105056583881378, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4564062356948853, 'eval_runtime': 10.5321, 'eval_samples_per_second': 94.948, 'eval_steps_per_second': 5.982, 'epoch': 1.0}
{'train_runtime': 482.1048, 'train_samples_per_second': 20.734, 'train_steps_per_second': 1.296, 'train_loss': 1.0225684997558593, 'epoch': 1.0}
train_results:  {'eval_loss': [1.503596544265747, 1.3841580152511597, 1.3791533708572388, 1.3876440525054932, 1.41265070438385, 1.389224648475647, 1.4018324613571167, 1.414966106414795, 1.4106181859970093, 1.4284417629241943, 1.4184160232543945, 1.4236056804656982, 1.414609432220459, 1.43442702293396, 1.4302009344100952, 1.4286051988601685, 1.4459484815597534, 1.455081582069397, 1.4526872634887695, 1.4453810453414917, 1.4383649826049805, 1.447080373764038, 1.4503965377807617, 1.4572978019714355, 1.4564062356948853], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.503596544265747, 1.3841580152511597, 1.3791533708572388, 1.3876440525054932, 1.41265070438385, 1.389224648475647, 1.4018324613571167, 1.414966106414795, 1.4106181859970093, 1.4284417629241943, 1.4184160232543945, 1.4236056804656982, 1.414609432220459, 1.43442702293396, 1.4302009344100952, 1.4286051988601685, 1.4459484815597534, 1.455081582069397, 1.4526872634887695, 1.4453810453414917, 1.4383649826049805, 1.447080373764038, 1.4503965377807617, 1.4572978019714355, 1.4564062356948853]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0218371152877808
current iteration best possible eval_loss (full train run):  -1.4564062356948853
max eval_loss so far:  -0.800920307636261
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648, -1.5357768535614014, -1.0416327714920044, -1.0556604862213135, -1.027631163597107, -1.3610514402389526, -1.0364465713500977, -1.0218371152877808]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.4792 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.0409280139255
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.0410629516867589
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.0410629516867589
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.0410629516867589
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.0410629516867589
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2988, dtype=torch.float64), tensor(0.1521, dtype=torch.float64), tensor(0.0609, dtype=torch.float64), 0, tensor(0.0160, dtype=torch.float64), tensor(0.1608, dtype=torch.float64), tensor(0.0292, dtype=torch.float64), tensor(0.2795, dtype=torch.float64), 0, 26, 0, 1, 0, 1, 1, 128, 0.043933153562750016, 20.681362004822294, 1]
normalized proposed parameters for next round by BO: [tensor(0.2988, dtype=torch.float64), tensor(0.1521, dtype=torch.float64), tensor(0.0609, dtype=torch.float64), tensor(0.0028, dtype=torch.float64), tensor(0.0160, dtype=torch.float64), tensor(0.1608, dtype=torch.float64), tensor(0.0292, dtype=torch.float64), tensor(0.2795, dtype=torch.float64), tensor(5.1181e-18, dtype=torch.float64), tensor(0.8005, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4393, dtype=torch.float64), tensor(0.4309, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.299
  gsm8k: 0.152
  rowan_hellaswag: 0.061
  sciq: 0
  triviaqa: 0.016
  truthfulqa_gen: 0.161
  wikitext: 0.029
  mmlu: 0.279
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.043933153562750016,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (20.681362004822294,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.043933153562750016
lora alpha:  20.681362004822294
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 139,722,752 || all params: 8,169,984,000 || trainable%: 1.7102
length of training data:  9967
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0547, 'grad_norm': 0.8282235264778137, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4143353700637817, 'eval_runtime': 10.0397, 'eval_samples_per_second': 99.604, 'eval_steps_per_second': 6.275, 'epoch': 0.04}
{'loss': 1.5536, 'grad_norm': 0.39702168107032776, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0515720844268799, 'eval_runtime': 10.0201, 'eval_samples_per_second': 99.8, 'eval_steps_per_second': 6.287, 'epoch': 0.08}
{'loss': 1.2939, 'grad_norm': 0.2591036856174469, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 0.9544455409049988, 'eval_runtime': 10.1171, 'eval_samples_per_second': 98.843, 'eval_steps_per_second': 6.227, 'epoch': 0.12}
{'loss': 1.2494, 'grad_norm': 0.2852558195590973, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 0.9304680228233337, 'eval_runtime': 10.1397, 'eval_samples_per_second': 98.623, 'eval_steps_per_second': 6.213, 'epoch': 0.16}
{'loss': 1.2146, 'grad_norm': 0.22563056647777557, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 0.9060640335083008, 'eval_runtime': 10.1026, 'eval_samples_per_second': 98.984, 'eval_steps_per_second': 6.236, 'epoch': 0.2}
{'loss': 1.0773, 'grad_norm': 0.18327151238918304, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 0.890510618686676, 'eval_runtime': 10.1186, 'eval_samples_per_second': 98.828, 'eval_steps_per_second': 6.226, 'epoch': 0.24}
{'loss': 1.1583, 'grad_norm': 0.18942154943943024, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 0.8809934854507446, 'eval_runtime': 10.1807, 'eval_samples_per_second': 98.226, 'eval_steps_per_second': 6.188, 'epoch': 0.28}
{'loss': 1.1533, 'grad_norm': 0.19511713087558746, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 0.8779910206794739, 'eval_runtime': 10.2158, 'eval_samples_per_second': 97.887, 'eval_steps_per_second': 6.167, 'epoch': 0.32}
{'loss': 1.1013, 'grad_norm': 0.25060123205184937, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 0.8723888397216797, 'eval_runtime': 10.1888, 'eval_samples_per_second': 98.147, 'eval_steps_per_second': 6.183, 'epoch': 0.36}
{'loss': 1.1174, 'grad_norm': 0.17738062143325806, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 0.8669679164886475, 'eval_runtime': 10.1242, 'eval_samples_per_second': 98.773, 'eval_steps_per_second': 6.223, 'epoch': 0.4}
{'loss': 1.1408, 'grad_norm': 0.1422959417104721, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 0.8636354804039001, 'eval_runtime': 10.0766, 'eval_samples_per_second': 99.24, 'eval_steps_per_second': 6.252, 'epoch': 0.44}
{'loss': 1.1027, 'grad_norm': 0.1888030618429184, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 0.8613780736923218, 'eval_runtime': 10.069, 'eval_samples_per_second': 99.315, 'eval_steps_per_second': 6.257, 'epoch': 0.48}
{'loss': 1.1199, 'grad_norm': 0.1646556854248047, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 0.8625786304473877, 'eval_runtime': 10.067, 'eval_samples_per_second': 99.335, 'eval_steps_per_second': 6.258, 'epoch': 0.52}
{'loss': 1.0831, 'grad_norm': 0.2812962234020233, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 0.8592491745948792, 'eval_runtime': 10.075, 'eval_samples_per_second': 99.256, 'eval_steps_per_second': 6.253, 'epoch': 0.56}
{'loss': 1.1054, 'grad_norm': 0.16234752535820007, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 0.8562496900558472, 'eval_runtime': 10.0817, 'eval_samples_per_second': 99.19, 'eval_steps_per_second': 6.249, 'epoch': 0.6}
{'loss': 1.1233, 'grad_norm': 0.1436646580696106, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 0.8536286354064941, 'eval_runtime': 10.079, 'eval_samples_per_second': 99.216, 'eval_steps_per_second': 6.251, 'epoch': 0.64}
{'loss': 1.0666, 'grad_norm': 0.17816537618637085, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 0.8537642955780029, 'eval_runtime': 10.0797, 'eval_samples_per_second': 99.209, 'eval_steps_per_second': 6.25, 'epoch': 0.68}
{'loss': 1.0588, 'grad_norm': 0.16163234412670135, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 0.8511914014816284, 'eval_runtime': 10.1106, 'eval_samples_per_second': 98.906, 'eval_steps_per_second': 6.231, 'epoch': 0.72}
{'loss': 1.111, 'grad_norm': 0.15251870453357697, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 0.8488851189613342, 'eval_runtime': 10.0959, 'eval_samples_per_second': 99.05, 'eval_steps_per_second': 6.24, 'epoch': 0.76}
{'loss': 1.0557, 'grad_norm': 0.15296505391597748, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.8483737707138062, 'eval_runtime': 10.1, 'eval_samples_per_second': 99.009, 'eval_steps_per_second': 6.238, 'epoch': 0.8}
{'loss': 1.0544, 'grad_norm': 0.17961376905441284, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.8488614559173584, 'eval_runtime': 10.1016, 'eval_samples_per_second': 98.995, 'eval_steps_per_second': 6.237, 'epoch': 0.84}
{'loss': 1.1163, 'grad_norm': 0.15882010757923126, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.845245361328125, 'eval_runtime': 10.1053, 'eval_samples_per_second': 98.958, 'eval_steps_per_second': 6.234, 'epoch': 0.88}
{'loss': 1.0184, 'grad_norm': 0.17007862031459808, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.8455480933189392, 'eval_runtime': 10.1143, 'eval_samples_per_second': 98.87, 'eval_steps_per_second': 6.229, 'epoch': 0.92}
{'loss': 1.0326, 'grad_norm': 0.16223174333572388, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.8427510857582092, 'eval_runtime': 10.1653, 'eval_samples_per_second': 98.373, 'eval_steps_per_second': 6.198, 'epoch': 0.96}
{'train_runtime': 469.9521, 'train_samples_per_second': 21.209, 'train_steps_per_second': 1.326, 'train_loss': 1.2087516539743586, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4143353700637817, 1.0515720844268799, 0.9544455409049988, 0.9304680228233337, 0.9060640335083008, 0.890510618686676, 0.8809934854507446, 0.8779910206794739, 0.8723888397216797, 0.8669679164886475, 0.8636354804039001, 0.8613780736923218, 0.8625786304473877, 0.8592491745948792, 0.8562496900558472, 0.8536286354064941, 0.8537642955780029, 0.8511914014816284, 0.8488851189613342, 0.8483737707138062, 0.8488614559173584, 0.845245361328125, 0.8455480933189392, 0.8427510857582092], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.4143353700637817, 1.0515720844268799, 0.9544455409049988, 0.9304680228233337, 0.9060640335083008, 0.890510618686676, 0.8809934854507446, 0.8779910206794739, 0.8723888397216797, 0.8669679164886475, 0.8636354804039001, 0.8613780736923218, 0.8625786304473877, 0.8592491745948792, 0.8562496900558472, 0.8536286354064941, 0.8537642955780029, 0.8511914014816284, 0.8488851189613342, 0.8483737707138062, 0.8488614559173584, 0.845245361328125, 0.8455480933189392, 0.8427510857582092]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0382390022277832
current iteration best possible eval_loss (full train run):  -0.8427510857582092
max eval_loss so far:  -0.800920307636261
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648, -1.5357768535614014, -1.0416327714920044, -1.0556604862213135, -1.027631163597107, -1.3610514402389526, -1.0364465713500977, -1.0218371152877808, -1.0382390022277832]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 29.3729 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.01858508586883545, 0.27087509632110596, 0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3516203463077545, 0.6742079257965088, 0.8425379991531372, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.3618133068084717, 0.971827507019043, 0.9077439308166504, 0.298198401927948]  ‚Üí  acq = -1.0111113062836583
X = [0.8427011370658875, 0.498738169670105, 0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.949406087398529, 0.2025597095489502, 0.1727500557899475, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6172329187393188, 0.7515861392021179, 0.5611653327941895, 0.20585447549819946]  ‚Üí  acq = -1.0121343257367048
X = [0.2902684807777405, 0.8506918549537659, 0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.44211840629577637, 0.9807340502738953, 0.05173856019973755, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5744302272796631, 0.3193025588989258, 0.46983620524406433, 0.4887549877166748]  ‚Üí  acq = -1.0121326271532256
X = [0.4835529327392578, 0.9963791966438293, 0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.5757967829704285, 0.605756938457489, 0.38045990467071533, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.12970638275146484, 0.7689643502235413, 0.6867401599884033, 0.6047636270523071]  ‚Üí  acq = -1.0121343257367048
X = [0.500048816204071, 0.24933886528015137, 0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.07476793229579926, 0.3871777653694153, 0.952457845211029, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.24435395002365112, 0.10478633642196655, 0.9867004156112671, 0.0338512659072876]  ‚Üí  acq = -1.0121343257367048
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0667, dtype=torch.float64), tensor(0.1845, dtype=torch.float64), tensor(0.0984, dtype=torch.float64), 0, tensor(0.1314, dtype=torch.float64), tensor(0.2700, dtype=torch.float64), tensor(0.0232, dtype=torch.float64), tensor(0.1199, dtype=torch.float64), tensor(0.1060, dtype=torch.float64), 30, 0, 1, 0, 1, 1, 128, 0.03915879619807624, 32.316104825365706, 1]
normalized proposed parameters for next round by BO: [tensor(0.0667, dtype=torch.float64), tensor(0.1845, dtype=torch.float64), tensor(0.0984, dtype=torch.float64), tensor(3.8675e-18, dtype=torch.float64), tensor(0.1314, dtype=torch.float64), tensor(0.2700, dtype=torch.float64), tensor(0.0232, dtype=torch.float64), tensor(0.1199, dtype=torch.float64), tensor(0.1060, dtype=torch.float64), tensor(0.9273, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3916, dtype=torch.float64), tensor(0.6733, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.067
  gsm8k: 0.184
  rowan_hellaswag: 0.098
  sciq: 0
  triviaqa: 0.131
  truthfulqa_gen: 0.27
  wikitext: 0.023
  mmlu: 0.12
  arc_challenge: 0.106

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03915879619807624,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (32.316104825365706,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.03915879619807624
lora alpha:  32.316104825365706
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 161,218,560 || all params: 8,191,479,808 || trainable%: 1.9681
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8818, 'grad_norm': 0.5687462091445923, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.324953317642212, 'eval_runtime': 10.3481, 'eval_samples_per_second': 96.636, 'eval_steps_per_second': 6.088, 'epoch': 0.04}
{'loss': 1.437, 'grad_norm': 0.25125810503959656, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9667968153953552, 'eval_runtime': 10.3709, 'eval_samples_per_second': 96.424, 'eval_steps_per_second': 6.075, 'epoch': 0.08}
{'loss': 1.2443, 'grad_norm': 0.3062126934528351, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9332184195518494, 'eval_runtime': 10.3632, 'eval_samples_per_second': 96.495, 'eval_steps_per_second': 6.079, 'epoch': 0.12}
{'loss': 1.2063, 'grad_norm': 0.26987195014953613, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.897611677646637, 'eval_runtime': 10.3632, 'eval_samples_per_second': 96.495, 'eval_steps_per_second': 6.079, 'epoch': 0.16}
{'loss': 1.1509, 'grad_norm': 0.19735701382160187, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8840553760528564, 'eval_runtime': 10.3736, 'eval_samples_per_second': 96.399, 'eval_steps_per_second': 6.073, 'epoch': 0.2}
{'loss': 1.1315, 'grad_norm': 0.24898003041744232, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8696515560150146, 'eval_runtime': 10.3752, 'eval_samples_per_second': 96.383, 'eval_steps_per_second': 6.072, 'epoch': 0.24}
{'loss': 1.1171, 'grad_norm': 0.251291424036026, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8617041110992432, 'eval_runtime': 10.3784, 'eval_samples_per_second': 96.354, 'eval_steps_per_second': 6.07, 'epoch': 0.28}
{'loss': 1.0511, 'grad_norm': 0.20586974918842316, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8551799654960632, 'eval_runtime': 10.3683, 'eval_samples_per_second': 96.448, 'eval_steps_per_second': 6.076, 'epoch': 0.32}
{'loss': 1.064, 'grad_norm': 0.18763528764247894, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8578500747680664, 'eval_runtime': 10.3674, 'eval_samples_per_second': 96.456, 'eval_steps_per_second': 6.077, 'epoch': 0.36}
{'loss': 1.0829, 'grad_norm': 0.19529934227466583, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8501763939857483, 'eval_runtime': 10.367, 'eval_samples_per_second': 96.46, 'eval_steps_per_second': 6.077, 'epoch': 0.4}
{'loss': 1.0581, 'grad_norm': 0.1937398761510849, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.849416971206665, 'eval_runtime': 10.3847, 'eval_samples_per_second': 96.295, 'eval_steps_per_second': 6.067, 'epoch': 0.44}
{'loss': 1.0476, 'grad_norm': 0.19569158554077148, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8493668437004089, 'eval_runtime': 10.3838, 'eval_samples_per_second': 96.304, 'eval_steps_per_second': 6.067, 'epoch': 0.48}
{'loss': 1.0507, 'grad_norm': 0.20595848560333252, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8471640348434448, 'eval_runtime': 10.3542, 'eval_samples_per_second': 96.579, 'eval_steps_per_second': 6.084, 'epoch': 0.52}
{'loss': 1.0802, 'grad_norm': 0.2145577371120453, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8421459794044495, 'eval_runtime': 10.3587, 'eval_samples_per_second': 96.537, 'eval_steps_per_second': 6.082, 'epoch': 0.56}
{'loss': 1.0299, 'grad_norm': 0.20645342767238617, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.839004635810852, 'eval_runtime': 10.3609, 'eval_samples_per_second': 96.517, 'eval_steps_per_second': 6.081, 'epoch': 0.6}
{'loss': 1.0547, 'grad_norm': 0.185271754860878, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8402878642082214, 'eval_runtime': 10.3499, 'eval_samples_per_second': 96.619, 'eval_steps_per_second': 6.087, 'epoch': 0.64}
{'loss': 1.045, 'grad_norm': 0.2098039984703064, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.839401364326477, 'eval_runtime': 10.3601, 'eval_samples_per_second': 96.524, 'eval_steps_per_second': 6.081, 'epoch': 0.68}
{'loss': 1.0185, 'grad_norm': 0.1722639501094818, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8343865871429443, 'eval_runtime': 10.3936, 'eval_samples_per_second': 96.213, 'eval_steps_per_second': 6.061, 'epoch': 0.72}
{'loss': 0.9688, 'grad_norm': 0.3051251471042633, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8353022933006287, 'eval_runtime': 10.3808, 'eval_samples_per_second': 96.332, 'eval_steps_per_second': 6.069, 'epoch': 0.76}
{'loss': 1.0019, 'grad_norm': 0.24600850045681, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.834044337272644, 'eval_runtime': 10.3947, 'eval_samples_per_second': 96.203, 'eval_steps_per_second': 6.061, 'epoch': 0.8}
{'loss': 1.0281, 'grad_norm': 0.1623886078596115, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8333690762519836, 'eval_runtime': 10.372, 'eval_samples_per_second': 96.413, 'eval_steps_per_second': 6.074, 'epoch': 0.84}
{'loss': 1.0442, 'grad_norm': 0.18940627574920654, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8307773470878601, 'eval_runtime': 10.3716, 'eval_samples_per_second': 96.418, 'eval_steps_per_second': 6.074, 'epoch': 0.88}
{'loss': 0.9675, 'grad_norm': 0.21727354824543, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8309082388877869, 'eval_runtime': 10.3629, 'eval_samples_per_second': 96.498, 'eval_steps_per_second': 6.079, 'epoch': 0.92}
{'loss': 1.009, 'grad_norm': 0.19815920293331146, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8295274972915649, 'eval_runtime': 10.3587, 'eval_samples_per_second': 96.537, 'eval_steps_per_second': 6.082, 'epoch': 0.96}
{'loss': 0.9993, 'grad_norm': 0.24505838751792908, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8292288780212402, 'eval_runtime': 10.3514, 'eval_samples_per_second': 96.605, 'eval_steps_per_second': 6.086, 'epoch': 1.0}
{'train_runtime': 494.0639, 'train_samples_per_second': 20.23, 'train_steps_per_second': 1.265, 'train_loss': 1.15080830078125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.324953317642212, 0.9667968153953552, 0.9332184195518494, 0.897611677646637, 0.8840553760528564, 0.8696515560150146, 0.8617041110992432, 0.8551799654960632, 0.8578500747680664, 0.8501763939857483, 0.849416971206665, 0.8493668437004089, 0.8471640348434448, 0.8421459794044495, 0.839004635810852, 0.8402878642082214, 0.839401364326477, 0.8343865871429443, 0.8353022933006287, 0.834044337272644, 0.8333690762519836, 0.8307773470878601, 0.8309082388877869, 0.8295274972915649, 0.8292288780212402], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.324953317642212, 0.9667968153953552, 0.9332184195518494, 0.897611677646637, 0.8840553760528564, 0.8696515560150146, 0.8617041110992432, 0.8551799654960632, 0.8578500747680664, 0.8501763939857483, 0.849416971206665, 0.8493668437004089, 0.8471640348434448, 0.8421459794044495, 0.839004635810852, 0.8402878642082214, 0.839401364326477, 0.8343865871429443, 0.8353022933006287, 0.834044337272644, 0.8333690762519836, 0.8307773470878601, 0.8309082388877869, 0.8295274972915649, 0.8292288780212402]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0483641624450684
current iteration best possible eval_loss (full train run):  -0.8292288780212402
max eval_loss so far:  -0.800920307636261
BO observations:  [-2.3896827697753906, -1.1563680171966553, -1.3532873392105103, -1.0237957239151, -1.6149659156799316, -1.472610354423523, -1.853865623474121, -1.4781453609466553, -1.0328302383422852, -1.0567100048065186, -1.020788550376892, -1.11109459400177, -1.065903663635254, -1.716129183769226, -1.9551810026168823, -2.3828556537628174, -1.0787910223007202, -1.162550687789917, -1.020788550376892, -1.020788550376892, -1.0377874374389648, -1.5357768535614014, -1.0416327714920044, -1.0556604862213135, -1.027631163597107, -1.3610514402389526, -1.0364465713500977, -1.0218371152877808, -1.0382390022277832, -1.0483641624450684]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.9288 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -1.0489757820359422
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -1.0477798388498765
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -1.0779557049142539
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -1.0489757820347967
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -1.0448697231039572
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0575, dtype=torch.float64), 0, tensor(0.0800, dtype=torch.float64), tensor(0.3223, dtype=torch.float64), tensor(0.0339, dtype=torch.float64), 0, 0, tensor(0.1797, dtype=torch.float64), tensor(0.3225, dtype=torch.float64), 14, 0, 1, 0, 1, 1, 26, 0.07409369499413417, 7.1483371955233785, 1]
normalized proposed parameters for next round by BO: [tensor(0.0575, dtype=torch.float64), tensor(4.3562e-19, dtype=torch.float64), tensor(0.0800, dtype=torch.float64), tensor(0.3223, dtype=torch.float64), tensor(0.0339, dtype=torch.float64), tensor(3.4257e-18, dtype=torch.float64), tensor(0.0041, dtype=torch.float64), tensor(0.1797, dtype=torch.float64), tensor(0.3225, dtype=torch.float64), tensor(0.4265, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2018, dtype=torch.float64), tensor(0.7409, dtype=torch.float64), tensor(0.1489, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-1.4836045503616333, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
gsm8k
evaluation dataset:
data domain:  gsm8k  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/gsm8k/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 9, 1, 1, 0, 0, 1, 14, 0.03669660801939172, 9, 1]
Checking history sample input_X_between_0_1:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 0.28125, 1.0, 1.0, 0.0, 0.0, 1.0, 0.109375, 0.3669660801939172, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1832736730575562
Checking history sample input_X:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 4, 1, 1, 0, 1, 0, 45, 0.02996529708553877, 41, 0]
Checking history sample input_X_between_0_1:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 0.125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3515625, 0.29965297085538767, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2217429876327515
Checking history sample input_X:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 5, 0, 1, 1, 0, 0, 115, 0.08044623754842878, 26, 0]
Checking history sample input_X_between_0_1:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 0.15625, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8984375, 0.8044623754842878, 0.5416666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1418228149414062
Checking history sample input_X:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 30, 0, 1, 0, 1, 1, 121, 0.019265932894845208, 29, 1]
Checking history sample input_X_between_0_1:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 0.9375, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9453125, 0.19265932894845206, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8670127987861633
Checking history sample input_X:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 19, 0, 0, 1, 1, 1, 108, 0.09306302255978231, 35, 1]
Checking history sample input_X_between_0_1:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 0.59375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.84375, 0.930630225597823, 0.7291666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.3226383924484253
Checking history sample input_X:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 14, 0, 0, 0, 1, 1, 44, 0.004820496247456452, 22, 1]
Checking history sample input_X_between_0_1:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 0.4375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.34375, 0.04820496247456452, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9786784052848816
Checking history sample input_X:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 24, 0, 1, 1, 0, 1, 79, 0.03164316476579598, 9, 1]
Checking history sample input_X_between_0_1:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6171875, 0.3164316476579597, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.201198697090149
Checking history sample input_X:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 29, 0, 1, 0, 1, 1, 23, 0.015858984905097274, 40, 0]
Checking history sample input_X_between_0_1:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 0.90625, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1796875, 0.15858984905097273, 0.8333333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.079849362373352
Checking history sample input_X:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 27, 0, 0, 1, 0, 0, 58, 0.02356583389888708, 4, 0]
Checking history sample input_X_between_0_1:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 0.84375, 0.0, 0.0, 1.0, 0.0, 0.0, 0.453125, 0.23565833898887079, 0.08333333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3622353076934814
Checking history sample input_X:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 18, 0, 1, 1, 0, 1, 102, 0.07944916714916157, 39, 0]
Checking history sample input_X_between_0_1:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 0.5625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.796875, 0.7944916714916157, 0.8125, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1885993480682373
Checking history sample input_X:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 16, 1, 0, 0, 0, 0, 57, 0.051177108415818844, 16, 1]
Checking history sample input_X_between_0_1:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4453125, 0.5117710841581884, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.7646844387054443
Checking history sample input_X:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 21, 0, 1, 0, 0, 1, 80, 0.08438899074092726, 11, 1]
Checking history sample input_X_between_0_1:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 0.65625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.625, 0.8438899074092726, 0.22916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.194746494293213
Checking history sample input_X:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 19, 0, 1, 1, 1, 0, 106, 0.021750066051679486, 16, 1]
Checking history sample input_X_between_0_1:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 0.59375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.828125, 0.21750066051679484, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.357306957244873
Checking history sample input_X:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 6, 1, 0, 0, 0, 1, 109, 0.0976089152670931, 17, 0]
Checking history sample input_X_between_0_1:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 0.1875, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8515625, 0.976089152670931, 0.3541666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0162200927734375
Checking history sample input_X:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 5, 0, 1, 1, 0, 1, 6, 0.045424701431822194, 31, 1]
Checking history sample input_X_between_0_1:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 0.15625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.046875, 0.4542470143182219, 0.6458333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2814624309539795
Checking history sample input_X:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 18, 1, 0, 0, 0, 0, 27, 0.050875722794158945, 27, 1]
Checking history sample input_X_between_0_1:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 0.5625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2109375, 0.5087572279415894, 0.5625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.6545288562774658
Checking history sample input_X:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 25, 0, 0, 0, 1, 1, 5, 0.0932969995849881, 3, 0]
Checking history sample input_X_between_0_1:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 0.78125, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0390625, 0.932969995849881, 0.0625, 0.0]
Checking history sample eval_loss at 625 steps:  -1.4124306440353394
Checking history sample input_X:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 14, 0, 0, 0, 1, 0, 14, 0.06234381073507801, 24, 1]
Checking history sample input_X_between_0_1:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 0.4375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.109375, 0.6234381073507801, 0.5, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9628919959068298
Checking history sample input_X:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 19, 1, 1, 0, 1, 0, 54, 0.0659080396027719, 16, 1]
Checking history sample input_X_between_0_1:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 0.59375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.421875, 0.659080396027719, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4756159782409668
Checking history sample input_X:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 30, 1, 1, 0, 1, 0, 113, 0.08743251828499332, 21, 0]
Checking history sample input_X_between_0_1:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 0.9375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8828125, 0.8743251828499332, 0.4375, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9556006193161011
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.1396 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.783851683139801, 0.24913036823272705, 0.4524749517440796, 0.38153332471847534, 0.9488375782966614, 0.0967053771018982, 0.23472976684570312, 0.2958238124847412, 0.6644061207771301, 0.9346109628677368, 0.7502865195274353, 0.10026943683624268, 0.19480091333389282, 0.9671425223350525, 0.8400884866714478, 0.489362508058548, 0.4079207181930542, 0.20619627833366394, 0.28701257705688477]  ‚Üí  acq = -0.609464322333146
X = [0.8464758396148682, 0.2014230489730835, 0.11860859394073486, 0.9874942898750305, 0.5662835836410522, 0.5275111794471741, 0.023939788341522217, 0.6864414811134338, 0.9413574934005737, 0.5217980742454529, 0.3246157169342041, 0.3060162663459778, 0.24693655967712402, 0.3577972650527954, 0.12188678979873657, 0.03954879194498062, 0.6221333742141724, 0.7347893714904785, 0.47663766145706177]  ‚Üí  acq = -0.6092937829680668
X = [0.6884494423866272, 0.8336878418922424, 0.018001914024353027, 0.7176263928413391, 0.989361584186554, 0.2666308879852295, 0.24889063835144043, 0.13956493139266968, 0.14550507068634033, 0.8165102005004883, 0.3124064803123474, 0.007756292819976807, 0.3866540193557739, 0.41762131452560425, 0.8699772953987122, 0.4418141543865204, 0.5272796750068665, 0.277340292930603, 0.16987395286560059]  ‚Üí  acq = -0.6092408127657368
X = [0.7973583340644836, 0.09157067537307739, 0.47823023796081543, 0.5121268033981323, 0.2526543140411377, 0.043666720390319824, 0.8283634781837463, 0.5997011065483093, 0.6987678408622742, 0.30673471093177795, 0.013873517513275146, 0.6025514602661133, 0.8564243912696838, 0.8063071966171265, 0.20607733726501465, 0.3968336880207062, 0.5656952857971191, 0.09493724256753922, 0.701043963432312]  ‚Üí  acq = -0.6093165115448042
X = [0.05593842267990112, 0.9080440402030945, 0.11720722913742065, 0.4897025227546692, 0.8531294465065002, 0.5992872714996338, 0.6929259896278381, 0.0011692047119140625, 0.12231272459030151, 0.17169101536273956, 0.339116632938385, 0.508143961429596, 0.4350828528404236, 0.289997398853302, 0.2512979507446289, 0.8461728692054749, 0.5082880854606628, 0.11696593463420868, 0.36222410202026367]  ‚Üí  acq = -0.6092909098484974
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.1805, dtype=torch.float64), 0, tensor(0.0213, dtype=torch.float64), tensor(0.2025, dtype=torch.float64), 0, tensor(0.3279, dtype=torch.float64), 0, 0, tensor(0.2679, dtype=torch.float64), 32, 0, 0, 1, 1, 0, 2, 0.0, 48.0, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.1805, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0213, dtype=torch.float64), tensor(0.2025, dtype=torch.float64), tensor(1.7202e-17, dtype=torch.float64), tensor(0.3279, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2679, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.18
  gsm8k: 0
  rowan_hellaswag: 0.021
  sciq: 0.202
  triviaqa: 0
  truthfulqa_gen: 0.328
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.268

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,359,296 || all params: 8,032,620,544 || trainable%: 0.0294
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 2.8656, 'grad_norm': 7.256443023681641, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4357436895370483, 'eval_runtime': 10.221, 'eval_samples_per_second': 97.838, 'eval_steps_per_second': 6.164, 'epoch': 0.04}
{'loss': 1.1131, 'grad_norm': 3.2026479244232178, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3988137245178223, 'eval_runtime': 10.2976, 'eval_samples_per_second': 97.11, 'eval_steps_per_second': 6.118, 'epoch': 0.08}
{'loss': 0.9738, 'grad_norm': 2.2800164222717285, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.432687520980835, 'eval_runtime': 10.3742, 'eval_samples_per_second': 96.393, 'eval_steps_per_second': 6.073, 'epoch': 0.12}
{'loss': 0.902, 'grad_norm': 1.8256503343582153, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3969640731811523, 'eval_runtime': 10.3555, 'eval_samples_per_second': 96.567, 'eval_steps_per_second': 6.084, 'epoch': 0.16}
{'loss': 0.8464, 'grad_norm': 2.0020804405212402, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4314631223678589, 'eval_runtime': 10.3758, 'eval_samples_per_second': 96.379, 'eval_steps_per_second': 6.072, 'epoch': 0.2}
{'loss': 0.8038, 'grad_norm': 1.8783842325210571, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4742742776870728, 'eval_runtime': 10.3835, 'eval_samples_per_second': 96.307, 'eval_steps_per_second': 6.067, 'epoch': 0.24}
{'loss': 0.7963, 'grad_norm': 2.2101621627807617, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4399220943450928, 'eval_runtime': 10.3664, 'eval_samples_per_second': 96.466, 'eval_steps_per_second': 6.077, 'epoch': 0.28}
{'loss': 0.84, 'grad_norm': 1.9653403759002686, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4400793313980103, 'eval_runtime': 10.3377, 'eval_samples_per_second': 96.733, 'eval_steps_per_second': 6.094, 'epoch': 0.32}
{'loss': 0.7824, 'grad_norm': 1.8141111135482788, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4730844497680664, 'eval_runtime': 10.3059, 'eval_samples_per_second': 97.031, 'eval_steps_per_second': 6.113, 'epoch': 0.36}
{'loss': 0.7676, 'grad_norm': 1.9149833917617798, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5015299320220947, 'eval_runtime': 10.3091, 'eval_samples_per_second': 97.001, 'eval_steps_per_second': 6.111, 'epoch': 0.4}
{'loss': 0.7417, 'grad_norm': 1.9352957010269165, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4931433200836182, 'eval_runtime': 10.3395, 'eval_samples_per_second': 96.717, 'eval_steps_per_second': 6.093, 'epoch': 0.44}
{'loss': 0.756, 'grad_norm': 2.111600399017334, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5275896787643433, 'eval_runtime': 10.3227, 'eval_samples_per_second': 96.874, 'eval_steps_per_second': 6.103, 'epoch': 0.48}
{'loss': 0.6892, 'grad_norm': 1.8044359683990479, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.535861611366272, 'eval_runtime': 10.3132, 'eval_samples_per_second': 96.963, 'eval_steps_per_second': 6.109, 'epoch': 0.52}
{'loss': 0.7263, 'grad_norm': 1.8638091087341309, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5055737495422363, 'eval_runtime': 10.2744, 'eval_samples_per_second': 97.329, 'eval_steps_per_second': 6.132, 'epoch': 0.56}
{'loss': 0.7345, 'grad_norm': 1.936425805091858, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5107430219650269, 'eval_runtime': 10.2841, 'eval_samples_per_second': 97.238, 'eval_steps_per_second': 6.126, 'epoch': 0.6}
{'loss': 0.6603, 'grad_norm': 2.304234266281128, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5199137926101685, 'eval_runtime': 10.2721, 'eval_samples_per_second': 97.351, 'eval_steps_per_second': 6.133, 'epoch': 0.64}
{'loss': 0.662, 'grad_norm': 2.2933030128479004, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.52646803855896, 'eval_runtime': 10.2906, 'eval_samples_per_second': 97.176, 'eval_steps_per_second': 6.122, 'epoch': 0.68}
{'loss': 0.6559, 'grad_norm': 2.103431224822998, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5311716794967651, 'eval_runtime': 10.2926, 'eval_samples_per_second': 97.157, 'eval_steps_per_second': 6.121, 'epoch': 0.72}
{'loss': 0.7087, 'grad_norm': 1.8678430318832397, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5275217294692993, 'eval_runtime': 10.2876, 'eval_samples_per_second': 97.204, 'eval_steps_per_second': 6.124, 'epoch': 0.76}
{'loss': 0.6604, 'grad_norm': 2.4495222568511963, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5256301164627075, 'eval_runtime': 10.2885, 'eval_samples_per_second': 97.196, 'eval_steps_per_second': 6.123, 'epoch': 0.8}
{'loss': 0.6363, 'grad_norm': 1.859099268913269, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5191032886505127, 'eval_runtime': 10.2876, 'eval_samples_per_second': 97.205, 'eval_steps_per_second': 6.124, 'epoch': 0.84}
{'loss': 0.6273, 'grad_norm': 2.36096453666687, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5308619737625122, 'eval_runtime': 10.2813, 'eval_samples_per_second': 97.264, 'eval_steps_per_second': 6.128, 'epoch': 0.88}
{'loss': 0.6351, 'grad_norm': 2.302349805831909, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5392340421676636, 'eval_runtime': 10.2977, 'eval_samples_per_second': 97.109, 'eval_steps_per_second': 6.118, 'epoch': 0.92}
{'loss': 0.6109, 'grad_norm': 2.080573558807373, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5371696949005127, 'eval_runtime': 10.2859, 'eval_samples_per_second': 97.221, 'eval_steps_per_second': 6.125, 'epoch': 0.96}
{'loss': 0.6482, 'grad_norm': 2.091198205947876, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5389292240142822, 'eval_runtime': 10.2699, 'eval_samples_per_second': 97.372, 'eval_steps_per_second': 6.134, 'epoch': 1.0}
{'train_runtime': 417.2479, 'train_samples_per_second': 23.962, 'train_steps_per_second': 1.498, 'train_loss': 0.8337493728637695, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4357436895370483, 1.3988137245178223, 1.432687520980835, 1.3969640731811523, 1.4314631223678589, 1.4742742776870728, 1.4399220943450928, 1.4400793313980103, 1.4730844497680664, 1.5015299320220947, 1.4931433200836182, 1.5275896787643433, 1.535861611366272, 1.5055737495422363, 1.5107430219650269, 1.5199137926101685, 1.52646803855896, 1.5311716794967651, 1.5275217294692993, 1.5256301164627075, 1.5191032886505127, 1.5308619737625122, 1.5392340421676636, 1.5371696949005127, 1.5389292240142822], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4357436895370483, 1.3988137245178223, 1.432687520980835, 1.3969640731811523, 1.4314631223678589, 1.4742742776870728, 1.4399220943450928, 1.4400793313980103, 1.4730844497680664, 1.5015299320220947, 1.4931433200836182, 1.5275896787643433, 1.535861611366272, 1.5055737495422363, 1.5107430219650269, 1.5199137926101685, 1.52646803855896, 1.5311716794967651, 1.5275217294692993, 1.5256301164627075, 1.5191032886505127, 1.5308619737625122, 1.5392340421676636, 1.5371696949005127, 1.5389292240142822]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.3928966522216797
current iteration best possible eval_loss (full train run):  -1.5389292240142822
max eval_loss so far:  -1.5389292240142822
BO observations:  [-2.3928966522216797]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5788 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.34609800472049446
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.3595091511080004
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.34559694786827966
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.37057336386890216
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.3598222038354868
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1935, dtype=torch.float64), tensor(0.3279, dtype=torch.float64), 0, tensor(0.4168, dtype=torch.float64), tensor(0.0206, dtype=torch.float64), 0, 0, 0, tensor(0.0412, dtype=torch.float64), 30, 0, 1, 0, 1, 1, 128, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.1935, dtype=torch.float64), tensor(0.3279, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4168, dtype=torch.float64), tensor(0.0206, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.6348e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0412, dtype=torch.float64), tensor(0.9408, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.193
  gsm8k: 0.328
  rowan_hellaswag: 0
  sciq: 0.417
  triviaqa: 0.021
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.041

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 161,218,560 || all params: 8,191,479,808 || trainable%: 1.9681
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4918, 'grad_norm': 0.5553399920463562, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.152994155883789, 'eval_runtime': 10.2606, 'eval_samples_per_second': 97.46, 'eval_steps_per_second': 6.14, 'epoch': 0.04}
{'loss': 1.0038, 'grad_norm': 0.2685449719429016, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9109365344047546, 'eval_runtime': 10.293, 'eval_samples_per_second': 97.153, 'eval_steps_per_second': 6.121, 'epoch': 0.08}
{'loss': 0.8894, 'grad_norm': 0.19634273648262024, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8893421292304993, 'eval_runtime': 10.3258, 'eval_samples_per_second': 96.845, 'eval_steps_per_second': 6.101, 'epoch': 0.12}
{'loss': 0.8667, 'grad_norm': 0.2062738835811615, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.881486713886261, 'eval_runtime': 10.3441, 'eval_samples_per_second': 96.673, 'eval_steps_per_second': 6.09, 'epoch': 0.16}
{'loss': 0.8626, 'grad_norm': 0.19533078372478485, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8723272681236267, 'eval_runtime': 10.3579, 'eval_samples_per_second': 96.544, 'eval_steps_per_second': 6.082, 'epoch': 0.2}
{'loss': 0.849, 'grad_norm': 0.19338050484657288, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8679606318473816, 'eval_runtime': 10.3651, 'eval_samples_per_second': 96.477, 'eval_steps_per_second': 6.078, 'epoch': 0.24}
{'loss': 0.8558, 'grad_norm': 0.20705635845661163, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8657999038696289, 'eval_runtime': 10.3718, 'eval_samples_per_second': 96.415, 'eval_steps_per_second': 6.074, 'epoch': 0.28}
{'loss': 0.8357, 'grad_norm': 0.2518835663795471, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8593037128448486, 'eval_runtime': 10.3655, 'eval_samples_per_second': 96.474, 'eval_steps_per_second': 6.078, 'epoch': 0.32}
{'loss': 0.8458, 'grad_norm': 0.20898392796516418, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8551858067512512, 'eval_runtime': 10.3577, 'eval_samples_per_second': 96.547, 'eval_steps_per_second': 6.082, 'epoch': 0.36}
{'loss': 0.8454, 'grad_norm': 0.1794937551021576, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8526571393013, 'eval_runtime': 10.3586, 'eval_samples_per_second': 96.538, 'eval_steps_per_second': 6.082, 'epoch': 0.4}
{'loss': 0.8043, 'grad_norm': 0.19994907081127167, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8508061766624451, 'eval_runtime': 10.3778, 'eval_samples_per_second': 96.359, 'eval_steps_per_second': 6.071, 'epoch': 0.44}
{'loss': 0.7901, 'grad_norm': 0.18352723121643066, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8514842391014099, 'eval_runtime': 10.3778, 'eval_samples_per_second': 96.36, 'eval_steps_per_second': 6.071, 'epoch': 0.48}
{'loss': 0.8167, 'grad_norm': 0.2081076055765152, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8468964695930481, 'eval_runtime': 10.4286, 'eval_samples_per_second': 95.89, 'eval_steps_per_second': 6.041, 'epoch': 0.52}
{'loss': 0.8236, 'grad_norm': 0.20677855610847473, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8463953733444214, 'eval_runtime': 10.3955, 'eval_samples_per_second': 96.196, 'eval_steps_per_second': 6.06, 'epoch': 0.56}
{'loss': 0.8152, 'grad_norm': 0.2588357627391815, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8450036644935608, 'eval_runtime': 10.442, 'eval_samples_per_second': 95.767, 'eval_steps_per_second': 6.033, 'epoch': 0.6}
{'loss': 0.8281, 'grad_norm': 0.22905535995960236, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8387613296508789, 'eval_runtime': 10.3855, 'eval_samples_per_second': 96.288, 'eval_steps_per_second': 6.066, 'epoch': 0.64}
{'loss': 0.7975, 'grad_norm': 0.19124457240104675, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8380612730979919, 'eval_runtime': 10.3825, 'eval_samples_per_second': 96.316, 'eval_steps_per_second': 6.068, 'epoch': 0.68}
{'loss': 0.7958, 'grad_norm': 0.19342564046382904, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8383565545082092, 'eval_runtime': 10.3797, 'eval_samples_per_second': 96.342, 'eval_steps_per_second': 6.07, 'epoch': 0.72}
{'loss': 0.8194, 'grad_norm': 0.2170787900686264, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8350611925125122, 'eval_runtime': 10.4046, 'eval_samples_per_second': 96.111, 'eval_steps_per_second': 6.055, 'epoch': 0.76}
{'loss': 0.7836, 'grad_norm': 0.20096151530742645, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8323777914047241, 'eval_runtime': 10.3992, 'eval_samples_per_second': 96.161, 'eval_steps_per_second': 6.058, 'epoch': 0.8}
{'loss': 0.7783, 'grad_norm': 0.18726369738578796, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8316581845283508, 'eval_runtime': 10.3933, 'eval_samples_per_second': 96.215, 'eval_steps_per_second': 6.062, 'epoch': 0.84}
{'loss': 0.7805, 'grad_norm': 0.22237887978553772, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8304042816162109, 'eval_runtime': 10.3845, 'eval_samples_per_second': 96.297, 'eval_steps_per_second': 6.067, 'epoch': 0.88}
{'loss': 0.7776, 'grad_norm': 0.1976039856672287, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8295716643333435, 'eval_runtime': 10.4171, 'eval_samples_per_second': 95.996, 'eval_steps_per_second': 6.048, 'epoch': 0.92}
{'loss': 0.7836, 'grad_norm': 0.18353749811649323, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8281676173210144, 'eval_runtime': 10.4013, 'eval_samples_per_second': 96.142, 'eval_steps_per_second': 6.057, 'epoch': 0.96}
{'loss': 0.7922, 'grad_norm': 0.2222181111574173, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8277751207351685, 'eval_runtime': 10.3817, 'eval_samples_per_second': 96.323, 'eval_steps_per_second': 6.068, 'epoch': 1.0}
{'train_runtime': 480.5969, 'train_samples_per_second': 20.803, 'train_steps_per_second': 1.3, 'train_loss': 0.8932926147460938, 'epoch': 1.0}
train_results:  {'eval_loss': [1.152994155883789, 0.9109365344047546, 0.8893421292304993, 0.881486713886261, 0.8723272681236267, 0.8679606318473816, 0.8657999038696289, 0.8593037128448486, 0.8551858067512512, 0.8526571393013, 0.8508061766624451, 0.8514842391014099, 0.8468964695930481, 0.8463953733444214, 0.8450036644935608, 0.8387613296508789, 0.8380612730979919, 0.8383565545082092, 0.8350611925125122, 0.8323777914047241, 0.8316581845283508, 0.8304042816162109, 0.8295716643333435, 0.8281676173210144, 0.8277751207351685], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.152994155883789, 0.9109365344047546, 0.8893421292304993, 0.881486713886261, 0.8723272681236267, 0.8679606318473816, 0.8657999038696289, 0.8593037128448486, 0.8551858067512512, 0.8526571393013, 0.8508061766624451, 0.8514842391014099, 0.8468964695930481, 0.8463953733444214, 0.8450036644935608, 0.8387613296508789, 0.8380612730979919, 0.8383565545082092, 0.8350611925125122, 0.8323777914047241, 0.8316581845283508, 0.8304042816162109, 0.8295716643333435, 0.8281676173210144, 0.8277751207351685]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1803230047225952
current iteration best possible eval_loss (full train run):  -0.8277751207351685
max eval_loss so far:  -0.8277751207351685
BO observations:  [-2.3928966522216797, -1.1803230047225952]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6378 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.5788545407847487
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.5788476511758092
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.5788521092878772
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.5787164006621348
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.5787032001534115
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0570, dtype=torch.float64), 0, tensor(0.0782, dtype=torch.float64), tensor(0.1259, dtype=torch.float64), tensor(0.3059, dtype=torch.float64), tensor(0.2707, dtype=torch.float64), tensor(0.1622, dtype=torch.float64), 5, 1, 1, 0, 1, 1, 128, 0.022154090819322445, 11.76229588111583, 1]
normalized proposed parameters for next round by BO: [tensor(8.3384e-18, dtype=torch.float64), tensor(1.0323e-17, dtype=torch.float64), tensor(0.0570, dtype=torch.float64), tensor(4.1162e-18, dtype=torch.float64), tensor(0.0782, dtype=torch.float64), tensor(0.1259, dtype=torch.float64), tensor(0.3059, dtype=torch.float64), tensor(0.2707, dtype=torch.float64), tensor(0.1622, dtype=torch.float64), tensor(0.1600, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2215, dtype=torch.float64), tensor(0.2450, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.057
  sciq: 0
  triviaqa: 0.078
  truthfulqa_gen: 0.126
  wikitext: 0.306
  mmlu: 0.271
  arc_challenge: 0.162

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.022154090819322445,)
  num_layers_to_apply: (5,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (11.76229588111583,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  5
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.022154090819322445
lora alpha:  11.76229588111583
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 32,112,640 || all params: 8,062,373,888 || trainable%: 0.3983
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5393, 'grad_norm': 1.3052668571472168, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8466507196426392, 'eval_runtime': 9.1179, 'eval_samples_per_second': 109.674, 'eval_steps_per_second': 6.909, 'epoch': 0.04}
{'loss': 2.4179, 'grad_norm': 1.007673978805542, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4930131435394287, 'eval_runtime': 9.1366, 'eval_samples_per_second': 109.45, 'eval_steps_per_second': 6.895, 'epoch': 0.08}
{'loss': 1.907, 'grad_norm': 0.40438228845596313, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4712878465652466, 'eval_runtime': 9.1402, 'eval_samples_per_second': 109.407, 'eval_steps_per_second': 6.893, 'epoch': 0.12}
{'loss': 1.7104, 'grad_norm': 0.29350414872169495, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.44197416305542, 'eval_runtime': 9.1549, 'eval_samples_per_second': 109.231, 'eval_steps_per_second': 6.882, 'epoch': 0.16}
{'loss': 1.6461, 'grad_norm': 0.30550792813301086, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4272382259368896, 'eval_runtime': 9.1675, 'eval_samples_per_second': 109.081, 'eval_steps_per_second': 6.872, 'epoch': 0.2}
{'loss': 1.6721, 'grad_norm': 0.2916833162307739, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4629253149032593, 'eval_runtime': 9.1919, 'eval_samples_per_second': 108.791, 'eval_steps_per_second': 6.854, 'epoch': 0.24}
{'loss': 1.6698, 'grad_norm': 0.31896936893463135, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4827138185501099, 'eval_runtime': 9.2356, 'eval_samples_per_second': 108.277, 'eval_steps_per_second': 6.821, 'epoch': 0.28}
{'loss': 1.559, 'grad_norm': 0.24578866362571716, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4613585472106934, 'eval_runtime': 9.2334, 'eval_samples_per_second': 108.302, 'eval_steps_per_second': 6.823, 'epoch': 0.32}
{'loss': 1.5108, 'grad_norm': 0.3052579164505005, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4422005414962769, 'eval_runtime': 9.231, 'eval_samples_per_second': 108.33, 'eval_steps_per_second': 6.825, 'epoch': 0.36}
{'loss': 1.4969, 'grad_norm': 0.28371307253837585, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4272968769073486, 'eval_runtime': 9.2299, 'eval_samples_per_second': 108.344, 'eval_steps_per_second': 6.826, 'epoch': 0.4}
{'loss': 1.511, 'grad_norm': 0.34361374378204346, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4566118717193604, 'eval_runtime': 9.2195, 'eval_samples_per_second': 108.466, 'eval_steps_per_second': 6.833, 'epoch': 0.44}
{'loss': 1.4904, 'grad_norm': 0.24975809454917908, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.423449993133545, 'eval_runtime': 9.1926, 'eval_samples_per_second': 108.783, 'eval_steps_per_second': 6.853, 'epoch': 0.48}
{'loss': 1.5104, 'grad_norm': 0.25040438771247864, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.411795735359192, 'eval_runtime': 9.2146, 'eval_samples_per_second': 108.523, 'eval_steps_per_second': 6.837, 'epoch': 0.52}
{'loss': 1.4578, 'grad_norm': 0.2183188647031784, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4031360149383545, 'eval_runtime': 9.2225, 'eval_samples_per_second': 108.43, 'eval_steps_per_second': 6.831, 'epoch': 0.56}
{'loss': 1.4401, 'grad_norm': 0.26202285289764404, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3997973203659058, 'eval_runtime': 9.2189, 'eval_samples_per_second': 108.473, 'eval_steps_per_second': 6.834, 'epoch': 0.6}
{'loss': 1.377, 'grad_norm': 0.2879588305950165, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4152997732162476, 'eval_runtime': 9.2281, 'eval_samples_per_second': 108.365, 'eval_steps_per_second': 6.827, 'epoch': 0.64}
{'loss': 1.5526, 'grad_norm': 0.29215317964553833, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4105726480484009, 'eval_runtime': 9.2353, 'eval_samples_per_second': 108.28, 'eval_steps_per_second': 6.822, 'epoch': 0.68}
{'loss': 1.4696, 'grad_norm': 0.6943034529685974, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3939777612686157, 'eval_runtime': 9.2293, 'eval_samples_per_second': 108.35, 'eval_steps_per_second': 6.826, 'epoch': 0.72}
{'loss': 1.4333, 'grad_norm': 0.318053662776947, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4096218347549438, 'eval_runtime': 9.2111, 'eval_samples_per_second': 108.565, 'eval_steps_per_second': 6.84, 'epoch': 0.76}
{'loss': 1.4696, 'grad_norm': 0.22577668726444244, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.397049069404602, 'eval_runtime': 9.2083, 'eval_samples_per_second': 108.597, 'eval_steps_per_second': 6.842, 'epoch': 0.8}
{'loss': 1.4506, 'grad_norm': 0.2465548813343048, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4028490781784058, 'eval_runtime': 9.1823, 'eval_samples_per_second': 108.905, 'eval_steps_per_second': 6.861, 'epoch': 0.84}
{'loss': 1.3969, 'grad_norm': 0.2578560709953308, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.404373049736023, 'eval_runtime': 9.1862, 'eval_samples_per_second': 108.859, 'eval_steps_per_second': 6.858, 'epoch': 0.88}
{'loss': 1.4402, 'grad_norm': 0.21623489260673523, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.397510051727295, 'eval_runtime': 9.1916, 'eval_samples_per_second': 108.795, 'eval_steps_per_second': 6.854, 'epoch': 0.92}
{'loss': 1.3887, 'grad_norm': 0.27521759271621704, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.39716637134552, 'eval_runtime': 9.1974, 'eval_samples_per_second': 108.726, 'eval_steps_per_second': 6.85, 'epoch': 0.96}
{'loss': 1.4756, 'grad_norm': 0.28545379638671875, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.398140788078308, 'eval_runtime': 9.1748, 'eval_samples_per_second': 108.994, 'eval_steps_per_second': 6.867, 'epoch': 1.0}
{'train_runtime': 420.6571, 'train_samples_per_second': 23.765, 'train_steps_per_second': 1.486, 'train_loss': 1.6397225830078126, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8466507196426392, 1.4930131435394287, 1.4712878465652466, 1.44197416305542, 1.4272382259368896, 1.4629253149032593, 1.4827138185501099, 1.4613585472106934, 1.4422005414962769, 1.4272968769073486, 1.4566118717193604, 1.423449993133545, 1.411795735359192, 1.4031360149383545, 1.3997973203659058, 1.4152997732162476, 1.4105726480484009, 1.3939777612686157, 1.4096218347549438, 1.397049069404602, 1.4028490781784058, 1.404373049736023, 1.397510051727295, 1.39716637134552, 1.398140788078308], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8466507196426392, 1.4930131435394287, 1.4712878465652466, 1.44197416305542, 1.4272382259368896, 1.4629253149032593, 1.4827138185501099, 1.4613585472106934, 1.4422005414962769, 1.4272968769073486, 1.4566118717193604, 1.423449993133545, 1.411795735359192, 1.4031360149383545, 1.3997973203659058, 1.4152997732162476, 1.4105726480484009, 1.3939777612686157, 1.4096218347549438, 1.397049069404602, 1.4028490781784058, 1.404373049736023, 1.397510051727295, 1.39716637134552, 1.398140788078308]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.343926191329956
current iteration best possible eval_loss (full train run):  -1.398140788078308
max eval_loss so far:  -0.8277751207351685
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.1097 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.6157521958508587
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.6157521958508587
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.6157521958508587
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.6157521958508587
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.6157521958508587
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.7710, dtype=torch.float64), 0, 0, 0, tensor(0.2290, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 1, 1, 0, 31, 0.08069580705108324, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(4.2686e-17, dtype=torch.float64), tensor(0.7710, dtype=torch.float64), tensor(1.6347e-17, dtype=torch.float64), tensor(2.4166e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2290, dtype=torch.float64), tensor(2.9294e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.6666e-18, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2448, dtype=torch.float64), tensor(0.8070, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.771
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.229
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (31,)
  lora_dropout: (0.08069580705108324,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  31
lora dropout:  0.08069580705108324
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 41,648,128 || all params: 8,071,909,376 || trainable%: 0.5160
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 1.8516, 'grad_norm': 0.8884565234184265, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 0.999902606010437, 'eval_runtime': 10.8478, 'eval_samples_per_second': 92.185, 'eval_steps_per_second': 5.808, 'epoch': 0.04}
{'loss': 0.91, 'grad_norm': 0.4712768495082855, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.8776212334632874, 'eval_runtime': 10.8216, 'eval_samples_per_second': 92.407, 'eval_steps_per_second': 5.822, 'epoch': 0.08}
{'loss': 0.8599, 'grad_norm': 0.48202428221702576, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8643382787704468, 'eval_runtime': 10.8671, 'eval_samples_per_second': 92.021, 'eval_steps_per_second': 5.797, 'epoch': 0.12}
{'loss': 0.8131, 'grad_norm': 0.36538663506507874, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8571508526802063, 'eval_runtime': 10.8687, 'eval_samples_per_second': 92.007, 'eval_steps_per_second': 5.796, 'epoch': 0.16}
{'loss': 0.8252, 'grad_norm': 0.4852098524570465, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8529255390167236, 'eval_runtime': 10.8796, 'eval_samples_per_second': 91.916, 'eval_steps_per_second': 5.791, 'epoch': 0.2}
{'loss': 0.8123, 'grad_norm': 0.36921924352645874, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8417454361915588, 'eval_runtime': 10.9116, 'eval_samples_per_second': 91.646, 'eval_steps_per_second': 5.774, 'epoch': 0.24}
{'loss': 0.7964, 'grad_norm': 0.4089329242706299, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.840478777885437, 'eval_runtime': 10.9075, 'eval_samples_per_second': 91.68, 'eval_steps_per_second': 5.776, 'epoch': 0.28}
{'loss': 0.7759, 'grad_norm': 0.4290584325790405, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8347522020339966, 'eval_runtime': 10.8782, 'eval_samples_per_second': 91.927, 'eval_steps_per_second': 5.791, 'epoch': 0.32}
{'loss': 0.7821, 'grad_norm': 0.3816252648830414, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8331547975540161, 'eval_runtime': 10.8424, 'eval_samples_per_second': 92.231, 'eval_steps_per_second': 5.811, 'epoch': 0.36}
{'loss': 0.7681, 'grad_norm': 0.42062821984291077, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8292049765586853, 'eval_runtime': 10.8213, 'eval_samples_per_second': 92.41, 'eval_steps_per_second': 5.822, 'epoch': 0.4}
{'loss': 0.761, 'grad_norm': 0.4340059161186218, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8306381106376648, 'eval_runtime': 10.8121, 'eval_samples_per_second': 92.489, 'eval_steps_per_second': 5.827, 'epoch': 0.44}
{'loss': 0.76, 'grad_norm': 0.4650941789150238, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8301689624786377, 'eval_runtime': 10.8542, 'eval_samples_per_second': 92.13, 'eval_steps_per_second': 5.804, 'epoch': 0.48}
{'loss': 0.736, 'grad_norm': 0.4000590443611145, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8258212208747864, 'eval_runtime': 10.8753, 'eval_samples_per_second': 91.951, 'eval_steps_per_second': 5.793, 'epoch': 0.52}
{'loss': 0.7399, 'grad_norm': 0.4704013466835022, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8232259750366211, 'eval_runtime': 10.9211, 'eval_samples_per_second': 91.565, 'eval_steps_per_second': 5.769, 'epoch': 0.56}
{'loss': 0.7207, 'grad_norm': 0.526611864566803, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.822386622428894, 'eval_runtime': 10.9605, 'eval_samples_per_second': 91.237, 'eval_steps_per_second': 5.748, 'epoch': 0.6}
{'loss': 0.7219, 'grad_norm': 0.5003682971000671, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8183792233467102, 'eval_runtime': 10.957, 'eval_samples_per_second': 91.266, 'eval_steps_per_second': 5.75, 'epoch': 0.64}
{'loss': 0.6912, 'grad_norm': 0.44265684485435486, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8196225762367249, 'eval_runtime': 10.9703, 'eval_samples_per_second': 91.155, 'eval_steps_per_second': 5.743, 'epoch': 0.68}
{'loss': 0.6981, 'grad_norm': 0.4171730577945709, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.816648542881012, 'eval_runtime': 10.9837, 'eval_samples_per_second': 91.044, 'eval_steps_per_second': 5.736, 'epoch': 0.72}
{'loss': 0.6866, 'grad_norm': 0.48413485288619995, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.821098268032074, 'eval_runtime': 10.9728, 'eval_samples_per_second': 91.134, 'eval_steps_per_second': 5.741, 'epoch': 0.76}
{'loss': 0.6876, 'grad_norm': 0.5101029276847839, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8178092837333679, 'eval_runtime': 10.9796, 'eval_samples_per_second': 91.078, 'eval_steps_per_second': 5.738, 'epoch': 0.8}
{'loss': 0.689, 'grad_norm': 0.5619882345199585, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8164633512496948, 'eval_runtime': 10.961, 'eval_samples_per_second': 91.233, 'eval_steps_per_second': 5.748, 'epoch': 0.84}
{'loss': 0.6798, 'grad_norm': 0.5170911550521851, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8168918490409851, 'eval_runtime': 10.9747, 'eval_samples_per_second': 91.118, 'eval_steps_per_second': 5.74, 'epoch': 0.88}
{'loss': 0.652, 'grad_norm': 0.485149621963501, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8150237798690796, 'eval_runtime': 11.0059, 'eval_samples_per_second': 90.86, 'eval_steps_per_second': 5.724, 'epoch': 0.92}
{'loss': 0.6704, 'grad_norm': 0.45090949535369873, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8146881461143494, 'eval_runtime': 10.9819, 'eval_samples_per_second': 91.059, 'eval_steps_per_second': 5.737, 'epoch': 0.96}
{'loss': 0.6471, 'grad_norm': 0.5325153470039368, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8152114152908325, 'eval_runtime': 10.9844, 'eval_samples_per_second': 91.038, 'eval_steps_per_second': 5.735, 'epoch': 1.0}
{'train_runtime': 534.2784, 'train_samples_per_second': 18.715, 'train_steps_per_second': 1.17, 'train_loss': 0.7894345581054687, 'epoch': 1.0}
train_results:  {'eval_loss': [0.999902606010437, 0.8776212334632874, 0.8643382787704468, 0.8571508526802063, 0.8529255390167236, 0.8417454361915588, 0.840478777885437, 0.8347522020339966, 0.8331547975540161, 0.8292049765586853, 0.8306381106376648, 0.8301689624786377, 0.8258212208747864, 0.8232259750366211, 0.822386622428894, 0.8183792233467102, 0.8196225762367249, 0.816648542881012, 0.821098268032074, 0.8178092837333679, 0.8164633512496948, 0.8168918490409851, 0.8150237798690796, 0.8146881461143494, 0.8152114152908325], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [0.999902606010437, 0.8776212334632874, 0.8643382787704468, 0.8571508526802063, 0.8529255390167236, 0.8417454361915588, 0.840478777885437, 0.8347522020339966, 0.8331547975540161, 0.8292049765586853, 0.8306381106376648, 0.8301689624786377, 0.8258212208747864, 0.8232259750366211, 0.822386622428894, 0.8183792233467102, 0.8196225762367249, 0.816648542881012, 0.821098268032074, 0.8178092837333679, 0.8164633512496948, 0.8168918490409851, 0.8150237798690796, 0.8146881461143494, 0.8152114152908325]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.9602372646331787
current iteration best possible eval_loss (full train run):  -0.8152114152908325
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.2196 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.6143319970889696
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.6143320254123134
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.6143319975699919
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.614331997520327
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.6143319975203099
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0834, dtype=torch.float64), tensor(0.1939, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), 0, tensor(0.1170, dtype=torch.float64), tensor(0.2938, dtype=torch.float64), tensor(0.0730, dtype=torch.float64), tensor(0.0912, dtype=torch.float64), tensor(0.1056, dtype=torch.float64), 29, 0, 1, 0, 0, 0, 106, 0.06596133499829604, 20.9444759079547, 0]
normalized proposed parameters for next round by BO: [tensor(0.0834, dtype=torch.float64), tensor(0.1939, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1170, dtype=torch.float64), tensor(0.2938, dtype=torch.float64), tensor(0.0730, dtype=torch.float64), tensor(0.0912, dtype=torch.float64), tensor(0.1056, dtype=torch.float64), tensor(0.9062, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8245, dtype=torch.float64), tensor(0.6596, dtype=torch.float64), tensor(0.4363, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.083
  gsm8k: 0.194
  rowan_hellaswag: 0.042
  sciq: 0
  triviaqa: 0.117
  truthfulqa_gen: 0.294
  wikitext: 0.073
  mmlu: 0.091
  arc_challenge: 0.106

LoRA Parameters:
  lora_r: (106,)
  lora_dropout: (0.06596133499829604,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (20.9444759079547,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  106
lora dropout:  0.06596133499829604
lora alpha:  20.9444759079547
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 15,738,880 || all params: 8,046,000,128 || trainable%: 0.1956
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5653, 'grad_norm': 0.7065085172653198, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.74656343460083, 'eval_runtime': 9.1454, 'eval_samples_per_second': 109.345, 'eval_steps_per_second': 6.889, 'epoch': 0.04}
{'loss': 1.9808, 'grad_norm': 0.3006306290626526, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2335920333862305, 'eval_runtime': 9.1684, 'eval_samples_per_second': 109.07, 'eval_steps_per_second': 6.871, 'epoch': 0.08}
{'loss': 1.46, 'grad_norm': 0.4113316833972931, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.092858910560608, 'eval_runtime': 9.1933, 'eval_samples_per_second': 108.775, 'eval_steps_per_second': 6.853, 'epoch': 0.12}
{'loss': 1.3628, 'grad_norm': 0.23431353271007538, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0441770553588867, 'eval_runtime': 9.2421, 'eval_samples_per_second': 108.201, 'eval_steps_per_second': 6.817, 'epoch': 0.16}
{'loss': 1.3062, 'grad_norm': 0.2953060269355774, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.013376235961914, 'eval_runtime': 9.2468, 'eval_samples_per_second': 108.145, 'eval_steps_per_second': 6.813, 'epoch': 0.2}
{'loss': 1.2385, 'grad_norm': 0.4542999267578125, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.996091902256012, 'eval_runtime': 9.2578, 'eval_samples_per_second': 108.017, 'eval_steps_per_second': 6.805, 'epoch': 0.24}
{'loss': 1.2408, 'grad_norm': 0.2704758644104004, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9711561799049377, 'eval_runtime': 9.2513, 'eval_samples_per_second': 108.093, 'eval_steps_per_second': 6.81, 'epoch': 0.28}
{'loss': 1.2066, 'grad_norm': 0.3317030370235443, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9571966528892517, 'eval_runtime': 9.2769, 'eval_samples_per_second': 107.795, 'eval_steps_per_second': 6.791, 'epoch': 0.32}
{'loss': 1.1688, 'grad_norm': 0.2996719777584076, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.947320818901062, 'eval_runtime': 9.257, 'eval_samples_per_second': 108.027, 'eval_steps_per_second': 6.806, 'epoch': 0.36}
{'loss': 1.1605, 'grad_norm': 0.2963264286518097, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9455723166465759, 'eval_runtime': 9.2314, 'eval_samples_per_second': 108.325, 'eval_steps_per_second': 6.825, 'epoch': 0.4}
{'loss': 1.2313, 'grad_norm': 0.353365033864975, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9367390871047974, 'eval_runtime': 9.2044, 'eval_samples_per_second': 108.644, 'eval_steps_per_second': 6.845, 'epoch': 0.44}
{'loss': 1.1464, 'grad_norm': 0.3278481662273407, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9297243356704712, 'eval_runtime': 9.2132, 'eval_samples_per_second': 108.54, 'eval_steps_per_second': 6.838, 'epoch': 0.48}
{'loss': 1.134, 'grad_norm': 0.2366616129875183, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9195680618286133, 'eval_runtime': 9.2069, 'eval_samples_per_second': 108.614, 'eval_steps_per_second': 6.843, 'epoch': 0.52}
{'loss': 1.1676, 'grad_norm': 0.3527063727378845, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9164937734603882, 'eval_runtime': 9.2126, 'eval_samples_per_second': 108.547, 'eval_steps_per_second': 6.838, 'epoch': 0.56}
{'loss': 1.0991, 'grad_norm': 0.29950660467147827, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9118695259094238, 'eval_runtime': 9.1885, 'eval_samples_per_second': 108.832, 'eval_steps_per_second': 6.856, 'epoch': 0.6}
{'loss': 1.0623, 'grad_norm': 0.34678760170936584, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9052168130874634, 'eval_runtime': 9.1858, 'eval_samples_per_second': 108.863, 'eval_steps_per_second': 6.858, 'epoch': 0.64}
{'loss': 1.0676, 'grad_norm': 0.33880484104156494, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.902284562587738, 'eval_runtime': 9.1812, 'eval_samples_per_second': 108.918, 'eval_steps_per_second': 6.862, 'epoch': 0.68}
{'loss': 1.1357, 'grad_norm': 0.2870415449142456, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9005018472671509, 'eval_runtime': 9.1397, 'eval_samples_per_second': 109.413, 'eval_steps_per_second': 6.893, 'epoch': 0.72}
{'loss': 1.0939, 'grad_norm': 0.2824700176715851, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8991941213607788, 'eval_runtime': 9.1454, 'eval_samples_per_second': 109.345, 'eval_steps_per_second': 6.889, 'epoch': 0.76}
{'loss': 1.111, 'grad_norm': 0.30684202909469604, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8958808779716492, 'eval_runtime': 9.1317, 'eval_samples_per_second': 109.508, 'eval_steps_per_second': 6.899, 'epoch': 0.8}
{'loss': 1.1027, 'grad_norm': 0.3883277177810669, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.897361695766449, 'eval_runtime': 9.136, 'eval_samples_per_second': 109.457, 'eval_steps_per_second': 6.896, 'epoch': 0.84}
{'loss': 1.0931, 'grad_norm': 0.2475956678390503, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8915336728096008, 'eval_runtime': 9.1461, 'eval_samples_per_second': 109.337, 'eval_steps_per_second': 6.888, 'epoch': 0.88}
{'loss': 1.0831, 'grad_norm': 0.39462295174598694, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8934957385063171, 'eval_runtime': 9.1451, 'eval_samples_per_second': 109.348, 'eval_steps_per_second': 6.889, 'epoch': 0.92}
{'loss': 1.0998, 'grad_norm': 0.34781625866889954, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.89220130443573, 'eval_runtime': 9.1389, 'eval_samples_per_second': 109.422, 'eval_steps_per_second': 6.894, 'epoch': 0.96}
{'loss': 1.1247, 'grad_norm': 0.3077628016471863, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8920864462852478, 'eval_runtime': 9.1474, 'eval_samples_per_second': 109.321, 'eval_steps_per_second': 6.887, 'epoch': 1.0}
{'train_runtime': 413.2422, 'train_samples_per_second': 24.189, 'train_steps_per_second': 1.512, 'train_loss': 1.2976997985839844, 'epoch': 1.0}
train_results:  {'eval_loss': [1.74656343460083, 1.2335920333862305, 1.092858910560608, 1.0441770553588867, 1.013376235961914, 0.996091902256012, 0.9711561799049377, 0.9571966528892517, 0.947320818901062, 0.9455723166465759, 0.9367390871047974, 0.9297243356704712, 0.9195680618286133, 0.9164937734603882, 0.9118695259094238, 0.9052168130874634, 0.902284562587738, 0.9005018472671509, 0.8991941213607788, 0.8958808779716492, 0.897361695766449, 0.8915336728096008, 0.8934957385063171, 0.89220130443573, 0.8920864462852478], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.74656343460083, 1.2335920333862305, 1.092858910560608, 1.0441770553588867, 1.013376235961914, 0.996091902256012, 0.9711561799049377, 0.9571966528892517, 0.947320818901062, 0.9455723166465759, 0.9367390871047974, 0.9297243356704712, 0.9195680618286133, 0.9164937734603882, 0.9118695259094238, 0.9052168130874634, 0.902284562587738, 0.9005018472671509, 0.8991941213607788, 0.8958808779716492, 0.897361695766449, 0.8915336728096008, 0.8934957385063171, 0.89220130443573, 0.8920864462852478]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0404719114303589
current iteration best possible eval_loss (full train run):  -0.8920864462852478
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1966 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.7528556966600843
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.7201622894297588
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.7207413041767162
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.7137214554729461
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.7202157382703325
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2731, dtype=torch.float64), tensor(0.4060, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.2554, dtype=torch.float64), 0, 0, tensor(0.0133, dtype=torch.float64), tensor(0.0223, dtype=torch.float64), 0, 1, 0, 1, 0, 1, 1, 114, 0.1, 19.680438026574357, 0]
normalized proposed parameters for next round by BO: [tensor(0.2731, dtype=torch.float64), tensor(0.4060, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.2554, dtype=torch.float64), tensor(0.0009, dtype=torch.float64), tensor(3.0599e-18, dtype=torch.float64), tensor(0.0133, dtype=torch.float64), tensor(0.0223, dtype=torch.float64), tensor(1.9641e-18, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8940, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4100, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.273
  gsm8k: 0.406
  rowan_hellaswag: 0.029
  sciq: 0.255
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.013
  mmlu: 0.022
  arc_challenge: 0

LoRA Parameters:
  lora_r: (114,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (19.680438026574357,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  114
lora dropout:  0.1
lora alpha:  19.680438026574357
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 4,786,176 || all params: 8,035,047,424 || trainable%: 0.0596
length of training data:  9987
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4945, 'grad_norm': 0.7375766038894653, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1062681674957275, 'eval_runtime': 9.0043, 'eval_samples_per_second': 111.058, 'eval_steps_per_second': 6.997, 'epoch': 0.04}
{'loss': 2.082, 'grad_norm': 0.41229164600372314, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3115283250808716, 'eval_runtime': 9.0169, 'eval_samples_per_second': 110.903, 'eval_steps_per_second': 6.987, 'epoch': 0.08}
{'loss': 1.4435, 'grad_norm': 0.29856306314468384, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1480903625488281, 'eval_runtime': 9.0961, 'eval_samples_per_second': 109.938, 'eval_steps_per_second': 6.926, 'epoch': 0.12}
{'loss': 1.2976, 'grad_norm': 0.2255440205335617, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0856918096542358, 'eval_runtime': 9.1126, 'eval_samples_per_second': 109.738, 'eval_steps_per_second': 6.913, 'epoch': 0.16}
{'loss': 1.1965, 'grad_norm': 0.17458851635456085, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0548522472381592, 'eval_runtime': 9.1273, 'eval_samples_per_second': 109.561, 'eval_steps_per_second': 6.902, 'epoch': 0.2}
{'loss': 1.1888, 'grad_norm': 0.1868610382080078, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0367835760116577, 'eval_runtime': 9.129, 'eval_samples_per_second': 109.541, 'eval_steps_per_second': 6.901, 'epoch': 0.24}
{'loss': 1.1843, 'grad_norm': 0.17691725492477417, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0246320962905884, 'eval_runtime': 9.1659, 'eval_samples_per_second': 109.1, 'eval_steps_per_second': 6.873, 'epoch': 0.28}
{'loss': 1.1633, 'grad_norm': 0.22592397034168243, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0205503702163696, 'eval_runtime': 9.1336, 'eval_samples_per_second': 109.485, 'eval_steps_per_second': 6.898, 'epoch': 0.32}
{'loss': 1.1153, 'grad_norm': 0.20970934629440308, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0091880559921265, 'eval_runtime': 9.1386, 'eval_samples_per_second': 109.425, 'eval_steps_per_second': 6.894, 'epoch': 0.36}
{'loss': 1.1153, 'grad_norm': 0.24029922485351562, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0025553703308105, 'eval_runtime': 9.1306, 'eval_samples_per_second': 109.521, 'eval_steps_per_second': 6.9, 'epoch': 0.4}
{'loss': 1.1055, 'grad_norm': 0.187667116522789, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9991028308868408, 'eval_runtime': 9.1333, 'eval_samples_per_second': 109.489, 'eval_steps_per_second': 6.898, 'epoch': 0.44}
{'loss': 1.1266, 'grad_norm': 0.24962373077869415, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9979057908058167, 'eval_runtime': 9.1536, 'eval_samples_per_second': 109.247, 'eval_steps_per_second': 6.883, 'epoch': 0.48}
{'loss': 1.1473, 'grad_norm': 0.1504572480916977, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9940124154090881, 'eval_runtime': 9.1383, 'eval_samples_per_second': 109.429, 'eval_steps_per_second': 6.894, 'epoch': 0.52}
{'loss': 1.108, 'grad_norm': 0.18894891440868378, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9907607436180115, 'eval_runtime': 9.1471, 'eval_samples_per_second': 109.324, 'eval_steps_per_second': 6.887, 'epoch': 0.56}
{'loss': 1.1058, 'grad_norm': 0.20491382479667664, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9895716309547424, 'eval_runtime': 9.1366, 'eval_samples_per_second': 109.45, 'eval_steps_per_second': 6.895, 'epoch': 0.6}
{'loss': 1.103, 'grad_norm': 0.22911208868026733, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9895455241203308, 'eval_runtime': 9.1634, 'eval_samples_per_second': 109.13, 'eval_steps_per_second': 6.875, 'epoch': 0.64}
{'loss': 1.0653, 'grad_norm': 0.1822589933872223, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9840987920761108, 'eval_runtime': 9.1591, 'eval_samples_per_second': 109.181, 'eval_steps_per_second': 6.878, 'epoch': 0.68}
{'loss': 1.1147, 'grad_norm': 0.15363237261772156, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9838343262672424, 'eval_runtime': 9.1382, 'eval_samples_per_second': 109.431, 'eval_steps_per_second': 6.894, 'epoch': 0.72}
{'loss': 1.1153, 'grad_norm': 0.16641545295715332, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.984038233757019, 'eval_runtime': 9.1171, 'eval_samples_per_second': 109.684, 'eval_steps_per_second': 6.91, 'epoch': 0.76}
{'loss': 1.0977, 'grad_norm': 0.24152034521102905, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9816941022872925, 'eval_runtime': 9.1354, 'eval_samples_per_second': 109.465, 'eval_steps_per_second': 6.896, 'epoch': 0.8}
{'loss': 1.1043, 'grad_norm': 0.16000831127166748, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9814894795417786, 'eval_runtime': 9.1173, 'eval_samples_per_second': 109.682, 'eval_steps_per_second': 6.91, 'epoch': 0.84}
{'loss': 1.0386, 'grad_norm': 0.17027921974658966, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9808609485626221, 'eval_runtime': 9.1399, 'eval_samples_per_second': 109.41, 'eval_steps_per_second': 6.893, 'epoch': 0.88}
{'loss': 1.0863, 'grad_norm': 0.17271113395690918, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9800370335578918, 'eval_runtime': 9.1548, 'eval_samples_per_second': 109.232, 'eval_steps_per_second': 6.882, 'epoch': 0.92}
{'loss': 1.0605, 'grad_norm': 0.16404105722904205, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9788068532943726, 'eval_runtime': 9.1217, 'eval_samples_per_second': 109.628, 'eval_steps_per_second': 6.907, 'epoch': 0.96}
{'loss': 1.0593, 'grad_norm': 0.36741122603416443, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.978496789932251, 'eval_runtime': 9.1573, 'eval_samples_per_second': 109.203, 'eval_steps_per_second': 6.88, 'epoch': 1.0}
{'train_runtime': 326.8793, 'train_samples_per_second': 30.553, 'train_steps_per_second': 1.912, 'train_loss': 1.2687736328125, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1062681674957275, 1.3115283250808716, 1.1480903625488281, 1.0856918096542358, 1.0548522472381592, 1.0367835760116577, 1.0246320962905884, 1.0205503702163696, 1.0091880559921265, 1.0025553703308105, 0.9991028308868408, 0.9979057908058167, 0.9940124154090881, 0.9907607436180115, 0.9895716309547424, 0.9895455241203308, 0.9840987920761108, 0.9838343262672424, 0.984038233757019, 0.9816941022872925, 0.9814894795417786, 0.9808609485626221, 0.9800370335578918, 0.9788068532943726, 0.978496789932251], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1062681674957275, 1.3115283250808716, 1.1480903625488281, 1.0856918096542358, 1.0548522472381592, 1.0367835760116577, 1.0246320962905884, 1.0205503702163696, 1.0091880559921265, 1.0025553703308105, 0.9991028308868408, 0.9979057908058167, 0.9940124154090881, 0.9907607436180115, 0.9895716309547424, 0.9895455241203308, 0.9840987920761108, 0.9838343262672424, 0.984038233757019, 0.9816941022872925, 0.9814894795417786, 0.9808609485626221, 0.9800370335578918, 0.9788068532943726, 0.978496789932251]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.4703216552734375
current iteration best possible eval_loss (full train run):  -0.978496789932251
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8704 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.7272626984678999
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.7272626984678999
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.7272626984678999
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.7272626984678999
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.7272626984678999
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3059, dtype=torch.float64), 0, 0, 0, 0, tensor(0.1702, dtype=torch.float64), 0, tensor(0.0840, dtype=torch.float64), tensor(0.4400, dtype=torch.float64), 32, 1, 0, 0, 1, 1, 2, 3.729655473350135e-18, 38.2276265634507, 1]
normalized proposed parameters for next round by BO: [tensor(0.3059, dtype=torch.float64), tensor(2.3272e-16, dtype=torch.float64), tensor(4.5151e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.0852e-16, dtype=torch.float64), tensor(0.1702, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0840, dtype=torch.float64), tensor(0.4400, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.7297e-17, dtype=torch.float64), tensor(0.7964, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.306
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.17
  wikitext: 0
  mmlu: 0.084
  arc_challenge: 0.44

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.729655473350135e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (38.2276265634507,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  2
lora dropout:  3.729655473350135e-18
lora alpha:  38.2276265634507
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,883,584 || all params: 8,033,144,832 || trainable%: 0.0359
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8226, 'grad_norm': 4.167359828948975, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4637279510498047, 'eval_runtime': 10.5268, 'eval_samples_per_second': 94.996, 'eval_steps_per_second': 5.985, 'epoch': 0.04}
{'loss': 1.0058, 'grad_norm': 2.2840702533721924, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.366782307624817, 'eval_runtime': 10.5967, 'eval_samples_per_second': 94.369, 'eval_steps_per_second': 5.945, 'epoch': 0.08}
{'loss': 0.9408, 'grad_norm': 1.458420991897583, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3688116073608398, 'eval_runtime': 10.6078, 'eval_samples_per_second': 94.27, 'eval_steps_per_second': 5.939, 'epoch': 0.12}
{'loss': 0.8933, 'grad_norm': 1.3445755243301392, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.367875099182129, 'eval_runtime': 10.6093, 'eval_samples_per_second': 94.257, 'eval_steps_per_second': 5.938, 'epoch': 0.16}
{'loss': 0.8637, 'grad_norm': 1.4345569610595703, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4024689197540283, 'eval_runtime': 10.6154, 'eval_samples_per_second': 94.203, 'eval_steps_per_second': 5.935, 'epoch': 0.2}
{'loss': 0.8242, 'grad_norm': 1.3298877477645874, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4071110486984253, 'eval_runtime': 10.6312, 'eval_samples_per_second': 94.063, 'eval_steps_per_second': 5.926, 'epoch': 0.24}
{'loss': 0.8403, 'grad_norm': 1.3721836805343628, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4128345251083374, 'eval_runtime': 10.6397, 'eval_samples_per_second': 93.987, 'eval_steps_per_second': 5.921, 'epoch': 0.28}
{'loss': 0.7978, 'grad_norm': 1.506467580795288, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4683879613876343, 'eval_runtime': 10.6344, 'eval_samples_per_second': 94.034, 'eval_steps_per_second': 5.924, 'epoch': 0.32}
{'loss': 0.7537, 'grad_norm': 1.975985050201416, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4639711380004883, 'eval_runtime': 10.6227, 'eval_samples_per_second': 94.138, 'eval_steps_per_second': 5.931, 'epoch': 0.36}
{'loss': 0.742, 'grad_norm': 1.5415213108062744, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4864274263381958, 'eval_runtime': 10.6037, 'eval_samples_per_second': 94.307, 'eval_steps_per_second': 5.941, 'epoch': 0.4}
{'loss': 0.7638, 'grad_norm': 1.8843183517456055, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4559147357940674, 'eval_runtime': 10.6239, 'eval_samples_per_second': 94.128, 'eval_steps_per_second': 5.93, 'epoch': 0.44}
{'loss': 0.7446, 'grad_norm': 1.9757096767425537, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4772346019744873, 'eval_runtime': 10.6432, 'eval_samples_per_second': 93.956, 'eval_steps_per_second': 5.919, 'epoch': 0.48}
{'loss': 0.7215, 'grad_norm': 1.8805614709854126, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4751554727554321, 'eval_runtime': 10.6231, 'eval_samples_per_second': 94.135, 'eval_steps_per_second': 5.93, 'epoch': 0.52}
{'loss': 0.6524, 'grad_norm': 1.689266562461853, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4938466548919678, 'eval_runtime': 10.5974, 'eval_samples_per_second': 94.363, 'eval_steps_per_second': 5.945, 'epoch': 0.56}
{'loss': 0.6685, 'grad_norm': 2.318655014038086, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4876630306243896, 'eval_runtime': 10.6087, 'eval_samples_per_second': 94.262, 'eval_steps_per_second': 5.939, 'epoch': 0.6}
{'loss': 0.6971, 'grad_norm': 2.1637837886810303, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4839177131652832, 'eval_runtime': 10.6175, 'eval_samples_per_second': 94.185, 'eval_steps_per_second': 5.934, 'epoch': 0.64}
{'loss': 0.6168, 'grad_norm': 2.052246332168579, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4855200052261353, 'eval_runtime': 10.7013, 'eval_samples_per_second': 93.447, 'eval_steps_per_second': 5.887, 'epoch': 0.68}
{'loss': 0.6132, 'grad_norm': 1.7980027198791504, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5136938095092773, 'eval_runtime': 10.716, 'eval_samples_per_second': 93.319, 'eval_steps_per_second': 5.879, 'epoch': 0.72}
{'loss': 0.6755, 'grad_norm': 2.0494258403778076, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.504799246788025, 'eval_runtime': 10.7101, 'eval_samples_per_second': 93.369, 'eval_steps_per_second': 5.882, 'epoch': 0.76}
{'loss': 0.6, 'grad_norm': 2.547999143600464, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5199521780014038, 'eval_runtime': 10.7165, 'eval_samples_per_second': 93.314, 'eval_steps_per_second': 5.879, 'epoch': 0.8}
{'loss': 0.5991, 'grad_norm': 2.430997848510742, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5438369512557983, 'eval_runtime': 10.718, 'eval_samples_per_second': 93.301, 'eval_steps_per_second': 5.878, 'epoch': 0.84}
{'loss': 0.5832, 'grad_norm': 1.8890000581741333, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.560023546218872, 'eval_runtime': 10.7256, 'eval_samples_per_second': 93.235, 'eval_steps_per_second': 5.874, 'epoch': 0.88}
{'loss': 0.5793, 'grad_norm': 1.7409522533416748, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5587167739868164, 'eval_runtime': 10.7227, 'eval_samples_per_second': 93.26, 'eval_steps_per_second': 5.875, 'epoch': 0.92}
{'loss': 0.5939, 'grad_norm': 1.5850309133529663, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5564790964126587, 'eval_runtime': 10.6623, 'eval_samples_per_second': 93.788, 'eval_steps_per_second': 5.909, 'epoch': 0.96}
{'loss': 0.5947, 'grad_norm': 2.115403413772583, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5557492971420288, 'eval_runtime': 10.6596, 'eval_samples_per_second': 93.812, 'eval_steps_per_second': 5.91, 'epoch': 1.0}
{'train_runtime': 450.8814, 'train_samples_per_second': 22.174, 'train_steps_per_second': 1.386, 'train_loss': 0.807519367980957, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4637279510498047, 1.366782307624817, 1.3688116073608398, 1.367875099182129, 1.4024689197540283, 1.4071110486984253, 1.4128345251083374, 1.4683879613876343, 1.4639711380004883, 1.4864274263381958, 1.4559147357940674, 1.4772346019744873, 1.4751554727554321, 1.4938466548919678, 1.4876630306243896, 1.4839177131652832, 1.4855200052261353, 1.5136938095092773, 1.504799246788025, 1.5199521780014038, 1.5438369512557983, 1.560023546218872, 1.5587167739868164, 1.5564790964126587, 1.5557492971420288], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4637279510498047, 1.366782307624817, 1.3688116073608398, 1.367875099182129, 1.4024689197540283, 1.4071110486984253, 1.4128345251083374, 1.4683879613876343, 1.4639711380004883, 1.4864274263381958, 1.4559147357940674, 1.4772346019744873, 1.4751554727554321, 1.4938466548919678, 1.4876630306243896, 1.4839177131652832, 1.4855200052261353, 1.5136938095092773, 1.504799246788025, 1.5199521780014038, 1.5438369512557983, 1.560023546218872, 1.5587167739868164, 1.5564790964126587, 1.5557492971420288]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.1625874042510986
current iteration best possible eval_loss (full train run):  -1.5557492971420288
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.5961 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.7520195374182167
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.752019535394007
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.7520195508057371
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.752014107640819
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.7522827681457377
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2064, dtype=torch.float64), tensor(0.3256, dtype=torch.float64), 0, tensor(0.1777, dtype=torch.float64), tensor(0.0148, dtype=torch.float64), tensor(0.0573, dtype=torch.float64), 0, 0, tensor(0.2182, dtype=torch.float64), 10, 0, 1, 0, 1, 1, 63, 0.08559619888385107, 22.887211590576612, 1]
normalized proposed parameters for next round by BO: [tensor(0.2064, dtype=torch.float64), tensor(0.3256, dtype=torch.float64), tensor(6.5983e-18, dtype=torch.float64), tensor(0.1777, dtype=torch.float64), tensor(0.0148, dtype=torch.float64), tensor(0.0573, dtype=torch.float64), tensor(5.0189e-18, dtype=torch.float64), tensor(4.2163e-18, dtype=torch.float64), tensor(0.2182, dtype=torch.float64), tensor(0.3212, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4918, dtype=torch.float64), tensor(0.8560, dtype=torch.float64), tensor(0.4768, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.206
  gsm8k: 0.326
  rowan_hellaswag: 0
  sciq: 0.178
  triviaqa: 0.015
  truthfulqa_gen: 0.057
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.218

LoRA Parameters:
  lora_r: (63,)
  lora_dropout: (0.08559619888385107,)
  num_layers_to_apply: (10,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (22.887211590576612,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  10
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  63
lora dropout:  0.08559619888385107
lora alpha:  22.887211590576612
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 26,449,920 || all params: 8,056,711,168 || trainable%: 0.3283
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9378, 'grad_norm': 1.2195295095443726, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4900466203689575, 'eval_runtime': 9.5318, 'eval_samples_per_second': 104.912, 'eval_steps_per_second': 6.609, 'epoch': 0.04}
{'loss': 1.4129, 'grad_norm': 0.8586015105247498, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.063295841217041, 'eval_runtime': 9.58, 'eval_samples_per_second': 104.384, 'eval_steps_per_second': 6.576, 'epoch': 0.08}
{'loss': 1.0553, 'grad_norm': 0.3285844027996063, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9719259738922119, 'eval_runtime': 9.5741, 'eval_samples_per_second': 104.448, 'eval_steps_per_second': 6.58, 'epoch': 0.12}
{'loss': 0.9865, 'grad_norm': 0.29476913809776306, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9468993544578552, 'eval_runtime': 9.5878, 'eval_samples_per_second': 104.299, 'eval_steps_per_second': 6.571, 'epoch': 0.16}
{'loss': 0.9386, 'grad_norm': 0.3175489604473114, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9199745655059814, 'eval_runtime': 9.6249, 'eval_samples_per_second': 103.897, 'eval_steps_per_second': 6.546, 'epoch': 0.2}
{'loss': 0.9402, 'grad_norm': 0.27898135781288147, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.910274863243103, 'eval_runtime': 9.619, 'eval_samples_per_second': 103.961, 'eval_steps_per_second': 6.55, 'epoch': 0.24}
{'loss': 0.9214, 'grad_norm': 0.285612016916275, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9034229516983032, 'eval_runtime': 9.6126, 'eval_samples_per_second': 104.031, 'eval_steps_per_second': 6.554, 'epoch': 0.28}
{'loss': 0.8964, 'grad_norm': 0.25696447491645813, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8916588425636292, 'eval_runtime': 9.6104, 'eval_samples_per_second': 104.054, 'eval_steps_per_second': 6.555, 'epoch': 0.32}
{'loss': 0.8872, 'grad_norm': 0.27796289324760437, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8961514830589294, 'eval_runtime': 9.6022, 'eval_samples_per_second': 104.143, 'eval_steps_per_second': 6.561, 'epoch': 0.36}
{'loss': 0.8813, 'grad_norm': 0.2418198138475418, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8862523436546326, 'eval_runtime': 9.6058, 'eval_samples_per_second': 104.104, 'eval_steps_per_second': 6.559, 'epoch': 0.4}
{'loss': 0.8689, 'grad_norm': 0.23895810544490814, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8860540390014648, 'eval_runtime': 9.6092, 'eval_samples_per_second': 104.067, 'eval_steps_per_second': 6.556, 'epoch': 0.44}
{'loss': 0.8844, 'grad_norm': 0.23772753775119781, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8806569576263428, 'eval_runtime': 9.5858, 'eval_samples_per_second': 104.321, 'eval_steps_per_second': 6.572, 'epoch': 0.48}
{'loss': 0.8607, 'grad_norm': 0.2816798985004425, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8775598406791687, 'eval_runtime': 9.7013, 'eval_samples_per_second': 103.079, 'eval_steps_per_second': 6.494, 'epoch': 0.52}
{'loss': 0.8636, 'grad_norm': 0.23517729341983795, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8743210434913635, 'eval_runtime': 9.679, 'eval_samples_per_second': 103.316, 'eval_steps_per_second': 6.509, 'epoch': 0.56}
{'loss': 0.8594, 'grad_norm': 0.2111082822084427, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8718536496162415, 'eval_runtime': 9.6909, 'eval_samples_per_second': 103.19, 'eval_steps_per_second': 6.501, 'epoch': 0.6}
{'loss': 0.8438, 'grad_norm': 0.24100197851657867, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.871248185634613, 'eval_runtime': 9.6672, 'eval_samples_per_second': 103.443, 'eval_steps_per_second': 6.517, 'epoch': 0.64}
{'loss': 0.8401, 'grad_norm': 0.2480638027191162, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8693611025810242, 'eval_runtime': 9.6351, 'eval_samples_per_second': 103.787, 'eval_steps_per_second': 6.539, 'epoch': 0.68}
{'loss': 0.8629, 'grad_norm': 0.3026583194732666, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8670817613601685, 'eval_runtime': 9.6088, 'eval_samples_per_second': 104.071, 'eval_steps_per_second': 6.556, 'epoch': 0.72}
{'loss': 0.8365, 'grad_norm': 0.23340019583702087, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8640332221984863, 'eval_runtime': 9.6223, 'eval_samples_per_second': 103.925, 'eval_steps_per_second': 6.547, 'epoch': 0.76}
{'loss': 0.8605, 'grad_norm': 0.24254703521728516, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8637230396270752, 'eval_runtime': 9.6245, 'eval_samples_per_second': 103.901, 'eval_steps_per_second': 6.546, 'epoch': 0.8}
{'loss': 0.8487, 'grad_norm': 0.21588000655174255, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8635789752006531, 'eval_runtime': 9.6228, 'eval_samples_per_second': 103.92, 'eval_steps_per_second': 6.547, 'epoch': 0.84}
{'loss': 0.8475, 'grad_norm': 0.22304029762744904, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8641570210456848, 'eval_runtime': 9.6089, 'eval_samples_per_second': 104.071, 'eval_steps_per_second': 6.556, 'epoch': 0.88}
{'loss': 0.841, 'grad_norm': 0.24926355481147766, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8613131046295166, 'eval_runtime': 9.6152, 'eval_samples_per_second': 104.002, 'eval_steps_per_second': 6.552, 'epoch': 0.92}
{'loss': 0.8527, 'grad_norm': 0.26000022888183594, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8617581725120544, 'eval_runtime': 9.5802, 'eval_samples_per_second': 104.382, 'eval_steps_per_second': 6.576, 'epoch': 0.96}
{'loss': 0.8431, 'grad_norm': 0.2824450731277466, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8608273863792419, 'eval_runtime': 9.5556, 'eval_samples_per_second': 104.651, 'eval_steps_per_second': 6.593, 'epoch': 1.0}
{'train_runtime': 445.1388, 'train_samples_per_second': 22.458, 'train_steps_per_second': 1.404, 'train_loss': 0.9868559265136718, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4900466203689575, 1.063295841217041, 0.9719259738922119, 0.9468993544578552, 0.9199745655059814, 0.910274863243103, 0.9034229516983032, 0.8916588425636292, 0.8961514830589294, 0.8862523436546326, 0.8860540390014648, 0.8806569576263428, 0.8775598406791687, 0.8743210434913635, 0.8718536496162415, 0.871248185634613, 0.8693611025810242, 0.8670817613601685, 0.8640332221984863, 0.8637230396270752, 0.8635789752006531, 0.8641570210456848, 0.8613131046295166, 0.8617581725120544, 0.8608273863792419], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4900466203689575, 1.063295841217041, 0.9719259738922119, 0.9468993544578552, 0.9199745655059814, 0.910274863243103, 0.9034229516983032, 0.8916588425636292, 0.8961514830589294, 0.8862523436546326, 0.8860540390014648, 0.8806569576263428, 0.8775598406791687, 0.8743210434913635, 0.8718536496162415, 0.871248185634613, 0.8693611025810242, 0.8670817613601685, 0.8640332221984863, 0.8637230396270752, 0.8635789752006531, 0.8641570210456848, 0.8613131046295166, 0.8617581725120544, 0.8608273863792419]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2998933792114258
current iteration best possible eval_loss (full train run):  -0.8608273863792419
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.4546 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.8050841092783316
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.794083815039018
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.8050841765011351
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.805072512989905
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.8050840817208232
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.5372, dtype=torch.float64), 0, 0, tensor(0.0810, dtype=torch.float64), 0, 0, tensor(0.0127, dtype=torch.float64), tensor(0.1726, dtype=torch.float64), tensor(0.1965, dtype=torch.float64), 14, 1, 1, 0, 0, 1, 6, 1.7996404748216113e-18, 24.536853542414534, 0]
normalized proposed parameters for next round by BO: [tensor(0.5372, dtype=torch.float64), tensor(7.5864e-18, dtype=torch.float64), tensor(1.8994e-19, dtype=torch.float64), tensor(0.0810, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.5566e-19, dtype=torch.float64), tensor(0.0127, dtype=torch.float64), tensor(0.1726, dtype=torch.float64), tensor(0.1965, dtype=torch.float64), tensor(0.4226, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0463, dtype=torch.float64), tensor(1.7996e-17, dtype=torch.float64), tensor(0.5112, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.537
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.081
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.013
  mmlu: 0.173
  arc_challenge: 0.196

LoRA Parameters:
  lora_r: (6,)
  lora_dropout: (1.7996404748216113e-18,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (24.536853542414534,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  6
lora dropout:  1.7996404748216113e-18
lora alpha:  24.536853542414534
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,666,496 || all params: 8,032,927,744 || trainable%: 0.0332
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.7133, 'grad_norm': 2.008225679397583, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6832075119018555, 'eval_runtime': 9.3587, 'eval_samples_per_second': 106.852, 'eval_steps_per_second': 6.732, 'epoch': 0.04}
{'loss': 1.6683, 'grad_norm': 0.6833177208900452, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4861568212509155, 'eval_runtime': 9.3461, 'eval_samples_per_second': 106.996, 'eval_steps_per_second': 6.741, 'epoch': 0.08}
{'loss': 1.3082, 'grad_norm': 0.5226571559906006, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4770400524139404, 'eval_runtime': 9.3791, 'eval_samples_per_second': 106.62, 'eval_steps_per_second': 6.717, 'epoch': 0.12}
{'loss': 1.2195, 'grad_norm': 0.6418577432632446, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4081816673278809, 'eval_runtime': 9.4336, 'eval_samples_per_second': 106.004, 'eval_steps_per_second': 6.678, 'epoch': 0.16}
{'loss': 1.1473, 'grad_norm': 0.6120383143424988, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4060003757476807, 'eval_runtime': 9.4828, 'eval_samples_per_second': 105.454, 'eval_steps_per_second': 6.644, 'epoch': 0.2}
{'loss': 1.0428, 'grad_norm': 0.7329456806182861, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4034547805786133, 'eval_runtime': 9.5096, 'eval_samples_per_second': 105.157, 'eval_steps_per_second': 6.625, 'epoch': 0.24}
{'loss': 1.0575, 'grad_norm': 0.6250157356262207, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.407135009765625, 'eval_runtime': 9.5303, 'eval_samples_per_second': 104.929, 'eval_steps_per_second': 6.611, 'epoch': 0.28}
{'loss': 1.0444, 'grad_norm': 0.5512191653251648, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4172191619873047, 'eval_runtime': 9.5422, 'eval_samples_per_second': 104.798, 'eval_steps_per_second': 6.602, 'epoch': 0.32}
{'loss': 1.0579, 'grad_norm': 0.6069541573524475, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4020839929580688, 'eval_runtime': 9.542, 'eval_samples_per_second': 104.8, 'eval_steps_per_second': 6.602, 'epoch': 0.36}
{'loss': 1.0014, 'grad_norm': 0.588154673576355, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.394874930381775, 'eval_runtime': 9.5553, 'eval_samples_per_second': 104.654, 'eval_steps_per_second': 6.593, 'epoch': 0.4}
{'loss': 0.9612, 'grad_norm': 0.5853562355041504, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4090737104415894, 'eval_runtime': 9.5621, 'eval_samples_per_second': 104.579, 'eval_steps_per_second': 6.588, 'epoch': 0.44}
{'loss': 1.0308, 'grad_norm': 0.6523728966712952, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4174375534057617, 'eval_runtime': 9.5911, 'eval_samples_per_second': 104.263, 'eval_steps_per_second': 6.569, 'epoch': 0.48}
{'loss': 0.9953, 'grad_norm': 0.5949349403381348, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4192475080490112, 'eval_runtime': 9.5982, 'eval_samples_per_second': 104.186, 'eval_steps_per_second': 6.564, 'epoch': 0.52}
{'loss': 1.0223, 'grad_norm': 0.6655742526054382, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.413314938545227, 'eval_runtime': 9.6184, 'eval_samples_per_second': 103.967, 'eval_steps_per_second': 6.55, 'epoch': 0.56}
{'loss': 0.9636, 'grad_norm': 0.6609668731689453, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.418337106704712, 'eval_runtime': 9.6158, 'eval_samples_per_second': 103.996, 'eval_steps_per_second': 6.552, 'epoch': 0.6}
{'loss': 0.9771, 'grad_norm': 0.68300461769104, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4104735851287842, 'eval_runtime': 9.622, 'eval_samples_per_second': 103.928, 'eval_steps_per_second': 6.547, 'epoch': 0.64}
{'loss': 0.9451, 'grad_norm': 0.7136871218681335, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4096530675888062, 'eval_runtime': 9.6214, 'eval_samples_per_second': 103.935, 'eval_steps_per_second': 6.548, 'epoch': 0.68}
{'loss': 0.9988, 'grad_norm': 0.6313570737838745, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4094418287277222, 'eval_runtime': 9.6081, 'eval_samples_per_second': 104.079, 'eval_steps_per_second': 6.557, 'epoch': 0.72}
{'loss': 0.9167, 'grad_norm': 0.7514051198959351, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4163490533828735, 'eval_runtime': 9.6287, 'eval_samples_per_second': 103.856, 'eval_steps_per_second': 6.543, 'epoch': 0.76}
{'loss': 1.0221, 'grad_norm': 0.7028525471687317, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4203957319259644, 'eval_runtime': 9.6297, 'eval_samples_per_second': 103.845, 'eval_steps_per_second': 6.542, 'epoch': 0.8}
{'loss': 0.9903, 'grad_norm': 0.6106595396995544, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4129948616027832, 'eval_runtime': 9.6135, 'eval_samples_per_second': 104.02, 'eval_steps_per_second': 6.553, 'epoch': 0.84}
{'loss': 0.9752, 'grad_norm': 0.694509744644165, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4287714958190918, 'eval_runtime': 9.6281, 'eval_samples_per_second': 103.863, 'eval_steps_per_second': 6.543, 'epoch': 0.88}
{'loss': 0.9764, 'grad_norm': 0.6677131056785583, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.427547812461853, 'eval_runtime': 9.604, 'eval_samples_per_second': 104.123, 'eval_steps_per_second': 6.56, 'epoch': 0.92}
{'loss': 0.9682, 'grad_norm': 0.7090554237365723, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4277961254119873, 'eval_runtime': 9.6005, 'eval_samples_per_second': 104.162, 'eval_steps_per_second': 6.562, 'epoch': 0.96}
{'loss': 0.9102, 'grad_norm': 0.7889277935028076, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4310176372528076, 'eval_runtime': 9.5926, 'eval_samples_per_second': 104.247, 'eval_steps_per_second': 6.568, 'epoch': 1.0}
{'train_runtime': 359.5622, 'train_samples_per_second': 27.803, 'train_steps_per_second': 1.738, 'train_loss': 1.1565692840576172, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6832075119018555, 1.4861568212509155, 1.4770400524139404, 1.4081816673278809, 1.4060003757476807, 1.4034547805786133, 1.407135009765625, 1.4172191619873047, 1.4020839929580688, 1.394874930381775, 1.4090737104415894, 1.4174375534057617, 1.4192475080490112, 1.413314938545227, 1.418337106704712, 1.4104735851287842, 1.4096530675888062, 1.4094418287277222, 1.4163490533828735, 1.4203957319259644, 1.4129948616027832, 1.4287714958190918, 1.427547812461853, 1.4277961254119873, 1.4310176372528076], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6832075119018555, 1.4861568212509155, 1.4770400524139404, 1.4081816673278809, 1.4060003757476807, 1.4034547805786133, 1.407135009765625, 1.4172191619873047, 1.4020839929580688, 1.394874930381775, 1.4090737104415894, 1.4174375534057617, 1.4192475080490112, 1.413314938545227, 1.418337106704712, 1.4104735851287842, 1.4096530675888062, 1.4094418287277222, 1.4163490533828735, 1.4203957319259644, 1.4129948616027832, 1.4287714958190918, 1.427547812461853, 1.4277961254119873, 1.4310176372528076]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.8948516845703125
current iteration best possible eval_loss (full train run):  -1.4310176372528076
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.6591 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.8296482646491286
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.8250896765450079
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.8296481301919157
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.8296482538991395
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.8299305317868662
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3033, dtype=torch.float64), 0, tensor(0.2523, dtype=torch.float64), tensor(0.0563, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(0.0990, dtype=torch.float64), tensor(0.0138, dtype=torch.float64), tensor(0.2632, dtype=torch.float64), 3, 0, 0, 0, 1, 1, 37, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(3.6421e-17, dtype=torch.float64), tensor(0.3033, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2523, dtype=torch.float64), tensor(0.0563, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(0.0990, dtype=torch.float64), tensor(0.0138, dtype=torch.float64), tensor(0.2632, dtype=torch.float64), tensor(0.0992, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2859, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.303
  rowan_hellaswag: 0
  sciq: 0.252
  triviaqa: 0.056
  truthfulqa_gen: 0.012
  wikitext: 0.099
  mmlu: 0.014
  arc_challenge: 0.263

LoRA Parameters:
  lora_r: (37,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (3,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  3
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  37
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 4,091,904 || all params: 8,034,353,152 || trainable%: 0.0509
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5486, 'grad_norm': 0.7158293724060059, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4066946506500244, 'eval_runtime': 9.0406, 'eval_samples_per_second': 110.612, 'eval_steps_per_second': 6.969, 'epoch': 0.04}
{'loss': 2.7912, 'grad_norm': 1.0979446172714233, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.7620954513549805, 'eval_runtime': 9.0649, 'eval_samples_per_second': 110.316, 'eval_steps_per_second': 6.95, 'epoch': 0.08}
{'loss': 2.0261, 'grad_norm': 0.46076151728630066, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.373726725578308, 'eval_runtime': 9.077, 'eval_samples_per_second': 110.169, 'eval_steps_per_second': 6.941, 'epoch': 0.12}
{'loss': 1.6194, 'grad_norm': 0.46600568294525146, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2736785411834717, 'eval_runtime': 9.1366, 'eval_samples_per_second': 109.45, 'eval_steps_per_second': 6.895, 'epoch': 0.16}
{'loss': 1.4254, 'grad_norm': 0.38603511452674866, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2071059942245483, 'eval_runtime': 9.1336, 'eval_samples_per_second': 109.486, 'eval_steps_per_second': 6.898, 'epoch': 0.2}
{'loss': 1.3171, 'grad_norm': 0.48636579513549805, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1359821557998657, 'eval_runtime': 9.1201, 'eval_samples_per_second': 109.649, 'eval_steps_per_second': 6.908, 'epoch': 0.24}
{'loss': 1.2782, 'grad_norm': 0.379190593957901, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1012424230575562, 'eval_runtime': 9.0925, 'eval_samples_per_second': 109.981, 'eval_steps_per_second': 6.929, 'epoch': 0.28}
{'loss': 1.2387, 'grad_norm': 0.2957950830459595, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0651658773422241, 'eval_runtime': 9.1139, 'eval_samples_per_second': 109.722, 'eval_steps_per_second': 6.912, 'epoch': 0.32}
{'loss': 1.2257, 'grad_norm': 0.3727037310600281, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0474425554275513, 'eval_runtime': 9.1473, 'eval_samples_per_second': 109.322, 'eval_steps_per_second': 6.887, 'epoch': 0.36}
{'loss': 1.2145, 'grad_norm': 0.31199032068252563, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0261881351470947, 'eval_runtime': 9.1777, 'eval_samples_per_second': 108.96, 'eval_steps_per_second': 6.864, 'epoch': 0.4}
{'loss': 1.2052, 'grad_norm': 0.6124671101570129, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.018896222114563, 'eval_runtime': 9.1543, 'eval_samples_per_second': 109.238, 'eval_steps_per_second': 6.882, 'epoch': 0.44}
{'loss': 1.1411, 'grad_norm': 0.20236709713935852, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0090497732162476, 'eval_runtime': 9.1528, 'eval_samples_per_second': 109.256, 'eval_steps_per_second': 6.883, 'epoch': 0.48}
{'loss': 1.1406, 'grad_norm': 0.2587103843688965, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0022165775299072, 'eval_runtime': 9.1362, 'eval_samples_per_second': 109.455, 'eval_steps_per_second': 6.896, 'epoch': 0.52}
{'loss': 1.18, 'grad_norm': 0.21585874259471893, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9935345649719238, 'eval_runtime': 9.1493, 'eval_samples_per_second': 109.298, 'eval_steps_per_second': 6.886, 'epoch': 0.56}
{'loss': 1.1006, 'grad_norm': 0.2777404189109802, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.987436830997467, 'eval_runtime': 9.1395, 'eval_samples_per_second': 109.416, 'eval_steps_per_second': 6.893, 'epoch': 0.6}
{'loss': 1.1617, 'grad_norm': 0.3774634301662445, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.978286862373352, 'eval_runtime': 9.1535, 'eval_samples_per_second': 109.248, 'eval_steps_per_second': 6.883, 'epoch': 0.64}
{'loss': 1.1213, 'grad_norm': 0.21164371073246002, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9763067364692688, 'eval_runtime': 9.1573, 'eval_samples_per_second': 109.203, 'eval_steps_per_second': 6.88, 'epoch': 0.68}
{'loss': 1.1098, 'grad_norm': 0.2681438624858856, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.966403067111969, 'eval_runtime': 9.1856, 'eval_samples_per_second': 108.866, 'eval_steps_per_second': 6.859, 'epoch': 0.72}
{'loss': 1.083, 'grad_norm': 0.2892443835735321, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9619489312171936, 'eval_runtime': 9.1966, 'eval_samples_per_second': 108.736, 'eval_steps_per_second': 6.85, 'epoch': 0.76}
{'loss': 1.0681, 'grad_norm': 0.3145580291748047, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9541571140289307, 'eval_runtime': 9.1956, 'eval_samples_per_second': 108.747, 'eval_steps_per_second': 6.851, 'epoch': 0.8}
{'loss': 1.0891, 'grad_norm': 0.3927556574344635, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9547984004020691, 'eval_runtime': 9.2208, 'eval_samples_per_second': 108.45, 'eval_steps_per_second': 6.832, 'epoch': 0.84}
{'loss': 1.1005, 'grad_norm': 0.20836886763572693, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9488024711608887, 'eval_runtime': 9.2158, 'eval_samples_per_second': 108.51, 'eval_steps_per_second': 6.836, 'epoch': 0.88}
{'loss': 1.1264, 'grad_norm': 0.3623672425746918, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9493445158004761, 'eval_runtime': 9.2273, 'eval_samples_per_second': 108.374, 'eval_steps_per_second': 6.828, 'epoch': 0.92}
{'loss': 1.1076, 'grad_norm': 0.2689577341079712, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9496229290962219, 'eval_runtime': 9.1874, 'eval_samples_per_second': 108.845, 'eval_steps_per_second': 6.857, 'epoch': 0.96}
{'loss': 1.0606, 'grad_norm': 0.21061904728412628, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.948711633682251, 'eval_runtime': 9.197, 'eval_samples_per_second': 108.731, 'eval_steps_per_second': 6.85, 'epoch': 1.0}
{'train_runtime': 416.7562, 'train_samples_per_second': 23.985, 'train_steps_per_second': 1.5, 'train_loss': 1.379215850830078, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4066946506500244, 1.7620954513549805, 1.373726725578308, 1.2736785411834717, 1.2071059942245483, 1.1359821557998657, 1.1012424230575562, 1.0651658773422241, 1.0474425554275513, 1.0261881351470947, 1.018896222114563, 1.0090497732162476, 1.0022165775299072, 0.9935345649719238, 0.987436830997467, 0.978286862373352, 0.9763067364692688, 0.966403067111969, 0.9619489312171936, 0.9541571140289307, 0.9547984004020691, 0.9488024711608887, 0.9493445158004761, 0.9496229290962219, 0.948711633682251], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.4066946506500244, 1.7620954513549805, 1.373726725578308, 1.2736785411834717, 1.2071059942245483, 1.1359821557998657, 1.1012424230575562, 1.0651658773422241, 1.0474425554275513, 1.0261881351470947, 1.018896222114563, 1.0090497732162476, 1.0022165775299072, 0.9935345649719238, 0.987436830997467, 0.978286862373352, 0.9763067364692688, 0.966403067111969, 0.9619489312171936, 0.9541571140289307, 0.9547984004020691, 0.9488024711608887, 0.9493445158004761, 0.9496229290962219, 0.948711633682251]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3337985277175903
current iteration best possible eval_loss (full train run):  -0.948711633682251
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.8802 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.8185278292556895
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.8185278292556895
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.8185278292556895
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.8185278292556895
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.8185278292556895
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.3871, dtype=torch.float64), 0, tensor(0.0493, dtype=torch.float64), tensor(0.3841, dtype=torch.float64), 0, tensor(0.1796, dtype=torch.float64), 0, 28, 0, 1, 0, 1, 1, 128, 2.3420200024815864e-20, 14.728685001823683, 1]
normalized proposed parameters for next round by BO: [tensor(2.5075e-17, dtype=torch.float64), tensor(4.3297e-19, dtype=torch.float64), tensor(0.3871, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0493, dtype=torch.float64), tensor(0.3841, dtype=torch.float64), tensor(1.9730e-19, dtype=torch.float64), tensor(0.1796, dtype=torch.float64), tensor(1.3484e-18, dtype=torch.float64), tensor(0.8901, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.3420e-19, dtype=torch.float64), tensor(0.3068, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.387
  sciq: 0
  triviaqa: 0.049
  truthfulqa_gen: 0.384
  wikitext: 0
  mmlu: 0.18
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.3420200024815864e-20,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (14.728685001823683,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  2.3420200024815864e-20
lora alpha:  14.728685001823683
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 150,470,656 || all params: 8,180,731,904 || trainable%: 1.8393
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5673, 'grad_norm': 0.7435179352760315, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5554981231689453, 'eval_runtime': 10.1824, 'eval_samples_per_second': 98.209, 'eval_steps_per_second': 6.187, 'epoch': 0.04}
{'loss': 1.9576, 'grad_norm': 0.2908836305141449, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4891471862792969, 'eval_runtime': 10.1927, 'eval_samples_per_second': 98.109, 'eval_steps_per_second': 6.181, 'epoch': 0.08}
{'loss': 1.7089, 'grad_norm': 0.2166992723941803, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4469178915023804, 'eval_runtime': 10.2363, 'eval_samples_per_second': 97.692, 'eval_steps_per_second': 6.155, 'epoch': 0.12}
{'loss': 1.5903, 'grad_norm': 0.17063985764980316, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.400815486907959, 'eval_runtime': 10.2754, 'eval_samples_per_second': 97.32, 'eval_steps_per_second': 6.131, 'epoch': 0.16}
{'loss': 1.5433, 'grad_norm': 0.16410879790782928, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3921875953674316, 'eval_runtime': 10.269, 'eval_samples_per_second': 97.38, 'eval_steps_per_second': 6.135, 'epoch': 0.2}
{'loss': 1.5168, 'grad_norm': 0.1976502388715744, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3710089921951294, 'eval_runtime': 10.2476, 'eval_samples_per_second': 97.584, 'eval_steps_per_second': 6.148, 'epoch': 0.24}
{'loss': 1.4895, 'grad_norm': 0.1627793312072754, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3847659826278687, 'eval_runtime': 10.2371, 'eval_samples_per_second': 97.684, 'eval_steps_per_second': 6.154, 'epoch': 0.28}
{'loss': 1.4497, 'grad_norm': 0.15214593708515167, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4017412662506104, 'eval_runtime': 10.2201, 'eval_samples_per_second': 97.846, 'eval_steps_per_second': 6.164, 'epoch': 0.32}
{'loss': 1.4393, 'grad_norm': 0.16675607860088348, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3874726295471191, 'eval_runtime': 10.2784, 'eval_samples_per_second': 97.292, 'eval_steps_per_second': 6.129, 'epoch': 0.36}
{'loss': 1.4579, 'grad_norm': 0.2123354971408844, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.390811800956726, 'eval_runtime': 10.2809, 'eval_samples_per_second': 97.267, 'eval_steps_per_second': 6.128, 'epoch': 0.4}
{'loss': 1.4758, 'grad_norm': 0.183625265955925, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3704450130462646, 'eval_runtime': 10.2787, 'eval_samples_per_second': 97.288, 'eval_steps_per_second': 6.129, 'epoch': 0.44}
{'loss': 1.453, 'grad_norm': 0.1728738397359848, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3879482746124268, 'eval_runtime': 10.2734, 'eval_samples_per_second': 97.339, 'eval_steps_per_second': 6.132, 'epoch': 0.48}
{'loss': 1.4541, 'grad_norm': 0.15369439125061035, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3867237567901611, 'eval_runtime': 10.3016, 'eval_samples_per_second': 97.072, 'eval_steps_per_second': 6.116, 'epoch': 0.52}
{'loss': 1.4116, 'grad_norm': 0.1846640557050705, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.393612027168274, 'eval_runtime': 10.2847, 'eval_samples_per_second': 97.232, 'eval_steps_per_second': 6.126, 'epoch': 0.56}
{'loss': 1.4304, 'grad_norm': 0.1650671362876892, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3958040475845337, 'eval_runtime': 10.3007, 'eval_samples_per_second': 97.08, 'eval_steps_per_second': 6.116, 'epoch': 0.6}
{'loss': 1.3551, 'grad_norm': 0.14376792311668396, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3967152833938599, 'eval_runtime': 10.2995, 'eval_samples_per_second': 97.092, 'eval_steps_per_second': 6.117, 'epoch': 0.64}
{'loss': 1.3665, 'grad_norm': 0.15128564834594727, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3952484130859375, 'eval_runtime': 10.2819, 'eval_samples_per_second': 97.259, 'eval_steps_per_second': 6.127, 'epoch': 0.68}
{'loss': 1.3953, 'grad_norm': 0.19718164205551147, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.397741436958313, 'eval_runtime': 10.2904, 'eval_samples_per_second': 97.178, 'eval_steps_per_second': 6.122, 'epoch': 0.72}
{'loss': 1.4292, 'grad_norm': 0.18289364874362946, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3911784887313843, 'eval_runtime': 10.2913, 'eval_samples_per_second': 97.169, 'eval_steps_per_second': 6.122, 'epoch': 0.76}
{'loss': 1.3257, 'grad_norm': 0.2005561888217926, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4013285636901855, 'eval_runtime': 10.2816, 'eval_samples_per_second': 97.261, 'eval_steps_per_second': 6.127, 'epoch': 0.8}
{'loss': 1.3641, 'grad_norm': 0.17554396390914917, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3986248970031738, 'eval_runtime': 10.2512, 'eval_samples_per_second': 97.55, 'eval_steps_per_second': 6.146, 'epoch': 0.84}
{'loss': 1.3509, 'grad_norm': 0.2143845409154892, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.39754056930542, 'eval_runtime': 10.2663, 'eval_samples_per_second': 97.406, 'eval_steps_per_second': 6.137, 'epoch': 0.88}
{'loss': 1.412, 'grad_norm': 0.19066065549850464, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3987663984298706, 'eval_runtime': 10.2581, 'eval_samples_per_second': 97.484, 'eval_steps_per_second': 6.141, 'epoch': 0.92}
{'loss': 1.352, 'grad_norm': 0.20020388066768646, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3955539464950562, 'eval_runtime': 10.2557, 'eval_samples_per_second': 97.507, 'eval_steps_per_second': 6.143, 'epoch': 0.96}
{'loss': 1.3613, 'grad_norm': 0.15540580451488495, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3960334062576294, 'eval_runtime': 10.2606, 'eval_samples_per_second': 97.46, 'eval_steps_per_second': 6.14, 'epoch': 1.0}
{'train_runtime': 494.1729, 'train_samples_per_second': 20.23, 'train_steps_per_second': 1.265, 'train_loss': 1.5463112365722655, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5554981231689453, 1.4891471862792969, 1.4469178915023804, 1.400815486907959, 1.3921875953674316, 1.3710089921951294, 1.3847659826278687, 1.4017412662506104, 1.3874726295471191, 1.390811800956726, 1.3704450130462646, 1.3879482746124268, 1.3867237567901611, 1.393612027168274, 1.3958040475845337, 1.3967152833938599, 1.3952484130859375, 1.397741436958313, 1.3911784887313843, 1.4013285636901855, 1.3986248970031738, 1.39754056930542, 1.3987663984298706, 1.3955539464950562, 1.3960334062576294], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5554981231689453, 1.4891471862792969, 1.4469178915023804, 1.400815486907959, 1.3921875953674316, 1.3710089921951294, 1.3847659826278687, 1.4017412662506104, 1.3874726295471191, 1.390811800956726, 1.3704450130462646, 1.3879482746124268, 1.3867237567901611, 1.393612027168274, 1.3958040475845337, 1.3967152833938599, 1.3952484130859375, 1.397741436958313, 1.3911784887313843, 1.4013285636901855, 1.3986248970031738, 1.39754056930542, 1.3987663984298706, 1.3955539464950562, 1.3960334062576294]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.031911849975586
current iteration best possible eval_loss (full train run):  -1.3960334062576294
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 26.1198 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.8413157572597921
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.8413157572597921
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.8413157572597921
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.8413157572597921
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.8413157572597921
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4867, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.4227, dtype=torch.float64), tensor(0.0864, dtype=torch.float64), 14, 0, 0, 0, 1, 1, 4, 3.725310268198181e-20, 31.82299542361982, 1]
normalized proposed parameters for next round by BO: [tensor(0.4867, dtype=torch.float64), tensor(4.2360e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.7350e-19, dtype=torch.float64), tensor(0.0042, dtype=torch.float64), tensor(1.8237e-18, dtype=torch.float64), tensor(1.2274e-18, dtype=torch.float64), tensor(0.4227, dtype=torch.float64), tensor(0.0864, dtype=torch.float64), tensor(0.4405, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0346, dtype=torch.float64), tensor(3.7253e-19, dtype=torch.float64), tensor(0.6630, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.487
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.423
  arc_challenge: 0.086

LoRA Parameters:
  lora_r: (4,)
  lora_dropout: (3.725310268198181e-20,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (31.82299542361982,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  4
lora dropout:  3.725310268198181e-20
lora alpha:  31.82299542361982
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,064,384 || all params: 8,032,325,632 || trainable%: 0.0257
length of training data:  9957
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.165, 'grad_norm': 3.2748916149139404, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5621795654296875, 'eval_runtime': 9.4664, 'eval_samples_per_second': 105.637, 'eval_steps_per_second': 6.655, 'epoch': 0.04}
{'loss': 1.4372, 'grad_norm': 2.3588204383850098, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.438586950302124, 'eval_runtime': 9.5551, 'eval_samples_per_second': 104.656, 'eval_steps_per_second': 6.593, 'epoch': 0.08}
{'loss': 1.2115, 'grad_norm': 1.951244831085205, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.4219752550125122, 'eval_runtime': 9.6108, 'eval_samples_per_second': 104.05, 'eval_steps_per_second': 6.555, 'epoch': 0.12}
{'loss': 1.16, 'grad_norm': 1.502636194229126, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.396219253540039, 'eval_runtime': 9.611, 'eval_samples_per_second': 104.048, 'eval_steps_per_second': 6.555, 'epoch': 0.16}
{'loss': 1.0799, 'grad_norm': 0.8987409472465515, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.4447228908538818, 'eval_runtime': 9.6458, 'eval_samples_per_second': 103.672, 'eval_steps_per_second': 6.531, 'epoch': 0.2}
{'loss': 1.0748, 'grad_norm': 0.905697762966156, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.4119367599487305, 'eval_runtime': 9.6159, 'eval_samples_per_second': 103.995, 'eval_steps_per_second': 6.552, 'epoch': 0.24}
{'loss': 1.0466, 'grad_norm': 0.9115954041481018, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.5426499843597412, 'eval_runtime': 9.6249, 'eval_samples_per_second': 103.897, 'eval_steps_per_second': 6.546, 'epoch': 0.28}
{'loss': 1.0748, 'grad_norm': 1.0688399076461792, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.4947926998138428, 'eval_runtime': 9.6306, 'eval_samples_per_second': 103.835, 'eval_steps_per_second': 6.542, 'epoch': 0.32}
{'loss': 1.0709, 'grad_norm': 0.9177244901657104, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.4592080116271973, 'eval_runtime': 9.6151, 'eval_samples_per_second': 104.003, 'eval_steps_per_second': 6.552, 'epoch': 0.36}
{'loss': 1.0782, 'grad_norm': 1.0198627710342407, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.4663838148117065, 'eval_runtime': 9.6242, 'eval_samples_per_second': 103.905, 'eval_steps_per_second': 6.546, 'epoch': 0.4}
{'loss': 1.0726, 'grad_norm': 0.9708695411682129, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.5052635669708252, 'eval_runtime': 9.6243, 'eval_samples_per_second': 103.904, 'eval_steps_per_second': 6.546, 'epoch': 0.44}
{'loss': 1.0962, 'grad_norm': 0.8633379340171814, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.4828310012817383, 'eval_runtime': 9.6202, 'eval_samples_per_second': 103.948, 'eval_steps_per_second': 6.549, 'epoch': 0.48}
{'loss': 1.0245, 'grad_norm': 0.8840548396110535, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.5315608978271484, 'eval_runtime': 9.6134, 'eval_samples_per_second': 104.021, 'eval_steps_per_second': 6.553, 'epoch': 0.52}
{'loss': 1.026, 'grad_norm': 0.8170233964920044, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.5241940021514893, 'eval_runtime': 9.6053, 'eval_samples_per_second': 104.109, 'eval_steps_per_second': 6.559, 'epoch': 0.56}
{'loss': 1.0209, 'grad_norm': 0.8392614722251892, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.5375357866287231, 'eval_runtime': 9.6383, 'eval_samples_per_second': 103.753, 'eval_steps_per_second': 6.536, 'epoch': 0.6}
{'loss': 1.0193, 'grad_norm': 1.053693413734436, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 1.5142964124679565, 'eval_runtime': 9.5941, 'eval_samples_per_second': 104.231, 'eval_steps_per_second': 6.567, 'epoch': 0.64}
{'loss': 1.0863, 'grad_norm': 0.7945745587348938, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 1.4972399473190308, 'eval_runtime': 9.5819, 'eval_samples_per_second': 104.364, 'eval_steps_per_second': 6.575, 'epoch': 0.68}
{'loss': 1.009, 'grad_norm': 3.297546863555908, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 1.5011335611343384, 'eval_runtime': 9.5754, 'eval_samples_per_second': 104.435, 'eval_steps_per_second': 6.579, 'epoch': 0.72}
{'loss': 1.0425, 'grad_norm': 0.8192133903503418, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 1.5272538661956787, 'eval_runtime': 9.6042, 'eval_samples_per_second': 104.121, 'eval_steps_per_second': 6.56, 'epoch': 0.76}
{'loss': 1.0174, 'grad_norm': 0.8186611533164978, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 1.5286277532577515, 'eval_runtime': 9.6004, 'eval_samples_per_second': 104.162, 'eval_steps_per_second': 6.562, 'epoch': 0.8}
{'loss': 1.0272, 'grad_norm': 1.8950241804122925, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 1.5472841262817383, 'eval_runtime': 9.5779, 'eval_samples_per_second': 104.407, 'eval_steps_per_second': 6.578, 'epoch': 0.84}
{'loss': 1.0562, 'grad_norm': 0.9376550316810608, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 1.544650673866272, 'eval_runtime': 9.5697, 'eval_samples_per_second': 104.497, 'eval_steps_per_second': 6.583, 'epoch': 0.88}
{'loss': 1.059, 'grad_norm': 0.8161920309066772, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 1.5425163507461548, 'eval_runtime': 9.5479, 'eval_samples_per_second': 104.735, 'eval_steps_per_second': 6.598, 'epoch': 0.92}
{'loss': 1.005, 'grad_norm': 4.735823631286621, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 1.544453740119934, 'eval_runtime': 9.5361, 'eval_samples_per_second': 104.864, 'eval_steps_per_second': 6.606, 'epoch': 0.96}
{'train_runtime': 415.6399, 'train_samples_per_second': 23.956, 'train_steps_per_second': 1.499, 'train_loss': 1.1609993899423467, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5621795654296875, 1.438586950302124, 1.4219752550125122, 1.396219253540039, 1.4447228908538818, 1.4119367599487305, 1.5426499843597412, 1.4947926998138428, 1.4592080116271973, 1.4663838148117065, 1.5052635669708252, 1.4828310012817383, 1.5315608978271484, 1.5241940021514893, 1.5375357866287231, 1.5142964124679565, 1.4972399473190308, 1.5011335611343384, 1.5272538661956787, 1.5286277532577515, 1.5472841262817383, 1.544650673866272, 1.5425163507461548, 1.544453740119934], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.5621795654296875, 1.438586950302124, 1.4219752550125122, 1.396219253540039, 1.4447228908538818, 1.4119367599487305, 1.5426499843597412, 1.4947926998138428, 1.4592080116271973, 1.4663838148117065, 1.5052635669708252, 1.4828310012817383, 1.5315608978271484, 1.5241940021514893, 1.5375357866287231, 1.5142964124679565, 1.4972399473190308, 1.5011335611343384, 1.5272538661956787, 1.5286277532577515, 1.5472841262817383, 1.544650673866272, 1.5425163507461548, 1.544453740119934]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.0560648441314697
current iteration best possible eval_loss (full train run):  -1.544453740119934
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.5401 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1795889139175415, 0.5639668703079224, 0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.6092051267623901, 0.8282220363616943, 0.8343875408172607, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.22647850215435028, 0.034817516803741455, 0.2369377166032791, 0.8301550149917603]  ‚Üí  acq = -0.8379641056831876
X = [0.3627225160598755, 0.4275026321411133, 0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.20957553386688232, 0.29544466733932495, 0.45958811044692993, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7996509075164795, 0.16252559423446655, 0.23583632707595825, 0.46233898401260376]  ‚Üí  acq = -0.8393367737216397
X = [0.3497363328933716, 0.9076562523841858, 0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.451727032661438, 0.6221595406532288, 0.021270394325256348, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.990592360496521, 0.6705264449119568, 0.5292245149612427, 0.4145408272743225]  ‚Üí  acq = -0.8379640619349157
X = [0.8545095920562744, 0.7290428876876831, 0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.1896294802427292, 0.13708847761154175, 0.2295888066291809, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.42133286595344543, 0.9889391660690308, 0.27955883741378784, 0.647262454032898]  ‚Üí  acq = -0.8379605662544652
X = [0.25802141427993774, 0.15090978145599365, 0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.16085723042488098, 0.13615381717681885, 0.3176853060722351, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7902160882949829, 0.012897372245788574, 0.5332701206207275, 0.154333233833313]  ‚Üí  acq = -0.8379094358821274
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.6971, dtype=torch.float64), tensor(0.0195, dtype=torch.float64), 0, 0, tensor(0.1023, dtype=torch.float64), 0, tensor(0.0484, dtype=torch.float64), 0, tensor(0.1326, dtype=torch.float64), 7, 0, 0, 0, 0, 1, 59, 1.2372528050078122e-18, 18.4590481955748, 0]
normalized proposed parameters for next round by BO: [tensor(0.6971, dtype=torch.float64), tensor(0.0195, dtype=torch.float64), tensor(3.6989e-18, dtype=torch.float64), tensor(1.2720e-18, dtype=torch.float64), tensor(0.1023, dtype=torch.float64), tensor(2.7236e-18, dtype=torch.float64), tensor(0.0484, dtype=torch.float64), tensor(4.4129e-19, dtype=torch.float64), tensor(0.1326, dtype=torch.float64), tensor(0.2251, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4631, dtype=torch.float64), tensor(1.2373e-17, dtype=torch.float64), tensor(0.3846, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.697
  gsm8k: 0.02
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.102
  truthfulqa_gen: 0
  wikitext: 0.048
  mmlu: 0
  arc_challenge: 0.133

LoRA Parameters:
  lora_r: (59,)
  lora_dropout: (1.2372528050078122e-18,)
  num_layers_to_apply: (7,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (18.4590481955748,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  7
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  59
lora dropout:  1.2372528050078122e-18
lora alpha:  18.4590481955748
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 7,612,416 || all params: 8,037,873,664 || trainable%: 0.0947
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.3649, 'grad_norm': 0.6245920062065125, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0522618293762207, 'eval_runtime': 9.0321, 'eval_samples_per_second': 110.717, 'eval_steps_per_second': 6.975, 'epoch': 0.04}
{'loss': 2.4731, 'grad_norm': 0.26579055190086365, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5892115831375122, 'eval_runtime': 9.1342, 'eval_samples_per_second': 109.479, 'eval_steps_per_second': 6.897, 'epoch': 0.08}
{'loss': 1.4858, 'grad_norm': 0.1696973592042923, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4078052043914795, 'eval_runtime': 9.0838, 'eval_samples_per_second': 110.086, 'eval_steps_per_second': 6.935, 'epoch': 0.12}
{'loss': 1.3035, 'grad_norm': 0.1437399983406067, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3578712940216064, 'eval_runtime': 9.1113, 'eval_samples_per_second': 109.754, 'eval_steps_per_second': 6.915, 'epoch': 0.16}
{'loss': 1.2586, 'grad_norm': 0.1718975454568863, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.280620813369751, 'eval_runtime': 9.162, 'eval_samples_per_second': 109.147, 'eval_steps_per_second': 6.876, 'epoch': 0.2}
{'loss': 1.1891, 'grad_norm': 0.16420802474021912, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2095823287963867, 'eval_runtime': 9.1422, 'eval_samples_per_second': 109.383, 'eval_steps_per_second': 6.891, 'epoch': 0.24}
{'loss': 1.1217, 'grad_norm': 0.1568593829870224, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1869760751724243, 'eval_runtime': 9.1369, 'eval_samples_per_second': 109.446, 'eval_steps_per_second': 6.895, 'epoch': 0.28}
{'loss': 1.0571, 'grad_norm': 0.15566271543502808, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1641323566436768, 'eval_runtime': 9.1332, 'eval_samples_per_second': 109.491, 'eval_steps_per_second': 6.898, 'epoch': 0.32}
{'loss': 1.0853, 'grad_norm': 0.13758927583694458, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1418794393539429, 'eval_runtime': 9.1504, 'eval_samples_per_second': 109.285, 'eval_steps_per_second': 6.885, 'epoch': 0.36}
{'loss': 1.08, 'grad_norm': 0.15356364846229553, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.130441665649414, 'eval_runtime': 9.2093, 'eval_samples_per_second': 108.586, 'eval_steps_per_second': 6.841, 'epoch': 0.4}
{'loss': 1.029, 'grad_norm': 0.16378898918628693, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1253432035446167, 'eval_runtime': 9.2196, 'eval_samples_per_second': 108.465, 'eval_steps_per_second': 6.833, 'epoch': 0.44}
{'loss': 1.0197, 'grad_norm': 0.15230078995227814, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.116971492767334, 'eval_runtime': 9.2152, 'eval_samples_per_second': 108.516, 'eval_steps_per_second': 6.837, 'epoch': 0.48}
{'loss': 1.0112, 'grad_norm': 0.15947435796260834, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1072864532470703, 'eval_runtime': 9.188, 'eval_samples_per_second': 108.837, 'eval_steps_per_second': 6.857, 'epoch': 0.52}
{'loss': 1.037, 'grad_norm': 0.1675342172384262, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0952378511428833, 'eval_runtime': 9.1975, 'eval_samples_per_second': 108.725, 'eval_steps_per_second': 6.85, 'epoch': 0.56}
{'loss': 1.0443, 'grad_norm': 0.15395578742027283, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0880178213119507, 'eval_runtime': 9.1841, 'eval_samples_per_second': 108.884, 'eval_steps_per_second': 6.86, 'epoch': 0.6}
{'loss': 0.9663, 'grad_norm': 0.16984891891479492, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0822886228561401, 'eval_runtime': 9.1735, 'eval_samples_per_second': 109.01, 'eval_steps_per_second': 6.868, 'epoch': 0.64}
{'loss': 1.0242, 'grad_norm': 0.1741279512643814, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0781482458114624, 'eval_runtime': 9.1961, 'eval_samples_per_second': 108.741, 'eval_steps_per_second': 6.851, 'epoch': 0.68}
{'loss': 1.0129, 'grad_norm': 0.16333261132240295, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.08027184009552, 'eval_runtime': 9.1947, 'eval_samples_per_second': 108.759, 'eval_steps_per_second': 6.852, 'epoch': 0.72}
{'loss': 1.0364, 'grad_norm': 0.18628256022930145, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0703285932540894, 'eval_runtime': 9.1965, 'eval_samples_per_second': 108.737, 'eval_steps_per_second': 6.85, 'epoch': 0.76}
{'loss': 0.9913, 'grad_norm': 0.16769176721572876, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0769087076187134, 'eval_runtime': 9.2101, 'eval_samples_per_second': 108.577, 'eval_steps_per_second': 6.84, 'epoch': 0.8}
{'loss': 0.9791, 'grad_norm': 0.16891418397426605, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0775086879730225, 'eval_runtime': 9.1973, 'eval_samples_per_second': 108.728, 'eval_steps_per_second': 6.85, 'epoch': 0.84}
{'loss': 0.9956, 'grad_norm': 0.1843874454498291, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0776457786560059, 'eval_runtime': 9.2062, 'eval_samples_per_second': 108.622, 'eval_steps_per_second': 6.843, 'epoch': 0.88}
{'loss': 1.0406, 'grad_norm': 0.17234088480472565, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0706312656402588, 'eval_runtime': 9.1872, 'eval_samples_per_second': 108.847, 'eval_steps_per_second': 6.857, 'epoch': 0.92}
{'loss': 0.9688, 'grad_norm': 0.1754278987646103, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0693304538726807, 'eval_runtime': 9.1765, 'eval_samples_per_second': 108.974, 'eval_steps_per_second': 6.865, 'epoch': 0.96}
{'loss': 1.0422, 'grad_norm': 0.1794544756412506, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0690165758132935, 'eval_runtime': 9.1434, 'eval_samples_per_second': 109.369, 'eval_steps_per_second': 6.89, 'epoch': 1.0}
{'train_runtime': 308.4182, 'train_samples_per_second': 32.42, 'train_steps_per_second': 2.026, 'train_loss': 1.264703173828125, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0522618293762207, 1.5892115831375122, 1.4078052043914795, 1.3578712940216064, 1.280620813369751, 1.2095823287963867, 1.1869760751724243, 1.1641323566436768, 1.1418794393539429, 1.130441665649414, 1.1253432035446167, 1.116971492767334, 1.1072864532470703, 1.0952378511428833, 1.0880178213119507, 1.0822886228561401, 1.0781482458114624, 1.08027184009552, 1.0703285932540894, 1.0769087076187134, 1.0775086879730225, 1.0776457786560059, 1.0706312656402588, 1.0693304538726807, 1.0690165758132935], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0522618293762207, 1.5892115831375122, 1.4078052043914795, 1.3578712940216064, 1.280620813369751, 1.2095823287963867, 1.1869760751724243, 1.1641323566436768, 1.1418794393539429, 1.130441665649414, 1.1253432035446167, 1.116971492767334, 1.1072864532470703, 1.0952378511428833, 1.0880178213119507, 1.0822886228561401, 1.0781482458114624, 1.08027184009552, 1.0703285932540894, 1.0769087076187134, 1.0775086879730225, 1.0776457786560059, 1.0706312656402588, 1.0693304538726807, 1.0690165758132935]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.5177440643310547
current iteration best possible eval_loss (full train run):  -1.0690165758132935
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.5445 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -0.8088848307769203
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -0.8240418310720727
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -0.8284367962458795
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.8592260591581558
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -0.8282508668461142
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1742, dtype=torch.float64), 0, tensor(0.1535, dtype=torch.float64), tensor(0.5477, dtype=torch.float64), tensor(0.1066, dtype=torch.float64), 0, tensor(0.0180, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 121, 0.006714441735491532, 36.4567109723056, 0]
normalized proposed parameters for next round by BO: [tensor(8.1712e-18, dtype=torch.float64), tensor(4.9610e-18, dtype=torch.float64), tensor(0.1742, dtype=torch.float64), tensor(6.7885e-18, dtype=torch.float64), tensor(0.1535, dtype=torch.float64), tensor(0.5477, dtype=torch.float64), tensor(0.1066, dtype=torch.float64), tensor(5.7531e-18, dtype=torch.float64), tensor(0.0180, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9465, dtype=torch.float64), tensor(0.0671, dtype=torch.float64), tensor(0.7595, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.174
  sciq: 0
  triviaqa: 0.153
  truthfulqa_gen: 0.548
  wikitext: 0.107
  mmlu: 0
  arc_challenge: 0.018

LoRA Parameters:
  lora_r: (121,)
  lora_dropout: (0.006714441735491532,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (36.4567109723056,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  121
lora dropout:  0.006714441735491532
lora alpha:  36.4567109723056
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 162,562,048 || all params: 8,192,823,296 || trainable%: 1.9842
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3057, 'grad_norm': 0.5058165192604065, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4705421924591064, 'eval_runtime': 11.1675, 'eval_samples_per_second': 89.546, 'eval_steps_per_second': 5.641, 'epoch': 0.04}
{'loss': 1.538, 'grad_norm': 0.3938136398792267, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.36891770362854, 'eval_runtime': 11.2205, 'eval_samples_per_second': 89.123, 'eval_steps_per_second': 5.615, 'epoch': 0.08}
{'loss': 1.489, 'grad_norm': 0.36670994758605957, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3983856439590454, 'eval_runtime': 11.2293, 'eval_samples_per_second': 89.052, 'eval_steps_per_second': 5.61, 'epoch': 0.12}
{'loss': 1.3191, 'grad_norm': 0.23977136611938477, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4426072835922241, 'eval_runtime': 11.2086, 'eval_samples_per_second': 89.217, 'eval_steps_per_second': 5.621, 'epoch': 0.16}
{'loss': 1.2009, 'grad_norm': 0.3086978495121002, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3901287317276, 'eval_runtime': 11.2662, 'eval_samples_per_second': 88.761, 'eval_steps_per_second': 5.592, 'epoch': 0.2}
{'loss': 1.1875, 'grad_norm': 0.23654207587242126, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4378403425216675, 'eval_runtime': 11.2462, 'eval_samples_per_second': 88.919, 'eval_steps_per_second': 5.602, 'epoch': 0.24}
{'loss': 1.1937, 'grad_norm': 0.2289031744003296, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4101171493530273, 'eval_runtime': 11.2696, 'eval_samples_per_second': 88.735, 'eval_steps_per_second': 5.59, 'epoch': 0.28}
{'loss': 1.1461, 'grad_norm': 0.3350525200366974, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4377844333648682, 'eval_runtime': 11.2741, 'eval_samples_per_second': 88.699, 'eval_steps_per_second': 5.588, 'epoch': 0.32}
{'loss': 1.1776, 'grad_norm': 0.2959466576576233, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4290657043457031, 'eval_runtime': 11.2501, 'eval_samples_per_second': 88.888, 'eval_steps_per_second': 5.6, 'epoch': 0.36}
{'loss': 1.1505, 'grad_norm': 0.3542792499065399, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4325567483901978, 'eval_runtime': 11.226, 'eval_samples_per_second': 89.079, 'eval_steps_per_second': 5.612, 'epoch': 0.4}
{'loss': 1.1614, 'grad_norm': 0.26569050550460815, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4143574237823486, 'eval_runtime': 11.2533, 'eval_samples_per_second': 88.863, 'eval_steps_per_second': 5.598, 'epoch': 0.44}
{'loss': 1.0914, 'grad_norm': 0.21962065994739532, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4347985982894897, 'eval_runtime': 11.232, 'eval_samples_per_second': 89.032, 'eval_steps_per_second': 5.609, 'epoch': 0.48}
{'loss': 1.0707, 'grad_norm': 0.2555367946624756, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.42946457862854, 'eval_runtime': 11.2328, 'eval_samples_per_second': 89.025, 'eval_steps_per_second': 5.609, 'epoch': 0.52}
{'loss': 1.0383, 'grad_norm': 0.6276042461395264, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4433096647262573, 'eval_runtime': 11.2316, 'eval_samples_per_second': 89.034, 'eval_steps_per_second': 5.609, 'epoch': 0.56}
{'loss': 1.1285, 'grad_norm': 0.30614587664604187, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4510862827301025, 'eval_runtime': 11.2358, 'eval_samples_per_second': 89.001, 'eval_steps_per_second': 5.607, 'epoch': 0.6}
{'loss': 1.1191, 'grad_norm': 0.26626789569854736, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4610166549682617, 'eval_runtime': 11.231, 'eval_samples_per_second': 89.04, 'eval_steps_per_second': 5.609, 'epoch': 0.64}
{'loss': 1.0544, 'grad_norm': 0.2031778246164322, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4273101091384888, 'eval_runtime': 11.2176, 'eval_samples_per_second': 89.145, 'eval_steps_per_second': 5.616, 'epoch': 0.68}
{'loss': 0.9994, 'grad_norm': 0.2779875099658966, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.40803861618042, 'eval_runtime': 11.2087, 'eval_samples_per_second': 89.216, 'eval_steps_per_second': 5.621, 'epoch': 0.72}
{'loss': 1.0388, 'grad_norm': 0.19700385630130768, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4124541282653809, 'eval_runtime': 11.2265, 'eval_samples_per_second': 89.075, 'eval_steps_per_second': 5.612, 'epoch': 0.76}
{'loss': 1.1421, 'grad_norm': 0.22553108632564545, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4233962297439575, 'eval_runtime': 11.2084, 'eval_samples_per_second': 89.219, 'eval_steps_per_second': 5.621, 'epoch': 0.8}
{'loss': 1.0431, 'grad_norm': 0.21607618033885956, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.429448127746582, 'eval_runtime': 11.2135, 'eval_samples_per_second': 89.179, 'eval_steps_per_second': 5.618, 'epoch': 0.84}
{'loss': 1.0283, 'grad_norm': 0.2603353261947632, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.426690936088562, 'eval_runtime': 11.2274, 'eval_samples_per_second': 89.068, 'eval_steps_per_second': 5.611, 'epoch': 0.88}
{'loss': 1.0262, 'grad_norm': 0.25508424639701843, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4312750101089478, 'eval_runtime': 11.2373, 'eval_samples_per_second': 88.99, 'eval_steps_per_second': 5.606, 'epoch': 0.92}
{'loss': 1.0554, 'grad_norm': 0.1782466471195221, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4367214441299438, 'eval_runtime': 11.2661, 'eval_samples_per_second': 88.762, 'eval_steps_per_second': 5.592, 'epoch': 0.96}
{'loss': 1.0909, 'grad_norm': 0.27476099133491516, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.438069462776184, 'eval_runtime': 11.2601, 'eval_samples_per_second': 88.809, 'eval_steps_per_second': 5.595, 'epoch': 1.0}
{'train_runtime': 529.262, 'train_samples_per_second': 18.89, 'train_steps_per_second': 1.181, 'train_loss': 1.23184375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4705421924591064, 1.36891770362854, 1.3983856439590454, 1.4426072835922241, 1.3901287317276, 1.4378403425216675, 1.4101171493530273, 1.4377844333648682, 1.4290657043457031, 1.4325567483901978, 1.4143574237823486, 1.4347985982894897, 1.42946457862854, 1.4433096647262573, 1.4510862827301025, 1.4610166549682617, 1.4273101091384888, 1.40803861618042, 1.4124541282653809, 1.4233962297439575, 1.429448127746582, 1.426690936088562, 1.4312750101089478, 1.4367214441299438, 1.438069462776184], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4705421924591064, 1.36891770362854, 1.3983856439590454, 1.4426072835922241, 1.3901287317276, 1.4378403425216675, 1.4101171493530273, 1.4377844333648682, 1.4290657043457031, 1.4325567483901978, 1.4143574237823486, 1.4347985982894897, 1.42946457862854, 1.4433096647262573, 1.4510862827301025, 1.4610166549682617, 1.4273101091384888, 1.40803861618042, 1.4124541282653809, 1.4233962297439575, 1.429448127746582, 1.426690936088562, 1.4312750101089478, 1.4367214441299438, 1.438069462776184]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0774685144424438
current iteration best possible eval_loss (full train run):  -1.438069462776184
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.5288 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.8439959611138605
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.9045211512935352
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.7380942783046652
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.8051044208407125
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.8494954314735399
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0709, dtype=torch.float64), tensor(0.0534, dtype=torch.float64), 0, 0, tensor(0.1625, dtype=torch.float64), 0, tensor(0.2786, dtype=torch.float64), tensor(0.4345, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 124, 0.0017693850945787821, 10.142521987853724, 1]
normalized proposed parameters for next round by BO: [tensor(5.4710e-18, dtype=torch.float64), tensor(0.0709, dtype=torch.float64), tensor(0.0534, dtype=torch.float64), tensor(3.0084e-18, dtype=torch.float64), tensor(6.1993e-18, dtype=torch.float64), tensor(0.1625, dtype=torch.float64), tensor(8.2749e-18, dtype=torch.float64), tensor(0.2786, dtype=torch.float64), tensor(0.4345, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9685, dtype=torch.float64), tensor(0.0177, dtype=torch.float64), tensor(0.2113, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.071
  rowan_hellaswag: 0.053
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.163
  wikitext: 0
  mmlu: 0.279
  arc_challenge: 0.435

LoRA Parameters:
  lora_r: (124,)
  lora_dropout: (0.0017693850945787821,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (10.142521987853724,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  124
lora dropout:  0.0017693850945787821
lora alpha:  10.142521987853724
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 166,592,512 || all params: 8,196,853,760 || trainable%: 2.0324
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1363, 'grad_norm': 0.5326613187789917, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.51255202293396, 'eval_runtime': 10.6771, 'eval_samples_per_second': 93.658, 'eval_steps_per_second': 5.9, 'epoch': 0.04}
{'loss': 1.4501, 'grad_norm': 0.24690771102905273, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1622062921524048, 'eval_runtime': 10.688, 'eval_samples_per_second': 93.563, 'eval_steps_per_second': 5.894, 'epoch': 0.08}
{'loss': 1.1887, 'grad_norm': 0.15357013046741486, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0082331895828247, 'eval_runtime': 10.7252, 'eval_samples_per_second': 93.238, 'eval_steps_per_second': 5.874, 'epoch': 0.12}
{'loss': 1.1169, 'grad_norm': 0.12875422835350037, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.954235851764679, 'eval_runtime': 10.7492, 'eval_samples_per_second': 93.03, 'eval_steps_per_second': 5.861, 'epoch': 0.16}
{'loss': 1.0734, 'grad_norm': 0.11197594553232193, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9379602074623108, 'eval_runtime': 10.7459, 'eval_samples_per_second': 93.059, 'eval_steps_per_second': 5.863, 'epoch': 0.2}
{'loss': 1.1321, 'grad_norm': 0.11049151420593262, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9268872141838074, 'eval_runtime': 10.6747, 'eval_samples_per_second': 93.68, 'eval_steps_per_second': 5.902, 'epoch': 0.24}
{'loss': 1.0763, 'grad_norm': 0.1277153044939041, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9318562746047974, 'eval_runtime': 10.7204, 'eval_samples_per_second': 93.28, 'eval_steps_per_second': 5.877, 'epoch': 0.28}
{'loss': 1.0599, 'grad_norm': 0.11932288110256195, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9326248168945312, 'eval_runtime': 10.7604, 'eval_samples_per_second': 92.933, 'eval_steps_per_second': 5.855, 'epoch': 0.32}
{'loss': 1.0541, 'grad_norm': 0.15815037488937378, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9240087270736694, 'eval_runtime': 10.7952, 'eval_samples_per_second': 92.634, 'eval_steps_per_second': 5.836, 'epoch': 0.36}
{'loss': 0.9837, 'grad_norm': 0.12331512570381165, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9162799715995789, 'eval_runtime': 10.7843, 'eval_samples_per_second': 92.727, 'eval_steps_per_second': 5.842, 'epoch': 0.4}
{'loss': 0.9924, 'grad_norm': 0.16511431336402893, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.915237545967102, 'eval_runtime': 10.7689, 'eval_samples_per_second': 92.86, 'eval_steps_per_second': 5.85, 'epoch': 0.44}
{'loss': 1.0071, 'grad_norm': 0.14907212555408478, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9090224504470825, 'eval_runtime': 10.7703, 'eval_samples_per_second': 92.848, 'eval_steps_per_second': 5.849, 'epoch': 0.48}
{'loss': 0.9532, 'grad_norm': 0.1474158614873886, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.909174919128418, 'eval_runtime': 10.7596, 'eval_samples_per_second': 92.94, 'eval_steps_per_second': 5.855, 'epoch': 0.52}
{'loss': 0.9817, 'grad_norm': 0.16452749073505402, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9044468998908997, 'eval_runtime': 10.7984, 'eval_samples_per_second': 92.606, 'eval_steps_per_second': 5.834, 'epoch': 0.56}
{'loss': 0.945, 'grad_norm': 0.15014880895614624, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9066135287284851, 'eval_runtime': 10.8527, 'eval_samples_per_second': 92.143, 'eval_steps_per_second': 5.805, 'epoch': 0.6}
{'loss': 0.9563, 'grad_norm': 0.1563255786895752, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8998234868049622, 'eval_runtime': 10.8461, 'eval_samples_per_second': 92.199, 'eval_steps_per_second': 5.809, 'epoch': 0.64}
{'loss': 0.8995, 'grad_norm': 0.18855442106723785, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8986448049545288, 'eval_runtime': 10.8622, 'eval_samples_per_second': 92.062, 'eval_steps_per_second': 5.8, 'epoch': 0.68}
{'loss': 0.9493, 'grad_norm': 0.19684456288814545, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8999359011650085, 'eval_runtime': 10.83, 'eval_samples_per_second': 92.336, 'eval_steps_per_second': 5.817, 'epoch': 0.72}
{'loss': 0.935, 'grad_norm': 0.21496137976646423, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8993848562240601, 'eval_runtime': 10.8559, 'eval_samples_per_second': 92.116, 'eval_steps_per_second': 5.803, 'epoch': 0.76}
{'loss': 0.904, 'grad_norm': 0.19076018035411835, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8932157158851624, 'eval_runtime': 10.8207, 'eval_samples_per_second': 92.415, 'eval_steps_per_second': 5.822, 'epoch': 0.8}
{'loss': 0.9148, 'grad_norm': 0.20957350730895996, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8936764597892761, 'eval_runtime': 10.7856, 'eval_samples_per_second': 92.716, 'eval_steps_per_second': 5.841, 'epoch': 0.84}
{'loss': 0.9176, 'grad_norm': 0.206283837556839, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8923512101173401, 'eval_runtime': 10.7895, 'eval_samples_per_second': 92.683, 'eval_steps_per_second': 5.839, 'epoch': 0.88}
{'loss': 0.8567, 'grad_norm': 0.22804993391036987, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8911301493644714, 'eval_runtime': 10.7979, 'eval_samples_per_second': 92.611, 'eval_steps_per_second': 5.834, 'epoch': 0.92}
{'loss': 0.9342, 'grad_norm': 0.14933829009532928, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8933956027030945, 'eval_runtime': 10.7815, 'eval_samples_per_second': 92.752, 'eval_steps_per_second': 5.843, 'epoch': 0.96}
{'loss': 0.9024, 'grad_norm': 0.1945645809173584, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8929427862167358, 'eval_runtime': 10.7953, 'eval_samples_per_second': 92.633, 'eval_steps_per_second': 5.836, 'epoch': 1.0}
{'train_runtime': 507.7817, 'train_samples_per_second': 19.692, 'train_steps_per_second': 1.231, 'train_loss': 1.0928242065429687, 'epoch': 1.0}
train_results:  {'eval_loss': [1.51255202293396, 1.1622062921524048, 1.0082331895828247, 0.954235851764679, 0.9379602074623108, 0.9268872141838074, 0.9318562746047974, 0.9326248168945312, 0.9240087270736694, 0.9162799715995789, 0.915237545967102, 0.9090224504470825, 0.909174919128418, 0.9044468998908997, 0.9066135287284851, 0.8998234868049622, 0.8986448049545288, 0.8999359011650085, 0.8993848562240601, 0.8932157158851624, 0.8936764597892761, 0.8923512101173401, 0.8911301493644714, 0.8933956027030945, 0.8929427862167358], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.51255202293396, 1.1622062921524048, 1.0082331895828247, 0.954235851764679, 0.9379602074623108, 0.9268872141838074, 0.9318562746047974, 0.9326248168945312, 0.9240087270736694, 0.9162799715995789, 0.915237545967102, 0.9090224504470825, 0.909174919128418, 0.9044468998908997, 0.9066135287284851, 0.8998234868049622, 0.8986448049545288, 0.8999359011650085, 0.8993848562240601, 0.8932157158851624, 0.8936764597892761, 0.8923512101173401, 0.8911301493644714, 0.8933956027030945, 0.8929427862167358]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.8929427862167358
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.5147 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -0.8452563810733869
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.8868297979633919
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -0.8850243612012548
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -0.7711115333313369
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -0.878455023626399
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0947, dtype=torch.float64), tensor(0.0997, dtype=torch.float64), tensor(0.1448, dtype=torch.float64), tensor(0.3427, dtype=torch.float64), tensor(0.0138, dtype=torch.float64), tensor(0.1980, dtype=torch.float64), tensor(0.0148, dtype=torch.float64), tensor(0.0915, dtype=torch.float64), 32, 1, 0, 0, 0, 1, 18, 0.01070487067953985, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(2.8080e-19, dtype=torch.float64), tensor(0.0947, dtype=torch.float64), tensor(0.0997, dtype=torch.float64), tensor(0.1448, dtype=torch.float64), tensor(0.3427, dtype=torch.float64), tensor(0.0138, dtype=torch.float64), tensor(0.1980, dtype=torch.float64), tensor(0.0148, dtype=torch.float64), tensor(0.0915, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1384, dtype=torch.float64), tensor(0.1070, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.095
  rowan_hellaswag: 0.1
  sciq: 0.145
  triviaqa: 0.343
  truthfulqa_gen: 0.014
  wikitext: 0.198
  mmlu: 0.015
  arc_challenge: 0.092

LoRA Parameters:
  lora_r: (18,)
  lora_dropout: (0.01070487067953985,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  18
lora dropout:  0.01070487067953985
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 15,335,424 || all params: 8,045,596,672 || trainable%: 0.1906
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2144, 'grad_norm': 1.645464539527893, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4326411485671997, 'eval_runtime': 9.7878, 'eval_samples_per_second': 102.168, 'eval_steps_per_second': 6.437, 'epoch': 0.04}
{'loss': 1.7704, 'grad_norm': 0.6850611567497253, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0594606399536133, 'eval_runtime': 9.8263, 'eval_samples_per_second': 101.768, 'eval_steps_per_second': 6.411, 'epoch': 0.08}
{'loss': 1.5659, 'grad_norm': 0.5028523206710815, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9919143319129944, 'eval_runtime': 9.8957, 'eval_samples_per_second': 101.054, 'eval_steps_per_second': 6.366, 'epoch': 0.12}
{'loss': 1.4464, 'grad_norm': 0.6018540263175964, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9415865540504456, 'eval_runtime': 9.9043, 'eval_samples_per_second': 100.966, 'eval_steps_per_second': 6.361, 'epoch': 0.16}
{'loss': 1.4868, 'grad_norm': 0.6010029315948486, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9091757535934448, 'eval_runtime': 9.8995, 'eval_samples_per_second': 101.015, 'eval_steps_per_second': 6.364, 'epoch': 0.2}
{'loss': 1.3381, 'grad_norm': 0.43479371070861816, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9018720984458923, 'eval_runtime': 9.9005, 'eval_samples_per_second': 101.005, 'eval_steps_per_second': 6.363, 'epoch': 0.24}
{'loss': 1.2177, 'grad_norm': 0.4716228246688843, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8968385457992554, 'eval_runtime': 9.9183, 'eval_samples_per_second': 100.824, 'eval_steps_per_second': 6.352, 'epoch': 0.28}
{'loss': 1.3286, 'grad_norm': 0.4626878499984741, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8969019055366516, 'eval_runtime': 9.9118, 'eval_samples_per_second': 100.89, 'eval_steps_per_second': 6.356, 'epoch': 0.32}
{'loss': 1.357, 'grad_norm': 0.4973432421684265, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.889968752861023, 'eval_runtime': 9.9129, 'eval_samples_per_second': 100.879, 'eval_steps_per_second': 6.355, 'epoch': 0.36}
{'loss': 1.216, 'grad_norm': 0.5385804772377014, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8895113468170166, 'eval_runtime': 9.916, 'eval_samples_per_second': 100.847, 'eval_steps_per_second': 6.353, 'epoch': 0.4}
{'loss': 1.3366, 'grad_norm': 0.6049398183822632, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8892938494682312, 'eval_runtime': 9.8812, 'eval_samples_per_second': 101.202, 'eval_steps_per_second': 6.376, 'epoch': 0.44}
{'loss': 1.2799, 'grad_norm': 0.43291935324668884, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8851480484008789, 'eval_runtime': 9.9027, 'eval_samples_per_second': 100.982, 'eval_steps_per_second': 6.362, 'epoch': 0.48}
{'loss': 1.3409, 'grad_norm': 0.45878374576568604, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8799954652786255, 'eval_runtime': 9.9318, 'eval_samples_per_second': 100.687, 'eval_steps_per_second': 6.343, 'epoch': 0.52}
{'loss': 1.2512, 'grad_norm': 0.41795697808265686, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8816070556640625, 'eval_runtime': 9.953, 'eval_samples_per_second': 100.473, 'eval_steps_per_second': 6.33, 'epoch': 0.56}
{'loss': 1.2943, 'grad_norm': 0.5359011888504028, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8766147494316101, 'eval_runtime': 9.9324, 'eval_samples_per_second': 100.681, 'eval_steps_per_second': 6.343, 'epoch': 0.6}
{'loss': 1.2725, 'grad_norm': 0.42620906233787537, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8759655952453613, 'eval_runtime': 9.922, 'eval_samples_per_second': 100.786, 'eval_steps_per_second': 6.35, 'epoch': 0.64}
{'loss': 1.1956, 'grad_norm': 0.5147215127944946, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8763353824615479, 'eval_runtime': 9.9271, 'eval_samples_per_second': 100.735, 'eval_steps_per_second': 6.346, 'epoch': 0.68}
{'loss': 1.2452, 'grad_norm': 0.46155139803886414, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8732025027275085, 'eval_runtime': 9.9317, 'eval_samples_per_second': 100.687, 'eval_steps_per_second': 6.343, 'epoch': 0.72}
{'loss': 1.1931, 'grad_norm': 0.5804511904716492, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.871110200881958, 'eval_runtime': 9.9586, 'eval_samples_per_second': 100.416, 'eval_steps_per_second': 6.326, 'epoch': 0.76}
{'loss': 1.289, 'grad_norm': 0.5063547492027283, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8699781894683838, 'eval_runtime': 9.9298, 'eval_samples_per_second': 100.707, 'eval_steps_per_second': 6.345, 'epoch': 0.8}
{'loss': 1.2753, 'grad_norm': 0.5615779757499695, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8686268925666809, 'eval_runtime': 9.9637, 'eval_samples_per_second': 100.365, 'eval_steps_per_second': 6.323, 'epoch': 0.84}
{'loss': 1.325, 'grad_norm': 0.5898225903511047, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8676862716674805, 'eval_runtime': 9.9557, 'eval_samples_per_second': 100.445, 'eval_steps_per_second': 6.328, 'epoch': 0.88}
{'loss': 1.1865, 'grad_norm': 0.5005173087120056, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8669030666351318, 'eval_runtime': 9.9172, 'eval_samples_per_second': 100.835, 'eval_steps_per_second': 6.353, 'epoch': 0.92}
{'loss': 1.21, 'grad_norm': 0.5024865865707397, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8679177761077881, 'eval_runtime': 9.8859, 'eval_samples_per_second': 101.154, 'eval_steps_per_second': 6.373, 'epoch': 0.96}
{'loss': 1.2095, 'grad_norm': 0.42929235100746155, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8678608536720276, 'eval_runtime': 9.8619, 'eval_samples_per_second': 101.4, 'eval_steps_per_second': 6.388, 'epoch': 1.0}
{'train_runtime': 456.6166, 'train_samples_per_second': 21.894, 'train_steps_per_second': 1.369, 'train_loss': 1.3938333923339843, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4326411485671997, 1.0594606399536133, 0.9919143319129944, 0.9415865540504456, 0.9091757535934448, 0.9018720984458923, 0.8968385457992554, 0.8969019055366516, 0.889968752861023, 0.8895113468170166, 0.8892938494682312, 0.8851480484008789, 0.8799954652786255, 0.8816070556640625, 0.8766147494316101, 0.8759655952453613, 0.8763353824615479, 0.8732025027275085, 0.871110200881958, 0.8699781894683838, 0.8686268925666809, 0.8676862716674805, 0.8669030666351318, 0.8679177761077881, 0.8678608536720276], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4326411485671997, 1.0594606399536133, 0.9919143319129944, 0.9415865540504456, 0.9091757535934448, 0.9018720984458923, 0.8968385457992554, 0.8969019055366516, 0.889968752861023, 0.8895113468170166, 0.8892938494682312, 0.8851480484008789, 0.8799954652786255, 0.8816070556640625, 0.8766147494316101, 0.8759655952453613, 0.8763353824615479, 0.8732025027275085, 0.871110200881958, 0.8699781894683838, 0.8686268925666809, 0.8676862716674805, 0.8669030666351318, 0.8679177761077881, 0.8678608536720276]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.3333349227905273
current iteration best possible eval_loss (full train run):  -0.8678608536720276
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4207 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -0.9359724307357922
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -0.9182432331547826
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -0.9213102800753543
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.921651787341229
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -0.9355506355280636
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0602, dtype=torch.float64), tensor(0.2739, dtype=torch.float64), tensor(0.1864, dtype=torch.float64), 0, tensor(0.0112, dtype=torch.float64), tensor(0.1057, dtype=torch.float64), tensor(0.3626, dtype=torch.float64), 27, 0, 0, 0, 1, 0, 126, 0.003968886578156947, 25.109118465170255, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(2.7882e-17, dtype=torch.float64), tensor(0.0602, dtype=torch.float64), tensor(0.2739, dtype=torch.float64), tensor(0.1864, dtype=torch.float64), tensor(1.6235e-17, dtype=torch.float64), tensor(0.0112, dtype=torch.float64), tensor(0.1057, dtype=torch.float64), tensor(0.3626, dtype=torch.float64), tensor(0.8323, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9835, dtype=torch.float64), tensor(0.0397, dtype=torch.float64), tensor(0.5231, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.06
  sciq: 0.274
  triviaqa: 0.186
  truthfulqa_gen: 0
  wikitext: 0.011
  mmlu: 0.106
  arc_challenge: 0.363

LoRA Parameters:
  lora_r: (126,)
  lora_dropout: (0.003968886578156947,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (25.109118465170255,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  126
lora dropout:  0.003968886578156947
lora alpha:  25.109118465170255
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 62,705,664 || all params: 8,092,966,912 || trainable%: 0.7748
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4456, 'grad_norm': 1.2295005321502686, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5293726921081543, 'eval_runtime': 9.4611, 'eval_samples_per_second': 105.696, 'eval_steps_per_second': 6.659, 'epoch': 0.04}
{'loss': 1.5938, 'grad_norm': 0.903252899646759, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4760007858276367, 'eval_runtime': 9.5057, 'eval_samples_per_second': 105.2, 'eval_steps_per_second': 6.628, 'epoch': 0.08}
{'loss': 1.2814, 'grad_norm': 0.2962026000022888, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4072015285491943, 'eval_runtime': 9.5544, 'eval_samples_per_second': 104.663, 'eval_steps_per_second': 6.594, 'epoch': 0.12}
{'loss': 1.1603, 'grad_norm': 0.2149306684732437, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4166251420974731, 'eval_runtime': 9.6101, 'eval_samples_per_second': 104.058, 'eval_steps_per_second': 6.556, 'epoch': 0.16}
{'loss': 1.161, 'grad_norm': 0.29839828610420227, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3855150938034058, 'eval_runtime': 9.627, 'eval_samples_per_second': 103.875, 'eval_steps_per_second': 6.544, 'epoch': 0.2}
{'loss': 1.0509, 'grad_norm': 0.21519817411899567, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3793107271194458, 'eval_runtime': 9.6572, 'eval_samples_per_second': 103.55, 'eval_steps_per_second': 6.524, 'epoch': 0.24}
{'loss': 1.1035, 'grad_norm': 0.26906004548072815, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3831355571746826, 'eval_runtime': 9.6404, 'eval_samples_per_second': 103.73, 'eval_steps_per_second': 6.535, 'epoch': 0.28}
{'loss': 1.0384, 'grad_norm': 0.3021266758441925, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3758318424224854, 'eval_runtime': 9.6408, 'eval_samples_per_second': 103.726, 'eval_steps_per_second': 6.535, 'epoch': 0.32}
{'loss': 1.0609, 'grad_norm': 0.20298385620117188, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.365968108177185, 'eval_runtime': 9.6514, 'eval_samples_per_second': 103.612, 'eval_steps_per_second': 6.528, 'epoch': 0.36}
{'loss': 1.1246, 'grad_norm': 0.22741466760635376, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3753652572631836, 'eval_runtime': 9.6291, 'eval_samples_per_second': 103.852, 'eval_steps_per_second': 6.543, 'epoch': 0.4}
{'loss': 1.0577, 'grad_norm': 0.23513248562812805, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3828147649765015, 'eval_runtime': 9.6162, 'eval_samples_per_second': 103.992, 'eval_steps_per_second': 6.551, 'epoch': 0.44}
{'loss': 1.0542, 'grad_norm': 0.23457549512386322, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3778965473175049, 'eval_runtime': 9.7109, 'eval_samples_per_second': 102.977, 'eval_steps_per_second': 6.488, 'epoch': 0.48}
{'loss': 0.9724, 'grad_norm': 0.24926148355007172, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3762619495391846, 'eval_runtime': 9.6817, 'eval_samples_per_second': 103.288, 'eval_steps_per_second': 6.507, 'epoch': 0.52}
{'loss': 1.089, 'grad_norm': 0.38949844241142273, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3738387823104858, 'eval_runtime': 9.6962, 'eval_samples_per_second': 103.133, 'eval_steps_per_second': 6.497, 'epoch': 0.56}
{'loss': 1.0025, 'grad_norm': 0.2287464141845703, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3809032440185547, 'eval_runtime': 9.7116, 'eval_samples_per_second': 102.969, 'eval_steps_per_second': 6.487, 'epoch': 0.6}
{'loss': 0.9782, 'grad_norm': 0.2511228322982788, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3785656690597534, 'eval_runtime': 9.6386, 'eval_samples_per_second': 103.75, 'eval_steps_per_second': 6.536, 'epoch': 0.64}
{'loss': 0.9745, 'grad_norm': 0.20787717401981354, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3830978870391846, 'eval_runtime': 9.6516, 'eval_samples_per_second': 103.61, 'eval_steps_per_second': 6.527, 'epoch': 0.68}
{'loss': 0.9964, 'grad_norm': 0.19730138778686523, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3855950832366943, 'eval_runtime': 9.6294, 'eval_samples_per_second': 103.849, 'eval_steps_per_second': 6.542, 'epoch': 0.72}
{'loss': 0.9837, 'grad_norm': 0.23004062473773956, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3909708261489868, 'eval_runtime': 9.6346, 'eval_samples_per_second': 103.792, 'eval_steps_per_second': 6.539, 'epoch': 0.76}
{'loss': 1.0463, 'grad_norm': 0.19045411050319672, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3924826383590698, 'eval_runtime': 9.6491, 'eval_samples_per_second': 103.637, 'eval_steps_per_second': 6.529, 'epoch': 0.8}
{'loss': 1.0115, 'grad_norm': 0.21446169912815094, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3905328512191772, 'eval_runtime': 9.6359, 'eval_samples_per_second': 103.778, 'eval_steps_per_second': 6.538, 'epoch': 0.84}
{'loss': 0.9519, 'grad_norm': 0.26651009917259216, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.384856939315796, 'eval_runtime': 9.6304, 'eval_samples_per_second': 103.838, 'eval_steps_per_second': 6.542, 'epoch': 0.88}
{'loss': 0.9694, 'grad_norm': 0.21357476711273193, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3826526403427124, 'eval_runtime': 9.6237, 'eval_samples_per_second': 103.91, 'eval_steps_per_second': 6.546, 'epoch': 0.92}
{'loss': 0.9909, 'grad_norm': 0.22317157685756683, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3854777812957764, 'eval_runtime': 9.6177, 'eval_samples_per_second': 103.975, 'eval_steps_per_second': 6.55, 'epoch': 0.96}
{'loss': 0.9807, 'grad_norm': 0.23916614055633545, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3855143785476685, 'eval_runtime': 9.6311, 'eval_samples_per_second': 103.83, 'eval_steps_per_second': 6.541, 'epoch': 1.0}
{'train_runtime': 426.8777, 'train_samples_per_second': 23.417, 'train_steps_per_second': 1.464, 'train_loss': 1.1631912292480469, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5293726921081543, 1.4760007858276367, 1.4072015285491943, 1.4166251420974731, 1.3855150938034058, 1.3793107271194458, 1.3831355571746826, 1.3758318424224854, 1.365968108177185, 1.3753652572631836, 1.3828147649765015, 1.3778965473175049, 1.3762619495391846, 1.3738387823104858, 1.3809032440185547, 1.3785656690597534, 1.3830978870391846, 1.3855950832366943, 1.3909708261489868, 1.3924826383590698, 1.3905328512191772, 1.384856939315796, 1.3826526403427124, 1.3854777812957764, 1.3855143785476685], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5293726921081543, 1.4760007858276367, 1.4072015285491943, 1.4166251420974731, 1.3855150938034058, 1.3793107271194458, 1.3831355571746826, 1.3758318424224854, 1.365968108177185, 1.3753652572631836, 1.3828147649765015, 1.3778965473175049, 1.3762619495391846, 1.3738387823104858, 1.3809032440185547, 1.3785656690597534, 1.3830978870391846, 1.3855950832366943, 1.3909708261489868, 1.3924826383590698, 1.3905328512191772, 1.384856939315796, 1.3826526403427124, 1.3854777812957764, 1.3855143785476685]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.066023826599121
current iteration best possible eval_loss (full train run):  -1.3855143785476685
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7431 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7144438028335571, 0.8165992498397827, 0.8581719994544983, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9551875591278076, 0.3273009657859802, 0.982620120048523, 0.5352497100830078, 0.8245673179626465, 0.7869956493377686]  ‚Üí  acq = -1.0387965595737785
X = [0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8929670453071594, 0.7314273715019226, 0.9218059778213501, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9409732222557068, 0.9331071972846985, 0.3031251132488251, 0.36077266931533813, 0.7808123826980591, 0.5021045207977295]  ‚Üí  acq = -1.0381362016209623
X = [0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5558621883392334, 0.5517953038215637, 0.7979342937469482, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9662931561470032, 0.5031551122665405, 0.721409022808075, 0.2269490361213684, 0.22567161917686462, 0.27278316020965576]  ‚Üí  acq = -1.0382945304854374
X = [0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.10871285200119019, 0.0291365385055542, 0.37302812933921814, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.31458795070648193, 0.4884251356124878, 0.9660760760307312, 0.2274898886680603, 0.6486310958862305, 0.36883002519607544]  ‚Üí  acq = -1.0382945423675265
X = [0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3409688472747803, 0.6981962323188782, 0.7720170617103577, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.50350022315979, 0.3907546401023865, 0.20211346447467804, 0.09688013792037964, 0.7278459072113037, 0.040509939193725586]  ‚Üí  acq = -1.0383046344116464
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2269, dtype=torch.float64), tensor(0.0104, dtype=torch.float64), tensor(0.2953, dtype=torch.float64), tensor(0.0822, dtype=torch.float64), 0, tensor(0.1945, dtype=torch.float64), tensor(0.1349, dtype=torch.float64), tensor(0.0559, dtype=torch.float64), 11, 0, 1, 0, 1, 1, 2, 0.0012352831605959868, 29.42695011850363, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2269, dtype=torch.float64), tensor(0.0104, dtype=torch.float64), tensor(0.2953, dtype=torch.float64), tensor(0.0822, dtype=torch.float64), tensor(5.4428e-18, dtype=torch.float64), tensor(0.1945, dtype=torch.float64), tensor(0.1349, dtype=torch.float64), tensor(0.0559, dtype=torch.float64), tensor(0.3322, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.0124, dtype=torch.float64), tensor(0.6131, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.227
  rowan_hellaswag: 0.01
  sciq: 0.295
  triviaqa: 0.082
  truthfulqa_gen: 0
  wikitext: 0.195
  mmlu: 0.135
  arc_challenge: 0.056

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0012352831605959868,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (29.42695011850363,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  2
lora dropout:  0.0012352831605959868
lora alpha:  29.42695011850363
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 923,648 || all params: 8,031,184,896 || trainable%: 0.0115
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1543, 'grad_norm': 5.734306812286377, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.532700538635254, 'eval_runtime': 9.3984, 'eval_samples_per_second': 106.401, 'eval_steps_per_second': 6.703, 'epoch': 0.04}
{'loss': 1.6704, 'grad_norm': 2.729449987411499, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0844675302505493, 'eval_runtime': 9.464, 'eval_samples_per_second': 105.663, 'eval_steps_per_second': 6.657, 'epoch': 0.08}
{'loss': 1.311, 'grad_norm': 1.900697112083435, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9749830365180969, 'eval_runtime': 9.4825, 'eval_samples_per_second': 105.457, 'eval_steps_per_second': 6.644, 'epoch': 0.12}
{'loss': 1.2546, 'grad_norm': 1.3821609020233154, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9543143510818481, 'eval_runtime': 9.5087, 'eval_samples_per_second': 105.167, 'eval_steps_per_second': 6.625, 'epoch': 0.16}
{'loss': 1.1955, 'grad_norm': 1.1203962564468384, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9434481263160706, 'eval_runtime': 9.5311, 'eval_samples_per_second': 104.92, 'eval_steps_per_second': 6.61, 'epoch': 0.2}
{'loss': 1.2266, 'grad_norm': 1.2894765138626099, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9358951449394226, 'eval_runtime': 9.5145, 'eval_samples_per_second': 105.102, 'eval_steps_per_second': 6.621, 'epoch': 0.24}
{'loss': 1.2414, 'grad_norm': 1.5013186931610107, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9260128140449524, 'eval_runtime': 9.5403, 'eval_samples_per_second': 104.818, 'eval_steps_per_second': 6.604, 'epoch': 0.28}
{'loss': 1.1869, 'grad_norm': 1.135067343711853, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.923748791217804, 'eval_runtime': 9.561, 'eval_samples_per_second': 104.592, 'eval_steps_per_second': 6.589, 'epoch': 0.32}
{'loss': 1.2096, 'grad_norm': 1.3479056358337402, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9266759157180786, 'eval_runtime': 9.5573, 'eval_samples_per_second': 104.632, 'eval_steps_per_second': 6.592, 'epoch': 0.36}
{'loss': 1.2246, 'grad_norm': 1.14944326877594, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.92115318775177, 'eval_runtime': 9.5487, 'eval_samples_per_second': 104.726, 'eval_steps_per_second': 6.598, 'epoch': 0.4}
{'loss': 1.1839, 'grad_norm': 1.1280517578125, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9237269759178162, 'eval_runtime': 9.5275, 'eval_samples_per_second': 104.959, 'eval_steps_per_second': 6.612, 'epoch': 0.44}
{'loss': 1.2, 'grad_norm': 1.0706616640090942, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9160659909248352, 'eval_runtime': 9.529, 'eval_samples_per_second': 104.943, 'eval_steps_per_second': 6.611, 'epoch': 0.48}
{'loss': 1.2158, 'grad_norm': 1.7071254253387451, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9169917106628418, 'eval_runtime': 9.5185, 'eval_samples_per_second': 105.058, 'eval_steps_per_second': 6.619, 'epoch': 0.52}
{'loss': 1.205, 'grad_norm': 1.0482831001281738, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9101279377937317, 'eval_runtime': 9.5228, 'eval_samples_per_second': 105.012, 'eval_steps_per_second': 6.616, 'epoch': 0.56}
{'loss': 1.1179, 'grad_norm': 0.9502348899841309, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9072405099868774, 'eval_runtime': 9.4984, 'eval_samples_per_second': 105.28, 'eval_steps_per_second': 6.633, 'epoch': 0.6}
{'loss': 1.1558, 'grad_norm': 0.9470677375793457, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9084478616714478, 'eval_runtime': 9.5379, 'eval_samples_per_second': 104.845, 'eval_steps_per_second': 6.605, 'epoch': 0.64}
{'loss': 1.1905, 'grad_norm': 0.9136092662811279, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9058395624160767, 'eval_runtime': 9.5186, 'eval_samples_per_second': 105.058, 'eval_steps_per_second': 6.619, 'epoch': 0.68}
{'loss': 1.2196, 'grad_norm': 0.8326947689056396, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9021783471107483, 'eval_runtime': 9.542, 'eval_samples_per_second': 104.8, 'eval_steps_per_second': 6.602, 'epoch': 0.72}
{'loss': 1.2002, 'grad_norm': 1.3240327835083008, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9042816162109375, 'eval_runtime': 9.5687, 'eval_samples_per_second': 104.508, 'eval_steps_per_second': 6.584, 'epoch': 0.76}
{'loss': 1.2011, 'grad_norm': 0.8581991791725159, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.90431809425354, 'eval_runtime': 9.6126, 'eval_samples_per_second': 104.03, 'eval_steps_per_second': 6.554, 'epoch': 0.8}
{'loss': 1.1826, 'grad_norm': 0.9253328442573547, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9018493294715881, 'eval_runtime': 9.6103, 'eval_samples_per_second': 104.055, 'eval_steps_per_second': 6.555, 'epoch': 0.84}
{'loss': 1.1718, 'grad_norm': 0.960412323474884, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9008716940879822, 'eval_runtime': 9.5543, 'eval_samples_per_second': 104.665, 'eval_steps_per_second': 6.594, 'epoch': 0.88}
{'loss': 1.1792, 'grad_norm': 0.9029216170310974, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9013029932975769, 'eval_runtime': 9.5495, 'eval_samples_per_second': 104.718, 'eval_steps_per_second': 6.597, 'epoch': 0.92}
{'loss': 1.1693, 'grad_norm': 1.0715640783309937, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9005770683288574, 'eval_runtime': 9.5354, 'eval_samples_per_second': 104.872, 'eval_steps_per_second': 6.607, 'epoch': 0.96}
{'loss': 1.1848, 'grad_norm': 1.3922734260559082, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9001981019973755, 'eval_runtime': 9.5428, 'eval_samples_per_second': 104.791, 'eval_steps_per_second': 6.602, 'epoch': 1.0}
{'train_runtime': 377.1642, 'train_samples_per_second': 26.5, 'train_steps_per_second': 1.657, 'train_loss': 1.2981006408691407, 'epoch': 1.0}
train_results:  {'eval_loss': [1.532700538635254, 1.0844675302505493, 0.9749830365180969, 0.9543143510818481, 0.9434481263160706, 0.9358951449394226, 0.9260128140449524, 0.923748791217804, 0.9266759157180786, 0.92115318775177, 0.9237269759178162, 0.9160659909248352, 0.9169917106628418, 0.9101279377937317, 0.9072405099868774, 0.9084478616714478, 0.9058395624160767, 0.9021783471107483, 0.9042816162109375, 0.90431809425354, 0.9018493294715881, 0.9008716940879822, 0.9013029932975769, 0.9005770683288574, 0.9001981019973755], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.532700538635254, 1.0844675302505493, 0.9749830365180969, 0.9543143510818481, 0.9434481263160706, 0.9358951449394226, 0.9260128140449524, 0.923748791217804, 0.9266759157180786, 0.92115318775177, 0.9237269759178162, 0.9160659909248352, 0.9169917106628418, 0.9101279377937317, 0.9072405099868774, 0.9084478616714478, 0.9058395624160767, 0.9021783471107483, 0.9042816162109375, 0.90431809425354, 0.9018493294715881, 0.9008716940879822, 0.9013029932975769, 0.9005770683288574, 0.9001981019973755]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.9471611976623535
current iteration best possible eval_loss (full train run):  -0.9001981019973755
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6678 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -0.9506063315293058
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.9556889837987801
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -1.0100168364671003
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -0.9403408580337126
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -0.978996849489126
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2391, dtype=torch.float64), 0, 0, tensor(0.2563, dtype=torch.float64), tensor(0.3100, dtype=torch.float64), 0, 0, tensor(0.1946, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 44, 0.002500465333614713, 31.732421426659037, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2391, dtype=torch.float64), tensor(2.3857e-16, dtype=torch.float64), tensor(3.8406e-19, dtype=torch.float64), tensor(0.2563, dtype=torch.float64), tensor(0.3100, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.2185e-20, dtype=torch.float64), tensor(0.1946, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3412, dtype=torch.float64), tensor(0.0250, dtype=torch.float64), tensor(0.6611, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.239
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.256
  truthfulqa_gen: 0.31
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.195

LoRA Parameters:
  lora_r: (44,)
  lora_dropout: (0.002500465333614713,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (31.732421426659037,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  44
lora dropout:  0.002500465333614713
lora alpha:  31.732421426659037
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 59,113,472 || all params: 8,089,374,720 || trainable%: 0.7308
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7095, 'grad_norm': 0.9469239115715027, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1836270093917847, 'eval_runtime': 10.4726, 'eval_samples_per_second': 95.487, 'eval_steps_per_second': 6.016, 'epoch': 0.04}
{'loss': 1.0406, 'grad_norm': 0.6947559714317322, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9363123774528503, 'eval_runtime': 10.5061, 'eval_samples_per_second': 95.182, 'eval_steps_per_second': 5.996, 'epoch': 0.08}
{'loss': 0.893, 'grad_norm': 0.46471869945526123, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.898402214050293, 'eval_runtime': 10.5268, 'eval_samples_per_second': 94.996, 'eval_steps_per_second': 5.985, 'epoch': 0.12}
{'loss': 0.8685, 'grad_norm': 0.41077920794487, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8954979181289673, 'eval_runtime': 10.5618, 'eval_samples_per_second': 94.68, 'eval_steps_per_second': 5.965, 'epoch': 0.16}
{'loss': 0.8654, 'grad_norm': 0.29961419105529785, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8826660513877869, 'eval_runtime': 10.5879, 'eval_samples_per_second': 94.447, 'eval_steps_per_second': 5.95, 'epoch': 0.2}
{'loss': 0.842, 'grad_norm': 0.32581135630607605, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8876095414161682, 'eval_runtime': 10.6249, 'eval_samples_per_second': 94.119, 'eval_steps_per_second': 5.929, 'epoch': 0.24}
{'loss': 0.7951, 'grad_norm': 0.3051535487174988, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8772000670433044, 'eval_runtime': 10.6828, 'eval_samples_per_second': 93.608, 'eval_steps_per_second': 5.897, 'epoch': 0.28}
{'loss': 0.8057, 'grad_norm': 0.2604127824306488, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8699139952659607, 'eval_runtime': 10.6755, 'eval_samples_per_second': 93.673, 'eval_steps_per_second': 5.901, 'epoch': 0.32}
{'loss': 0.8089, 'grad_norm': 0.3364271819591522, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8641175031661987, 'eval_runtime': 10.676, 'eval_samples_per_second': 93.668, 'eval_steps_per_second': 5.901, 'epoch': 0.36}
{'loss': 0.7989, 'grad_norm': 0.2888930141925812, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8594325184822083, 'eval_runtime': 10.6307, 'eval_samples_per_second': 94.067, 'eval_steps_per_second': 5.926, 'epoch': 0.4}
{'loss': 0.752, 'grad_norm': 0.3400215804576874, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8596325516700745, 'eval_runtime': 10.5767, 'eval_samples_per_second': 94.548, 'eval_steps_per_second': 5.956, 'epoch': 0.44}
{'loss': 0.7876, 'grad_norm': 0.3586975932121277, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8566269874572754, 'eval_runtime': 10.6051, 'eval_samples_per_second': 94.294, 'eval_steps_per_second': 5.941, 'epoch': 0.48}
{'loss': 0.7366, 'grad_norm': 0.32498374581336975, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8517118096351624, 'eval_runtime': 10.5961, 'eval_samples_per_second': 94.374, 'eval_steps_per_second': 5.946, 'epoch': 0.52}
{'loss': 0.7419, 'grad_norm': 0.31084907054901123, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8534238338470459, 'eval_runtime': 10.6083, 'eval_samples_per_second': 94.266, 'eval_steps_per_second': 5.939, 'epoch': 0.56}
{'loss': 0.7653, 'grad_norm': 0.3266385495662689, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8518109917640686, 'eval_runtime': 10.6055, 'eval_samples_per_second': 94.291, 'eval_steps_per_second': 5.94, 'epoch': 0.6}
{'loss': 0.7299, 'grad_norm': 0.3989716172218323, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8453678488731384, 'eval_runtime': 10.6036, 'eval_samples_per_second': 94.308, 'eval_steps_per_second': 5.941, 'epoch': 0.64}
{'loss': 0.7325, 'grad_norm': 0.320017546415329, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8430376052856445, 'eval_runtime': 10.6081, 'eval_samples_per_second': 94.268, 'eval_steps_per_second': 5.939, 'epoch': 0.68}
{'loss': 0.6875, 'grad_norm': 0.4892357587814331, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8459526300430298, 'eval_runtime': 10.6069, 'eval_samples_per_second': 94.278, 'eval_steps_per_second': 5.94, 'epoch': 0.72}
{'loss': 0.735, 'grad_norm': 0.3357386589050293, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8425372838973999, 'eval_runtime': 10.5988, 'eval_samples_per_second': 94.35, 'eval_steps_per_second': 5.944, 'epoch': 0.76}
{'loss': 0.712, 'grad_norm': 0.3396036624908447, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8396910429000854, 'eval_runtime': 10.5283, 'eval_samples_per_second': 94.983, 'eval_steps_per_second': 5.984, 'epoch': 0.8}
{'loss': 0.7066, 'grad_norm': 0.4088384211063385, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8400213122367859, 'eval_runtime': 10.5266, 'eval_samples_per_second': 94.997, 'eval_steps_per_second': 5.985, 'epoch': 0.84}
{'loss': 0.6809, 'grad_norm': 0.4244941473007202, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8393458127975464, 'eval_runtime': 10.5243, 'eval_samples_per_second': 95.018, 'eval_steps_per_second': 5.986, 'epoch': 0.88}
{'loss': 0.6679, 'grad_norm': 0.3229701817035675, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.838193953037262, 'eval_runtime': 10.5072, 'eval_samples_per_second': 95.173, 'eval_steps_per_second': 5.996, 'epoch': 0.92}
{'loss': 0.6813, 'grad_norm': 0.31638503074645996, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8373761177062988, 'eval_runtime': 10.5179, 'eval_samples_per_second': 95.076, 'eval_steps_per_second': 5.99, 'epoch': 0.96}
{'loss': 0.6788, 'grad_norm': 0.4426581561565399, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8371533155441284, 'eval_runtime': 10.5201, 'eval_samples_per_second': 95.056, 'eval_steps_per_second': 5.989, 'epoch': 1.0}
{'train_runtime': 487.5419, 'train_samples_per_second': 20.507, 'train_steps_per_second': 1.282, 'train_loss': 0.8489371368408203, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1836270093917847, 0.9363123774528503, 0.898402214050293, 0.8954979181289673, 0.8826660513877869, 0.8876095414161682, 0.8772000670433044, 0.8699139952659607, 0.8641175031661987, 0.8594325184822083, 0.8596325516700745, 0.8566269874572754, 0.8517118096351624, 0.8534238338470459, 0.8518109917640686, 0.8453678488731384, 0.8430376052856445, 0.8459526300430298, 0.8425372838973999, 0.8396910429000854, 0.8400213122367859, 0.8393458127975464, 0.838193953037262, 0.8373761177062988, 0.8371533155441284], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1836270093917847, 0.9363123774528503, 0.898402214050293, 0.8954979181289673, 0.8826660513877869, 0.8876095414161682, 0.8772000670433044, 0.8699139952659607, 0.8641175031661987, 0.8594325184822083, 0.8596325516700745, 0.8566269874572754, 0.8517118096351624, 0.8534238338470459, 0.8518109917640686, 0.8453678488731384, 0.8430376052856445, 0.8459526300430298, 0.8425372838973999, 0.8396910429000854, 0.8400213122367859, 0.8393458127975464, 0.838193953037262, 0.8373761177062988, 0.8371533155441284]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1642560958862305
current iteration best possible eval_loss (full train run):  -0.8371533155441284
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8108 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -1.022587138704703
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.9861853493355348
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.9809733556389253
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.9911815746798726
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -1.0161870468939083
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1887, dtype=torch.float64), tensor(0.2264, dtype=torch.float64), 0, 0, 0, tensor(0.0400, dtype=torch.float64), tensor(0.0220, dtype=torch.float64), tensor(0.0938, dtype=torch.float64), tensor(0.4291, dtype=torch.float64), 13, 0, 0, 0, 1, 0, 100, 0.08226646575754476, 18.950945804511566, 1]
normalized proposed parameters for next round by BO: [tensor(0.1887, dtype=torch.float64), tensor(0.2264, dtype=torch.float64), tensor(1.4806e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.6334e-17, dtype=torch.float64), tensor(0.0400, dtype=torch.float64), tensor(0.0220, dtype=torch.float64), tensor(0.0938, dtype=torch.float64), tensor(0.4291, dtype=torch.float64), tensor(0.4030, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7813, dtype=torch.float64), tensor(0.8227, dtype=torch.float64), tensor(0.3948, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.189
  gsm8k: 0.226
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.04
  wikitext: 0.022
  mmlu: 0.094
  arc_challenge: 0.429

LoRA Parameters:
  lora_r: (100,)
  lora_dropout: (0.08226646575754476,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (18.950945804511566,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  100
lora dropout:  0.08226646575754476
lora alpha:  18.950945804511566
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 23,961,600 || all params: 8,054,222,848 || trainable%: 0.2975
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0982, 'grad_norm': 1.1276365518569946, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6462956666946411, 'eval_runtime': 9.1711, 'eval_samples_per_second': 109.038, 'eval_steps_per_second': 6.869, 'epoch': 0.04}
{'loss': 1.6114, 'grad_norm': 0.6642746925354004, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2140969038009644, 'eval_runtime': 9.2256, 'eval_samples_per_second': 108.394, 'eval_steps_per_second': 6.829, 'epoch': 0.08}
{'loss': 1.1467, 'grad_norm': 0.2401498407125473, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.048511028289795, 'eval_runtime': 9.3175, 'eval_samples_per_second': 107.324, 'eval_steps_per_second': 6.761, 'epoch': 0.12}
{'loss': 1.1027, 'grad_norm': 0.2997346520423889, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9751551747322083, 'eval_runtime': 9.3233, 'eval_samples_per_second': 107.258, 'eval_steps_per_second': 6.757, 'epoch': 0.16}
{'loss': 1.0257, 'grad_norm': 0.35470449924468994, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9547613263130188, 'eval_runtime': 9.3266, 'eval_samples_per_second': 107.22, 'eval_steps_per_second': 6.755, 'epoch': 0.2}
{'loss': 1.0616, 'grad_norm': 0.2175983488559723, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9387779831886292, 'eval_runtime': 9.2766, 'eval_samples_per_second': 107.798, 'eval_steps_per_second': 6.791, 'epoch': 0.24}
{'loss': 1.0152, 'grad_norm': 0.23808738589286804, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9244155883789062, 'eval_runtime': 9.2577, 'eval_samples_per_second': 108.018, 'eval_steps_per_second': 6.805, 'epoch': 0.28}
{'loss': 1.0178, 'grad_norm': 0.29160159826278687, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9118847846984863, 'eval_runtime': 9.2865, 'eval_samples_per_second': 107.684, 'eval_steps_per_second': 6.784, 'epoch': 0.32}
{'loss': 0.9935, 'grad_norm': 0.2865077257156372, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9149068593978882, 'eval_runtime': 9.2753, 'eval_samples_per_second': 107.814, 'eval_steps_per_second': 6.792, 'epoch': 0.36}
{'loss': 0.9339, 'grad_norm': 0.22746941447257996, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9005826711654663, 'eval_runtime': 9.2649, 'eval_samples_per_second': 107.934, 'eval_steps_per_second': 6.8, 'epoch': 0.4}
{'loss': 0.9356, 'grad_norm': 0.21151188015937805, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8972781896591187, 'eval_runtime': 9.2758, 'eval_samples_per_second': 107.807, 'eval_steps_per_second': 6.792, 'epoch': 0.44}
{'loss': 0.9379, 'grad_norm': 0.21688483655452728, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8939537405967712, 'eval_runtime': 9.2662, 'eval_samples_per_second': 107.919, 'eval_steps_per_second': 6.799, 'epoch': 0.48}
{'loss': 0.9501, 'grad_norm': 0.21791508793830872, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8913120627403259, 'eval_runtime': 9.2824, 'eval_samples_per_second': 107.731, 'eval_steps_per_second': 6.787, 'epoch': 0.52}
{'loss': 0.9024, 'grad_norm': 0.22989530861377716, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8906044363975525, 'eval_runtime': 9.2982, 'eval_samples_per_second': 107.548, 'eval_steps_per_second': 6.776, 'epoch': 0.56}
{'loss': 0.9403, 'grad_norm': 0.23862066864967346, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8918291330337524, 'eval_runtime': 9.2594, 'eval_samples_per_second': 107.999, 'eval_steps_per_second': 6.804, 'epoch': 0.6}
{'loss': 0.9186, 'grad_norm': 0.2046194076538086, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.887338399887085, 'eval_runtime': 9.2549, 'eval_samples_per_second': 108.051, 'eval_steps_per_second': 6.807, 'epoch': 0.64}
{'loss': 0.9491, 'grad_norm': 0.20998631417751312, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8866904973983765, 'eval_runtime': 9.2706, 'eval_samples_per_second': 107.868, 'eval_steps_per_second': 6.796, 'epoch': 0.68}
{'loss': 0.9241, 'grad_norm': 0.18607263267040253, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8831325769424438, 'eval_runtime': 9.2984, 'eval_samples_per_second': 107.545, 'eval_steps_per_second': 6.775, 'epoch': 0.72}
{'loss': 0.9001, 'grad_norm': 0.21751658618450165, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8812049627304077, 'eval_runtime': 9.2806, 'eval_samples_per_second': 107.751, 'eval_steps_per_second': 6.788, 'epoch': 0.76}
{'loss': 0.9352, 'grad_norm': 0.22979359328746796, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8779218792915344, 'eval_runtime': 9.2531, 'eval_samples_per_second': 108.071, 'eval_steps_per_second': 6.808, 'epoch': 0.8}
{'loss': 0.9244, 'grad_norm': 0.22354842722415924, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.881883442401886, 'eval_runtime': 9.2616, 'eval_samples_per_second': 107.973, 'eval_steps_per_second': 6.802, 'epoch': 0.84}
{'loss': 0.9312, 'grad_norm': 0.18554234504699707, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8823299407958984, 'eval_runtime': 9.2576, 'eval_samples_per_second': 108.02, 'eval_steps_per_second': 6.805, 'epoch': 0.88}
{'loss': 0.9026, 'grad_norm': 0.18326108157634735, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8758642673492432, 'eval_runtime': 9.2429, 'eval_samples_per_second': 108.191, 'eval_steps_per_second': 6.816, 'epoch': 0.92}
{'loss': 0.912, 'grad_norm': 0.20013459026813507, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8776296377182007, 'eval_runtime': 9.2107, 'eval_samples_per_second': 108.569, 'eval_steps_per_second': 6.84, 'epoch': 0.96}
{'loss': 0.9233, 'grad_norm': 0.22169867157936096, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8765466213226318, 'eval_runtime': 9.2559, 'eval_samples_per_second': 108.039, 'eval_steps_per_second': 6.806, 'epoch': 1.0}
{'train_runtime': 419.7459, 'train_samples_per_second': 23.817, 'train_steps_per_second': 1.489, 'train_loss': 1.0757467803955079, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6462956666946411, 1.2140969038009644, 1.048511028289795, 0.9751551747322083, 0.9547613263130188, 0.9387779831886292, 0.9244155883789062, 0.9118847846984863, 0.9149068593978882, 0.9005826711654663, 0.8972781896591187, 0.8939537405967712, 0.8913120627403259, 0.8906044363975525, 0.8918291330337524, 0.887338399887085, 0.8866904973983765, 0.8831325769424438, 0.8812049627304077, 0.8779218792915344, 0.881883442401886, 0.8823299407958984, 0.8758642673492432, 0.8776296377182007, 0.8765466213226318], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6462956666946411, 1.2140969038009644, 1.048511028289795, 0.9751551747322083, 0.9547613263130188, 0.9387779831886292, 0.9244155883789062, 0.9118847846984863, 0.9149068593978882, 0.9005826711654663, 0.8972781896591187, 0.8939537405967712, 0.8913120627403259, 0.8906044363975525, 0.8918291330337524, 0.887338399887085, 0.8866904973983765, 0.8831325769424438, 0.8812049627304077, 0.8779218792915344, 0.881883442401886, 0.8823299407958984, 0.8758642673492432, 0.8776296377182007, 0.8765466213226318]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2092981338500977
current iteration best possible eval_loss (full train run):  -0.8765466213226318
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4234 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -1.200286047305282
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -1.012316450071308
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -1.004373806759697
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -1.0536241371909982
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -1.0168679845510873
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2997, dtype=torch.float64), tensor(0.2771, dtype=torch.float64), 0, tensor(0.3172, dtype=torch.float64), 0, 0, tensor(0.0359, dtype=torch.float64), 0, tensor(0.0701, dtype=torch.float64), 2, 0, 0, 0, 1, 1, 38, 0.010201960813941355, 15.72494546177599, 1]
normalized proposed parameters for next round by BO: [tensor(0.2997, dtype=torch.float64), tensor(0.2771, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3172, dtype=torch.float64), tensor(1.1539e-17, dtype=torch.float64), tensor(1.8769e-18, dtype=torch.float64), tensor(0.0359, dtype=torch.float64), tensor(9.0370e-19, dtype=torch.float64), tensor(0.0701, dtype=torch.float64), tensor(0.0752, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2988, dtype=torch.float64), tensor(0.1020, dtype=torch.float64), tensor(0.3276, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.3
  gsm8k: 0.277
  rowan_hellaswag: 0
  sciq: 0.317
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.036
  mmlu: 0
  arc_challenge: 0.07

LoRA Parameters:
  lora_r: (38,)
  lora_dropout: (0.010201960813941355,)
  num_layers_to_apply: (2,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (15.72494546177599,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  2
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  38
lora dropout:  0.010201960813941355
lora alpha:  15.72494546177599
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,801,664 || all params: 8,033,062,912 || trainable%: 0.0349
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5423, 'grad_norm': 4.746723651885986, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9993246793746948, 'eval_runtime': 8.903, 'eval_samples_per_second': 112.322, 'eval_steps_per_second': 7.076, 'epoch': 0.04}
{'loss': 2.248, 'grad_norm': 1.3434244394302368, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3965367078781128, 'eval_runtime': 8.9206, 'eval_samples_per_second': 112.1, 'eval_steps_per_second': 7.062, 'epoch': 0.08}
{'loss': 1.4399, 'grad_norm': 1.3230780363082886, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2130776643753052, 'eval_runtime': 8.9334, 'eval_samples_per_second': 111.939, 'eval_steps_per_second': 7.052, 'epoch': 0.12}
{'loss': 1.1868, 'grad_norm': 0.7399327158927917, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0536034107208252, 'eval_runtime': 8.992, 'eval_samples_per_second': 111.21, 'eval_steps_per_second': 7.006, 'epoch': 0.16}
{'loss': 1.0741, 'grad_norm': 1.2973897457122803, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.002950668334961, 'eval_runtime': 8.9992, 'eval_samples_per_second': 111.121, 'eval_steps_per_second': 7.001, 'epoch': 0.2}
{'loss': 1.025, 'grad_norm': 0.636515200138092, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9617281556129456, 'eval_runtime': 9.0162, 'eval_samples_per_second': 110.912, 'eval_steps_per_second': 6.987, 'epoch': 0.24}
{'loss': 0.9873, 'grad_norm': 0.4818914830684662, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9505211710929871, 'eval_runtime': 9.0259, 'eval_samples_per_second': 110.793, 'eval_steps_per_second': 6.98, 'epoch': 0.28}
{'loss': 0.9893, 'grad_norm': 0.5546190738677979, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9452702403068542, 'eval_runtime': 9.0235, 'eval_samples_per_second': 110.821, 'eval_steps_per_second': 6.982, 'epoch': 0.32}
{'loss': 0.995, 'grad_norm': 0.5671288967132568, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.944091796875, 'eval_runtime': 8.9967, 'eval_samples_per_second': 111.152, 'eval_steps_per_second': 7.003, 'epoch': 0.36}
{'loss': 0.9472, 'grad_norm': 0.37459173798561096, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9376829862594604, 'eval_runtime': 9.017, 'eval_samples_per_second': 110.901, 'eval_steps_per_second': 6.987, 'epoch': 0.4}
{'loss': 0.9633, 'grad_norm': 0.36498358845710754, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9287555813789368, 'eval_runtime': 9.073, 'eval_samples_per_second': 110.218, 'eval_steps_per_second': 6.944, 'epoch': 0.44}
{'loss': 1.0233, 'grad_norm': 0.4206593334674835, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9281057119369507, 'eval_runtime': 9.0218, 'eval_samples_per_second': 110.843, 'eval_steps_per_second': 6.983, 'epoch': 0.48}
{'loss': 0.9459, 'grad_norm': 0.48652276396751404, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.927646279335022, 'eval_runtime': 9.0146, 'eval_samples_per_second': 110.931, 'eval_steps_per_second': 6.989, 'epoch': 0.52}
{'loss': 0.9394, 'grad_norm': 0.4062967896461487, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9252879619598389, 'eval_runtime': 9.0146, 'eval_samples_per_second': 110.931, 'eval_steps_per_second': 6.989, 'epoch': 0.56}
{'loss': 0.93, 'grad_norm': 0.3701660633087158, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9236154556274414, 'eval_runtime': 9.0156, 'eval_samples_per_second': 110.919, 'eval_steps_per_second': 6.988, 'epoch': 0.6}
{'loss': 0.9757, 'grad_norm': 0.44513821601867676, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9195188283920288, 'eval_runtime': 9.0497, 'eval_samples_per_second': 110.502, 'eval_steps_per_second': 6.962, 'epoch': 0.64}
{'loss': 0.9225, 'grad_norm': 0.3516426086425781, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9183623194694519, 'eval_runtime': 9.0464, 'eval_samples_per_second': 110.541, 'eval_steps_per_second': 6.964, 'epoch': 0.68}
{'loss': 0.9285, 'grad_norm': 0.4213149845600128, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9147016406059265, 'eval_runtime': 9.0516, 'eval_samples_per_second': 110.478, 'eval_steps_per_second': 6.96, 'epoch': 0.72}
{'loss': 0.9336, 'grad_norm': 0.36411499977111816, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9139090776443481, 'eval_runtime': 9.0613, 'eval_samples_per_second': 110.359, 'eval_steps_per_second': 6.953, 'epoch': 0.76}
{'loss': 0.9391, 'grad_norm': 0.4095207452774048, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9145320057868958, 'eval_runtime': 9.0993, 'eval_samples_per_second': 109.899, 'eval_steps_per_second': 6.924, 'epoch': 0.8}
{'loss': 0.9007, 'grad_norm': 0.538519561290741, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9154287576675415, 'eval_runtime': 9.0462, 'eval_samples_per_second': 110.544, 'eval_steps_per_second': 6.964, 'epoch': 0.84}
{'loss': 0.9558, 'grad_norm': 0.364763468503952, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9150064587593079, 'eval_runtime': 9.0519, 'eval_samples_per_second': 110.474, 'eval_steps_per_second': 6.96, 'epoch': 0.88}
{'loss': 0.9127, 'grad_norm': 0.5186458826065063, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9121812582015991, 'eval_runtime': 9.0219, 'eval_samples_per_second': 110.842, 'eval_steps_per_second': 6.983, 'epoch': 0.92}
{'loss': 0.9421, 'grad_norm': 0.4082312285900116, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9095972776412964, 'eval_runtime': 9.0338, 'eval_samples_per_second': 110.696, 'eval_steps_per_second': 6.974, 'epoch': 0.96}
{'loss': 0.9361, 'grad_norm': 0.4570213258266449, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9094929099082947, 'eval_runtime': 9.0261, 'eval_samples_per_second': 110.79, 'eval_steps_per_second': 6.98, 'epoch': 1.0}
{'train_runtime': 404.8575, 'train_samples_per_second': 24.693, 'train_steps_per_second': 1.544, 'train_loss': 1.1433492126464844, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9993246793746948, 1.3965367078781128, 1.2130776643753052, 1.0536034107208252, 1.002950668334961, 0.9617281556129456, 0.9505211710929871, 0.9452702403068542, 0.944091796875, 0.9376829862594604, 0.9287555813789368, 0.9281057119369507, 0.927646279335022, 0.9252879619598389, 0.9236154556274414, 0.9195188283920288, 0.9183623194694519, 0.9147016406059265, 0.9139090776443481, 0.9145320057868958, 0.9154287576675415, 0.9150064587593079, 0.9121812582015991, 0.9095972776412964, 0.9094929099082947], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9993246793746948, 1.3965367078781128, 1.2130776643753052, 1.0536034107208252, 1.002950668334961, 0.9617281556129456, 0.9505211710929871, 0.9452702403068542, 0.944091796875, 0.9376829862594604, 0.9287555813789368, 0.9281057119369507, 0.927646279335022, 0.9252879619598389, 0.9236154556274414, 0.9195188283920288, 0.9183623194694519, 0.9147016406059265, 0.9139090776443481, 0.9145320057868958, 0.9154287576675415, 0.9150064587593079, 0.9121812582015991, 0.9095972776412964, 0.9094929099082947]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.6423835754394531
current iteration best possible eval_loss (full train run):  -0.9094929099082947
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3779 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -0.9818284484377636
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -0.9901464159946647
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -0.9687710331781707
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -0.9882261936427025
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.9784663566844756
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2551, dtype=torch.float64), 0, tensor(0.1103, dtype=torch.float64), 0, tensor(0.0769, dtype=torch.float64), tensor(0.1526, dtype=torch.float64), tensor(0.0371, dtype=torch.float64), tensor(0.1512, dtype=torch.float64), tensor(0.2167, dtype=torch.float64), 26, 0, 1, 0, 0, 1, 72, 0.016461771037997203, 36.578383609164625, 1]
normalized proposed parameters for next round by BO: [tensor(0.2551, dtype=torch.float64), tensor(8.5428e-18, dtype=torch.float64), tensor(0.1103, dtype=torch.float64), tensor(1.8279e-17, dtype=torch.float64), tensor(0.0769, dtype=torch.float64), tensor(0.1526, dtype=torch.float64), tensor(0.0371, dtype=torch.float64), tensor(0.1512, dtype=torch.float64), tensor(0.2167, dtype=torch.float64), tensor(0.8256, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5621, dtype=torch.float64), tensor(0.1646, dtype=torch.float64), tensor(0.7620, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.255
  gsm8k: 0
  rowan_hellaswag: 0.11
  sciq: 0
  triviaqa: 0.077
  truthfulqa_gen: 0.153
  wikitext: 0.037
  mmlu: 0.151
  arc_challenge: 0.217

LoRA Parameters:
  lora_r: (72,)
  lora_dropout: (0.016461771037997203,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (36.578383609164625,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  72
lora dropout:  0.016461771037997203
lora alpha:  36.578383609164625
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 44,089,344 || all params: 8,074,350,592 || trainable%: 0.5460
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.275, 'grad_norm': 1.210331916809082, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6428738832473755, 'eval_runtime': 9.4681, 'eval_samples_per_second': 105.617, 'eval_steps_per_second': 6.654, 'epoch': 0.04}
{'loss': 1.6343, 'grad_norm': 0.3292854428291321, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.4573675394058228, 'eval_runtime': 9.4911, 'eval_samples_per_second': 105.362, 'eval_steps_per_second': 6.638, 'epoch': 0.08}
{'loss': 1.4409, 'grad_norm': 0.32181304693222046, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4203451871871948, 'eval_runtime': 9.5248, 'eval_samples_per_second': 104.989, 'eval_steps_per_second': 6.614, 'epoch': 0.12}
{'loss': 1.2933, 'grad_norm': 0.40829700231552124, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4284965991973877, 'eval_runtime': 9.5628, 'eval_samples_per_second': 104.572, 'eval_steps_per_second': 6.588, 'epoch': 0.16}
{'loss': 1.3149, 'grad_norm': 0.29573893547058105, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4326306581497192, 'eval_runtime': 9.5639, 'eval_samples_per_second': 104.559, 'eval_steps_per_second': 6.587, 'epoch': 0.2}
{'loss': 1.3224, 'grad_norm': 0.2726225256919861, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4287105798721313, 'eval_runtime': 9.5842, 'eval_samples_per_second': 104.338, 'eval_steps_per_second': 6.573, 'epoch': 0.24}
{'loss': 1.1976, 'grad_norm': 0.3012442886829376, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4030243158340454, 'eval_runtime': 9.5901, 'eval_samples_per_second': 104.274, 'eval_steps_per_second': 6.569, 'epoch': 0.28}
{'loss': 1.2084, 'grad_norm': 0.2804328203201294, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4279921054840088, 'eval_runtime': 9.5907, 'eval_samples_per_second': 104.268, 'eval_steps_per_second': 6.569, 'epoch': 0.32}
{'loss': 1.277, 'grad_norm': 0.32564839720726013, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4260145425796509, 'eval_runtime': 9.5805, 'eval_samples_per_second': 104.379, 'eval_steps_per_second': 6.576, 'epoch': 0.36}
{'loss': 1.2305, 'grad_norm': 0.27263110876083374, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4219552278518677, 'eval_runtime': 9.6102, 'eval_samples_per_second': 104.056, 'eval_steps_per_second': 6.556, 'epoch': 0.4}
{'loss': 1.1938, 'grad_norm': 0.418468177318573, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4037353992462158, 'eval_runtime': 9.6275, 'eval_samples_per_second': 103.869, 'eval_steps_per_second': 6.544, 'epoch': 0.44}
{'loss': 1.1837, 'grad_norm': 0.2829340994358063, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4244563579559326, 'eval_runtime': 9.6326, 'eval_samples_per_second': 103.814, 'eval_steps_per_second': 6.54, 'epoch': 0.48}
{'loss': 1.1347, 'grad_norm': 0.26209503412246704, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4579170942306519, 'eval_runtime': 9.5994, 'eval_samples_per_second': 104.174, 'eval_steps_per_second': 6.563, 'epoch': 0.52}
{'loss': 1.1641, 'grad_norm': 0.31853222846984863, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3910338878631592, 'eval_runtime': 9.5774, 'eval_samples_per_second': 104.412, 'eval_steps_per_second': 6.578, 'epoch': 0.56}
{'loss': 1.0985, 'grad_norm': 0.28721484541893005, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4051893949508667, 'eval_runtime': 9.5815, 'eval_samples_per_second': 104.368, 'eval_steps_per_second': 6.575, 'epoch': 0.6}
{'loss': 1.1278, 'grad_norm': 0.2828178405761719, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4231586456298828, 'eval_runtime': 9.5866, 'eval_samples_per_second': 104.312, 'eval_steps_per_second': 6.572, 'epoch': 0.64}
{'loss': 1.1191, 'grad_norm': 0.2840428352355957, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4424266815185547, 'eval_runtime': 9.5911, 'eval_samples_per_second': 104.264, 'eval_steps_per_second': 6.569, 'epoch': 0.68}
{'loss': 1.1352, 'grad_norm': 0.2782188057899475, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4509896039962769, 'eval_runtime': 9.5803, 'eval_samples_per_second': 104.381, 'eval_steps_per_second': 6.576, 'epoch': 0.72}
{'loss': 1.08, 'grad_norm': 0.2599547803401947, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4127323627471924, 'eval_runtime': 9.5812, 'eval_samples_per_second': 104.371, 'eval_steps_per_second': 6.575, 'epoch': 0.76}
{'loss': 1.0897, 'grad_norm': 0.304647833108902, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4594948291778564, 'eval_runtime': 9.5918, 'eval_samples_per_second': 104.256, 'eval_steps_per_second': 6.568, 'epoch': 0.8}
{'loss': 1.1068, 'grad_norm': 0.3513945937156677, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4703569412231445, 'eval_runtime': 9.5757, 'eval_samples_per_second': 104.431, 'eval_steps_per_second': 6.579, 'epoch': 0.84}
{'loss': 1.0962, 'grad_norm': 0.3028821647167206, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4760932922363281, 'eval_runtime': 9.5986, 'eval_samples_per_second': 104.182, 'eval_steps_per_second': 6.563, 'epoch': 0.88}
{'loss': 1.0668, 'grad_norm': 0.3209502696990967, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.470716953277588, 'eval_runtime': 9.5936, 'eval_samples_per_second': 104.236, 'eval_steps_per_second': 6.567, 'epoch': 0.92}
{'loss': 1.1067, 'grad_norm': 0.3258785903453827, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4748525619506836, 'eval_runtime': 9.601, 'eval_samples_per_second': 104.156, 'eval_steps_per_second': 6.562, 'epoch': 0.96}
{'loss': 1.124, 'grad_norm': 0.32975056767463684, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4776674509048462, 'eval_runtime': 9.5987, 'eval_samples_per_second': 104.181, 'eval_steps_per_second': 6.563, 'epoch': 1.0}
{'train_runtime': 438.8424, 'train_samples_per_second': 22.778, 'train_steps_per_second': 1.424, 'train_loss': 1.2808609130859374, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6428738832473755, 1.4573675394058228, 1.4203451871871948, 1.4284965991973877, 1.4326306581497192, 1.4287105798721313, 1.4030243158340454, 1.4279921054840088, 1.4260145425796509, 1.4219552278518677, 1.4037353992462158, 1.4244563579559326, 1.4579170942306519, 1.3910338878631592, 1.4051893949508667, 1.4231586456298828, 1.4424266815185547, 1.4509896039962769, 1.4127323627471924, 1.4594948291778564, 1.4703569412231445, 1.4760932922363281, 1.470716953277588, 1.4748525619506836, 1.4776674509048462], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6428738832473755, 1.4573675394058228, 1.4203451871871948, 1.4284965991973877, 1.4326306581497192, 1.4287105798721313, 1.4030243158340454, 1.4279921054840088, 1.4260145425796509, 1.4219552278518677, 1.4037353992462158, 1.4244563579559326, 1.4579170942306519, 1.3910338878631592, 1.4051893949508667, 1.4231586456298828, 1.4424266815185547, 1.4509896039962769, 1.4127323627471924, 1.4594948291778564, 1.4703569412231445, 1.4760932922363281, 1.470716953277588, 1.4748525619506836, 1.4776674509048462]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3131517171859741
current iteration best possible eval_loss (full train run):  -1.4776674509048462
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531, -1.3131517171859741]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1677 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -1.0528137709063143
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -1.0376479455838306
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -1.0053715770289533
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -1.0558269295885356
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -0.8793730530246409
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1065, dtype=torch.float64), 0, 0, tensor(0.0244, dtype=torch.float64), tensor(0.3598, dtype=torch.float64), tensor(0.1224, dtype=torch.float64), tensor(0.0518, dtype=torch.float64), tensor(0.0860, dtype=torch.float64), tensor(0.2491, dtype=torch.float64), 20, 0, 1, 0, 1, 1, 120, 0.014285441661450718, 18.376353835470212, 0]
normalized proposed parameters for next round by BO: [tensor(0.1065, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0244, dtype=torch.float64), tensor(0.3598, dtype=torch.float64), tensor(0.1224, dtype=torch.float64), tensor(0.0518, dtype=torch.float64), tensor(0.0860, dtype=torch.float64), tensor(0.2491, dtype=torch.float64), tensor(0.6383, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9357, dtype=torch.float64), tensor(0.1429, dtype=torch.float64), tensor(0.3828, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.106
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.024
  triviaqa: 0.36
  truthfulqa_gen: 0.122
  wikitext: 0.052
  mmlu: 0.086
  arc_challenge: 0.249

LoRA Parameters:
  lora_r: (120,)
  lora_dropout: (0.014285441661450718,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (18.376353835470212,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  120
lora dropout:  0.014285441661450718
lora alpha:  18.376353835470212
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 100,761,600 || all params: 8,131,022,848 || trainable%: 1.2392
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5799, 'grad_norm': 0.8250672817230225, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5392705202102661, 'eval_runtime': 9.7873, 'eval_samples_per_second': 102.174, 'eval_steps_per_second': 6.437, 'epoch': 0.04}
{'loss': 1.4389, 'grad_norm': 0.4422285556793213, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.337377905845642, 'eval_runtime': 9.8708, 'eval_samples_per_second': 101.309, 'eval_steps_per_second': 6.382, 'epoch': 0.08}
{'loss': 1.072, 'grad_norm': 0.14764472842216492, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3511630296707153, 'eval_runtime': 9.8675, 'eval_samples_per_second': 101.342, 'eval_steps_per_second': 6.385, 'epoch': 0.12}
{'loss': 1.0935, 'grad_norm': 0.1643581986427307, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3264708518981934, 'eval_runtime': 9.9592, 'eval_samples_per_second': 100.409, 'eval_steps_per_second': 6.326, 'epoch': 0.16}
{'loss': 1.0186, 'grad_norm': 0.1626676768064499, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3683571815490723, 'eval_runtime': 9.9736, 'eval_samples_per_second': 100.264, 'eval_steps_per_second': 6.317, 'epoch': 0.2}
{'loss': 1.0703, 'grad_norm': 0.1336088627576828, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3475679159164429, 'eval_runtime': 9.9634, 'eval_samples_per_second': 100.368, 'eval_steps_per_second': 6.323, 'epoch': 0.24}
{'loss': 1.0408, 'grad_norm': 0.1403091549873352, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3756935596466064, 'eval_runtime': 9.9803, 'eval_samples_per_second': 100.197, 'eval_steps_per_second': 6.312, 'epoch': 0.28}
{'loss': 1.0436, 'grad_norm': 0.15860489010810852, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.392499566078186, 'eval_runtime': 9.9741, 'eval_samples_per_second': 100.26, 'eval_steps_per_second': 6.316, 'epoch': 0.32}
{'loss': 0.9954, 'grad_norm': 0.1638113260269165, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4172805547714233, 'eval_runtime': 9.9748, 'eval_samples_per_second': 100.252, 'eval_steps_per_second': 6.316, 'epoch': 0.36}
{'loss': 1.0452, 'grad_norm': 0.16417458653450012, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4127453565597534, 'eval_runtime': 9.9743, 'eval_samples_per_second': 100.258, 'eval_steps_per_second': 6.316, 'epoch': 0.4}
{'loss': 0.9655, 'grad_norm': 0.1862575262784958, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4009007215499878, 'eval_runtime': 9.9908, 'eval_samples_per_second': 100.092, 'eval_steps_per_second': 6.306, 'epoch': 0.44}
{'loss': 1.0063, 'grad_norm': 0.17781586945056915, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3994805812835693, 'eval_runtime': 9.9996, 'eval_samples_per_second': 100.004, 'eval_steps_per_second': 6.3, 'epoch': 0.48}
{'loss': 0.9874, 'grad_norm': 0.21975217759609222, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4018932580947876, 'eval_runtime': 9.9829, 'eval_samples_per_second': 100.171, 'eval_steps_per_second': 6.311, 'epoch': 0.52}
{'loss': 1.0041, 'grad_norm': 0.1865158975124359, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4044196605682373, 'eval_runtime': 9.9732, 'eval_samples_per_second': 100.269, 'eval_steps_per_second': 6.317, 'epoch': 0.56}
{'loss': 0.9635, 'grad_norm': 0.1974107325077057, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.424109935760498, 'eval_runtime': 9.9695, 'eval_samples_per_second': 100.306, 'eval_steps_per_second': 6.319, 'epoch': 0.6}
{'loss': 0.9919, 'grad_norm': 0.17771556973457336, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4293564558029175, 'eval_runtime': 9.9625, 'eval_samples_per_second': 100.377, 'eval_steps_per_second': 6.324, 'epoch': 0.64}
{'loss': 1.0013, 'grad_norm': 0.1939520388841629, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.443474292755127, 'eval_runtime': 9.9638, 'eval_samples_per_second': 100.364, 'eval_steps_per_second': 6.323, 'epoch': 0.68}
{'loss': 1.0322, 'grad_norm': 0.17491687834262848, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4275577068328857, 'eval_runtime': 9.9726, 'eval_samples_per_second': 100.274, 'eval_steps_per_second': 6.317, 'epoch': 0.72}
{'loss': 0.9355, 'grad_norm': 0.20490705966949463, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4412695169448853, 'eval_runtime': 9.9669, 'eval_samples_per_second': 100.332, 'eval_steps_per_second': 6.321, 'epoch': 0.76}
{'loss': 0.9767, 'grad_norm': 0.2180740386247635, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4390360116958618, 'eval_runtime': 9.9739, 'eval_samples_per_second': 100.262, 'eval_steps_per_second': 6.316, 'epoch': 0.8}
{'loss': 0.9879, 'grad_norm': 0.23126409947872162, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4362047910690308, 'eval_runtime': 9.9658, 'eval_samples_per_second': 100.343, 'eval_steps_per_second': 6.322, 'epoch': 0.84}
{'loss': 1.0422, 'grad_norm': 0.2145274579524994, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.440920352935791, 'eval_runtime': 9.9782, 'eval_samples_per_second': 100.219, 'eval_steps_per_second': 6.314, 'epoch': 0.88}
{'loss': 0.97, 'grad_norm': 0.19616837799549103, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4433385133743286, 'eval_runtime': 9.9732, 'eval_samples_per_second': 100.268, 'eval_steps_per_second': 6.317, 'epoch': 0.92}
{'loss': 0.9035, 'grad_norm': 0.21208864450454712, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4454175233840942, 'eval_runtime': 9.9693, 'eval_samples_per_second': 100.308, 'eval_steps_per_second': 6.319, 'epoch': 0.96}
{'loss': 0.9285, 'grad_norm': 0.24004817008972168, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4456300735473633, 'eval_runtime': 9.9768, 'eval_samples_per_second': 100.232, 'eval_steps_per_second': 6.315, 'epoch': 1.0}
{'train_runtime': 388.2758, 'train_samples_per_second': 25.747, 'train_steps_per_second': 1.61, 'train_loss': 1.1237883697509765, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5392705202102661, 1.337377905845642, 1.3511630296707153, 1.3264708518981934, 1.3683571815490723, 1.3475679159164429, 1.3756935596466064, 1.392499566078186, 1.4172805547714233, 1.4127453565597534, 1.4009007215499878, 1.3994805812835693, 1.4018932580947876, 1.4044196605682373, 1.424109935760498, 1.4293564558029175, 1.443474292755127, 1.4275577068328857, 1.4412695169448853, 1.4390360116958618, 1.4362047910690308, 1.440920352935791, 1.4433385133743286, 1.4454175233840942, 1.4456300735473633], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5392705202102661, 1.337377905845642, 1.3511630296707153, 1.3264708518981934, 1.3683571815490723, 1.3475679159164429, 1.3756935596466064, 1.392499566078186, 1.4172805547714233, 1.4127453565597534, 1.4009007215499878, 1.3994805812835693, 1.4018932580947876, 1.4044196605682373, 1.424109935760498, 1.4293564558029175, 1.443474292755127, 1.4275577068328857, 1.4412695169448853, 1.4390360116958618, 1.4362047910690308, 1.440920352935791, 1.4433385133743286, 1.4454175233840942, 1.4456300735473633]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0799239873886108
current iteration best possible eval_loss (full train run):  -1.4456300735473633
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531, -1.3131517171859741, -1.0799239873886108]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6608 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -1.0643183732668111
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -1.0620144903100786
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -1.0613622958975306
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -1.0621223587964659
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -1.060171851365061
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1983, dtype=torch.float64), tensor(0.0634, dtype=torch.float64), 0, 0, 0, tensor(0.3130, dtype=torch.float64), 0, tensor(0.1826, dtype=torch.float64), tensor(0.2364, dtype=torch.float64), 23, 0, 0, 1, 1, 1, 109, 0.09397553258277558, 20.102841946012667, 1]
normalized proposed parameters for next round by BO: [tensor(0.1983, dtype=torch.float64), tensor(0.0634, dtype=torch.float64), tensor(3.2181e-17, dtype=torch.float64), tensor(9.4528e-18, dtype=torch.float64), tensor(2.9489e-17, dtype=torch.float64), tensor(0.3130, dtype=torch.float64), tensor(0.0063, dtype=torch.float64), tensor(0.1826, dtype=torch.float64), tensor(0.2364, dtype=torch.float64), tensor(0.7260, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8515, dtype=torch.float64), tensor(0.9398, dtype=torch.float64), tensor(0.4188, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.198
  gsm8k: 0.063
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.313
  wikitext: 0
  mmlu: 0.183
  arc_challenge: 0.236

LoRA Parameters:
  lora_r: (109,)
  lora_dropout: (0.09397553258277558,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (20.102841946012667,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  109
lora dropout:  0.09397553258277558
lora alpha:  20.102841946012667
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 138,627,072 || all params: 8,168,888,320 || trainable%: 1.6970
length of training data:  9935
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0847, 'grad_norm': 1.0620347261428833, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.486049771308899, 'eval_runtime': 10.9047, 'eval_samples_per_second': 91.704, 'eval_steps_per_second': 5.777, 'epoch': 0.04}
{'loss': 1.3185, 'grad_norm': 0.6121087670326233, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1514513492584229, 'eval_runtime': 10.9292, 'eval_samples_per_second': 91.498, 'eval_steps_per_second': 5.764, 'epoch': 0.08}
{'loss': 1.1903, 'grad_norm': 0.20029962062835693, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.0086308717727661, 'eval_runtime': 10.9557, 'eval_samples_per_second': 91.277, 'eval_steps_per_second': 5.75, 'epoch': 0.12}
{'loss': 1.0826, 'grad_norm': 0.21881967782974243, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.9806898236274719, 'eval_runtime': 10.9746, 'eval_samples_per_second': 91.119, 'eval_steps_per_second': 5.741, 'epoch': 0.16}
{'loss': 1.0191, 'grad_norm': 0.21138827502727509, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.9536832571029663, 'eval_runtime': 11.0114, 'eval_samples_per_second': 90.815, 'eval_steps_per_second': 5.721, 'epoch': 0.2}
{'loss': 0.956, 'grad_norm': 0.2594672441482544, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.9329924583435059, 'eval_runtime': 10.9938, 'eval_samples_per_second': 90.96, 'eval_steps_per_second': 5.73, 'epoch': 0.24}
{'loss': 0.9902, 'grad_norm': 0.1672123670578003, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.9215489625930786, 'eval_runtime': 10.9979, 'eval_samples_per_second': 90.926, 'eval_steps_per_second': 5.728, 'epoch': 0.28}
{'loss': 0.9039, 'grad_norm': 0.22763806581497192, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.9126194715499878, 'eval_runtime': 10.9956, 'eval_samples_per_second': 90.945, 'eval_steps_per_second': 5.73, 'epoch': 0.32}
{'loss': 0.9098, 'grad_norm': 0.19384780526161194, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.9142965078353882, 'eval_runtime': 11.0003, 'eval_samples_per_second': 90.906, 'eval_steps_per_second': 5.727, 'epoch': 0.36}
{'loss': 0.9332, 'grad_norm': 0.18285223841667175, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.9183269143104553, 'eval_runtime': 10.9933, 'eval_samples_per_second': 90.965, 'eval_steps_per_second': 5.731, 'epoch': 0.4}
{'loss': 0.9224, 'grad_norm': 0.2217399924993515, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.9137083888053894, 'eval_runtime': 10.9967, 'eval_samples_per_second': 90.936, 'eval_steps_per_second': 5.729, 'epoch': 0.44}
{'loss': 0.8892, 'grad_norm': 0.19022218883037567, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.90373694896698, 'eval_runtime': 11.0126, 'eval_samples_per_second': 90.805, 'eval_steps_per_second': 5.721, 'epoch': 0.48}
{'loss': 0.9128, 'grad_norm': 0.18248550593852997, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.9008939266204834, 'eval_runtime': 11.0089, 'eval_samples_per_second': 90.836, 'eval_steps_per_second': 5.723, 'epoch': 0.52}
{'loss': 0.8682, 'grad_norm': 0.20551645755767822, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.9018661379814148, 'eval_runtime': 11.0297, 'eval_samples_per_second': 90.664, 'eval_steps_per_second': 5.712, 'epoch': 0.56}
{'loss': 0.88, 'grad_norm': 0.16232821345329285, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.8957620859146118, 'eval_runtime': 11.0786, 'eval_samples_per_second': 90.264, 'eval_steps_per_second': 5.687, 'epoch': 0.6}
{'loss': 0.8676, 'grad_norm': 0.2343827486038208, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.8920148611068726, 'eval_runtime': 11.0709, 'eval_samples_per_second': 90.327, 'eval_steps_per_second': 5.691, 'epoch': 0.64}
{'loss': 0.8179, 'grad_norm': 0.2351633459329605, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.8920006155967712, 'eval_runtime': 11.0786, 'eval_samples_per_second': 90.264, 'eval_steps_per_second': 5.687, 'epoch': 0.68}
{'loss': 0.8393, 'grad_norm': 0.22419369220733643, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.8922910094261169, 'eval_runtime': 11.076, 'eval_samples_per_second': 90.285, 'eval_steps_per_second': 5.688, 'epoch': 0.72}
{'loss': 0.8327, 'grad_norm': 0.21310752630233765, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.8860676288604736, 'eval_runtime': 10.993, 'eval_samples_per_second': 90.967, 'eval_steps_per_second': 5.731, 'epoch': 0.76}
{'loss': 0.8326, 'grad_norm': 0.2504015862941742, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.8921141028404236, 'eval_runtime': 10.9921, 'eval_samples_per_second': 90.975, 'eval_steps_per_second': 5.731, 'epoch': 0.81}
{'loss': 0.8123, 'grad_norm': 0.2827228307723999, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.8888376355171204, 'eval_runtime': 11.001, 'eval_samples_per_second': 90.901, 'eval_steps_per_second': 5.727, 'epoch': 0.85}
{'loss': 0.8275, 'grad_norm': 0.2769073247909546, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.8862954378128052, 'eval_runtime': 10.9977, 'eval_samples_per_second': 90.928, 'eval_steps_per_second': 5.728, 'epoch': 0.89}
{'loss': 0.8601, 'grad_norm': 0.22316709160804749, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.8881436586380005, 'eval_runtime': 10.9986, 'eval_samples_per_second': 90.921, 'eval_steps_per_second': 5.728, 'epoch': 0.93}
{'loss': 0.769, 'grad_norm': 0.2713106572628021, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.8865056037902832, 'eval_runtime': 11.0036, 'eval_samples_per_second': 90.879, 'eval_steps_per_second': 5.725, 'epoch': 0.97}
{'train_runtime': 477.6579, 'train_samples_per_second': 20.799, 'train_steps_per_second': 1.3, 'train_loss': 1.0057354403386753, 'epoch': 1.0}
train_results:  {'eval_loss': [1.486049771308899, 1.1514513492584229, 1.0086308717727661, 0.9806898236274719, 0.9536832571029663, 0.9329924583435059, 0.9215489625930786, 0.9126194715499878, 0.9142965078353882, 0.9183269143104553, 0.9137083888053894, 0.90373694896698, 0.9008939266204834, 0.9018661379814148, 0.8957620859146118, 0.8920148611068726, 0.8920006155967712, 0.8922910094261169, 0.8860676288604736, 0.8921141028404236, 0.8888376355171204, 0.8862954378128052, 0.8881436586380005, 0.8865056037902832], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.486049771308899, 1.1514513492584229, 1.0086308717727661, 0.9806898236274719, 0.9536832571029663, 0.9329924583435059, 0.9215489625930786, 0.9126194715499878, 0.9142965078353882, 0.9183269143104553, 0.9137083888053894, 0.90373694896698, 0.9008939266204834, 0.9018661379814148, 0.8957620859146118, 0.8920148611068726, 0.8920006155967712, 0.8922910094261169, 0.8860676288604736, 0.8921141028404236, 0.8888376355171204, 0.8862954378128052, 0.8881436586380005, 0.8865056037902832]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0632292032241821
current iteration best possible eval_loss (full train run):  -0.8865056037902832
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531, -1.3131517171859741, -1.0799239873886108, -1.0632292032241821]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.3815 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3668936491012573, 0.891264021396637, 0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.8238186836242676, 0.9171960949897766, 0.8835806846618652, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.45111533999443054, 0.2139490246772766, 0.9017742872238159, 0.11642980575561523]  ‚Üí  acq = -1.0809217775330249
X = [0.10986042022705078, 0.20953714847564697, 0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9494266510009766, 0.8764203786849976, 0.9284661412239075, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6602510213851929, 0.18004006147384644, 0.9259414672851562, 0.9654239416122437]  ‚Üí  acq = -1.0857982961716164
X = [0.17275136709213257, 0.6849347949028015, 0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.31699109077453613, 0.9090394377708435, 0.2634289264678955, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.03518377244472504, 0.6907520294189453, 0.12004825472831726, 0.8972193002700806]  ‚Üí  acq = -1.0793972371521996
X = [0.6180925965309143, 0.16254359483718872, 0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4304628074169159, 0.8458959460258484, 0.5030843019485474, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.715866208076477, 0.002808213233947754, 0.6724920272827148, 0.5655561685562134]  ‚Üí  acq = -1.0800345743467532
X = [0.32551831007003784, 0.7355300188064575, 0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.8032635450363159, 0.7881297469139099, 0.10880237817764282, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.07442650943994522, 0.0027261972427368164, 0.3981722295284271, 0.7018656134605408]  ‚Üí  acq = -1.0824894703610664
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1573, dtype=torch.float64), 0, tensor(0.3605, dtype=torch.float64), tensor(0.0603, dtype=torch.float64), tensor(0.1468, dtype=torch.float64), 0, tensor(0.0609, dtype=torch.float64), tensor(0.2052, dtype=torch.float64), 25, 0, 0, 0, 1, 0, 126, 0.07774426138931445, 17.783503634781255, 1]
normalized proposed parameters for next round by BO: [tensor(6.4470e-18, dtype=torch.float64), tensor(0.1573, dtype=torch.float64), tensor(5.9671e-18, dtype=torch.float64), tensor(0.3605, dtype=torch.float64), tensor(0.0603, dtype=torch.float64), tensor(0.1468, dtype=torch.float64), tensor(0.0089, dtype=torch.float64), tensor(0.0609, dtype=torch.float64), tensor(0.2052, dtype=torch.float64), tensor(0.7831, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9852, dtype=torch.float64), tensor(0.7774, dtype=torch.float64), tensor(0.3705, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.157
  rowan_hellaswag: 0
  sciq: 0.361
  triviaqa: 0.06
  truthfulqa_gen: 0.147
  wikitext: 0
  mmlu: 0.061
  arc_challenge: 0.205

LoRA Parameters:
  lora_r: (126,)
  lora_dropout: (0.07774426138931445,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (17.783503634781255,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  126
lora dropout:  0.07774426138931445
lora alpha:  17.783503634781255
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 58,060,800 || all params: 8,088,322,048 || trainable%: 0.7178
length of training data:  9909
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.385, 'grad_norm': 1.3932676315307617, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5079526901245117, 'eval_runtime': 9.4732, 'eval_samples_per_second': 105.561, 'eval_steps_per_second': 6.65, 'epoch': 0.04}
{'loss': 1.4624, 'grad_norm': 0.5302587151527405, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1466803550720215, 'eval_runtime': 9.5081, 'eval_samples_per_second': 105.174, 'eval_steps_per_second': 6.626, 'epoch': 0.08}
{'loss': 1.0993, 'grad_norm': 0.2503257095813751, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.0042880773544312, 'eval_runtime': 9.5416, 'eval_samples_per_second': 104.804, 'eval_steps_per_second': 6.603, 'epoch': 0.12}
{'loss': 1.0365, 'grad_norm': 0.2347250133752823, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.9669345021247864, 'eval_runtime': 9.5676, 'eval_samples_per_second': 104.519, 'eval_steps_per_second': 6.585, 'epoch': 0.16}
{'loss': 0.9925, 'grad_norm': 0.2141183763742447, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.9412039518356323, 'eval_runtime': 9.583, 'eval_samples_per_second': 104.351, 'eval_steps_per_second': 6.574, 'epoch': 0.2}
{'loss': 0.9492, 'grad_norm': 0.261019766330719, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.9197595119476318, 'eval_runtime': 9.6183, 'eval_samples_per_second': 103.968, 'eval_steps_per_second': 6.55, 'epoch': 0.24}
{'loss': 0.9756, 'grad_norm': 0.18765661120414734, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.9120445847511292, 'eval_runtime': 9.602, 'eval_samples_per_second': 104.145, 'eval_steps_per_second': 6.561, 'epoch': 0.28}
{'loss': 0.9181, 'grad_norm': 0.24495109915733337, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.909138560295105, 'eval_runtime': 9.5985, 'eval_samples_per_second': 104.182, 'eval_steps_per_second': 6.563, 'epoch': 0.32}
{'loss': 0.8858, 'grad_norm': 0.18205894529819489, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.9019968509674072, 'eval_runtime': 9.601, 'eval_samples_per_second': 104.156, 'eval_steps_per_second': 6.562, 'epoch': 0.36}
{'loss': 0.9156, 'grad_norm': 0.15986418724060059, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.8968446254730225, 'eval_runtime': 9.6113, 'eval_samples_per_second': 104.044, 'eval_steps_per_second': 6.555, 'epoch': 0.4}
{'loss': 0.8808, 'grad_norm': 0.16927269101142883, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.8957257270812988, 'eval_runtime': 9.6052, 'eval_samples_per_second': 104.11, 'eval_steps_per_second': 6.559, 'epoch': 0.44}
{'loss': 0.8982, 'grad_norm': 0.1761130392551422, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.8877060413360596, 'eval_runtime': 9.615, 'eval_samples_per_second': 104.004, 'eval_steps_per_second': 6.552, 'epoch': 0.48}
{'loss': 0.8653, 'grad_norm': 0.15587225556373596, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.885678231716156, 'eval_runtime': 9.5959, 'eval_samples_per_second': 104.211, 'eval_steps_per_second': 6.565, 'epoch': 0.52}
{'loss': 0.8615, 'grad_norm': 0.16816934943199158, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.8839617967605591, 'eval_runtime': 9.583, 'eval_samples_per_second': 104.351, 'eval_steps_per_second': 6.574, 'epoch': 0.56}
{'loss': 0.8926, 'grad_norm': 0.2005908340215683, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.8888386487960815, 'eval_runtime': 9.5972, 'eval_samples_per_second': 104.198, 'eval_steps_per_second': 6.564, 'epoch': 0.6}
{'loss': 0.8466, 'grad_norm': 0.15976770222187042, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.8842127919197083, 'eval_runtime': 9.5973, 'eval_samples_per_second': 104.196, 'eval_steps_per_second': 6.564, 'epoch': 0.65}
{'loss': 0.8472, 'grad_norm': 0.16806793212890625, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.8777135014533997, 'eval_runtime': 9.6002, 'eval_samples_per_second': 104.164, 'eval_steps_per_second': 6.562, 'epoch': 0.69}
{'loss': 0.8481, 'grad_norm': 0.16349157691001892, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.8771602511405945, 'eval_runtime': 9.5837, 'eval_samples_per_second': 104.344, 'eval_steps_per_second': 6.574, 'epoch': 0.73}
{'loss': 0.9085, 'grad_norm': 0.14834453165531158, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.8804131150245667, 'eval_runtime': 9.5893, 'eval_samples_per_second': 104.283, 'eval_steps_per_second': 6.57, 'epoch': 0.77}
{'loss': 0.8658, 'grad_norm': 0.15642589330673218, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.8751967549324036, 'eval_runtime': 9.58, 'eval_samples_per_second': 104.384, 'eval_steps_per_second': 6.576, 'epoch': 0.81}
{'loss': 0.891, 'grad_norm': 0.14427630603313446, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.8736891746520996, 'eval_runtime': 9.6049, 'eval_samples_per_second': 104.114, 'eval_steps_per_second': 6.559, 'epoch': 0.85}
{'loss': 0.8742, 'grad_norm': 0.17154890298843384, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.8757424354553223, 'eval_runtime': 9.6438, 'eval_samples_per_second': 103.694, 'eval_steps_per_second': 6.533, 'epoch': 0.89}
{'loss': 0.912, 'grad_norm': 0.1641332060098648, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.8717319369316101, 'eval_runtime': 9.6783, 'eval_samples_per_second': 103.324, 'eval_steps_per_second': 6.509, 'epoch': 0.93}
{'loss': 0.8837, 'grad_norm': 0.14873014390468597, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.8718048334121704, 'eval_runtime': 9.6659, 'eval_samples_per_second': 103.456, 'eval_steps_per_second': 6.518, 'epoch': 0.97}
{'train_runtime': 416.9743, 'train_samples_per_second': 23.764, 'train_steps_per_second': 1.487, 'train_loss': 1.0317051887512207, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5079526901245117, 1.1466803550720215, 1.0042880773544312, 0.9669345021247864, 0.9412039518356323, 0.9197595119476318, 0.9120445847511292, 0.909138560295105, 0.9019968509674072, 0.8968446254730225, 0.8957257270812988, 0.8877060413360596, 0.885678231716156, 0.8839617967605591, 0.8888386487960815, 0.8842127919197083, 0.8777135014533997, 0.8771602511405945, 0.8804131150245667, 0.8751967549324036, 0.8736891746520996, 0.8757424354553223, 0.8717319369316101, 0.8718048334121704], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.5079526901245117, 1.1466803550720215, 1.0042880773544312, 0.9669345021247864, 0.9412039518356323, 0.9197595119476318, 0.9120445847511292, 0.909138560295105, 0.9019968509674072, 0.8968446254730225, 0.8957257270812988, 0.8877060413360596, 0.885678231716156, 0.8839617967605591, 0.8888386487960815, 0.8842127919197083, 0.8777135014533997, 0.8771602511405945, 0.8804131150245667, 0.8751967549324036, 0.8736891746520996, 0.8757424354553223, 0.8717319369316101, 0.8718048334121704]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0440541505813599
current iteration best possible eval_loss (full train run):  -0.8718048334121704
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531, -1.3131517171859741, -1.0799239873886108, -1.0632292032241821, -1.0440541505813599]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.8512 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.08295202255249023, 0.8953140377998352, 0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.36218807101249695, 0.6549691557884216, 0.8173236846923828, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3347404897212982, 0.355441689491272, 0.4367692470550537, 0.45111382007598877]  ‚Üí  acq = -1.0798300486851486
X = [0.35591208934783936, 0.098782479763031, 0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.36877870559692383, 0.8672244548797607, 0.5090312361717224, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9806126356124878, 0.5262150168418884, 0.617688775062561, 0.313107967376709]  ‚Üí  acq = -1.0798792726699278
X = [0.2322162389755249, 0.03715479373931885, 0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.40351834893226624, 0.9447525143623352, 0.40425533056259155, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9491793513298035, 0.21524584293365479, 0.22934073209762573, 0.15074169635772705]  ‚Üí  acq = -1.087220788549998
X = [0.5550482869148254, 0.7731989026069641, 0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6225687265396118, 0.3806919455528259, 0.9421022534370422, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7150893807411194, 0.5437468886375427, 0.6237008571624756, 0.9093855619430542]  ‚Üí  acq = -1.0800901171703683
X = [0.7380771636962891, 0.9890440702438354, 0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.4263235926628113, 0.9630946516990662, 0.6111319065093994, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.834472119808197, 0.3034810423851013, 0.033137477934360504, 0.012760698795318604]  ‚Üí  acq = -1.0798811346494632
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5508, dtype=torch.float64), 0, 0, 0, 0, tensor(0.0278, dtype=torch.float64), tensor(0.1728, dtype=torch.float64), tensor(0.2397, dtype=torch.float64), 4, 0, 0, 0, 1, 1, 128, 0.0049925143898692514, 19.288948548274956, 1]
normalized proposed parameters for next round by BO: [tensor(1.0190e-17, dtype=torch.float64), tensor(0.5508, dtype=torch.float64), tensor(0.0090, dtype=torch.float64), tensor(1.8828e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0278, dtype=torch.float64), tensor(0.1728, dtype=torch.float64), tensor(0.2397, dtype=torch.float64), tensor(0.1160, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0499, dtype=torch.float64), tensor(0.4019, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.551
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.028
  mmlu: 0.173
  arc_challenge: 0.24

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0049925143898692514,)
  num_layers_to_apply: (4,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (19.288948548274956,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  4
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  128
lora dropout:  0.0049925143898692514
lora alpha:  19.288948548274956
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 18,874,368 || all params: 8,049,135,616 || trainable%: 0.2345
length of training data:  9908
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7342, 'grad_norm': 1.3349800109863281, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7769229412078857, 'eval_runtime': 8.9649, 'eval_samples_per_second': 111.546, 'eval_steps_per_second': 7.027, 'epoch': 0.04}
{'loss': 1.7273, 'grad_norm': 0.6932967305183411, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2057381868362427, 'eval_runtime': 9.0206, 'eval_samples_per_second': 110.857, 'eval_steps_per_second': 6.984, 'epoch': 0.08}
{'loss': 1.2851, 'grad_norm': 0.36111608147621155, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.0341743230819702, 'eval_runtime': 9.1169, 'eval_samples_per_second': 109.686, 'eval_steps_per_second': 6.91, 'epoch': 0.12}
{'loss': 1.1184, 'grad_norm': 0.3085003197193146, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.9901487827301025, 'eval_runtime': 9.121, 'eval_samples_per_second': 109.638, 'eval_steps_per_second': 6.907, 'epoch': 0.16}
{'loss': 1.0941, 'grad_norm': 0.22043274343013763, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.9690935611724854, 'eval_runtime': 9.1367, 'eval_samples_per_second': 109.449, 'eval_steps_per_second': 6.895, 'epoch': 0.2}
{'loss': 1.0558, 'grad_norm': 0.216644287109375, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.9520989060401917, 'eval_runtime': 9.1353, 'eval_samples_per_second': 109.466, 'eval_steps_per_second': 6.896, 'epoch': 0.24}
{'loss': 1.0285, 'grad_norm': 0.20635400712490082, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.9418961405754089, 'eval_runtime': 9.1314, 'eval_samples_per_second': 109.512, 'eval_steps_per_second': 6.899, 'epoch': 0.28}
{'loss': 1.0443, 'grad_norm': 0.2333880215883255, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.9288260340690613, 'eval_runtime': 9.1351, 'eval_samples_per_second': 109.467, 'eval_steps_per_second': 6.896, 'epoch': 0.32}
{'loss': 1.0356, 'grad_norm': 0.24305066466331482, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.921375572681427, 'eval_runtime': 9.1228, 'eval_samples_per_second': 109.616, 'eval_steps_per_second': 6.906, 'epoch': 0.36}
{'loss': 1.0201, 'grad_norm': 0.22703930735588074, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.9073053002357483, 'eval_runtime': 9.1282, 'eval_samples_per_second': 109.551, 'eval_steps_per_second': 6.902, 'epoch': 0.4}
{'loss': 1.0054, 'grad_norm': 0.20944218337535858, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.9023240804672241, 'eval_runtime': 9.1037, 'eval_samples_per_second': 109.845, 'eval_steps_per_second': 6.92, 'epoch': 0.44}
{'loss': 0.98, 'grad_norm': 0.2082010805606842, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.8985732793807983, 'eval_runtime': 9.1283, 'eval_samples_per_second': 109.55, 'eval_steps_per_second': 6.902, 'epoch': 0.48}
{'loss': 0.982, 'grad_norm': 0.17611786723136902, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.8950060606002808, 'eval_runtime': 9.1195, 'eval_samples_per_second': 109.656, 'eval_steps_per_second': 6.908, 'epoch': 0.52}
{'loss': 0.9888, 'grad_norm': 0.18110835552215576, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.8934187889099121, 'eval_runtime': 9.1188, 'eval_samples_per_second': 109.664, 'eval_steps_per_second': 6.909, 'epoch': 0.56}
{'loss': 0.9725, 'grad_norm': 0.19701889157295227, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.8902803063392639, 'eval_runtime': 9.1071, 'eval_samples_per_second': 109.804, 'eval_steps_per_second': 6.918, 'epoch': 0.6}
{'loss': 1.0094, 'grad_norm': 0.2287028282880783, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.8926838636398315, 'eval_runtime': 9.0866, 'eval_samples_per_second': 110.052, 'eval_steps_per_second': 6.933, 'epoch': 0.65}
{'loss': 0.9485, 'grad_norm': 0.17722845077514648, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.8878180980682373, 'eval_runtime': 9.0877, 'eval_samples_per_second': 110.039, 'eval_steps_per_second': 6.932, 'epoch': 0.69}
{'loss': 0.9703, 'grad_norm': 0.2940901219844818, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.8878066539764404, 'eval_runtime': 9.1155, 'eval_samples_per_second': 109.703, 'eval_steps_per_second': 6.911, 'epoch': 0.73}
{'loss': 0.9766, 'grad_norm': 0.20952054858207703, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.8847300410270691, 'eval_runtime': 9.1322, 'eval_samples_per_second': 109.502, 'eval_steps_per_second': 6.899, 'epoch': 0.77}
{'loss': 0.9681, 'grad_norm': 0.23057855665683746, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.8839929103851318, 'eval_runtime': 9.1268, 'eval_samples_per_second': 109.567, 'eval_steps_per_second': 6.903, 'epoch': 0.81}
{'loss': 0.9312, 'grad_norm': 0.24529008567333221, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.8832072615623474, 'eval_runtime': 9.1118, 'eval_samples_per_second': 109.747, 'eval_steps_per_second': 6.914, 'epoch': 0.85}
{'loss': 0.956, 'grad_norm': 0.1756758987903595, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.8798474073410034, 'eval_runtime': 9.1345, 'eval_samples_per_second': 109.475, 'eval_steps_per_second': 6.897, 'epoch': 0.89}
{'loss': 0.9584, 'grad_norm': 0.22687163949012756, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.8808701038360596, 'eval_runtime': 9.1273, 'eval_samples_per_second': 109.561, 'eval_steps_per_second': 6.902, 'epoch': 0.93}
{'loss': 0.9473, 'grad_norm': 0.24179977178573608, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.8797035813331604, 'eval_runtime': 9.1371, 'eval_samples_per_second': 109.444, 'eval_steps_per_second': 6.895, 'epoch': 0.97}
{'train_runtime': 412.4793, 'train_samples_per_second': 24.021, 'train_steps_per_second': 1.503, 'train_loss': 1.109169969251079, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7769229412078857, 1.2057381868362427, 1.0341743230819702, 0.9901487827301025, 0.9690935611724854, 0.9520989060401917, 0.9418961405754089, 0.9288260340690613, 0.921375572681427, 0.9073053002357483, 0.9023240804672241, 0.8985732793807983, 0.8950060606002808, 0.8934187889099121, 0.8902803063392639, 0.8926838636398315, 0.8878180980682373, 0.8878066539764404, 0.8847300410270691, 0.8839929103851318, 0.8832072615623474, 0.8798474073410034, 0.8808701038360596, 0.8797035813331604], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.7769229412078857, 1.2057381868362427, 1.0341743230819702, 0.9901487827301025, 0.9690935611724854, 0.9520989060401917, 0.9418961405754089, 0.9288260340690613, 0.921375572681427, 0.9073053002357483, 0.9023240804672241, 0.8985732793807983, 0.8950060606002808, 0.8934187889099121, 0.8902803063392639, 0.8926838636398315, 0.8878180980682373, 0.8878066539764404, 0.8847300410270691, 0.8839929103851318, 0.8832072615623474, 0.8798474073410034, 0.8808701038360596, 0.8797035813331604]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.403893232345581
current iteration best possible eval_loss (full train run):  -0.8797035813331604
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531, -1.3131517171859741, -1.0799239873886108, -1.0632292032241821, -1.0440541505813599, -1.403893232345581]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6395 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.038794755935668945, 0.5530740022659302, 0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.32418566942214966, 0.03446310758590698, 0.8077076077461243, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5978026986122131, 0.24681228399276733, 0.3197188079357147, 0.43591630458831787]  ‚Üí  acq = -1.064753669716867
X = [0.06298112869262695, 0.4111078381538391, 0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.2800779640674591, 0.1305062174797058, 0.7458388209342957, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2502773702144623, 0.26675117015838623, 0.721631646156311, 0.1087694764137268]  ‚Üí  acq = -1.0819934514448715
X = [0.7721713781356812, 0.36252284049987793, 0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8363065719604492, 0.2568463683128357, 0.6493399143218994, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7879849672317505, 0.044145405292510986, 0.17305481433868408, 0.9866487383842468]  ‚Üí  acq = -0.9762654700281392
X = [0.5755511522293091, 0.49270403385162354, 0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2550579309463501, 0.3171401023864746, 0.8667199611663818, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6549527049064636, 0.34602898359298706, 0.037224020808935165, 0.7405623197555542]  ‚Üí  acq = -1.0008823429227307
X = [0.7848239541053772, 0.7650286555290222, 0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.2828661799430847, 0.7089584469795227, 0.5923287272453308, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.9000691771507263, 0.3606852889060974, 0.7490195035934448, 0.591159462928772]  ‚Üí  acq = -1.0445652898590976
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0750, dtype=torch.float64), tensor(0.1128, dtype=torch.float64), tensor(0.0201, dtype=torch.float64), tensor(0.2632, dtype=torch.float64), tensor(0.2718, dtype=torch.float64), tensor(0.0481, dtype=torch.float64), tensor(0.0839, dtype=torch.float64), tensor(0.1251, dtype=torch.float64), 4, 0, 1, 0, 1, 1, 33, 0.02406505743903775, 20.70721083275133, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0750, dtype=torch.float64), tensor(0.1128, dtype=torch.float64), tensor(0.0201, dtype=torch.float64), tensor(0.2632, dtype=torch.float64), tensor(0.2718, dtype=torch.float64), tensor(0.0481, dtype=torch.float64), tensor(0.0839, dtype=torch.float64), tensor(0.1251, dtype=torch.float64), tensor(0.1162, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2549, dtype=torch.float64), tensor(0.2407, dtype=torch.float64), tensor(0.4314, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.075
  rowan_hellaswag: 0.113
  sciq: 0.02
  triviaqa: 0.263
  truthfulqa_gen: 0.272
  wikitext: 0.048
  mmlu: 0.084
  arc_challenge: 0.125

LoRA Parameters:
  lora_r: (33,)
  lora_dropout: (0.02406505743903775,)
  num_layers_to_apply: (4,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (20.70721083275133,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  4
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  33
lora dropout:  0.02406505743903775
lora alpha:  20.70721083275133
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 5,541,888 || all params: 8,035,803,136 || trainable%: 0.0690
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.8775, 'grad_norm': 0.8524147272109985, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7342661619186401, 'eval_runtime': 9.1218, 'eval_samples_per_second': 109.627, 'eval_steps_per_second': 6.907, 'epoch': 0.04}
{'loss': 2.2154, 'grad_norm': 0.6296128630638123, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2478818893432617, 'eval_runtime': 9.1834, 'eval_samples_per_second': 108.892, 'eval_steps_per_second': 6.86, 'epoch': 0.08}
{'loss': 1.5204, 'grad_norm': 0.3730587661266327, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1526930332183838, 'eval_runtime': 9.1579, 'eval_samples_per_second': 109.195, 'eval_steps_per_second': 6.879, 'epoch': 0.12}
{'loss': 1.4282, 'grad_norm': 0.3524608016014099, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1140141487121582, 'eval_runtime': 9.2195, 'eval_samples_per_second': 108.466, 'eval_steps_per_second': 6.833, 'epoch': 0.16}
{'loss': 1.4173, 'grad_norm': 0.3429792821407318, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.055078148841858, 'eval_runtime': 9.23, 'eval_samples_per_second': 108.342, 'eval_steps_per_second': 6.826, 'epoch': 0.2}
{'loss': 1.3979, 'grad_norm': 0.3011874258518219, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0630682706832886, 'eval_runtime': 9.29, 'eval_samples_per_second': 107.643, 'eval_steps_per_second': 6.782, 'epoch': 0.24}
{'loss': 1.3837, 'grad_norm': 0.3127470910549164, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0340708494186401, 'eval_runtime': 9.2352, 'eval_samples_per_second': 108.282, 'eval_steps_per_second': 6.822, 'epoch': 0.28}
{'loss': 1.3861, 'grad_norm': 0.41573455929756165, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0207536220550537, 'eval_runtime': 9.2421, 'eval_samples_per_second': 108.201, 'eval_steps_per_second': 6.817, 'epoch': 0.32}
{'loss': 1.3455, 'grad_norm': 0.4057139754295349, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.021047592163086, 'eval_runtime': 9.2521, 'eval_samples_per_second': 108.084, 'eval_steps_per_second': 6.809, 'epoch': 0.36}
{'loss': 1.3368, 'grad_norm': 0.3184153437614441, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0089356899261475, 'eval_runtime': 9.2562, 'eval_samples_per_second': 108.036, 'eval_steps_per_second': 6.806, 'epoch': 0.4}
{'loss': 1.3169, 'grad_norm': 0.3906865417957306, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0072963237762451, 'eval_runtime': 9.2722, 'eval_samples_per_second': 107.849, 'eval_steps_per_second': 6.795, 'epoch': 0.44}
{'loss': 1.3305, 'grad_norm': 0.3793984651565552, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0050538778305054, 'eval_runtime': 9.2681, 'eval_samples_per_second': 107.897, 'eval_steps_per_second': 6.798, 'epoch': 0.48}
{'loss': 1.367, 'grad_norm': 0.34873461723327637, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0027071237564087, 'eval_runtime': 9.2959, 'eval_samples_per_second': 107.574, 'eval_steps_per_second': 6.777, 'epoch': 0.52}
{'loss': 1.267, 'grad_norm': 0.38506755232810974, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9959616661071777, 'eval_runtime': 9.2584, 'eval_samples_per_second': 108.01, 'eval_steps_per_second': 6.805, 'epoch': 0.56}
{'loss': 1.3318, 'grad_norm': 0.41494911909103394, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9877697229385376, 'eval_runtime': 9.2779, 'eval_samples_per_second': 107.783, 'eval_steps_per_second': 6.79, 'epoch': 0.6}
{'loss': 1.3155, 'grad_norm': 0.4208196997642517, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9891747832298279, 'eval_runtime': 9.2479, 'eval_samples_per_second': 108.133, 'eval_steps_per_second': 6.812, 'epoch': 0.64}
{'loss': 1.3672, 'grad_norm': 0.31460514664649963, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9903160929679871, 'eval_runtime': 9.2711, 'eval_samples_per_second': 107.863, 'eval_steps_per_second': 6.795, 'epoch': 0.68}
{'loss': 1.4003, 'grad_norm': 0.3498280942440033, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9877504706382751, 'eval_runtime': 9.2416, 'eval_samples_per_second': 108.207, 'eval_steps_per_second': 6.817, 'epoch': 0.72}
{'loss': 1.2895, 'grad_norm': 0.33418262004852295, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9869028925895691, 'eval_runtime': 9.2664, 'eval_samples_per_second': 107.917, 'eval_steps_per_second': 6.799, 'epoch': 0.76}
{'loss': 1.275, 'grad_norm': 0.3323211371898651, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9822987914085388, 'eval_runtime': 9.2514, 'eval_samples_per_second': 108.091, 'eval_steps_per_second': 6.81, 'epoch': 0.8}
{'loss': 1.3161, 'grad_norm': 0.38394448161125183, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9827914834022522, 'eval_runtime': 9.2376, 'eval_samples_per_second': 108.254, 'eval_steps_per_second': 6.82, 'epoch': 0.84}
{'loss': 1.2981, 'grad_norm': 0.39280521869659424, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9802876114845276, 'eval_runtime': 9.2414, 'eval_samples_per_second': 108.209, 'eval_steps_per_second': 6.817, 'epoch': 0.88}
{'loss': 1.3085, 'grad_norm': 0.42625537514686584, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9809656739234924, 'eval_runtime': 9.2489, 'eval_samples_per_second': 108.121, 'eval_steps_per_second': 6.812, 'epoch': 0.92}
{'loss': 1.3042, 'grad_norm': 0.34532666206359863, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9800060987472534, 'eval_runtime': 9.2517, 'eval_samples_per_second': 108.088, 'eval_steps_per_second': 6.81, 'epoch': 0.96}
{'loss': 1.3346, 'grad_norm': 0.3881193697452545, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9793766736984253, 'eval_runtime': 9.2549, 'eval_samples_per_second': 108.051, 'eval_steps_per_second': 6.807, 'epoch': 1.0}
{'train_runtime': 340.1407, 'train_samples_per_second': 29.388, 'train_steps_per_second': 1.837, 'train_loss': 1.485246792602539, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7342661619186401, 1.2478818893432617, 1.1526930332183838, 1.1140141487121582, 1.055078148841858, 1.0630682706832886, 1.0340708494186401, 1.0207536220550537, 1.021047592163086, 1.0089356899261475, 1.0072963237762451, 1.0050538778305054, 1.0027071237564087, 0.9959616661071777, 0.9877697229385376, 0.9891747832298279, 0.9903160929679871, 0.9877504706382751, 0.9869028925895691, 0.9822987914085388, 0.9827914834022522, 0.9802876114845276, 0.9809656739234924, 0.9800060987472534, 0.9793766736984253], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7342661619186401, 1.2478818893432617, 1.1526930332183838, 1.1140141487121582, 1.055078148841858, 1.0630682706832886, 1.0340708494186401, 1.0207536220550537, 1.021047592163086, 1.0089356899261475, 1.0072963237762451, 1.0050538778305054, 1.0027071237564087, 0.9959616661071777, 0.9877697229385376, 0.9891747832298279, 0.9903160929679871, 0.9877504706382751, 0.9869028925895691, 0.9822987914085388, 0.9827914834022522, 0.9802876114845276, 0.9809656739234924, 0.9800060987472534, 0.9793766736984253]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.7438075542449951
current iteration best possible eval_loss (full train run):  -0.9793766736984253
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531, -1.3131517171859741, -1.0799239873886108, -1.0632292032241821, -1.0440541505813599, -1.403893232345581, -1.7438075542449951]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2001 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -1.0280969940942866
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -1.0537664799651445
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -0.9837619394229903
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -1.0690697453152258
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -1.0125632689564652
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0223, dtype=torch.float64), 0, tensor(0.1108, dtype=torch.float64), 0, tensor(0.4885, dtype=torch.float64), 0, tensor(0.0666, dtype=torch.float64), tensor(0.3119, dtype=torch.float64), 29, 0, 1, 0, 1, 1, 128, 0.009797977853828674, 14.746967341106059, 1]
normalized proposed parameters for next round by BO: [tensor(3.3296e-17, dtype=torch.float64), tensor(0.0223, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1108, dtype=torch.float64), tensor(7.2568e-17, dtype=torch.float64), tensor(0.4885, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0666, dtype=torch.float64), tensor(0.3119, dtype=torch.float64), tensor(0.9155, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0980, dtype=torch.float64), tensor(0.3072, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.022
  rowan_hellaswag: 0
  sciq: 0.111
  triviaqa: 0
  truthfulqa_gen: 0.489
  wikitext: 0
  mmlu: 0.067
  arc_challenge: 0.312

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.009797977853828674,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (14.746967341106059,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.009797977853828674
lora alpha:  14.746967341106059
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 155,844,608 || all params: 8,186,105,856 || trainable%: 1.9038
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4424, 'grad_norm': 0.9214819669723511, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5322265625, 'eval_runtime': 10.2203, 'eval_samples_per_second': 97.844, 'eval_steps_per_second': 6.164, 'epoch': 0.04}
{'loss': 1.2835, 'grad_norm': 0.4820875823497772, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2435861825942993, 'eval_runtime': 10.2383, 'eval_samples_per_second': 97.673, 'eval_steps_per_second': 6.153, 'epoch': 0.08}
{'loss': 1.0318, 'grad_norm': 0.28081047534942627, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1051456928253174, 'eval_runtime': 10.2713, 'eval_samples_per_second': 97.359, 'eval_steps_per_second': 6.134, 'epoch': 0.12}
{'loss': 0.974, 'grad_norm': 0.2894437611103058, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0276682376861572, 'eval_runtime': 10.2844, 'eval_samples_per_second': 97.235, 'eval_steps_per_second': 6.126, 'epoch': 0.16}
{'loss': 0.8981, 'grad_norm': 0.25507524609565735, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0069211721420288, 'eval_runtime': 10.3, 'eval_samples_per_second': 97.087, 'eval_steps_per_second': 6.116, 'epoch': 0.2}
{'loss': 0.8307, 'grad_norm': 0.17939943075180054, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9736962914466858, 'eval_runtime': 10.3011, 'eval_samples_per_second': 97.077, 'eval_steps_per_second': 6.116, 'epoch': 0.24}
{'loss': 0.784, 'grad_norm': 0.20346826314926147, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9720281958580017, 'eval_runtime': 10.3095, 'eval_samples_per_second': 96.998, 'eval_steps_per_second': 6.111, 'epoch': 0.28}
{'loss': 0.8122, 'grad_norm': 0.2337731271982193, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9649970531463623, 'eval_runtime': 10.3007, 'eval_samples_per_second': 97.081, 'eval_steps_per_second': 6.116, 'epoch': 0.32}
{'loss': 0.7649, 'grad_norm': 0.21730034053325653, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9658294916152954, 'eval_runtime': 10.2895, 'eval_samples_per_second': 97.186, 'eval_steps_per_second': 6.123, 'epoch': 0.36}
{'loss': 0.7568, 'grad_norm': 0.2297065705060959, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9522307515144348, 'eval_runtime': 10.3063, 'eval_samples_per_second': 97.028, 'eval_steps_per_second': 6.113, 'epoch': 0.4}
{'loss': 0.74, 'grad_norm': 0.22519706189632416, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9610139727592468, 'eval_runtime': 10.329, 'eval_samples_per_second': 96.814, 'eval_steps_per_second': 6.099, 'epoch': 0.44}
{'loss': 0.7293, 'grad_norm': 0.26837635040283203, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9564448595046997, 'eval_runtime': 10.352, 'eval_samples_per_second': 96.6, 'eval_steps_per_second': 6.086, 'epoch': 0.48}
{'loss': 0.6866, 'grad_norm': 0.2556677758693695, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9542875289916992, 'eval_runtime': 10.4101, 'eval_samples_per_second': 96.061, 'eval_steps_per_second': 6.052, 'epoch': 0.52}
{'loss': 0.6913, 'grad_norm': 0.2503143846988678, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9466979503631592, 'eval_runtime': 10.4196, 'eval_samples_per_second': 95.973, 'eval_steps_per_second': 6.046, 'epoch': 0.56}
{'loss': 0.6274, 'grad_norm': 0.23554456233978271, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9482617974281311, 'eval_runtime': 10.4009, 'eval_samples_per_second': 96.146, 'eval_steps_per_second': 6.057, 'epoch': 0.6}
{'loss': 0.6716, 'grad_norm': 0.2551717162132263, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9422164559364319, 'eval_runtime': 10.4007, 'eval_samples_per_second': 96.147, 'eval_steps_per_second': 6.057, 'epoch': 0.64}
{'loss': 0.6675, 'grad_norm': 0.25279125571250916, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9438750147819519, 'eval_runtime': 10.396, 'eval_samples_per_second': 96.191, 'eval_steps_per_second': 6.06, 'epoch': 0.68}
{'loss': 0.6517, 'grad_norm': 0.3067331910133362, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9377706050872803, 'eval_runtime': 10.3821, 'eval_samples_per_second': 96.32, 'eval_steps_per_second': 6.068, 'epoch': 0.72}
{'loss': 0.579, 'grad_norm': 0.23100857436656952, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9420734643936157, 'eval_runtime': 10.3474, 'eval_samples_per_second': 96.642, 'eval_steps_per_second': 6.088, 'epoch': 0.76}
{'loss': 0.6605, 'grad_norm': 0.25000742077827454, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9342961311340332, 'eval_runtime': 10.3463, 'eval_samples_per_second': 96.653, 'eval_steps_per_second': 6.089, 'epoch': 0.8}
{'loss': 0.607, 'grad_norm': 0.22897464036941528, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9338983297348022, 'eval_runtime': 10.3284, 'eval_samples_per_second': 96.82, 'eval_steps_per_second': 6.1, 'epoch': 0.84}
{'loss': 0.6087, 'grad_norm': 0.31656113266944885, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9365843534469604, 'eval_runtime': 10.3299, 'eval_samples_per_second': 96.806, 'eval_steps_per_second': 6.099, 'epoch': 0.88}
{'loss': 0.5829, 'grad_norm': 0.31532061100006104, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9303317666053772, 'eval_runtime': 10.334, 'eval_samples_per_second': 96.768, 'eval_steps_per_second': 6.096, 'epoch': 0.92}
{'loss': 0.5752, 'grad_norm': 0.293426513671875, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.932349681854248, 'eval_runtime': 10.3268, 'eval_samples_per_second': 96.836, 'eval_steps_per_second': 6.101, 'epoch': 0.96}
{'loss': 0.5436, 'grad_norm': 0.2607053518295288, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9315770268440247, 'eval_runtime': 10.3094, 'eval_samples_per_second': 96.999, 'eval_steps_per_second': 6.111, 'epoch': 1.0}
{'train_runtime': 435.1008, 'train_samples_per_second': 22.976, 'train_steps_per_second': 1.436, 'train_loss': 0.8480275115966797, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5322265625, 1.2435861825942993, 1.1051456928253174, 1.0276682376861572, 1.0069211721420288, 0.9736962914466858, 0.9720281958580017, 0.9649970531463623, 0.9658294916152954, 0.9522307515144348, 0.9610139727592468, 0.9564448595046997, 0.9542875289916992, 0.9466979503631592, 0.9482617974281311, 0.9422164559364319, 0.9438750147819519, 0.9377706050872803, 0.9420734643936157, 0.9342961311340332, 0.9338983297348022, 0.9365843534469604, 0.9303317666053772, 0.932349681854248, 0.9315770268440247], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5322265625, 1.2435861825942993, 1.1051456928253174, 1.0276682376861572, 1.0069211721420288, 0.9736962914466858, 0.9720281958580017, 0.9649970531463623, 0.9658294916152954, 0.9522307515144348, 0.9610139727592468, 0.9564448595046997, 0.9542875289916992, 0.9466979503631592, 0.9482617974281311, 0.9422164559364319, 0.9438750147819519, 0.9377706050872803, 0.9420734643936157, 0.9342961311340332, 0.9338983297348022, 0.9365843534469604, 0.9303317666053772, 0.932349681854248, 0.9315770268440247]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.9315770268440247
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531, -1.3131517171859741, -1.0799239873886108, -1.0632292032241821, -1.0440541505813599, -1.403893232345581, -1.7438075542449951, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.9564 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.1188230776261967
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.085150211798706
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.1528682566634978
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.078634507120575
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.1015485365022453
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2093, dtype=torch.float64), 0, 0, tensor(0.3890, dtype=torch.float64), tensor(0.2952, dtype=torch.float64), tensor(0.1062, dtype=torch.float64), 0, 0, 30, 0, 1, 0, 1, 1, 71, 0.0725221422800709, 21.653727843843154, 0]
normalized proposed parameters for next round by BO: [tensor(1.1816e-17, dtype=torch.float64), tensor(0.2093, dtype=torch.float64), tensor(1.2047e-17, dtype=torch.float64), tensor(8.2327e-18, dtype=torch.float64), tensor(0.3890, dtype=torch.float64), tensor(0.2952, dtype=torch.float64), tensor(0.1062, dtype=torch.float64), tensor(7.5685e-18, dtype=torch.float64), tensor(0.0004, dtype=torch.float64), tensor(0.9510, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5521, dtype=torch.float64), tensor(0.7252, dtype=torch.float64), tensor(0.4511, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.209
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.389
  truthfulqa_gen: 0.295
  wikitext: 0.106
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (71,)
  lora_dropout: (0.0725221422800709,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (21.653727843843154,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  71
lora dropout:  0.0725221422800709
lora alpha:  21.653727843843154
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 89,425,920 || all params: 8,119,687,168 || trainable%: 1.1013
length of training data:  9994
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8919, 'grad_norm': 1.1557948589324951, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.347500205039978, 'eval_runtime': 10.8345, 'eval_samples_per_second': 92.298, 'eval_steps_per_second': 5.815, 'epoch': 0.04}
{'loss': 1.2444, 'grad_norm': 0.2526554763317108, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9434488415718079, 'eval_runtime': 10.8729, 'eval_samples_per_second': 91.972, 'eval_steps_per_second': 5.794, 'epoch': 0.08}
{'loss': 1.049, 'grad_norm': 0.25135329365730286, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9116536974906921, 'eval_runtime': 10.9216, 'eval_samples_per_second': 91.562, 'eval_steps_per_second': 5.768, 'epoch': 0.12}
{'loss': 1.0866, 'grad_norm': 0.2528539001941681, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9005963206291199, 'eval_runtime': 10.9151, 'eval_samples_per_second': 91.616, 'eval_steps_per_second': 5.772, 'epoch': 0.16}
{'loss': 1.0035, 'grad_norm': 0.2920669913291931, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8897982239723206, 'eval_runtime': 10.9224, 'eval_samples_per_second': 91.555, 'eval_steps_per_second': 5.768, 'epoch': 0.2}
{'loss': 0.9981, 'grad_norm': 0.2749389410018921, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8856626152992249, 'eval_runtime': 10.9303, 'eval_samples_per_second': 91.489, 'eval_steps_per_second': 5.764, 'epoch': 0.24}
{'loss': 1.0033, 'grad_norm': 0.23534630239009857, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8786453008651733, 'eval_runtime': 10.9419, 'eval_samples_per_second': 91.392, 'eval_steps_per_second': 5.758, 'epoch': 0.28}
{'loss': 0.9632, 'grad_norm': 0.19690652191638947, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8749964833259583, 'eval_runtime': 10.9378, 'eval_samples_per_second': 91.426, 'eval_steps_per_second': 5.76, 'epoch': 0.32}
{'loss': 1.0118, 'grad_norm': 0.2834639549255371, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.877710223197937, 'eval_runtime': 10.9353, 'eval_samples_per_second': 91.447, 'eval_steps_per_second': 5.761, 'epoch': 0.36}
{'loss': 1.0031, 'grad_norm': 0.2691657543182373, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8756160736083984, 'eval_runtime': 10.9226, 'eval_samples_per_second': 91.553, 'eval_steps_per_second': 5.768, 'epoch': 0.4}
{'loss': 0.935, 'grad_norm': 0.20264318585395813, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8646154999732971, 'eval_runtime': 10.9323, 'eval_samples_per_second': 91.472, 'eval_steps_per_second': 5.763, 'epoch': 0.44}
{'loss': 0.9323, 'grad_norm': 0.24973133206367493, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8651618361473083, 'eval_runtime': 10.9299, 'eval_samples_per_second': 91.492, 'eval_steps_per_second': 5.764, 'epoch': 0.48}
{'loss': 0.8876, 'grad_norm': 0.23900410532951355, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8627311587333679, 'eval_runtime': 10.9514, 'eval_samples_per_second': 91.312, 'eval_steps_per_second': 5.753, 'epoch': 0.52}
{'loss': 0.9054, 'grad_norm': 0.244588702917099, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8579291105270386, 'eval_runtime': 10.9596, 'eval_samples_per_second': 91.244, 'eval_steps_per_second': 5.748, 'epoch': 0.56}
{'loss': 0.9119, 'grad_norm': 0.24446646869182587, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8584490418434143, 'eval_runtime': 10.9693, 'eval_samples_per_second': 91.163, 'eval_steps_per_second': 5.743, 'epoch': 0.6}
{'loss': 0.8944, 'grad_norm': 0.24419257044792175, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8553745150566101, 'eval_runtime': 10.9806, 'eval_samples_per_second': 91.07, 'eval_steps_per_second': 5.737, 'epoch': 0.64}
{'loss': 0.8991, 'grad_norm': 0.2773834466934204, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8562159538269043, 'eval_runtime': 10.955, 'eval_samples_per_second': 91.283, 'eval_steps_per_second': 5.751, 'epoch': 0.68}
{'loss': 0.8699, 'grad_norm': 0.2958041727542877, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8521679043769836, 'eval_runtime': 10.9912, 'eval_samples_per_second': 90.982, 'eval_steps_per_second': 5.732, 'epoch': 0.72}
{'loss': 0.9032, 'grad_norm': 0.25175929069519043, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8544373512268066, 'eval_runtime': 10.9691, 'eval_samples_per_second': 91.165, 'eval_steps_per_second': 5.743, 'epoch': 0.76}
{'loss': 0.8178, 'grad_norm': 0.3850176930427551, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8509724140167236, 'eval_runtime': 10.9208, 'eval_samples_per_second': 91.568, 'eval_steps_per_second': 5.769, 'epoch': 0.8}
{'loss': 0.9779, 'grad_norm': 0.2838967740535736, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8512320518493652, 'eval_runtime': 10.9332, 'eval_samples_per_second': 91.465, 'eval_steps_per_second': 5.762, 'epoch': 0.84}
{'loss': 0.8523, 'grad_norm': 0.3016432821750641, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8479870557785034, 'eval_runtime': 10.9199, 'eval_samples_per_second': 91.576, 'eval_steps_per_second': 5.769, 'epoch': 0.88}
{'loss': 0.7894, 'grad_norm': 0.20872806012630463, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8480429649353027, 'eval_runtime': 10.916, 'eval_samples_per_second': 91.609, 'eval_steps_per_second': 5.771, 'epoch': 0.92}
{'loss': 0.8431, 'grad_norm': 0.3178240656852722, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8471239805221558, 'eval_runtime': 15.6537, 'eval_samples_per_second': 63.883, 'eval_steps_per_second': 4.025, 'epoch': 0.96}
{'loss': 0.9002, 'grad_norm': 0.5085222721099854, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8474256992340088, 'eval_runtime': 16.8582, 'eval_samples_per_second': 59.318, 'eval_steps_per_second': 3.737, 'epoch': 1.0}
{'train_runtime': 510.9155, 'train_samples_per_second': 19.561, 'train_steps_per_second': 1.223, 'train_loss': 1.0229744079589844, 'epoch': 1.0}
train_results:  {'eval_loss': [1.347500205039978, 0.9434488415718079, 0.9116536974906921, 0.9005963206291199, 0.8897982239723206, 0.8856626152992249, 0.8786453008651733, 0.8749964833259583, 0.877710223197937, 0.8756160736083984, 0.8646154999732971, 0.8651618361473083, 0.8627311587333679, 0.8579291105270386, 0.8584490418434143, 0.8553745150566101, 0.8562159538269043, 0.8521679043769836, 0.8544373512268066, 0.8509724140167236, 0.8512320518493652, 0.8479870557785034, 0.8480429649353027, 0.8471239805221558, 0.8474256992340088], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.347500205039978, 0.9434488415718079, 0.9116536974906921, 0.9005963206291199, 0.8897982239723206, 0.8856626152992249, 0.8786453008651733, 0.8749964833259583, 0.877710223197937, 0.8756160736083984, 0.8646154999732971, 0.8651618361473083, 0.8627311587333679, 0.8579291105270386, 0.8584490418434143, 0.8553745150566101, 0.8562159538269043, 0.8521679043769836, 0.8544373512268066, 0.8509724140167236, 0.8512320518493652, 0.8479870557785034, 0.8480429649353027, 0.8471239805221558, 0.8474256992340088]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0254647731781006
current iteration best possible eval_loss (full train run):  -0.8474256992340088
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531, -1.3131517171859741, -1.0799239873886108, -1.0632292032241821, -1.0440541505813599, -1.403893232345581, -1.7438075542449951, -1.020788550376892, -1.0254647731781006]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2163 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -1.0381347476597125
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -1.069851904780994
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -1.0455345581976523
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -1.042539771510329
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -1.0582906094552333
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2317, dtype=torch.float64), 0, 0, tensor(0.1395, dtype=torch.float64), tensor(0.3637, dtype=torch.float64), tensor(0.0578, dtype=torch.float64), tensor(0.2072, dtype=torch.float64), 0, 32, 0, 0, 0, 1, 1, 128, 0.06582760650435633, 27.20374325889146, 0]
normalized proposed parameters for next round by BO: [tensor(1.2974e-17, dtype=torch.float64), tensor(0.2317, dtype=torch.float64), tensor(1.1621e-18, dtype=torch.float64), tensor(1.1088e-17, dtype=torch.float64), tensor(0.1395, dtype=torch.float64), tensor(0.3637, dtype=torch.float64), tensor(0.0578, dtype=torch.float64), tensor(0.2072, dtype=torch.float64), tensor(4.6971e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6583, dtype=torch.float64), tensor(0.5667, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.232
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.139
  truthfulqa_gen: 0.364
  wikitext: 0.058
  mmlu: 0.207
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.06582760650435633,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (27.20374325889146,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  128
lora dropout:  0.06582760650435633
lora alpha:  27.20374325889146
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8049, 'grad_norm': 0.5410122275352478, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2905192375183105, 'eval_runtime': 10.1091, 'eval_samples_per_second': 98.921, 'eval_steps_per_second': 6.232, 'epoch': 0.04}
{'loss': 1.3061, 'grad_norm': 0.6067646145820618, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.959574818611145, 'eval_runtime': 10.1466, 'eval_samples_per_second': 98.555, 'eval_steps_per_second': 6.209, 'epoch': 0.08}
{'loss': 1.1106, 'grad_norm': 0.2373429536819458, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9103580117225647, 'eval_runtime': 10.1851, 'eval_samples_per_second': 98.182, 'eval_steps_per_second': 6.185, 'epoch': 0.12}
{'loss': 1.0741, 'grad_norm': 0.16973841190338135, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8956018686294556, 'eval_runtime': 10.26, 'eval_samples_per_second': 97.466, 'eval_steps_per_second': 6.14, 'epoch': 0.16}
{'loss': 1.0715, 'grad_norm': 0.1655741184949875, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8864472508430481, 'eval_runtime': 10.2506, 'eval_samples_per_second': 97.555, 'eval_steps_per_second': 6.146, 'epoch': 0.2}
{'loss': 1.0387, 'grad_norm': 0.16207829117774963, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8912210464477539, 'eval_runtime': 10.2562, 'eval_samples_per_second': 97.502, 'eval_steps_per_second': 6.143, 'epoch': 0.24}
{'loss': 0.9957, 'grad_norm': 0.19311831891536713, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8769904971122742, 'eval_runtime': 10.2609, 'eval_samples_per_second': 97.457, 'eval_steps_per_second': 6.14, 'epoch': 0.28}
{'loss': 0.965, 'grad_norm': 0.18668241798877716, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8711431622505188, 'eval_runtime': 10.2242, 'eval_samples_per_second': 97.807, 'eval_steps_per_second': 6.162, 'epoch': 0.32}
{'loss': 0.9469, 'grad_norm': 0.17326350510120392, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8694692254066467, 'eval_runtime': 10.2312, 'eval_samples_per_second': 97.74, 'eval_steps_per_second': 6.158, 'epoch': 0.36}
{'loss': 0.9394, 'grad_norm': 0.18514800071716309, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8642447590827942, 'eval_runtime': 10.2394, 'eval_samples_per_second': 97.662, 'eval_steps_per_second': 6.153, 'epoch': 0.4}
{'loss': 0.956, 'grad_norm': 0.19501085579395294, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8649131059646606, 'eval_runtime': 10.2356, 'eval_samples_per_second': 97.698, 'eval_steps_per_second': 6.155, 'epoch': 0.44}
{'loss': 0.9454, 'grad_norm': 0.2069297730922699, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8589740991592407, 'eval_runtime': 10.2525, 'eval_samples_per_second': 97.538, 'eval_steps_per_second': 6.145, 'epoch': 0.48}
{'loss': 0.9033, 'grad_norm': 0.1845024675130844, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8575342893600464, 'eval_runtime': 10.2781, 'eval_samples_per_second': 97.295, 'eval_steps_per_second': 6.13, 'epoch': 0.52}
{'loss': 0.9523, 'grad_norm': 0.21590255200862885, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8569371700286865, 'eval_runtime': 10.2582, 'eval_samples_per_second': 97.483, 'eval_steps_per_second': 6.141, 'epoch': 0.56}
{'loss': 0.9309, 'grad_norm': 0.17901746928691864, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8574559092521667, 'eval_runtime': 10.2338, 'eval_samples_per_second': 97.716, 'eval_steps_per_second': 6.156, 'epoch': 0.6}
{'loss': 0.9111, 'grad_norm': 0.23231595754623413, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.854271650314331, 'eval_runtime': 10.2317, 'eval_samples_per_second': 97.735, 'eval_steps_per_second': 6.157, 'epoch': 0.64}
{'loss': 0.902, 'grad_norm': 0.19943425059318542, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8500250577926636, 'eval_runtime': 10.2368, 'eval_samples_per_second': 97.687, 'eval_steps_per_second': 6.154, 'epoch': 0.68}
{'loss': 0.8839, 'grad_norm': 0.2540704309940338, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8502082228660583, 'eval_runtime': 10.2219, 'eval_samples_per_second': 97.829, 'eval_steps_per_second': 6.163, 'epoch': 0.72}
{'loss': 0.8352, 'grad_norm': 0.16615749895572662, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8494797348976135, 'eval_runtime': 10.2346, 'eval_samples_per_second': 97.708, 'eval_steps_per_second': 6.156, 'epoch': 0.76}
{'loss': 0.9032, 'grad_norm': 0.1756204515695572, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8468726873397827, 'eval_runtime': 10.2247, 'eval_samples_per_second': 97.803, 'eval_steps_per_second': 6.162, 'epoch': 0.8}
{'loss': 0.885, 'grad_norm': 0.16582737863063812, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8458834886550903, 'eval_runtime': 10.2471, 'eval_samples_per_second': 97.589, 'eval_steps_per_second': 6.148, 'epoch': 0.84}
{'loss': 0.9184, 'grad_norm': 0.17128077149391174, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8444904685020447, 'eval_runtime': 10.3064, 'eval_samples_per_second': 97.027, 'eval_steps_per_second': 6.113, 'epoch': 0.88}
{'loss': 0.8559, 'grad_norm': 0.17257927358150482, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8438286781311035, 'eval_runtime': 10.3011, 'eval_samples_per_second': 97.077, 'eval_steps_per_second': 6.116, 'epoch': 0.92}
{'loss': 0.8697, 'grad_norm': 0.14868316054344177, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.84345543384552, 'eval_runtime': 10.2481, 'eval_samples_per_second': 97.579, 'eval_steps_per_second': 6.147, 'epoch': 0.96}
{'loss': 0.9034, 'grad_norm': 0.39729875326156616, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8431440591812134, 'eval_runtime': 10.2658, 'eval_samples_per_second': 97.41, 'eval_steps_per_second': 6.137, 'epoch': 1.0}
{'train_runtime': 483.9134, 'train_samples_per_second': 20.661, 'train_steps_per_second': 1.292, 'train_loss': 1.0323325927734375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2905192375183105, 0.959574818611145, 0.9103580117225647, 0.8956018686294556, 0.8864472508430481, 0.8912210464477539, 0.8769904971122742, 0.8711431622505188, 0.8694692254066467, 0.8642447590827942, 0.8649131059646606, 0.8589740991592407, 0.8575342893600464, 0.8569371700286865, 0.8574559092521667, 0.854271650314331, 0.8500250577926636, 0.8502082228660583, 0.8494797348976135, 0.8468726873397827, 0.8458834886550903, 0.8444904685020447, 0.8438286781311035, 0.84345543384552, 0.8431440591812134], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2905192375183105, 0.959574818611145, 0.9103580117225647, 0.8956018686294556, 0.8864472508430481, 0.8912210464477539, 0.8769904971122742, 0.8711431622505188, 0.8694692254066467, 0.8642447590827942, 0.8649131059646606, 0.8589740991592407, 0.8575342893600464, 0.8569371700286865, 0.8574559092521667, 0.854271650314331, 0.8500250577926636, 0.8502082228660583, 0.8494797348976135, 0.8468726873397827, 0.8458834886550903, 0.8444904685020447, 0.8438286781311035, 0.84345543384552, 0.8431440591812134]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0350538492202759
current iteration best possible eval_loss (full train run):  -0.8431440591812134
max eval_loss so far:  -0.8152114152908325
BO observations:  [-2.3928966522216797, -1.1803230047225952, -1.343926191329956, -1.9602372646331787, -1.0404719114303589, -1.4703216552734375, -2.1625874042510986, -1.2998933792114258, -1.8948516845703125, -1.3337985277175903, -1.031911849975586, -2.0560648441314697, -1.5177440643310547, -1.0774685144424438, -1.020788550376892, -2.3333349227905273, -1.066023826599121, -1.9471611976623535, -1.1642560958862305, -1.2092981338500977, -1.6423835754394531, -1.3131517171859741, -1.0799239873886108, -1.0632292032241821, -1.0440541505813599, -1.403893232345581, -1.7438075542449951, -1.020788550376892, -1.0254647731781006, -1.0350538492202759]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3495 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -1.0743263196791508
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -1.0749227475940342
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -0.9929435007718814
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -1.0163612080010842
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -1.0441690064026075
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2317, dtype=torch.float64), 0, tensor(0.0374, dtype=torch.float64), tensor(0.0893, dtype=torch.float64), tensor(0.1234, dtype=torch.float64), tensor(0.1315, dtype=torch.float64), tensor(0.0246, dtype=torch.float64), tensor(0.0705, dtype=torch.float64), tensor(0.2831, dtype=torch.float64), 21, 0, 0, 0, 1, 0, 35, 0.1, 4.39766110368862, 1]
normalized proposed parameters for next round by BO: [tensor(0.2317, dtype=torch.float64), tensor(0.0084, dtype=torch.float64), tensor(0.0374, dtype=torch.float64), tensor(0.0893, dtype=torch.float64), tensor(0.1234, dtype=torch.float64), tensor(0.1315, dtype=torch.float64), tensor(0.0246, dtype=torch.float64), tensor(0.0705, dtype=torch.float64), tensor(0.2831, dtype=torch.float64), tensor(0.6566, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2700, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0916, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-1.5389292240142822, -0.8277751207351685, -0.8277751207351685, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
gsm8k
evaluation dataset:
data domain:  gsm8k  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/gsm8k/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 9, 1, 1, 0, 0, 1, 14, 0.03669660801939172, 9, 1]
Checking history sample input_X_between_0_1:  [0.07408584376407944, 0.15027941106200063, 0.0689380667115247, 0.0447459525126138, 0.22295299440305, 0.3438846960883639, 0.05553581449476961, 0.038835078283166284, 0.0007421426804315853, 0.28125, 1.0, 1.0, 0.0, 0.0, 1.0, 0.109375, 0.3669660801939172, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1832736730575562
Checking history sample input_X:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 4, 1, 1, 0, 1, 0, 45, 0.02996529708553877, 41, 0]
Checking history sample input_X_between_0_1:  [0.038480798095986285, 0.08977751677591088, 0.08840604749540168, 0.12084794854621976, 0.05416929367491721, 0.24167274063376856, 0.0019838006979300193, 0.27288636690288, 0.09177548717698555, 0.125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3515625, 0.29965297085538767, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.2217429876327515
Checking history sample input_X:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 5, 0, 1, 1, 0, 0, 115, 0.08044623754842878, 26, 0]
Checking history sample input_X_between_0_1:  [0.10879211072915738, 0.38636659896525977, 0.06159288187975191, 0.0011679038497443167, 0.21383906233399314, 0.032885701238468255, 0.06401457171358228, 0.07489536690583765, 0.05644580238420544, 0.15625, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8984375, 0.8044623754842878, 0.5416666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1418228149414062
Checking history sample input_X:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 30, 0, 1, 0, 1, 1, 121, 0.019265932894845208, 29, 1]
Checking history sample input_X_between_0_1:  [0.05777171985305044, 0.33211728235723353, 0.08331240736019478, 0.005247487835703332, 0.012074042738735122, 0.2905211502048659, 0.012364593810816001, 0.11779222782018725, 0.0887990880192138, 0.9375, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9453125, 0.19265932894845206, 0.6041666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8670127987861633
Checking history sample input_X:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 19, 0, 0, 1, 1, 1, 108, 0.09306302255978231, 35, 1]
Checking history sample input_X_between_0_1:  [0.046982436280863585, 0.12379368773421767, 0.11139775224232805, 0.018010313721598555, 0.022957631547658678, 0.026547529820895602, 0.44494779336383106, 0.05629605131219265, 0.14906680397641403, 0.59375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.84375, 0.930630225597823, 0.7291666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.3226383924484253
Checking history sample input_X:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 14, 0, 0, 0, 1, 1, 44, 0.004820496247456452, 22, 1]
Checking history sample input_X_between_0_1:  [0.4079746621410325, 0.028312830859974717, 0.002946608792392125, 0.04398938866760232, 0.07394500546740626, 0.012385458648252032, 0.07039221687585992, 0.16630338217630794, 0.19375044637117209, 0.4375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.34375, 0.04820496247456452, 0.4583333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9786784052848816
Checking history sample input_X:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 24, 0, 1, 1, 0, 1, 79, 0.03164316476579598, 9, 1]
Checking history sample input_X_between_0_1:  [0.29761969838554997, 0.049797633866638866, 0.14331317969394894, 0.056082767460932825, 0.06708151773579908, 0.09695019593087313, 0.0024236573723774696, 0.21510756165018888, 0.07162378790369094, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6171875, 0.3164316476579597, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.201198697090149
Checking history sample input_X:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 29, 0, 1, 0, 1, 1, 23, 0.015858984905097274, 40, 0]
Checking history sample input_X_between_0_1:  [0.03245317365963995, 0.08815642336441788, 0.12313983046506799, 0.004430230371801564, 0.348042383787267, 0.2171209599176883, 0.1126651867580396, 0.05794810305953566, 0.016043708616542217, 0.90625, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1796875, 0.15858984905097273, 0.8333333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.079849362373352
Checking history sample input_X:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 27, 0, 0, 1, 0, 0, 58, 0.02356583389888708, 4, 0]
Checking history sample input_X_between_0_1:  [0.06059174033885753, 0.18395655956134466, 0.09163256862977745, 0.10713746518203753, 0.004337947417733217, 0.0952349973335051, 0.39823818583230597, 0.03747679824625911, 0.021393737458179456, 0.84375, 0.0, 0.0, 1.0, 0.0, 0.0, 0.453125, 0.23565833898887079, 0.08333333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3622353076934814
Checking history sample input_X:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 18, 0, 1, 1, 0, 1, 102, 0.07944916714916157, 39, 0]
Checking history sample input_X_between_0_1:  [0.1039925747646396, 0.15068080812774604, 0.16663203390412612, 0.14294104101497263, 0.16761691155620892, 0.12106299457254986, 0.10141566528804294, 0.035586631535411334, 0.010071339236302665, 0.5625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.796875, 0.7944916714916157, 0.8125, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1885993480682373
Checking history sample input_X:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 16, 1, 0, 0, 0, 0, 57, 0.051177108415818844, 16, 1]
Checking history sample input_X_between_0_1:  [0.0007978288851898076, 0.06990312132117849, 0.15127524188162678, 0.059334648699515345, 0.23803917367302657, 0.09878741312934516, 0.12508875387462318, 0.18797240373587748, 0.06880141479961709, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4453125, 0.5117710841581884, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.7646844387054443
Checking history sample input_X:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 21, 0, 1, 0, 0, 1, 80, 0.08438899074092726, 11, 1]
Checking history sample input_X_between_0_1:  [0.024711348065954108, 0.06370765158589989, 0.04110065573202076, 0.13895876658710615, 0.19765658955003454, 0.08278256586108182, 0.08556260464115961, 0.22905589664507908, 0.13646392133166418, 0.65625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.625, 0.8438899074092726, 0.22916666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -1.194746494293213
Checking history sample input_X:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 19, 0, 1, 1, 1, 0, 106, 0.021750066051679486, 16, 1]
Checking history sample input_X_between_0_1:  [0.0357682734714734, 0.12962463440140576, 0.2722410565946169, 0.13654535820624622, 0.023757962166410955, 0.08220500213486735, 0.13371737000900008, 0.15539133737038288, 0.03074900564559654, 0.59375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.828125, 0.21750066051679484, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.357306957244873
Checking history sample input_X:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 6, 1, 0, 0, 0, 1, 109, 0.0976089152670931, 17, 0]
Checking history sample input_X_between_0_1:  [0.05028869947461046, 0.37788942401378656, 0.009633208152875114, 0.08090225243417618, 0.026414524226565102, 0.11329432928502454, 0.08216223548378845, 0.03736937483330006, 0.22204595209587355, 0.1875, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8515625, 0.976089152670931, 0.3541666666666667, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0162200927734375
Checking history sample input_X:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 5, 0, 1, 1, 0, 1, 6, 0.045424701431822194, 31, 1]
Checking history sample input_X_between_0_1:  [0.02572225878557758, 0.021301865326785904, 0.027488008371185296, 0.04187474840191461, 0.04602201160005438, 0.092631782107687, 0.3955043570610349, 0.043186876892430476, 0.30626809145332984, 0.15625, 0.0, 1.0, 1.0, 0.0, 1.0, 0.046875, 0.4542470143182219, 0.6458333333333334, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2814624309539795
Checking history sample input_X:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 18, 1, 0, 0, 0, 0, 27, 0.050875722794158945, 27, 1]
Checking history sample input_X_between_0_1:  [0.038611426129531876, 0.02229110035066881, 0.14083242977692165, 0.17909092860230444, 0.1965955499272737, 0.09121210016441658, 0.0845845077007334, 0.034997681747954, 0.21178427560019542, 0.5625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2109375, 0.5087572279415894, 0.5625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.6545288562774658
Checking history sample input_X:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 25, 0, 0, 0, 1, 1, 5, 0.0932969995849881, 3, 0]
Checking history sample input_X_between_0_1:  [0.02280127982660309, 0.049872947916042805, 0.02629473834420144, 0.02360717254411583, 0.16875091483325855, 0.1472203574274002, 0.421143070505935, 0.09269335642899032, 0.04761616217345274, 0.78125, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0390625, 0.932969995849881, 0.0625, 0.0]
Checking history sample eval_loss at 625 steps:  -1.4124306440353394
Checking history sample input_X:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 14, 0, 0, 0, 1, 0, 14, 0.06234381073507801, 24, 1]
Checking history sample input_X_between_0_1:  [0.05830471708910319, 0.1744376030276079, 0.05023211978888397, 0.3331717447599354, 0.03651552903900966, 0.0059785748329532415, 0.05424560407284155, 0.048018444775530196, 0.23909566261413484, 0.4375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.109375, 0.6234381073507801, 0.5, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9628919959068298
Checking history sample input_X:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 19, 1, 1, 0, 1, 0, 54, 0.0659080396027719, 16, 1]
Checking history sample input_X_between_0_1:  [0.08564141502188617, 0.023306756210795282, 0.2910812809592018, 0.019965525063565536, 0.28486998014382237, 0.055800897720622945, 0.1626531740402163, 0.01948501759096579, 0.05719595324892397, 0.59375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.421875, 0.659080396027719, 0.3333333333333333, 1.0]
Checking history sample eval_loss at 625 steps:  -1.4756159782409668
Checking history sample input_X:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 30, 1, 1, 0, 1, 0, 113, 0.08743251828499332, 21, 0]
Checking history sample input_X_between_0_1:  [0.07900195267013801, 0.1285493484151591, 0.02366967822809756, 0.009916250841851722, 0.09585780051668592, 0.2978186618217684, 0.10253171984720943, 0.11270791301748574, 0.1499466746416041, 0.9375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8828125, 0.8743251828499332, 0.4375, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9556006193161011
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.7424 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5415134429931641, 0.635921835899353, 0.17285668849945068, 0.7329596281051636, 0.525409996509552, 0.3549891710281372, 0.9316291809082031, 0.025579988956451416, 0.20366358757019043, 0.796625018119812, 0.9944829344749451, 0.03618258237838745, 0.0982237458229065, 0.7049162983894348, 0.19342893362045288, 0.7825830578804016, 0.9113210439682007, 0.7374570369720459, 0.38655322790145874]  ‚Üí  acq = -0.6087232908535004
X = [0.9658668041229248, 0.1375868320465088, 0.9390822052955627, 0.6210303902626038, 0.4899604320526123, 0.7141364216804504, 0.8376624584197998, 0.6347618699073792, 0.09093230962753296, 0.3734595775604248, 0.9806047677993774, 0.5510079264640808, 0.7273470163345337, 0.7472496032714844, 0.8345333933830261, 0.3114771544933319, 0.32012927532196045, 0.900371789932251, 0.02603095769882202]  ‚Üí  acq = -0.609293832468507
X = [0.1504339575767517, 0.24254333972930908, 0.8739622235298157, 0.8536970019340515, 0.7247304320335388, 0.19661849737167358, 0.565993070602417, 0.36742067337036133, 0.9658461809158325, 0.3245685398578644, 0.26492440700531006, 0.8127020597457886, 0.7616298198699951, 0.4069160223007202, 0.28610050678253174, 0.033198870718479156, 0.648709237575531, 0.45562446117401123, 0.8855143785476685]  ‚Üí  acq = -0.6092948701494795
X = [0.9490978717803955, 0.5894943475723267, 0.08964037895202637, 0.2615838050842285, 0.27792543172836304, 0.021983087062835693, 0.4216662049293518, 0.7895143628120422, 0.46796733140945435, 0.050548046827316284, 0.3928382992744446, 0.7967085242271423, 0.4988960027694702, 0.9386184811592102, 0.019295811653137207, 0.23020200431346893, 0.2741682529449463, 0.4562031030654907, 0.29729413986206055]  ‚Üí  acq = -0.6086394689381593
X = [0.35703402757644653, 0.25585347414016724, 0.7500284910202026, 0.39152616262435913, 0.015694797039031982, 0.5321981906890869, 0.21621084213256836, 0.7421044111251831, 0.5429164171218872, 0.756330132484436, 0.5231150388717651, 0.3123241066932678, 0.034733712673187256, 0.5963308811187744, 0.49610430002212524, 0.14257949590682983, 0.5382137894630432, 0.441458135843277, 0.3583519458770752]  ‚Üí  acq = -0.6092872842368747
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.1791, dtype=torch.float64), 0, tensor(0.0223, dtype=torch.float64), tensor(0.1849, dtype=torch.float64), 0, tensor(0.3278, dtype=torch.float64), 0, 0, tensor(0.2859, dtype=torch.float64), 32, 0, 0, 1, 1, 0, 2, 0.0, 48.0, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.1791, dtype=torch.float64), tensor(3.1518e-17, dtype=torch.float64), tensor(0.0223, dtype=torch.float64), tensor(0.1849, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3278, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9105e-18, dtype=torch.float64), tensor(0.2859, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.179
  gsm8k: 0
  rowan_hellaswag: 0.022
  sciq: 0.185
  triviaqa: 0
  truthfulqa_gen: 0.328
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.286

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,359,296 || all params: 8,032,620,544 || trainable%: 0.0294
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 2.8277, 'grad_norm': 4.363999843597412, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4401233196258545, 'eval_runtime': 10.1296, 'eval_samples_per_second': 98.72, 'eval_steps_per_second': 6.219, 'epoch': 0.04}
{'loss': 1.013, 'grad_norm': 19.314411163330078, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3649412393569946, 'eval_runtime': 10.1057, 'eval_samples_per_second': 98.954, 'eval_steps_per_second': 6.234, 'epoch': 0.08}
{'loss': 0.9384, 'grad_norm': 1.7075631618499756, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3332387208938599, 'eval_runtime': 10.1438, 'eval_samples_per_second': 98.582, 'eval_steps_per_second': 6.211, 'epoch': 0.12}
{'loss': 0.9309, 'grad_norm': 1.6177951097488403, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.361615777015686, 'eval_runtime': 10.1665, 'eval_samples_per_second': 98.362, 'eval_steps_per_second': 6.197, 'epoch': 0.16}
{'loss': 0.8632, 'grad_norm': 1.9382424354553223, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3888142108917236, 'eval_runtime': 10.205, 'eval_samples_per_second': 97.991, 'eval_steps_per_second': 6.173, 'epoch': 0.2}
{'loss': 0.8247, 'grad_norm': 1.8919049501419067, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.419145941734314, 'eval_runtime': 10.2378, 'eval_samples_per_second': 97.677, 'eval_steps_per_second': 6.154, 'epoch': 0.24}
{'loss': 0.8482, 'grad_norm': 1.6995036602020264, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.412695050239563, 'eval_runtime': 10.2771, 'eval_samples_per_second': 97.304, 'eval_steps_per_second': 6.13, 'epoch': 0.28}
{'loss': 0.7974, 'grad_norm': 1.8676408529281616, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4278401136398315, 'eval_runtime': 10.2733, 'eval_samples_per_second': 97.34, 'eval_steps_per_second': 6.132, 'epoch': 0.32}
{'loss': 0.7722, 'grad_norm': 2.5626792907714844, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4388940334320068, 'eval_runtime': 10.2684, 'eval_samples_per_second': 97.386, 'eval_steps_per_second': 6.135, 'epoch': 0.36}
{'loss': 0.7403, 'grad_norm': 1.781036376953125, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.44257652759552, 'eval_runtime': 10.2651, 'eval_samples_per_second': 97.417, 'eval_steps_per_second': 6.137, 'epoch': 0.4}
{'loss': 0.712, 'grad_norm': 1.8178348541259766, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.426006555557251, 'eval_runtime': 10.2857, 'eval_samples_per_second': 97.222, 'eval_steps_per_second': 6.125, 'epoch': 0.44}
{'loss': 0.7985, 'grad_norm': 2.185690402984619, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.437944769859314, 'eval_runtime': 10.2859, 'eval_samples_per_second': 97.221, 'eval_steps_per_second': 6.125, 'epoch': 0.48}
{'loss': 0.7361, 'grad_norm': 2.007136344909668, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.457876205444336, 'eval_runtime': 10.2823, 'eval_samples_per_second': 97.254, 'eval_steps_per_second': 6.127, 'epoch': 0.52}
{'loss': 0.7368, 'grad_norm': 2.088273763656616, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4666446447372437, 'eval_runtime': 10.2832, 'eval_samples_per_second': 97.246, 'eval_steps_per_second': 6.126, 'epoch': 0.56}
{'loss': 0.7038, 'grad_norm': 3.1025373935699463, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4563374519348145, 'eval_runtime': 10.278, 'eval_samples_per_second': 97.295, 'eval_steps_per_second': 6.13, 'epoch': 0.6}
{'loss': 0.6508, 'grad_norm': 2.3145079612731934, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.453000783920288, 'eval_runtime': 10.2975, 'eval_samples_per_second': 97.111, 'eval_steps_per_second': 6.118, 'epoch': 0.64}
{'loss': 0.6261, 'grad_norm': 2.089585065841675, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4736390113830566, 'eval_runtime': 10.3405, 'eval_samples_per_second': 96.707, 'eval_steps_per_second': 6.093, 'epoch': 0.68}
{'loss': 0.6309, 'grad_norm': 2.061131715774536, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.480785608291626, 'eval_runtime': 10.3209, 'eval_samples_per_second': 96.89, 'eval_steps_per_second': 6.104, 'epoch': 0.72}
{'loss': 0.6277, 'grad_norm': 2.3844332695007324, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4740322828292847, 'eval_runtime': 10.3084, 'eval_samples_per_second': 97.008, 'eval_steps_per_second': 6.112, 'epoch': 0.76}
{'loss': 0.6216, 'grad_norm': 2.429903984069824, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.489406943321228, 'eval_runtime': 10.3, 'eval_samples_per_second': 97.087, 'eval_steps_per_second': 6.117, 'epoch': 0.8}
{'loss': 0.6494, 'grad_norm': 1.7767990827560425, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.483664870262146, 'eval_runtime': 10.3097, 'eval_samples_per_second': 96.996, 'eval_steps_per_second': 6.111, 'epoch': 0.84}
{'loss': 0.6414, 'grad_norm': 2.820446729660034, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5008001327514648, 'eval_runtime': 10.3087, 'eval_samples_per_second': 97.005, 'eval_steps_per_second': 6.111, 'epoch': 0.88}
{'loss': 0.6582, 'grad_norm': 2.0440196990966797, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5011540651321411, 'eval_runtime': 10.3438, 'eval_samples_per_second': 96.676, 'eval_steps_per_second': 6.091, 'epoch': 0.92}
{'loss': 0.5856, 'grad_norm': 2.038666248321533, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.508453607559204, 'eval_runtime': 10.3176, 'eval_samples_per_second': 96.922, 'eval_steps_per_second': 6.106, 'epoch': 0.96}
{'loss': 0.6421, 'grad_norm': 2.523940324783325, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5093419551849365, 'eval_runtime': 10.3177, 'eval_samples_per_second': 96.921, 'eval_steps_per_second': 6.106, 'epoch': 1.0}
{'train_runtime': 417.8107, 'train_samples_per_second': 23.927, 'train_steps_per_second': 1.496, 'train_loss': 0.823080615234375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4401233196258545, 1.3649412393569946, 1.3332387208938599, 1.361615777015686, 1.3888142108917236, 1.419145941734314, 1.412695050239563, 1.4278401136398315, 1.4388940334320068, 1.44257652759552, 1.426006555557251, 1.437944769859314, 1.457876205444336, 1.4666446447372437, 1.4563374519348145, 1.453000783920288, 1.4736390113830566, 1.480785608291626, 1.4740322828292847, 1.489406943321228, 1.483664870262146, 1.5008001327514648, 1.5011540651321411, 1.508453607559204, 1.5093419551849365], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4401233196258545, 1.3649412393569946, 1.3332387208938599, 1.361615777015686, 1.3888142108917236, 1.419145941734314, 1.412695050239563, 1.4278401136398315, 1.4388940334320068, 1.44257652759552, 1.426006555557251, 1.437944769859314, 1.457876205444336, 1.4666446447372437, 1.4563374519348145, 1.453000783920288, 1.4736390113830566, 1.480785608291626, 1.4740322828292847, 1.489406943321228, 1.483664870262146, 1.5008001327514648, 1.5011540651321411, 1.508453607559204, 1.5093419551849365]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.3906335830688477
current iteration best possible eval_loss (full train run):  -1.5093419551849365
max eval_loss so far:  -1.5093419551849365
BO observations:  [-2.3906335830688477]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1724 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.35090100031355376
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.36496307974132836
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.35048402642104226
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.3738364253008357
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.365007986877915
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2177, dtype=torch.float64), tensor(0.3340, dtype=torch.float64), 0, tensor(0.3939, dtype=torch.float64), tensor(0.0184, dtype=torch.float64), 0, 0, 0, tensor(0.0361, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 128, 0.09997800204917702, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2177, dtype=torch.float64), tensor(0.3340, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3939, dtype=torch.float64), tensor(0.0184, dtype=torch.float64), tensor(4.4270e-17, dtype=torch.float64), tensor(5.7630e-18, dtype=torch.float64), tensor(2.2266e-17, dtype=torch.float64), tensor(0.0361, dtype=torch.float64), tensor(0.9898, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9998, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.218
  gsm8k: 0.334
  rowan_hellaswag: 0
  sciq: 0.394
  triviaqa: 0.018
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.036

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09997800204917702,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.09997800204917702
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.3192, 'grad_norm': 0.5042477250099182, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1125078201293945, 'eval_runtime': 10.3443, 'eval_samples_per_second': 96.671, 'eval_steps_per_second': 6.09, 'epoch': 0.04}
{'loss': 0.9744, 'grad_norm': 0.41676419973373413, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9117883443832397, 'eval_runtime': 10.3529, 'eval_samples_per_second': 96.591, 'eval_steps_per_second': 6.085, 'epoch': 0.08}
{'loss': 0.887, 'grad_norm': 0.24103949964046478, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.887498140335083, 'eval_runtime': 10.4042, 'eval_samples_per_second': 96.115, 'eval_steps_per_second': 6.055, 'epoch': 0.12}
{'loss': 0.8693, 'grad_norm': 0.23133574426174164, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8824945688247681, 'eval_runtime': 10.4398, 'eval_samples_per_second': 95.787, 'eval_steps_per_second': 6.035, 'epoch': 0.16}
{'loss': 0.8636, 'grad_norm': 0.27615901827812195, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8786603808403015, 'eval_runtime': 10.4376, 'eval_samples_per_second': 95.807, 'eval_steps_per_second': 6.036, 'epoch': 0.2}
{'loss': 0.8361, 'grad_norm': 0.2140982300043106, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8700733780860901, 'eval_runtime': 10.4504, 'eval_samples_per_second': 95.69, 'eval_steps_per_second': 6.028, 'epoch': 0.24}
{'loss': 0.835, 'grad_norm': 0.187538281083107, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8654256463050842, 'eval_runtime': 10.4389, 'eval_samples_per_second': 95.796, 'eval_steps_per_second': 6.035, 'epoch': 0.28}
{'loss': 0.8197, 'grad_norm': 0.22441019117832184, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.863021194934845, 'eval_runtime': 10.43, 'eval_samples_per_second': 95.877, 'eval_steps_per_second': 6.04, 'epoch': 0.32}
{'loss': 0.8159, 'grad_norm': 0.2110256403684616, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8662858605384827, 'eval_runtime': 10.4265, 'eval_samples_per_second': 95.91, 'eval_steps_per_second': 6.042, 'epoch': 0.36}
{'loss': 0.8236, 'grad_norm': 0.22987914085388184, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8557727932929993, 'eval_runtime': 10.4462, 'eval_samples_per_second': 95.728, 'eval_steps_per_second': 6.031, 'epoch': 0.4}
{'loss': 0.8039, 'grad_norm': 0.21532578766345978, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8547898530960083, 'eval_runtime': 10.4907, 'eval_samples_per_second': 95.323, 'eval_steps_per_second': 6.005, 'epoch': 0.44}
{'loss': 0.8276, 'grad_norm': 0.25854602456092834, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8520169854164124, 'eval_runtime': 10.4363, 'eval_samples_per_second': 95.819, 'eval_steps_per_second': 6.037, 'epoch': 0.48}
{'loss': 0.8003, 'grad_norm': 0.1990494728088379, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8523088693618774, 'eval_runtime': 10.43, 'eval_samples_per_second': 95.878, 'eval_steps_per_second': 6.04, 'epoch': 0.52}
{'loss': 0.8147, 'grad_norm': 0.20220793783664703, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8472980856895447, 'eval_runtime': 10.437, 'eval_samples_per_second': 95.813, 'eval_steps_per_second': 6.036, 'epoch': 0.56}
{'loss': 0.7987, 'grad_norm': 0.21954984962940216, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8474995493888855, 'eval_runtime': 10.4385, 'eval_samples_per_second': 95.8, 'eval_steps_per_second': 6.035, 'epoch': 0.6}
{'loss': 0.8026, 'grad_norm': 0.2026393860578537, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8464331030845642, 'eval_runtime': 10.4337, 'eval_samples_per_second': 95.843, 'eval_steps_per_second': 6.038, 'epoch': 0.64}
{'loss': 0.7914, 'grad_norm': 0.20916886627674103, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8441407084465027, 'eval_runtime': 10.4355, 'eval_samples_per_second': 95.826, 'eval_steps_per_second': 6.037, 'epoch': 0.68}
{'loss': 0.7859, 'grad_norm': 0.26564809679985046, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8413825631141663, 'eval_runtime': 10.4329, 'eval_samples_per_second': 95.851, 'eval_steps_per_second': 6.039, 'epoch': 0.72}
{'loss': 0.7833, 'grad_norm': 0.19570498168468475, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8361168503761292, 'eval_runtime': 10.4505, 'eval_samples_per_second': 95.689, 'eval_steps_per_second': 6.028, 'epoch': 0.76}
{'loss': 0.7638, 'grad_norm': 0.2179003357887268, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8356832265853882, 'eval_runtime': 10.4535, 'eval_samples_per_second': 95.662, 'eval_steps_per_second': 6.027, 'epoch': 0.8}
{'loss': 0.763, 'grad_norm': 0.25037458539009094, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8356021642684937, 'eval_runtime': 10.4863, 'eval_samples_per_second': 95.362, 'eval_steps_per_second': 6.008, 'epoch': 0.84}
{'loss': 0.7935, 'grad_norm': 0.2118377685546875, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8339635133743286, 'eval_runtime': 10.4495, 'eval_samples_per_second': 95.698, 'eval_steps_per_second': 6.029, 'epoch': 0.88}
{'loss': 0.7836, 'grad_norm': 0.20577265322208405, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8310953378677368, 'eval_runtime': 10.4352, 'eval_samples_per_second': 95.829, 'eval_steps_per_second': 6.037, 'epoch': 0.92}
{'loss': 0.7672, 'grad_norm': 0.2105521857738495, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.830967128276825, 'eval_runtime': 10.4369, 'eval_samples_per_second': 95.814, 'eval_steps_per_second': 6.036, 'epoch': 0.96}
{'loss': 0.7912, 'grad_norm': 0.23558153212070465, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.830283522605896, 'eval_runtime': 10.4324, 'eval_samples_per_second': 95.855, 'eval_steps_per_second': 6.039, 'epoch': 1.0}
{'train_runtime': 490.6623, 'train_samples_per_second': 20.375, 'train_steps_per_second': 1.274, 'train_loss': 0.8765819763183593, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1125078201293945, 0.9117883443832397, 0.887498140335083, 0.8824945688247681, 0.8786603808403015, 0.8700733780860901, 0.8654256463050842, 0.863021194934845, 0.8662858605384827, 0.8557727932929993, 0.8547898530960083, 0.8520169854164124, 0.8523088693618774, 0.8472980856895447, 0.8474995493888855, 0.8464331030845642, 0.8441407084465027, 0.8413825631141663, 0.8361168503761292, 0.8356832265853882, 0.8356021642684937, 0.8339635133743286, 0.8310953378677368, 0.830967128276825, 0.830283522605896], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1125078201293945, 0.9117883443832397, 0.887498140335083, 0.8824945688247681, 0.8786603808403015, 0.8700733780860901, 0.8654256463050842, 0.863021194934845, 0.8662858605384827, 0.8557727932929993, 0.8547898530960083, 0.8520169854164124, 0.8523088693618774, 0.8472980856895447, 0.8474995493888855, 0.8464331030845642, 0.8441407084465027, 0.8413825631141663, 0.8361168503761292, 0.8356832265853882, 0.8356021642684937, 0.8339635133743286, 0.8310953378677368, 0.830967128276825, 0.830283522605896]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.137067198753357
current iteration best possible eval_loss (full train run):  -0.830283522605896
max eval_loss so far:  -0.830283522605896
BO observations:  [-2.3906335830688477, -1.137067198753357]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7621 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.5756985065841765
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.5756890393219554
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.575695314385618
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.5755247530792468
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.5755145110064097
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0540, dtype=torch.float64), 0, tensor(0.0756, dtype=torch.float64), tensor(0.1112, dtype=torch.float64), tensor(0.3416, dtype=torch.float64), tensor(0.2341, dtype=torch.float64), tensor(0.1835, dtype=torch.float64), 3, 1, 1, 0, 1, 1, 128, 0.01248339803243374, 15.13995965302991, 1]
normalized proposed parameters for next round by BO: [tensor(5.8558e-18, dtype=torch.float64), tensor(1.1473e-18, dtype=torch.float64), tensor(0.0540, dtype=torch.float64), tensor(1.7242e-18, dtype=torch.float64), tensor(0.0756, dtype=torch.float64), tensor(0.1112, dtype=torch.float64), tensor(0.3416, dtype=torch.float64), tensor(0.2341, dtype=torch.float64), tensor(0.1835, dtype=torch.float64), tensor(0.0837, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1248, dtype=torch.float64), tensor(0.3154, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.054
  sciq: 0
  triviaqa: 0.076
  truthfulqa_gen: 0.111
  wikitext: 0.342
  mmlu: 0.234
  arc_challenge: 0.183

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.01248339803243374,)
  num_layers_to_apply: (3,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (15.13995965302991,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  3
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.01248339803243374
lora alpha:  15.13995965302991
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 19,267,584 || all params: 8,049,528,832 || trainable%: 0.2394
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.536, 'grad_norm': 1.630238652229309, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.904615879058838, 'eval_runtime': 9.0238, 'eval_samples_per_second': 110.818, 'eval_steps_per_second': 6.982, 'epoch': 0.04}
{'loss': 2.5823, 'grad_norm': 0.4502263069152832, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5233126878738403, 'eval_runtime': 9.1291, 'eval_samples_per_second': 109.54, 'eval_steps_per_second': 6.901, 'epoch': 0.08}
{'loss': 1.9392, 'grad_norm': 0.3095669448375702, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4592795372009277, 'eval_runtime': 9.1307, 'eval_samples_per_second': 109.521, 'eval_steps_per_second': 6.9, 'epoch': 0.12}
{'loss': 1.7222, 'grad_norm': 0.30205658078193665, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.469839096069336, 'eval_runtime': 9.1241, 'eval_samples_per_second': 109.6, 'eval_steps_per_second': 6.905, 'epoch': 0.16}
{'loss': 1.6611, 'grad_norm': 0.2768338918685913, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.481259822845459, 'eval_runtime': 9.1417, 'eval_samples_per_second': 109.389, 'eval_steps_per_second': 6.891, 'epoch': 0.2}
{'loss': 1.6468, 'grad_norm': 0.3504110276699066, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4815233945846558, 'eval_runtime': 9.1193, 'eval_samples_per_second': 109.658, 'eval_steps_per_second': 6.908, 'epoch': 0.24}
{'loss': 1.6266, 'grad_norm': 0.3261759579181671, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4484508037567139, 'eval_runtime': 9.1343, 'eval_samples_per_second': 109.477, 'eval_steps_per_second': 6.897, 'epoch': 0.28}
{'loss': 1.5512, 'grad_norm': 0.2515423595905304, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.400600552558899, 'eval_runtime': 9.1433, 'eval_samples_per_second': 109.37, 'eval_steps_per_second': 6.89, 'epoch': 0.32}
{'loss': 1.3961, 'grad_norm': 0.2236369550228119, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4229648113250732, 'eval_runtime': 9.0592, 'eval_samples_per_second': 110.386, 'eval_steps_per_second': 6.954, 'epoch': 0.36}
{'loss': 1.5119, 'grad_norm': 0.2859349846839905, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4377868175506592, 'eval_runtime': 9.0663, 'eval_samples_per_second': 110.299, 'eval_steps_per_second': 6.949, 'epoch': 0.4}
{'loss': 1.5392, 'grad_norm': 0.27706390619277954, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4466496706008911, 'eval_runtime': 9.0629, 'eval_samples_per_second': 110.34, 'eval_steps_per_second': 6.951, 'epoch': 0.44}
{'loss': 1.4606, 'grad_norm': 0.3221472203731537, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.458965539932251, 'eval_runtime': 9.0663, 'eval_samples_per_second': 110.298, 'eval_steps_per_second': 6.949, 'epoch': 0.48}
{'loss': 1.4974, 'grad_norm': 0.24762891232967377, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4404510259628296, 'eval_runtime': 9.0619, 'eval_samples_per_second': 110.352, 'eval_steps_per_second': 6.952, 'epoch': 0.52}
{'loss': 1.4801, 'grad_norm': 0.3982606828212738, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4244529008865356, 'eval_runtime': 9.0634, 'eval_samples_per_second': 110.334, 'eval_steps_per_second': 6.951, 'epoch': 0.56}
{'loss': 1.4904, 'grad_norm': 0.2990607023239136, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4264202117919922, 'eval_runtime': 9.0578, 'eval_samples_per_second': 110.402, 'eval_steps_per_second': 6.955, 'epoch': 0.6}
{'loss': 1.4247, 'grad_norm': 0.266775906085968, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.44904363155365, 'eval_runtime': 9.0729, 'eval_samples_per_second': 110.219, 'eval_steps_per_second': 6.944, 'epoch': 0.64}
{'loss': 1.4917, 'grad_norm': 0.2888297140598297, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4623734951019287, 'eval_runtime': 9.0664, 'eval_samples_per_second': 110.297, 'eval_steps_per_second': 6.949, 'epoch': 0.68}
{'loss': 1.4619, 'grad_norm': 0.21869279444217682, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4777339696884155, 'eval_runtime': 9.0707, 'eval_samples_per_second': 110.245, 'eval_steps_per_second': 6.945, 'epoch': 0.72}
{'loss': 1.4473, 'grad_norm': 0.2600381672382355, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4743671417236328, 'eval_runtime': 9.0685, 'eval_samples_per_second': 110.272, 'eval_steps_per_second': 6.947, 'epoch': 0.76}
{'loss': 1.4571, 'grad_norm': 0.21316780149936676, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4546095132827759, 'eval_runtime': 9.0564, 'eval_samples_per_second': 110.419, 'eval_steps_per_second': 6.956, 'epoch': 0.8}
{'loss': 1.4224, 'grad_norm': 0.2776113450527191, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4735978841781616, 'eval_runtime': 9.0628, 'eval_samples_per_second': 110.341, 'eval_steps_per_second': 6.951, 'epoch': 0.84}
{'loss': 1.4306, 'grad_norm': 0.27701568603515625, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.470802903175354, 'eval_runtime': 9.0662, 'eval_samples_per_second': 110.3, 'eval_steps_per_second': 6.949, 'epoch': 0.88}
{'loss': 1.4829, 'grad_norm': 0.27105844020843506, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4622832536697388, 'eval_runtime': 9.0814, 'eval_samples_per_second': 110.116, 'eval_steps_per_second': 6.937, 'epoch': 0.92}
{'loss': 1.4961, 'grad_norm': 0.2196943163871765, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4581063985824585, 'eval_runtime': 9.0426, 'eval_samples_per_second': 110.587, 'eval_steps_per_second': 6.967, 'epoch': 0.96}
{'loss': 1.4412, 'grad_norm': 0.25717779994010925, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4581928253173828, 'eval_runtime': 9.0382, 'eval_samples_per_second': 110.641, 'eval_steps_per_second': 6.97, 'epoch': 1.0}
{'train_runtime': 414.4967, 'train_samples_per_second': 24.118, 'train_steps_per_second': 1.508, 'train_loss': 1.6478798522949218, 'epoch': 1.0}
train_results:  {'eval_loss': [1.904615879058838, 1.5233126878738403, 1.4592795372009277, 1.469839096069336, 1.481259822845459, 1.4815233945846558, 1.4484508037567139, 1.400600552558899, 1.4229648113250732, 1.4377868175506592, 1.4466496706008911, 1.458965539932251, 1.4404510259628296, 1.4244529008865356, 1.4264202117919922, 1.44904363155365, 1.4623734951019287, 1.4777339696884155, 1.4743671417236328, 1.4546095132827759, 1.4735978841781616, 1.470802903175354, 1.4622832536697388, 1.4581063985824585, 1.4581928253173828], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.904615879058838, 1.5233126878738403, 1.4592795372009277, 1.469839096069336, 1.481259822845459, 1.4815233945846558, 1.4484508037567139, 1.400600552558899, 1.4229648113250732, 1.4377868175506592, 1.4466496706008911, 1.458965539932251, 1.4404510259628296, 1.4244529008865356, 1.4264202117919922, 1.44904363155365, 1.4623734951019287, 1.4777339696884155, 1.4743671417236328, 1.4546095132827759, 1.4735978841781616, 1.470802903175354, 1.4622832536697388, 1.4581063985824585, 1.4581928253173828]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.4195952415466309
current iteration best possible eval_loss (full train run):  -1.4581928253173828
max eval_loss so far:  -0.830283522605896
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.6796 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.5676697513050079
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.5676697536724253
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.5676697536852651
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.5676697536556943
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.5676697536836572
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0186, dtype=torch.float64), tensor(0.3888, dtype=torch.float64), 0, 0, tensor(0.0875, dtype=torch.float64), tensor(0.3752, dtype=torch.float64), 0, 0, tensor(0.1299, dtype=torch.float64), 20, 1, 1, 0, 1, 1, 91, 0.1, 1.5303256169925739, 0]
normalized proposed parameters for next round by BO: [tensor(0.0186, dtype=torch.float64), tensor(0.3888, dtype=torch.float64), tensor(3.8422e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0875, dtype=torch.float64), tensor(0.3752, dtype=torch.float64), tensor(6.5622e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1299, dtype=torch.float64), tensor(0.6248, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7102, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0319, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.019
  gsm8k: 0.389
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.087
  truthfulqa_gen: 0.375
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.13

LoRA Parameters:
  lora_r: (91,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (1.5303256169925739,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  91
lora dropout:  0.1
lora alpha:  1.5303256169925739
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 91,320,320 || all params: 8,121,581,568 || trainable%: 1.1244
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3844, 'grad_norm': 0.16752155125141144, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1178297996520996, 'eval_runtime': 10.5344, 'eval_samples_per_second': 94.927, 'eval_steps_per_second': 5.98, 'epoch': 0.04}
{'loss': 1.955, 'grad_norm': 0.20175020396709442, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.292182207107544, 'eval_runtime': 10.5839, 'eval_samples_per_second': 94.483, 'eval_steps_per_second': 5.952, 'epoch': 0.08}
{'loss': 1.1814, 'grad_norm': 0.06904233247041702, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0328305959701538, 'eval_runtime': 10.6978, 'eval_samples_per_second': 93.477, 'eval_steps_per_second': 5.889, 'epoch': 0.12}
{'loss': 0.9888, 'grad_norm': 0.05830603837966919, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9433808922767639, 'eval_runtime': 10.6834, 'eval_samples_per_second': 93.603, 'eval_steps_per_second': 5.897, 'epoch': 0.16}
{'loss': 0.9165, 'grad_norm': 0.05871499702334404, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9255906939506531, 'eval_runtime': 10.6325, 'eval_samples_per_second': 94.051, 'eval_steps_per_second': 5.925, 'epoch': 0.2}
{'loss': 0.9376, 'grad_norm': 0.05840151011943817, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9175090193748474, 'eval_runtime': 10.6298, 'eval_samples_per_second': 94.075, 'eval_steps_per_second': 5.927, 'epoch': 0.24}
{'loss': 0.8918, 'grad_norm': 0.04675137996673584, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9060385823249817, 'eval_runtime': 10.6355, 'eval_samples_per_second': 94.024, 'eval_steps_per_second': 5.924, 'epoch': 0.28}
{'loss': 0.8875, 'grad_norm': 0.05365990474820137, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9008838534355164, 'eval_runtime': 10.6339, 'eval_samples_per_second': 94.039, 'eval_steps_per_second': 5.924, 'epoch': 0.32}
{'loss': 0.878, 'grad_norm': 0.05151019245386124, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9005821347236633, 'eval_runtime': 10.6141, 'eval_samples_per_second': 94.215, 'eval_steps_per_second': 5.936, 'epoch': 0.36}
{'loss': 0.8547, 'grad_norm': 0.055616579949855804, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.894533097743988, 'eval_runtime': 10.6169, 'eval_samples_per_second': 94.19, 'eval_steps_per_second': 5.934, 'epoch': 0.4}
{'loss': 0.8546, 'grad_norm': 0.06502286344766617, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8942902088165283, 'eval_runtime': 10.6152, 'eval_samples_per_second': 94.205, 'eval_steps_per_second': 5.935, 'epoch': 0.44}
{'loss': 0.8523, 'grad_norm': 0.05057286471128464, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8899058699607849, 'eval_runtime': 10.6113, 'eval_samples_per_second': 94.239, 'eval_steps_per_second': 5.937, 'epoch': 0.48}
{'loss': 0.8653, 'grad_norm': 0.057581670582294464, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8882681727409363, 'eval_runtime': 10.6182, 'eval_samples_per_second': 94.178, 'eval_steps_per_second': 5.933, 'epoch': 0.52}
{'loss': 0.8711, 'grad_norm': 0.0627816691994667, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8870948553085327, 'eval_runtime': 10.6199, 'eval_samples_per_second': 94.162, 'eval_steps_per_second': 5.932, 'epoch': 0.56}
{'loss': 0.8534, 'grad_norm': 0.056558363139629364, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8832054734230042, 'eval_runtime': 10.6102, 'eval_samples_per_second': 94.249, 'eval_steps_per_second': 5.938, 'epoch': 0.6}
{'loss': 0.8481, 'grad_norm': 0.0623343326151371, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8813567757606506, 'eval_runtime': 10.6246, 'eval_samples_per_second': 94.121, 'eval_steps_per_second': 5.93, 'epoch': 0.64}
{'loss': 0.8259, 'grad_norm': 0.06343093514442444, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8840151429176331, 'eval_runtime': 10.6126, 'eval_samples_per_second': 94.228, 'eval_steps_per_second': 5.936, 'epoch': 0.68}
{'loss': 0.8292, 'grad_norm': 0.06452670693397522, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8797388672828674, 'eval_runtime': 10.609, 'eval_samples_per_second': 94.26, 'eval_steps_per_second': 5.938, 'epoch': 0.72}
{'loss': 0.8273, 'grad_norm': 0.0765886902809143, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8782791495323181, 'eval_runtime': 10.6134, 'eval_samples_per_second': 94.221, 'eval_steps_per_second': 5.936, 'epoch': 0.76}
{'loss': 0.8064, 'grad_norm': 0.0708766058087349, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8764703273773193, 'eval_runtime': 10.6062, 'eval_samples_per_second': 94.284, 'eval_steps_per_second': 5.94, 'epoch': 0.8}
{'loss': 0.8146, 'grad_norm': 0.07141739130020142, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.87825608253479, 'eval_runtime': 10.6198, 'eval_samples_per_second': 94.163, 'eval_steps_per_second': 5.932, 'epoch': 0.84}
{'loss': 0.8237, 'grad_norm': 0.06688089668750763, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8775450587272644, 'eval_runtime': 10.6125, 'eval_samples_per_second': 94.228, 'eval_steps_per_second': 5.936, 'epoch': 0.88}
{'loss': 0.8353, 'grad_norm': 0.06398473680019379, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8752703070640564, 'eval_runtime': 10.6062, 'eval_samples_per_second': 94.284, 'eval_steps_per_second': 5.94, 'epoch': 0.92}
{'loss': 0.8401, 'grad_norm': 0.06990057229995728, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8762760162353516, 'eval_runtime': 10.6591, 'eval_samples_per_second': 93.816, 'eval_steps_per_second': 5.91, 'epoch': 0.96}
{'loss': 0.8238, 'grad_norm': 0.05551203712821007, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8750349879264832, 'eval_runtime': 10.627, 'eval_samples_per_second': 94.1, 'eval_steps_per_second': 5.928, 'epoch': 1.0}
{'train_runtime': 463.9481, 'train_samples_per_second': 21.548, 'train_steps_per_second': 1.347, 'train_loss': 1.0178772644042968, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1178297996520996, 1.292182207107544, 1.0328305959701538, 0.9433808922767639, 0.9255906939506531, 0.9175090193748474, 0.9060385823249817, 0.9008838534355164, 0.9005821347236633, 0.894533097743988, 0.8942902088165283, 0.8899058699607849, 0.8882681727409363, 0.8870948553085327, 0.8832054734230042, 0.8813567757606506, 0.8840151429176331, 0.8797388672828674, 0.8782791495323181, 0.8764703273773193, 0.87825608253479, 0.8775450587272644, 0.8752703070640564, 0.8762760162353516, 0.8750349879264832], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1178297996520996, 1.292182207107544, 1.0328305959701538, 0.9433808922767639, 0.9255906939506531, 0.9175090193748474, 0.9060385823249817, 0.9008838534355164, 0.9005821347236633, 0.894533097743988, 0.8942902088165283, 0.8899058699607849, 0.8882681727409363, 0.8870948553085327, 0.8832054734230042, 0.8813567757606506, 0.8840151429176331, 0.8797388672828674, 0.8782791495323181, 0.8764703273773193, 0.87825608253479, 0.8775450587272644, 0.8752703070640564, 0.8762760162353516, 0.8750349879264832]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.022498607635498
current iteration best possible eval_loss (full train run):  -0.8750349879264832
max eval_loss so far:  -0.830283522605896
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9995 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.6593385167842175
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.6593385167842175
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.6593385167842175
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.6593385167842175
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.6593385167842175
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0107, dtype=torch.float64), tensor(0.4295, dtype=torch.float64), tensor(0.1318, dtype=torch.float64), 0, 0, tensor(0.4279, dtype=torch.float64), 0, 0, 0, 1, 0, 1, 1, 0, 0, 101, 0.1, 33.36005947752008, 0]
normalized proposed parameters for next round by BO: [tensor(0.0107, dtype=torch.float64), tensor(0.4295, dtype=torch.float64), tensor(0.1318, dtype=torch.float64), tensor(2.2591e-18, dtype=torch.float64), tensor(0.0001, dtype=torch.float64), tensor(0.4279, dtype=torch.float64), tensor(2.0232e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.7181e-19, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7858, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6950, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.011
  gsm8k: 0.429
  rowan_hellaswag: 0.132
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.428
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (101,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 1, 0, 0],)
  lora_alpha: (33.36005947752008,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 0]
lora rank:  101
lora dropout:  0.1
lora alpha:  33.36005947752008
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,378,752 || all params: 8,032,640,000 || trainable%: 0.0296
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4462, 'grad_norm': 0.770006537437439, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.225250482559204, 'eval_runtime': 8.8778, 'eval_samples_per_second': 112.641, 'eval_steps_per_second': 7.096, 'epoch': 0.04}
{'loss': 2.2897, 'grad_norm': 0.3700285255908966, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5021904706954956, 'eval_runtime': 8.8891, 'eval_samples_per_second': 112.497, 'eval_steps_per_second': 7.087, 'epoch': 0.08}
{'loss': 1.8253, 'grad_norm': 0.31677404046058655, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2881786823272705, 'eval_runtime': 8.9337, 'eval_samples_per_second': 111.935, 'eval_steps_per_second': 7.052, 'epoch': 0.12}
{'loss': 1.6319, 'grad_norm': 0.25389954447746277, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.184390664100647, 'eval_runtime': 8.9659, 'eval_samples_per_second': 111.534, 'eval_steps_per_second': 7.027, 'epoch': 0.16}
{'loss': 1.4049, 'grad_norm': 0.23285384476184845, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1063473224639893, 'eval_runtime': 8.9872, 'eval_samples_per_second': 111.27, 'eval_steps_per_second': 7.01, 'epoch': 0.2}
{'loss': 1.3568, 'grad_norm': 0.281469464302063, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0752251148223877, 'eval_runtime': 9.0025, 'eval_samples_per_second': 111.08, 'eval_steps_per_second': 6.998, 'epoch': 0.24}
{'loss': 1.3294, 'grad_norm': 0.245336651802063, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0392206907272339, 'eval_runtime': 9.0175, 'eval_samples_per_second': 110.895, 'eval_steps_per_second': 6.986, 'epoch': 0.28}
{'loss': 1.1882, 'grad_norm': 0.2674168050289154, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0237165689468384, 'eval_runtime': 9.0206, 'eval_samples_per_second': 110.858, 'eval_steps_per_second': 6.984, 'epoch': 0.32}
{'loss': 1.2009, 'grad_norm': 0.29020559787750244, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0179001092910767, 'eval_runtime': 9.023, 'eval_samples_per_second': 110.828, 'eval_steps_per_second': 6.982, 'epoch': 0.36}
{'loss': 1.2552, 'grad_norm': 0.44248655438423157, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.015127420425415, 'eval_runtime': 9.0168, 'eval_samples_per_second': 110.904, 'eval_steps_per_second': 6.987, 'epoch': 0.4}
{'loss': 1.2438, 'grad_norm': 0.2824147045612335, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0083634853363037, 'eval_runtime': 9.0044, 'eval_samples_per_second': 111.057, 'eval_steps_per_second': 6.997, 'epoch': 0.44}
{'loss': 1.1694, 'grad_norm': 0.2777979075908661, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0069797039031982, 'eval_runtime': 8.9851, 'eval_samples_per_second': 111.295, 'eval_steps_per_second': 7.012, 'epoch': 0.48}
{'loss': 1.1423, 'grad_norm': 0.28820741176605225, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0000889301300049, 'eval_runtime': 9.0411, 'eval_samples_per_second': 110.607, 'eval_steps_per_second': 6.968, 'epoch': 0.52}
{'loss': 1.229, 'grad_norm': 0.31916531920433044, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9971910119056702, 'eval_runtime': 9.0437, 'eval_samples_per_second': 110.574, 'eval_steps_per_second': 6.966, 'epoch': 0.56}
{'loss': 1.2371, 'grad_norm': 0.26235535740852356, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9987096190452576, 'eval_runtime': 9.0554, 'eval_samples_per_second': 110.431, 'eval_steps_per_second': 6.957, 'epoch': 0.6}
{'loss': 1.1701, 'grad_norm': 0.2699863314628601, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9946909546852112, 'eval_runtime': 9.0454, 'eval_samples_per_second': 110.554, 'eval_steps_per_second': 6.965, 'epoch': 0.64}
{'loss': 1.2412, 'grad_norm': 0.27108126878738403, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9938986897468567, 'eval_runtime': 9.0605, 'eval_samples_per_second': 110.369, 'eval_steps_per_second': 6.953, 'epoch': 0.68}
{'loss': 1.1592, 'grad_norm': 0.25604599714279175, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9933623671531677, 'eval_runtime': 9.0463, 'eval_samples_per_second': 110.543, 'eval_steps_per_second': 6.964, 'epoch': 0.72}
{'loss': 1.1873, 'grad_norm': 0.29030993580818176, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9931647777557373, 'eval_runtime': 9.0406, 'eval_samples_per_second': 110.612, 'eval_steps_per_second': 6.969, 'epoch': 0.76}
{'loss': 1.1867, 'grad_norm': 0.3023476302623749, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9898504614830017, 'eval_runtime': 9.0533, 'eval_samples_per_second': 110.457, 'eval_steps_per_second': 6.959, 'epoch': 0.8}
{'loss': 1.1877, 'grad_norm': 0.3204353451728821, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.990953266620636, 'eval_runtime': 9.0383, 'eval_samples_per_second': 110.64, 'eval_steps_per_second': 6.97, 'epoch': 0.84}
{'loss': 1.2004, 'grad_norm': 0.26072919368743896, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9878905415534973, 'eval_runtime': 9.025, 'eval_samples_per_second': 110.804, 'eval_steps_per_second': 6.981, 'epoch': 0.88}
{'loss': 1.1645, 'grad_norm': 0.32804226875305176, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9875813126564026, 'eval_runtime': 9.0633, 'eval_samples_per_second': 110.335, 'eval_steps_per_second': 6.951, 'epoch': 0.92}
{'loss': 1.2246, 'grad_norm': 0.29139286279678345, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9880579710006714, 'eval_runtime': 9.1053, 'eval_samples_per_second': 109.826, 'eval_steps_per_second': 6.919, 'epoch': 0.96}
{'loss': 1.1987, 'grad_norm': 0.3400193750858307, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9876929521560669, 'eval_runtime': 9.0809, 'eval_samples_per_second': 110.121, 'eval_steps_per_second': 6.938, 'epoch': 1.0}
{'train_runtime': 324.045, 'train_samples_per_second': 30.848, 'train_steps_per_second': 1.929, 'train_loss': 1.3948181274414062, 'epoch': 1.0}
train_results:  {'eval_loss': [2.225250482559204, 1.5021904706954956, 1.2881786823272705, 1.184390664100647, 1.1063473224639893, 1.0752251148223877, 1.0392206907272339, 1.0237165689468384, 1.0179001092910767, 1.015127420425415, 1.0083634853363037, 1.0069797039031982, 1.0000889301300049, 0.9971910119056702, 0.9987096190452576, 0.9946909546852112, 0.9938986897468567, 0.9933623671531677, 0.9931647777557373, 0.9898504614830017, 0.990953266620636, 0.9878905415534973, 0.9875813126564026, 0.9880579710006714, 0.9876929521560669], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.225250482559204, 1.5021904706954956, 1.2881786823272705, 1.184390664100647, 1.1063473224639893, 1.0752251148223877, 1.0392206907272339, 1.0237165689468384, 1.0179001092910767, 1.015127420425415, 1.0083634853363037, 1.0069797039031982, 1.0000889301300049, 0.9971910119056702, 0.9987096190452576, 0.9946909546852112, 0.9938986897468567, 0.9933623671531677, 0.9931647777557373, 0.9898504614830017, 0.990953266620636, 0.9878905415534973, 0.9875813126564026, 0.9880579710006714, 0.9876929521560669]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.925295114517212
current iteration best possible eval_loss (full train run):  -0.9876929521560669
max eval_loss so far:  -0.830283522605896
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.3503 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.6742024352655737
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.6742024352655737
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.6742024352655737
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.6742024352655737
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.6742024352655737
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0834, dtype=torch.float64), tensor(0.0214, dtype=torch.float64), tensor(0.0251, dtype=torch.float64), tensor(0.3300, dtype=torch.float64), tensor(0.0102, dtype=torch.float64), tensor(0.1625, dtype=torch.float64), tensor(0.1345, dtype=torch.float64), tensor(0.0321, dtype=torch.float64), tensor(0.2008, dtype=torch.float64), 23, 1, 1, 1, 0, 1, 61, 0.09711657166481019, 40.86248588562012, 0]
normalized proposed parameters for next round by BO: [tensor(0.0834, dtype=torch.float64), tensor(0.0214, dtype=torch.float64), tensor(0.0251, dtype=torch.float64), tensor(0.3300, dtype=torch.float64), tensor(0.0102, dtype=torch.float64), tensor(0.1625, dtype=torch.float64), tensor(0.1345, dtype=torch.float64), tensor(0.0321, dtype=torch.float64), tensor(0.2008, dtype=torch.float64), tensor(0.7175, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4759, dtype=torch.float64), tensor(0.9712, dtype=torch.float64), tensor(0.8513, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.083
  gsm8k: 0.021
  rowan_hellaswag: 0.025
  sciq: 0.33
  triviaqa: 0.01
  truthfulqa_gen: 0.162
  wikitext: 0.135
  mmlu: 0.032
  arc_challenge: 0.201

LoRA Parameters:
  lora_r: (61,)
  lora_dropout: (0.09711657166481019,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (40.86248588562012,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  61
lora dropout:  0.09711657166481019
lora alpha:  40.86248588562012
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 70,396,928 || all params: 8,100,658,176 || trainable%: 0.8690
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1007, 'grad_norm': 0.929283618927002, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4259082078933716, 'eval_runtime': 10.534, 'eval_samples_per_second': 94.931, 'eval_steps_per_second': 5.981, 'epoch': 0.04}
{'loss': 1.443, 'grad_norm': 0.4123944640159607, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.219542145729065, 'eval_runtime': 10.5723, 'eval_samples_per_second': 94.586, 'eval_steps_per_second': 5.959, 'epoch': 0.08}
{'loss': 1.2569, 'grad_norm': 0.4470446705818176, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0797030925750732, 'eval_runtime': 10.541, 'eval_samples_per_second': 94.868, 'eval_steps_per_second': 5.977, 'epoch': 0.12}
{'loss': 1.2665, 'grad_norm': 0.3781338334083557, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9901684522628784, 'eval_runtime': 10.5605, 'eval_samples_per_second': 94.693, 'eval_steps_per_second': 5.966, 'epoch': 0.16}
{'loss': 1.0979, 'grad_norm': 0.29018449783325195, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9669119715690613, 'eval_runtime': 10.5644, 'eval_samples_per_second': 94.658, 'eval_steps_per_second': 5.963, 'epoch': 0.2}
{'loss': 1.0763, 'grad_norm': 0.3352937400341034, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.979912519454956, 'eval_runtime': 10.5664, 'eval_samples_per_second': 94.64, 'eval_steps_per_second': 5.962, 'epoch': 0.24}
{'loss': 1.0249, 'grad_norm': 0.3268565237522125, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9635987281799316, 'eval_runtime': 10.5672, 'eval_samples_per_second': 94.633, 'eval_steps_per_second': 5.962, 'epoch': 0.28}
{'loss': 1.0532, 'grad_norm': 0.3653429448604584, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9557391405105591, 'eval_runtime': 10.5578, 'eval_samples_per_second': 94.717, 'eval_steps_per_second': 5.967, 'epoch': 0.32}
{'loss': 1.0541, 'grad_norm': 0.31368789076805115, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9553056359291077, 'eval_runtime': 10.56, 'eval_samples_per_second': 94.697, 'eval_steps_per_second': 5.966, 'epoch': 0.36}
{'loss': 1.1009, 'grad_norm': 0.3534829318523407, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9415057301521301, 'eval_runtime': 10.5654, 'eval_samples_per_second': 94.649, 'eval_steps_per_second': 5.963, 'epoch': 0.4}
{'loss': 1.0244, 'grad_norm': 0.38913896679878235, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9421197772026062, 'eval_runtime': 10.5661, 'eval_samples_per_second': 94.642, 'eval_steps_per_second': 5.962, 'epoch': 0.44}
{'loss': 1.0096, 'grad_norm': 0.3352440595626831, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.945590078830719, 'eval_runtime': 10.5797, 'eval_samples_per_second': 94.521, 'eval_steps_per_second': 5.955, 'epoch': 0.48}
{'loss': 1.0094, 'grad_norm': 0.4441729187965393, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9373326897621155, 'eval_runtime': 10.5854, 'eval_samples_per_second': 94.47, 'eval_steps_per_second': 5.952, 'epoch': 0.52}
{'loss': 0.9953, 'grad_norm': 0.4279102683067322, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9298242330551147, 'eval_runtime': 10.5744, 'eval_samples_per_second': 94.568, 'eval_steps_per_second': 5.958, 'epoch': 0.56}
{'loss': 0.9253, 'grad_norm': 0.39531728625297546, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9319053888320923, 'eval_runtime': 10.579, 'eval_samples_per_second': 94.527, 'eval_steps_per_second': 5.955, 'epoch': 0.6}
{'loss': 1.01, 'grad_norm': 0.32101497054100037, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.930404543876648, 'eval_runtime': 10.5738, 'eval_samples_per_second': 94.573, 'eval_steps_per_second': 5.958, 'epoch': 0.64}
{'loss': 0.9614, 'grad_norm': 0.3443489372730255, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9291507601737976, 'eval_runtime': 10.6068, 'eval_samples_per_second': 94.279, 'eval_steps_per_second': 5.94, 'epoch': 0.68}
{'loss': 1.0075, 'grad_norm': 0.36637499928474426, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9217508435249329, 'eval_runtime': 10.5926, 'eval_samples_per_second': 94.405, 'eval_steps_per_second': 5.948, 'epoch': 0.72}
{'loss': 0.9229, 'grad_norm': 0.44488856196403503, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9270046353340149, 'eval_runtime': 10.5926, 'eval_samples_per_second': 94.405, 'eval_steps_per_second': 5.948, 'epoch': 0.76}
{'loss': 0.9877, 'grad_norm': 0.4191400706768036, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9190512895584106, 'eval_runtime': 10.5665, 'eval_samples_per_second': 94.639, 'eval_steps_per_second': 5.962, 'epoch': 0.8}
{'loss': 0.951, 'grad_norm': 0.4127666652202606, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.923632800579071, 'eval_runtime': 10.5955, 'eval_samples_per_second': 94.38, 'eval_steps_per_second': 5.946, 'epoch': 0.84}
{'loss': 1.0376, 'grad_norm': 0.4306136965751648, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9197039604187012, 'eval_runtime': 10.5882, 'eval_samples_per_second': 94.444, 'eval_steps_per_second': 5.95, 'epoch': 0.88}
{'loss': 1.0642, 'grad_norm': 0.3752119541168213, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9193888902664185, 'eval_runtime': 10.5669, 'eval_samples_per_second': 94.635, 'eval_steps_per_second': 5.962, 'epoch': 0.92}
{'loss': 0.8412, 'grad_norm': 0.3881179988384247, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9165459871292114, 'eval_runtime': 10.5702, 'eval_samples_per_second': 94.605, 'eval_steps_per_second': 5.96, 'epoch': 0.96}
{'loss': 0.9823, 'grad_norm': 0.5829675793647766, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9161890745162964, 'eval_runtime': 10.5654, 'eval_samples_per_second': 94.649, 'eval_steps_per_second': 5.963, 'epoch': 1.0}
{'train_runtime': 444.1206, 'train_samples_per_second': 22.505, 'train_steps_per_second': 1.407, 'train_loss': 1.1281651641845702, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4259082078933716, 1.219542145729065, 1.0797030925750732, 0.9901684522628784, 0.9669119715690613, 0.979912519454956, 0.9635987281799316, 0.9557391405105591, 0.9553056359291077, 0.9415057301521301, 0.9421197772026062, 0.945590078830719, 0.9373326897621155, 0.9298242330551147, 0.9319053888320923, 0.930404543876648, 0.9291507601737976, 0.9217508435249329, 0.9270046353340149, 0.9190512895584106, 0.923632800579071, 0.9197039604187012, 0.9193888902664185, 0.9165459871292114, 0.9161890745162964], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4259082078933716, 1.219542145729065, 1.0797030925750732, 0.9901684522628784, 0.9669119715690613, 0.979912519454956, 0.9635987281799316, 0.9557391405105591, 0.9553056359291077, 0.9415057301521301, 0.9421197772026062, 0.945590078830719, 0.9373326897621155, 0.9298242330551147, 0.9319053888320923, 0.930404543876648, 0.9291507601737976, 0.9217508435249329, 0.9270046353340149, 0.9190512895584106, 0.923632800579071, 0.9197039604187012, 0.9193888902664185, 0.9165459871292114, 0.9161890745162964]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.6537652015686035
current iteration best possible eval_loss (full train run):  -0.9161890745162964
max eval_loss so far:  -0.830283522605896
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.8080 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.7425078819688571
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.7425078824326183
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.7425078824326183
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.7425078741462012
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.7425078812045043
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0829, dtype=torch.float64), tensor(0.3070, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), 0, tensor(0.1491, dtype=torch.float64), tensor(0.0974, dtype=torch.float64), tensor(0.1078, dtype=torch.float64), tensor(0.0805, dtype=torch.float64), tensor(0.1188, dtype=torch.float64), 19, 0, 1, 0, 0, 1, 124, 0.0005815094685760494, 38.49222174181113, 0]
normalized proposed parameters for next round by BO: [tensor(0.0829, dtype=torch.float64), tensor(0.3070, dtype=torch.float64), tensor(0.0565, dtype=torch.float64), tensor(1.8399e-18, dtype=torch.float64), tensor(0.1491, dtype=torch.float64), tensor(0.0974, dtype=torch.float64), tensor(0.1078, dtype=torch.float64), tensor(0.0805, dtype=torch.float64), tensor(0.1188, dtype=torch.float64), tensor(0.5861, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9694, dtype=torch.float64), tensor(0.0058, dtype=torch.float64), tensor(0.8019, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.083
  gsm8k: 0.307
  rowan_hellaswag: 0.056
  sciq: 0
  triviaqa: 0.149
  truthfulqa_gen: 0.097
  wikitext: 0.108
  mmlu: 0.08
  arc_challenge: 0.119

LoRA Parameters:
  lora_r: (124,)
  lora_dropout: (0.0005815094685760494,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (38.49222174181113,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  124
lora dropout:  0.0005815094685760494
lora alpha:  38.49222174181113
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 55,488,512 || all params: 8,085,749,760 || trainable%: 0.6863
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9623, 'grad_norm': 0.36263951659202576, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4629532098770142, 'eval_runtime': 9.4278, 'eval_samples_per_second': 106.069, 'eval_steps_per_second': 6.682, 'epoch': 0.04}
{'loss': 1.633, 'grad_norm': 0.2011871188879013, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0745551586151123, 'eval_runtime': 9.4672, 'eval_samples_per_second': 105.628, 'eval_steps_per_second': 6.655, 'epoch': 0.08}
{'loss': 1.293, 'grad_norm': 0.18137922883033752, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.006474494934082, 'eval_runtime': 9.5175, 'eval_samples_per_second': 105.07, 'eval_steps_per_second': 6.619, 'epoch': 0.12}
{'loss': 1.2204, 'grad_norm': 0.22893604636192322, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9617308378219604, 'eval_runtime': 9.5626, 'eval_samples_per_second': 104.574, 'eval_steps_per_second': 6.588, 'epoch': 0.16}
{'loss': 1.2075, 'grad_norm': 0.17213407158851624, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9386501312255859, 'eval_runtime': 9.5679, 'eval_samples_per_second': 104.517, 'eval_steps_per_second': 6.585, 'epoch': 0.2}
{'loss': 1.1843, 'grad_norm': 0.20719636976718903, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9146121740341187, 'eval_runtime': 9.5653, 'eval_samples_per_second': 104.544, 'eval_steps_per_second': 6.586, 'epoch': 0.24}
{'loss': 1.1283, 'grad_norm': 0.19707390666007996, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8992384076118469, 'eval_runtime': 9.5654, 'eval_samples_per_second': 104.543, 'eval_steps_per_second': 6.586, 'epoch': 0.28}
{'loss': 1.104, 'grad_norm': 0.19452974200248718, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8919914364814758, 'eval_runtime': 9.5948, 'eval_samples_per_second': 104.223, 'eval_steps_per_second': 6.566, 'epoch': 0.32}
{'loss': 1.1249, 'grad_norm': 0.17559228837490082, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8898162245750427, 'eval_runtime': 9.5814, 'eval_samples_per_second': 104.369, 'eval_steps_per_second': 6.575, 'epoch': 0.36}
{'loss': 1.1216, 'grad_norm': 0.15537527203559875, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8871057033538818, 'eval_runtime': 9.5843, 'eval_samples_per_second': 104.338, 'eval_steps_per_second': 6.573, 'epoch': 0.4}
{'loss': 1.035, 'grad_norm': 0.18092086911201477, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8840879201889038, 'eval_runtime': 9.5914, 'eval_samples_per_second': 104.26, 'eval_steps_per_second': 6.568, 'epoch': 0.44}
{'loss': 1.1337, 'grad_norm': 0.16013777256011963, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8796675801277161, 'eval_runtime': 9.5821, 'eval_samples_per_second': 104.361, 'eval_steps_per_second': 6.575, 'epoch': 0.48}
{'loss': 1.1267, 'grad_norm': 0.18723730742931366, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8797590136528015, 'eval_runtime': 9.5829, 'eval_samples_per_second': 104.353, 'eval_steps_per_second': 6.574, 'epoch': 0.52}
{'loss': 1.099, 'grad_norm': 0.2147446721792221, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8762462139129639, 'eval_runtime': 9.5932, 'eval_samples_per_second': 104.24, 'eval_steps_per_second': 6.567, 'epoch': 0.56}
{'loss': 1.0503, 'grad_norm': 0.18813037872314453, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8730219602584839, 'eval_runtime': 9.5818, 'eval_samples_per_second': 104.365, 'eval_steps_per_second': 6.575, 'epoch': 0.6}
{'loss': 1.1322, 'grad_norm': 0.16032657027244568, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8742043375968933, 'eval_runtime': 9.5753, 'eval_samples_per_second': 104.435, 'eval_steps_per_second': 6.579, 'epoch': 0.64}
{'loss': 1.1482, 'grad_norm': 0.1617867648601532, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8713130950927734, 'eval_runtime': 9.5858, 'eval_samples_per_second': 104.321, 'eval_steps_per_second': 6.572, 'epoch': 0.68}
{'loss': 1.1209, 'grad_norm': 0.17147088050842285, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8704603314399719, 'eval_runtime': 9.5808, 'eval_samples_per_second': 104.376, 'eval_steps_per_second': 6.576, 'epoch': 0.72}
{'loss': 1.0873, 'grad_norm': 0.20836299657821655, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8684444427490234, 'eval_runtime': 9.5631, 'eval_samples_per_second': 104.569, 'eval_steps_per_second': 6.588, 'epoch': 0.76}
{'loss': 1.1317, 'grad_norm': 0.17946766316890717, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8675097823143005, 'eval_runtime': 9.5768, 'eval_samples_per_second': 104.419, 'eval_steps_per_second': 6.578, 'epoch': 0.8}
{'loss': 1.0913, 'grad_norm': 0.17736665904521942, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8666101694107056, 'eval_runtime': 9.593, 'eval_samples_per_second': 104.243, 'eval_steps_per_second': 6.567, 'epoch': 0.84}
{'loss': 1.0533, 'grad_norm': 0.18143069744110107, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8650771379470825, 'eval_runtime': 9.5932, 'eval_samples_per_second': 104.241, 'eval_steps_per_second': 6.567, 'epoch': 0.88}
{'loss': 1.1228, 'grad_norm': 0.17438577115535736, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.864506721496582, 'eval_runtime': 9.6017, 'eval_samples_per_second': 104.148, 'eval_steps_per_second': 6.561, 'epoch': 0.92}
{'loss': 1.0539, 'grad_norm': 0.18693020939826965, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8640522956848145, 'eval_runtime': 9.5827, 'eval_samples_per_second': 104.355, 'eval_steps_per_second': 6.574, 'epoch': 0.96}
{'loss': 1.0993, 'grad_norm': 0.20844802260398865, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.864062488079071, 'eval_runtime': 9.6231, 'eval_samples_per_second': 103.917, 'eval_steps_per_second': 6.547, 'epoch': 1.0}
{'train_runtime': 406.6142, 'train_samples_per_second': 24.581, 'train_steps_per_second': 1.537, 'train_loss': 1.2186028686523438, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4629532098770142, 1.0745551586151123, 1.006474494934082, 0.9617308378219604, 0.9386501312255859, 0.9146121740341187, 0.8992384076118469, 0.8919914364814758, 0.8898162245750427, 0.8871057033538818, 0.8840879201889038, 0.8796675801277161, 0.8797590136528015, 0.8762462139129639, 0.8730219602584839, 0.8742043375968933, 0.8713130950927734, 0.8704603314399719, 0.8684444427490234, 0.8675097823143005, 0.8666101694107056, 0.8650771379470825, 0.864506721496582, 0.8640522956848145, 0.864062488079071], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4629532098770142, 1.0745551586151123, 1.006474494934082, 0.9617308378219604, 0.9386501312255859, 0.9146121740341187, 0.8992384076118469, 0.8919914364814758, 0.8898162245750427, 0.8871057033538818, 0.8840879201889038, 0.8796675801277161, 0.8797590136528015, 0.8762462139129639, 0.8730219602584839, 0.8742043375968933, 0.8713130950927734, 0.8704603314399719, 0.8684444427490234, 0.8675097823143005, 0.8666101694107056, 0.8650771379470825, 0.864506721496582, 0.8640522956848145, 0.864062488079071]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3393288850784302
current iteration best possible eval_loss (full train run):  -0.864062488079071
max eval_loss so far:  -0.830283522605896
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2913 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.7492658626527506
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.7492658626527506
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.7492658626527506
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.7492658626527506
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.7492658626527506
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1070, dtype=torch.float64), tensor(0.1648, dtype=torch.float64), tensor(0.0180, dtype=torch.float64), tensor(0.2006, dtype=torch.float64), tensor(0.3650, dtype=torch.float64), 0, tensor(0.1170, dtype=torch.float64), tensor(0.0277, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 61, 0.002644511386695995, 36.080155637308806, 1]
normalized proposed parameters for next round by BO: [tensor(4.1290e-06, dtype=torch.float64), tensor(0.1070, dtype=torch.float64), tensor(0.1648, dtype=torch.float64), tensor(0.0180, dtype=torch.float64), tensor(0.2006, dtype=torch.float64), tensor(0.3650, dtype=torch.float64), tensor(3.7659e-19, dtype=torch.float64), tensor(0.1170, dtype=torch.float64), tensor(0.0277, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4731, dtype=torch.float64), tensor(0.0264, dtype=torch.float64), tensor(0.7517, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.107
  rowan_hellaswag: 0.165
  sciq: 0.018
  triviaqa: 0.201
  truthfulqa_gen: 0.365
  wikitext: 0
  mmlu: 0.117
  arc_challenge: 0.028

LoRA Parameters:
  lora_r: (61,)
  lora_dropout: (0.002644511386695995,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (36.080155637308806,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  61
lora dropout:  0.002644511386695995
lora alpha:  36.080155637308806
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 81,952,768 || all params: 8,112,214,016 || trainable%: 1.0102
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9168, 'grad_norm': 0.6953280568122864, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2681347131729126, 'eval_runtime': 10.8751, 'eval_samples_per_second': 91.953, 'eval_steps_per_second': 5.793, 'epoch': 0.04}
{'loss': 1.5159, 'grad_norm': 0.7119457125663757, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9647896885871887, 'eval_runtime': 10.8984, 'eval_samples_per_second': 91.757, 'eval_steps_per_second': 5.781, 'epoch': 0.08}
{'loss': 1.2258, 'grad_norm': 0.4067069888114929, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9351238012313843, 'eval_runtime': 10.9142, 'eval_samples_per_second': 91.624, 'eval_steps_per_second': 5.772, 'epoch': 0.12}
{'loss': 1.2193, 'grad_norm': 0.2709532082080841, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9273728728294373, 'eval_runtime': 10.9311, 'eval_samples_per_second': 91.482, 'eval_steps_per_second': 5.763, 'epoch': 0.16}
{'loss': 1.1995, 'grad_norm': 0.27468347549438477, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9071909189224243, 'eval_runtime': 10.9362, 'eval_samples_per_second': 91.44, 'eval_steps_per_second': 5.761, 'epoch': 0.2}
{'loss': 1.2323, 'grad_norm': 0.2963232696056366, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.901266872882843, 'eval_runtime': 10.9562, 'eval_samples_per_second': 91.273, 'eval_steps_per_second': 5.75, 'epoch': 0.24}
{'loss': 1.144, 'grad_norm': 0.30253562331199646, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9097314476966858, 'eval_runtime': 10.9591, 'eval_samples_per_second': 91.248, 'eval_steps_per_second': 5.749, 'epoch': 0.28}
{'loss': 1.2029, 'grad_norm': 0.26940247416496277, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9014479517936707, 'eval_runtime': 10.9549, 'eval_samples_per_second': 91.283, 'eval_steps_per_second': 5.751, 'epoch': 0.32}
{'loss': 1.1706, 'grad_norm': 0.35550251603126526, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8942442536354065, 'eval_runtime': 10.9666, 'eval_samples_per_second': 91.186, 'eval_steps_per_second': 5.745, 'epoch': 0.36}
{'loss': 1.1188, 'grad_norm': 0.33518874645233154, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.889636754989624, 'eval_runtime': 10.998, 'eval_samples_per_second': 90.926, 'eval_steps_per_second': 5.728, 'epoch': 0.4}
{'loss': 1.0817, 'grad_norm': 0.5093197822570801, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8901272416114807, 'eval_runtime': 11.0182, 'eval_samples_per_second': 90.759, 'eval_steps_per_second': 5.718, 'epoch': 0.44}
{'loss': 1.0801, 'grad_norm': 0.31992965936660767, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8952519297599792, 'eval_runtime': 11.0545, 'eval_samples_per_second': 90.461, 'eval_steps_per_second': 5.699, 'epoch': 0.48}
{'loss': 1.184, 'grad_norm': 0.3048177659511566, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8908808827400208, 'eval_runtime': 11.0569, 'eval_samples_per_second': 90.441, 'eval_steps_per_second': 5.698, 'epoch': 0.52}
{'loss': 1.1029, 'grad_norm': 0.35111376643180847, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8777807354927063, 'eval_runtime': 10.9959, 'eval_samples_per_second': 90.943, 'eval_steps_per_second': 5.729, 'epoch': 0.56}
{'loss': 1.1418, 'grad_norm': 0.41010555624961853, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8757330179214478, 'eval_runtime': 11.0419, 'eval_samples_per_second': 90.564, 'eval_steps_per_second': 5.706, 'epoch': 0.6}
{'loss': 1.0799, 'grad_norm': 0.34773123264312744, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8778656125068665, 'eval_runtime': 11.0132, 'eval_samples_per_second': 90.8, 'eval_steps_per_second': 5.72, 'epoch': 0.64}
{'loss': 1.0386, 'grad_norm': 0.3021673858165741, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8785178065299988, 'eval_runtime': 10.9732, 'eval_samples_per_second': 91.131, 'eval_steps_per_second': 5.741, 'epoch': 0.68}
{'loss': 1.0899, 'grad_norm': 0.3834209442138672, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.874055802822113, 'eval_runtime': 10.9759, 'eval_samples_per_second': 91.109, 'eval_steps_per_second': 5.74, 'epoch': 0.72}
{'loss': 1.0399, 'grad_norm': 0.43216055631637573, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8725777268409729, 'eval_runtime': 10.9721, 'eval_samples_per_second': 91.14, 'eval_steps_per_second': 5.742, 'epoch': 0.76}
{'loss': 1.0693, 'grad_norm': 0.2433696687221527, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8700538277626038, 'eval_runtime': 10.9663, 'eval_samples_per_second': 91.188, 'eval_steps_per_second': 5.745, 'epoch': 0.8}
{'loss': 1.0931, 'grad_norm': 0.3658648133277893, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8698331117630005, 'eval_runtime': 11.002, 'eval_samples_per_second': 90.892, 'eval_steps_per_second': 5.726, 'epoch': 0.84}
{'loss': 1.0487, 'grad_norm': 0.2530248463153839, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8694913387298584, 'eval_runtime': 10.9803, 'eval_samples_per_second': 91.073, 'eval_steps_per_second': 5.738, 'epoch': 0.88}
{'loss': 1.0357, 'grad_norm': 0.3729221224784851, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8667503595352173, 'eval_runtime': 10.9824, 'eval_samples_per_second': 91.055, 'eval_steps_per_second': 5.736, 'epoch': 0.92}
{'loss': 1.0062, 'grad_norm': 0.2858753800392151, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.866795539855957, 'eval_runtime': 10.9754, 'eval_samples_per_second': 91.113, 'eval_steps_per_second': 5.74, 'epoch': 0.96}
{'loss': 1.0324, 'grad_norm': 0.3522915244102478, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8664753437042236, 'eval_runtime': 10.987, 'eval_samples_per_second': 91.016, 'eval_steps_per_second': 5.734, 'epoch': 1.0}
{'train_runtime': 523.9239, 'train_samples_per_second': 19.079, 'train_steps_per_second': 1.193, 'train_loss': 1.202795166015625, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2681347131729126, 0.9647896885871887, 0.9351238012313843, 0.9273728728294373, 0.9071909189224243, 0.901266872882843, 0.9097314476966858, 0.9014479517936707, 0.8942442536354065, 0.889636754989624, 0.8901272416114807, 0.8952519297599792, 0.8908808827400208, 0.8777807354927063, 0.8757330179214478, 0.8778656125068665, 0.8785178065299988, 0.874055802822113, 0.8725777268409729, 0.8700538277626038, 0.8698331117630005, 0.8694913387298584, 0.8667503595352173, 0.866795539855957, 0.8664753437042236], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2681347131729126, 0.9647896885871887, 0.9351238012313843, 0.9273728728294373, 0.9071909189224243, 0.901266872882843, 0.9097314476966858, 0.9014479517936707, 0.8942442536354065, 0.889636754989624, 0.8901272416114807, 0.8952519297599792, 0.8908808827400208, 0.8777807354927063, 0.8757330179214478, 0.8778656125068665, 0.8785178065299988, 0.874055802822113, 0.8725777268409729, 0.8700538277626038, 0.8698331117630005, 0.8694913387298584, 0.8667503595352173, 0.866795539855957, 0.8664753437042236]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1837950944900513
current iteration best possible eval_loss (full train run):  -0.8664753437042236
max eval_loss so far:  -0.830283522605896
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.1847 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.8596260236071134
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.8596260236071134
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.8596260236071134
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.8596260236071134
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.8596260236071134
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3479, dtype=torch.float64), 0, tensor(0.0142, dtype=torch.float64), 0, 0, tensor(0.6378, dtype=torch.float64), 1, 0, 1, 0, 0, 1, 48, 0.1, 1.4800000190734983, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.5448e-16, dtype=torch.float64), tensor(0.3479, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0142, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6378, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3746, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.348
  triviaqa: 0
  truthfulqa_gen: 0.014
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.638

LoRA Parameters:
  lora_r: (48,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (1.4800000190734983,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  48
lora dropout:  0.1
lora alpha:  1.4800000190734983
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,130,496 || all params: 8,031,391,744 || trainable%: 0.0141
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.3866, 'grad_norm': 0.7928602695465088, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.565934658050537, 'eval_runtime': 8.8581, 'eval_samples_per_second': 112.892, 'eval_steps_per_second': 7.112, 'epoch': 0.04}
{'loss': 3.8983, 'grad_norm': 0.4506080746650696, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.186218500137329, 'eval_runtime': 8.9052, 'eval_samples_per_second': 112.294, 'eval_steps_per_second': 7.075, 'epoch': 0.08}
{'loss': 3.1177, 'grad_norm': 0.5051995515823364, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9092304706573486, 'eval_runtime': 8.9188, 'eval_samples_per_second': 112.123, 'eval_steps_per_second': 7.064, 'epoch': 0.12}
{'loss': 2.5203, 'grad_norm': 0.541404664516449, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8284733295440674, 'eval_runtime': 8.9693, 'eval_samples_per_second': 111.492, 'eval_steps_per_second': 7.024, 'epoch': 0.16}
{'loss': 2.0444, 'grad_norm': 0.5656974911689758, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7759970426559448, 'eval_runtime': 8.9865, 'eval_samples_per_second': 111.277, 'eval_steps_per_second': 7.01, 'epoch': 0.2}
{'loss': 1.6878, 'grad_norm': 0.6373710036277771, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.758994221687317, 'eval_runtime': 9.0084, 'eval_samples_per_second': 111.008, 'eval_steps_per_second': 6.994, 'epoch': 0.24}
{'loss': 1.5084, 'grad_norm': 1.1010587215423584, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7661439180374146, 'eval_runtime': 9.0258, 'eval_samples_per_second': 110.793, 'eval_steps_per_second': 6.98, 'epoch': 0.28}
{'loss': 1.4197, 'grad_norm': 0.521093487739563, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7577592134475708, 'eval_runtime': 9.0384, 'eval_samples_per_second': 110.639, 'eval_steps_per_second': 6.97, 'epoch': 0.32}
{'loss': 1.3738, 'grad_norm': 0.7232033610343933, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7690974473953247, 'eval_runtime': 9.0143, 'eval_samples_per_second': 110.935, 'eval_steps_per_second': 6.989, 'epoch': 0.36}
{'loss': 1.3682, 'grad_norm': 0.5259646773338318, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7813044786453247, 'eval_runtime': 9.0409, 'eval_samples_per_second': 110.608, 'eval_steps_per_second': 6.968, 'epoch': 0.4}
{'loss': 1.3175, 'grad_norm': 0.6429805755615234, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7709243297576904, 'eval_runtime': 8.9984, 'eval_samples_per_second': 111.131, 'eval_steps_per_second': 7.001, 'epoch': 0.44}
{'loss': 1.3225, 'grad_norm': 0.3606138527393341, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7913891077041626, 'eval_runtime': 9.0467, 'eval_samples_per_second': 110.538, 'eval_steps_per_second': 6.964, 'epoch': 0.48}
{'loss': 1.345, 'grad_norm': 0.3619822561740875, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.764554500579834, 'eval_runtime': 9.036, 'eval_samples_per_second': 110.669, 'eval_steps_per_second': 6.972, 'epoch': 0.52}
{'loss': 1.3087, 'grad_norm': 0.39696118235588074, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.75034761428833, 'eval_runtime': 8.9956, 'eval_samples_per_second': 111.166, 'eval_steps_per_second': 7.003, 'epoch': 0.56}
{'loss': 1.3166, 'grad_norm': 0.4821540415287018, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7411291599273682, 'eval_runtime': 8.9842, 'eval_samples_per_second': 111.306, 'eval_steps_per_second': 7.012, 'epoch': 0.6}
{'loss': 1.2892, 'grad_norm': 0.4234471023082733, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7400532960891724, 'eval_runtime': 9.007, 'eval_samples_per_second': 111.025, 'eval_steps_per_second': 6.995, 'epoch': 0.64}
{'loss': 1.2794, 'grad_norm': 0.5510028004646301, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7321956157684326, 'eval_runtime': 8.9819, 'eval_samples_per_second': 111.335, 'eval_steps_per_second': 7.014, 'epoch': 0.68}
{'loss': 1.2788, 'grad_norm': 0.345572829246521, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7303657531738281, 'eval_runtime': 8.9927, 'eval_samples_per_second': 111.202, 'eval_steps_per_second': 7.006, 'epoch': 0.72}
{'loss': 1.3002, 'grad_norm': 0.4784984290599823, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.70863938331604, 'eval_runtime': 8.9917, 'eval_samples_per_second': 111.213, 'eval_steps_per_second': 7.006, 'epoch': 0.76}
{'loss': 1.2723, 'grad_norm': 0.4112498462200165, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7307790517807007, 'eval_runtime': 8.9963, 'eval_samples_per_second': 111.157, 'eval_steps_per_second': 7.003, 'epoch': 0.8}
{'loss': 1.2617, 'grad_norm': 0.2889469265937805, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7009084224700928, 'eval_runtime': 8.9989, 'eval_samples_per_second': 111.125, 'eval_steps_per_second': 7.001, 'epoch': 0.84}
{'loss': 1.2624, 'grad_norm': 0.3936747908592224, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7022597789764404, 'eval_runtime': 8.9901, 'eval_samples_per_second': 111.233, 'eval_steps_per_second': 7.008, 'epoch': 0.88}
{'loss': 1.2222, 'grad_norm': 0.40753862261772156, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6929205656051636, 'eval_runtime': 8.9856, 'eval_samples_per_second': 111.289, 'eval_steps_per_second': 7.011, 'epoch': 0.92}
{'loss': 1.2321, 'grad_norm': 0.3306090533733368, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6959738731384277, 'eval_runtime': 8.9957, 'eval_samples_per_second': 111.165, 'eval_steps_per_second': 7.003, 'epoch': 0.96}
{'loss': 1.2357, 'grad_norm': 0.3342651426792145, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.687941074371338, 'eval_runtime': 9.0002, 'eval_samples_per_second': 111.109, 'eval_steps_per_second': 7.0, 'epoch': 1.0}
{'train_runtime': 357.8601, 'train_samples_per_second': 27.941, 'train_steps_per_second': 1.746, 'train_loss': 1.7027812042236328, 'epoch': 1.0}
train_results:  {'eval_loss': [2.565934658050537, 2.186218500137329, 1.9092304706573486, 1.8284733295440674, 1.7759970426559448, 1.758994221687317, 1.7661439180374146, 1.7577592134475708, 1.7690974473953247, 1.7813044786453247, 1.7709243297576904, 1.7913891077041626, 1.764554500579834, 1.75034761428833, 1.7411291599273682, 1.7400532960891724, 1.7321956157684326, 1.7303657531738281, 1.70863938331604, 1.7307790517807007, 1.7009084224700928, 1.7022597789764404, 1.6929205656051636, 1.6959738731384277, 1.687941074371338], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.565934658050537, 2.186218500137329, 1.9092304706573486, 1.8284733295440674, 1.7759970426559448, 1.758994221687317, 1.7661439180374146, 1.7577592134475708, 1.7690974473953247, 1.7813044786453247, 1.7709243297576904, 1.7913891077041626, 1.764554500579834, 1.75034761428833, 1.7411291599273682, 1.7400532960891724, 1.7321956157684326, 1.7303657531738281, 1.70863938331604, 1.7307790517807007, 1.7009084224700928, 1.7022597789764404, 1.6929205656051636, 1.6959738731384277, 1.687941074371338]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.4292274713516235
current iteration best possible eval_loss (full train run):  -1.687941074371338
max eval_loss so far:  -0.830283522605896
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4066 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.8643538531510548
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.8643538532054273
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.8643538531448669
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.86435385315103
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.8643538531591459
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5918, dtype=torch.float64), tensor(0.0593, dtype=torch.float64), tensor(0.1237, dtype=torch.float64), 0, tensor(0.1442, dtype=torch.float64), 0, tensor(0.0811, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 0, 122, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(3.0626e-17, dtype=torch.float64), tensor(0.5918, dtype=torch.float64), tensor(0.0593, dtype=torch.float64), tensor(0.1237, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1442, dtype=torch.float64), tensor(1.0399e-17, dtype=torch.float64), tensor(0.0811, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9526, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.592
  rowan_hellaswag: 0.059
  sciq: 0.124
  triviaqa: 0
  truthfulqa_gen: 0.144
  wikitext: 0
  mmlu: 0.081
  arc_challenge: 0

LoRA Parameters:
  lora_r: (122,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  122
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 91,947,008 || all params: 8,122,208,256 || trainable%: 1.1320
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.2396, 'grad_norm': 0.4737553894519806, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1205312013626099, 'eval_runtime': 9.9149, 'eval_samples_per_second': 100.859, 'eval_steps_per_second': 6.354, 'epoch': 0.04}
{'loss': 1.1048, 'grad_norm': 0.45038795471191406, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9128164052963257, 'eval_runtime': 9.9476, 'eval_samples_per_second': 100.527, 'eval_steps_per_second': 6.333, 'epoch': 0.08}
{'loss': 1.0636, 'grad_norm': 0.24519142508506775, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8868873119354248, 'eval_runtime': 9.9679, 'eval_samples_per_second': 100.322, 'eval_steps_per_second': 6.32, 'epoch': 0.12}
{'loss': 0.9734, 'grad_norm': 0.22692640125751495, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8718405961990356, 'eval_runtime': 9.9896, 'eval_samples_per_second': 100.104, 'eval_steps_per_second': 6.307, 'epoch': 0.16}
{'loss': 0.9115, 'grad_norm': 0.1958099901676178, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8673912882804871, 'eval_runtime': 9.9935, 'eval_samples_per_second': 100.066, 'eval_steps_per_second': 6.304, 'epoch': 0.2}
{'loss': 0.9241, 'grad_norm': 0.2501288056373596, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8622772097587585, 'eval_runtime': 10.0024, 'eval_samples_per_second': 99.976, 'eval_steps_per_second': 6.298, 'epoch': 0.24}
{'loss': 0.9422, 'grad_norm': 0.22975626587867737, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8565540909767151, 'eval_runtime': 10.0219, 'eval_samples_per_second': 99.781, 'eval_steps_per_second': 6.286, 'epoch': 0.28}
{'loss': 0.9074, 'grad_norm': 0.20804515480995178, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8545083999633789, 'eval_runtime': 10.007, 'eval_samples_per_second': 99.93, 'eval_steps_per_second': 6.296, 'epoch': 0.32}
{'loss': 0.9427, 'grad_norm': 0.1902526617050171, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8497041463851929, 'eval_runtime': 10.0037, 'eval_samples_per_second': 99.963, 'eval_steps_per_second': 6.298, 'epoch': 0.36}
{'loss': 0.9337, 'grad_norm': 0.2391313761472702, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8461184501647949, 'eval_runtime': 10.0087, 'eval_samples_per_second': 99.913, 'eval_steps_per_second': 6.295, 'epoch': 0.4}
{'loss': 0.9036, 'grad_norm': 0.21696442365646362, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8456484079360962, 'eval_runtime': 10.0068, 'eval_samples_per_second': 99.932, 'eval_steps_per_second': 6.296, 'epoch': 0.44}
{'loss': 0.9249, 'grad_norm': 0.1946096122264862, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8396230340003967, 'eval_runtime': 10.007, 'eval_samples_per_second': 99.93, 'eval_steps_per_second': 6.296, 'epoch': 0.48}
{'loss': 0.9051, 'grad_norm': 0.21034084260463715, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8377278447151184, 'eval_runtime': 10.0062, 'eval_samples_per_second': 99.938, 'eval_steps_per_second': 6.296, 'epoch': 0.52}
{'loss': 0.9067, 'grad_norm': 0.2003403604030609, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8367996215820312, 'eval_runtime': 10.0448, 'eval_samples_per_second': 99.554, 'eval_steps_per_second': 6.272, 'epoch': 0.56}
{'loss': 0.8744, 'grad_norm': 0.20807144045829773, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8337691426277161, 'eval_runtime': 10.0761, 'eval_samples_per_second': 99.245, 'eval_steps_per_second': 6.252, 'epoch': 0.6}
{'loss': 0.9301, 'grad_norm': 0.18330593407154083, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8318779468536377, 'eval_runtime': 10.0934, 'eval_samples_per_second': 99.074, 'eval_steps_per_second': 6.242, 'epoch': 0.64}
{'loss': 0.8924, 'grad_norm': 0.20204105973243713, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8326298594474792, 'eval_runtime': 10.0747, 'eval_samples_per_second': 99.258, 'eval_steps_per_second': 6.253, 'epoch': 0.68}
{'loss': 0.9507, 'grad_norm': 0.22725079953670502, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8297749161720276, 'eval_runtime': 10.0303, 'eval_samples_per_second': 99.697, 'eval_steps_per_second': 6.281, 'epoch': 0.72}
{'loss': 0.8729, 'grad_norm': 0.22126834094524384, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8279723525047302, 'eval_runtime': 10.0135, 'eval_samples_per_second': 99.865, 'eval_steps_per_second': 6.291, 'epoch': 0.76}
{'loss': 0.9122, 'grad_norm': 0.2377946674823761, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.82784104347229, 'eval_runtime': 10.0234, 'eval_samples_per_second': 99.767, 'eval_steps_per_second': 6.285, 'epoch': 0.8}
{'loss': 0.9048, 'grad_norm': 0.22142529487609863, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8279851675033569, 'eval_runtime': 10.0107, 'eval_samples_per_second': 99.893, 'eval_steps_per_second': 6.293, 'epoch': 0.84}
{'loss': 0.9117, 'grad_norm': 0.22508104145526886, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8277016878128052, 'eval_runtime': 10.0217, 'eval_samples_per_second': 99.784, 'eval_steps_per_second': 6.286, 'epoch': 0.88}
{'loss': 0.8839, 'grad_norm': 0.23244653642177582, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8244622945785522, 'eval_runtime': 10.0067, 'eval_samples_per_second': 99.933, 'eval_steps_per_second': 6.296, 'epoch': 0.92}
{'loss': 0.9396, 'grad_norm': 0.2126539796590805, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8240386247634888, 'eval_runtime': 10.005, 'eval_samples_per_second': 99.951, 'eval_steps_per_second': 6.297, 'epoch': 0.96}
{'loss': 0.8863, 'grad_norm': 0.27878332138061523, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.82343590259552, 'eval_runtime': 10.0146, 'eval_samples_per_second': 99.855, 'eval_steps_per_second': 6.291, 'epoch': 1.0}
{'train_runtime': 473.7443, 'train_samples_per_second': 21.102, 'train_steps_per_second': 1.319, 'train_loss': 0.9816919311523438, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1205312013626099, 0.9128164052963257, 0.8868873119354248, 0.8718405961990356, 0.8673912882804871, 0.8622772097587585, 0.8565540909767151, 0.8545083999633789, 0.8497041463851929, 0.8461184501647949, 0.8456484079360962, 0.8396230340003967, 0.8377278447151184, 0.8367996215820312, 0.8337691426277161, 0.8318779468536377, 0.8326298594474792, 0.8297749161720276, 0.8279723525047302, 0.82784104347229, 0.8279851675033569, 0.8277016878128052, 0.8244622945785522, 0.8240386247634888, 0.82343590259552], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1205312013626099, 0.9128164052963257, 0.8868873119354248, 0.8718405961990356, 0.8673912882804871, 0.8622772097587585, 0.8565540909767151, 0.8545083999633789, 0.8497041463851929, 0.8461184501647949, 0.8456484079360962, 0.8396230340003967, 0.8377278447151184, 0.8367996215820312, 0.8337691426277161, 0.8318779468536377, 0.8326298594474792, 0.8297749161720276, 0.8279723525047302, 0.82784104347229, 0.8279851675033569, 0.8277016878128052, 0.8244622945785522, 0.8240386247634888, 0.82343590259552]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.153781771659851
current iteration best possible eval_loss (full train run):  -0.82343590259552
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.0111 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.9851029989636433
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.9851029989704334
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.9851029989968167
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.9851029989706563
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.985102998971273
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5741, dtype=torch.float64), 0, tensor(0.0308, dtype=torch.float64), 0, tensor(0.0850, dtype=torch.float64), 0, 0, tensor(0.3101, dtype=torch.float64), 24, 0, 0, 0, 1, 1, 2, 0.1, 1.4800000190734877, 1]
normalized proposed parameters for next round by BO: [tensor(1.3029e-16, dtype=torch.float64), tensor(0.5741, dtype=torch.float64), tensor(2.2648e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(4.5513e-18, dtype=torch.float64), tensor(0.0850, dtype=torch.float64), tensor(4.1943e-17, dtype=torch.float64), tensor(4.0200e-17, dtype=torch.float64), tensor(0.3101, dtype=torch.float64), tensor(0.7390, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.574
  rowan_hellaswag: 0
  sciq: 0.031
  triviaqa: 0
  truthfulqa_gen: 0.085
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.31

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (1.4800000190734877,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.1
lora alpha:  1.4800000190734877
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,769,472 || all params: 8,032,030,720 || trainable%: 0.0220
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9252, 'grad_norm': 1.9375985860824585, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.030503988265991, 'eval_runtime': 9.8674, 'eval_samples_per_second': 101.344, 'eval_steps_per_second': 6.385, 'epoch': 0.04}
{'loss': 1.8723, 'grad_norm': 2.86818265914917, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3658008575439453, 'eval_runtime': 9.8826, 'eval_samples_per_second': 101.188, 'eval_steps_per_second': 6.375, 'epoch': 0.08}
{'loss': 1.2227, 'grad_norm': 0.774754524230957, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0616623163223267, 'eval_runtime': 9.9546, 'eval_samples_per_second': 100.457, 'eval_steps_per_second': 6.329, 'epoch': 0.12}
{'loss': 1.0581, 'grad_norm': 0.5030289888381958, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0058907270431519, 'eval_runtime': 9.9602, 'eval_samples_per_second': 100.399, 'eval_steps_per_second': 6.325, 'epoch': 0.16}
{'loss': 0.985, 'grad_norm': 0.3173065483570099, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9469532370567322, 'eval_runtime': 9.9628, 'eval_samples_per_second': 100.374, 'eval_steps_per_second': 6.324, 'epoch': 0.2}
{'loss': 0.9438, 'grad_norm': 0.29994329810142517, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9264763593673706, 'eval_runtime': 9.9676, 'eval_samples_per_second': 100.325, 'eval_steps_per_second': 6.32, 'epoch': 0.24}
{'loss': 0.9202, 'grad_norm': 0.40814751386642456, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.895837664604187, 'eval_runtime': 9.9666, 'eval_samples_per_second': 100.335, 'eval_steps_per_second': 6.321, 'epoch': 0.28}
{'loss': 0.8928, 'grad_norm': 0.3265590965747833, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8841989040374756, 'eval_runtime': 9.9674, 'eval_samples_per_second': 100.328, 'eval_steps_per_second': 6.321, 'epoch': 0.32}
{'loss': 0.8829, 'grad_norm': 0.3171234726905823, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8784391283988953, 'eval_runtime': 9.9603, 'eval_samples_per_second': 100.398, 'eval_steps_per_second': 6.325, 'epoch': 0.36}
{'loss': 0.8575, 'grad_norm': 0.2832079529762268, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8734534382820129, 'eval_runtime': 9.969, 'eval_samples_per_second': 100.311, 'eval_steps_per_second': 6.32, 'epoch': 0.4}
{'loss': 0.8593, 'grad_norm': 0.37733596563339233, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8725767731666565, 'eval_runtime': 9.9707, 'eval_samples_per_second': 100.294, 'eval_steps_per_second': 6.319, 'epoch': 0.44}
{'loss': 0.8765, 'grad_norm': 0.3106731176376343, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8699977993965149, 'eval_runtime': 9.9577, 'eval_samples_per_second': 100.425, 'eval_steps_per_second': 6.327, 'epoch': 0.48}
{'loss': 0.8501, 'grad_norm': 0.297977477312088, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8655989766120911, 'eval_runtime': 9.9743, 'eval_samples_per_second': 100.258, 'eval_steps_per_second': 6.316, 'epoch': 0.52}
{'loss': 0.8411, 'grad_norm': 0.2761746048927307, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.866206169128418, 'eval_runtime': 9.9652, 'eval_samples_per_second': 100.349, 'eval_steps_per_second': 6.322, 'epoch': 0.56}
{'loss': 0.8248, 'grad_norm': 0.38834473490715027, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8648462295532227, 'eval_runtime': 9.9682, 'eval_samples_per_second': 100.319, 'eval_steps_per_second': 6.32, 'epoch': 0.6}
{'loss': 0.8387, 'grad_norm': 0.3762415945529938, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8605178594589233, 'eval_runtime': 9.9912, 'eval_samples_per_second': 100.088, 'eval_steps_per_second': 6.306, 'epoch': 0.64}
{'loss': 0.8291, 'grad_norm': 0.3419022560119629, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8594339489936829, 'eval_runtime': 9.9826, 'eval_samples_per_second': 100.175, 'eval_steps_per_second': 6.311, 'epoch': 0.68}
{'loss': 0.836, 'grad_norm': 0.3130713403224945, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8583545684814453, 'eval_runtime': 9.9649, 'eval_samples_per_second': 100.352, 'eval_steps_per_second': 6.322, 'epoch': 0.72}
{'loss': 0.8231, 'grad_norm': 0.32436105608940125, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8575947284698486, 'eval_runtime': 9.9698, 'eval_samples_per_second': 100.303, 'eval_steps_per_second': 6.319, 'epoch': 0.76}
{'loss': 0.8185, 'grad_norm': 0.30566856265068054, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8560166954994202, 'eval_runtime': 9.9666, 'eval_samples_per_second': 100.335, 'eval_steps_per_second': 6.321, 'epoch': 0.8}
{'loss': 0.8357, 'grad_norm': 0.38245677947998047, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8551929593086243, 'eval_runtime': 9.9631, 'eval_samples_per_second': 100.37, 'eval_steps_per_second': 6.323, 'epoch': 0.84}
{'loss': 0.8276, 'grad_norm': 0.3463493883609772, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8543392419815063, 'eval_runtime': 9.9657, 'eval_samples_per_second': 100.344, 'eval_steps_per_second': 6.322, 'epoch': 0.88}
{'loss': 0.819, 'grad_norm': 0.34304192662239075, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8531292080879211, 'eval_runtime': 9.9663, 'eval_samples_per_second': 100.338, 'eval_steps_per_second': 6.321, 'epoch': 0.92}
{'loss': 0.832, 'grad_norm': 0.3174978792667389, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8529404401779175, 'eval_runtime': 9.965, 'eval_samples_per_second': 100.351, 'eval_steps_per_second': 6.322, 'epoch': 0.96}
{'loss': 0.8035, 'grad_norm': 0.38335007429122925, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8526705503463745, 'eval_runtime': 9.9615, 'eval_samples_per_second': 100.387, 'eval_steps_per_second': 6.324, 'epoch': 1.0}
{'train_runtime': 468.9263, 'train_samples_per_second': 21.321, 'train_steps_per_second': 1.333, 'train_loss': 1.003023696899414, 'epoch': 1.0}
train_results:  {'eval_loss': [2.030503988265991, 1.3658008575439453, 1.0616623163223267, 1.0058907270431519, 0.9469532370567322, 0.9264763593673706, 0.895837664604187, 0.8841989040374756, 0.8784391283988953, 0.8734534382820129, 0.8725767731666565, 0.8699977993965149, 0.8655989766120911, 0.866206169128418, 0.8648462295532227, 0.8605178594589233, 0.8594339489936829, 0.8583545684814453, 0.8575947284698486, 0.8560166954994202, 0.8551929593086243, 0.8543392419815063, 0.8531292080879211, 0.8529404401779175, 0.8526705503463745], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.030503988265991, 1.3658008575439453, 1.0616623163223267, 1.0058907270431519, 0.9469532370567322, 0.9264763593673706, 0.895837664604187, 0.8841989040374756, 0.8784391283988953, 0.8734534382820129, 0.8725767731666565, 0.8699977993965149, 0.8655989766120911, 0.866206169128418, 0.8648462295532227, 0.8605178594589233, 0.8594339489936829, 0.8583545684814453, 0.8575947284698486, 0.8560166954994202, 0.8551929593086243, 0.8543392419815063, 0.8531292080879211, 0.8529404401779175, 0.8526705503463745]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.102095127105713
current iteration best possible eval_loss (full train run):  -0.8526705503463745
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.1457 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.9976866188200907
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.9976866545525764
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.9976866188433205
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.9976866186065522
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.9976866190315506
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2334, dtype=torch.float64), tensor(0.1003, dtype=torch.float64), 0, tensor(0.4804, dtype=torch.float64), 0, tensor(0.0519, dtype=torch.float64), 0, 0, tensor(0.1340, dtype=torch.float64), 1, 0, 0, 0, 1, 1, 60, 0.1, 1.480000019073496, 1]
normalized proposed parameters for next round by BO: [tensor(0.2334, dtype=torch.float64), tensor(0.1003, dtype=torch.float64), tensor(1.4503e-16, dtype=torch.float64), tensor(0.4804, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0519, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1340, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4678, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.233
  gsm8k: 0.1
  rowan_hellaswag: 0
  sciq: 0.48
  triviaqa: 0
  truthfulqa_gen: 0.052
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.134

LoRA Parameters:
  lora_r: (60,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (1.480000019073496,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  60
lora dropout:  0.1
lora alpha:  1.480000019073496
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 2,211,840 || all params: 8,032,473,088 || trainable%: 0.0275
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.5292, 'grad_norm': 0.2542514204978943, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.545711040496826, 'eval_runtime': 8.9086, 'eval_samples_per_second': 112.251, 'eval_steps_per_second': 7.072, 'epoch': 0.04}
{'loss': 3.9352, 'grad_norm': 0.32452765107154846, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.077462911605835, 'eval_runtime': 8.8995, 'eval_samples_per_second': 112.366, 'eval_steps_per_second': 7.079, 'epoch': 0.08}
{'loss': 2.9749, 'grad_norm': 0.24086496233940125, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7813903093338013, 'eval_runtime': 8.9485, 'eval_samples_per_second': 111.751, 'eval_steps_per_second': 7.04, 'epoch': 0.12}
{'loss': 2.3063, 'grad_norm': 0.31448060274124146, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6178969144821167, 'eval_runtime': 8.9952, 'eval_samples_per_second': 111.17, 'eval_steps_per_second': 7.004, 'epoch': 0.16}
{'loss': 1.8434, 'grad_norm': 0.5476018190383911, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.513110876083374, 'eval_runtime': 9.0052, 'eval_samples_per_second': 111.047, 'eval_steps_per_second': 6.996, 'epoch': 0.2}
{'loss': 1.6296, 'grad_norm': 0.917514443397522, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.458359956741333, 'eval_runtime': 9.0053, 'eval_samples_per_second': 111.046, 'eval_steps_per_second': 6.996, 'epoch': 0.24}
{'loss': 1.5606, 'grad_norm': 1.4523102045059204, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4598075151443481, 'eval_runtime': 8.9894, 'eval_samples_per_second': 111.242, 'eval_steps_per_second': 7.008, 'epoch': 0.28}
{'loss': 1.5007, 'grad_norm': 0.903144896030426, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4241464138031006, 'eval_runtime': 9.0168, 'eval_samples_per_second': 110.904, 'eval_steps_per_second': 6.987, 'epoch': 0.32}
{'loss': 1.5006, 'grad_norm': 0.9726330637931824, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4271899461746216, 'eval_runtime': 9.0015, 'eval_samples_per_second': 111.093, 'eval_steps_per_second': 6.999, 'epoch': 0.36}
{'loss': 1.4805, 'grad_norm': 1.0168015956878662, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3877397775650024, 'eval_runtime': 9.0132, 'eval_samples_per_second': 110.948, 'eval_steps_per_second': 6.99, 'epoch': 0.4}
{'loss': 1.4545, 'grad_norm': 0.7961354851722717, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3669826984405518, 'eval_runtime': 9.0032, 'eval_samples_per_second': 111.071, 'eval_steps_per_second': 6.997, 'epoch': 0.44}
{'loss': 1.449, 'grad_norm': 0.6652625799179077, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3494857549667358, 'eval_runtime': 9.0172, 'eval_samples_per_second': 110.9, 'eval_steps_per_second': 6.987, 'epoch': 0.48}
{'loss': 1.4366, 'grad_norm': 0.3861530125141144, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3395836353302002, 'eval_runtime': 9.0348, 'eval_samples_per_second': 110.683, 'eval_steps_per_second': 6.973, 'epoch': 0.52}
{'loss': 1.4394, 'grad_norm': 0.7838324904441833, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3336375951766968, 'eval_runtime': 9.0235, 'eval_samples_per_second': 110.821, 'eval_steps_per_second': 6.982, 'epoch': 0.56}
{'loss': 1.4387, 'grad_norm': 0.6951334476470947, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3137938976287842, 'eval_runtime': 9.0142, 'eval_samples_per_second': 110.937, 'eval_steps_per_second': 6.989, 'epoch': 0.6}
{'loss': 1.4028, 'grad_norm': 0.7611044049263, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3073911666870117, 'eval_runtime': 9.022, 'eval_samples_per_second': 110.84, 'eval_steps_per_second': 6.983, 'epoch': 0.64}
{'loss': 1.4007, 'grad_norm': 0.6030601859092712, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3016448020935059, 'eval_runtime': 9.0131, 'eval_samples_per_second': 110.95, 'eval_steps_per_second': 6.99, 'epoch': 0.68}
{'loss': 1.4029, 'grad_norm': 0.7038454413414001, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3073334693908691, 'eval_runtime': 9.0207, 'eval_samples_per_second': 110.856, 'eval_steps_per_second': 6.984, 'epoch': 0.72}
{'loss': 1.4143, 'grad_norm': 0.48725467920303345, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3039106130599976, 'eval_runtime': 9.0184, 'eval_samples_per_second': 110.885, 'eval_steps_per_second': 6.986, 'epoch': 0.76}
{'loss': 1.4108, 'grad_norm': 0.520927369594574, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2937402725219727, 'eval_runtime': 9.0464, 'eval_samples_per_second': 110.541, 'eval_steps_per_second': 6.964, 'epoch': 0.8}
{'loss': 1.3742, 'grad_norm': 0.505141019821167, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2914193868637085, 'eval_runtime': 9.0517, 'eval_samples_per_second': 110.476, 'eval_steps_per_second': 6.96, 'epoch': 0.84}
{'loss': 1.409, 'grad_norm': 0.6115139126777649, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2884118556976318, 'eval_runtime': 9.0322, 'eval_samples_per_second': 110.715, 'eval_steps_per_second': 6.975, 'epoch': 0.88}
{'loss': 1.3921, 'grad_norm': 0.6349015831947327, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.281891942024231, 'eval_runtime': 9.0255, 'eval_samples_per_second': 110.797, 'eval_steps_per_second': 6.98, 'epoch': 0.92}
{'loss': 1.3988, 'grad_norm': 0.4026172459125519, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.283800482749939, 'eval_runtime': 9.0126, 'eval_samples_per_second': 110.956, 'eval_steps_per_second': 6.99, 'epoch': 0.96}
{'loss': 1.3912, 'grad_norm': 0.5559055805206299, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2824517488479614, 'eval_runtime': 8.9997, 'eval_samples_per_second': 111.115, 'eval_steps_per_second': 7.0, 'epoch': 1.0}
{'train_runtime': 379.6025, 'train_samples_per_second': 26.338, 'train_steps_per_second': 1.646, 'train_loss': 1.7790405029296874, 'epoch': 1.0}
train_results:  {'eval_loss': [2.545711040496826, 2.077462911605835, 1.7813903093338013, 1.6178969144821167, 1.513110876083374, 1.458359956741333, 1.4598075151443481, 1.4241464138031006, 1.4271899461746216, 1.3877397775650024, 1.3669826984405518, 1.3494857549667358, 1.3395836353302002, 1.3336375951766968, 1.3137938976287842, 1.3073911666870117, 1.3016448020935059, 1.3073334693908691, 1.3039106130599976, 1.2937402725219727, 1.2914193868637085, 1.2884118556976318, 1.281891942024231, 1.283800482749939, 1.2824517488479614], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.545711040496826, 2.077462911605835, 1.7813903093338013, 1.6178969144821167, 1.513110876083374, 1.458359956741333, 1.4598075151443481, 1.4241464138031006, 1.4271899461746216, 1.3877397775650024, 1.3669826984405518, 1.3494857549667358, 1.3395836353302002, 1.3336375951766968, 1.3137938976287842, 1.3073911666870117, 1.3016448020935059, 1.3073334693908691, 1.3039106130599976, 1.2937402725219727, 1.2914193868637085, 1.2884118556976318, 1.281891942024231, 1.283800482749939, 1.2824517488479614]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.377115249633789
current iteration best possible eval_loss (full train run):  -1.2824517488479614
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2591 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -0.841877129962756
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -0.8418772300638073
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -0.8418771424774415
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -0.8418770752046587
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -0.8418771305925153
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0928, dtype=torch.float64), tensor(0.2769, dtype=torch.float64), 0, 0, tensor(0.0312, dtype=torch.float64), 0, 0, tensor(0.5018, dtype=torch.float64), tensor(0.0973, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 36, 0.05228788354327571, 22.52410901731025, 1]
normalized proposed parameters for next round by BO: [tensor(0.0928, dtype=torch.float64), tensor(0.2769, dtype=torch.float64), tensor(1.3985e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0312, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0515e-17, dtype=torch.float64), tensor(0.5018, dtype=torch.float64), tensor(0.0973, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2798, dtype=torch.float64), tensor(0.5229, dtype=torch.float64), tensor(0.4693, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.093
  gsm8k: 0.277
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.031
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.502
  arc_challenge: 0.097

LoRA Parameters:
  lora_r: (36,)
  lora_dropout: (0.05228788354327571,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (22.52410901731025,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  36
lora dropout:  0.05228788354327571
lora alpha:  22.52410901731025
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 48,365,568 || all params: 8,078,626,816 || trainable%: 0.5987
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4822, 'grad_norm': 0.6504096984863281, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2813529968261719, 'eval_runtime': 10.4637, 'eval_samples_per_second': 95.569, 'eval_steps_per_second': 6.021, 'epoch': 0.04}
{'loss': 1.3175, 'grad_norm': 0.6116767525672913, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9508591890335083, 'eval_runtime': 10.4819, 'eval_samples_per_second': 95.403, 'eval_steps_per_second': 6.01, 'epoch': 0.08}
{'loss': 1.0939, 'grad_norm': 0.4449652135372162, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9073160290718079, 'eval_runtime': 10.5359, 'eval_samples_per_second': 94.913, 'eval_steps_per_second': 5.98, 'epoch': 0.12}
{'loss': 1.0843, 'grad_norm': 0.26879265904426575, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8904710412025452, 'eval_runtime': 10.569, 'eval_samples_per_second': 94.617, 'eval_steps_per_second': 5.961, 'epoch': 0.16}
{'loss': 1.0639, 'grad_norm': 0.25260400772094727, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8848634958267212, 'eval_runtime': 10.5695, 'eval_samples_per_second': 94.612, 'eval_steps_per_second': 5.961, 'epoch': 0.2}
{'loss': 1.0768, 'grad_norm': 0.3100734055042267, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8788803815841675, 'eval_runtime': 10.5768, 'eval_samples_per_second': 94.547, 'eval_steps_per_second': 5.956, 'epoch': 0.24}
{'loss': 1.0633, 'grad_norm': 0.2351987361907959, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8738092184066772, 'eval_runtime': 10.5732, 'eval_samples_per_second': 94.579, 'eval_steps_per_second': 5.958, 'epoch': 0.28}
{'loss': 1.0725, 'grad_norm': 0.23975597321987152, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8688679337501526, 'eval_runtime': 10.5722, 'eval_samples_per_second': 94.587, 'eval_steps_per_second': 5.959, 'epoch': 0.32}
{'loss': 1.0392, 'grad_norm': 0.24257227778434753, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8676548600196838, 'eval_runtime': 10.5708, 'eval_samples_per_second': 94.6, 'eval_steps_per_second': 5.96, 'epoch': 0.36}
{'loss': 1.0051, 'grad_norm': 0.3998636305332184, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8661245107650757, 'eval_runtime': 10.5661, 'eval_samples_per_second': 94.643, 'eval_steps_per_second': 5.962, 'epoch': 0.4}
{'loss': 0.9985, 'grad_norm': 0.2965821325778961, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8670309782028198, 'eval_runtime': 10.5658, 'eval_samples_per_second': 94.645, 'eval_steps_per_second': 5.963, 'epoch': 0.44}
{'loss': 1.0424, 'grad_norm': 0.23348042368888855, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8603785634040833, 'eval_runtime': 10.5701, 'eval_samples_per_second': 94.606, 'eval_steps_per_second': 5.96, 'epoch': 0.48}
{'loss': 1.0178, 'grad_norm': 0.2612488865852356, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8581884503364563, 'eval_runtime': 10.5801, 'eval_samples_per_second': 94.517, 'eval_steps_per_second': 5.955, 'epoch': 0.52}
{'loss': 1.0457, 'grad_norm': 0.2839215397834778, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8587610721588135, 'eval_runtime': 10.5676, 'eval_samples_per_second': 94.629, 'eval_steps_per_second': 5.962, 'epoch': 0.56}
{'loss': 1.0308, 'grad_norm': 0.24560943245887756, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8534409403800964, 'eval_runtime': 10.5647, 'eval_samples_per_second': 94.655, 'eval_steps_per_second': 5.963, 'epoch': 0.6}
{'loss': 0.9937, 'grad_norm': 0.26462072134017944, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8517470359802246, 'eval_runtime': 10.5773, 'eval_samples_per_second': 94.542, 'eval_steps_per_second': 5.956, 'epoch': 0.64}
{'loss': 1.0008, 'grad_norm': 0.2598874270915985, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8542231917381287, 'eval_runtime': 10.5736, 'eval_samples_per_second': 94.575, 'eval_steps_per_second': 5.958, 'epoch': 0.68}
{'loss': 1.0061, 'grad_norm': 0.24367153644561768, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8492656946182251, 'eval_runtime': 10.5657, 'eval_samples_per_second': 94.646, 'eval_steps_per_second': 5.963, 'epoch': 0.72}
{'loss': 0.9909, 'grad_norm': 0.2645251750946045, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8471823930740356, 'eval_runtime': 10.5808, 'eval_samples_per_second': 94.511, 'eval_steps_per_second': 5.954, 'epoch': 0.76}
{'loss': 1.0281, 'grad_norm': 0.26989036798477173, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8459824323654175, 'eval_runtime': 10.6148, 'eval_samples_per_second': 94.208, 'eval_steps_per_second': 5.935, 'epoch': 0.8}
{'loss': 0.9906, 'grad_norm': 0.27129584550857544, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8462663292884827, 'eval_runtime': 10.6169, 'eval_samples_per_second': 94.19, 'eval_steps_per_second': 5.934, 'epoch': 0.84}
{'loss': 0.9783, 'grad_norm': 0.24793948233127594, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8451234102249146, 'eval_runtime': 10.6492, 'eval_samples_per_second': 93.904, 'eval_steps_per_second': 5.916, 'epoch': 0.88}
{'loss': 0.9281, 'grad_norm': 0.26316651701927185, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.843893826007843, 'eval_runtime': 10.5832, 'eval_samples_per_second': 94.489, 'eval_steps_per_second': 5.953, 'epoch': 0.92}
{'loss': 0.9685, 'grad_norm': 0.23602455854415894, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8436512351036072, 'eval_runtime': 10.599, 'eval_samples_per_second': 94.348, 'eval_steps_per_second': 5.944, 'epoch': 0.96}
{'loss': 0.9974, 'grad_norm': 0.2487296611070633, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8429969549179077, 'eval_runtime': 10.5864, 'eval_samples_per_second': 94.46, 'eval_steps_per_second': 5.951, 'epoch': 1.0}
{'train_runtime': 506.3652, 'train_samples_per_second': 19.743, 'train_steps_per_second': 1.234, 'train_loss': 1.092650686645508, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2813529968261719, 0.9508591890335083, 0.9073160290718079, 0.8904710412025452, 0.8848634958267212, 0.8788803815841675, 0.8738092184066772, 0.8688679337501526, 0.8676548600196838, 0.8661245107650757, 0.8670309782028198, 0.8603785634040833, 0.8581884503364563, 0.8587610721588135, 0.8534409403800964, 0.8517470359802246, 0.8542231917381287, 0.8492656946182251, 0.8471823930740356, 0.8459824323654175, 0.8462663292884827, 0.8451234102249146, 0.843893826007843, 0.8436512351036072, 0.8429969549179077], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2813529968261719, 0.9508591890335083, 0.9073160290718079, 0.8904710412025452, 0.8848634958267212, 0.8788803815841675, 0.8738092184066772, 0.8688679337501526, 0.8676548600196838, 0.8661245107650757, 0.8670309782028198, 0.8603785634040833, 0.8581884503364563, 0.8587610721588135, 0.8534409403800964, 0.8517470359802246, 0.8542231917381287, 0.8492656946182251, 0.8471823930740356, 0.8459824323654175, 0.8462663292884827, 0.8451234102249146, 0.843893826007843, 0.8436512351036072, 0.8429969549179077]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0588634014129639
current iteration best possible eval_loss (full train run):  -0.8429969549179077
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.6034 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -1.007663504394643
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -1.0076635105773861
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -1.0076635051254126
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -1.0076634976701533
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -1.0076634174245775
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2715, dtype=torch.float64), tensor(0.3991, dtype=torch.float64), 0, tensor(0.1475, dtype=torch.float64), tensor(0.0852, dtype=torch.float64), 0, tensor(0.0567, dtype=torch.float64), 0, tensor(0.0401, dtype=torch.float64), 1, 1, 0, 0, 1, 0, 2, 0.1, 46.97565037650543, 0]
normalized proposed parameters for next round by BO: [tensor(0.2715, dtype=torch.float64), tensor(0.3991, dtype=torch.float64), tensor(2.0566e-17, dtype=torch.float64), tensor(0.1475, dtype=torch.float64), tensor(0.0852, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0567, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0401, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9787, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.271
  gsm8k: 0.399
  rowan_hellaswag: 0
  sciq: 0.148
  triviaqa: 0.085
  truthfulqa_gen: 0
  wikitext: 0.057
  mmlu: 0
  arc_challenge: 0.04

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 0, 1, 0],)
  lora_alpha: (46.97565037650543,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  46.97565037650543
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3708, 'grad_norm': 15.139976501464844, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0471086502075195, 'eval_runtime': 8.8496, 'eval_samples_per_second': 113.0, 'eval_steps_per_second': 7.119, 'epoch': 0.04}
{'loss': 2.2247, 'grad_norm': 3.5759403705596924, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3843849897384644, 'eval_runtime': 8.9479, 'eval_samples_per_second': 111.758, 'eval_steps_per_second': 7.041, 'epoch': 0.08}
{'loss': 1.4457, 'grad_norm': 1.6310346126556396, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1614819765090942, 'eval_runtime': 8.9105, 'eval_samples_per_second': 112.227, 'eval_steps_per_second': 7.07, 'epoch': 0.12}
{'loss': 1.2673, 'grad_norm': 1.8609589338302612, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0774569511413574, 'eval_runtime': 8.9607, 'eval_samples_per_second': 111.598, 'eval_steps_per_second': 7.031, 'epoch': 0.16}
{'loss': 1.2158, 'grad_norm': 2.0040664672851562, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0652939081192017, 'eval_runtime': 8.9828, 'eval_samples_per_second': 111.323, 'eval_steps_per_second': 7.013, 'epoch': 0.2}
{'loss': 1.1524, 'grad_norm': 1.6489174365997314, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.048717737197876, 'eval_runtime': 9.0055, 'eval_samples_per_second': 111.043, 'eval_steps_per_second': 6.996, 'epoch': 0.24}
{'loss': 1.147, 'grad_norm': 2.2322962284088135, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0406973361968994, 'eval_runtime': 9.0516, 'eval_samples_per_second': 110.478, 'eval_steps_per_second': 6.96, 'epoch': 0.28}
{'loss': 1.1354, 'grad_norm': 2.093258857727051, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0355916023254395, 'eval_runtime': 9.0566, 'eval_samples_per_second': 110.417, 'eval_steps_per_second': 6.956, 'epoch': 0.32}
{'loss': 1.1028, 'grad_norm': 1.6267915964126587, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0254117250442505, 'eval_runtime': 9.0652, 'eval_samples_per_second': 110.312, 'eval_steps_per_second': 6.95, 'epoch': 0.36}
{'loss': 1.1283, 'grad_norm': 1.107155442237854, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.024088978767395, 'eval_runtime': 9.0449, 'eval_samples_per_second': 110.56, 'eval_steps_per_second': 6.965, 'epoch': 0.4}
{'loss': 1.1505, 'grad_norm': 1.3371706008911133, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0196847915649414, 'eval_runtime': 9.0549, 'eval_samples_per_second': 110.438, 'eval_steps_per_second': 6.958, 'epoch': 0.44}
{'loss': 1.127, 'grad_norm': 1.3951796293258667, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0167309045791626, 'eval_runtime': 9.039, 'eval_samples_per_second': 110.632, 'eval_steps_per_second': 6.97, 'epoch': 0.48}
{'loss': 1.1239, 'grad_norm': 1.442270278930664, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0175681114196777, 'eval_runtime': 9.0291, 'eval_samples_per_second': 110.753, 'eval_steps_per_second': 6.977, 'epoch': 0.52}
{'loss': 1.0914, 'grad_norm': 1.2387843132019043, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0119138956069946, 'eval_runtime': 9.0254, 'eval_samples_per_second': 110.798, 'eval_steps_per_second': 6.98, 'epoch': 0.56}
{'loss': 1.0879, 'grad_norm': 1.5778281688690186, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0173026323318481, 'eval_runtime': 9.0167, 'eval_samples_per_second': 110.905, 'eval_steps_per_second': 6.987, 'epoch': 0.6}
{'loss': 1.1112, 'grad_norm': 1.2999818325042725, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0144963264465332, 'eval_runtime': 9.0391, 'eval_samples_per_second': 110.63, 'eval_steps_per_second': 6.97, 'epoch': 0.64}
{'loss': 1.1405, 'grad_norm': 1.3978313207626343, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0090360641479492, 'eval_runtime': 9.0174, 'eval_samples_per_second': 110.897, 'eval_steps_per_second': 6.987, 'epoch': 0.68}
{'loss': 1.0799, 'grad_norm': 1.594887375831604, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0115898847579956, 'eval_runtime': 9.0427, 'eval_samples_per_second': 110.587, 'eval_steps_per_second': 6.967, 'epoch': 0.72}
{'loss': 1.1332, 'grad_norm': 2.378293752670288, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0105454921722412, 'eval_runtime': 9.0279, 'eval_samples_per_second': 110.768, 'eval_steps_per_second': 6.978, 'epoch': 0.76}
{'loss': 1.1217, 'grad_norm': 1.1703944206237793, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0083478689193726, 'eval_runtime': 9.0174, 'eval_samples_per_second': 110.896, 'eval_steps_per_second': 6.986, 'epoch': 0.8}
{'loss': 1.0929, 'grad_norm': 1.156908392906189, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0058826208114624, 'eval_runtime': 9.0615, 'eval_samples_per_second': 110.357, 'eval_steps_per_second': 6.952, 'epoch': 0.84}
{'loss': 1.0691, 'grad_norm': 1.4523338079452515, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.008506178855896, 'eval_runtime': 9.1031, 'eval_samples_per_second': 109.852, 'eval_steps_per_second': 6.921, 'epoch': 0.88}
{'loss': 1.0846, 'grad_norm': 1.6715947389602661, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.005747675895691, 'eval_runtime': 9.1047, 'eval_samples_per_second': 109.834, 'eval_steps_per_second': 6.92, 'epoch': 0.92}
{'loss': 1.083, 'grad_norm': 1.4522202014923096, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0061174631118774, 'eval_runtime': 9.0647, 'eval_samples_per_second': 110.317, 'eval_steps_per_second': 6.95, 'epoch': 0.96}
{'loss': 1.0747, 'grad_norm': 1.4467408657073975, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0062391757965088, 'eval_runtime': 9.039, 'eval_samples_per_second': 110.631, 'eval_steps_per_second': 6.97, 'epoch': 1.0}
{'train_runtime': 321.4113, 'train_samples_per_second': 31.1, 'train_steps_per_second': 1.945, 'train_loss': 1.2704719512939453, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0471086502075195, 1.3843849897384644, 1.1614819765090942, 1.0774569511413574, 1.0652939081192017, 1.048717737197876, 1.0406973361968994, 1.0355916023254395, 1.0254117250442505, 1.024088978767395, 1.0196847915649414, 1.0167309045791626, 1.0175681114196777, 1.0119138956069946, 1.0173026323318481, 1.0144963264465332, 1.0090360641479492, 1.0115898847579956, 1.0105454921722412, 1.0083478689193726, 1.0058826208114624, 1.008506178855896, 1.005747675895691, 1.0061174631118774, 1.0062391757965088], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.0471086502075195, 1.3843849897384644, 1.1614819765090942, 1.0774569511413574, 1.0652939081192017, 1.048717737197876, 1.0406973361968994, 1.0355916023254395, 1.0254117250442505, 1.024088978767395, 1.0196847915649414, 1.0167309045791626, 1.0175681114196777, 1.0119138956069946, 1.0173026323318481, 1.0144963264465332, 1.0090360641479492, 1.0115898847579956, 1.0105454921722412, 1.0083478689193726, 1.0058826208114624, 1.008506178855896, 1.005747675895691, 1.0061174631118774, 1.0062391757965088]
current iteration observed (possibly low-fid or predicted) eval_loss:  -2.273756504058838
current iteration best possible eval_loss (full train run):  -1.0062391757965088
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.5698 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.8827270313057622
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.8827726721095721
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.8489660232508568
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.8826788904280233
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.8814033888128435
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2789, dtype=torch.float64), tensor(0.2297, dtype=torch.float64), 0, tensor(0.3680, dtype=torch.float64), tensor(0.0134, dtype=torch.float64), 0, 0, 0, tensor(0.1018, dtype=torch.float64), 20, 0, 1, 0, 1, 1, 12, 0.0165460109759191, 18.68651224380943, 0]
normalized proposed parameters for next round by BO: [tensor(0.2789, dtype=torch.float64), tensor(0.2297, dtype=torch.float64), tensor(0.0081, dtype=torch.float64), tensor(0.3680, dtype=torch.float64), tensor(0.0134, dtype=torch.float64), tensor(2.4282e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.3631e-18, dtype=torch.float64), tensor(0.1018, dtype=torch.float64), tensor(0.6303, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0939, dtype=torch.float64), tensor(0.1655, dtype=torch.float64), tensor(0.3893, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.279
  gsm8k: 0.23
  rowan_hellaswag: 0
  sciq: 0.368
  triviaqa: 0.013
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.102

LoRA Parameters:
  lora_r: (12,)
  lora_dropout: (0.0165460109759191,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (18.68651224380943,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  12
lora dropout:  0.0165460109759191
lora alpha:  18.68651224380943
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 10,076,160 || all params: 8,040,337,408 || trainable%: 0.1253
length of training data:  9917
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.233, 'grad_norm': 1.8469953536987305, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.431579351425171, 'eval_runtime': 9.8304, 'eval_samples_per_second': 101.725, 'eval_steps_per_second': 6.409, 'epoch': 0.04}
{'loss': 1.2514, 'grad_norm': 0.827881932258606, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9871262311935425, 'eval_runtime': 9.8755, 'eval_samples_per_second': 101.261, 'eval_steps_per_second': 6.379, 'epoch': 0.08}
{'loss': 0.9228, 'grad_norm': 0.43284162878990173, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 0.932171642780304, 'eval_runtime': 9.9089, 'eval_samples_per_second': 100.919, 'eval_steps_per_second': 6.358, 'epoch': 0.12}
{'loss': 0.91, 'grad_norm': 0.39924320578575134, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.915601372718811, 'eval_runtime': 9.9762, 'eval_samples_per_second': 100.238, 'eval_steps_per_second': 6.315, 'epoch': 0.16}
{'loss': 0.8867, 'grad_norm': 0.41572582721710205, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.9126456379890442, 'eval_runtime': 9.9675, 'eval_samples_per_second': 100.326, 'eval_steps_per_second': 6.321, 'epoch': 0.2}
{'loss': 0.8831, 'grad_norm': 0.3852362036705017, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.9046664237976074, 'eval_runtime': 9.9638, 'eval_samples_per_second': 100.363, 'eval_steps_per_second': 6.323, 'epoch': 0.24}
{'loss': 0.8759, 'grad_norm': 0.38692304491996765, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.8942837119102478, 'eval_runtime': 9.9617, 'eval_samples_per_second': 100.385, 'eval_steps_per_second': 6.324, 'epoch': 0.28}
{'loss': 0.8732, 'grad_norm': 0.4731910824775696, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.890541136264801, 'eval_runtime': 9.9667, 'eval_samples_per_second': 100.334, 'eval_steps_per_second': 6.321, 'epoch': 0.32}
{'loss': 0.8778, 'grad_norm': 0.31967148184776306, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.8893515467643738, 'eval_runtime': 9.9615, 'eval_samples_per_second': 100.386, 'eval_steps_per_second': 6.324, 'epoch': 0.36}
{'loss': 0.852, 'grad_norm': 0.36804264783859253, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.8862470984458923, 'eval_runtime': 9.9529, 'eval_samples_per_second': 100.474, 'eval_steps_per_second': 6.33, 'epoch': 0.4}
{'loss': 0.8571, 'grad_norm': 0.44120320677757263, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.881367564201355, 'eval_runtime': 9.9602, 'eval_samples_per_second': 100.399, 'eval_steps_per_second': 6.325, 'epoch': 0.44}
{'loss': 0.8544, 'grad_norm': 0.3964424729347229, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.8792431354522705, 'eval_runtime': 9.9606, 'eval_samples_per_second': 100.396, 'eval_steps_per_second': 6.325, 'epoch': 0.48}
{'loss': 0.8402, 'grad_norm': 0.36059150099754333, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.8787765502929688, 'eval_runtime': 9.9546, 'eval_samples_per_second': 100.456, 'eval_steps_per_second': 6.329, 'epoch': 0.52}
{'loss': 0.8355, 'grad_norm': 0.40386998653411865, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.8753630518913269, 'eval_runtime': 9.9556, 'eval_samples_per_second': 100.446, 'eval_steps_per_second': 6.328, 'epoch': 0.56}
{'loss': 0.822, 'grad_norm': 0.3217267096042633, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.8750976324081421, 'eval_runtime': 9.9718, 'eval_samples_per_second': 100.283, 'eval_steps_per_second': 6.318, 'epoch': 0.6}
{'loss': 0.8295, 'grad_norm': 0.37193894386291504, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.876001238822937, 'eval_runtime': 9.9534, 'eval_samples_per_second': 100.468, 'eval_steps_per_second': 6.329, 'epoch': 0.65}
{'loss': 0.8347, 'grad_norm': 0.35397207736968994, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.8717354536056519, 'eval_runtime': 9.9504, 'eval_samples_per_second': 100.498, 'eval_steps_per_second': 6.331, 'epoch': 0.69}
{'loss': 0.8365, 'grad_norm': 0.4237106144428253, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.8723256587982178, 'eval_runtime': 9.9666, 'eval_samples_per_second': 100.335, 'eval_steps_per_second': 6.321, 'epoch': 0.73}
{'loss': 0.843, 'grad_norm': 0.3729858696460724, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.8684859871864319, 'eval_runtime': 9.9671, 'eval_samples_per_second': 100.331, 'eval_steps_per_second': 6.321, 'epoch': 0.77}
{'loss': 0.826, 'grad_norm': 0.36395779252052307, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.8690581917762756, 'eval_runtime': 9.9692, 'eval_samples_per_second': 100.309, 'eval_steps_per_second': 6.319, 'epoch': 0.81}
{'loss': 0.8431, 'grad_norm': 0.4015444815158844, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.8676174879074097, 'eval_runtime': 9.9623, 'eval_samples_per_second': 100.378, 'eval_steps_per_second': 6.324, 'epoch': 0.85}
{'loss': 0.8268, 'grad_norm': 0.35882461071014404, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.8664630055427551, 'eval_runtime': 9.9631, 'eval_samples_per_second': 100.37, 'eval_steps_per_second': 6.323, 'epoch': 0.89}
{'loss': 0.8202, 'grad_norm': 0.3568653166294098, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.865684986114502, 'eval_runtime': 9.9565, 'eval_samples_per_second': 100.437, 'eval_steps_per_second': 6.328, 'epoch': 0.93}
{'loss': 0.8004, 'grad_norm': 0.4307810366153717, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.865248441696167, 'eval_runtime': 9.9553, 'eval_samples_per_second': 100.449, 'eval_steps_per_second': 6.328, 'epoch': 0.97}
{'train_runtime': 408.4027, 'train_samples_per_second': 24.282, 'train_steps_per_second': 1.518, 'train_loss': 0.9634510747848019, 'epoch': 1.0}
train_results:  {'eval_loss': [1.431579351425171, 0.9871262311935425, 0.932171642780304, 0.915601372718811, 0.9126456379890442, 0.9046664237976074, 0.8942837119102478, 0.890541136264801, 0.8893515467643738, 0.8862470984458923, 0.881367564201355, 0.8792431354522705, 0.8787765502929688, 0.8753630518913269, 0.8750976324081421, 0.876001238822937, 0.8717354536056519, 0.8723256587982178, 0.8684859871864319, 0.8690581917762756, 0.8676174879074097, 0.8664630055427551, 0.865684986114502, 0.865248441696167], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.431579351425171, 0.9871262311935425, 0.932171642780304, 0.915601372718811, 0.9126456379890442, 0.9046664237976074, 0.8942837119102478, 0.890541136264801, 0.8893515467643738, 0.8862470984458923, 0.881367564201355, 0.8792431354522705, 0.8787765502929688, 0.8753630518913269, 0.8750976324081421, 0.876001238822937, 0.8717354536056519, 0.8723256587982178, 0.8684859871864319, 0.8690581917762756, 0.8676174879074097, 0.8664630055427551, 0.865684986114502, 0.865248441696167]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3657252788543701
current iteration best possible eval_loss (full train run):  -0.865248441696167
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 21.8056 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4017324447631836, 0.8098095655441284, 0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.9023115634918213, 0.3290713429450989, 0.8067486882209778, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.7156268954277039, 0.5243701934814453, 0.35216549038887024, 0.006528973579406738]  ‚Üí  acq = -0.8876216841196866
X = [0.083668053150177, 0.21730327606201172, 0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4629462659358978, 0.49664074182510376, 0.07627946138381958, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.09527727216482162, 0.25031810998916626, 0.8219367265701294, 0.7197057604789734]  ‚Üí  acq = -0.9013838804371197
X = [0.2491104006767273, 0.20842111110687256, 0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.4023188650608063, 0.07552939653396606, 0.27186697721481323, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.9325355887413025, 0.1334356665611267, 0.16606317460536957, 0.5709797143936157]  ‚Üí  acq = -0.8963631466143714
X = [0.6106637716293335, 0.9664523601531982, 0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.13759151101112366, 0.28943711519241333, 0.03323030471801758, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8348419070243835, 0.620255172252655, 0.1275952160358429, 0.9232467412948608]  ‚Üí  acq = -0.9400358988493382
X = [0.332327663898468, 0.6604408025741577, 0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6904752850532532, 0.5186182260513306, 0.2655754089355469, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.19332276284694672, 0.0007928609848022461, 0.8442171812057495, 0.3439427614212036]  ‚Üí  acq = -0.9262018277984405
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0909, dtype=torch.float64), tensor(0.5672, dtype=torch.float64), 0, 0, tensor(0.0249, dtype=torch.float64), tensor(0.2865, dtype=torch.float64), tensor(0.0305, dtype=torch.float64), 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.06894938133868687, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0909, dtype=torch.float64), tensor(0.5672, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0230e-17, dtype=torch.float64), tensor(0.0249, dtype=torch.float64), tensor(0.2865, dtype=torch.float64), tensor(0.0305, dtype=torch.float64), tensor(5.8061e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6895, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.091
  gsm8k: 0.567
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.025
  truthfulqa_gen: 0.286
  wikitext: 0.03
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.06894938133868687,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  0.06894938133868687
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0588, 'grad_norm': 0.3042903542518616, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.1142354011535645, 'eval_runtime': 9.5708, 'eval_samples_per_second': 104.485, 'eval_steps_per_second': 6.583, 'epoch': 0.04}
{'loss': 1.8439, 'grad_norm': 0.178365096449852, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2924375534057617, 'eval_runtime': 9.6457, 'eval_samples_per_second': 103.674, 'eval_steps_per_second': 6.531, 'epoch': 0.08}
{'loss': 1.2422, 'grad_norm': 0.06971416622400284, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.024108648300171, 'eval_runtime': 9.6663, 'eval_samples_per_second': 103.452, 'eval_steps_per_second': 6.517, 'epoch': 0.12}
{'loss': 1.0019, 'grad_norm': 0.06855907291173935, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9474618434906006, 'eval_runtime': 9.6851, 'eval_samples_per_second': 103.251, 'eval_steps_per_second': 6.505, 'epoch': 0.16}
{'loss': 0.9122, 'grad_norm': 0.050604067742824554, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.915636420249939, 'eval_runtime': 9.6982, 'eval_samples_per_second': 103.111, 'eval_steps_per_second': 6.496, 'epoch': 0.2}
{'loss': 0.9263, 'grad_norm': 0.04422992840409279, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9058974981307983, 'eval_runtime': 9.7085, 'eval_samples_per_second': 103.003, 'eval_steps_per_second': 6.489, 'epoch': 0.24}
{'loss': 0.914, 'grad_norm': 0.054936483502388, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9048449993133545, 'eval_runtime': 9.6894, 'eval_samples_per_second': 103.206, 'eval_steps_per_second': 6.502, 'epoch': 0.28}
{'loss': 0.9188, 'grad_norm': 0.05435607209801674, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8950966596603394, 'eval_runtime': 9.7313, 'eval_samples_per_second': 102.761, 'eval_steps_per_second': 6.474, 'epoch': 0.32}
{'loss': 0.9526, 'grad_norm': 0.05401000753045082, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8912007808685303, 'eval_runtime': 9.6651, 'eval_samples_per_second': 103.465, 'eval_steps_per_second': 6.518, 'epoch': 0.36}
{'loss': 0.9201, 'grad_norm': 0.055833492428064346, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8883846402168274, 'eval_runtime': 9.6704, 'eval_samples_per_second': 103.409, 'eval_steps_per_second': 6.515, 'epoch': 0.4}
{'loss': 0.9093, 'grad_norm': 0.054010167717933655, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8848055005073547, 'eval_runtime': 9.6554, 'eval_samples_per_second': 103.569, 'eval_steps_per_second': 6.525, 'epoch': 0.44}
{'loss': 0.8822, 'grad_norm': 0.04530690237879753, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8826981782913208, 'eval_runtime': 9.6649, 'eval_samples_per_second': 103.467, 'eval_steps_per_second': 6.518, 'epoch': 0.48}
{'loss': 0.862, 'grad_norm': 0.04994255304336548, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8810978531837463, 'eval_runtime': 9.6674, 'eval_samples_per_second': 103.441, 'eval_steps_per_second': 6.517, 'epoch': 0.52}
{'loss': 0.9169, 'grad_norm': 0.06027952954173088, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8790093064308167, 'eval_runtime': 9.6562, 'eval_samples_per_second': 103.56, 'eval_steps_per_second': 6.524, 'epoch': 0.56}
{'loss': 0.897, 'grad_norm': 0.05454975366592407, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8765318393707275, 'eval_runtime': 9.6635, 'eval_samples_per_second': 103.482, 'eval_steps_per_second': 6.519, 'epoch': 0.6}
{'loss': 0.8768, 'grad_norm': 0.0484161339700222, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8745182156562805, 'eval_runtime': 9.6542, 'eval_samples_per_second': 103.582, 'eval_steps_per_second': 6.526, 'epoch': 0.64}
{'loss': 0.9038, 'grad_norm': 0.04988894984126091, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8739981055259705, 'eval_runtime': 9.6459, 'eval_samples_per_second': 103.671, 'eval_steps_per_second': 6.531, 'epoch': 0.68}
{'loss': 0.8888, 'grad_norm': 0.06040917709469795, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8721131682395935, 'eval_runtime': 9.6555, 'eval_samples_per_second': 103.568, 'eval_steps_per_second': 6.525, 'epoch': 0.72}
{'loss': 0.8785, 'grad_norm': 0.05365776643157005, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8706441521644592, 'eval_runtime': 9.7153, 'eval_samples_per_second': 102.93, 'eval_steps_per_second': 6.485, 'epoch': 0.76}
{'loss': 0.8637, 'grad_norm': 0.05070473253726959, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8704057335853577, 'eval_runtime': 9.7339, 'eval_samples_per_second': 102.734, 'eval_steps_per_second': 6.472, 'epoch': 0.8}
{'loss': 0.8755, 'grad_norm': 0.04988665506243706, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8706403970718384, 'eval_runtime': 9.7066, 'eval_samples_per_second': 103.023, 'eval_steps_per_second': 6.49, 'epoch': 0.84}
{'loss': 0.8889, 'grad_norm': 0.05811973288655281, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.870781660079956, 'eval_runtime': 9.6734, 'eval_samples_per_second': 103.376, 'eval_steps_per_second': 6.513, 'epoch': 0.88}
{'loss': 0.8886, 'grad_norm': 0.0656663179397583, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8691298365592957, 'eval_runtime': 9.6702, 'eval_samples_per_second': 103.41, 'eval_steps_per_second': 6.515, 'epoch': 0.92}
{'loss': 0.8955, 'grad_norm': 0.052302464842796326, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8685858249664307, 'eval_runtime': 9.6749, 'eval_samples_per_second': 103.361, 'eval_steps_per_second': 6.512, 'epoch': 0.96}
{'loss': 0.8913, 'grad_norm': 0.061279021203517914, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8681933879852295, 'eval_runtime': 9.6767, 'eval_samples_per_second': 103.341, 'eval_steps_per_second': 6.511, 'epoch': 1.0}
{'train_runtime': 455.2396, 'train_samples_per_second': 21.96, 'train_steps_per_second': 1.373, 'train_loss': 1.0403686340332032, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1142354011535645, 1.2924375534057617, 1.024108648300171, 0.9474618434906006, 0.915636420249939, 0.9058974981307983, 0.9048449993133545, 0.8950966596603394, 0.8912007808685303, 0.8883846402168274, 0.8848055005073547, 0.8826981782913208, 0.8810978531837463, 0.8790093064308167, 0.8765318393707275, 0.8745182156562805, 0.8739981055259705, 0.8721131682395935, 0.8706441521644592, 0.8704057335853577, 0.8706403970718384, 0.870781660079956, 0.8691298365592957, 0.8685858249664307, 0.8681933879852295], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.1142354011535645, 1.2924375534057617, 1.024108648300171, 0.9474618434906006, 0.915636420249939, 0.9058974981307983, 0.9048449993133545, 0.8950966596603394, 0.8912007808685303, 0.8883846402168274, 0.8848055005073547, 0.8826981782913208, 0.8810978531837463, 0.8790093064308167, 0.8765318393707275, 0.8745182156562805, 0.8739981055259705, 0.8721131682395935, 0.8706441521644592, 0.8704057335853577, 0.8706403970718384, 0.870781660079956, 0.8691298365592957, 0.8685858249664307, 0.8681933879852295]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.8681933879852295
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.6913 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9865272641181946, 0.754863977432251, 0.9141794443130493, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8765165209770203, 0.16884422302246094, 0.4735041558742523, 0.9369502067565918, 0.09805838763713837, 0.632781982421875]  ‚Üí  acq = -0.9462655529375229
X = [0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7402988076210022, 0.5261915326118469, 0.958617627620697, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.11568903923034668, 0.18786925077438354, 0.9194891452789307, 0.21238505840301514, 0.206920325756073, 0.5671297311782837]  ‚Üí  acq = -0.942762021125642
X = [0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.31678032875061035, 0.8766421675682068, 0.527535080909729, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.29103684425354004, 0.8914388418197632, 0.938625156879425, 0.7902622818946838, 0.49184754490852356, 0.8949254751205444]  ‚Üí  acq = -0.9462655524938458
X = [0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.44876259565353394, 0.27662307024002075, 0.14264680445194244, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.18000507354736328, 0.8357580900192261, 0.8570732474327087, 0.6040682792663574, 0.1705421358346939, 0.1752747893333435]  ‚Üí  acq = -0.9413747699067515
X = [0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.5871034860610962, 0.18114352226257324, 0.5352886319160461, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.4854152798652649, 0.7254683971405029, 0.4659062325954437, 0.9238991737365723, 0.21354550123214722, 0.7559280395507812]  ‚Üí  acq = -0.946265552493846
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1085, dtype=torch.float64), tensor(0.3506, dtype=torch.float64), tensor(0.1132, dtype=torch.float64), tensor(0.0691, dtype=torch.float64), tensor(0.2200, dtype=torch.float64), tensor(0.0376, dtype=torch.float64), tensor(0.0219, dtype=torch.float64), tensor(0.0775, dtype=torch.float64), 0, 11, 0, 1, 1, 1, 1, 128, 0.09480442163818074, 24.797342026662605, 0]
normalized proposed parameters for next round by BO: [tensor(0.1085, dtype=torch.float64), tensor(0.3506, dtype=torch.float64), tensor(0.1132, dtype=torch.float64), tensor(0.0691, dtype=torch.float64), tensor(0.2200, dtype=torch.float64), tensor(0.0376, dtype=torch.float64), tensor(0.0219, dtype=torch.float64), tensor(0.0775, dtype=torch.float64), tensor(0.0018, dtype=torch.float64), tensor(0.3380, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9480, dtype=torch.float64), tensor(0.5166, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.109
  gsm8k: 0.351
  rowan_hellaswag: 0.113
  sciq: 0.069
  triviaqa: 0.22
  truthfulqa_gen: 0.038
  wikitext: 0.022
  mmlu: 0.077
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09480442163818074,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (24.797342026662605,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.09480442163818074
lora alpha:  24.797342026662605
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 85,065,728 || all params: 8,115,326,976 || trainable%: 1.0482
length of training data:  9978
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8829, 'grad_norm': 0.5661194324493408, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4184097051620483, 'eval_runtime': 9.54, 'eval_samples_per_second': 104.822, 'eval_steps_per_second': 6.604, 'epoch': 0.04}
{'loss': 1.643, 'grad_norm': 0.27244120836257935, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0305391550064087, 'eval_runtime': 9.5927, 'eval_samples_per_second': 104.246, 'eval_steps_per_second': 6.567, 'epoch': 0.08}
{'loss': 1.2952, 'grad_norm': 0.15428797900676727, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.9619833827018738, 'eval_runtime': 9.6119, 'eval_samples_per_second': 104.037, 'eval_steps_per_second': 6.554, 'epoch': 0.12}
{'loss': 1.1891, 'grad_norm': 0.1452745497226715, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.9509913325309753, 'eval_runtime': 9.6335, 'eval_samples_per_second': 103.804, 'eval_steps_per_second': 6.54, 'epoch': 0.16}
{'loss': 1.254, 'grad_norm': 0.12287437915802002, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.9368938207626343, 'eval_runtime': 9.6676, 'eval_samples_per_second': 103.438, 'eval_steps_per_second': 6.517, 'epoch': 0.2}
{'loss': 1.2141, 'grad_norm': 0.1338743269443512, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.9316897988319397, 'eval_runtime': 9.7066, 'eval_samples_per_second': 103.022, 'eval_steps_per_second': 6.49, 'epoch': 0.24}
{'loss': 1.2125, 'grad_norm': 0.11680260300636292, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.9268227815628052, 'eval_runtime': 9.7187, 'eval_samples_per_second': 102.895, 'eval_steps_per_second': 6.482, 'epoch': 0.28}
{'loss': 1.173, 'grad_norm': 0.12257350236177444, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.922191858291626, 'eval_runtime': 9.7282, 'eval_samples_per_second': 102.794, 'eval_steps_per_second': 6.476, 'epoch': 0.32}
{'loss': 1.1871, 'grad_norm': 0.13503721356391907, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.9182322025299072, 'eval_runtime': 9.7267, 'eval_samples_per_second': 102.809, 'eval_steps_per_second': 6.477, 'epoch': 0.36}
{'loss': 1.1948, 'grad_norm': 0.11742357164621353, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.9142884612083435, 'eval_runtime': 9.7413, 'eval_samples_per_second': 102.655, 'eval_steps_per_second': 6.467, 'epoch': 0.4}
{'loss': 1.1994, 'grad_norm': 0.13074956834316254, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.9136707782745361, 'eval_runtime': 9.7522, 'eval_samples_per_second': 102.541, 'eval_steps_per_second': 6.46, 'epoch': 0.44}
{'loss': 1.1845, 'grad_norm': 0.13719698786735535, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.9087216854095459, 'eval_runtime': 9.7834, 'eval_samples_per_second': 102.214, 'eval_steps_per_second': 6.44, 'epoch': 0.48}
{'loss': 1.2291, 'grad_norm': 0.11899713426828384, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.9086239337921143, 'eval_runtime': 9.7511, 'eval_samples_per_second': 102.553, 'eval_steps_per_second': 6.461, 'epoch': 0.52}
{'loss': 1.177, 'grad_norm': 0.12160598486661911, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.9079582095146179, 'eval_runtime': 9.7509, 'eval_samples_per_second': 102.555, 'eval_steps_per_second': 6.461, 'epoch': 0.56}
{'loss': 1.2165, 'grad_norm': 0.13756605982780457, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.9045191407203674, 'eval_runtime': 9.777, 'eval_samples_per_second': 102.281, 'eval_steps_per_second': 6.444, 'epoch': 0.6}
{'loss': 1.1409, 'grad_norm': 0.12538093328475952, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.9037281274795532, 'eval_runtime': 9.7747, 'eval_samples_per_second': 102.305, 'eval_steps_per_second': 6.445, 'epoch': 0.64}
{'loss': 1.1341, 'grad_norm': 0.15753741562366486, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.9014606475830078, 'eval_runtime': 9.7646, 'eval_samples_per_second': 102.41, 'eval_steps_per_second': 6.452, 'epoch': 0.68}
{'loss': 1.1578, 'grad_norm': 0.1479446291923523, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.9004350304603577, 'eval_runtime': 9.7387, 'eval_samples_per_second': 102.683, 'eval_steps_per_second': 6.469, 'epoch': 0.72}
{'loss': 1.1382, 'grad_norm': 0.13322098553180695, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.8965632319450378, 'eval_runtime': 9.7542, 'eval_samples_per_second': 102.519, 'eval_steps_per_second': 6.459, 'epoch': 0.76}
{'loss': 1.145, 'grad_norm': 0.14864641427993774, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.8984389305114746, 'eval_runtime': 9.7319, 'eval_samples_per_second': 102.755, 'eval_steps_per_second': 6.474, 'epoch': 0.8}
{'loss': 1.126, 'grad_norm': 0.12881922721862793, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.8965684771537781, 'eval_runtime': 9.742, 'eval_samples_per_second': 102.649, 'eval_steps_per_second': 6.467, 'epoch': 0.84}
{'loss': 1.099, 'grad_norm': 0.13635113835334778, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.8946414589881897, 'eval_runtime': 9.7196, 'eval_samples_per_second': 102.884, 'eval_steps_per_second': 6.482, 'epoch': 0.88}
{'loss': 1.1421, 'grad_norm': 0.11756471544504166, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.895186722278595, 'eval_runtime': 9.7021, 'eval_samples_per_second': 103.07, 'eval_steps_per_second': 6.493, 'epoch': 0.92}
{'loss': 1.1376, 'grad_norm': 0.12952721118927002, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.8945295214653015, 'eval_runtime': 9.6992, 'eval_samples_per_second': 103.101, 'eval_steps_per_second': 6.495, 'epoch': 0.96}
{'train_runtime': 382.7266, 'train_samples_per_second': 26.071, 'train_steps_per_second': 1.63, 'train_loss': 1.2648437267694719, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4184097051620483, 1.0305391550064087, 0.9619833827018738, 0.9509913325309753, 0.9368938207626343, 0.9316897988319397, 0.9268227815628052, 0.922191858291626, 0.9182322025299072, 0.9142884612083435, 0.9136707782745361, 0.9087216854095459, 0.9086239337921143, 0.9079582095146179, 0.9045191407203674, 0.9037281274795532, 0.9014606475830078, 0.9004350304603577, 0.8965632319450378, 0.8984389305114746, 0.8965684771537781, 0.8946414589881897, 0.895186722278595, 0.8945295214653015], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.4184097051620483, 1.0305391550064087, 0.9619833827018738, 0.9509913325309753, 0.9368938207626343, 0.9316897988319397, 0.9268227815628052, 0.922191858291626, 0.9182322025299072, 0.9142884612083435, 0.9136707782745361, 0.9087216854095459, 0.9086239337921143, 0.9079582095146179, 0.9045191407203674, 0.9037281274795532, 0.9014606475830078, 0.9004350304603577, 0.8965632319450378, 0.8984389305114746, 0.8965684771537781, 0.8946414589881897, 0.895186722278595, 0.8945295214653015]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.3278779983520508
current iteration best possible eval_loss (full train run):  -0.8945295214653015
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 11.0555 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.35491812229156494, 0.0927686095237732, 0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7262229919433594, 0.8165992498397827, 0.8520699143409729, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9559857845306396, 0.3273009657859802, 0.9828505516052246, 0.5352497100830078]  ‚Üí  acq = -1.047417253049645
X = [0.8189860582351685, 0.7869956493377686, 0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8973821401596069, 0.7314273715019226, 0.9184417128562927, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9420246481895447, 0.9331071972846985, 0.31236356496810913, 0.36077266931533813]  ‚Üí  acq = -1.0520339724852308
X = [0.7738390564918518, 0.5021045207977295, 0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5741828680038452, 0.5517953038215637, 0.7892404794692993, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9668935537338257, 0.5031551122665405, 0.7251023054122925, 0.2269490361213684]  ‚Üí  acq = -1.0520339724850487
X = [0.20103693008422852, 0.27278316020965576, 0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.14547844231128693, 0.0291365385055542, 0.3460528254508972, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.32679685950279236, 0.4884251356124878, 0.9665257930755615, 0.2274898886680603]  ‚Üí  acq = -1.0520185175588046
X = [0.6374526023864746, 0.36883002519607544, 0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3681538701057434, 0.6981962323188782, 0.7622081637382507, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.5123441219329834, 0.3907546401023865, 0.21269100904464722, 0.09688013792037964]  ‚Üí  acq = -1.0520339660971356
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0620, dtype=torch.float64), tensor(0.1276, dtype=torch.float64), tensor(0.0712, dtype=torch.float64), 0, 0, tensor(0.3682, dtype=torch.float64), 0, tensor(0.1300, dtype=torch.float64), tensor(0.2400, dtype=torch.float64), 20, 1, 1, 0, 0, 1, 126, 0.09485615505450255, 24.987985084984828, 0]
normalized proposed parameters for next round by BO: [tensor(0.0620, dtype=torch.float64), tensor(0.1276, dtype=torch.float64), tensor(0.0712, dtype=torch.float64), tensor(0.0010, dtype=torch.float64), tensor(1.5210e-18, dtype=torch.float64), tensor(0.3682, dtype=torch.float64), tensor(2.3997e-18, dtype=torch.float64), tensor(0.1300, dtype=torch.float64), tensor(0.2400, dtype=torch.float64), tensor(0.6270, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9821, dtype=torch.float64), tensor(0.9486, dtype=torch.float64), tensor(0.5206, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.062
  gsm8k: 0.128
  rowan_hellaswag: 0.071
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.368
  wikitext: 0
  mmlu: 0.13
  arc_challenge: 0.24

LoRA Parameters:
  lora_r: (126,)
  lora_dropout: (0.09485615505450255,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (24.987985084984828,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  126
lora dropout:  0.09485615505450255
lora alpha:  24.987985084984828
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 79,994,880 || all params: 8,110,256,128 || trainable%: 0.9863
length of training data:  9988
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2909, 'grad_norm': 0.4543541669845581, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5532082319259644, 'eval_runtime': 9.7154, 'eval_samples_per_second': 102.93, 'eval_steps_per_second': 6.485, 'epoch': 0.04}
{'loss': 1.692, 'grad_norm': 0.18690042197704315, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1846561431884766, 'eval_runtime': 9.749, 'eval_samples_per_second': 102.575, 'eval_steps_per_second': 6.462, 'epoch': 0.08}
{'loss': 1.3046, 'grad_norm': 0.17149753868579865, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0497056245803833, 'eval_runtime': 9.7855, 'eval_samples_per_second': 102.192, 'eval_steps_per_second': 6.438, 'epoch': 0.12}
{'loss': 1.255, 'grad_norm': 0.1932743340730667, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.008910894393921, 'eval_runtime': 9.8224, 'eval_samples_per_second': 101.808, 'eval_steps_per_second': 6.414, 'epoch': 0.16}
{'loss': 1.1592, 'grad_norm': 0.1686718761920929, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9703704714775085, 'eval_runtime': 9.8356, 'eval_samples_per_second': 101.672, 'eval_steps_per_second': 6.405, 'epoch': 0.2}
{'loss': 1.0567, 'grad_norm': 0.17739984393119812, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9546133279800415, 'eval_runtime': 9.8378, 'eval_samples_per_second': 101.649, 'eval_steps_per_second': 6.404, 'epoch': 0.24}
{'loss': 1.0289, 'grad_norm': 0.20641833543777466, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9278301000595093, 'eval_runtime': 9.8438, 'eval_samples_per_second': 101.587, 'eval_steps_per_second': 6.4, 'epoch': 0.28}
{'loss': 0.9933, 'grad_norm': 0.16651037335395813, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9280186891555786, 'eval_runtime': 9.8433, 'eval_samples_per_second': 101.592, 'eval_steps_per_second': 6.4, 'epoch': 0.32}
{'loss': 1.0689, 'grad_norm': 0.18371818959712982, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9199906587600708, 'eval_runtime': 9.8965, 'eval_samples_per_second': 101.046, 'eval_steps_per_second': 6.366, 'epoch': 0.36}
{'loss': 0.9953, 'grad_norm': 0.16926081478595734, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.914483904838562, 'eval_runtime': 9.8807, 'eval_samples_per_second': 101.207, 'eval_steps_per_second': 6.376, 'epoch': 0.4}
{'loss': 1.0653, 'grad_norm': 0.19100339710712433, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.910603404045105, 'eval_runtime': 9.8782, 'eval_samples_per_second': 101.233, 'eval_steps_per_second': 6.378, 'epoch': 0.44}
{'loss': 1.0023, 'grad_norm': 0.18908317387104034, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9107556939125061, 'eval_runtime': 9.8701, 'eval_samples_per_second': 101.316, 'eval_steps_per_second': 6.383, 'epoch': 0.48}
{'loss': 0.9942, 'grad_norm': 0.1911027431488037, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9048025608062744, 'eval_runtime': 9.8775, 'eval_samples_per_second': 101.24, 'eval_steps_per_second': 6.378, 'epoch': 0.52}
{'loss': 0.9764, 'grad_norm': 0.17557679116725922, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.90805983543396, 'eval_runtime': 9.8768, 'eval_samples_per_second': 101.247, 'eval_steps_per_second': 6.379, 'epoch': 0.56}
{'loss': 0.8629, 'grad_norm': 0.2263198047876358, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9048902988433838, 'eval_runtime': 9.8691, 'eval_samples_per_second': 101.327, 'eval_steps_per_second': 6.384, 'epoch': 0.6}
{'loss': 0.9144, 'grad_norm': 0.2064901441335678, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9033344984054565, 'eval_runtime': 9.8672, 'eval_samples_per_second': 101.346, 'eval_steps_per_second': 6.385, 'epoch': 0.64}
{'loss': 0.9123, 'grad_norm': 0.25333264470100403, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9004067182540894, 'eval_runtime': 9.8505, 'eval_samples_per_second': 101.518, 'eval_steps_per_second': 6.396, 'epoch': 0.68}
{'loss': 0.9936, 'grad_norm': 0.21866953372955322, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8983684182167053, 'eval_runtime': 9.8202, 'eval_samples_per_second': 101.831, 'eval_steps_per_second': 6.415, 'epoch': 0.72}
{'loss': 0.9999, 'grad_norm': 0.21569302678108215, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8968592882156372, 'eval_runtime': 9.8151, 'eval_samples_per_second': 101.884, 'eval_steps_per_second': 6.419, 'epoch': 0.76}
{'loss': 0.9469, 'grad_norm': 0.24777597188949585, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8983062505722046, 'eval_runtime': 9.8229, 'eval_samples_per_second': 101.803, 'eval_steps_per_second': 6.414, 'epoch': 0.8}
{'loss': 0.9004, 'grad_norm': 0.18769441545009613, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8952518105506897, 'eval_runtime': 9.8596, 'eval_samples_per_second': 101.424, 'eval_steps_per_second': 6.39, 'epoch': 0.84}
{'loss': 0.9479, 'grad_norm': 0.19770410656929016, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.895189106464386, 'eval_runtime': 9.8773, 'eval_samples_per_second': 101.242, 'eval_steps_per_second': 6.378, 'epoch': 0.88}
{'loss': 0.93, 'grad_norm': 0.29460984468460083, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8950226306915283, 'eval_runtime': 9.8532, 'eval_samples_per_second': 101.49, 'eval_steps_per_second': 6.394, 'epoch': 0.92}
{'loss': 0.8644, 'grad_norm': 0.18819484114646912, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8938290476799011, 'eval_runtime': 9.8597, 'eval_samples_per_second': 101.423, 'eval_steps_per_second': 6.39, 'epoch': 0.96}
{'loss': 0.9647, 'grad_norm': 0.45169955492019653, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8935175538063049, 'eval_runtime': 9.8736, 'eval_samples_per_second': 101.28, 'eval_steps_per_second': 6.381, 'epoch': 1.0}
{'train_runtime': 419.4626, 'train_samples_per_second': 23.811, 'train_steps_per_second': 1.49, 'train_loss': 1.1248142364501954, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5532082319259644, 1.1846561431884766, 1.0497056245803833, 1.008910894393921, 0.9703704714775085, 0.9546133279800415, 0.9278301000595093, 0.9280186891555786, 0.9199906587600708, 0.914483904838562, 0.910603404045105, 0.9107556939125061, 0.9048025608062744, 0.90805983543396, 0.9048902988433838, 0.9033344984054565, 0.9004067182540894, 0.8983684182167053, 0.8968592882156372, 0.8983062505722046, 0.8952518105506897, 0.895189106464386, 0.8950226306915283, 0.8938290476799011, 0.8935175538063049], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5532082319259644, 1.1846561431884766, 1.0497056245803833, 1.008910894393921, 0.9703704714775085, 0.9546133279800415, 0.9278301000595093, 0.9280186891555786, 0.9199906587600708, 0.914483904838562, 0.910603404045105, 0.9107556939125061, 0.9048025608062744, 0.90805983543396, 0.9048902988433838, 0.9033344984054565, 0.9004067182540894, 0.8983684182167053, 0.8968592882156372, 0.8983062505722046, 0.8952518105506897, 0.895189106464386, 0.8950226306915283, 0.8938290476799011, 0.8935175538063049]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1507148742675781
current iteration best possible eval_loss (full train run):  -0.8935175538063049
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.9219 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -1.07488611050538
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -1.0748861278447697
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -1.0756560121577063
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -1.0748861278447697
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -1.0748861263888587
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0542, dtype=torch.float64), tensor(0.2887, dtype=torch.float64), 0, 0, 0, tensor(0.2419, dtype=torch.float64), 0, tensor(0.1157, dtype=torch.float64), tensor(0.2918, dtype=torch.float64), 8, 0, 1, 0, 1, 1, 76, 0.1, 31.761156779430387, 1]
normalized proposed parameters for next round by BO: [tensor(0.0542, dtype=torch.float64), tensor(0.2887, dtype=torch.float64), tensor(2.9298e-18, dtype=torch.float64), tensor(0.0028, dtype=torch.float64), tensor(0.0048, dtype=torch.float64), tensor(0.2419, dtype=torch.float64), tensor(1.5433e-17, dtype=torch.float64), tensor(0.1157, dtype=torch.float64), tensor(0.2918, dtype=torch.float64), tensor(0.2430, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5951, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6617, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.054
  gsm8k: 0.289
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.242
  wikitext: 0
  mmlu: 0.116
  arc_challenge: 0.292

LoRA Parameters:
  lora_r: (76,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (8,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (31.761156779430387,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  8
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  76
lora dropout:  0.1
lora alpha:  31.761156779430387
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 25,526,272 || all params: 8,055,787,520 || trainable%: 0.3169
length of training data:  9921
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8928, 'grad_norm': 1.0549980401992798, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4883922338485718, 'eval_runtime': 9.2617, 'eval_samples_per_second': 107.971, 'eval_steps_per_second': 6.802, 'epoch': 0.04}
{'loss': 1.4298, 'grad_norm': 0.7720988988876343, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0613276958465576, 'eval_runtime': 9.2837, 'eval_samples_per_second': 107.715, 'eval_steps_per_second': 6.786, 'epoch': 0.08}
{'loss': 1.0996, 'grad_norm': 0.396966814994812, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.9860257506370544, 'eval_runtime': 9.3119, 'eval_samples_per_second': 107.389, 'eval_steps_per_second': 6.766, 'epoch': 0.12}
{'loss': 1.0313, 'grad_norm': 0.365151971578598, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.9560779929161072, 'eval_runtime': 9.3216, 'eval_samples_per_second': 107.278, 'eval_steps_per_second': 6.759, 'epoch': 0.16}
{'loss': 0.9699, 'grad_norm': 0.3310239613056183, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.9394927024841309, 'eval_runtime': 9.3387, 'eval_samples_per_second': 107.081, 'eval_steps_per_second': 6.746, 'epoch': 0.2}
{'loss': 0.9823, 'grad_norm': 0.4222886562347412, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.9205210208892822, 'eval_runtime': 9.3348, 'eval_samples_per_second': 107.126, 'eval_steps_per_second': 6.749, 'epoch': 0.24}
{'loss': 0.8944, 'grad_norm': 0.44345593452453613, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.9128472805023193, 'eval_runtime': 9.3438, 'eval_samples_per_second': 107.023, 'eval_steps_per_second': 6.742, 'epoch': 0.28}
{'loss': 0.9417, 'grad_norm': 0.2629241943359375, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.9039457440376282, 'eval_runtime': 9.3631, 'eval_samples_per_second': 106.802, 'eval_steps_per_second': 6.729, 'epoch': 0.32}
{'loss': 0.9222, 'grad_norm': 0.2925085723400116, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.9048159122467041, 'eval_runtime': 9.3508, 'eval_samples_per_second': 106.943, 'eval_steps_per_second': 6.737, 'epoch': 0.36}
{'loss': 0.9544, 'grad_norm': 0.25421586632728577, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.8947682976722717, 'eval_runtime': 9.3639, 'eval_samples_per_second': 106.794, 'eval_steps_per_second': 6.728, 'epoch': 0.4}
{'loss': 0.8868, 'grad_norm': 0.25599467754364014, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.8959276080131531, 'eval_runtime': 9.3727, 'eval_samples_per_second': 106.693, 'eval_steps_per_second': 6.722, 'epoch': 0.44}
{'loss': 0.8739, 'grad_norm': 0.26662176847457886, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.9001079201698303, 'eval_runtime': 9.3568, 'eval_samples_per_second': 106.874, 'eval_steps_per_second': 6.733, 'epoch': 0.48}
{'loss': 0.8831, 'grad_norm': 0.2434421181678772, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.891038715839386, 'eval_runtime': 9.3625, 'eval_samples_per_second': 106.809, 'eval_steps_per_second': 6.729, 'epoch': 0.52}
{'loss': 0.8684, 'grad_norm': 0.2908819019794464, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.8904561996459961, 'eval_runtime': 9.3816, 'eval_samples_per_second': 106.591, 'eval_steps_per_second': 6.715, 'epoch': 0.56}
{'loss': 0.8795, 'grad_norm': 0.23006582260131836, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.8911833763122559, 'eval_runtime': 9.3726, 'eval_samples_per_second': 106.694, 'eval_steps_per_second': 6.722, 'epoch': 0.6}
{'loss': 0.8973, 'grad_norm': 0.29690274596214294, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.8874201774597168, 'eval_runtime': 9.3693, 'eval_samples_per_second': 106.731, 'eval_steps_per_second': 6.724, 'epoch': 0.64}
{'loss': 0.9257, 'grad_norm': 0.2556553781032562, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.88612961769104, 'eval_runtime': 9.369, 'eval_samples_per_second': 106.735, 'eval_steps_per_second': 6.724, 'epoch': 0.68}
{'loss': 0.8765, 'grad_norm': 0.2786919176578522, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.8816397786140442, 'eval_runtime': 9.3561, 'eval_samples_per_second': 106.882, 'eval_steps_per_second': 6.734, 'epoch': 0.72}
{'loss': 0.8916, 'grad_norm': 0.24599754810333252, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.88189697265625, 'eval_runtime': 9.3649, 'eval_samples_per_second': 106.781, 'eval_steps_per_second': 6.727, 'epoch': 0.76}
{'loss': 0.8876, 'grad_norm': 0.26000064611434937, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.8797659277915955, 'eval_runtime': 9.3944, 'eval_samples_per_second': 106.446, 'eval_steps_per_second': 6.706, 'epoch': 0.81}
{'loss': 0.871, 'grad_norm': 0.2867593467235565, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.8788125514984131, 'eval_runtime': 9.374, 'eval_samples_per_second': 106.677, 'eval_steps_per_second': 6.721, 'epoch': 0.85}
{'loss': 0.8794, 'grad_norm': 0.4291272461414337, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.8781189918518066, 'eval_runtime': 9.366, 'eval_samples_per_second': 106.769, 'eval_steps_per_second': 6.726, 'epoch': 0.89}
{'loss': 0.8558, 'grad_norm': 0.2993745803833008, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.8772360682487488, 'eval_runtime': 9.3739, 'eval_samples_per_second': 106.68, 'eval_steps_per_second': 6.721, 'epoch': 0.93}
{'loss': 0.8578, 'grad_norm': 0.3611430525779724, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.8773691058158875, 'eval_runtime': 9.3834, 'eval_samples_per_second': 106.571, 'eval_steps_per_second': 6.714, 'epoch': 0.97}
{'train_runtime': 419.9514, 'train_samples_per_second': 23.624, 'train_steps_per_second': 1.479, 'train_loss': 1.013226805485773, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4883922338485718, 1.0613276958465576, 0.9860257506370544, 0.9560779929161072, 0.9394927024841309, 0.9205210208892822, 0.9128472805023193, 0.9039457440376282, 0.9048159122467041, 0.8947682976722717, 0.8959276080131531, 0.9001079201698303, 0.891038715839386, 0.8904561996459961, 0.8911833763122559, 0.8874201774597168, 0.88612961769104, 0.8816397786140442, 0.88189697265625, 0.8797659277915955, 0.8788125514984131, 0.8781189918518066, 0.8772360682487488, 0.8773691058158875], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.4883922338485718, 1.0613276958465576, 0.9860257506370544, 0.9560779929161072, 0.9394927024841309, 0.9205210208892822, 0.9128472805023193, 0.9039457440376282, 0.9048159122467041, 0.8947682976722717, 0.8959276080131531, 0.9001079201698303, 0.891038715839386, 0.8904561996459961, 0.8911833763122559, 0.8874201774597168, 0.88612961769104, 0.8816397786140442, 0.88189697265625, 0.8797659277915955, 0.8788125514984131, 0.8781189918518066, 0.8772360682487488, 0.8773691058158875]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.5638645887374878
current iteration best possible eval_loss (full train run):  -0.8773691058158875
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.8147 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.769386351108551, 0.48460155725479126, 0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.871427059173584, 0.9379879832267761, 0.9101236462593079, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.43461495637893677, 0.05005854368209839, 0.09836432337760925, 0.9400144815444946]  ‚Üí  acq = -0.9699360688373692
X = [0.9526360034942627, 0.8012327551841736, 0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5360714197158813, 0.12353461980819702, 0.3528776168823242, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5302047729492188, 0.31215590238571167, 0.6163817644119263, 0.9380749464035034]  ‚Üí  acq = -0.9668285407845603
X = [0.34730666875839233, 0.06246674060821533, 0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3383713960647583, 0.12947320938110352, 0.50956130027771, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.48614978790283203, 0.43715083599090576, 0.6119240522384644, 0.47161221504211426]  ‚Üí  acq = -0.9683637317556801
X = [0.45309585332870483, 0.0678371787071228, 0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7435163855552673, 0.12291663885116577, 0.4224488139152527, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.23402123153209686, 0.37334394454956055, 0.5466699600219727, 0.06654441356658936]  ‚Üí  acq = -0.9680457725152071
X = [0.18636363744735718, 0.6823987364768982, 0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9869434833526611, 0.8804963827133179, 0.9389960169792175, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.33730313181877136, 0.484236478805542, 0.44418343901634216, 0.7705504298210144]  ‚Üí  acq = -0.9927815903943373
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1861, dtype=torch.float64), tensor(0.5128, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(0.0275, dtype=torch.float64), 0, tensor(0.2153, dtype=torch.float64), 0, 0, tensor(0.0162, dtype=torch.float64), 32, 0, 0, 0, 1, 1, 67, 0.0, 15.71522187836332, 1]
normalized proposed parameters for next round by BO: [tensor(0.1861, dtype=torch.float64), tensor(0.5128, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(0.0275, dtype=torch.float64), tensor(1.5774e-18, dtype=torch.float64), tensor(0.2153, dtype=torch.float64), tensor(3.6781e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0162, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5205, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3274, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.186
  gsm8k: 0.513
  rowan_hellaswag: 0.042
  sciq: 0.027
  triviaqa: 0
  truthfulqa_gen: 0.215
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.016

LoRA Parameters:
  lora_r: (67,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (15.71522187836332,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  67
lora dropout:  0.0
lora alpha:  15.71522187836332
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 79,036,416 || all params: 8,109,297,664 || trainable%: 0.9746
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6381, 'grad_norm': 0.681204080581665, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3347011804580688, 'eval_runtime': 10.6088, 'eval_samples_per_second': 94.262, 'eval_steps_per_second': 5.938, 'epoch': 0.04}
{'loss': 1.2709, 'grad_norm': 0.28320297598838806, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9594962000846863, 'eval_runtime': 10.6311, 'eval_samples_per_second': 94.063, 'eval_steps_per_second': 5.926, 'epoch': 0.08}
{'loss': 0.9655, 'grad_norm': 0.22401022911071777, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8946569561958313, 'eval_runtime': 10.6507, 'eval_samples_per_second': 93.891, 'eval_steps_per_second': 5.915, 'epoch': 0.12}
{'loss': 0.9373, 'grad_norm': 0.15343037247657776, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8841866850852966, 'eval_runtime': 10.6673, 'eval_samples_per_second': 93.744, 'eval_steps_per_second': 5.906, 'epoch': 0.16}
{'loss': 0.9276, 'grad_norm': 0.13503414392471313, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8750479817390442, 'eval_runtime': 10.6873, 'eval_samples_per_second': 93.569, 'eval_steps_per_second': 5.895, 'epoch': 0.2}
{'loss': 0.9156, 'grad_norm': 0.13628016412258148, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.867070198059082, 'eval_runtime': 10.6912, 'eval_samples_per_second': 93.535, 'eval_steps_per_second': 5.893, 'epoch': 0.24}
{'loss': 0.8836, 'grad_norm': 0.15668419003486633, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8635404109954834, 'eval_runtime': 10.6968, 'eval_samples_per_second': 93.486, 'eval_steps_per_second': 5.89, 'epoch': 0.28}
{'loss': 0.9016, 'grad_norm': 0.13667188584804535, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8624143004417419, 'eval_runtime': 10.6964, 'eval_samples_per_second': 93.489, 'eval_steps_per_second': 5.89, 'epoch': 0.32}
{'loss': 0.8951, 'grad_norm': 0.17435766756534576, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8567165732383728, 'eval_runtime': 10.6893, 'eval_samples_per_second': 93.552, 'eval_steps_per_second': 5.894, 'epoch': 0.36}
{'loss': 0.9024, 'grad_norm': 0.1383824646472931, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8536133766174316, 'eval_runtime': 10.704, 'eval_samples_per_second': 93.423, 'eval_steps_per_second': 5.886, 'epoch': 0.4}
{'loss': 0.9247, 'grad_norm': 0.16910958290100098, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8490003943443298, 'eval_runtime': 10.7289, 'eval_samples_per_second': 93.206, 'eval_steps_per_second': 5.872, 'epoch': 0.44}
{'loss': 0.8823, 'grad_norm': 0.16026237607002258, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8490098714828491, 'eval_runtime': 10.7422, 'eval_samples_per_second': 93.091, 'eval_steps_per_second': 5.865, 'epoch': 0.48}
{'loss': 0.8818, 'grad_norm': 0.1590191274881363, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8475950956344604, 'eval_runtime': 10.7268, 'eval_samples_per_second': 93.224, 'eval_steps_per_second': 5.873, 'epoch': 0.52}
{'loss': 0.8857, 'grad_norm': 0.15057389438152313, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8451727032661438, 'eval_runtime': 10.7128, 'eval_samples_per_second': 93.346, 'eval_steps_per_second': 5.881, 'epoch': 0.56}
{'loss': 0.8369, 'grad_norm': 0.1900518536567688, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.842494010925293, 'eval_runtime': 10.7119, 'eval_samples_per_second': 93.354, 'eval_steps_per_second': 5.881, 'epoch': 0.6}
{'loss': 0.8605, 'grad_norm': 0.16890302300453186, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.842622697353363, 'eval_runtime': 10.7146, 'eval_samples_per_second': 93.33, 'eval_steps_per_second': 5.88, 'epoch': 0.64}
{'loss': 0.8968, 'grad_norm': 0.17457886040210724, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8425456285476685, 'eval_runtime': 10.6963, 'eval_samples_per_second': 93.49, 'eval_steps_per_second': 5.89, 'epoch': 0.68}
{'loss': 0.866, 'grad_norm': 0.15638193488121033, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8393313884735107, 'eval_runtime': 10.7359, 'eval_samples_per_second': 93.146, 'eval_steps_per_second': 5.868, 'epoch': 0.72}
{'loss': 0.8389, 'grad_norm': 0.15893541276454926, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8373598456382751, 'eval_runtime': 10.7114, 'eval_samples_per_second': 93.358, 'eval_steps_per_second': 5.882, 'epoch': 0.76}
{'loss': 0.861, 'grad_norm': 0.21464864909648895, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.83644700050354, 'eval_runtime': 10.7105, 'eval_samples_per_second': 93.366, 'eval_steps_per_second': 5.882, 'epoch': 0.8}
{'loss': 0.8327, 'grad_norm': 0.17144496738910675, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8362841606140137, 'eval_runtime': 10.7274, 'eval_samples_per_second': 93.219, 'eval_steps_per_second': 5.873, 'epoch': 0.84}
{'loss': 0.8549, 'grad_norm': 0.16673703491687775, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8342871069908142, 'eval_runtime': 10.724, 'eval_samples_per_second': 93.248, 'eval_steps_per_second': 5.875, 'epoch': 0.88}
{'loss': 0.8987, 'grad_norm': 0.18216343224048615, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8340470194816589, 'eval_runtime': 10.7241, 'eval_samples_per_second': 93.248, 'eval_steps_per_second': 5.875, 'epoch': 0.92}
{'loss': 0.8379, 'grad_norm': 0.1610037088394165, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8333082795143127, 'eval_runtime': 10.7253, 'eval_samples_per_second': 93.237, 'eval_steps_per_second': 5.874, 'epoch': 0.96}
{'loss': 0.8123, 'grad_norm': 0.2148965746164322, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8328214287757874, 'eval_runtime': 10.7136, 'eval_samples_per_second': 93.339, 'eval_steps_per_second': 5.88, 'epoch': 1.0}
{'train_runtime': 502.2246, 'train_samples_per_second': 19.907, 'train_steps_per_second': 1.244, 'train_loss': 0.9683503387451172, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3347011804580688, 0.9594962000846863, 0.8946569561958313, 0.8841866850852966, 0.8750479817390442, 0.867070198059082, 0.8635404109954834, 0.8624143004417419, 0.8567165732383728, 0.8536133766174316, 0.8490003943443298, 0.8490098714828491, 0.8475950956344604, 0.8451727032661438, 0.842494010925293, 0.842622697353363, 0.8425456285476685, 0.8393313884735107, 0.8373598456382751, 0.83644700050354, 0.8362841606140137, 0.8342871069908142, 0.8340470194816589, 0.8333082795143127, 0.8328214287757874], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3347011804580688, 0.9594962000846863, 0.8946569561958313, 0.8841866850852966, 0.8750479817390442, 0.867070198059082, 0.8635404109954834, 0.8624143004417419, 0.8567165732383728, 0.8536133766174316, 0.8490003943443298, 0.8490098714828491, 0.8475950956344604, 0.8451727032661438, 0.842494010925293, 0.842622697353363, 0.8425456285476685, 0.8393313884735107, 0.8373598456382751, 0.83644700050354, 0.8362841606140137, 0.8342871069908142, 0.8340470194816589, 0.8333082795143127, 0.8328214287757874]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.8328214287757874
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.5622 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.8763366121007885
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -0.9308946517876093
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.9029064588454931
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -0.9308946517876093
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -0.9357981727340132
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1166, dtype=torch.float64), tensor(0.2456, dtype=torch.float64), tensor(0.0781, dtype=torch.float64), tensor(0.1433, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), tensor(0.0974, dtype=torch.float64), tensor(0.0573, dtype=torch.float64), tensor(0.1321, dtype=torch.float64), tensor(0.0593, dtype=torch.float64), 27, 0, 1, 0, 1, 1, 26, 0.023601646618101525, 26.684768086600748, 1]
normalized proposed parameters for next round by BO: [tensor(0.1166, dtype=torch.float64), tensor(0.2456, dtype=torch.float64), tensor(0.0781, dtype=torch.float64), tensor(0.1433, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), tensor(0.0974, dtype=torch.float64), tensor(0.0573, dtype=torch.float64), tensor(0.1321, dtype=torch.float64), tensor(0.0593, dtype=torch.float64), tensor(0.8534, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2009, dtype=torch.float64), tensor(0.2360, dtype=torch.float64), tensor(0.5559, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.117
  gsm8k: 0.246
  rowan_hellaswag: 0.078
  sciq: 0.143
  triviaqa: 0.07
  truthfulqa_gen: 0.097
  wikitext: 0.057
  mmlu: 0.132
  arc_challenge: 0.059

LoRA Parameters:
  lora_r: (26,)
  lora_dropout: (0.023601646618101525,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (26.684768086600748,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  26
lora dropout:  0.023601646618101525
lora alpha:  26.684768086600748
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 29,472,768 || all params: 8,059,734,016 || trainable%: 0.3657
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8435, 'grad_norm': 2.239942789077759, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3645439147949219, 'eval_runtime': 10.1792, 'eval_samples_per_second': 98.24, 'eval_steps_per_second': 6.189, 'epoch': 0.04}
{'loss': 1.4419, 'grad_norm': 0.48605960607528687, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.982145369052887, 'eval_runtime': 10.2232, 'eval_samples_per_second': 97.817, 'eval_steps_per_second': 6.162, 'epoch': 0.08}
{'loss': 1.2409, 'grad_norm': 0.5768404006958008, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9425485730171204, 'eval_runtime': 10.2642, 'eval_samples_per_second': 97.426, 'eval_steps_per_second': 6.138, 'epoch': 0.12}
{'loss': 1.2073, 'grad_norm': 0.5402387380599976, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9163414239883423, 'eval_runtime': 10.2773, 'eval_samples_per_second': 97.301, 'eval_steps_per_second': 6.13, 'epoch': 0.16}
{'loss': 1.13, 'grad_norm': 0.46282172203063965, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8960323333740234, 'eval_runtime': 10.2757, 'eval_samples_per_second': 97.317, 'eval_steps_per_second': 6.131, 'epoch': 0.2}
{'loss': 1.1798, 'grad_norm': 0.4285065829753876, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8952104449272156, 'eval_runtime': 10.2663, 'eval_samples_per_second': 97.406, 'eval_steps_per_second': 6.137, 'epoch': 0.24}
{'loss': 1.1258, 'grad_norm': 0.3317631781101227, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8876184225082397, 'eval_runtime': 10.2644, 'eval_samples_per_second': 97.424, 'eval_steps_per_second': 6.138, 'epoch': 0.28}
{'loss': 1.1031, 'grad_norm': 0.3958151042461395, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8751463890075684, 'eval_runtime': 10.2741, 'eval_samples_per_second': 97.332, 'eval_steps_per_second': 6.132, 'epoch': 0.32}
{'loss': 1.1005, 'grad_norm': 0.43429437279701233, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8748095631599426, 'eval_runtime': 10.2754, 'eval_samples_per_second': 97.32, 'eval_steps_per_second': 6.131, 'epoch': 0.36}
{'loss': 1.1703, 'grad_norm': 0.702714741230011, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8765903115272522, 'eval_runtime': 10.271, 'eval_samples_per_second': 97.361, 'eval_steps_per_second': 6.134, 'epoch': 0.4}
{'loss': 1.144, 'grad_norm': 0.4196980595588684, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8737604022026062, 'eval_runtime': 10.2685, 'eval_samples_per_second': 97.386, 'eval_steps_per_second': 6.135, 'epoch': 0.44}
{'loss': 1.0574, 'grad_norm': 0.3535303473472595, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8734264969825745, 'eval_runtime': 10.2637, 'eval_samples_per_second': 97.431, 'eval_steps_per_second': 6.138, 'epoch': 0.48}
{'loss': 1.0611, 'grad_norm': 0.3767510950565338, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8601530194282532, 'eval_runtime': 10.2662, 'eval_samples_per_second': 97.407, 'eval_steps_per_second': 6.137, 'epoch': 0.52}
{'loss': 1.1123, 'grad_norm': 0.4946281611919403, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8615829348564148, 'eval_runtime': 10.2522, 'eval_samples_per_second': 97.54, 'eval_steps_per_second': 6.145, 'epoch': 0.56}
{'loss': 0.9998, 'grad_norm': 0.4880765378475189, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8651254773139954, 'eval_runtime': 10.263, 'eval_samples_per_second': 97.437, 'eval_steps_per_second': 6.139, 'epoch': 0.6}
{'loss': 1.0357, 'grad_norm': 0.3517223298549652, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8598611950874329, 'eval_runtime': 10.2572, 'eval_samples_per_second': 97.493, 'eval_steps_per_second': 6.142, 'epoch': 0.64}
{'loss': 1.0983, 'grad_norm': 0.3277548551559448, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8563939929008484, 'eval_runtime': 10.2687, 'eval_samples_per_second': 97.384, 'eval_steps_per_second': 6.135, 'epoch': 0.68}
{'loss': 1.0326, 'grad_norm': 0.4268125891685486, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8521355986595154, 'eval_runtime': 10.2692, 'eval_samples_per_second': 97.379, 'eval_steps_per_second': 6.135, 'epoch': 0.72}
{'loss': 1.1002, 'grad_norm': 0.40059995651245117, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8548210859298706, 'eval_runtime': 10.2642, 'eval_samples_per_second': 97.426, 'eval_steps_per_second': 6.138, 'epoch': 0.76}
{'loss': 1.0782, 'grad_norm': 0.4486613869667053, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8494895696640015, 'eval_runtime': 10.2679, 'eval_samples_per_second': 97.391, 'eval_steps_per_second': 6.136, 'epoch': 0.8}
{'loss': 1.0744, 'grad_norm': 0.33982783555984497, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8495364785194397, 'eval_runtime': 10.3084, 'eval_samples_per_second': 97.008, 'eval_steps_per_second': 6.112, 'epoch': 0.84}
{'loss': 1.0857, 'grad_norm': 0.3450982868671417, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8486273884773254, 'eval_runtime': 10.2689, 'eval_samples_per_second': 97.382, 'eval_steps_per_second': 6.135, 'epoch': 0.88}
{'loss': 1.0762, 'grad_norm': 0.35539352893829346, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8478429913520813, 'eval_runtime': 10.3004, 'eval_samples_per_second': 97.084, 'eval_steps_per_second': 6.116, 'epoch': 0.92}
{'loss': 1.0194, 'grad_norm': 0.4323571026325226, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8465802073478699, 'eval_runtime': 10.3025, 'eval_samples_per_second': 97.064, 'eval_steps_per_second': 6.115, 'epoch': 0.96}
{'loss': 1.0599, 'grad_norm': 0.39357417821884155, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8460307717323303, 'eval_runtime': 10.2894, 'eval_samples_per_second': 97.187, 'eval_steps_per_second': 6.123, 'epoch': 1.0}
{'train_runtime': 487.576, 'train_samples_per_second': 20.501, 'train_steps_per_second': 1.282, 'train_loss': 1.1831279846191407, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3645439147949219, 0.982145369052887, 0.9425485730171204, 0.9163414239883423, 0.8960323333740234, 0.8952104449272156, 0.8876184225082397, 0.8751463890075684, 0.8748095631599426, 0.8765903115272522, 0.8737604022026062, 0.8734264969825745, 0.8601530194282532, 0.8615829348564148, 0.8651254773139954, 0.8598611950874329, 0.8563939929008484, 0.8521355986595154, 0.8548210859298706, 0.8494895696640015, 0.8495364785194397, 0.8486273884773254, 0.8478429913520813, 0.8465802073478699, 0.8460307717323303], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3645439147949219, 0.982145369052887, 0.9425485730171204, 0.9163414239883423, 0.8960323333740234, 0.8952104449272156, 0.8876184225082397, 0.8751463890075684, 0.8748095631599426, 0.8765903115272522, 0.8737604022026062, 0.8734264969825745, 0.8601530194282532, 0.8615829348564148, 0.8651254773139954, 0.8598611950874329, 0.8563939929008484, 0.8521355986595154, 0.8548210859298706, 0.8494895696640015, 0.8495364785194397, 0.8486273884773254, 0.8478429913520813, 0.8465802073478699, 0.8460307717323303]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2884118556976318
current iteration best possible eval_loss (full train run):  -0.8460307717323303
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.3097 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8786115646362305, 0.9964625239372253, 0.801994264125824, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.32018935680389404, 0.9432199001312256, 0.32261133193969727, 0.7924636006355286, 0.7065163850784302, 0.0029845833778381348]  ‚Üí  acq = -0.9553208587040201
X = [0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9268273115158081, 0.9158058762550354, 0.63909512758255, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5700327754020691, 0.40536046028137207, 0.8583176136016846, 0.07649117708206177, 0.7816433906555176, 0.45509761571884155]  ‚Üí  acq = -0.9543473966985785
X = [0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.04227423667907715, 0.07738137245178223, 0.8804585933685303, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7815406322479248, 0.9972329139709473, 0.8300705552101135, 0.5278875827789307, 0.6398637294769287, 0.25792115926742554]  ‚Üí  acq = -0.9572139317893655
X = [0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.6390325427055359, 0.4825098514556885, 0.15652796626091003, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.7462385892868042, 0.8871928453445435, 0.8694287538528442, 0.8900622725486755, 0.7508230209350586, 0.7939770817756653]  ‚Üí  acq = -0.9572138256007547
X = [0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.06267750263214111, 0.695570707321167, 0.9000268578529358, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2732275724411011, 0.8315107822418213, 0.7352238893508911, 0.4935872554779053, 0.26904296875, 0.028178691864013672]  ‚Üí  acq = -0.9607005785198609
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2109, dtype=torch.float64), tensor(0.4704, dtype=torch.float64), tensor(0.0103, dtype=torch.float64), 0, 0, tensor(0.1378, dtype=torch.float64), 0, tensor(0.0515, dtype=torch.float64), tensor(0.1018, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 126, 0.0841729378197147, 18.287016460002214, 0]
normalized proposed parameters for next round by BO: [tensor(0.2109, dtype=torch.float64), tensor(0.4704, dtype=torch.float64), tensor(0.0103, dtype=torch.float64), tensor(0.0078, dtype=torch.float64), tensor(0.0096, dtype=torch.float64), tensor(0.1378, dtype=torch.float64), tensor(8.1539e-18, dtype=torch.float64), tensor(0.0515, dtype=torch.float64), tensor(0.1018, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9837, dtype=torch.float64), tensor(0.8417, dtype=torch.float64), tensor(0.3810, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.211
  gsm8k: 0.47
  rowan_hellaswag: 0.01
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.138
  wikitext: 0
  mmlu: 0.051
  arc_challenge: 0.102

LoRA Parameters:
  lora_r: (126,)
  lora_dropout: (0.0841729378197147,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (18.287016460002214,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  126
lora dropout:  0.0841729378197147
lora alpha:  18.287016460002214
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 169,279,488 || all params: 8,199,540,736 || trainable%: 2.0645
length of training data:  9822
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4154, 'grad_norm': 0.41446009278297424, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2731525897979736, 'eval_runtime': 10.6612, 'eval_samples_per_second': 93.798, 'eval_steps_per_second': 5.909, 'epoch': 0.04}
{'loss': 1.2112, 'grad_norm': 0.2141321748495102, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9461879134178162, 'eval_runtime': 10.6737, 'eval_samples_per_second': 93.688, 'eval_steps_per_second': 5.902, 'epoch': 0.08}
{'loss': 0.9593, 'grad_norm': 0.13658389449119568, 'learning_rate': 0.0002872340425531915, 'epoch': 0.12}
{'eval_loss': 0.8948188424110413, 'eval_runtime': 10.6953, 'eval_samples_per_second': 93.499, 'eval_steps_per_second': 5.89, 'epoch': 0.12}
{'loss': 0.9497, 'grad_norm': 0.12841828167438507, 'learning_rate': 0.00027393617021276593, 'epoch': 0.16}
{'eval_loss': 0.8833886981010437, 'eval_runtime': 10.7315, 'eval_samples_per_second': 93.184, 'eval_steps_per_second': 5.871, 'epoch': 0.16}
{'loss': 0.9166, 'grad_norm': 0.13245521485805511, 'learning_rate': 0.0002606382978723404, 'epoch': 0.2}
{'eval_loss': 0.8735696077346802, 'eval_runtime': 10.7563, 'eval_samples_per_second': 92.969, 'eval_steps_per_second': 5.857, 'epoch': 0.2}
{'loss': 0.8945, 'grad_norm': 0.12366515398025513, 'learning_rate': 0.0002473404255319149, 'epoch': 0.24}
{'eval_loss': 0.8661795854568481, 'eval_runtime': 10.7755, 'eval_samples_per_second': 92.803, 'eval_steps_per_second': 5.847, 'epoch': 0.24}
{'loss': 0.9091, 'grad_norm': 0.10673080384731293, 'learning_rate': 0.00023404255319148934, 'epoch': 0.29}
{'eval_loss': 0.8663269281387329, 'eval_runtime': 10.7708, 'eval_samples_per_second': 92.843, 'eval_steps_per_second': 5.849, 'epoch': 0.29}
{'loss': 0.8846, 'grad_norm': 0.12486568838357925, 'learning_rate': 0.0002207446808510638, 'epoch': 0.33}
{'eval_loss': 0.8603219985961914, 'eval_runtime': 10.7666, 'eval_samples_per_second': 92.88, 'eval_steps_per_second': 5.851, 'epoch': 0.33}
{'loss': 0.8873, 'grad_norm': 0.11536497622728348, 'learning_rate': 0.0002074468085106383, 'epoch': 0.37}
{'eval_loss': 0.8562431931495667, 'eval_runtime': 10.7647, 'eval_samples_per_second': 92.897, 'eval_steps_per_second': 5.852, 'epoch': 0.37}
{'loss': 0.8827, 'grad_norm': 0.11470702290534973, 'learning_rate': 0.00019414893617021275, 'epoch': 0.41}
{'eval_loss': 0.8554969429969788, 'eval_runtime': 10.7667, 'eval_samples_per_second': 92.879, 'eval_steps_per_second': 5.851, 'epoch': 0.41}
{'loss': 0.8944, 'grad_norm': 0.12042102217674255, 'learning_rate': 0.0001808510638297872, 'epoch': 0.45}
{'eval_loss': 0.8508074283599854, 'eval_runtime': 10.7739, 'eval_samples_per_second': 92.817, 'eval_steps_per_second': 5.847, 'epoch': 0.45}
{'loss': 0.8916, 'grad_norm': 0.11470456421375275, 'learning_rate': 0.00016755319148936167, 'epoch': 0.49}
{'eval_loss': 0.8484030961990356, 'eval_runtime': 10.8197, 'eval_samples_per_second': 92.424, 'eval_steps_per_second': 5.823, 'epoch': 0.49}
{'loss': 0.8397, 'grad_norm': 0.12942533195018768, 'learning_rate': 0.00015425531914893615, 'epoch': 0.53}
{'eval_loss': 0.8452376127243042, 'eval_runtime': 10.7692, 'eval_samples_per_second': 92.857, 'eval_steps_per_second': 5.85, 'epoch': 0.53}
{'loss': 0.8578, 'grad_norm': 0.1403840333223343, 'learning_rate': 0.0001409574468085106, 'epoch': 0.57}
{'eval_loss': 0.8448930382728577, 'eval_runtime': 10.7701, 'eval_samples_per_second': 92.849, 'eval_steps_per_second': 5.85, 'epoch': 0.57}
{'loss': 0.8694, 'grad_norm': 0.1623140424489975, 'learning_rate': 0.0001276595744680851, 'epoch': 0.61}
{'eval_loss': 0.8462029099464417, 'eval_runtime': 10.835, 'eval_samples_per_second': 92.293, 'eval_steps_per_second': 5.814, 'epoch': 0.61}
{'loss': 0.8223, 'grad_norm': 0.10406092554330826, 'learning_rate': 0.00011436170212765957, 'epoch': 0.65}
{'eval_loss': 0.8434786796569824, 'eval_runtime': 10.8308, 'eval_samples_per_second': 92.329, 'eval_steps_per_second': 5.817, 'epoch': 0.65}
{'loss': 0.8589, 'grad_norm': 0.13252213597297668, 'learning_rate': 0.00010106382978723403, 'epoch': 0.69}
{'eval_loss': 0.8409764766693115, 'eval_runtime': 10.7847, 'eval_samples_per_second': 92.724, 'eval_steps_per_second': 5.842, 'epoch': 0.69}
{'loss': 0.8369, 'grad_norm': 0.1327696293592453, 'learning_rate': 8.77659574468085e-05, 'epoch': 0.73}
{'eval_loss': 0.8380702137947083, 'eval_runtime': 10.7838, 'eval_samples_per_second': 92.732, 'eval_steps_per_second': 5.842, 'epoch': 0.73}
{'loss': 0.8497, 'grad_norm': 0.14174683392047882, 'learning_rate': 7.446808510638297e-05, 'epoch': 0.77}
{'eval_loss': 0.8371264934539795, 'eval_runtime': 10.7937, 'eval_samples_per_second': 92.646, 'eval_steps_per_second': 5.837, 'epoch': 0.77}
{'loss': 0.8411, 'grad_norm': 0.11460115760564804, 'learning_rate': 6.117021276595744e-05, 'epoch': 0.81}
{'eval_loss': 0.8359193801879883, 'eval_runtime': 10.8084, 'eval_samples_per_second': 92.521, 'eval_steps_per_second': 5.829, 'epoch': 0.81}
{'loss': 0.8159, 'grad_norm': 0.1396731436252594, 'learning_rate': 4.787234042553191e-05, 'epoch': 0.86}
{'eval_loss': 0.8350526690483093, 'eval_runtime': 10.7855, 'eval_samples_per_second': 92.717, 'eval_steps_per_second': 5.841, 'epoch': 0.86}
{'loss': 0.8319, 'grad_norm': 0.1315949261188507, 'learning_rate': 3.457446808510638e-05, 'epoch': 0.9}
{'eval_loss': 0.833034336566925, 'eval_runtime': 10.7826, 'eval_samples_per_second': 92.742, 'eval_steps_per_second': 5.843, 'epoch': 0.9}
{'loss': 0.8316, 'grad_norm': 0.133448526263237, 'learning_rate': 2.1276595744680852e-05, 'epoch': 0.94}
{'eval_loss': 0.8338157534599304, 'eval_runtime': 10.7847, 'eval_samples_per_second': 92.724, 'eval_steps_per_second': 5.842, 'epoch': 0.94}
{'loss': 0.8484, 'grad_norm': 0.13805067539215088, 'learning_rate': 7.978723404255319e-06, 'epoch': 0.98}
{'eval_loss': 0.8324068188667297, 'eval_runtime': 10.769, 'eval_samples_per_second': 92.859, 'eval_steps_per_second': 5.85, 'epoch': 0.98}
{'train_runtime': 503.1465, 'train_samples_per_second': 19.521, 'train_steps_per_second': 1.22, 'train_loss': 0.946647984197163, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2731525897979736, 0.9461879134178162, 0.8948188424110413, 0.8833886981010437, 0.8735696077346802, 0.8661795854568481, 0.8663269281387329, 0.8603219985961914, 0.8562431931495667, 0.8554969429969788, 0.8508074283599854, 0.8484030961990356, 0.8452376127243042, 0.8448930382728577, 0.8462029099464417, 0.8434786796569824, 0.8409764766693115, 0.8380702137947083, 0.8371264934539795, 0.8359193801879883, 0.8350526690483093, 0.833034336566925, 0.8338157534599304, 0.8324068188667297], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.2731525897979736, 0.9461879134178162, 0.8948188424110413, 0.8833886981010437, 0.8735696077346802, 0.8661795854568481, 0.8663269281387329, 0.8603219985961914, 0.8562431931495667, 0.8554969429969788, 0.8508074283599854, 0.8484030961990356, 0.8452376127243042, 0.8448930382728577, 0.8462029099464417, 0.8434786796569824, 0.8409764766693115, 0.8380702137947083, 0.8371264934539795, 0.8359193801879883, 0.8350526690483093, 0.833034336566925, 0.8338157534599304, 0.8324068188667297]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.8324068188667297
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8492 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1793595552444458, 0.3586342930793762, 0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.34960928559303284, 0.057854533195495605, 0.7710180282592773, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6546881198883057, 0.3577408194541931, 0.8840495347976685, 0.5400984883308411]  ‚Üí  acq = -0.9627548586008863
X = [0.7988576889038086, 0.1653779149055481, 0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9513463973999023, 0.8898367881774902, 0.2988312840461731, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.24033895134925842, 0.2050917148590088, 0.051226988434791565, 0.8115118741989136]  ‚Üí  acq = -0.963856452564334
X = [0.22445803880691528, 0.07812052965164185, 0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9895481467247009, 0.6373879313468933, 0.9490465521812439, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6339337825775146, 0.44919973611831665, 0.9787859916687012, 0.45290279388427734]  ‚Üí  acq = -0.9636346650207893
X = [0.3654218316078186, 0.6524207592010498, 0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.8971459269523621, 0.9170647859573364, 0.6597838997840881, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.8601893186569214, 0.164650559425354, 0.14350587129592896, 0.7966952323913574]  ‚Üí  acq = -0.9638546511978755
X = [0.4475772976875305, 0.6951091885566711, 0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.06307335197925568, 0.7526708841323853, 0.20946532487869263, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.10499551892280579, 0.07692551612854004, 0.719855546951294, 0.04362046718597412]  ‚Üí  acq = -0.9635731409690302
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2844, dtype=torch.float64), tensor(0.2288, dtype=torch.float64), 0, 0, tensor(0.0663, dtype=torch.float64), tensor(0.3131, dtype=torch.float64), 0, tensor(0.0578, dtype=torch.float64), tensor(0.0495, dtype=torch.float64), 32, 0, 0, 0, 1, 1, 117, 0.021825040286688302, 15.919299942378913, 1]
normalized proposed parameters for next round by BO: [tensor(0.2844, dtype=torch.float64), tensor(0.2288, dtype=torch.float64), tensor(6.0562e-18, dtype=torch.float64), tensor(5.8393e-05, dtype=torch.float64), tensor(0.0663, dtype=torch.float64), tensor(0.3131, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0578, dtype=torch.float64), tensor(0.0495, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9105, dtype=torch.float64), tensor(0.2183, dtype=torch.float64), tensor(0.3317, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.284
  gsm8k: 0.229
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.066
  truthfulqa_gen: 0.313
  wikitext: 0
  mmlu: 0.058
  arc_challenge: 0.05

LoRA Parameters:
  lora_r: (117,)
  lora_dropout: (0.021825040286688302,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (15.919299942378913,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  117
lora dropout:  0.021825040286688302
lora alpha:  15.919299942378913
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 138,018,816 || all params: 8,168,280,064 || trainable%: 1.6897
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0349, 'grad_norm': 0.6462294459342957, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3987128734588623, 'eval_runtime': 10.7821, 'eval_samples_per_second': 92.746, 'eval_steps_per_second': 5.843, 'epoch': 0.04}
{'loss': 1.3019, 'grad_norm': 0.2198316752910614, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9992905259132385, 'eval_runtime': 10.8031, 'eval_samples_per_second': 92.566, 'eval_steps_per_second': 5.832, 'epoch': 0.08}
{'loss': 0.9833, 'grad_norm': 0.15476182103157043, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.929198145866394, 'eval_runtime': 10.8352, 'eval_samples_per_second': 92.292, 'eval_steps_per_second': 5.814, 'epoch': 0.12}
{'loss': 0.9295, 'grad_norm': 0.1290929615497589, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.910017728805542, 'eval_runtime': 10.8538, 'eval_samples_per_second': 92.134, 'eval_steps_per_second': 5.804, 'epoch': 0.16}
{'loss': 0.9106, 'grad_norm': 0.14504314959049225, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9037194848060608, 'eval_runtime': 10.8716, 'eval_samples_per_second': 91.983, 'eval_steps_per_second': 5.795, 'epoch': 0.2}
{'loss': 0.8936, 'grad_norm': 0.1499309092760086, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.895057737827301, 'eval_runtime': 10.8944, 'eval_samples_per_second': 91.79, 'eval_steps_per_second': 5.783, 'epoch': 0.24}
{'loss': 0.8964, 'grad_norm': 0.11906144767999649, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8886522054672241, 'eval_runtime': 10.9089, 'eval_samples_per_second': 91.668, 'eval_steps_per_second': 5.775, 'epoch': 0.28}
{'loss': 0.8845, 'grad_norm': 0.12438005954027176, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8824883103370667, 'eval_runtime': 10.9059, 'eval_samples_per_second': 91.694, 'eval_steps_per_second': 5.777, 'epoch': 0.32}
{'loss': 0.8832, 'grad_norm': 0.12364491820335388, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8868834376335144, 'eval_runtime': 10.9106, 'eval_samples_per_second': 91.654, 'eval_steps_per_second': 5.774, 'epoch': 0.36}
{'loss': 0.9042, 'grad_norm': 0.1615668535232544, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8748242259025574, 'eval_runtime': 10.908, 'eval_samples_per_second': 91.675, 'eval_steps_per_second': 5.776, 'epoch': 0.4}
{'loss': 0.8692, 'grad_norm': 0.13254600763320923, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8793935775756836, 'eval_runtime': 10.9023, 'eval_samples_per_second': 91.724, 'eval_steps_per_second': 5.779, 'epoch': 0.44}
{'loss': 0.8343, 'grad_norm': 0.17892447113990784, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8740134239196777, 'eval_runtime': 10.9055, 'eval_samples_per_second': 91.697, 'eval_steps_per_second': 5.777, 'epoch': 0.48}
{'loss': 0.8249, 'grad_norm': 0.13937433063983917, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8687517642974854, 'eval_runtime': 10.9077, 'eval_samples_per_second': 91.678, 'eval_steps_per_second': 5.776, 'epoch': 0.52}
{'loss': 0.8313, 'grad_norm': 0.14588037133216858, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8670017123222351, 'eval_runtime': 10.9489, 'eval_samples_per_second': 91.333, 'eval_steps_per_second': 5.754, 'epoch': 0.56}
{'loss': 0.8282, 'grad_norm': 0.15071164071559906, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8695781826972961, 'eval_runtime': 10.911, 'eval_samples_per_second': 91.651, 'eval_steps_per_second': 5.774, 'epoch': 0.6}
{'loss': 0.8293, 'grad_norm': 0.13436101377010345, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8657081127166748, 'eval_runtime': 10.8913, 'eval_samples_per_second': 91.817, 'eval_steps_per_second': 5.784, 'epoch': 0.64}
{'loss': 0.7956, 'grad_norm': 0.1182672306895256, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8645606637001038, 'eval_runtime': 10.8847, 'eval_samples_per_second': 91.872, 'eval_steps_per_second': 5.788, 'epoch': 0.68}
{'loss': 0.8255, 'grad_norm': 0.23481914401054382, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8626567125320435, 'eval_runtime': 10.8865, 'eval_samples_per_second': 91.857, 'eval_steps_per_second': 5.787, 'epoch': 0.72}
{'loss': 0.8033, 'grad_norm': 0.14794859290122986, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.859548807144165, 'eval_runtime': 10.8828, 'eval_samples_per_second': 91.888, 'eval_steps_per_second': 5.789, 'epoch': 0.76}
{'loss': 0.7463, 'grad_norm': 0.16580507159233093, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8587130904197693, 'eval_runtime': 10.8818, 'eval_samples_per_second': 91.896, 'eval_steps_per_second': 5.789, 'epoch': 0.8}
{'loss': 0.7938, 'grad_norm': 0.18133704364299774, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8602666854858398, 'eval_runtime': 10.8747, 'eval_samples_per_second': 91.956, 'eval_steps_per_second': 5.793, 'epoch': 0.84}
{'loss': 0.7704, 'grad_norm': 0.12738636136054993, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8593102693557739, 'eval_runtime': 10.868, 'eval_samples_per_second': 92.013, 'eval_steps_per_second': 5.797, 'epoch': 0.88}
{'loss': 0.7719, 'grad_norm': 0.16544686257839203, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8563323020935059, 'eval_runtime': 10.8658, 'eval_samples_per_second': 92.032, 'eval_steps_per_second': 5.798, 'epoch': 0.92}
{'loss': 0.8021, 'grad_norm': 0.14254848659038544, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8566818833351135, 'eval_runtime': 10.8759, 'eval_samples_per_second': 91.947, 'eval_steps_per_second': 5.793, 'epoch': 0.96}
{'loss': 0.7787, 'grad_norm': 0.18144264817237854, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.855530858039856, 'eval_runtime': 10.8918, 'eval_samples_per_second': 91.812, 'eval_steps_per_second': 5.784, 'epoch': 1.0}
{'train_runtime': 500.6605, 'train_samples_per_second': 19.968, 'train_steps_per_second': 1.248, 'train_loss': 0.9490709320068359, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3987128734588623, 0.9992905259132385, 0.929198145866394, 0.910017728805542, 0.9037194848060608, 0.895057737827301, 0.8886522054672241, 0.8824883103370667, 0.8868834376335144, 0.8748242259025574, 0.8793935775756836, 0.8740134239196777, 0.8687517642974854, 0.8670017123222351, 0.8695781826972961, 0.8657081127166748, 0.8645606637001038, 0.8626567125320435, 0.859548807144165, 0.8587130904197693, 0.8602666854858398, 0.8593102693557739, 0.8563323020935059, 0.8566818833351135, 0.855530858039856], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3987128734588623, 0.9992905259132385, 0.929198145866394, 0.910017728805542, 0.9037194848060608, 0.895057737827301, 0.8886522054672241, 0.8824883103370667, 0.8868834376335144, 0.8748242259025574, 0.8793935775756836, 0.8740134239196777, 0.8687517642974854, 0.8670017123222351, 0.8695781826972961, 0.8657081127166748, 0.8645606637001038, 0.8626567125320435, 0.859548807144165, 0.8587130904197693, 0.8602666854858398, 0.8593102693557739, 0.8563323020935059, 0.8566818833351135, 0.855530858039856]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.855530858039856
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318, -1.020788550376892, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.0369 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -0.9843775106377373
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -0.9718251786981985
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -0.9586322778419734
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -0.970934090297344
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -0.9716784744875793
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0550, dtype=torch.float64), tensor(0.2430, dtype=torch.float64), tensor(0.0306, dtype=torch.float64), 0, tensor(0.0152, dtype=torch.float64), tensor(0.2607, dtype=torch.float64), 0, tensor(0.1866, dtype=torch.float64), tensor(0.2077, dtype=torch.float64), 5, 1, 1, 0, 1, 0, 38, 0.009096640353116017, 20.169365223798362, 0]
normalized proposed parameters for next round by BO: [tensor(0.0550, dtype=torch.float64), tensor(0.2430, dtype=torch.float64), tensor(0.0306, dtype=torch.float64), tensor(0.0013, dtype=torch.float64), tensor(0.0152, dtype=torch.float64), tensor(0.2607, dtype=torch.float64), tensor(4.1320e-19, dtype=torch.float64), tensor(0.1866, dtype=torch.float64), tensor(0.2077, dtype=torch.float64), tensor(0.1596, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2966, dtype=torch.float64), tensor(0.0910, dtype=torch.float64), tensor(0.4202, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.055
  gsm8k: 0.243
  rowan_hellaswag: 0.031
  sciq: 0
  triviaqa: 0.015
  truthfulqa_gen: 0.261
  wikitext: 0
  mmlu: 0.187
  arc_challenge: 0.208

LoRA Parameters:
  lora_r: (38,)
  lora_dropout: (0.009096640353116017,)
  num_layers_to_apply: (5,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (20.169365223798362,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  5
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  38
lora dropout:  0.009096640353116017
lora alpha:  20.169365223798362
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 6,031,360 || all params: 8,036,292,608 || trainable%: 0.0751
length of training data:  9984
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3939, 'grad_norm': 1.2180912494659424, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7384792566299438, 'eval_runtime': 9.0319, 'eval_samples_per_second': 110.718, 'eval_steps_per_second': 6.975, 'epoch': 0.04}
{'loss': 1.7821, 'grad_norm': 0.5863512754440308, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.171932339668274, 'eval_runtime': 9.1276, 'eval_samples_per_second': 109.558, 'eval_steps_per_second': 6.902, 'epoch': 0.08}
{'loss': 1.3193, 'grad_norm': 0.5017686486244202, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.0685184001922607, 'eval_runtime': 9.0977, 'eval_samples_per_second': 109.918, 'eval_steps_per_second': 6.925, 'epoch': 0.12}
{'loss': 1.2316, 'grad_norm': 0.23992057144641876, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.0161871910095215, 'eval_runtime': 9.1662, 'eval_samples_per_second': 109.096, 'eval_steps_per_second': 6.873, 'epoch': 0.16}
{'loss': 1.2559, 'grad_norm': 0.2790292203426361, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.004628300666809, 'eval_runtime': 9.1781, 'eval_samples_per_second': 108.955, 'eval_steps_per_second': 6.864, 'epoch': 0.2}
{'loss': 1.178, 'grad_norm': 0.2721153795719147, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.9905234575271606, 'eval_runtime': 9.2124, 'eval_samples_per_second': 108.549, 'eval_steps_per_second': 6.839, 'epoch': 0.24}
{'loss': 1.1546, 'grad_norm': 0.2743780314922333, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.978426992893219, 'eval_runtime': 9.1966, 'eval_samples_per_second': 108.736, 'eval_steps_per_second': 6.85, 'epoch': 0.28}
{'loss': 1.0595, 'grad_norm': 0.2735997438430786, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.9736696481704712, 'eval_runtime': 9.1902, 'eval_samples_per_second': 108.812, 'eval_steps_per_second': 6.855, 'epoch': 0.32}
{'loss': 1.1233, 'grad_norm': 0.2631358802318573, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.9699763059616089, 'eval_runtime': 9.1969, 'eval_samples_per_second': 108.732, 'eval_steps_per_second': 6.85, 'epoch': 0.36}
{'loss': 1.1013, 'grad_norm': 0.43332740664482117, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.9633829593658447, 'eval_runtime': 9.2219, 'eval_samples_per_second': 108.438, 'eval_steps_per_second': 6.832, 'epoch': 0.4}
{'loss': 1.1263, 'grad_norm': 0.259052574634552, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.9617953896522522, 'eval_runtime': 9.1997, 'eval_samples_per_second': 108.699, 'eval_steps_per_second': 6.848, 'epoch': 0.44}
{'loss': 1.0681, 'grad_norm': 0.2682790756225586, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.959386944770813, 'eval_runtime': 9.2145, 'eval_samples_per_second': 108.524, 'eval_steps_per_second': 6.837, 'epoch': 0.48}
{'loss': 1.0806, 'grad_norm': 0.2490147054195404, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.957404375076294, 'eval_runtime': 9.2216, 'eval_samples_per_second': 108.441, 'eval_steps_per_second': 6.832, 'epoch': 0.52}
{'loss': 1.0743, 'grad_norm': 0.24042488634586334, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.9587350487709045, 'eval_runtime': 9.2208, 'eval_samples_per_second': 108.45, 'eval_steps_per_second': 6.832, 'epoch': 0.56}
{'loss': 1.063, 'grad_norm': 0.2513265311717987, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.9538483023643494, 'eval_runtime': 9.2049, 'eval_samples_per_second': 108.638, 'eval_steps_per_second': 6.844, 'epoch': 0.6}
{'loss': 1.0608, 'grad_norm': 0.23732487857341766, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.9533752799034119, 'eval_runtime': 9.2052, 'eval_samples_per_second': 108.635, 'eval_steps_per_second': 6.844, 'epoch': 0.64}
{'loss': 1.0812, 'grad_norm': 0.24299557507038116, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.9532835483551025, 'eval_runtime': 9.1948, 'eval_samples_per_second': 108.758, 'eval_steps_per_second': 6.852, 'epoch': 0.68}
{'loss': 1.1075, 'grad_norm': 0.26027175784111023, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.9507724046707153, 'eval_runtime': 9.2108, 'eval_samples_per_second': 108.569, 'eval_steps_per_second': 6.84, 'epoch': 0.72}
{'loss': 1.1138, 'grad_norm': 0.2788717448711395, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.9515041708946228, 'eval_runtime': 9.2194, 'eval_samples_per_second': 108.467, 'eval_steps_per_second': 6.833, 'epoch': 0.76}
{'loss': 1.0718, 'grad_norm': 0.25709858536720276, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.9499499797821045, 'eval_runtime': 9.2069, 'eval_samples_per_second': 108.615, 'eval_steps_per_second': 6.843, 'epoch': 0.8}
{'loss': 1.101, 'grad_norm': 0.26314711570739746, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.9489954710006714, 'eval_runtime': 9.2235, 'eval_samples_per_second': 108.418, 'eval_steps_per_second': 6.83, 'epoch': 0.84}
{'loss': 1.0386, 'grad_norm': 0.2540677785873413, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.9463537931442261, 'eval_runtime': 9.2123, 'eval_samples_per_second': 108.551, 'eval_steps_per_second': 6.839, 'epoch': 0.88}
{'loss': 1.037, 'grad_norm': 0.2633112967014313, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.9466654062271118, 'eval_runtime': 9.192, 'eval_samples_per_second': 108.79, 'eval_steps_per_second': 6.854, 'epoch': 0.92}
{'loss': 1.0655, 'grad_norm': 0.23957467079162598, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.9454211592674255, 'eval_runtime': 9.1744, 'eval_samples_per_second': 108.999, 'eval_steps_per_second': 6.867, 'epoch': 0.96}
{'train_runtime': 334.1865, 'train_samples_per_second': 29.876, 'train_steps_per_second': 1.867, 'train_loss': 1.229831982881595, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7384792566299438, 1.171932339668274, 1.0685184001922607, 1.0161871910095215, 1.004628300666809, 0.9905234575271606, 0.978426992893219, 0.9736696481704712, 0.9699763059616089, 0.9633829593658447, 0.9617953896522522, 0.959386944770813, 0.957404375076294, 0.9587350487709045, 0.9538483023643494, 0.9533752799034119, 0.9532835483551025, 0.9507724046707153, 0.9515041708946228, 0.9499499797821045, 0.9489954710006714, 0.9463537931442261, 0.9466654062271118, 0.9454211592674255], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.7384792566299438, 1.171932339668274, 1.0685184001922607, 1.0161871910095215, 1.004628300666809, 0.9905234575271606, 0.978426992893219, 0.9736696481704712, 0.9699763059616089, 0.9633829593658447, 0.9617953896522522, 0.959386944770813, 0.957404375076294, 0.9587350487709045, 0.9538483023643494, 0.9533752799034119, 0.9532835483551025, 0.9507724046707153, 0.9515041708946228, 0.9499499797821045, 0.9489954710006714, 0.9463537931442261, 0.9466654062271118, 0.9454211592674255]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.666443943977356
current iteration best possible eval_loss (full train run):  -0.9454211592674255
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318, -1.020788550376892, -1.020788550376892, -1.666443943977356]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2902 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3668936491012573, 0.891264021396637, 0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.8238186836242676, 0.9171960949897766, 0.8835806846618652, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.45111533999443054, 0.2139490246772766, 0.9017742872238159, 0.11642980575561523]  ‚Üí  acq = -0.9560184838377119
X = [0.10986042022705078, 0.20953714847564697, 0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9494266510009766, 0.8764203786849976, 0.9284661412239075, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6602510213851929, 0.18004006147384644, 0.9259414672851562, 0.9654239416122437]  ‚Üí  acq = -0.9613597029515085
X = [0.17275136709213257, 0.6849347949028015, 0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.31699109077453613, 0.9090394377708435, 0.2634289264678955, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.03518377244472504, 0.6907520294189453, 0.12004825472831726, 0.8972193002700806]  ‚Üí  acq = -0.9614228472546928
X = [0.6180925965309143, 0.16254359483718872, 0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4304628074169159, 0.8458959460258484, 0.5030843019485474, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.715866208076477, 0.002808213233947754, 0.6724920272827148, 0.5655561685562134]  ‚Üí  acq = -0.9600395466664299
X = [0.32551831007003784, 0.7355300188064575, 0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.8032635450363159, 0.7881297469139099, 0.10880237817764282, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.07442650943994522, 0.0027261972427368164, 0.3981722295284271, 0.7018656134605408]  ‚Üí  acq = -0.9371876468554672
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0764, dtype=torch.float64), tensor(0.3225, dtype=torch.float64), tensor(0.0232, dtype=torch.float64), 0, tensor(0.1227, dtype=torch.float64), tensor(0.0892, dtype=torch.float64), tensor(0.1724, dtype=torch.float64), 0, tensor(0.1791, dtype=torch.float64), 14, 1, 1, 1, 0, 0, 116, 0.08100415430545024, 10.255576211882236, 0]
normalized proposed parameters for next round by BO: [tensor(0.0764, dtype=torch.float64), tensor(0.3225, dtype=torch.float64), tensor(0.0232, dtype=torch.float64), tensor(0.0091, dtype=torch.float64), tensor(0.1227, dtype=torch.float64), tensor(0.0892, dtype=torch.float64), tensor(0.1724, dtype=torch.float64), tensor(0.0054, dtype=torch.float64), tensor(0.1791, dtype=torch.float64), tensor(0.4438, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9041, dtype=torch.float64), tensor(0.8100, dtype=torch.float64), tensor(0.2137, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.076
  gsm8k: 0.322
  rowan_hellaswag: 0.023
  sciq: 0
  triviaqa: 0.123
  truthfulqa_gen: 0.089
  wikitext: 0.172
  mmlu: 0
  arc_challenge: 0.179

LoRA Parameters:
  lora_r: (116,)
  lora_dropout: (0.08100415430545024,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (10.255576211882236,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  116
lora dropout:  0.08100415430545024
lora alpha:  10.255576211882236
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 51,552,256 || all params: 8,081,813,504 || trainable%: 0.6379
length of training data:  9852
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2375, 'grad_norm': 0.2681284546852112, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7812021970748901, 'eval_runtime': 9.4723, 'eval_samples_per_second': 105.571, 'eval_steps_per_second': 6.651, 'epoch': 0.04}
{'loss': 1.82, 'grad_norm': 0.15113908052444458, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2220109701156616, 'eval_runtime': 9.4886, 'eval_samples_per_second': 105.39, 'eval_steps_per_second': 6.64, 'epoch': 0.08}
{'loss': 1.4481, 'grad_norm': 0.1315450370311737, 'learning_rate': 0.0002872791519434629, 'epoch': 0.12}
{'eval_loss': 1.084485650062561, 'eval_runtime': 9.5195, 'eval_samples_per_second': 105.048, 'eval_steps_per_second': 6.618, 'epoch': 0.12}
{'loss': 1.3125, 'grad_norm': 0.10570721328258514, 'learning_rate': 0.00027402826855123675, 'epoch': 0.16}
{'eval_loss': 1.0541026592254639, 'eval_runtime': 9.5556, 'eval_samples_per_second': 104.651, 'eval_steps_per_second': 6.593, 'epoch': 0.16}
{'loss': 1.3094, 'grad_norm': 0.11152403056621552, 'learning_rate': 0.00026077738515901055, 'epoch': 0.2}
{'eval_loss': 1.0305366516113281, 'eval_runtime': 9.577, 'eval_samples_per_second': 104.417, 'eval_steps_per_second': 6.578, 'epoch': 0.2}
{'loss': 1.3032, 'grad_norm': 0.12647809088230133, 'learning_rate': 0.0002475265017667844, 'epoch': 0.24}
{'eval_loss': 1.0125163793563843, 'eval_runtime': 9.5866, 'eval_samples_per_second': 104.312, 'eval_steps_per_second': 6.572, 'epoch': 0.24}
{'loss': 1.2892, 'grad_norm': 0.10398893803358078, 'learning_rate': 0.00023427561837455828, 'epoch': 0.28}
{'eval_loss': 0.9922001361846924, 'eval_runtime': 9.5748, 'eval_samples_per_second': 104.44, 'eval_steps_per_second': 6.58, 'epoch': 0.28}
{'loss': 1.2344, 'grad_norm': 0.10860585421323776, 'learning_rate': 0.00022102473498233213, 'epoch': 0.32}
{'eval_loss': 0.9760949611663818, 'eval_runtime': 9.5825, 'eval_samples_per_second': 104.357, 'eval_steps_per_second': 6.574, 'epoch': 0.32}
{'loss': 1.254, 'grad_norm': 0.11715071648359299, 'learning_rate': 0.00020777385159010599, 'epoch': 0.37}
{'eval_loss': 0.9620427489280701, 'eval_runtime': 9.5853, 'eval_samples_per_second': 104.327, 'eval_steps_per_second': 6.573, 'epoch': 0.37}
{'loss': 1.1502, 'grad_norm': 0.11822349578142166, 'learning_rate': 0.00019452296819787987, 'epoch': 0.41}
{'eval_loss': 0.955166757106781, 'eval_runtime': 9.5773, 'eval_samples_per_second': 104.413, 'eval_steps_per_second': 6.578, 'epoch': 0.41}
{'loss': 1.1601, 'grad_norm': 0.10852397233247757, 'learning_rate': 0.0001812720848056537, 'epoch': 0.45}
{'eval_loss': 0.9524809718132019, 'eval_runtime': 9.5854, 'eval_samples_per_second': 104.325, 'eval_steps_per_second': 6.572, 'epoch': 0.45}
{'loss': 1.1793, 'grad_norm': 0.11854300647974014, 'learning_rate': 0.00016802120141342754, 'epoch': 0.49}
{'eval_loss': 0.9450456500053406, 'eval_runtime': 9.5861, 'eval_samples_per_second': 104.318, 'eval_steps_per_second': 6.572, 'epoch': 0.49}
{'loss': 1.1017, 'grad_norm': 0.12049058824777603, 'learning_rate': 0.0001547703180212014, 'epoch': 0.53}
{'eval_loss': 0.939735472202301, 'eval_runtime': 9.5823, 'eval_samples_per_second': 104.359, 'eval_steps_per_second': 6.575, 'epoch': 0.53}
{'loss': 1.154, 'grad_norm': 0.09916909784078598, 'learning_rate': 0.00014151943462897525, 'epoch': 0.57}
{'eval_loss': 0.9319660663604736, 'eval_runtime': 9.62, 'eval_samples_per_second': 103.95, 'eval_steps_per_second': 6.549, 'epoch': 0.57}
{'loss': 1.1756, 'grad_norm': 0.12651987373828888, 'learning_rate': 0.0001282685512367491, 'epoch': 0.61}
{'eval_loss': 0.9225872755050659, 'eval_runtime': 9.6059, 'eval_samples_per_second': 104.103, 'eval_steps_per_second': 6.558, 'epoch': 0.61}
{'loss': 1.1284, 'grad_norm': 0.11492276191711426, 'learning_rate': 0.00011501766784452296, 'epoch': 0.65}
{'eval_loss': 0.9143834710121155, 'eval_runtime': 9.6093, 'eval_samples_per_second': 104.066, 'eval_steps_per_second': 6.556, 'epoch': 0.65}
{'loss': 1.1082, 'grad_norm': 0.12029682099819183, 'learning_rate': 0.00010176678445229682, 'epoch': 0.69}
{'eval_loss': 0.9097728729248047, 'eval_runtime': 9.5936, 'eval_samples_per_second': 104.237, 'eval_steps_per_second': 6.567, 'epoch': 0.69}
{'loss': 1.1205, 'grad_norm': 0.15353696048259735, 'learning_rate': 8.851590106007066e-05, 'epoch': 0.73}
{'eval_loss': 0.9053876399993896, 'eval_runtime': 9.5988, 'eval_samples_per_second': 104.18, 'eval_steps_per_second': 6.563, 'epoch': 0.73}
{'loss': 1.0842, 'grad_norm': 0.11381521075963974, 'learning_rate': 7.526501766784451e-05, 'epoch': 0.77}
{'eval_loss': 0.9027532935142517, 'eval_runtime': 9.5975, 'eval_samples_per_second': 104.194, 'eval_steps_per_second': 6.564, 'epoch': 0.77}
{'loss': 1.0795, 'grad_norm': 0.10568395256996155, 'learning_rate': 6.201413427561837e-05, 'epoch': 0.81}
{'eval_loss': 0.9015023112297058, 'eval_runtime': 9.5972, 'eval_samples_per_second': 104.197, 'eval_steps_per_second': 6.564, 'epoch': 0.81}
{'loss': 1.1321, 'grad_norm': 0.1187426969408989, 'learning_rate': 4.876325088339222e-05, 'epoch': 0.85}
{'eval_loss': 0.9002514481544495, 'eval_runtime': 9.6042, 'eval_samples_per_second': 104.121, 'eval_steps_per_second': 6.56, 'epoch': 0.85}
{'loss': 1.0891, 'grad_norm': 0.11700420826673508, 'learning_rate': 3.551236749116607e-05, 'epoch': 0.89}
{'eval_loss': 0.8996109366416931, 'eval_runtime': 9.6173, 'eval_samples_per_second': 103.979, 'eval_steps_per_second': 6.551, 'epoch': 0.89}
{'loss': 1.125, 'grad_norm': 0.1124875470995903, 'learning_rate': 2.2261484098939926e-05, 'epoch': 0.93}
{'eval_loss': 0.8989770412445068, 'eval_runtime': 9.5953, 'eval_samples_per_second': 104.218, 'eval_steps_per_second': 6.566, 'epoch': 0.93}
{'loss': 1.0629, 'grad_norm': 0.12719419598579407, 'learning_rate': 9.010600706713779e-06, 'epoch': 0.97}
{'eval_loss': 0.8985024690628052, 'eval_runtime': 9.5982, 'eval_samples_per_second': 104.187, 'eval_steps_per_second': 6.564, 'epoch': 0.97}
{'train_runtime': 378.6152, 'train_samples_per_second': 26.021, 'train_steps_per_second': 1.627, 'train_loss': 1.287625941363248, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7812021970748901, 1.2220109701156616, 1.084485650062561, 1.0541026592254639, 1.0305366516113281, 1.0125163793563843, 0.9922001361846924, 0.9760949611663818, 0.9620427489280701, 0.955166757106781, 0.9524809718132019, 0.9450456500053406, 0.939735472202301, 0.9319660663604736, 0.9225872755050659, 0.9143834710121155, 0.9097728729248047, 0.9053876399993896, 0.9027532935142517, 0.9015023112297058, 0.9002514481544495, 0.8996109366416931, 0.8989770412445068, 0.8985024690628052], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.7812021970748901, 1.2220109701156616, 1.084485650062561, 1.0541026592254639, 1.0305366516113281, 1.0125163793563843, 0.9922001361846924, 0.9760949611663818, 0.9620427489280701, 0.955166757106781, 0.9524809718132019, 0.9450456500053406, 0.939735472202301, 0.9319660663604736, 0.9225872755050659, 0.9143834710121155, 0.9097728729248047, 0.9053876399993896, 0.9027532935142517, 0.9015023112297058, 0.9002514481544495, 0.8996109366416931, 0.8989770412445068, 0.8985024690628052]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.163820505142212
current iteration best possible eval_loss (full train run):  -0.8985024690628052
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318, -1.020788550376892, -1.020788550376892, -1.666443943977356, -1.163820505142212]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9221 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.08295202255249023, 0.8953140377998352, 0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.36218807101249695, 0.6549691557884216, 0.8173236846923828, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3347404897212982, 0.355441689491272, 0.4367692470550537, 0.45111382007598877]  ‚Üí  acq = -1.009981529242513
X = [0.35591208934783936, 0.098782479763031, 0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.36877870559692383, 0.8672244548797607, 0.5090312361717224, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9806126356124878, 0.5262150168418884, 0.617688775062561, 0.313107967376709]  ‚Üí  acq = -1.009981526921516
X = [0.2322162389755249, 0.03715479373931885, 0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.40351834893226624, 0.9447525143623352, 0.40425533056259155, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9491793513298035, 0.21524584293365479, 0.22934073209762573, 0.15074169635772705]  ‚Üí  acq = -1.0099820559206099
X = [0.5550482869148254, 0.7731989026069641, 0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6225687265396118, 0.3806919455528259, 0.9421022534370422, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7150893807411194, 0.5437468886375427, 0.6237008571624756, 0.9093855619430542]  ‚Üí  acq = -1.0099815281383724
X = [0.7380771636962891, 0.9890440702438354, 0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.4263235926628113, 0.9630946516990662, 0.6111319065093994, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.834472119808197, 0.3034810423851013, 0.033137477934360504, 0.012760698795318604]  ‚Üí  acq = -1.0099815292147796
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1768, dtype=torch.float64), 0, tensor(0.0801, dtype=torch.float64), 0, tensor(0.4852, dtype=torch.float64), tensor(0.1671, dtype=torch.float64), tensor(0.0908, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 119, 0.07014613903862053, 1.4800000190734868, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1768, dtype=torch.float64), tensor(7.7928e-18, dtype=torch.float64), tensor(0.0801, dtype=torch.float64), tensor(2.0725e-17, dtype=torch.float64), tensor(0.4852, dtype=torch.float64), tensor(0.1671, dtype=torch.float64), tensor(0.0908, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9305, dtype=torch.float64), tensor(0.7015, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.177
  sciq: 0
  triviaqa: 0.08
  truthfulqa_gen: 0
  wikitext: 0.485
  mmlu: 0.167
  arc_challenge: 0.091

LoRA Parameters:
  lora_r: (119,)
  lora_dropout: (0.07014613903862053,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  119
lora dropout:  0.07014613903862053
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 159,875,072 || all params: 8,190,136,320 || trainable%: 1.9520
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.6884, 'grad_norm': 0.23984885215759277, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9815027713775635, 'eval_runtime': 11.2078, 'eval_samples_per_second': 89.223, 'eval_steps_per_second': 5.621, 'epoch': 0.04}
{'loss': 2.6674, 'grad_norm': 0.15657424926757812, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.5197453498840332, 'eval_runtime': 11.2086, 'eval_samples_per_second': 89.217, 'eval_steps_per_second': 5.621, 'epoch': 0.08}
{'loss': 2.0127, 'grad_norm': 0.11922655999660492, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4935801029205322, 'eval_runtime': 11.2179, 'eval_samples_per_second': 89.144, 'eval_steps_per_second': 5.616, 'epoch': 0.12}
{'loss': 1.8107, 'grad_norm': 0.1515258252620697, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4993325471878052, 'eval_runtime': 11.2189, 'eval_samples_per_second': 89.135, 'eval_steps_per_second': 5.616, 'epoch': 0.16}
{'loss': 1.7802, 'grad_norm': 0.15533697605133057, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.474176287651062, 'eval_runtime': 11.2093, 'eval_samples_per_second': 89.211, 'eval_steps_per_second': 5.62, 'epoch': 0.2}
{'loss': 1.7514, 'grad_norm': 0.06991933286190033, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.467157244682312, 'eval_runtime': 11.2245, 'eval_samples_per_second': 89.091, 'eval_steps_per_second': 5.613, 'epoch': 0.24}
{'loss': 1.7047, 'grad_norm': 0.077790267765522, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4356127977371216, 'eval_runtime': 11.2196, 'eval_samples_per_second': 89.13, 'eval_steps_per_second': 5.615, 'epoch': 0.28}
{'loss': 1.6586, 'grad_norm': 0.08325065672397614, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4402275085449219, 'eval_runtime': 11.2213, 'eval_samples_per_second': 89.116, 'eval_steps_per_second': 5.614, 'epoch': 0.32}
{'loss': 1.673, 'grad_norm': 0.07964516431093216, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.445186734199524, 'eval_runtime': 11.2271, 'eval_samples_per_second': 89.07, 'eval_steps_per_second': 5.611, 'epoch': 0.36}
{'loss': 1.643, 'grad_norm': 0.05612229183316231, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4351015090942383, 'eval_runtime': 11.237, 'eval_samples_per_second': 88.992, 'eval_steps_per_second': 5.607, 'epoch': 0.4}
{'loss': 1.6634, 'grad_norm': 0.06282836943864822, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4472365379333496, 'eval_runtime': 11.2672, 'eval_samples_per_second': 88.754, 'eval_steps_per_second': 5.591, 'epoch': 0.44}
{'loss': 1.6744, 'grad_norm': 0.053201764822006226, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.435070276260376, 'eval_runtime': 11.2439, 'eval_samples_per_second': 88.937, 'eval_steps_per_second': 5.603, 'epoch': 0.48}
{'loss': 1.722, 'grad_norm': 0.1064014658331871, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4377681016921997, 'eval_runtime': 11.2343, 'eval_samples_per_second': 89.013, 'eval_steps_per_second': 5.608, 'epoch': 0.52}
{'loss': 1.7158, 'grad_norm': 0.0594903826713562, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.446366548538208, 'eval_runtime': 11.2365, 'eval_samples_per_second': 88.996, 'eval_steps_per_second': 5.607, 'epoch': 0.56}
{'loss': 1.628, 'grad_norm': 0.08830321580171585, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4426991939544678, 'eval_runtime': 11.2541, 'eval_samples_per_second': 88.857, 'eval_steps_per_second': 5.598, 'epoch': 0.6}
{'loss': 1.6287, 'grad_norm': 0.07007954269647598, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4402387142181396, 'eval_runtime': 11.2426, 'eval_samples_per_second': 88.948, 'eval_steps_per_second': 5.604, 'epoch': 0.64}
{'loss': 1.7087, 'grad_norm': 0.07064613699913025, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.436044454574585, 'eval_runtime': 11.2179, 'eval_samples_per_second': 89.143, 'eval_steps_per_second': 5.616, 'epoch': 0.68}
{'loss': 1.653, 'grad_norm': 0.06537078320980072, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.433499813079834, 'eval_runtime': 11.2587, 'eval_samples_per_second': 88.82, 'eval_steps_per_second': 5.596, 'epoch': 0.72}
{'loss': 1.6737, 'grad_norm': 0.07733023911714554, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4399851560592651, 'eval_runtime': 11.3284, 'eval_samples_per_second': 88.274, 'eval_steps_per_second': 5.561, 'epoch': 0.76}
{'loss': 1.6737, 'grad_norm': 0.08888420462608337, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4482232332229614, 'eval_runtime': 11.2951, 'eval_samples_per_second': 88.534, 'eval_steps_per_second': 5.578, 'epoch': 0.8}
{'loss': 1.6862, 'grad_norm': 0.07163732498884201, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.439294695854187, 'eval_runtime': 11.2163, 'eval_samples_per_second': 89.156, 'eval_steps_per_second': 5.617, 'epoch': 0.84}
{'loss': 1.6986, 'grad_norm': 0.06346020102500916, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.444624900817871, 'eval_runtime': 11.2195, 'eval_samples_per_second': 89.13, 'eval_steps_per_second': 5.615, 'epoch': 0.88}
{'loss': 1.6859, 'grad_norm': 0.06876926869153976, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.440866470336914, 'eval_runtime': 11.2131, 'eval_samples_per_second': 89.182, 'eval_steps_per_second': 5.618, 'epoch': 0.92}
{'loss': 1.6497, 'grad_norm': 0.08235187828540802, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4402738809585571, 'eval_runtime': 11.227, 'eval_samples_per_second': 89.071, 'eval_steps_per_second': 5.611, 'epoch': 0.96}
{'loss': 1.6901, 'grad_norm': 0.07475141435861588, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4410817623138428, 'eval_runtime': 11.2143, 'eval_samples_per_second': 89.172, 'eval_steps_per_second': 5.618, 'epoch': 1.0}
{'train_runtime': 541.6664, 'train_samples_per_second': 18.456, 'train_steps_per_second': 1.154, 'train_loss': 1.821683563232422, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9815027713775635, 1.5197453498840332, 1.4935801029205322, 1.4993325471878052, 1.474176287651062, 1.467157244682312, 1.4356127977371216, 1.4402275085449219, 1.445186734199524, 1.4351015090942383, 1.4472365379333496, 1.435070276260376, 1.4377681016921997, 1.446366548538208, 1.4426991939544678, 1.4402387142181396, 1.436044454574585, 1.433499813079834, 1.4399851560592651, 1.4482232332229614, 1.439294695854187, 1.444624900817871, 1.440866470336914, 1.4402738809585571, 1.4410817623138428], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9815027713775635, 1.5197453498840332, 1.4935801029205322, 1.4993325471878052, 1.474176287651062, 1.467157244682312, 1.4356127977371216, 1.4402275085449219, 1.445186734199524, 1.4351015090942383, 1.4472365379333496, 1.435070276260376, 1.4377681016921997, 1.446366548538208, 1.4426991939544678, 1.4402387142181396, 1.436044454574585, 1.433499813079834, 1.4399851560592651, 1.4482232332229614, 1.439294695854187, 1.444624900817871, 1.440866470336914, 1.4402738809585571, 1.4410817623138428]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -1.4410817623138428
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318, -1.020788550376892, -1.020788550376892, -1.666443943977356, -1.163820505142212, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.6027 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -1.024733034880128
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -1.0247330215990647
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -1.0247331220893785
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -1.0247324025158657
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -1.0247330890402555
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4046, dtype=torch.float64), tensor(0.1654, dtype=torch.float64), tensor(0.0459, dtype=torch.float64), 0, tensor(0.3680, dtype=torch.float64), tensor(0.0133, dtype=torch.float64), 0, 0, 0, 12, 0, 1, 0, 1, 1, 56, 0.010030024731067696, 5.91694980287402, 1]
normalized proposed parameters for next round by BO: [tensor(0.4046, dtype=torch.float64), tensor(0.1654, dtype=torch.float64), tensor(0.0459, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3680, dtype=torch.float64), tensor(0.0133, dtype=torch.float64), tensor(6.3715e-18, dtype=torch.float64), tensor(0.0027, dtype=torch.float64), tensor(4.7525e-18, dtype=torch.float64), tensor(0.3657, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4342, dtype=torch.float64), tensor(0.1003, dtype=torch.float64), tensor(0.1233, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.405
  gsm8k: 0.165
  rowan_hellaswag: 0.046
  sciq: 0
  triviaqa: 0.368
  truthfulqa_gen: 0.013
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (56,)
  lora_dropout: (0.010030024731067696,)
  num_layers_to_apply: (12,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (5.91694980287402,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  12
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  56
lora dropout:  0.010030024731067696
lora alpha:  5.91694980287402
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 28,213,248 || all params: 8,058,474,496 || trainable%: 0.3501
length of training data:  9971
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.7385, 'grad_norm': 0.8417084813117981, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8125311136245728, 'eval_runtime': 9.4233, 'eval_samples_per_second': 106.12, 'eval_steps_per_second': 6.686, 'epoch': 0.04}
{'loss': 2.0902, 'grad_norm': 0.6962170600891113, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.400772213935852, 'eval_runtime': 9.3905, 'eval_samples_per_second': 106.49, 'eval_steps_per_second': 6.709, 'epoch': 0.08}
{'loss': 1.3884, 'grad_norm': 0.4542025029659271, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.1127756834030151, 'eval_runtime': 9.4386, 'eval_samples_per_second': 105.948, 'eval_steps_per_second': 6.675, 'epoch': 0.12}
{'loss': 1.2382, 'grad_norm': 0.23471017181873322, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.0387018918991089, 'eval_runtime': 9.4554, 'eval_samples_per_second': 105.76, 'eval_steps_per_second': 6.663, 'epoch': 0.16}
{'loss': 1.1462, 'grad_norm': 0.30405065417289734, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.9880815148353577, 'eval_runtime': 9.4664, 'eval_samples_per_second': 105.637, 'eval_steps_per_second': 6.655, 'epoch': 0.2}
{'loss': 1.1019, 'grad_norm': 0.1880844235420227, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.9537083506584167, 'eval_runtime': 9.4931, 'eval_samples_per_second': 105.34, 'eval_steps_per_second': 6.636, 'epoch': 0.24}
{'loss': 1.0843, 'grad_norm': 0.2302255630493164, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.939940333366394, 'eval_runtime': 9.5008, 'eval_samples_per_second': 105.254, 'eval_steps_per_second': 6.631, 'epoch': 0.28}
{'loss': 1.0809, 'grad_norm': 0.18134409189224243, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.9316809177398682, 'eval_runtime': 9.4995, 'eval_samples_per_second': 105.269, 'eval_steps_per_second': 6.632, 'epoch': 0.32}
{'loss': 1.014, 'grad_norm': 0.1731700301170349, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.9271175265312195, 'eval_runtime': 9.503, 'eval_samples_per_second': 105.23, 'eval_steps_per_second': 6.63, 'epoch': 0.36}
{'loss': 1.0059, 'grad_norm': 0.1994505375623703, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.9226359128952026, 'eval_runtime': 9.4797, 'eval_samples_per_second': 105.488, 'eval_steps_per_second': 6.646, 'epoch': 0.4}
{'loss': 0.9772, 'grad_norm': 0.1790420562028885, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.9178372621536255, 'eval_runtime': 9.4745, 'eval_samples_per_second': 105.546, 'eval_steps_per_second': 6.649, 'epoch': 0.44}
{'loss': 1.0077, 'grad_norm': 0.17629370093345642, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.9116916656494141, 'eval_runtime': 9.4718, 'eval_samples_per_second': 105.577, 'eval_steps_per_second': 6.651, 'epoch': 0.48}
{'loss': 0.9851, 'grad_norm': 0.21682798862457275, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.9128466844558716, 'eval_runtime': 9.4875, 'eval_samples_per_second': 105.401, 'eval_steps_per_second': 6.64, 'epoch': 0.52}
{'loss': 1.0216, 'grad_norm': 0.1795128583908081, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.9152467250823975, 'eval_runtime': 9.4839, 'eval_samples_per_second': 105.442, 'eval_steps_per_second': 6.643, 'epoch': 0.56}
{'loss': 1.0259, 'grad_norm': 0.16538918018341064, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.9078973531723022, 'eval_runtime': 9.473, 'eval_samples_per_second': 105.564, 'eval_steps_per_second': 6.651, 'epoch': 0.6}
{'loss': 0.9919, 'grad_norm': 0.20117907226085663, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.9079161286354065, 'eval_runtime': 9.4711, 'eval_samples_per_second': 105.584, 'eval_steps_per_second': 6.652, 'epoch': 0.64}
{'loss': 0.9874, 'grad_norm': 0.1466280072927475, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.9055572748184204, 'eval_runtime': 9.4731, 'eval_samples_per_second': 105.562, 'eval_steps_per_second': 6.65, 'epoch': 0.68}
{'loss': 0.9864, 'grad_norm': 0.1817668229341507, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.9046131372451782, 'eval_runtime': 9.4855, 'eval_samples_per_second': 105.424, 'eval_steps_per_second': 6.642, 'epoch': 0.72}
{'loss': 1.0088, 'grad_norm': 0.2187272310256958, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.9043067097663879, 'eval_runtime': 9.4821, 'eval_samples_per_second': 105.462, 'eval_steps_per_second': 6.644, 'epoch': 0.76}
{'loss': 0.9836, 'grad_norm': 0.17813695967197418, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.9031312465667725, 'eval_runtime': 9.4981, 'eval_samples_per_second': 105.285, 'eval_steps_per_second': 6.633, 'epoch': 0.8}
{'loss': 0.9589, 'grad_norm': 0.21852365136146545, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.9035930037498474, 'eval_runtime': 9.5449, 'eval_samples_per_second': 104.768, 'eval_steps_per_second': 6.6, 'epoch': 0.84}
{'loss': 1.0356, 'grad_norm': 0.17988942563533783, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.9000900387763977, 'eval_runtime': 9.5793, 'eval_samples_per_second': 104.392, 'eval_steps_per_second': 6.577, 'epoch': 0.88}
{'loss': 0.9941, 'grad_norm': 0.19642379879951477, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.8985424637794495, 'eval_runtime': 9.5653, 'eval_samples_per_second': 104.545, 'eval_steps_per_second': 6.586, 'epoch': 0.92}
{'loss': 1.0213, 'grad_norm': 0.18946708738803864, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.8986577987670898, 'eval_runtime': 9.5957, 'eval_samples_per_second': 104.214, 'eval_steps_per_second': 6.565, 'epoch': 0.96}
{'train_runtime': 422.8472, 'train_samples_per_second': 23.581, 'train_steps_per_second': 1.476, 'train_loss': 1.1968913017175136, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8125311136245728, 1.400772213935852, 1.1127756834030151, 1.0387018918991089, 0.9880815148353577, 0.9537083506584167, 0.939940333366394, 0.9316809177398682, 0.9271175265312195, 0.9226359128952026, 0.9178372621536255, 0.9116916656494141, 0.9128466844558716, 0.9152467250823975, 0.9078973531723022, 0.9079161286354065, 0.9055572748184204, 0.9046131372451782, 0.9043067097663879, 0.9031312465667725, 0.9035930037498474, 0.9000900387763977, 0.8985424637794495, 0.8986577987670898], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.8125311136245728, 1.400772213935852, 1.1127756834030151, 1.0387018918991089, 0.9880815148353577, 0.9537083506584167, 0.939940333366394, 0.9316809177398682, 0.9271175265312195, 0.9226359128952026, 0.9178372621536255, 0.9116916656494141, 0.9128466844558716, 0.9152467250823975, 0.9078973531723022, 0.9079161286354065, 0.9055572748184204, 0.9046131372451782, 0.9043067097663879, 0.9031312465667725, 0.9035930037498474, 0.9000900387763977, 0.8985424637794495, 0.8986577987670898]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0877562761306763
current iteration best possible eval_loss (full train run):  -0.8986577987670898
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318, -1.020788550376892, -1.020788550376892, -1.666443943977356, -1.163820505142212, -1.020788550376892, -1.0877562761306763]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.3200 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.647038459777832, 0.3565073609352112, 0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.6441194415092468, 0.6647308468818665, 0.9432829022407532, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.47025445103645325, 0.5915921330451965, 0.563302755355835, 0.236403226852417]  ‚Üí  acq = -1.0037542675321025
X = [0.7882768511772156, 0.0865660309791565, 0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.06684881448745728, 0.476356565952301, 0.49730604887008667, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.06370062381029129, 0.8247895836830139, 0.642969012260437, 0.2869639992713928]  ‚Üí  acq = -1.006819149214792
X = [0.9537772536277771, 0.5254051685333252, 0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5333559513092041, 0.6058185696601868, 0.9840016961097717, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.07915887981653214, 0.6141131520271301, 0.8602699041366577, 0.6692355275154114]  ‚Üí  acq = -1.0013372456360257
X = [0.10701495409011841, 0.8236895203590393, 0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6551105976104736, 0.23645484447479248, 0.007582306861877441, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5136890411376953, 0.0011958479881286621, 0.378928005695343, 0.21825557947158813]  ‚Üí  acq = -1.0074720431386692
X = [0.37967562675476074, 0.21945631504058838, 0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4969014823436737, 0.5960436463356018, 0.011648118495941162, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.918420135974884, 0.3473445177078247, 0.7110291719436646, 0.30779868364334106]  ‚Üí  acq = -0.9933437796290624
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0899, dtype=torch.float64), tensor(0.4222, dtype=torch.float64), 0, tensor(0.0962, dtype=torch.float64), tensor(0.1679, dtype=torch.float64), 0, tensor(0.1077, dtype=torch.float64), 0, tensor(0.1161, dtype=torch.float64), 11, 0, 0, 0, 1, 1, 117, 0.069870193783375, 11.740754774889016, 0]
normalized proposed parameters for next round by BO: [tensor(0.0899, dtype=torch.float64), tensor(0.4222, dtype=torch.float64), tensor(2.4352e-18, dtype=torch.float64), tensor(0.0962, dtype=torch.float64), tensor(0.1679, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1077, dtype=torch.float64), tensor(8.2631e-18, dtype=torch.float64), tensor(0.1161, dtype=torch.float64), tensor(0.3432, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9133, dtype=torch.float64), tensor(0.6987, dtype=torch.float64), tensor(0.2446, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.09
  gsm8k: 0.422
  rowan_hellaswag: 0
  sciq: 0.096
  triviaqa: 0.168
  truthfulqa_gen: 0
  wikitext: 0.108
  mmlu: 0
  arc_challenge: 0.116

LoRA Parameters:
  lora_r: (117,)
  lora_dropout: (0.069870193783375,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (11.740754774889016,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  117
lora dropout:  0.069870193783375
lora alpha:  11.740754774889016
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 47,443,968 || all params: 8,077,705,216 || trainable%: 0.5873
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1176, 'grad_norm': 0.3627093434333801, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7146596908569336, 'eval_runtime': 9.5886, 'eval_samples_per_second': 104.291, 'eval_steps_per_second': 6.57, 'epoch': 0.04}
{'loss': 1.6503, 'grad_norm': 0.24675332009792328, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1270878314971924, 'eval_runtime': 9.6601, 'eval_samples_per_second': 103.519, 'eval_steps_per_second': 6.522, 'epoch': 0.08}
{'loss': 1.2097, 'grad_norm': 0.16220548748970032, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.994733452796936, 'eval_runtime': 9.7328, 'eval_samples_per_second': 102.745, 'eval_steps_per_second': 6.473, 'epoch': 0.12}
{'loss': 1.0824, 'grad_norm': 0.09564775228500366, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9650488495826721, 'eval_runtime': 9.7307, 'eval_samples_per_second': 102.768, 'eval_steps_per_second': 6.474, 'epoch': 0.16}
{'loss': 1.1149, 'grad_norm': 0.08217577636241913, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9528737664222717, 'eval_runtime': 9.7396, 'eval_samples_per_second': 102.673, 'eval_steps_per_second': 6.468, 'epoch': 0.2}
{'loss': 1.0588, 'grad_norm': 0.1311168223619461, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9509336948394775, 'eval_runtime': 9.7473, 'eval_samples_per_second': 102.593, 'eval_steps_per_second': 6.463, 'epoch': 0.24}
{'loss': 1.0414, 'grad_norm': 0.0935945138335228, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9365889430046082, 'eval_runtime': 9.7369, 'eval_samples_per_second': 102.702, 'eval_steps_per_second': 6.47, 'epoch': 0.28}
{'loss': 1.0395, 'grad_norm': 0.08975453674793243, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.927780032157898, 'eval_runtime': 9.7311, 'eval_samples_per_second': 102.763, 'eval_steps_per_second': 6.474, 'epoch': 0.32}
{'loss': 1.0349, 'grad_norm': 0.1141299158334732, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9272353649139404, 'eval_runtime': 9.6877, 'eval_samples_per_second': 103.223, 'eval_steps_per_second': 6.503, 'epoch': 0.36}
{'loss': 1.0215, 'grad_norm': 0.10234387218952179, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9233159422874451, 'eval_runtime': 9.6582, 'eval_samples_per_second': 103.539, 'eval_steps_per_second': 6.523, 'epoch': 0.4}
{'loss': 1.0454, 'grad_norm': 0.10102879256010056, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.918522834777832, 'eval_runtime': 9.6613, 'eval_samples_per_second': 103.506, 'eval_steps_per_second': 6.521, 'epoch': 0.44}
{'loss': 1.0177, 'grad_norm': 0.10323210805654526, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9193031787872314, 'eval_runtime': 9.6573, 'eval_samples_per_second': 103.548, 'eval_steps_per_second': 6.524, 'epoch': 0.48}
{'loss': 1.0307, 'grad_norm': 0.10492326319217682, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9143651127815247, 'eval_runtime': 9.6605, 'eval_samples_per_second': 103.514, 'eval_steps_per_second': 6.521, 'epoch': 0.52}
{'loss': 0.9534, 'grad_norm': 0.10527435690164566, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9120745658874512, 'eval_runtime': 9.6592, 'eval_samples_per_second': 103.528, 'eval_steps_per_second': 6.522, 'epoch': 0.56}
{'loss': 1.0141, 'grad_norm': 0.09188847988843918, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.912738025188446, 'eval_runtime': 9.6494, 'eval_samples_per_second': 103.633, 'eval_steps_per_second': 6.529, 'epoch': 0.6}
{'loss': 0.9696, 'grad_norm': 0.09277348965406418, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9116870164871216, 'eval_runtime': 9.6639, 'eval_samples_per_second': 103.477, 'eval_steps_per_second': 6.519, 'epoch': 0.64}
{'loss': 0.9851, 'grad_norm': 0.09747792035341263, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.908754825592041, 'eval_runtime': 9.6654, 'eval_samples_per_second': 103.461, 'eval_steps_per_second': 6.518, 'epoch': 0.68}
{'loss': 1.0474, 'grad_norm': 0.09899239242076874, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9066337943077087, 'eval_runtime': 9.665, 'eval_samples_per_second': 103.466, 'eval_steps_per_second': 6.518, 'epoch': 0.72}
{'loss': 1.0595, 'grad_norm': 0.12366575002670288, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.906220555305481, 'eval_runtime': 9.6681, 'eval_samples_per_second': 103.433, 'eval_steps_per_second': 6.516, 'epoch': 0.76}
{'loss': 1.0115, 'grad_norm': 0.09530846029520035, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9049171209335327, 'eval_runtime': 9.6592, 'eval_samples_per_second': 103.528, 'eval_steps_per_second': 6.522, 'epoch': 0.8}
{'loss': 1.0268, 'grad_norm': 0.10580825805664062, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9056501984596252, 'eval_runtime': 9.6751, 'eval_samples_per_second': 103.358, 'eval_steps_per_second': 6.512, 'epoch': 0.84}
{'loss': 1.0483, 'grad_norm': 0.10676757246255875, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9031678438186646, 'eval_runtime': 9.6713, 'eval_samples_per_second': 103.399, 'eval_steps_per_second': 6.514, 'epoch': 0.88}
{'loss': 0.9672, 'grad_norm': 0.10699862986803055, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9030048847198486, 'eval_runtime': 9.6651, 'eval_samples_per_second': 103.465, 'eval_steps_per_second': 6.518, 'epoch': 0.92}
{'loss': 0.9908, 'grad_norm': 0.1026947945356369, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9027234315872192, 'eval_runtime': 9.6745, 'eval_samples_per_second': 103.364, 'eval_steps_per_second': 6.512, 'epoch': 0.96}
{'loss': 0.9885, 'grad_norm': 0.1577604115009308, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9026172161102295, 'eval_runtime': 9.6654, 'eval_samples_per_second': 103.462, 'eval_steps_per_second': 6.518, 'epoch': 1.0}
{'train_runtime': 385.4275, 'train_samples_per_second': 25.935, 'train_steps_per_second': 1.622, 'train_loss': 1.1410736267089843, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7146596908569336, 1.1270878314971924, 0.994733452796936, 0.9650488495826721, 0.9528737664222717, 0.9509336948394775, 0.9365889430046082, 0.927780032157898, 0.9272353649139404, 0.9233159422874451, 0.918522834777832, 0.9193031787872314, 0.9143651127815247, 0.9120745658874512, 0.912738025188446, 0.9116870164871216, 0.908754825592041, 0.9066337943077087, 0.906220555305481, 0.9049171209335327, 0.9056501984596252, 0.9031678438186646, 0.9030048847198486, 0.9027234315872192, 0.9026172161102295], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7146596908569336, 1.1270878314971924, 0.994733452796936, 0.9650488495826721, 0.9528737664222717, 0.9509336948394775, 0.9365889430046082, 0.927780032157898, 0.9272353649139404, 0.9233159422874451, 0.918522834777832, 0.9193031787872314, 0.9143651127815247, 0.9120745658874512, 0.912738025188446, 0.9116870164871216, 0.908754825592041, 0.9066337943077087, 0.906220555305481, 0.9049171209335327, 0.9056501984596252, 0.9031678438186646, 0.9030048847198486, 0.9027234315872192, 0.9026172161102295]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1580394506454468
current iteration best possible eval_loss (full train run):  -0.9026172161102295
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318, -1.020788550376892, -1.020788550376892, -1.666443943977356, -1.163820505142212, -1.020788550376892, -1.0877562761306763, -1.1580394506454468]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.2531 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.0366812340733889
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.0370794583686362
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.0367234969213148
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.037086275989338
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.0364649645844766
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2142, dtype=torch.float64), tensor(0.2080, dtype=torch.float64), tensor(0.0817, dtype=torch.float64), 0, tensor(0.1588, dtype=torch.float64), tensor(0.2244, dtype=torch.float64), tensor(0.0248, dtype=torch.float64), 0, tensor(0.0782, dtype=torch.float64), 29, 1, 0, 0, 0, 1, 116, 0.08612425302643882, 47.6305485643988, 0]
normalized proposed parameters for next round by BO: [tensor(0.2142, dtype=torch.float64), tensor(0.2080, dtype=torch.float64), tensor(0.0817, dtype=torch.float64), tensor(0.0099, dtype=torch.float64), tensor(0.1588, dtype=torch.float64), tensor(0.2244, dtype=torch.float64), tensor(0.0248, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0782, dtype=torch.float64), tensor(0.9068, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9091, dtype=torch.float64), tensor(0.8612, dtype=torch.float64), tensor(0.9923, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.214
  gsm8k: 0.208
  rowan_hellaswag: 0.082
  sciq: 0
  triviaqa: 0.159
  truthfulqa_gen: 0.224
  wikitext: 0.025
  mmlu: 0
  arc_challenge: 0.078

LoRA Parameters:
  lora_r: (116,)
  lora_dropout: (0.08612425302643882,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (47.6305485643988,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  116
lora dropout:  0.08612425302643882
lora alpha:  47.6305485643988
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 89,563,136 || all params: 8,119,824,384 || trainable%: 1.1030
length of training data:  9897
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1017, 'grad_norm': 0.3781169652938843, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4445304870605469, 'eval_runtime': 9.844, 'eval_samples_per_second': 101.585, 'eval_steps_per_second': 6.4, 'epoch': 0.04}
{'loss': 1.5457, 'grad_norm': 0.22542735934257507, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0635368824005127, 'eval_runtime': 9.955, 'eval_samples_per_second': 100.452, 'eval_steps_per_second': 6.328, 'epoch': 0.08}
{'loss': 1.2826, 'grad_norm': 0.2001262754201889, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 0.995510995388031, 'eval_runtime': 9.9592, 'eval_samples_per_second': 100.41, 'eval_steps_per_second': 6.326, 'epoch': 0.12}
{'loss': 1.1969, 'grad_norm': 0.21324849128723145, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 0.9279773831367493, 'eval_runtime': 9.9659, 'eval_samples_per_second': 100.343, 'eval_steps_per_second': 6.322, 'epoch': 0.16}
{'loss': 1.0562, 'grad_norm': 0.21735736727714539, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 0.8942345976829529, 'eval_runtime': 9.9648, 'eval_samples_per_second': 100.353, 'eval_steps_per_second': 6.322, 'epoch': 0.2}
{'loss': 1.0903, 'grad_norm': 0.18501737713813782, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 0.8944273591041565, 'eval_runtime': 9.9645, 'eval_samples_per_second': 100.356, 'eval_steps_per_second': 6.322, 'epoch': 0.24}
{'loss': 1.0792, 'grad_norm': 0.20966562628746033, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 0.8866409659385681, 'eval_runtime': 9.9698, 'eval_samples_per_second': 100.303, 'eval_steps_per_second': 6.319, 'epoch': 0.28}
{'loss': 1.0818, 'grad_norm': 0.16736792027950287, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 0.8827903866767883, 'eval_runtime': 9.9555, 'eval_samples_per_second': 100.447, 'eval_steps_per_second': 6.328, 'epoch': 0.32}
{'loss': 1.0649, 'grad_norm': 0.19431641697883606, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 0.8785704374313354, 'eval_runtime': 9.9673, 'eval_samples_per_second': 100.328, 'eval_steps_per_second': 6.321, 'epoch': 0.36}
{'loss': 1.0071, 'grad_norm': 0.1747286468744278, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 0.8780292868614197, 'eval_runtime': 9.9585, 'eval_samples_per_second': 100.417, 'eval_steps_per_second': 6.326, 'epoch': 0.4}
{'loss': 0.9788, 'grad_norm': 0.20906175673007965, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 0.8770228028297424, 'eval_runtime': 9.9626, 'eval_samples_per_second': 100.375, 'eval_steps_per_second': 6.324, 'epoch': 0.44}
{'loss': 0.9613, 'grad_norm': 0.1964145451784134, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 0.8753932118415833, 'eval_runtime': 9.9632, 'eval_samples_per_second': 100.369, 'eval_steps_per_second': 6.323, 'epoch': 0.48}
{'loss': 1.0305, 'grad_norm': 0.17713303864002228, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 0.8705281019210815, 'eval_runtime': 9.9574, 'eval_samples_per_second': 100.428, 'eval_steps_per_second': 6.327, 'epoch': 0.53}
{'loss': 1.0504, 'grad_norm': 0.2171899676322937, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 0.868847131729126, 'eval_runtime': 9.9535, 'eval_samples_per_second': 100.467, 'eval_steps_per_second': 6.329, 'epoch': 0.57}
{'loss': 0.9797, 'grad_norm': 0.17709971964359283, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 0.8671263456344604, 'eval_runtime': 9.9728, 'eval_samples_per_second': 100.273, 'eval_steps_per_second': 6.317, 'epoch': 0.61}
{'loss': 0.9858, 'grad_norm': 0.17490644752979279, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 0.8649722933769226, 'eval_runtime': 9.9785, 'eval_samples_per_second': 100.216, 'eval_steps_per_second': 6.314, 'epoch': 0.65}
{'loss': 1.0132, 'grad_norm': 0.18427489697933197, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 0.8632045984268188, 'eval_runtime': 9.9806, 'eval_samples_per_second': 100.194, 'eval_steps_per_second': 6.312, 'epoch': 0.69}
{'loss': 0.9546, 'grad_norm': 0.20415018498897552, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 0.8642464876174927, 'eval_runtime': 9.9635, 'eval_samples_per_second': 100.366, 'eval_steps_per_second': 6.323, 'epoch': 0.73}
{'loss': 0.9579, 'grad_norm': 0.17066292464733124, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 0.8614053130149841, 'eval_runtime': 9.9617, 'eval_samples_per_second': 100.384, 'eval_steps_per_second': 6.324, 'epoch': 0.77}
{'loss': 0.938, 'grad_norm': 0.23507341742515564, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 0.8600664734840393, 'eval_runtime': 9.9602, 'eval_samples_per_second': 100.4, 'eval_steps_per_second': 6.325, 'epoch': 0.81}
{'loss': 0.9637, 'grad_norm': 0.16279277205467224, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 0.8578640222549438, 'eval_runtime': 9.9745, 'eval_samples_per_second': 100.256, 'eval_steps_per_second': 6.316, 'epoch': 0.85}
{'loss': 0.9575, 'grad_norm': 0.1703198254108429, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 0.856596052646637, 'eval_runtime': 9.9732, 'eval_samples_per_second': 100.269, 'eval_steps_per_second': 6.317, 'epoch': 0.89}
{'loss': 0.9091, 'grad_norm': 0.19496531784534454, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 0.8578762412071228, 'eval_runtime': 9.973, 'eval_samples_per_second': 100.271, 'eval_steps_per_second': 6.317, 'epoch': 0.93}
{'loss': 1.0111, 'grad_norm': 0.18781337141990662, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 0.8559422492980957, 'eval_runtime': 9.9645, 'eval_samples_per_second': 100.356, 'eval_steps_per_second': 6.322, 'epoch': 0.97}
{'train_runtime': 441.321, 'train_samples_per_second': 22.426, 'train_steps_per_second': 1.403, 'train_loss': 1.1290953671604829, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4445304870605469, 1.0635368824005127, 0.995510995388031, 0.9279773831367493, 0.8942345976829529, 0.8944273591041565, 0.8866409659385681, 0.8827903866767883, 0.8785704374313354, 0.8780292868614197, 0.8770228028297424, 0.8753932118415833, 0.8705281019210815, 0.868847131729126, 0.8671263456344604, 0.8649722933769226, 0.8632045984268188, 0.8642464876174927, 0.8614053130149841, 0.8600664734840393, 0.8578640222549438, 0.856596052646637, 0.8578762412071228, 0.8559422492980957], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.4445304870605469, 1.0635368824005127, 0.995510995388031, 0.9279773831367493, 0.8942345976829529, 0.8944273591041565, 0.8866409659385681, 0.8827903866767883, 0.8785704374313354, 0.8780292868614197, 0.8770228028297424, 0.8753932118415833, 0.8705281019210815, 0.868847131729126, 0.8671263456344604, 0.8649722933769226, 0.8632045984268188, 0.8642464876174927, 0.8614053130149841, 0.8600664734840393, 0.8578640222549438, 0.856596052646637, 0.8578762412071228, 0.8559422492980957]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.30768620967865
current iteration best possible eval_loss (full train run):  -0.8559422492980957
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318, -1.020788550376892, -1.020788550376892, -1.666443943977356, -1.163820505142212, -1.020788550376892, -1.0877562761306763, -1.1580394506454468, -1.30768620967865]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.1264 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.01858508586883545, 0.27087509632110596, 0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3516203463077545, 0.6742079257965088, 0.8425379991531372, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.3618133068084717, 0.971827507019043, 0.9077439308166504, 0.298198401927948]  ‚Üí  acq = -1.0866470980615073
X = [0.8427011370658875, 0.498738169670105, 0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.949406087398529, 0.2025597095489502, 0.1727500557899475, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6172329187393188, 0.7515861392021179, 0.5611653327941895, 0.20585447549819946]  ‚Üí  acq = -1.089642368991773
X = [0.2902684807777405, 0.8506918549537659, 0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.44211840629577637, 0.9807340502738953, 0.05173856019973755, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5744302272796631, 0.3193025588989258, 0.46983620524406433, 0.4887549877166748]  ‚Üí  acq = -1.0896423725552467
X = [0.4835529327392578, 0.9963791966438293, 0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.5757967829704285, 0.605756938457489, 0.38045990467071533, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.12970638275146484, 0.7689643502235413, 0.6867401599884033, 0.6047636270523071]  ‚Üí  acq = -1.0896031923860567
X = [0.500048816204071, 0.24933886528015137, 0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.07476793229579926, 0.3871777653694153, 0.952457845211029, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.24435395002365112, 0.10478633642196655, 0.9867004156112671, 0.0338512659072876]  ‚Üí  acq = -1.0907802447303983
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0542, dtype=torch.float64), tensor(0.3855, dtype=torch.float64), tensor(0.0950, dtype=torch.float64), 0, tensor(0.2206, dtype=torch.float64), tensor(0.0619, dtype=torch.float64), tensor(0.0380, dtype=torch.float64), tensor(0.0569, dtype=torch.float64), tensor(0.0879, dtype=torch.float64), 32, 0, 0, 0, 1, 1, 85, 0.07473528010234402, 19.89272889954652, 1]
normalized proposed parameters for next round by BO: [tensor(0.0542, dtype=torch.float64), tensor(0.3855, dtype=torch.float64), tensor(0.0950, dtype=torch.float64), tensor(6.6870e-19, dtype=torch.float64), tensor(0.2206, dtype=torch.float64), tensor(0.0619, dtype=torch.float64), tensor(0.0380, dtype=torch.float64), tensor(0.0569, dtype=torch.float64), tensor(0.0879, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6629, dtype=torch.float64), tensor(0.7474, dtype=torch.float64), tensor(0.4144, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.054
  gsm8k: 0.385
  rowan_hellaswag: 0.095
  sciq: 0
  triviaqa: 0.221
  truthfulqa_gen: 0.062
  wikitext: 0.038
  mmlu: 0.057
  arc_challenge: 0.088

LoRA Parameters:
  lora_r: (85,)
  lora_dropout: (0.07473528010234402,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (19.89272889954652,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  85
lora dropout:  0.07473528010234402
lora alpha:  19.89272889954652
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 100,270,080 || all params: 8,130,531,328 || trainable%: 1.2333
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.798, 'grad_norm': 0.5530897974967957, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3323012590408325, 'eval_runtime': 10.6525, 'eval_samples_per_second': 93.875, 'eval_steps_per_second': 5.914, 'epoch': 0.04}
{'loss': 1.3825, 'grad_norm': 0.29780951142311096, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9657586812973022, 'eval_runtime': 10.6728, 'eval_samples_per_second': 93.696, 'eval_steps_per_second': 5.903, 'epoch': 0.08}
{'loss': 1.2054, 'grad_norm': 0.22937801480293274, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9055213332176208, 'eval_runtime': 10.7116, 'eval_samples_per_second': 93.357, 'eval_steps_per_second': 5.881, 'epoch': 0.12}
{'loss': 1.186, 'grad_norm': 0.17715872824192047, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8911950588226318, 'eval_runtime': 10.7367, 'eval_samples_per_second': 93.138, 'eval_steps_per_second': 5.868, 'epoch': 0.16}
{'loss': 1.1297, 'grad_norm': 0.14096903800964355, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8847222924232483, 'eval_runtime': 10.7547, 'eval_samples_per_second': 92.983, 'eval_steps_per_second': 5.858, 'epoch': 0.2}
{'loss': 1.0915, 'grad_norm': 0.1674252599477768, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8841825723648071, 'eval_runtime': 10.7643, 'eval_samples_per_second': 92.9, 'eval_steps_per_second': 5.853, 'epoch': 0.24}
{'loss': 1.1151, 'grad_norm': 0.1287556290626526, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8716506361961365, 'eval_runtime': 10.7787, 'eval_samples_per_second': 92.776, 'eval_steps_per_second': 5.845, 'epoch': 0.28}
{'loss': 1.0498, 'grad_norm': 0.170723557472229, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8690986037254333, 'eval_runtime': 10.7666, 'eval_samples_per_second': 92.879, 'eval_steps_per_second': 5.851, 'epoch': 0.32}
{'loss': 1.0069, 'grad_norm': 0.17042353749275208, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8630927801132202, 'eval_runtime': 10.7685, 'eval_samples_per_second': 92.864, 'eval_steps_per_second': 5.85, 'epoch': 0.36}
{'loss': 1.0691, 'grad_norm': 0.14769448339939117, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8613288402557373, 'eval_runtime': 10.7649, 'eval_samples_per_second': 92.894, 'eval_steps_per_second': 5.852, 'epoch': 0.4}
{'loss': 1.0706, 'grad_norm': 0.1428171992301941, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8557032942771912, 'eval_runtime': 10.7883, 'eval_samples_per_second': 92.693, 'eval_steps_per_second': 5.84, 'epoch': 0.44}
{'loss': 0.9728, 'grad_norm': 0.1495032161474228, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8566644191741943, 'eval_runtime': 10.8487, 'eval_samples_per_second': 92.177, 'eval_steps_per_second': 5.807, 'epoch': 0.48}
{'loss': 1.0202, 'grad_norm': 0.1526075005531311, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8512138724327087, 'eval_runtime': 10.8244, 'eval_samples_per_second': 92.384, 'eval_steps_per_second': 5.82, 'epoch': 0.52}
{'loss': 1.0487, 'grad_norm': 0.14533914625644684, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.848709225654602, 'eval_runtime': 10.8574, 'eval_samples_per_second': 92.103, 'eval_steps_per_second': 5.802, 'epoch': 0.56}
{'loss': 1.0772, 'grad_norm': 0.15444369614124298, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8523565530776978, 'eval_runtime': 10.7727, 'eval_samples_per_second': 92.827, 'eval_steps_per_second': 5.848, 'epoch': 0.6}
{'loss': 1.0379, 'grad_norm': 0.12950929999351501, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8499352931976318, 'eval_runtime': 10.7669, 'eval_samples_per_second': 92.877, 'eval_steps_per_second': 5.851, 'epoch': 0.64}
{'loss': 1.026, 'grad_norm': 0.16807691752910614, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.847186803817749, 'eval_runtime': 10.7847, 'eval_samples_per_second': 92.724, 'eval_steps_per_second': 5.842, 'epoch': 0.68}
{'loss': 1.05, 'grad_norm': 0.15299615263938904, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.843167245388031, 'eval_runtime': 10.7748, 'eval_samples_per_second': 92.809, 'eval_steps_per_second': 5.847, 'epoch': 0.72}
{'loss': 1.0493, 'grad_norm': 0.18270638585090637, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8444881439208984, 'eval_runtime': 10.7687, 'eval_samples_per_second': 92.862, 'eval_steps_per_second': 5.85, 'epoch': 0.76}
{'loss': 1.0662, 'grad_norm': 0.14836591482162476, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8427547216415405, 'eval_runtime': 10.7628, 'eval_samples_per_second': 92.913, 'eval_steps_per_second': 5.854, 'epoch': 0.8}
{'loss': 1.0435, 'grad_norm': 0.15665431320667267, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8411692976951599, 'eval_runtime': 10.7588, 'eval_samples_per_second': 92.947, 'eval_steps_per_second': 5.856, 'epoch': 0.84}
{'loss': 1.0288, 'grad_norm': 0.14822770655155182, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8387925624847412, 'eval_runtime': 10.76, 'eval_samples_per_second': 92.937, 'eval_steps_per_second': 5.855, 'epoch': 0.88}
{'loss': 0.9966, 'grad_norm': 0.17463527619838715, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8388403058052063, 'eval_runtime': 10.7895, 'eval_samples_per_second': 92.683, 'eval_steps_per_second': 5.839, 'epoch': 0.92}
{'loss': 1.0614, 'grad_norm': 0.16034802794456482, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8385141491889954, 'eval_runtime': 10.7808, 'eval_samples_per_second': 92.758, 'eval_steps_per_second': 5.844, 'epoch': 0.96}
{'loss': 0.985, 'grad_norm': 0.19381576776504517, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8382471799850464, 'eval_runtime': 10.7663, 'eval_samples_per_second': 92.882, 'eval_steps_per_second': 5.852, 'epoch': 1.0}
{'train_runtime': 512.4006, 'train_samples_per_second': 19.508, 'train_steps_per_second': 1.22, 'train_loss': 1.1427139709472656, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3323012590408325, 0.9657586812973022, 0.9055213332176208, 0.8911950588226318, 0.8847222924232483, 0.8841825723648071, 0.8716506361961365, 0.8690986037254333, 0.8630927801132202, 0.8613288402557373, 0.8557032942771912, 0.8566644191741943, 0.8512138724327087, 0.848709225654602, 0.8523565530776978, 0.8499352931976318, 0.847186803817749, 0.843167245388031, 0.8444881439208984, 0.8427547216415405, 0.8411692976951599, 0.8387925624847412, 0.8388403058052063, 0.8385141491889954, 0.8382471799850464], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3323012590408325, 0.9657586812973022, 0.9055213332176208, 0.8911950588226318, 0.8847222924232483, 0.8841825723648071, 0.8716506361961365, 0.8690986037254333, 0.8630927801132202, 0.8613288402557373, 0.8557032942771912, 0.8566644191741943, 0.8512138724327087, 0.848709225654602, 0.8523565530776978, 0.8499352931976318, 0.847186803817749, 0.843167245388031, 0.8444881439208984, 0.8427547216415405, 0.8411692976951599, 0.8387925624847412, 0.8388403058052063, 0.8385141491889954, 0.8382471799850464]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.020788550376892
current iteration best possible eval_loss (full train run):  -0.8382471799850464
max eval_loss so far:  -0.82343590259552
BO observations:  [-2.3906335830688477, -1.137067198753357, -1.4195952415466309, -1.022498607635498, -1.925295114517212, -1.6537652015686035, -1.3393288850784302, -1.1837950944900513, -1.4292274713516235, -1.153781771659851, -1.102095127105713, -1.377115249633789, -1.0588634014129639, -2.273756504058838, -1.3657252788543701, -1.020788550376892, -1.3278779983520508, -1.1507148742675781, -1.5638645887374878, -1.020788550376892, -1.2884118556976318, -1.020788550376892, -1.020788550376892, -1.666443943977356, -1.163820505142212, -1.020788550376892, -1.0877562761306763, -1.1580394506454468, -1.30768620967865, -1.020788550376892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.4896 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -1.042456386352194
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -1.0424563123737507
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -1.0424564233379443
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -1.0424567371994302
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -1.042456331536549
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0475, dtype=torch.float64), tensor(0.2210, dtype=torch.float64), 0, 0, tensor(0.3154, dtype=torch.float64), 0, tensor(0.4161, dtype=torch.float64), 0, 31, 0, 1, 0, 1, 1, 105, 0.027979319321288026, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0475, dtype=torch.float64), tensor(0.2210, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1232e-17, dtype=torch.float64), tensor(0.3154, dtype=torch.float64), tensor(7.1323e-18, dtype=torch.float64), tensor(0.4161, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9719, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8198, dtype=torch.float64), tensor(0.2798, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-1.5093419551849365, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'gsm8k', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_gsm8k_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['gsm8k'], 'weight': [1.0], 'random': [[-1.4836045503616333, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.808354914188385, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261, -0.800920307636261], [-1.5389292240142822, -0.8277751207351685, -0.8277751207351685, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325, -0.8152114152908325], [-1.5093419551849365, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.830283522605896, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552, -0.82343590259552]], 'random_full_inputs': [[[0.17917586718321796, 0, 0.021146709933097117, 0.2048444356155024, 0, 0.3280121645298629, 0, 0, 0.2668208227383196, 32, 0, 0, 1, 1, 0, 2, 0.0, 48.0, 0], [0.20676021568226358, 0.3356288536965836, 0, 0.40173732180372895, 0.02003059518622133, 0, 0, 0, 0.03584301363120256, 31, 0, 1, 0, 1, 1, 128, 0.09976883115501559, 48.0, 0], [0, 0, 0.04327728174692097, 0, 0.07997634397141147, 0.12457913441523133, 0.28312141815749997, 0.2948903210192983, 0.17415550068963803, 5, 1, 1, 0, 1, 1, 128, 0.03394422492111412, 12.52993411337282, 1], [0.05031873825468075, 0.370076539557212, 0, 0, 0.011725301309503118, 0.3023181179191914, 0.05215602004976402, 0, 0.21340528290964875, 24, 1, 1, 0, 1, 0, 128, 0.1, 7.947738874650424, 1], [0, 0.6794528699997595, 0, 0, 0, 0.09573628143296235, 0, 0, 0.22481084856727812, 1, 1, 0, 0, 1, 1, 128, 0.1, 28.10351891421814, 0], [0.32427644732055877, 0.23229937249214086, 0, 0.1898472740389416, 0, 0, 0, 0.027813063374484578, 0.22576384277387423, 26, 0, 1, 0, 1, 1, 2, 3.0357660829594127e-19, 19.675877712940242, 0], [0.06546134261652985, 0.2510761009468178, 0, 0.10759382315560337, 0, 0.054065920744349544, 0.18930197536800134, 0, 0.332500837168698, 1, 0, 1, 0, 0, 0, 128, 0.026018275590245665, 33.02209143916859, 0], [0.6061372410318733, 0.25295223004085654, 0, 0, 0, 0, 0, 0, 0.1409105289272699, 1, 0, 0, 1, 0, 1, 2, 0.09999999999999037, 1.48000001907349, 0], [0.04250852935074925, 0.3172445517755194, 0.08167916136926502, 0.04237298307938665, 0.017073126022935037, 0.30402511089460704, 0.01864127615955688, 0.11704194483685655, 0.059413316511124165, 32, 1, 1, 1, 1, 0, 113, 0.04754490589832197, 23.42063691116207, 0], [0, 0.21206576808709254, 0, 0.4449595395187403, 0, 0, 0, 0.06974331064214956, 0.27323138175201755, 13, 0, 1, 0, 1, 1, 75, 0.1, 3.786098016392903, 1], [0, 0.4200230341414507, 0, 0.16474780581193704, 0.020296009303197558, 0.248271618431521, 0, 0.06200908767843562, 0.08465244463345803, 26, 0, 1, 0, 1, 1, 128, 0.0868514220044383, 7.40042499817633, 1], [0.17823003488638967, 0.17938600333199609, 0, 0.17648166257663053, 0.037630987617548635, 0.09808977896747906, 0, 0.11202073858712436, 0.2181607940328317, 27, 0, 1, 0, 0, 1, 54, 0.0261306443329736, 25.873480143783276, 1], [0.2101869512712872, 0.1714770559576645, 0.05698076970787043, 0.13595800273604572, 0.041360458728388325, 0.33992738567985337, 0, 0.04410937591889022, 0, 32, 0, 0, 0, 1, 0, 115, 0.03481212708410577, 35.86978397832534, 1], [0.03787308977409052, 0.20568869716520755, 0.03386550600817149, 0, 0, 0.5084405920322627, 0.050225544792307696, 0, 0.1608834757376396, 1, 0, 0, 0, 1, 1, 30, 0.006220020922815982, 15.109825549878174, 1], [0.14498325837554982, 0.2581664140473166, 0, 0.22735976624811513, 0, 0, 0.07019375580536201, 0.0796233468368953, 0.21967345868676103, 1, 0, 1, 1, 1, 1, 2, 0.07687089220180171, 31.667482324515483, 1], [0.24196840328929747, 0, 0, 0, 0, 0, 0.46028656707060295, 0.039153009688319834, 0.25859201995177944, 11, 0, 0, 1, 0, 1, 2, 0.03292437310728359, 48.0, 0], [0.355814043374784, 0.1874423754577234, 0.02585899960177162, 0.14764043330604248, 0.05295982739073584, 0, 0, 0, 0.22664290582363072, 14, 1, 1, 0, 1, 1, 88, 0.1, 10.39501895284633, 0], [0, 0.8141723602503408, 0.05125778217261171, 0, 0.02844430778058653, 0, 0, 0, 0.10612554979646106, 16, 0, 1, 0, 1, 1, 128, 0.1, 26.539926571520834, 0], [0.23727423454333335, 0.20878048879608552, 0.0959964022369331, 0.19735074134846708, 0.026033627010554818, 0, 0, 0.06253764356327814, 0.17202686250134772, 32, 0, 1, 0, 1, 1, 58, 0.01676321355996016, 17.972908416021703, 0], [0, 0.4749464175206125, 0, 0, 0.0113575322639332, 0, 0, 0.2028804674830593, 0.31081558273239507, 32, 0, 1, 0, 1, 1, 128, 0.04261842420913471, 12.601480064615009, 0], [0, 0.04143553772838725, 0, 0, 0.016721726813988917, 0.7842998185963873, 0, 0.15754291686123648, 0, 32, 0, 1, 0, 1, 1, 127, 5.2358595498595044e-20, 30.351870885748518, 0], [0, 0.8026949622400583, 0.014862689699410686, 0, 0.015088426195589899, 0.06507731939000222, 0, 0.10227660247493837, 0, 32, 0, 1, 0, 1, 0, 2, 1.0318782230199975e-16, 25.31839397594849, 0], [0.2194463207000792, 0.11162837052891338, 0, 0.041194363592581065, 0.09712855904335015, 0.4398310977498811, 0, 0, 0.0907712883851952, 32, 0, 1, 0, 1, 1, 106, 0.09999999999999998, 34.15097035806102, 0], [0.10921482414430969, 0.30898427229802883, 0.05021270869856746, 0.13944675765204403, 0.07942105840995752, 0.10262890946166228, 0, 0.08219983756108318, 0.12789163177434704, 28, 0, 1, 0, 1, 1, 63, 0.04529822474538283, 26.06585062232876, 0], [0, 0.08208983846595072, 0, 0.3594273954741378, 0.031032289199482237, 0.015820100883299618, 0.03675675236653163, 0.17964004299272532, 0.2952335806178727, 24, 0, 0, 0, 1, 0, 59, 0.08249818523828001, 7.616486875926668, 1], [0, 0, 0, 0.3045555159430898, 0.04881833303498328, 0.5952168986336576, 0, 0.05140925238826934, 0, 14, 0, 1, 0, 1, 1, 128, 0.1, 31.914363267421024, 0], [0, 0, 0.06913397424421086, 0.4706883229808377, 0.019289581073155697, 0, 0, 0.2894201320422515, 0.15146798965954436, 32, 0, 1, 1, 0, 1, 128, 0.051916152849778345, 17.720178550524302, 1], [0.19375612189200622, 0, 0.032093616517330885, 0, 0.027780727163152, 0.04145477866047161, 0, 0.2592395791313786, 0.4456751766356608, 32, 0, 1, 0, 1, 1, 128, 0.1, 18.945024636939902, 0], [0.29877358692855893, 0.15207157981667369, 0.06085230344330663, 0, 0.01598336625634538, 0.16084180785527222, 0.029183853761987153, 0.2794754798579346, 0, 26, 0, 1, 0, 1, 1, 128, 0.043933153562750016, 20.681362004822294, 1], [0.0667305487997937, 0.1844651291489567, 0.0984154262731701, 0, 0.13139095782890564, 0.26996888981133005, 0.023182383741064058, 0.11985252810690612, 0.10599413628987352, 30, 0, 1, 0, 1, 1, 128, 0.03915879619807624, 32.316104825365706, 1]], [[0.18045446184298441, 0, 0.021270887015428497, 0.20245022136310784, 0, 0.3279066330430615, 0, 0, 0.26791779673541793, 32, 0, 0, 1, 1, 0, 2, 0.0, 48.0, 0], [0.19346044545463728, 0.3279079508145731, 0, 0.4168268472992345, 0.020605150175910587, 0, 0, 0, 0.041199606255644325, 30, 0, 1, 0, 1, 1, 128, 0.1, 48.0, 0], [0, 0, 0.05702539713881937, 0, 0.0781952838733827, 0.1258766940638094, 0.3059249949731925, 0.2707497618563381, 0.16222786809445805, 5, 1, 1, 0, 1, 1, 128, 0.022154090819322445, 11.76229588111583, 1], [0, 0.7709776859028583, 0, 0, 0, 0.2290223140971416, 0, 0, 0, 32, 0, 1, 1, 1, 0, 31, 0.08069580705108324, 47.99999999999999, 0], [0.0833878486002668, 0.19391350636744986, 0.0420486501621838, 0, 0.11697525683751046, 0.2937975936342582, 0.07302382465719964, 0.0912112645729644, 0.10564205516816683, 29, 0, 1, 0, 0, 0, 106, 0.06596133499829604, 20.9444759079547, 0], [0.27313029747777756, 0.405951318786456, 0.029050274158466095, 0.25536181332722196, 0, 0, 0.013287987536494928, 0.0222743663233793, 0, 1, 0, 1, 0, 1, 1, 114, 0.1, 19.680438026574357, 0], [0.3058675036736382, 0, 0, 0, 0, 0.17020845642654306, 0, 0.08395475542176181, 0.4399692844780564, 32, 1, 0, 0, 1, 1, 2, 3.729655473350135e-18, 38.2276265634507, 1], [0.20637980737560174, 0.3255647367595307, 0, 0.1776622153057633, 0.014831072822170856, 0.05733438836435547, 0, 0, 0.2182277793725778, 10, 0, 1, 0, 1, 1, 63, 0.08559619888385107, 22.887211590576612, 1], [0.5372330173014178, 0, 0, 0.08101926070150951, 0, 0, 0.012698444542550842, 0.17256924767207577, 0.19648002978244616, 14, 1, 1, 0, 0, 1, 6, 1.7996404748216113e-18, 24.536853542414534, 0], [0, 0.3032727952914429, 0, 0.25229249951915916, 0.05627951761903902, 0.012107603797833072, 0.09901533202821287, 0.013784538394616722, 0.2632477133496962, 3, 0, 0, 0, 1, 1, 37, 0.1, 1.4800000190734863, 1], [0, 0, 0.38707071675468485, 0, 0.04925479311764462, 0.384094080373462, 0, 0.17958040975420858, 0, 28, 0, 1, 0, 1, 1, 128, 2.3420200024815864e-20, 14.728685001823683, 1], [0.486650832096823, 0, 0, 0, 0, 0, 0, 0.4227030170475906, 0.08643815960328945, 14, 0, 0, 0, 1, 1, 4, 3.725310268198181e-20, 31.82299542361982, 1], [0.6971145086772449, 0.01953677965876025, 0, 0, 0.10231029548885148, 0, 0.048415618856771855, 0, 0.13262279731837165, 7, 0, 0, 0, 0, 1, 59, 1.2372528050078122e-18, 18.4590481955748, 0], [0, 0, 0.1741947489430033, 0, 0.15345185077692922, 0.5477321728751754, 0.10662056491061597, 0, 0.018000662494275756, 32, 0, 1, 0, 1, 1, 121, 0.006714441735491532, 36.4567109723056, 0], [0, 0.07090437169982701, 0.0534287315636905, 0, 0, 0.16251225494465665, 0, 0.2786395087244463, 0.43451513306737977, 32, 0, 1, 0, 1, 1, 124, 0.0017693850945787821, 10.142521987853724, 1], [0, 0.09470357253284616, 0.0997321883045942, 0.1447525466957406, 0.3427316828518456, 0.013814498653953577, 0.19796167277921708, 0.014780327157936597, 0.09152351102386629, 32, 1, 0, 0, 0, 1, 18, 0.01070487067953985, 48.0, 0], [0, 0, 0.060222497631298964, 0.27389009493524474, 0.18637891540921922, 0, 0.011242573986847257, 0.1056783022434865, 0.36258761579390325, 27, 0, 0, 0, 1, 0, 126, 0.003968886578156947, 25.109118465170255, 1], [0, 0.226876099157616, 0.010396740295688364, 0.295294315370615, 0.08215571546287359, 0, 0.19450571701952438, 0.13487766592889822, 0.055893746764784494, 11, 0, 1, 0, 1, 1, 2, 0.0012352831605959868, 29.42695011850363, 0], [0, 0.23913076659943056, 0, 0, 0.25626390078378974, 0.3100403660165144, 0, 0, 0.19456496660026495, 32, 0, 1, 0, 1, 1, 44, 0.002500465333614713, 31.732421426659037, 1], [0.18867150967375543, 0.22642702435220832, 0, 0, 0, 0.03999145336195439, 0.021975537132523346, 0.09382480127646735, 0.4291096742030912, 13, 0, 0, 0, 1, 0, 100, 0.08226646575754476, 18.950945804511566, 1], [0.2996699654227262, 0.27708337748397227, 0, 0.3172451739436115, 0, 0, 0.035899111737608284, 0, 0.07010237141208182, 2, 0, 0, 0, 1, 1, 38, 0.010201960813941355, 15.72494546177599, 1], [0.2550619237256006, 0, 0.11028894371918689, 0, 0.07694004936075523, 0.15264791048976667, 0.03713959126012152, 0.15124220746006223, 0.2166793739845069, 26, 0, 1, 0, 0, 1, 72, 0.016461771037997203, 36.578383609164625, 1], [0.10648163291388339, 0, 0, 0.024406203895668416, 0.35976953231123343, 0.12238933021601045, 0.05184516443113425, 0.08600792728276113, 0.249100208949309, 20, 0, 1, 0, 1, 1, 120, 0.014285441661450718, 18.376353835470212, 0], [0.1983386089920088, 0.06335121131076461, 0, 0, 0, 0.31300900548981975, 0, 0.18262219059807874, 0.23637761461315804, 23, 0, 0, 1, 1, 1, 109, 0.09397553258277558, 20.102841946012667, 1], [0, 0.15726596819240812, 0, 0.3605421653269954, 0.0603024111736132, 0.14681899534268958, 0, 0.0609284642577834, 0.20523990587247695, 25, 0, 0, 0, 1, 0, 126, 0.07774426138931445, 17.783503634781255, 1], [0, 0.550781122806298, 0, 0, 0, 0, 0.02781981572200232, 0.17278116335618843, 0.23966657967210922, 4, 0, 0, 0, 1, 1, 128, 0.0049925143898692514, 19.288948548274956, 1], [0, 0.07495866135234663, 0.1127590713545007, 0.020054381605555338, 0.26319234726760227, 0.2718386459175403, 0.04812250908191426, 0.0839391896797775, 0.1251351937407631, 4, 0, 1, 0, 1, 1, 33, 0.02406505743903775, 20.70721083275133, 0], [0, 0.02225642487144336, 0, 0.11076542263923991, 0, 0.48853351513858384, 0, 0.06655920569833182, 0.311885431652401, 29, 0, 1, 0, 1, 1, 128, 0.009797977853828674, 14.746967341106059, 1], [0, 0.20927592718688742, 0, 0, 0.3890060893080211, 0.29517235002066194, 0.10616134389368866, 0, 0, 30, 0, 1, 0, 1, 1, 71, 0.0725221422800709, 21.653727843843154, 0], [0, 0.23170487164803383, 0, 0, 0.13949645576070085, 0.36372355843653065, 0.05784970320940614, 0.20722541094532856, 0, 32, 0, 0, 0, 1, 1, 128, 0.06582760650435633, 27.20374325889146, 0]], [[0.17913818978377055, 0, 0.022271729106308882, 0.18488751660310365, 0, 0.32775440850421744, 0, 0, 0.28594815600259954, 32, 0, 0, 1, 1, 0, 2, 0.0, 48.0, 0], [0.21767222611659195, 0.3339592053951001, 0, 0.39386404539254205, 0.018408935760408418, 0, 0, 0, 0.03609558733535758, 32, 0, 1, 0, 1, 1, 128, 0.09997800204917702, 48.0, 0], [0, 0, 0.053965257937480776, 0, 0.07564381536174106, 0.11122547042832881, 0.34162096358471167, 0.23407592295400123, 0.18346856973373646, 3, 1, 1, 0, 1, 1, 128, 0.01248339803243374, 15.13995965302991, 1], [0.018626626181861606, 0.38879365587903886, 0, 0, 0.08747908167257522, 0.3751675258117877, 0, 0, 0.12993311045473674, 20, 1, 1, 0, 1, 1, 91, 0.1, 1.5303256169925739, 0], [0.010668097087222028, 0.4294946179239461, 0.13178916995183657, 0, 0, 0.4279290281687588, 0, 0, 0, 1, 0, 1, 1, 0, 0, 101, 0.1, 33.36005947752008, 0], [0.08339264243841171, 0.021403037011623383, 0.025084450840950012, 0.32999086380004883, 0.01019161194562912, 0.16248297691345215, 0.1345221996307373, 0.03211229294538498, 0.2008199244737625, 23, 1, 1, 1, 0, 1, 61, 0.09711657166481019, 40.86248588562012, 0], [0.08292642688232009, 0.30697652084581717, 0.05649952830296807, 0, 0.14912715896753645, 0.09738927179319601, 0.10780955496093073, 0.08049548865468675, 0.11877604959254477, 19, 0, 1, 0, 0, 1, 124, 0.0005815094685760494, 38.49222174181113, 0], [0, 0.10701879696529247, 0.164781971207755, 0.017952513447275387, 0.20055770273798473, 0.3649838061311821, 0, 0.11696140627389127, 0.02773967419029692, 32, 0, 1, 0, 1, 1, 61, 0.002644511386695995, 36.080155637308806, 1], [0, 0, 0, 0.34794099355085106, 0, 0.01423123826978699, 0, 0, 0.6378277681793619, 1, 0, 1, 0, 0, 1, 48, 0.1, 1.4800000190734983, 1], [0, 0.5917617292618692, 0.05927133206581947, 0.12374094058771962, 0, 0.1441744344034655, 0, 0.0810515636811262, 0, 32, 0, 1, 0, 1, 0, 122, 0.0, 48.0, 1], [0, 0.5741232357149799, 0, 0.030790394407798863, 0, 0.08496859839210058, 0, 0, 0.31011777148512043, 24, 0, 0, 0, 1, 1, 2, 0.1, 1.4800000190734877, 1], [0.2333963713637669, 0.10031405250747977, 0, 0.48037172756052793, 0, 0.0519101421477703, 0, 0, 0.1340077064204551, 1, 0, 0, 0, 1, 1, 60, 0.1, 1.480000019073496, 1], [0.09277328710488988, 0.27689686711921757, 0, 0, 0.031231206387107236, 0, 0, 0.5017918829500518, 0.09730675643873335, 32, 0, 1, 0, 1, 1, 36, 0.05228788354327571, 22.52410901731025, 1], [0.2714608628699849, 0.3990811350450071, 0, 0.14752870121755923, 0.08518285972369768, 0, 0.05669517768657496, 0, 0.04005126345717613, 1, 1, 0, 0, 1, 0, 2, 0.1, 46.97565037650543, 0], [0.27893331330695464, 0.22974372507818513, 0, 0.3679815063991229, 0.013420189170717618, 0, 0, 0, 0.10182162741104328, 20, 0, 1, 0, 1, 1, 12, 0.0165460109759191, 18.68651224380943, 0], [0.0909108820589208, 0.5671957811477579, 0, 0, 0.02492721809002392, 0.2864919038093124, 0.03047421489398501, 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.06894938133868687, 1.4800000190734863, 1], [0.10853979935915098, 0.35055449935788136, 0.11315105889110239, 0.06905177059573422, 0.21995535885691733, 0.03757412442834516, 0.021927876978411413, 0.07749025422866615, 0, 11, 0, 1, 1, 1, 1, 128, 0.09480442163818074, 24.797342026662605, 0], [0.06201437713066878, 0.12756028665720456, 0.07121817736604032, 0, 0, 0.3682085851411333, 0, 0.12999702786855727, 0.2400133230341937, 20, 1, 1, 0, 0, 1, 126, 0.09485615505450255, 24.987985084984828, 0], [0.05422594756314397, 0.2886835804653019, 0, 0, 0, 0.24193619983250875, 0, 0.11568459362369103, 0.29180821685797065, 8, 0, 1, 0, 1, 1, 76, 0.1, 31.761156779430387, 1], [0.1861363173053297, 0.5128217723289593, 0.04204081483366071, 0.027478581912906898, 0, 0.21531624903136007, 0, 0, 0.01620626458778334, 32, 0, 0, 0, 1, 1, 67, 0.0, 15.71522187836332, 1], [0.11656437562624992, 0.24564690063525307, 0.07814041407703871, 0.14330052886625047, 0.07018058862954883, 0.09743512215386174, 0.057335912649395346, 0.1321429434374859, 0.059253213924915955, 27, 0, 1, 0, 1, 1, 26, 0.023601646618101525, 26.684768086600748, 1], [0.2108586672321761, 0.4703772481837154, 0.010347354947016189, 0, 0, 0.13775659475571986, 0, 0.05149823640385788, 0.10177629778649497, 32, 0, 1, 0, 1, 1, 126, 0.0841729378197147, 18.287016460002214, 0], [0.2843523745565139, 0.22879937608190717, 0, 0, 0.06631566742649488, 0.3131320943766106, 0, 0.057812102687787546, 0.04952999181593114, 32, 0, 0, 0, 1, 1, 117, 0.021825040286688302, 15.919299942378913, 1], [0.054990006880134557, 0.24300283558599847, 0.030553209299021684, 0, 0.015217811101224417, 0.26072112815491205, 0, 0.18656029306015223, 0.20768386035952568, 5, 1, 1, 0, 1, 0, 38, 0.009096640353116017, 20.169365223798362, 0], [0.07636486550211662, 0.3224881959876424, 0.023219275180643918, 0, 0.12272190938116351, 0.0892209409437586, 0.172356451721539, 0, 0.17913126740316876, 14, 1, 1, 1, 0, 0, 116, 0.08100415430545024, 10.255576211882236, 0], [0, 0, 0.17676658050017668, 0, 0.08013200103164465, 0, 0.48524868804969357, 0.16705414853941178, 0.0907985818790733, 32, 0, 1, 0, 1, 1, 119, 0.07014613903862053, 1.4800000190734868, 1], [0.4046046599482391, 0.16542758049084663, 0.04592729409676499, 0, 0.36797593873692297, 0.013322008525159974, 0, 0, 0, 12, 0, 1, 0, 1, 1, 56, 0.010030024731067696, 5.91694980287402, 1], [0.08989086128329726, 0.42218941298433976, 0, 0.09620322485798019, 0.16788720232751764, 0, 0.10769321640958031, 0, 0.11613608213728487, 11, 0, 0, 0, 1, 1, 117, 0.069870193783375, 11.740754774889016, 0], [0.21424959284238146, 0.20797918042658556, 0.08168684978321107, 0, 0.15881392648130865, 0.2243788926659205, 0.024823677124534, 0, 0.07816845936319626, 29, 1, 0, 0, 0, 1, 116, 0.08612425302643882, 47.6305485643988, 0], [0.05423005468882434, 0.38548550963020817, 0.09495996769994824, 0, 0.2205675664962187, 0.0619316625756757, 0.03802630334025683, 0.056863935856053524, 0.08793499971281452, 32, 0, 0, 0, 1, 1, 85, 0.07473528010234402, 19.89272889954652, 1]]], 'random_full_train_performance': [-1.5093419551849365, -0.830283522605896, -1.4581928253173828, -0.8750349879264832, -0.9876929521560669, -0.9161890745162964, -0.864062488079071, -0.8664753437042236, -1.687941074371338, -0.82343590259552, -0.8526705503463745, -1.2824517488479614, -0.8429969549179077, -1.0062391757965088, -0.865248441696167, -0.8681933879852295, -0.8945295214653015, -0.8935175538063049, -0.8773691058158875, -0.8328214287757874, -0.8460307717323303, -0.8324068188667297, -0.855530858039856, -0.9454211592674255, -0.8985024690628052, -1.4410817623138428, -0.8986577987670898, -0.9026172161102295, -0.8559422492980957, -0.8382471799850464]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.055 MB of 0.055 MB uploadedwandb: - 0.055 MB of 0.081 MB uploadedwandb: \ 0.055 MB of 1.259 MB uploadedwandb: | 1.259 MB of 1.259 MB uploadedwandb: / 1.259 MB of 1.259 MB uploadedwandb: - 1.259 MB of 1.259 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:            eval/runtime ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÇ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÑ
wandb: eval/samples_per_second ‚ñÑ‚ñá‚ñÑ‚ñá‚ñÅ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñà‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñÖ
wandb:   eval/steps_per_second ‚ñÑ‚ñá‚ñÑ‚ñá‚ñÅ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñà‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñÖ
wandb:             train/epoch ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñà‚ñÖ‚ñÇ‚ñÜ
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñà‚ñÖ‚ñÇ‚ñÜ
wandb:         train/grad_norm ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÇ
wandb:              train/loss ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.83825
wandb:             eval/runtime 10.7663
wandb:  eval/samples_per_second 92.882
wandb:    eval/steps_per_second 5.852
wandb:               total_flos 1.1492337557019034e+17
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 0.19382
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.985
wandb:               train_loss 1.14271
wandb:            train_runtime 512.4006
wandb: train_samples_per_second 19.508
wandb:   train_steps_per_second 1.22
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/sd03w17x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260101_152436-sd03w17x/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
